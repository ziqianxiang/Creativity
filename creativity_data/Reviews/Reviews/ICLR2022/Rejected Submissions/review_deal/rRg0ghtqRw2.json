{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper tackles the problem of Unsupervised Environment Design to train more robust agents. The proposed method trains RL agents by generating a curriculum of training tasks to enable agents to generalize to many tasks. The key contribution is an algorithm to generate this curriculum by incremental edits of the grid world environments. The reviewers all agreed that the paper is well-written and the method is intuitive. However, the weakness of this work is also obvious: the proposed method is only evaluated in grid worlds, and it's unclear how the editing approach can be easily generalized to more complex environments. This submission would benefit from more comprehensive evaluation in non-grid world environments, especially given that the compared baselines have results in other environments too."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper introduces the idea of adding small incremental changes to grid world levels, to create an automatic curriculum for navigating agents.",
            "main_review": "\n\n**Strenghts:**\n\n- The idea is straight-forward.\n- The paper is well-written.\n- Table 1 is a nice comparison\n- S.4 The results seems mostly good. (On some levels, DR performs on-par and that's never really explained)\n\n**Weaknesses:**\n\n- W.1 I think it's hard to generalize this method to other domains without a lot of domain knowledge on what constitutes small incremental changes for the agent. For example I could see this being applied to generate a curriculum for a robot arm to learn grasping different objects by incrementally deforming a mesh but I think a lot of trial-and-error would be required to find the right deformation size that's enough change to be interesting but not enough to make grasping the object physically impossible. So maybe the authors could give a few examples of how this applies to other possibly non-discrete domains or if the paper should get rejected, include an experiment in a non-gridworld setting (i.e. something where it's not trivial what the incremental changes are), like the POET paper.\n- W.2 Clarity - (a) Is Fig.4 reported on a held-out test set or during training? Because if it's from during training, the level difficulty would dictate the meaning of the return plot. So maybe a different metric would be better like return divided by  difficulty metric (like shortest path length) or something like that. (b) How do you calibrate the threshold that the PLR score has to match?  (c) In Fig.6, why is DR performing on-par with ACCEL in 3 cases?\n\n**Nitpicks & Questions:**\n\n- Q.1 Fig.2 is well-made but unintuitive and I don't think in its current state it does a great job at summarizing your method: (a) The order of things is unclear. Adding numbers to indicate the order of operations would be helpful. (b) The generator isn't really crucial in that sure, it creates the initial set of levels but shouldn't it generate them into the level buffer on the left? From the current diagram it's not clear if the generator is a one-off or a recurring thing. (c) What's the \"Curator\" in practice? The level buffer + threshold? I.e. if a given level has a PLR value above the threshold, it's considered ready for play? This is a very personal opinion but I think \"Curator\" implies more of an active system than what's currently there. (d) Why is there a red-tinged level in the level buffer? (e) Why are there 2 arrows going from the student into the level buffer? (f) What is \"update buffer\" from the student?\n- Q.2 I feel like you're glossing over a lot of non-uniform domain randomization literature that's also maintaining a population of environments and tries to make them harder one step at a time like ADR, and SS-ADR [1,2]. Wouldn't these methods also work (similarly) to generate the curriculum for your agent?\n\n**References:**\n\n- [1] Mehta, Bhairav, Manfred Diaz, Florian Golemo, Christopher J. Pal, and Liam Paull. \"Active domain randomization.\" In Conference on Robot Learning, pp. 1162-1176. PMLR, 2020.\n- [2] Raparthy, Sharath Chandra, Bhairav Mehta, Florian Golemo, and Liam Paull. \"Generating Automatic Curricula via Self-Supervised Active Domain Randomization.\" arXiv preprint arXiv:2002.07911 (2020).\n\n",
            "summary_of_the_review": "The paper feel like it's a small incremental change to existing literature and since I can't really imagine how this is going to generalize to non-gridworld domains, I can't recommend acceptance at the moment. If the authors could maybe include additional discussion of how their method applies to other environments or better, include experiments in non-gridworld settings, I would change my rating.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces Adversarially Compounding Complexity by Editing Levels (ACCEL). ACCEL is an Unsupervised Environment Design (UED) algorithm, a method of generating a curriculum of environments so as to train agents that generalize well to either a training distribution of environments or off-distribution environments. ACCEL bares similarity to a recent addition in the UED literature, Robust PLR, but importantly uses an editor to modify previously seen environments. Edited levels are used if they satisfy a criterion based on the PLR score, and hence this can be thought of as a sort of evolutionary algorithm.\n\nAfter introducing ACCEL, the paper presents a series of experiments in the Lava grid and Minigrid environments, benchmarking ACCEL against a suite of UED and simpler baselines. It demonstrates emergent complexity generated in algorithms' curricula (with metrics such as number of lava tiles/blocks, shortest path length) as well as generalization to held-out levels.",
            "main_review": "This paper puts forth compelling evidence that it is the state-of-the-art UED algorithm, at least within experimental domains it considers. Its suite of baselines (DR, PAIRED, PLR, Minimax adversarial) are to my knowledge well-chosen: PAIRED and PLR represent prior state-of-the-art UED approaches, and DR and Minimax adversarial simpler baselines. It highlights, by certain measures, higher levels of emergent complexity as well as superior performance on heldout environments both within the training distribution and outside of it. While some comparisons (esp. the Lava ones) show modest gains, others show more meaningful ones, especially the PerfectMaze heldout environment performance. Aside from the main comparison, it contains ablation studies helpful for understanding the utility of various choices made.\n\nThe paper is well-written and fairly easy to understand. The authors make effective summaries: Figure 2 and Algorithm 1 (especially with the highlighted difference from PLR, which can be confusing otherwise) are useful, and Table 1 is helpful as well.\n\nMy main criticism is that the evaluations seem somewhat limited in scope. While Lava and MiniGrid comparisons of this form are standard for UED methods, they are not the only ones, and in particular, they seem to be the most obvious contenders for an editing-type approach to be successful. As the paper points out, these environments have a natural empty-room starting point from which to build the curriculum and edit off of. These edits naturally seem to curricularize and, small edits easily make for what can be readily interpreted as reasonable additions for training: edits that functionally change how an agent must explore, encouraging exploration generally, or data augmentations that change irrelevant bits. Other benchmarks that existing works use (CarRacing, MuJoCo Hopper, Procgen) intuitively have a different flavor. This makes me wonder whether this method is useful beyond these sorts of environments that this sort of editing process is intuitively suited for.\n\nThe ablation study also raises questions. If I am interpreting this correctly, it seems to show that, at least on MiniGrid metrics, several versions with pieces stripped out perform statistically very similarly, except when editing is removed, which substantially drops performance. This makes me think two things. (1) It heightens my worry about the limited scope of evaluations, where editing seems tailor-made to succeed. (2) It makes me wonder whether something a good deal simpler than that proposed algorithm works just as well.\n\nOne choice that I found odd was the inclusion of Figure 4 in the main text but the placement of Figure 15 in the appendix. While emergent complexity is interesting to see, it is hard to judge whether obtaining higher scores on these metrics is objectively \"good\" (and e.g. I can imagine ways in which adding more lava tiles is not particularly useful towards an end performance objective. Figure 15 is, on the other hand, a more straightforward comparison of methods.\n\nMinor: Figure 4 color choices are confusing. If I am reading correctly, (b) contains information entirely about the ACCEL runs, but it uses the same color scheme as the baseline.",
            "summary_of_the_review": "This paper presents a novel UED approach that presents performance gains on several baselines. My chief concern is that the benchmarks are limited in scope to situations that seem most likely for an editing process to succeed, whereas prior work demonstrates efficacy in domains that seem less suited for editing approaches. I recognize that this is an intuitive judgement about the benchmarks on my part; if the authors can put forth a compelling argument that the experiments are not so limited in scope, I would be inclined to raise my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The work proposes an extension to the recently proposed \"Robust Prioritized Level Replay\" (R-PLR) method that aims to train RL agents capable of generalizing to a large range of parameterized tasks by curating a curriculum over training tasks. The authors show that by editing levels that are at the frontier of the current agent capabilities instead of proposing novel ones by random sampling, the performance of agents trained with R-PLR can be further increased.",
            "main_review": "Strengths:\n\nThe main idea of the paper is intuitively reasonable and the experimental section shows promising performance on the investigated tasks. The related work section is extensive and adequately situates the conducted research in a broad context.\n\nWeaknesses:\n\nAlthough the actual idea behind the paper is very incremental, the reader is still left with questions particularly w.r.t. details of the method and Remark 1:\n- In Remark 1, the authors state that if their method finds a Nash Euqilibrium, then the student policy is following a minimax regret strategy. I understand that the authors take this result from the Robust PLR paper, to which their method is very similar. However, I have certain concerns regarding this statement:\n    - The proofs in the Robust PLR paper are relying on the assumption that the replay teacher is utilizing the true regret to guide its sampling. However, there is no guarantee that either the MaxMC or PositiveValueLoss are actually estimating the true regret in a realistic setting. If e.g. an agent is not observing any reward signal in a sparse reward task, it can have zero residuals by predicting a value of 0 in all states, although there may exist a policy solving the task. In this case, MaxMC and PositiveValueLoss would not estimate the true regret.\n     - By using the level editing scheme, the second teacher in the dual curriculum seems to employ a different utility function than the constant one assumed in Robust-PLR. Does this change the theoretical results?\n     - These disconnects between the theory in the Robust-PLR method and the presented method should be carefully addressed in order to avoid false conclusions from reader of the papers.\n- Looking at Table 5, it is unclear whether the additional environment interaction required to estimate the regret on the edited levels is taken into account in the presented results. Table 5 already shows that the number of environment steps differs by around 10% between methods. Can the authors explain the reason for this difference? Further, the plots in the main paper use \"Student PPO Updates\" as the x-axis unit. I guess that this unit only takes into account the environment steps taken in levels WITHOUT the stop gradient, i.e. does not take into account the additional required evaluations incured by the level generator? Given that multiple new levels can be generated for each level selected for student training, this could be a high hidden cost.\n- From the statements at the beginning and end of Section 4.2, it seems that ACCEL does not use the random level generation, as the authors state that \"for ACCEL we begin with empty rooms and randomly edit the block locations\" and further investigate an ablation where they edit uniformly sampled levels. If the regular ACCEL algorithm is not using the domain randomization, Algorithm 1 yields a wrong picture of the ACCEL algorithm, as the random sampling case will never take place in practice.\n\nGiven the large amount of space that the authors spend on high-level introduction and discussion w.r.t. related work, it seems unsatisfying that these technical details are not clarified in the main paper.\n\nMinor Points:\n - Question: Would it be possible to compare to some adaptive domain randomization methods that do not just randomly sample tasks? Or are they ill-suited for the investigated experiments? If so, why? It would be interesting to see how a method following a different methodology performs in these experiments.\n - Another related work that should be mentioned is the work on contextual MDPs [1], which is conceptually similar to the UPOMDPs used to model the space of possible tasks.\n\n[1] Modi, Aditya, et al. \"Markov decision processes with continuous side information.\" Algorithmic Learning Theory. PMLR, 2018.",
            "summary_of_the_review": "The incremental nature of the method in combination with the unclarity w.r.t. details and theory behind the method are not allowing me to recommend acceptance. I may improve my score if my concerns are adequately addressed.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "A simple method of curriculum design by editing existing levels solvable by the agent to more challenging ones. Empirically it performs well, as it resolves the difficulty of randomly searching globally for a challenging curriculum by building on already challenging ones incrementally. ",
            "main_review": "## Strength\nI am convinced that this is generally a useful problem to solve, and the existing results seem good. \nThe paper is easy to follow, the method is clearly explained, and the experiment is well presented.\n\n## Weakness\n\nI would say the environment considered are too simple, where the editing functions are fairly restrictive to the kind of \"changing the landscape of the level\" kind. I would have liked experiments where the dynamics of the game can be altered, for instance, the game of pong or break out with additional balls in play, or maybe mario or minecraft where the agent and the world interacts in richer ways. If we are calling them levels, I truly expect it should work for a more extensive level editor.\n\nIt is still unclear to me why the proposed method solves the problem of not having the editor generating levels that are straight up unsolvable. Or, put it in another way, how do you measure regret of a newly generated level without access to pi* ? Maybe I mis-read part of the paper but I could not find the answer to this fairly important question. Perhaps the answer to this question should be highlighted further?\n\nminor: \nI appreciated the consistent usage of pink to indicate ACCEL results, making the result section easy to read.",
            "summary_of_the_review": "there is not much to say except that this is an intuitive solution to a good problem. the results over existing tasks look promising, but it would really benefit from more complex environments and edits, where the agents and environments interact in more ways than simply pathing.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}