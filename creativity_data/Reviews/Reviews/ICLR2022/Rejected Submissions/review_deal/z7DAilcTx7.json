{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "In this paper, authors study adversarial examples from a distributional robustness point of view. Reviewers had several concerns about the work and all thought the paper is not above the accept threshold. In particular, they mentioned that the presentation and writing of the paper need to be improved and results (specially the ones presented in Section 2) are not significant contributions and novel. Given all, I think the paper needs more work before being accepted."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors present a connection between adversarial robustness and distributional robustness. A duality result is given linking the $\\infinity$-Wasserstein distance to an entropy regularisation problem. The problem is then solved using a Langevan-sampling based algorithm.\n\n\n",
            "main_review": "\n\nThe results linking adversarial robustness to distributional robustness in Section 2 are both presented without proof, and already known and published. There are quite a few papers that present similar results, for example: \nhttps://arxiv.org/pdf/1710.10016.pdf \nhttp://proceedings.mlr.press/v139/cranko21a/cranko21a.pdf\n\nA significant weakness of this submission is the mathematical rigour, which is sorely lacking. Many of the propositions are presented without proofs, and there are simple mistakes of measure theory present throughout. For example on page 3, $\\pi$ is declared to be a coupling on the product $\\mathcal X \\times \\mathcal X$, yet later $\\pi:\\mathcal X \\times \\mathcal X \\to [0,+\\infty]$ is written. When this should in fact be $\\pi:\\mathcal B(\\mathcal X \\times \\mathcal X) \\to [0,+\\infty]$. Classically couplings are required to also be probability distributions, so this should in fact be  $\\pi:\\mathcal B(\\mathcal X) \\otimes \\mathcal B (X) \\to [0,1]$. This is an aesthetic issue, but in Proposition 3.1 it has real significance since the constraint $\\pi(x,\\tilde x) = 0$ is meaningless. A proof of this proposition is not presented and so it is impossible to evaluate the rigour here. The same issue occurs through the proof of Lemma A.2, where, more egregiously $\\log ( \\pi(x,x') )$ is integrated without a treatment of infinities. \n\nI believe it would substantially improve the paper to provide a proper discussion of the literature and characterising how these results improve upon existing ones. As an editorial note, if a proposition or theorem is simple enough to present without proof then it should not be leveraged as a cornerstone result, or perhaps included at all. \n\nThe experimental results however are encouraging, in the time since submission have you been able to more thoroughly \nevaluate your model against other state of the art approaches?\n\n\n",
            "summary_of_the_review": "The authors present an encouraging algorithm that appears to perform well. However, section 2 seems to consist almost entirely of known results. The new theoretical results included however are often shown without proof and are lacking in rigour.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "- This paper proposes using $\\infty$-Wasserstein distributional robustness for improving model robustness. Specifically, to obtain a tractable solution, it adds an entropic regularization term to the primal form of $\\infty$-Wasserstein distributional robustness. The Langevin algorithm is used to sample adversarial examples for adversarial training.\n\n- Although the experimental results show the merit of the proposed method, the experiments are still humble with a lack of experiments on more impactful architectures such as WideResNet and comparison to other SOTA adversarial training methods.",
            "main_review": "**Strengths of the paper:**\n- The technical contribution of this paper is significant because it shows a connection between standard adversarial training such as PGD and distributional robustness. \n-  All obtained theoretical results are meaningful and have clear impacts.\n\n**Weaknesses of the paper:** \n- Although the obtained theoretical results sound reasonable and meaningful, the mathematical communication and proof in this paper seem not solid to me.  Specifically, in all derivations in Appendix, it only mentions $\\pi(x, x')$ with a lack of the label $y$. Still, it is unclear how to associate label y to Eq. (11) since Eqs. (9,10) only mention $x,x'$ and a WS distance between data distributions.\n- There are no proofs of Theorems 3.1 and 3.2 in Appendix. Moreover, Appendix is not well-organized to connect to the main paper.\n-  The experiments are too humble which need a comparison with more baselines using more advanced architectures. Moreover, the authors should report the robust accuracy for black-box attacks and Auto-Attack.",
            "summary_of_the_review": "This is a technically significant paper because it shows the connection between $\\infty$-Wasserstein distributional robustness and standard adversarial training PGD. All obtained theoretical results are meaningful and have clear impacts. However, the presentation and writing of the paper need to be improved to make it a more solid paper. \n- How to associate the label $y$ to the formulation?\n-  Appendix needs to be organized better and all proofs need to be consistent with the main paper.\n\nThe experiments are still humble which need a comparison with more baselines using more advanced architectures. Moreover, the robust accuracy for black-box attacks and Auto-Attack should be reported.\n\nIt is unclear why the proposed method is faster than PGD even a single adversarial particle is searched using the Langevin algorithm.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper establishes the equivalence between the adversarial training problem (under infinity-norm) and the Distributionally robust optimization (DRO) problem with \\infty-\\infty-Wasserstein distance. Then they propose to use an entropic regularizer to find the regularized optimal adversarial distribution more efficiently. Numerical results on MNIST and CIFAR-10 validate the performance of the proposed method.",
            "main_review": "The strength of this paper is a new adversarial training method that treats the problem as distributional-wise, as compared with traditional sample-wise adversarial training methods, and the proposed method has been shown to have better performance than some baselines.\n\nThe main weakness is the novelty of this paper. The technical or algorithmic contribution of this paper seems to be very limited. \nThe equivalence between the adversarial training under infinity-norm perturbations and the DRO under \\infty-\\infty-Wasserstein distance is very straightforward. Moreover, given such equivalence, the paper does not provide enough motivation about why we still want to consider the distributionally adversarial training. Also, instead of solving the DRO problem, the entropic regularizer is imposed in order to get a closed-form solution of the least favorable distribution. And it would be good to provide more theoretical insights.\nAlso, in the numerical results, how about the training time comparison between the proposed method and baseline methods? \n\nTypoes and minor issues:\n1 Page 6: “The latter point can be supported that the following…” should be “supported by the following”\n2 Page 7: the Lesbegue measure $q(\\tilde x)$ should be proportional to $exp(-U(\\tilde x))$\n3 Algorithm 1 line 9, should be $\\tilde{x}^{t}$ on the right hand side\n4 Figure 1 and 2: It would be better to use same color to denote the same method.",
            "summary_of_the_review": "In summary, although this paper show certain equivalences between adversarial training and DRO, it is mostly a straightforward result under the infinity norm, and the overall contribution of this paper seems to be limited.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}