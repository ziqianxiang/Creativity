{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The reviewers think the proposed method is well motivated and interesting. However, the novelty needs to be improved. At the moment, the paper seems to be a minor improvement over existing works."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies existing sampling schemes employed in training graph neural network architectures (e.g., FastGCN [Chen et al.] and LADIES [Zon et al.]) that are improvements to graph convolutional networks (GCN) [Kipf & Welling 2017] from a matrix approximation perspective (looking a Frobenius norm).  The paper focuses on layer-wise sampling (cf. node and subgraph sampling) and observes that there are two drawbacks in common practice layer-wise sampling. First drawback is current probabilities (probability distributions) used for sampling are sub-optimal. The reason the paper gives is that a core assumption made by certain GCN schemes such as FastGCN and LADIES do not hold in datasets such as Reddit and OGB. Second drawback is the implementations of these schemes slightly deviate from their respective theoretical results. The implementations use sampling without replacement, and thus introduces bias.  To this, the paper presents new sampling probabilities (uniform proposal distribution), and an algorithm to reduce the bias. With these adjustments, training of GCN converges faster and potentially leads to higher prediction accuracy (for node prediction). Theoretical analysis is presented and experiments are conducted on common benchmarks (Reddit, ogbn-{arxiv,proteins,products,mag}).\n",
            "main_review": "Highlights:\n- Paper proposes uniform distribution over nodes in the batch when training\n  - Looks at the empirical Frobenius norm (matrix approximation error) of the difference between a non-sampling and sampling batch for the first layer outputs. Empirical results show a reduction when using uniform distribution.\n  - Applies regression analysis between the $\\ell_2$ norm of one row $i$ in $\\boldsymbol{H}^{(l)}\\boldsymbol{W}^{(l)}$ and corresponding column $i$ in $\\boldsymbol{P}$ for different layers $l$. Empirical observation of instability and potential negative correlation demonstrates that the LADIES assumption of $||(HW)_{[i]}|| \\propto ||P^{[i]}||$ is violated.\n  - Theoretical analysis is adapted from LADIES (Zou et al. 2019) result. Assumption here is when the assumption in LADIES is not met ( $||\\boldsymbol{H}\\boldsymbol{W}_{(i)}|| \\not\\propto$ corresponding $\\ell_2$ norm of column $(\\boldsymbol{Q}\\boldsymbol{P})^{(i)}$), then the proposed method will work well. \n\n- Paper proposes algorithm to reduce bias\n  - Provides analysis of non-uniform sampling without replacement in FastGCN and LADIES\n  - Gives simple recursive weighted average adjustment to sampling without replacement and runtime complexity\n\nStrength:\n- Empirical observations and analyses are interesting\n- Results show improvement in prediction accuracy\n\nWeakness:\n- Theoretical analysis needs more rigor\n- Analysis and comments as it relates to network structure would be helpful\n- Dataset dependent and limited\n- Incremental improvement to LADIES\n\nMisc:\n\n- Section 4.1: second paragraph, second line, \"improves\"\n- Section 4.4: extra $||$ in line 4\n- The way the citations are applied can be improved (Section 1: first paragraph, first sentence)\n\n",
            "summary_of_the_review": "Although the paper presents interesting empirical observations and analysis, mainly, it is dataset centric. The theoretical justification of uniform distribution is weak -- again, using uniform distribution comes down to data dependency.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose remedies to address sub-optimal sampling probabilities and the approximation bias induced by sampling without replacement  issues in layer-wise sampling methods of GNNs.",
            "main_review": "Strength:\n\nThe summary of related work is good and clear. The authors provide theoretically analysis which can be verified by their experiments.\n\nWeakness&Advice:\n\n1. Need citations after “the phenomenon of ‘neighbor explosion’ ”.\n\n2. What is the running time of your method compared to the existing layer-wise sampling methods and baseline methods?\n\n3. What is the regression result of your proposed sampling method with regard to $H^{(l)}W^{(l)}$\n\n4. The visualization in Figure 3 needs to be improved. The font size is too small and the curves are messy.",
            "summary_of_the_review": "The novelty and difference with existing works need to be emphasized. I'll consider raise my score if the authors can address my concerns properly.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper revisits layer-wise sampling methods of graph neural networks by addressing issues such as sub-optimal sampling probabilities and the approximation bias due to the usage of sampling without replacement. This paper also presents a new metric to evaluate the performance of the sampling strategy. Through comprehensive experiments, the authors validate the effectiveness of their proposed methods.",
            "main_review": "**Strengths**\n\n(1) This paper is the first to address the issue of strong assumption, which induces a sub-optimal solution.\n\n(2) Overall, the paper is well written. In particular, the Notations and Preliminaries section is well discussed. \n\n(3) This paper provides comprehensive experiments to show the effectiveness of the proposed learning schemes. \n\n**Weakness**\n\n(1) The time complexity is an important issue of sampling schemes of graph neural networks. However, there is no empirical study of the efficiency of the proposed method.\n\n(2) What's the main intuition behind the sampling probabilities in Equation (3)?\n\n",
            "summary_of_the_review": "Overall, I vote for accepting. The paper well presents their proposed method with motivation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper introduced a new layer-wise sampling probability as well as a new debiasing sampling algorithm to obtain a better approximation accuracy.",
            "main_review": "## Strengths:\nOverall, the presentation of this paper is clear. The experiments on the large-scale dataset look comprehensive.\n\n## Weaknesses:\n### 1. The theoretical contribution is limited.\nThis paper proposes a  sampling probability construction that abandons the hidden embedding matrix $H$ and the parameter matrix $W$. The core theoretical contribution is to prove that such construction has a better variance than that of `LADIES`. However, this result is weak:\n- Limited scope: Lemma 1 only prove the new sampling probability is better than that of `LADIES`. The superiority of new sampling probability over the other layer-wise sampling method, such as FastGCN and AS-GCN, is still unclear. It’s hard to say the new sampling probability is superior to the other layer-wise sampling methods from the theoretical perspective.\n- Strong assumption: To complete this proof, the authors give a very strong assumption (Assumption 1) without any justification of the existence and rationality.  There is no in-depth discussion about why and when this assumption is held in practice. For instance, Since the assumption ($C_1/C_2^2 \\leq 1$) is highly related to $HW$,  does this assumption still hold during the training process? Therefore, such a strong assumption makes Lemma  1 impractical and its theoretical value to the real application is limited.\n\n\n### 2. The experimental results are weak.\n- The important layer-wise baseline `ASGCN` is missing.\n\n- On Reddit dataset, the results of several baselines are significantly lower than the results in the existing literature. For example, the original F1-score of `GraphSAINT` is  0.966, while in this paper, the F1-score is only 0.8947. I guess this inconsistency may due to the different data splitting. However, the author doesn’t give the details of the data splitting setting, nor gives the reason why does not use the public splitting.\n\n-  As shown in Table 2, this paper only applies the new sampling probability and debiasing method to `LADIES` and observes an improvement. However, even with such a new sampling probability and debiasing approach, the performance is still worse than the other baselines.  Additionally, in Figure 3, the curve of  `LADIES+flat+debiased` is lower than that of other baselines. It’s hard to say the new method is better than that of other methods.\n\n-  There is no time comparison,  especially the sampling time comparison,  across different methods. Since the debiasing operation may introduce the additional computation overhead, I’m curious about such overhead in practice.\n\n\n### 3. Minor issue\n- In Figure 1, why `FastGCN` achieves a better approximation on `ogbn-proteins` than `LADIES`?\n- In my understanding, the sampling probability and sampling method are two core parts of a layer-wise sampling method. Therefore, it is improper to treat the new approach as the extension of `LADIES` and use the notation `+flat+debiased` to represent the new approach.\n\n## Reference:\n[`FastGCN`] FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling\n\n[`ASGCN`] Adaptive Sampling Towards Fast Graph Representation\n\n[`LADIES`] Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks\n\n[`GraphSAINT`] GraphSAINT: Graph Sampling Based Inductive Learning Method\n",
            "summary_of_the_review": "Overall, I think that this paper is not ready to be accepted. The details are elaborated in main review.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}