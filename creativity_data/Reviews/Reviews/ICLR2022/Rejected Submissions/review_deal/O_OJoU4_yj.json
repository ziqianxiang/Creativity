{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper studies stability issues of GNN training when data are limited. The key contribution of this work is to use reweighted self-training and negative sampling to stabilize GNN. Multiple reviewers raised major concerns on the technical novelty, experimental setup, comparison, and results. No response was provided during discussion. I recommend this submission be rejected."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper discusses *unstable* training procedure of graph neural network when training data is extremely limited. The author proposes use self-training and negative sampling to mitigate this issue. Specifically, it uses high-confidence prediction of the model as training seeds and control the importance by population. The result shows improvement on several node classification dataset.",
            "main_review": "Both the problem setting and technical contribution of this paper is problematic. First of all, the proposed method is basically a bootstrapping-style self-training method, it needs to compare with other self-training methods instead claiming it can improve the performance. The comparison in the experiment is unfair, as proposed method utilizes way more training seeds. Second, the **unstable** phenomenon is not defined in a mathematical way. It's widely known that few training seeds leads to larger variance. The terminology here looks like proposing a new measurement without any mathematical support.\n\n1. **Lack of proper baselines**\nAs a self-training method, author should compare more baselines that considers additional training inputs. Also, experiments on larger benchmarks are necessary. It's a common sense that an algorithm can work well on Cora/Citeseer/PubMed because the network is simple and small.\n\n2. **Limited novelty**\nThe threshold-based detector has been widely used in out-of-distribution detection [1]. The proposed method is neither novel nor insightful.\n\n\n\n[1] Ren, Jie, et al. \"Likelihood Ratios for Out-of-Distribution Detection.\" Advances in Neural Information Processing Systems 32 (2019): 14707-14718.",
            "summary_of_the_review": "Overall, I recommend a rejection to this paper. The problem it studied is a common challenge (amount of training data vs. variance) in semi-supervised learning, while experiments are not conducted properly to support the effectiveness of it against other similar approaches. Moreover, the technical writing in the paper contains a lot of hand-waving example instead of sound mathematical justifications.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a self-training method with negative sampling for node classification on few-labeled graph data. The proposed method applies data augmentation (i.e., pseudo label) and negative sampling regularization to augment node classification model. Experiments are conducted to show that the proposed method outperforms some baseline methods. ",
            "main_review": "Strength \n\n1 - Propose a new method for node classification with few-shot labels.\n\n2 - Presentation is clear for me. \n\nWeakness\n\n1 - The novelty of this work is limited. \n\n2 - Miss important related works. \n\n3 - Experiments should be improved.   \n\nDetailed Review\n\nThe novelty of proposed method is limited for me. The major contribution: data augmentation strategy (pseudo label) and negative sampling regularization, is simple and intuitively, depending on the threshold value. There are many works proposing better data augmentation strategies or pseudo label generation, such as:\n\nGraph contrastive learning with augmentations, NeurIPS 2020 \n\nBig Self-Supervised Models are Strong Semi-Supervised Learners, NeurIPS 2020\n\nMore importantly, this paper lacks discussion and comparison of important related works. The authors seem to be not aware of graph few-shot learning works. There are many studies working on graph learning with few-shot labels, such as followings:\n\nGraph meta learning via local subgraphs, NeurIPS 2020\n\nGraph few-shot learning via knowledge transfer, AAAI 2020\n\nMeta-gnn: On few-shot node classification in graph meta-learning, CIKM 2019\n\nNode classification on graphs with few-shot novel labels via meta transformed network embedding, NeurIPS 2020",
            "summary_of_the_review": "The novelty of this work is limited. It lacks important related work (graph few-shot learning) discussion and comparison. In my opinion, this work is not suitable for ICLR. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a stabilized self-training with negative sampling method to improve GNN performance on the node classification task when only few labels are provided for training. The main idea of this paper is to use the reweighted self-training method and the negative sampling method to stabilize GNN. ",
            "main_review": "Strengths:\n1. This paper addresses an important problem of semi-supervised node classification with very few training labels. \n2. The few label problem is well motivated with a clear problem formulation. \n\nWeakness:\n1. The technical novelty of this paper is very limited. The proposed method is simply adapted from existing self-training and negative sampling approaches with incremental changes. \n2. The reweighing scheme for pseudo labels is somewhat ad-hoc. The authors need to provide more convincing justifications why the reweighing approach can help mitigate the training unstability.\n3. The proposed negative sampling method operates on labels rather than embeddings. However, When you choose negative samples that are not directly connected to a positive sample vi, there is no guarantee that the positive and negative sample would have different labels. How would you address this issue?\n4. The proposed method is not compared with more recent baselines on self-training methods, for example,\nZiang Zhou, Shenzhong Zhang, and Zengfeng Huang. 2019.   Dynamic Self-training  Framework  for  Graph  Convolutional Networks.arXiv preprintarXiv:1910.02684(2019)\nKe Sun, Zhanxing Zhu, and Zhouchen Lin. 2020.  Multi-Stage Self-Supervised Learning for Graph Convolutional Networks. InAAAI.\n5. The writing and clarity of the paper need be improved for better readability.",
            "summary_of_the_review": "The technical novelty and originality of the paper are limited. The proposed method is an adaptation of the existing self-training and negative sampling strategies. This paper lacks comparisons with more recent self-training baselines to prove the effectiveness of the proposed method.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper considers the semi-supervised node classification on graph data with only a few node labels available. Under this extreme situation, the paper demonstrates the unstable performances of existing GNNs, such as GCN and DAGNN. To address the unstable problem, the paper proposes two strategies consisting of pseudo-labelling based self-training and negative-sampling based regularization. The experiments show that the two strategies work well on graph data with only a few node labels.",
            "main_review": "Strengths：\n1. The proposed problem is interesting in the graph learning community. The problem is similar to the topic of few-shot learning on graphs, which usually assumes that only a few labeled nodes or graphs are available. Compared with the existing few-shot learning methods, the paper assumes all the unlabelled nodes are available at the training stage and addresses it with popular training technics such as self-training and negative sampling.\n2. The paper is well organized with a clear problem definition and technical solution.\n3. The proposed two strategies are easy to be reproduced.\n\n\nWeaknesses:\n1. Although the paper tries to address the challenging and exciting problem, i.e., semi-supervised node classification on graphs with an extreme case when only a small subset of nodes have class labels, my major concern is that the self-training and negative sampling are standard methods and have been discussed in many graph learning papers, as referenced by this paper. Therefore, without significant technical improvement, I think the technical contribution is limited. \n2. I suggest the authors to make the Figure 1 more clear. It is not apparent to see the unstable training of GCN and DAGNN compared with the improved ones, due to the considerable overlap between 7 classes. For example, the variance of Class 3 in (a) seems to be smaller than the improved version shown in (c). \n3. For the stabilized self-training, the ablation experiments need to be performed to show the differences between the proposed stabilized self-training and existing self-training. \n\n4. Moreover, in implementing the stabilizer, a re-weighted pseudo-label loss (see Eq 5) is proposed, and the importance of each node is dependent on the number of their belonging classes. How to deal with the situation when a class has no node whose confidence is beyond the threshold? How to deal with the situation when a class dominates the most nodes on a graph? I suggest incorporating the neighbor information to design a more adaptive stabilizer.\n\n5. The negative sampling method, “The negative samples of a positive sample vi are the nodes that are not directly connected to vi in the graph G”, may bring false negative samples because even two nodes are not directly connected, they still have the same class. How to deal with this problem? \n6. I think the work can be better if the author considers problem 4 and 5. \n",
            "summary_of_the_review": "I think the main contribution is: the proposed problem is challenging and interesting, this paper considers the semi-supervised node classification on graph data with only a few node labels available. The main weakness is: the technical contribution is limited. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a self-training GCN framework. It refines the predicted results as pseudo labels and pseudo negative labels to train GCN, which is applicable to existing GCNs to stabilize the training process and enhance the training data.",
            "main_review": "Strengths:\n1. This paper proposes a stabilized self-training technique which the pseudo labels are update adaptively per epoch and the weights of different pseudo labels in the loss function are proposed to address the unstable issue of existing GNNs.\n2. The author chooses negative samples from the nodes that are not directly connected and designs a new loss function to make positive and negative samples as different as possible.\n3. The classification accuracy are reported when varying the number of labeled nodes per class in {1,3,5,10,20}, the results show the good performance compared with the baselines.\n\nWeaknesses: \n1. The proposed algorithm has some new ideas, such as negative samples, but the model of GCN is not changed.\n2. The authors said that the unstable issue of existing GNNs can be addressed by the weights of different pseudo labels in the loss function, which named stabilizer in the paper. However, it has not been clearly proved by theoretical analysis or experiment. \n3. The choices of negative samples and positive samples have some uncertainties. The authors should show the effect of the uncertainties. \n4. Although the authors compared the proposed method with some unsupervised methods and supervised methods, some self-training or pseudo labeling methods for GCN or semi-supervised learning should be considered.\n5. The performance of the method is not fully verified by experiment, for example, (1) the importance of stabilizer and the number of positive and negative samples are not reported. (2) the performance comparison between the proposed pseudo labels and existing pseudo labels.\n\nMinor problem:\n1. The figures can not match with the words in the body of article. For example, “The dashed lines are the ground-truth percentage of each class in the Cora dataset” or “the average number of predicted labels in percentage per class at each epoch”\n2. There are many problems in the article, some of the description details remain in doubt, some of the statements are not exact. For example, (1) In the second paragraph of 3.2, “Fi representing the output probability vector of node vi ∈ V”, however, V is the node set. (2) In the last paragraph of 3.2, the author proposed “a pseudo label in previous epoch will be removed in the next epoch if its confidence becomes low.”, however, it hasn’t been depicted in the paper. \n",
            "summary_of_the_review": "The present paper presents an effective framework for semi-supervised node classification on few-labeled graph data. The paper is well organized and the motivation of this paper is clear. Although this paper has some innovation, but the model of GCN is not changed. Besides, some comparison experiments with pseudo labeling methods are lack. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}