{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The authors consider the problem of training a fair classifier on decentralized data, and compare three methods: training locally, training the proposed FedAvg algorithm with local fairness, and a global fairness approach.\n\nThe reviewers agreed that the setting was interesting and novel, but had concerns about the writing quality, experimental setup, and, most importantly, the organization of the paper, with several reviewers complaining that necessary information was relegated to the appendix.\n\nOverall, this work is not quite ready for publication. With that said, the reviewers agreed that it was interesting and highly promising (it just needs refinement). Please seriously consider the reviewers' recommendations, which on the whole were very constructive and, if followed, should lead to a significant improvement in your manuscript."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper compares and analyzes the fairness guarantee(Demographic Parity in particular) provided in three different settings: training local models, training FedAvg with local fair training, training fair global model. This paper provides a method on how to enforce group fairness into federated learning by applying a current fair training method, FairBatch, into FL.",
            "main_review": "This paper provides a novel analysis on fair training in three different settings. I'm concerned about part of this paper's main theoretically claim that UFL provides weaker guarantee then FFL via FedAvg. It is unclear to me why it is necessary to compute the \\hat{y}|x,a across all clients instead of evaluating Demographic Parity just separately on individual clients. Basically in this scenario local clients are not collaborating and they are just training separate global model. In fact, client 0 and client 1 even use different models to produce \\hat{y}. Why does studying the joint distribution of \\hat{y}|x,a matters here?\n\nIt is also not clear how do the authors optimize the FairBatch objective in the FL setting. I assume the step of solving the optimal global to follow the standard FedAvg algorithm. It is not clear, however, how the group-specific loss is calculated. From Figure 4 it seems that the authors are summing over that loss over all the clients? Could you explain on this part?\n\nThis paper also compares the empirical results of FedFB with other fair FL training methods and demonstrates an advantage of using FedFB over other methods for fairness. However, the results of several baselines in this paper look confusing to me. In the section of client parity, the qffl accuracy is surprisingly low for adult, which does not match up with the results in the original qffl paper. It is also not clear to me why FedFB outperforms CFL in some cases. Could you explain why this happens?\n",
            "summary_of_the_review": "This paper focuses on fairness in FL from an interesting angle. However, the current comparison between UFL and FFL via FedAvg doesn't make sense to me. More explanations are also needed for both algorithm and empirical results.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a novel algorithm to train statistical models respecting fairness criteria like client parity or demographic parity. This problem has been investigated in the literature for the centralized setting and the authors propose to extend FairBatch (FB) to the federated setting. Finally, the authors show experimentally that their algorithm FedFB provides better fairness than when a server centralizes all the clients data. This work is quite novel but its theoretical statements solely rely on very specific use cases and lack clarity.",
            "main_review": "The authors proposition appears to be novel: fairness in Federated Learning is currently understudied, despite being a critical aspect for a wider adoption. Authors did a great job at introducing their work and rigorously used a mathematical definition for fairness evaluation (Demographic Parity) which is often lacking in this field.\n- Could you please explain in more details UFL ? \nUFL accounts for the case where clients do not participate to FL. With UFL, every client trains a fair model on its data. However, could you elaborate why considering a random classifier that makes a prediction by randomly selecting a client classifier makes a good baseline? \nFrom a theoretical point of view, I do not see yet the use of UFL nor what the resulting model means. Finally, I do not see what the disparity of UFL is measuring. Giving some context would be very appreciated.\nUnfortunately this work should have prioritized its main content over appendix: this work is not clear enough and always requires going to the appendix for full understanding of what the authors are claiming. Therefore, unless UFL is strongly important to the conclusions of this work, Section 3.1 could have been suppressed to focus on other sections instead.\n\n- Could you please elaborate on the statements of Lemma 4? If we consider a setting where clients return the same model, e.g. clients have identical data, wouldn’t FFL via FedAvg and CFL be identical problems while $q_0 \\neq q_1$?\n\n- Section 4 would really benefit from having the code of FedFB and explanation of the theoretical work in Section B.1. You state that “it [FedFB] provably converges under some mild technical conditions […] See Theorem 23 for more details”. I have the following questions regarding this theorem:\n  - This work seems to rely on Assumption 2-5 for the local loss functions that are standard in Federated Learning and on Assumption 1 “to have a decreasing direction in Lemma 22”. Could you please cite previous work using such an Assumption? Also, you say that with convexity Assumption 1 holds. Could you please prove it or provide references for the proof of that statement?\n  - Theorem 23 relies on the work of Li et al. to bound the impact of FedAvg over one iteration. As a result, the obtained bound only depends on the amount of local work $R$. It is standard to consider that the amount of aggregations $T$ goes to infinity but not for the amount of local work $R$. Am I missing something here? Please elaborate or point me to a reference? Especially considering that most work done on FL optimization (including the one of Li et al., or Wang et al. Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization) shows that the convergence rate and thus the asymptotic convergence bound are proportional to $R$\n  - Can you elaborate on the steps to obtain the bound obtained on $\\lambda_a$? It seems to me that the first term in the maximum is always smaller than the left term of the inequality as $\\alpha_t$ is positive.\n  - Lastly, do you know under which condition we could generalize to A>2?\n\n - The experiments consider 10 communication rounds and 30 local epochs when considering two clients. Also, the values of some hyperparameters are missing. The experiments like the theory are very specific and do not seem to cover realistic FL cases: could you communicate other parameters lr, batch size, $\\epsilon$ ?\n\n - Also, do you have an explanation why CFL is providing worse fairness than FedFB? The authors speak about extending with their work FB. It would be interesting to compare FedFb with FB where every clients data is centralized on the server.\n",
            "summary_of_the_review": "The authors proposition appears to be novel and with a genuine interest. Authors did a great job at scoping their work and rigorously used a mathematical definition for fairness evaluation.\nHowever, paper structure needs some rework: some keys components, required for the reader to understand the paper, are not in the main content but are in the appendix. Some elements (e.g. UFL) are presented as keys, but their impact on the overall demonstration is not clear and may be moved to appendix. Also, some claims are not fully supported yet.\nFinally, experimental cases, and some proofs, are not straightforward to generalize enough mainly due to binary class problems only and federating only two clients.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper tried to propose a generic federated learning method to train a fairness model. It compared the ",
            "main_review": "1. The 0-1 loss is a non-continuous function. Thus the most assumptions in B.1 are valid. For example, the convexity and twice differential assumptions. In addition, could you elaborate more on how to perform client updates in Algorithm 1? Given the noncontinuity of the 0-1 loss function, do you perform any approximation for the client updates? If there is no approximation, how can it be optimized? In addition, how can the DEMOGRAPHIC PARITY constraint be satisfied in FedFB?\n2. The experiments in the paper are not extensive. The proposed methods have not been compared with some recent papers on this topic. For example, \na. Du W, Xu D, Wu X, Tong H. Fairness-aware Agnostic Federated Learning. InProceedings of the 2021 SIAM International Conference on Data Mining (SDM) 2021 (pp. 181-189). Society for Industrial and Applied Mathematics.\nThe authors use the data setting in the above paper but fail to compare it.\n3. What is the data distribution among the clients in the experiments? It seems the authors use the iid setting across the clients and it is not a practical setting.\n4. The notations in the main body and appendix are not consistent. For example, in the main body  $\\cal A$ is used for the sensitivity attributes set but $[A]$ is used in the appendix. This type of inconsistency makes the paper really hard to follow. If I misunderstand something, please clarify these notations in the paper.\n5. The authors may also discuss the privacy of the proposed method. Is there any increased risk of leaking the information from passing $L$ other than only transmitting model weights $w$.",
            "summary_of_the_review": "1. The paper is not well-written and hard to follow. Some of the details are missing, which makes me hard to justify the claims.\n2. The complete convergence analysis of the proposed algorithm is missing.\n3. The experiments are not performed extensively. Thus, the claims in the paper are not fully supported.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigates how one can achieve group fairness under a decentralized setting. The authors develop a theoretical framework for decentralized fair learning algorithms and analyzed the performance of existing approaches including UFL, FFL via FedAvg, and CFL. They provide novel insights showing that UFL<FFL via FedAvg<CFL. They also propose a new federated fair learning algorithm FEDFB by letting each client share extra information about the unfairness of its local classifier with the server, which then computes the optimal samples weights for the following round of local training. The experimental results demonstrate that FEDFB achieves state-of-the-art performance, while still ensuring data privacy.\n",
            "main_review": "Strength:\n- The topic of fair federated learning is important and useful.\n- This paper first gives a theoretical analysis of existing algorithms for fair federated learning. I enjoy reading this part.\n- The empirical results are complete and confusing.\n\nQuestion:\n- ",
            "summary_of_the_review": "Overall, I recommend the paper. It considers an important problem, fair federated learning, analyzes and compares existing approaches, and proposes a convincing framework. The writing is nice and clear, especially the discussion in Section 3.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}