{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This work proposes a so-called self-supervised approach for few-shot learning. The self-supervision doesn't refer to the lack of use of any labels as in regular self-supervised embedding learning methods (here support sets are labelled), but refer to the fact that the query set's labels aren't used in their proposed objective. Instead the query labels are predicted by a primary network, which then uses these predicted labels to predict the ground truth labels on the support set. The support set label predictions can thus be used to derive a learning signal for the model. Some results are presented that suggest the method is competitive with respect to the state of the art.\n\nReviewers are quite split on this work. Even the reviewers who are technically leaning toward accepting this work (rating of 6) mention concerns that are worrying, e.g. reviewer Gmcb and wTnW both mention concerns related to the fairness of the evaluation. \n\nI too share similar concerns. First, the method in question is effectively a transductive method (as opposed to inductive, as are many of the baselines this work compares to), a distinction that the paper does not make explicit or address directly. This distinction is important, as it is well known that transductive methods have an advantage over inductive methods. I did try to look for some published transductive baselines. One is the method of Zhang et al. (2021a), which the authors do beat on mini-ImageNet, but not on CUB (and in fact, the paper only reports the results from Zhang et al. on mini-ImageNet, even though the original paper actually reports results on CUB, which I find odd). On CUB, for 1-shot, Zhang et al. (2021a) outperforms SPDN, while for 5-shot SPDN does only very slightly better. The paper is not clear as to whether the compared baselines are transductive or not in the cross-domain experiments either. \n\nSecond, by introducing a dual network that is separate from the primal network, the proposed model effectively is increasing the capacity of their model, relative to using only a primal network. This capacity is mostly used when performing the self-supervised optimization of the query labels, which would explain why this aspect of the proposed method is what yields the largest improvements. Given that capacity has a large effect on the performance of methods on few-shot learning benchmarks, I'm quite concerned that this is the more likely explanation for the (sometimes surprisingly large) improved performance.\n\nThat said, I don't find the paper entirely without merit. The label optimization procedure is neat, and is probably the most interesting innovation of the paper. The use of a primal-dual architecture on the other hand is more incremental, e.g. relative to architectures used in semi-supervised few-shot learning.\n\nOverall, at this point, given the lukewarm evaluation by reviewers and the lingering concerns (or at a minimum, lack of clarity) on the fairness of the evaluation, I'm afraid I'm not comfortable to recommend accepting this work as it currently stands."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposed a self-supervised learning framework and a dual network to improve the performance of few-shot learning. The dual network is based on GNN architectures and learn the relationship between the features from the support set and query set. Experiment shows improvement over the baselines on several few-shot learning benchmarks. ",
            "main_review": "Although predicting based a GNN structure is not novel in few-shot learning. The idea of self-supervision is interesting. \n\nThe notations need improvement. For example, the prediction L in equation (6) - (7) are not consistent equation (11) - (14). And i would suggest use other notation because of the confusion of loss function. \n\nFor figure 2, what is the difference between two types of samples with labels? what is the difference between two types of samples without labels? Although I can understand through the text but the figure can be improved a bit.\n\nThe ablation study, the authors divide it into two parts, i.e., self-supervised optimization and self-supervised learning. I do not understand why these two can be divided? Are there any details for how to do that?\n\nTo my understanding, the essence of the prediction from the GNN is still based on feature similarity. In this way, is there any feature representation visualization for the learned model please? I am interested in how the representation would change by using the self-supervised learning.\n",
            "summary_of_the_review": "The idea is interesting and seems to perform well. The writting needs improvement.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a prime-dual network for few shot image classification, where the support set and query set are used in a flip manner so that the labeled samples in support set can be compared with themselves to determine a loss for network update.",
            "main_review": "STREHGTH\nThe proposed method is easy to follow and understand, and the experiment results are promising when directly compared with state-of-that-art methods.\n\nWEAKNESS\n1. This method reminds me of the cycle consistency loss in the well-known CycleGAN approach, but since CycleGAN is not referenced and discussed in the paper, I was wondering if I misunderstood that or is there a reason for that? Although it's a distantly related work in terms of applications, the formulation of problem and loss functions seem quite relevant. If that's the case, it would be desired to discuss the connection to prior work for better justification.\n\n2. In ablation study, it appears that simply doing SSL is not helping much compared with baseline approach. Does it mean the discrepancy between training classes and unseen testing classes is still the dominant reason for generally low accuracy in few-shot learning task? If so, is it indicating that the SSL stage is not the major concern in such setting, and one should focus more on addressing the query and support sets in testing classes?\n\n3. Following 2, if SSO is more essential for the performance gain, is it fair comparison with other SOTA results? In other words, did other methods go through an optimization stage on unseen testing classes, or do some of them in fact not rely on updates during testing?",
            "summary_of_the_review": "The overall approach makes sense to me, but I'm not entirely sure how does the proposed approach compare with existing work, in terms of methodology as well as experiment settings, so I'll need the questions above to be addressed before making a most informed decision.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors design a prime-dual architecture for few-shot learning to characterize the inherent relationship between the support set and the query set and introduce a self-supervision constraint for performance improvement. In particular, it proposes to correct query sample labels between the prime network and dual network and design an optimized prediction for the query samples with a selection mechanism. Extensive experiments are conducted on datasets with the ablation analysis, and the method achieves favorable results over existing algorithms. ",
            "main_review": "Strengths: 1. The paper tackles one of the important issues of meta/few-shot learning: ground-truth is not available for test samples. The issue is important and very practical in my opinion. 2. The proposed method utilizes the trained dual network to predict the class labels of support samples, which is kind of the concept of the autoencoder technique, simple and promising. The idea of constructing and learning the relationship between support set and query set is reasonable and interesting. 3. In this paper, the authors provide comprehensive experiments, including the comparison of the state-of-the-art methods in public benchmarks, the ablation analysis, and how the parameters affect the performance such as the searched position number, etc. For example, Table 6 shows the number of the proposed SSO that can help mis-correct the query labels. The analysis of ablation and SSO modules provide evidence and make it convincing. \n\nWeaknesses: 1. In Section 3.2, the bipartite graph was mentioned, but I didn't see the authors use the property of the bipartite graph since there are edges constructed in the support set and also in the query set (Figure 4-a). This may not utilize the property from a bipartite graph. 2. As the paper mentioned, the prime and dual networks share the same network design, but I'd like to know whether the authors have used different network designs for prime and dual. What would be the pros and cons with the same/different designs? 3. In Figure 7, the 6 examples are shown for the 5-way 1-shot image classification experiment, I'd like to know why we show the position up to 20? Will the maximum be calculated by K(N + 1) as mentioned in the paper? 3. For the cross-domain result, the discussion may need to add more. For example, why the proposed method has less gain in Cars compared to 1-shot and 5-shot, and why does only CUB increase the performance gain when it's 5-way 5-shot? ",
            "summary_of_the_review": " Overall the paper is clear to understand, and it's a simple idea but effective approach. In this paper, most of the experiments provide the discussion and are convincing, except for a more detailed discussion in the cross-domain FSL result. I believe this paper will provide different insights into this field. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a prime-dual network to extract discriminative relationships between support and query examples of the multi classes with few examples. While the prime network predicts labels of a query out of the support set, the dual network reverses the process and infer the category labels of the support set using query examples. For training, a weight self-supervised loss contains the sum of dual and prime networks and is evaluated on inner-domain and cross-domain cases.  ",
            "main_review": "Strengths: \n\nThe paper is well written and the classification improvements seem to be significant. \n\n\nWeekness: \n- While the training stage is a stochastic process and dual network is the reverse of the prime of in mapping support to the query, I was wondering how both of the networks wouldn’t be the copy of each other at the end of the training? \n\n- What happens if we attach the used auxiliary learning module on top of ResNet and ablate for example prototypical network? How we can be sure that the accuracy gain of the model is not obtained by an extra learning module. \n\n- I was wondering why 224x224 images are used while miniImageNet dataset contains 84x84 images?\n",
            "summary_of_the_review": "The idea of the paper looks interesting, but I have some concerns about the motivation of the method and some of the evaluations of the work are missing.  ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "[1] This paper follows the line of metric-based few-shot learning methods with GNNs. The novelty of this paper is to design dual GNN graphs to capture the consistency of label prediction from the support set to the query set and its reversible task (from the query set to the support set).\n\n[2] The proposed method achieves considerable performance gains compared with other state-of-the-art methods.\n",
            "main_review": "Pros:\n\n[1] This paper extends the usage of Graph Neural Networks in few-shot image classification tasks, which is to design dual graphs and regularize the consistencies of prediction from the support set to the query set and from the query set to the support set.\n\n[2] The experiments are comprehensive. The author conducted experiments on intra-domain classification and inter-domain classification tasks.\n\n======================\n\nCons:\n\n[1] **About motivation.** The novelty of this paper is to design a dual network to make bi-directional predictions. However, the motivation is not clearly stated in the paper. In the testing stage, the authors only use the support samples to make label predictions in the query set, so why the consistency loss can improve the performance in the few-shot learning? I suggest the author visualize the feature distributions of samples in the support set and the query set to explain how the consistency loss influences the feature distributions.\n\n[2] **About the model design.** Do prime networks and dual networks share the same parameters? Or individual training? When testing, how does the author use the dual networks?\n\n[3] **About the relation between this work and EGNN[A], DPGN[B], and Mutual CRF-GNN[C].** These methods follow the line of the graph-based methods and also focus on the relation between the query samples and support samples. The author should carefully compare their method with these papers. (i) These papers also design losses to supervise the prediction of support samples after message passing and also use complex graph network structures to capture the relation in edges, distributions, and label predictions. Does this paper share similar ideas? I would like to remind that when message passing in GNN, the information also transforms from the query set to the suppert set. (ii) This paper should add semi-supervised learning settings in EGNN and DPGN to show their effectiveness. \n\nA. Edge-labeling Graph Neural Network for Few-shot Learning, CVPR19\n\nB. Distribution propagation graph network for few-shot learning, CVPR20\n\nC. Mutual crf-rnn for few-shot learning, CVPR21\n\n[4] It seems that this work is more effective for cross-domain few-shot classification than intra-domain few-shot classification. The author should explain why their methods are effective as I fail to find any specific design for cross-domain classification.\n\n[5] As the author recognized, the dual network will be forwarded for each neighborhood search, resulting in much extra computation overhead. I think it is a bit unfair to compare their methods with other methods in Table 1.",
            "summary_of_the_review": "Although the paper achieves promising results, I have some concerns about motivation, model design, and experimental comparisons with other GNN-based methods, as listed in Main Review.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}