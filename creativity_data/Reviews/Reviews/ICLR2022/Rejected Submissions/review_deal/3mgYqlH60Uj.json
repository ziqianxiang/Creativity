{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper investigated using bio-inspired cumulative fatigue model to improve bipedal locomotion via deep RL. The proposed method marginally improved bipedal locomotion behavior. The size of the experiments should be improved, together with generalization to other symmetric walkers. AC agrees with the reviewers that the empirical performance is not significant enough. The paper may fit the scope of a bipedal locomotion journal/conference better than ICLR."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a Reinforcement Learning (RL) model that uses cumulative effort to improve full-body movement generation. The use of a cumulative fatigue reward is a proper way of including biomechanical motion constrains when learning full-body movements.\n\nThe proposed model uses a so-called Normalized Cumulative Fatigue (NFC), which is based on the Three Compartment Controller (3CC) model proposed by Xia & Frey Law (2008).\n\nThe main contribution of this work is the proposed fatigue reward.\n",
            "main_review": "1. The paper is well written and very clear. The analysis of the state of the art is correct and no relevant references are missing.\n\n2. The main contribution of the paper is the so-called Normalized Cumulative Fatigue (NFC), which is based on the Three Compartment Controller (3CC) model proposed by Xia & Frey Law (2008).\n\n3. Results\n\nIn the reported experiments (Walker2D, Walker3D and Stepper) authors build upon the open-source implementation of Abdolhosseini et al. (2019), and include the proposed NFC and the symmetry loss proposed by (Yu et al., 2018) as a symmetry enforcement method. In these experiments: PPO denotes the Abdolhosseini original method, PPO+Fatigue denotes PPO plus NFC, SYMM denotes PPO plus the symmetry loss (Yu et al., 2018), and SYMM+Fatigue is SYMM plus NFC. \n\nThe use of the fatigue reward improves the learning of the different movements, when compared to the use of instantaneous torque reward. The amount of improvement depends on what index is measured. For instance, when the number of successfully learned locomotion models is measured, the improvement is marginal, in fact best results are obtained by SYMM. When the Vectorised symmetry index is used, best method is SYMM+Fatigue, and the amount of improvement in some cases is marginal (Walker2D and Walker3D), and in other more substantial (Stepper).\n\n\n",
            "summary_of_the_review": "The proposed method is based on a previous work (Abdolhosseini et al. (2019)), which is modified by including the fatigue reward and the symmetry loss proposed by (Yu et al., 2018). The fatigue reward is based on the Three Compartment Controller (3CC) model (proposed by Xia & Frey Law (2008)). So, I am not very clear about how to assess the novelty of this work.\n\nRegarding the obtained results, the use of both mechanisms (fatigue+symmetry) improve the learning results in the reported experiments. This improvement is some cases is marginal and in other substantial.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes an improvement to the existing locomotion algorithm by incorporating a bio-inspired fatigue model, the Three Compartment Controller (3CC) model. The fatigue model computes for each motor and each actuation direction a fatigue value that penalizes the motor from exerting high force for a long period of time. The resulting model is used to construct a reward function to replace the traditional energy penalty term in locomotion learning. Results show that the proposed model can achieve some improvements on average compared to methods without the model in terms of motion symmetry.\n",
            "main_review": "The paper introduces a method for training symmetric locomotion controllers for bipedal characters by leveraging a fatigue model. The idea of using bio-inspired model to model fatigues in the motors make a lot of sense and the proposed method seems reasonable. However, I do have a few questions and concerns regarding the proposed work:\n\n- Is torque modified by the fatigue model, or it’s only used to compute the fatigue reward? If the former, what is the impact of the reward vs modifying the actions? If the latter, would modifying the action (maximum torque or action bound) based on the fatigue model also achieve good performance?\n\n- The results indeed show improved performance on average. However, the results are not statistically significant with the large standard deviation in the metrics. Also, it’s hard to draw the conclusion that the fatigue reward would lead to more symmetric results (the walker results actually show worse symmetry with PPO only).\n\n- For the SYMM baseline, it might be worth doing some hyper-parameter search for the coefficient of the energy term. If it can be shown that such a procedure couldn’t produce good result it would further stress the advantage of the proposed method.\n\n- Some ablation on how the parameters of the proposed fatigue model (R and F) would impact the results is interesting to see.\n",
            "summary_of_the_review": "In general I think the idea of using a generic fatigue model to encourage more natural locomotion animation is a neat one. The authors proposed a concrete algorithm. However, the current result doesn’t seem to generate a significantly different result (judging from the metric statistics). If more analysis/results could be shown to demonstrate the advantage of the proposed method I think it would be a good and interesting contribution to the field.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes to use a normalized cumulative fatigue (NCF)-based reward to learn symmetric locomotion with Deep RL. The motivation is that most prior work on locomotion synthesis does not estimate cumulative biomechanical effort and only minimizes instantaneous joint torques. The paper derives the NCF reward based on the Three Component Controlller (3CC) fatigue model uses the reward alongside a symmetric loss proposed in prior work. The main contributions claimed in the paper are (1) it is the first work to use biomechanical effort to improve full-body movement generation, and (2) experiments show that the method can generate more natural and symmetric locomotion compared to the baselines.",
            "main_review": "**Strength:**\n\n- The motivation for using biomechanical effort to improve the realism of synthesized motions seems reasonable, as it mimics how real humans generate their motions.\n- The paper derives a simple NCF reward based on the 3CC fatigue model, which has very few hyperparameters and seems quite easy to use as a plug-in.\n- The paper employs various metrics to demonstrate the effectiveness of the proposed approach compared against the baselines.\n- The paper is generally well-written and easy to follow.\n\n**Weakness:**\n\n- The novelty of the proposed method is limited. The 3CC model has been used by prior work [Cheetma et al, 2020] for motion synthesis inside a Deep RL framework. Although [Cheetma et al, 2020] performs only synthesize upper-body motion, in principle, it can also be applied for full-body motion synthesis. The main difference of this paper is the different design of the fatigue function, but the ideas are quite similar.\n- The experiment results are not also very convincing. The method needs to also use the symmetry loss proposed in [Yu et al. 2018] to achieve competitive performance in all the metrics and environments. Without the symmetry loss, the performance of the proposed approach is (sometimes significantly) worse than [Yu et al. 2018]. This makes the method seems a bit incremental. Additionally, there is no comparison with the closest related work [Cheetma et al, 2020]. The paper argued in Sec. 4.2 about the difference with [Cheetma et al, 2020] in reward design but there is no comparison with it. Without experiments, it is difficult to tell which design is better.\n- The method has only been tested on simple walking or stepping environments. What about running and other motions? The paper would be much stronger and less incremental if it can show the generation of new motions (even with manually-designed reward) instead of just walking forward.\n- Finally, I feel the paper may be more suitable for graphics, animation, or biomechanics conferences than ICLR as its main focus and contribution is on bio-inspired reward design rather than learning. I feel the paper may pique more interest of graphics and animation audiences.\n\n**Additional Comments:**\n\n- How to choose the hyperparameter R in the reward? An ablation study on different R values could help.\n- Can the proposed reward also be used to improve imitation learning-based methods such as GAIL [1] or DeepMimic [2]?\n\n[1] Learning human behaviors from motion capture by adversarial imitation. Merel et al. arXiv 2017.\n[2] DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills. Peng et al. SIGGRAPH 2018.",
            "summary_of_the_review": "In summary, this paper has limited novelty compared to prior work [Cheetma et al, 2020] and the proposed reward is incremental in its usefulness. The experimental results only show simple walking motions, which is similar to the motion generation ability of prior work [Yu et al. 2018].\n\n------- post rebuttal ----\nThe authors' responses addressed some of my concerns so I increase my score. Still, I feel more qualitative results in other motion types are needed to showcase a clear advantage of the proposed approach.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to replace the commonly used torque square penalty for RL locomotion task with a fatigue-based reward that is more biologically plausible. Result demonstrates that the new reward is able to produce more symmetry motion compared to baseline on various benchmarks.",
            "main_review": "Pro:\n1. A novel biologically plausible energy reward for learning locomotion tasks.\n2. Proposed reward is able to achieve more symmetry motion compared to baseline based on many metrics.\n\nIssues:\n1. There is no qualitative result, i.e., a video showing motions from different methods.\n2. The percentage of successful models is smaller compared to baseline, it will be nice if the authors can comment on the reason behind it.\n3. I am not sure why Spectral Entropy indicates the robustness of the model.\n4. A lot of metrics are introduced to measure the symmetry of the model. It will be nice to provide other qualitative results. For example, what are their performance in terms of torque square, the proposed cumulative fatigue, and other task rewards?",
            "summary_of_the_review": "This paper introduces a biologically plausible reward based on cumulative fatigue. Many efforts are put into quantifying the advantage of the proposed method in terms of generating symmetry gait. It will be a nice contribution if the paper can also provide other qualitative results mentioned in the main review as well as clarify additional concerns raised. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}