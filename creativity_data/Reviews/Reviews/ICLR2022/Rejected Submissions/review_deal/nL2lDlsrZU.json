{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper presents a deep-learning network architecture for (semi)-supervised tabular data classification and regression problems based on a new attention mechanism between samples (rows) and features (columns).\nThe model is compared to 10 sota methods, studied on 30 diverse datasets (10 for binary classification, 10 for multiclass classification and 10 for regression).\ncontrastive learning approach for pre-training on unlabeled data and fine-tuning on a small number of labels\nExplainability capabilities are not presented in a very convincing way. \nWhile the reviewers find the problem relevant, they criticise novelty and, in particular, the experimental comparison.\nConcerns about hyperparameter tuning of own vs. comparison methods voiced by the reviewers.\nWhile these concerns have partially been addressed in the author response, the reviewers still doubt the fairness of comparison."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper provides (main contributions) a new deep learning architecture (SAINT) for tabular data that performs attention over both samples and features. For datasets with missing labels, the authors also analyse a  new contrastive self-supervised pre-training approach. The paper also introduces a new embedding method for continuous attributes.\n",
            "main_review": "Strengths:\nThe authors address an important problem setting since deep learning has not yet met its expectations on tabular data and the submission is one step further to this goal.\nThe paper is well and clearly written and the different components of the SAINT architecture are described in detail.\nThe importance of each introduced component is verified by several ablations and comparisons with strong baselines for tabular data show that the suggested architecture performs competitively across several datasets.\nThe performance increase of the suggested pre-training method is well-documented in the case of unlabeled data.\n\nConcerns / Questions:\nDatasets: The authors state that the datasets were selected based on their usage in TabNet and TabTransformer, and their availability in OpenML. Why was the UCI Repo here excluded? It is well-known and datasets are easily accessible such that datasets like “dota2games” or “htru2” could have been used from TabTransformer. While it is understandable that due to the separation in binary/multiclass classification and regression tasks not all datasets from TabNet and TabTransformer could be used it seems a lot of them have been replaced by other datasets from OpenML. I would kindly ask the authors to explain why this is the case and to communicate the underlying substitution rule. For example “poker hand”, “Higgs”, “Mushroom”, “KDD Churn”, “KDD Upselling”, “KDD Census Income” from TabNet and “albert”, “1995_income”, “bank-marketing”, “insurance_co (coil2000)”, “jannis”, “jasmine”, “online shoppers”, “philippine”, “seismic bumps”, “sylvine” from TabTransformer could have been used since they are in OpenML. Please also state the rule by which additional datasets for the respective task were selected from the vast number of datasets in OpenML.\n\nSelf-supervised Pre-Training: While the authors experimentally confirm the advantage of their pre-training method one could still raise the question whether it is really necessary. One could argue that by the similarity-based approach in intersample attention, SAINT could use the feature information of the unlabeled data during training (where a loss is only backpropagated on data for which labels exist) without the need for pre-training. It would have been interesting to see investigations in this direction. A similar approach was followed in [1].\nIn the submission, it is stated that pre-training is done on a plethora of datasets. On how many and which datasets was this done and what is the additional compute time needed for pre-training? Why are the results in the supervised setting (Table 2, last 3 lines, last column) different to the semi-supervised setting without missing labels without pre-training (Table 3, “middle” 3 rows, last column)?\nOne can see that the results for some datasets are (nearly) identical between different methods or are particularly bad (40685, 41169, 422, 42563, 42571, 42724 for supervised) Was it double-checked that there do not exist issues with the datasets themselves and what do the negative error values in the regression task in the semi-supervised setting mean (Tables 9-11)? \n\nHyperparameter-Settings: What are the exact hyperparameter search runs for each deep learning method (including MLP), how much effort (number of runs) was put into the hyperparameter selection for each? Were some of the ablations already part of the selection process?\n\nComputational Cost: Table 1 shows the computational cost for the different SAINT architectures. What are the values for the other deep learning architectures and how do the runtime estimates compare to the classical machine learning methods?\n[1] Kossen et al. Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning. ArXiv, 2106.02584, 2021\n",
            "summary_of_the_review": "Overall, the paper contributes important ideas where the core ideas are well-supported by experiments and the strengths outweigh my concerns. So I vote for accepting the submission.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes SAINT, a neural network model for handling tabular data with both continuous and discrete values. SAINT uses both self-attention among variables and inter-sample attention among different samples. Using both InfoNCE and denoising objectives for pre-training strategies, SAINT was able to outrank all baseline methods in experiments with 30 different tabular datasets, consisting of binary-classification, multi-class classification, and regression.",
            "main_review": "Strengths:\n- The proposed model reflects the characteristics of tabular data well, and easy to follow.\n- The authors conduct an extensive set of experiments with 30 different datasets, which is much appreicated.\n\nWeaknesses:\n- Debatable prediction performance of SAINT: The main evaluation result in Table 2 is reported in ranks, not the actual relevant metrics such as AUROC, AUPRC, Accuracy, MSE, R^2. This makes it very hard to trust the results, since the reader has no idea how well SAINT is doing compared to baselines. And when you look at the actual performance metrics in the Appendix (Table 5, 6, 7), it is evident that SAINT's performance against strong baselines is comparable at best. Furthermore, the standard deviations of the ranks in Table 2 are quite high, therefore making the highest ranks of SAINT statistically insignificant. \n- The efficacy of inter-sample attention: The efficacy of the proposed inter-sample attention is questionable. The authors claim that inter-sample attention is helpful when the number of features is large (i.e. many columns in the table), but this claim is not backed up by data in TAble 5, 6, and 7 in the Appendix. For datasets with more than 100 features, SAINT-i's performance metrics are inconsistent. Furthermore, the interpretation of the inter-sample attentions in Section 5.2 and Section G do not fully shed light on why such behavior is shown by SAINT, but simply states guesswork.\n- Missing baselines: Relevant methods are missing in the baseline such as VIME [1], TABBIE[2]. Is there any reason why they are not part of the baselines?\n\n[1] Yoon, J., Zhang, Y., Jordon, J. and van der Schaar, M., 2020. Vime: Extending the success of self-and semi-supervised learning to tabular domain. Advances in Neural Information Processing Systems, 33.\n[2] Iida, H., Thai, D., Manjunatha, V. and Iyyer, M., 2021. TABBIE: Pretrained Representations of Tabular Data. arXiv preprint arXiv:2105.02584.\n",
            "summary_of_the_review": "The proposed model, SAINT, seems like a well-motivated model, but the presentation of the experiment results and related analysis is either misleading or incomplete, therefore making it difficult to accurately evaluate the work in its current form. Reading the supplementary material further decreases the reviewer's confidence in this work's claimed contributions. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a new deep-learning network architecture for tabular data classification and regression problems, called SAINT. While deep-learning provided significant improvements in many domains, tabular data problems are still dominated by classical algorithms like XGBoost and Catboost. The new architecture is based on a new attention mechanism, applying attention both between samples (rows) and features (columns). It also embeds continuous features rather than embedding just categorical ones. The new model is studied on 30 diverse datasets (10 for binary classification, 10 for multiclass classification and 10 for regression), and compared to 10 previous types of models, including both classical models and recently proposed deep-learning models. The authors show that the average rank of SAINT across the various datasets is the best compared to these 10 algorithms (average rank of 2.7, vs. average rank of 4.0 for Catboost). Ablation studies are performed to evaluate the impact of each direction of attention and the impact of the embedding, and the explainability is demonstrated on MNIST. Additionally, the paper presents a contrastive learning approach for pre-training on unlabeled data and fine-tuning on a small number of labels, and it show that pre-training improves the rank for these small numbers of labels (on average by 1.4 for 50 labels and by 0.5 for 200 labels, not improving the rank when all labels are available).",
            "main_review": "This paper presents interesting techniques and addresses an important problem. The main concern is regarding the validity of the main claim, regarding outperforming existing classical algorithms. According to Appendix D, the hyperparameter search for these algorithms included a small set of arbitrarily selected hyperparameters, which might not reflect the actual performance they could provide. For example, the XGBoost search is only over two possible values of lambda and a small range of learning rate values, without changing the number of trees, the max-depth or any of the other hyper-parameters. LightGBM also has just two different configurations checked for num_leaves and min_data_in_leaf, with a small learning rate search range (0.01,0.1). In CatBoost, the search is only on the learning rate in that range, without changing any other hyperparameter. The CatBoost result for dataset 1596, for example, which is about 10% below LightGBM, may reflect missing tuning. \nOn the other hand, the proposed SAINT model has a carefully architectured network with various settings of hyperparameters. The paper says that hyperparameter search was done for each dataset on a validation set, but the specifics of this search per dataset are not specified for the SAINT model (e.g., how many options were checked).\nRather than comparing SAINT to many models that might not be well-tuned, this model should be compared to at least one or two well-tuned models, e.g., XGBoost and CatBoost. A reasonable tuning would include all the key hyperparameters (e.g., in XGBoost the number of trees ranging between 50 to 500 and their depth ranging between 1 and 20). It should perform the same number of iterations for each model, e.g., 100 or 1000 iterations with a standard Bayesian optimization package. When different models have a different number of hyperparameter tuning iterations, as reported in this paper, the comparison would not be apples-to-apples.  \n\nAnother issue is the presentation of the results in ranks rather than actual values. While this simplifies things, it may be confusing, since in several cases the differences in accuracy were very small (even 0.1% or less, as reported in Appendix E). It would therefore be good to additionally note each model’s average accuracy or AUC for the classification tasks, and some normalized value of the average RMSE.\n\nAn additional issue to check is why the regression task on dataset 422 had RMSE of exactly 0.03 with all the models (Table 7). Also, when using just 50 labels (Table 9), some of them had RMSE < -30 (both the sign and scale may need checking). The negative RMSE in all the entries in Tables 9-11 looks like a typo.\n",
            "summary_of_the_review": "This paper considers an important problem and presents interesting technical contributions which extend what was done in previous works. However, the experimental evaluation was not done rigorously enough to support the main claim. Different models had a different number of hyperparameter tuning iterations, and the tuned hyperparameters were not necessarily the most important ones. Therefore, based on the current data, the paper does not meet the criteria of ICLR. I would reconsider it if the authors provide the required data along the lines described above (even if the updated results would show a smaller improvement, its significance and soundness could be much higher).",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper introduces SAINT, a deep learning model for structured data that utilizes attention between rows, as well as contrastive pre-training. Performance improvements in supervised and semi-supervised settings are demonstrated, along with robustness and explainability capabilities. ",
            "main_review": "- Introduction does not cite literature sufficiently when discussing general tabular data ML.\n\n- It is unclear why encoding continuous features with embeddings improves the performance. Although the experimental results support that, more motivations and analysis on this should be presented.\n\n- The major concern with benchmark methods is that their hyperparameters are not fully tuned. For deep learning based methods, tuning learning rate and the number of hidden units is very important for high accuracy. For ensemble decision trees, tuning the number of decision trees is very important. Without full-scale tuning, the outperformance claims of SAINT are not convincing. I suggest carefully looking at how the corresponding methods tune the hyperparameters in the original papers and mimic the same process. Otherwise, the outperformance claims of SAINT are not credible. \n\n- The impact of augmentation on the results is unclear. More results with different augmentations methods should be added to validated the CutMix choice. \n\n- Explainability capabilities are not presented in a very convincing way. Is there a ground truth for feature importance to quantify the accuracy of the explanations? Maybe such an experiment on synthetic data would add value. Merely showing the sparse activations does not prove whether the attention maps are accurate.  \n\n- Beyond showing the average improvement results, it would be great to focus the discussions on on the specific tasks and datasets where SAINT shows the biggest improvements. \n\n- Overall, the novelty of the paper is not high. Most of the constituent ideas (e.g. sample-wise attention, transformer modules, contrastive learning etc.) were published in other papers. This paper combines them in a judicious way and shows important improvements, but most of the contributions are empirical. \n\n\n",
            "summary_of_the_review": "Overall, the paper has important contributions, especially in improving semi-supervised learning for tabular data. However, there are different issues in the content listed above, that need to be addressed. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}