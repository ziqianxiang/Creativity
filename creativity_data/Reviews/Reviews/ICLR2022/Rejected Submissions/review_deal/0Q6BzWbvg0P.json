{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "In this work, authors study query efficiency in the zeroth-order setting of adversarial examples. Reviewers pointed out several weaknesses in the work. They mentioned the paper is not well-organized and poorly written, experiments are not comprehensive and the practical significance of the proposed method is unclear. Although reviewers appreciated authors' efforts and responses in the discussion period, they felt that the paper is not above the accept threshold this round and still needs a bit more work."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper concerns zeroth-order hard-label adversarial attacks on machine learning models, and in particular, how to strengthen them through leveraging manifold information. The core idea is inspired from a series of intuitions and observations around the fact that the gradient of a robust model has more information regarding the underlying low-dimensional manifold of the data, compared to that of a naturally trained model. Author(s) have built upon this inspiration and proposed the noisy manifold distance oracle, which can leak manifold information to the adversary during forging its attack. Author(s) have also implemented their method on a number of real-world datasets.",
            "main_review": "I am not completely familiar with this line of research, and thus cannot properly position the current work w.r.t. existing literature. Therefore, I have lowered my confidence until I see other reviewers' comments. But, for now:\n\nPaper is hard to read, unless the reader has complete and exact knowledge about at least 2 or 3 previous works. Lots of technical terms, specific algorithms and methodologies have been borrowed from the said works, without any discussion or re-definition inside the main body. IMO, this has made the paper less informative. For example, it might not be a bad idea to briefly overview zeroth-order hard-label attacks, or \"boundary-tilting assumption\" which according to author(s) plays an important role in this paper. In particular, this part:\n- \"We investigate the scenario where an adversary uses zeroth-order information to estimate the desired gradient direction (Cheng et al., 2020; Chen et al., 2019). Thus the adversary uses only the top-1 label feedback from their model query to synthesize samples. The desire\nfor better query efficiency motivated the use of dimension reduction in hard-label attacks.\"\n\nof the \"Related Works\" section can be replaced by one or two paragraphs that briefly describe the main approaches behind each of these sentences. Another example is that author(s) have repeatedly referred to a new MI measure which 1) helps the adversary to craft on-manifold attacks, and also 2) its value \"increases as a function of data dimensionality\". However, the meaning and possible significance of the latter argument is vague and non-informative, at least for a non-familiar reader.\n\nAuthor(s) have relied on a claimed argument, that MI between manifold distribution and the gradient of a robust model is larger than that of an ordinary model, in order to propose their own method of leveraging manifold information during the attack. Is it rigorously justified with exact theoretical arguments? or the claim about MI value only supports the proposed method, intuitively? In other words, do you have any \"theorems\" to theoretically support your method? If the answer is no, then this paper is more of an experimental investigation of an idea, rather than a theoretical analysis.\n\nMoreover, the transitions from some parts of the paper to the next parts are not smooth. For example, transition from Hypothesis 1 to Definition 3.4 can be made more smooth through adding proper explanations. Definition 3.6 is vague. Please add some explanations on how the given formulation corresponds to the MI between low-dimensional manifold distribution $\\mathcal{M}$ and the gradient distribution (?) $\\mathcal{G}$. Let us begin with what is $p_{\\mathcal{G}}(1)$?\n\nI haven't checked the experimental parts.\n\n----------------------------------------------------------------\n\nMinor comments and suggestions:\n- Please correct the citation format at the end of Par. 1 in Introduction section.",
            "summary_of_the_review": "I am not completely familiar with this line of research, and thus cannot properly assess the current work. IMO, paper suffers from non-informative and cryptic explanations. This issue can be solved by briefly discussing the many technical terms and concepts that paper has implicitly or explicitly utilized.\n\nAlso, it seems that this work is not completely grounded on a firm theoretical ground. A number of core ideas and intuitions have been triggered by some theoretical analysis, but have been mostly investigated through experiments.\n\nAt this moment, my vote is weak reject (with a low confidence). However, I want to see other reviewers' comments and study a number of references inside the manuscript to have a better judgement.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies zeroth-order hard-label adversarial attacks. In particular, the authors explore the connection between how the gradient can reveal information about the data manifold, and its dependence on data dimensionality. They also empirically consider attacks with reduced dimensionality in practice.",
            "main_review": "The focus of this paper—black-box attacks against publicly available deep learning systems—is an important issue from a security perspective, and thus a key research problem. That being said, the paper is poorly written: without a clear hypothesis, or convincing experimental or theoretical results. Several assumptions and claims are made in the paper without being properly substantiated. I have listed more specific comments/questions below:\n\n[Section 3] \n\n- The motivation behind NMD is not provided. In particular, even if the gradients of the robust model are aligned with the data manifold (which is not a direct takeaway from Santurkar et al.), it is not clear why the adversary would have access to a robust model in the first place.\n- The assumptions after Definition 3 are not sufficiently justified. References are cited, however these claims are not made by prior work.\n- The upped bound being larger does not imply that I(M, $\\\"{G}$) is larger.\n\n[Section 4]\n\n- What is the motivation for random BiLN?\n- How exactly is the gradient deviation measured? In particular, how (and when) is the true gradient calculated?\n\n[Section 5]\n\n- It is not clear why the LPIPS distance or the gradient deviation are quantities we should care about in practice. After all, in this setting, the adversary would care only about the attack success rate. And as the authors themselves report, there is no direct correlation between LPIPS/gradient deviation and SR.\n- The results reported in Tables 1 and 2 do not show a consistent trend from the proposed modifications.\n- Results are reported over 50-100 examples of the dataset (which for ImageNet does not even have 1 example per class), and thus could have large statistical fluctuations. Why did the authors choose to report metrics on such small subsets of the data?\n- The connections between the theoretical section (which explores the relationship between manifold-gradient MI and dimensionality for robust models), and doing dimensionality reduction in practice for *standard* (non-robust) models isn't provided.",
            "summary_of_the_review": "The presentation of this paper could be substantially improved---including outlining the central hypothesis and performing experimental evaluations in more standard settings.\n\n### Post-rebuttal update\n\nI thank the authors for their detailed response. Unfortunately, my concerns with the paper regarding the presentation and motivation (which are also shared by other reviewers), as well as the practical significance of the proposed method still hold. In particular:\n\n- I believe that the paper would benefit from a significant rewrite to clarify precisely the main claims, justify assumptions, and better connect the different sections. \n\n- Regarding the experimental findings, the canonical measure of success of black-box-attacks is the ASR for a fixed epsilon. In this regard, does not seem to be a significant improvement. Moreover, the statistical significance of the findings is hard to verify---in Tables 1 and 2, the differences w.r.t. baselines often lie within confidence intervals, and in many cases the standard deviation is larger than the mean itself. The time constraints of the rebuttal phase are not a justification for not providing results over enough samples. The authors should have verified the statistical significance of their results (by increasing the number of samples till they get reasonable confidence intervals) as part of the original submission.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper addresses hard-label adversarial attacks from a geometric/manifold perspective. Specifically, it examines examples that live close to or on the data manifold. The paper presents a noisy manifold distance based on information-theoretic considerations and proposes three ways to approximate it (taking into account that the data manifold is unknown). Experimental results of HSJA and Sign-OPT attacks on CIFAR-10 and ImageNet are presented.",
            "main_review": "Strengths:\n- The paper is interesting to read and presents several results. \n- The topic of understanding adversarial attacks is important and timely. \n- The findings on dimension reduction attacks, reduced resolution, and query efficiency seem to be new and possibly significant.\n\nWeaknesses:\n- The main shortcoming is the presentation of the paper, which makes it difficult to understand and appreciate. It is often unclear from the text what exactly the authors mean in their arguments and how the arguments are supported.\n- The structure of the paper should be improved. Currently, the paper is written as a stream of arguments (definitions, hypotheses, and observations), which are not well supported. For example, it is unclear whether the observations rely only on the presented empirical study.\n- In this regard, I believe the experimental study should be more extensive in order to provide a more substantial empirical evidence for the arguments made in the paper.",
            "summary_of_the_review": "Interesting paper, but arguments are not sufficiently supported",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "[Summary]\nThis paper is based on a basic assumption that adversarial data are off-manifold and cannot be generated naturally from the true manifold. \n\nThis paper provides information-theoretic analysis on the hard-label adversarial attacks from the manifold perspective. \n",
            "main_review": "[Strength]\n1 It is very interesting to see the theoretical illustrations that compared with the natural model, the robust model could provide more information about data manifold (i.e. verification of hypothesis 1), which actually provides theoretical justification of (Santurkar et al. (2019)). \n\n2 This paper further argues dimension-reduced hard-label attacks are more effective on robust models than on standardly trained models. \n\n[Weakness]\n1 This paper is based on the assumption that adversarial data are features, not bugs. However, there is evidence that weakens the basic assumption this paper is built on, e.g., adversarial data are just bugs. <https://distill.pub/2019/advex-bugs-discussion/response-5/>\n\n2 Although this paper provides some interesting insights about the hard-label attacks, it does not produce the effective designs of hard-label attack methods, which undermines the paper's significance.\n\n3 The arguments are scattered, which does not form a uniform and compelling viewpoint. Sadly, I still find some experiments in Section 5 cannot well support the paper's arguments.\n\n[Questions]\n1 In the top of page 3, the author(s) stated that \"The desire for better query efficiency motivated the use of dimension reduction in hard-label attacks. However, to date it is not completely understood how this relates to traversal through the data manifold.\"\nI do not catch up with the logical link of the above statement. Could authors illustrate more to me?\n\n2 I get confused about the linkage between NMD and Hypothesis 1. May I know why NMD definition matters to Hypothesis 1?\nBesides, at the bottom of page 5, the authors stated that \"the NMD oracle acts as a side channel leaking sensitive information as a factor of the model robustness and data dimensionality.\"  Could I know what the authors mean about this?  Could I understand in this way that hard-label attacks against a robust model could reveal more sensitive training data information; the low data's dimensionality could worsen this information leakage?\nIf my understanding were correct, this could contradict the common sense that memoization by standard training could leak sensitive information.\n\n3 In Figure 1, why the variance (shade) is larger for blue lines and smaller for red lines? Besides, it seems that all red, blue, and yellow lines are not well separated. \n\n4 Could I check the success rate of methods stated in Table 1. \n\n5 May I know what is manifold distance. In other words, what is d'' in Hypothesis 2? Could the author(s) provide more details to me? ",
            "summary_of_the_review": "Although this paper provides some interesting points relating to hard-label attacks on robust models, I feel the paper's arguments are scattered, which does not form a uniform and compelling viewpoint. Thereby, I hope the authors could reorganize the paper and highlight a compelling point. \n\n\n\n##### Post rebuttal #### \nMany thanks for authors' feedback. \nI have read other reviewers' comments and corresponding feedback. \nI agree with other reviewers' evaluations such as \"poor writing\", \"unclear points\", etc. \nI keep my score unchanged. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}