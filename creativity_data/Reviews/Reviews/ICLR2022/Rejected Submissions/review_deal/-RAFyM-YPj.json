{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This submission has been evaluated by 5 reviewers with 3 leaning towards borderline accept and 2 leaning towards borderline reject. Reviewers have been consistently concerned about several aspects of this work, i.e. that *the method is only demonstrated on toy datasets*, that there is an issue with the scalability to larger substructures, that the proposed approach did not excel *in the simple task of triangle counting* or even that *the authors did not perform any other experiments even on a toy dataset*, and that comparisons on Deep-LRP re. efficiency were not provided, and *more complex settings and sensitivity* were not investigated. Reviewers also noted that the general idea of recursion did already appear in GNNs in one or another setting.\n\nIn making this decision, AC agrees that there is some potential in the proposed analysis and reviewers also highlighted this as a positive side of the submission. Yet, it is really hard to overlook at the same time the rebuttal where authors had the chance to address all reviewers comments regarding the experiments, their various details, and their variations.\n\nFailing to address these comments to the satisfaction of the majority of reviewers makes it impossible for AC to recommend the acceptance even tough there is every chance that the paper will ultimately make it to a high quality venue after a thorough revision (reviewers have really given a fair number of good suggestions that should assist authors)."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Motivated by the limitations of the expressive power of MPNNs this paper addresses one of the expressive shortcomings --- counting substructures. \nWhile previous works allowed counting by suggesting higher-order GNNs, they come at a high computational cost. This paper introduces a flexible recursive pooling technique that provides control over the expressivity-complexity tradeoff. The authors analyze the proposed framework and show expressivity results, complexity results under sparse graph assumptions, and an information-theoretic lower bound. Finally, the paper presents results on toy datasets validating the theoretically guaranteed expressive power.",
            "main_review": "The paper presents a fundamental pooling technique that could inspire the construction of future GNN architectures.\nThe theoretical analysis is quite thorough and places the work in the context of other works. \n\nI have however encountered difficulty reading it and understanding some of it:\n\n1. **Clarity and Readability** - I find the paper packed and hard to follow, although it seems that the authors put a lot of effort into defining every bit of notation.\n\n2. **The algorithm** - \n     - It is not clear to me what happens in the recursive pooling calls when the removed vertex disconnects the remaining subgraph? \n     - How do the authors choose $(r_1,..._r_{\\tau})$ and for instance - how many subgraphs of interest share the same covering sequence? or whether several covering sequences should be used together?\n\n3. **Scalability** - Although providing results on improved complexity, the method is only demonstrated on toy datasets, what are the gaps for scalability in real world datasets?\n\n4. **Larger substructures** - results are shown only on substructures of size 3, I would have wanted to see results on larger ones. \n\n- result in table 3 of PPGN performance seem weird as PPGN expressivity should allow to learn the EXP dataset, see [1]\n\n[1] Balcilar, M. et al. “Breaking the Limits of Message Passing Graph Neural Networks.” ICML (2021).\n\n",
            "summary_of_the_review": "The paper introduces a novel framework and provides a detailed analysis, but the readability issues are shading it. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new theoretical paradigm for designing Graph Neural Network architectures, based on recursion. Differently from conventional architectures that iteratively update node embeddings by computing a function of the embeddings of the previous step (locally for MPNNs or globally for higher-order GNNs), *Recursive Neighborhood pooling (RNP)* recursively updates the node embeddings by computing a function of the embeddings of the next level in the recursion hierarchy. Importantly, at each level the computations happen in different neighborhoods in the graph (of potentially different sizes) and node embeddings are enhanced with identifiers that keep track of the graph connectivity.  The authors prove that RNP can count any substructure (induced or not necessarily induced, attributed or not) by appropriately selecting the neighborhood sizes at each recursion level, and show that this implies universal approximation of local functions that depend on substructure counts. Importantly, they provide an explicit procedure to compute the required neighborhood sizes for each substructure of interest, which can potentially be useful for practitioners. In addition, they analyse the computational complexity of RNP and contrast it with theoretical lower bounds, showing that it is close-to-optimal for specific classes of graphs. The proposed framework is tested on two tasks including subgraph counting and graph isomorphism partially verifying the theoretical results.",
            "main_review": "To the best of my knowledge, this is the first time that the algorithmic technique of recursion is employed to define a Graph Neural Network. From a theoretical perspective, the paper is quite strong, and given that substructure counting is arguably a necessary property for a plethora of tasks in graph learning, the universality of RNP in that respect makes the proposed solution quite appealing. Moreover, the computational complexity analysis, as well as the fact that the authors bring impossibility results from the field of complexity theory to the machine learning community, is very insightful. \n\nMy major concern right now is the practicality of the method which is reinforced by the fact that the experimental section is quite weak. In particular, in my opinion, the paper raises the reader’s expectations, since the fact that there is a clean procedure to define the recursion neighbourhood sizes creates the impression that it is straightforward to design RNP-GNNs that can perform well not only in counting tasks but also in downstream applications (recently many papers have demonstrated the superiority of GNNs that use information from substructures), albeit with arguably increased computational complexity. However, these expectations are not met, since RNP does not seem to enjoy a clear advantage in simple counting tasks in comparison to other methods, while real-world data are completely omitted. \n\nThe above, as well as the fact that RNP is not tested in more challenging tasks, indicate that there might be an inherent practical limitation of RNP, regarding which I am afraid that the authors are not completely upfront. For example, an obvious disadvantage is the exponential dependence in the maximum size of the subgraphs of interest, which might imply that the method is practical only for very small subgraphs. Although the authors argue that this might be hard to improve, there are some cases that this is not true. For example, as far as I know, it is not clear if k-WL algorithms can count *induced* subgraphs of size $>k$, but in some cases, they can achieve this for *not necessarily induced subgraphs*. For example, Arvind et al., FCT’19 and Fürer, CIAC’17 show that 2-WL can count not necessarily induced cycles and paths of up to 7 vertices with $O(n^2)$ complexity, while RNP will need $O(nr_1^6\\Delta^{6r_1})$ to solve this task, which will be prohibitive unless the valence of the graph is very small. \n\nAt this stage, I am reluctant to suggest a clear acceptance for the paper, since (1) I cannot be sure if it can be implemented in practice and (2) I believe that a stronger experimental section would increase the impact, which will be beneficial both for the authors and the community in general. Below, I provide further details:\n\n**Strengths**:\n-\tThe idea is interesting and novel since it introduces a completely new paradigm for GNNs with clear expressivity advantages.\n-\tThe theoretical results are important, well-posed and in some cases quite general providing new insights into how to prove substructure counting abilities for GNNs. Several constructions and proof techniques are of independent interest which is also positive. For example, the construction of a graph family using prime numbers in Theorem 4, Claim 3, is in my opinion quite clever.\n-\tThe fact that we can define the recursion neighbourhood sizes via the construction of the covering sequences of the subgraphs of interest, provides practical advice and can save the practitioner from tedious hyperparameter tuning.\n-\tThe complexity analysis, although it mainly contains worst-case results, has wide applicability and provides insights into the limitations of GNNs in general.\n\n**Weaknesses**:\n- The presentation of RNP does not seem to imply that it is a purely theoretical construction. For example, the argument that the complexity of RNP depends on the sparsity of the graph, contrary to higher-order GNNs is compelling (note that this argument is introduced early on, in the abstract, and this perhaps exceedingly raises expectations). However, the experimental section currently points towards the opposite direction. Can the authors clarify their position w.r.t. that?\n- For example, I would expect the triangle counting task to be relatively easy for this network, but RNP does not seem to be better than other more expressive GNNs. The same holds for the 3-star where GraphSage performs much better (question: is GraphSage permutation sensitive here?). How do the authors comment on that? What is the depth of the recursion that you used (is it 2 as expected from the theory)? In general, what is it that prevents RNP from being implemented in a practical form and why isn't it clearly better in these two experiments. Is it only the computational complexity (that perhaps makes it harder to do more extensive experimentation)? Does it have to do with optimization and/or generalisation (it wasn't clear to me if there is a hold-out test here)? \n- Unless the method is purely theoretical, I think the authors would benefit a lot from creating a working instantiation of their model and a simple experimental section with more convincing results. Suggestions: (1) counting various substructures and contrasting their r parameters with those predicted by the theory, (2) real-world datasets where substructure counting is important (a single one would be OK – it might be useful here to measure the runtime of your method and contrast it with the sparsity of the graphs)  and optionally (3) predicting graph properties. \n- Although the paper is generally well-written, it is notation-heavy and at some points requires significant effort to parse the mathematical expressions. In my opinion, the authors should try to simplify some parts in order to make it more accessible to readers not versed in the theory. For example:\n  1.  Architecture - Eq (3-7). This is the heart of the proposed method, but it is a bit hard to follow. Notation can be off-loaded at some points (e.g., Eq.  (5) is very important since it is the recursion formula, yet it’s quite complicated - what is G’? Why do you write h_{u,v}  in Eq. (5) and h_{v,u} in Eq. (6)?)\n  2. Could the authors provide an explanation by unrolling the recursion? This might help the reader develop intuition and compare RNP to conventional iterative GNNs.\n\n**Other comments and questions**: (I don’t expect all these to be addressed in the rebuttal, but it might be useful to clarify in the next revision of the paper)\n-\tHow important are the identifiers used to augment the node embeddings (Eq. (3))? Can RNP retain its expressive power without them or it might collapse to that of an MPNN? Regarding the connection with the reconstruction conjecture: How important is the removal of the central node while recursing? In terms of complexity it’s beneficial to remove the central node, but is it crucial in terms of expressive power?\n-\tWhat happens in case we want to count a subgraph that is disconnected? How can you define the covering sequence in this case? I think the authors make such an argument in the proof of Theorem 2, but this is not clear from the main paper.\n-\tI believe at least the proof idea for Theorem 1 should be given in the main paper.\n-\tAs far as I understand, the lower bound of Theorem 4 applies only to functions of the form of Eq. (12), i.e., those that encode $t$ subgraphs with $s$ different unique values and then take the resulting histogram. Is that general enough? Also, although, as I said above, I found the construction of clique-containing graphs very clever, I am wondering if the lower bound is very loose since it is based on counting these graphs only. Finally, personally, I would call it a counting argument rather than information-theoretic (I was expecting to see something else in the proof when I first read it). \n\n\n**Minor**:\n-\tYou wrote: (Background. Higher-Order GNNs.) “At initialization, each k-tuple is labelled such that two k-tuples are labelled differently if their induced subgraphs are not isomorphic”. If I am not mistaken, k-WL labels k-tuples based on their isomorphism *types*, which is different from isomorphism *classes* since it takes into account the order of the vertices in the tuple as well. \n-\tThe following sentence is unclear: “In particular, we show how the aggregation “augments” local encodings, if they play together and the subgraphs are selected appropriately”.\n-\tProposition 6: You state “there is no difference between counting induced attributed graphs and counting induced unattributed graphs in RNP-GNNs”, however, if I am not mistaken your argument does not go both ways. Is that correct?\n-\tDefinition 7 requires further clarifications/is quite hard to follow (e.g., I am not sure if I understand the term homomorphism here since homomorphisms allow vertex repetitions).\n-\t“However, it is known that MPNNs can count at most star structures or edges”: missing citation: \"On Weisfeiler-Leman Invariance: Subgraph Counts and Related Graph Properties\", Arvind et al., FCT’19\n\n\n### --------------- After rebuttal ---------------\n\nMy major concerns were not addressed by the rebuttal, hence I will keep my recommendation unchanged. Please see my final comment to the authors for more details.\n",
            "summary_of_the_review": "The paper is quite rich and insightful from a theoretical perspective, but currently, it is unclear if and how it can be implemented in practice and if it can work well. The authors should prepare a much stronger experimental section and clearly discuss the limitations of their approach. I am not negative about the paper, but I believe that this part is crucial to recommend acceptance. I would be willing to discuss my score after that.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "While GNN architectures became very popular in recent years, their expressive power is limited. Higher order architectures offer more expressive power, but in turn require sometimes infeasible computation power. This paper offers a new architecture (RNP-GNN) that uses recursive pooling in and offers a trade-off between the expressiveness and the computational needs. The paper provides a theoretical analysis of the proposed architecture, as well as 2 experiments. ",
            "main_review": "Strengths:\n* The paper addresses and important problem: most GNN architectures are not expressive enough for many tasks, and architectures that are, are not always feasible. RNP-GNN offers a trade-off between the expressiveness and computational-needs.\n* The proposed architecture is backed with wide theoretical analysis, proving that it can count substructures and that it can approximate local graph functions. \n\nMain weaknesses:\n* The presentation of the method, described in section 4, was hard to follow, even with the example in Figure 1. I think a clearer explanation would improve the paper.\n* According the computational complexity analysis, even with small $c$ and $\\tau$ (paragraph after Theorem 3), RNP-GNN will join most higher-order architecture and become unfeasible. I.e., in large, not so sparse graphs the method will not be feasible, yet for small graphs other GNN architectures can be just as good. \n* In the experiments sections, I'd like to see a comparison between RNP-GNN and the other architectures with similar performance, so I can see if there is a real improvement w.r.t. accuracy/speed/memory etc.\n\nAddressing the first and last weaknesses will help me change my score.",
            "summary_of_the_review": "To summarize, the paper studies an important problem, and proposes an architecture with strong theoretical justifications. In practice, I'm not convinced that the new architecture has any real advantages over existing, simpler architectures.\n\nI (weakly) support the acceptance of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the problem of counting isomorphic sub-graphs in a given graph. The author proposed a new graph neural network (GNN), named RNP-GNN, which operates by recursively aggregating node representations based on different settings of the radius. A difference with previous work is that, during recursion, this aggregation is always performed on recursive *subsets* of neighborhoods.  Figure 1 gives an intuitive toy example of the computation process.\n\nThe authors also did some theoretical analysis, to show that it is more capable to count sub-graphs, to show that RNP-GNN satisfies universal approximation properties, and to give the exact computation complexity.\n\nThe method is tested on counting small structures on random graphs and Erdos-Renyi graphs (which have a very small size, <=30 nodes), and counting non-trivial sub-graphs on the EXP dataset by Abboud et al. (2021), showing better performance than most baselines.",
            "main_review": "Overall, this is a practical deep learning contribution, with a specific problem (counting sub-graphs) to solve, despite than a large portion of the paper is spent on theoretical analysis. The novelty mainly lies on how the recursive neighbourhoods are defined. This is not significant and sits in a family of similar structured GNNs. From my view, the paper is borderline on the negative side.\n\nThe writing is well polished. However, the notations system is a bit cumbersome and hard to interpret.  The quality of the statements can be improved. For example, it is not clear what is the exact computation of \"AGGREGATE\" and what is the non-linear activation here.  Theorem 1 is only significant if f outputs an integer, of the difference |f(G_1,\\theta)-f(G_2,\\theta)| can be lower-bounded.  Similar for theorem 4. Theorem 2 is a weird way to state universal approximation, as we usual states upper-bound on the difference between the neural network and a given function by an arbitrary small \\epsilon.\n\nAs an important hyper-parameter, the proposed method relies on a vector of radius (r_1, r_2, ... ) that are recursively used in the neighborhood nodes when building the presentation of any node. The overall complexity scales with the size of N_{r_1}, or the size of the\nr_1-neighborhoods. In the experiments, $r_1$ is set to a very small value (1 or 2). This is in general an expensive method and only suitable for small-degree graphs, and related application (like counting triangles).\n\nAs a practical deep learning contribution, the experiments section is too brief.  First, what is the setting of  (r_1, r_2, ...), which is perhaps the most important parameter, is not specified (instead, there is pointer to Appendix). The statistics of the dataset are also missing, especially, it  is important to say more details about the EXP dataset, and what are \"certain propositional formulas\" and their sizes.\nSecond, in the first experiment on counting regular sub-graphs. As Deep-LRP (rather than the authors' method) has the best performance,\nit is important to have another experiments to compare their efficiency. Ideally, the authors can show that Deep-LRP is much slower than the proposed RNP-GNN, so the proposed method still wins overall.\n\n#### Minor comments:\n\nThe notion of \"count substructure\" should be clearly defined when it is mentioned in the introduction. What is a substructure, and what is the space when the counting is performed, etc.\n\nSec 2 the introduction to Higher-Order GNNs can be improved,  as currently it is not so clear what are the underlying operations\nof Higher-Order GNNs. I suggest to use some equations similar to eq.(1)\n\neq.(7) explain what is MLP, and how its parameters/structures are set\n\neq.(7) explain \\epsilon\n\nSection 4, at the end, explain on what settings, the proposed RNP-GNN becomes exactly MPNN.\n\neq.(8) explain the \"\\bold{1}\" notation, also \"\\simeq\"\n\nDefinition 4, explain the difference between {S} and \\calligraphic{S}\n\nDefinition 4, is \\caligraphic{V} introduced in the body of the definition?",
            "summary_of_the_review": "Pro:\n- A simple GNN that is well explained\n- Good writing\n\nCon:\n- Insufficient experimental evaluation\n- Quality of mathematical statements",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The main contribution of this paper is to propose a novel GNN architecture called RNP-GNN. This architecture seems to be specifically designed to tackle the task of counting subgraphs. The authors prove two expressiveness results: (1) RPN-GNN succeeds in the task of counting subgraphs (with an appropriate parameter choice); (2) RPN-GNN has the universal approximation property w.r.t local graph functions. Also, the computational complexity of this architecture is calculated. The authors also provide information-theoretic and computational complexity lower bounds. Finally, several experiments are done on the EXP dataset, and on counting substructures from random graphs.",
            "main_review": "I think this paper suggests a very interesting GNN architecture, which essentially allows more expressive power than standard MPNN, while not using k-th order tensors such as in higher-order GNNs. In this aspect, this paper makes a step forward in our understanding of the expressive power of GNNs. Theorem 1 and Theorem 2 successfully provide theoretical guarantees for this architecture, while the experimental part shows that this architecture is indeed successful in tasks in which standard MPNNs fail (such as the EXP dataset, and counting subgraphs). Finally, the information-theoretic and computational complexity lower bounds show that such tasks cannot be solved in a more efficient way. The paper is also well presented and easy to follow.\n\nI have two concerns about the paper which I would be happy to see the author’s response:\n\n1) The comparison of computational complexity to other models in Section 5.3 is unclear, and it is also unclear whether RNP is really more efficient asymptotically than k-WL. \nTo put into context, suppose \\Delta=O(1), and our task at hand is to count a certain subgraph with k nodes. by Table 1, the time complexity of k-WL is n^k, and for RNP it is O(n). But, according to Theorem 1, to count subgraphs with k nodes requires that the \\tau in RNP is k-1. This means, according to Theorem 3, that the time complexity (asymptotically) is exponential in k. Hence, it is not clear that RNP is more efficient than k-WL. Also, if k=O(1), then the time complexity of k-WL is n^{O(1)}, which seems to be the same as RNP. I think the authors should give a more concrete case on why RNP is more efficient than higher-order GNNs.\n\n2) The experiments are done only on artificial datasets such as counting subgraphs from random graphs, and the EXP dataset from Abboud et al. which is specifically designed to test 3-WL. It would be beneficial to test whether RNP is successful also on real datasets, otherwise, it is not clear whether RNP can also be used in more realistic settings. For example, testing on datasets from the OGB collection or TUDatasets.\n\nOther minor comments:\n\n1) I think the authors should give more motivation on why counting subgraphs is an interesting task, besides being studied in a previous paper.\n\n2) There are a couple of notations that are used throughout the paper but are not defined, e.g. V(G), \\Epsilon(G) G(A) (where A is a subset of nodes), \\mathbb{G}_n. They may be standard graph theory notations, but I still suggest adding a short notations section to prevent ambiguity.\n\n3) Although Theorem 2 shows a universal approximation property, the connection to the approximation power of k-WL is unclear. Given a certain choice of (r_1,...,r_\\tau), where \\tau=k-1, is RNP and k-WL equivalent in approximation power, or is there still a gap? I think this should be clarified in the paper.\n\n4) In Appendix F, r_t-> r_\\tau (in the input).\n\n5) In Section 8 it says: “we leave further developments of practical variants of RNP-GNN to future work.” What does it mean? Is RNP as described in the paper not practical?\n",
            "summary_of_the_review": "I think this is a good paper that provides a novel GNN architecture that on the one hand is more expressive than standard MPNN and on the other doesn’t require k-th order tensors to compute. The theoretical aspect of the paper is very thorough and gives insights both into the expressive power of RGP and on lower bounds for the task of counting subgraphs. On the experimental side, the current experiments presented in the paper provide evidence of the expressive power of RGP.\nMy two concerns are whether RGP is really more computationally efficient than k-WL, and whether it would outperform other MPNNs on standard benchmarks from the GNN literature. I would be happy to read the author’s response and consider raising my score accordingly.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}