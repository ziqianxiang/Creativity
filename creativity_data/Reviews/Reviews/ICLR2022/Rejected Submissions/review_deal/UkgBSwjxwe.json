{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "Unfortunately, the reviewers have unanimously voted to reject this paper. \nThere was some discussion of whether the paper was out-of-scope for ICLR; \nI don't think that it is, necessarily, but I think that we can kind of screen off that topic because the reviewers had plenty of non-scope-related concerns that seem disqualifying to me, including both issues of novelty and issues related to the experimental validation.\nTherefore, I am also recommending rejection in this case."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose a model for doing forwards reasoning. There are three parts: The model decomposes a scene into object, converts that to a grounded symbolic representation and then performs differentiable reasoning. The authors show classification performance on two datasets.\n\nWhile the authors propose a Neuro-Symbolic  model, they do not compare to any of the state of the art neuro-symbolic models such as NeuralSymbolic VQA or NeuroSymbolic concept learner for one of the datasets and only consider one neuro-symbolic model for the other task.\n\nA number of implementation details are also not clear. It’s unclear how much supervision is needed and what form that supervision takes?\n\nIt is also not clear what the advantages of the proposed model are over existing models? This is not discussed clearly in the paper and there are no significant experimental results.\n\nThe high level direction of this work is very interesting and indeed Neuro-Symbolic models do offer some advantages over deep learning models. However, the authors have failed to discuss or demonstrate any of these advantages (more details below). ",
            "main_review": "The caption on Figure 1 is not clear.\n\nFrom Page 1 it is not clear what a Kandinsky pattern is.\n\n“it is difficult, if not impossible, to solve object-centric reasoning tasks such as Kandinsky patterns due to several underlying challenges: (i) the perception of the objects from the raw inputs and (ii) the reasoning on the attributes and the relations to capture the complex patterns”. It’s not clear what the object-centric reasoning task in question is here? I would also argue that a Neural-Symbolic VQA (Kexin Yi et al.) type model could solve this type of problem? Since it has been applied very successfully to Clevr. What makes Kandisky patterns more challenging? This is not made clear in the paper.\n\nPage 2: “It computes the set of ground atoms” <— It is not clear what **it** refers to here.\n\nIt would be helpful in the introduction to talk about the level of supervision in the model, just at a high level.\n\nRelated work:\n“They either do not employ a differentiable forward reasoner or miss objet-centric learning in the end-to-end reasoning architecture.” It would be helpful to be clear about which methods fall in to which category. It is also unclear how this work build on previous work.\n\nFurther,  “Deep Compositional Question Answering with Neural Module Networks”,  and “NeuroSymbolic Concept Learner” use differentiable components. It would be good to include some comparison here between differentiable components and a differentiable end-to-end reasoning architecture.\n\n“NSFR encapsulates different object-perception models, thus allows us to choose a proper model depending on the situation and the problem to be solved.” Does this mean that NSFR can learn both with and without supervision? However, later in the paper is appears that you do need supervision? Please clarify.\n\nFrom the introduction it is not clear exactly what the contributions of the paper are and what the related work is? The related work section is very hard to follow.\n\nObject-centric Perception: It is not clear what kinds of supervision are needed for this process? In the facts converter the authors assume that “The output of the visual-perception module is already factorized in terms of objects.” How is this assumption enforced?\n\nIn figure 3 and Example 2 it is not clear what a “valuation function” is. What is $v_{color}$ defined only for $A_{red}$ and $v_{shape}$ only for $A_{circle}$. \n\nFacts Converter: The Facts Converter is not clearly described, however, Figure 3 was helpful for understanding the process.\n\nWhere do the ground atoms in Figure 3 come from? It appears that you can have significantly more ground atoms than are necessary to describe the two scene? Do you need to define new ground atom for each task? It would be helpful to discuss this.\n\nExperiments:\n\nIt would be helpful to be very explicit about the supervision provided for each of the baselines compared to the NSFR model?\n\nTable 1: Why not compare to other Neuro-Symbolic methods such as NeuroSymbolic concept learner or Neural-Symbolic VQA? Or even using a relation net (A simple neural network module for relational reasoning, Santoro et al)? \n\nTable 1: The ResNet is trained on how many samples? 5k or 15k or 20k? Perhaps a smaller model would perform better? From the appendix it looks like the ResNet is trained on 5k examples, while the NSFR model gets an **extra** 15k examples to train the visual perception module on, so this does not look like a fair comparison. Further, NeuroSymbolic VQA is able to learn complex visual reasoning from only 7k examples. A relation net can learn from 10k sort-of-clevr images.\n\nTable 1 and 2: How many runs were done? What is the variance? It would be helpful to see the training curves to better understand the overfitting in table 1. \n\nExisting Neuro-Symbolic papers look at more than classification accuracy. “Neural-Symbolic VQA”, “NeuroSymbolic Concept learner,” “Deep Compositional Question Answering with Neural Module Networks”, “Learning by Abstraction: The Neural State Machine” and “An Explicitly Relational Neural Network Architecture” all show interesting transfer capabilities like generalising to unseen colour combinations or to problems with more objects than those seen during training.\n\nWhat are the losses used for training? These are not described anywhere?\n\nTypos:\n\nPage 1: Fig. (a) and Fig. (b) missing figure number.\n\nPage 2: “Facts converter converts” —> **The* facts converter\n\nPage 2: “Finally, differentiable reasoning” —> Finally, **the** differentiable\n\nPage 3: “The typical approach is the object detection (or supervised) approach such as FasterRCNN”. It is not clear what its meant here.\n\nPage 3: “where the models acquire the ability of object-perception without or fewer annotations”. This is not clear.",
            "summary_of_the_review": "My main concerns with this paper are (1) there are not sufficient details to understand how the model works in terms of the supervision needed and the losses computed, (2) it is not clear what the motivations and contributions are (the authors only claim to show improvement in overall accuracy on very simple tasks), (3) the experimental results are not sufficient. The tasks appear to be quite simple (even though ResNets perform badly this appears to be due to lack of data, but the paper does not discuss specifically learning in low data regimes); the authors could have compared to stronger baselines; some of the comparisons may be unfair (see above) and the authors only show classification accuracy.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a method for differentiable reasoning using soft first-order logic. The key idea is to combine forward reasoning with object-based deep learning. In particular, after the perception process from a pre-trained object detector, the proposed Neuro-Symbolic Forward Reasoner (NSFR) converts the object-based representation into probabilistic atoms and performs forward reasoning using differentiable logic. In Kandinsky and CLEVR-Hands datasets, the method shows slightly improved performance over neural methods and other symbolic approaches.",
            "main_review": "Strengths\n\nUsing differentiable reasoning methods towards this task makes much more sense than deep learning models. Such a method should be ideally more interpretable, verifiable, robust, and generalizable. Neuro-symbolic reasoning has received notable attention recently and I think this paper is another step towards general AI with good reasoning capability.\n\nThe formality in this work should be encouraged. Compared to earlier works in this topic, the paper clearly defines the notions and related concepts.\n\nThe work is well presented with important notions explained with timely and concise examples. I like the flow the work in general: it's easy to understand with exemplified motivation and illustrative graphics.\n\nWeaknesses\n\nMy major concern with this work is the novelty. Specifically, I'm not sure what the contribution of this work is with respective to earlier papers. Differentiable reasoning with / without object-centric representation has been investigated in works that could be dated back to DeepProbLog [1], NS-VQA [2], NS-CL [3], NLM [4]. [1] and [4] discuss how logic reasoning can be combined with differentiable learning (batched or not) and [2] and [3] use object-based representation to solve visual reasoning problems with clearly defined programs. Therefore, I'm confused on the specific novelty of this work. It reads like another work using differentiable reasoning without much new contribution to the problem.\n\nA common issue with logic-based reasoning is the prior knowledge required. Before learning, the designer needs to supply the algorithm with space of atoms, the background knowledge (rules), etc. How does it compare with other deep learning models that can be trained without them?\n\nWhile I do not doubt the effectiveness of the method, the improvement over previous methods looks limited. The object-based YOLO+MLP model looks bad in complex scenes but could it be because of the amount of data you use? I assume the space for complex scenes is much larger for simpler scenes and with the fixed amount of training data, complex scenes will be less well-covered than simpler ones. And on simpler scenes, YOLO+MLP is not really bad. On the CLEVR-Hans experiments, NSFR is not always the best either. \n\n[1] DeepProbLog: Neural Probabilistic Logic Programming\n[2] Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding\n[3] The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision\n[4] Neural Logic Machines\n",
            "summary_of_the_review": "I appreciate the merits in this work as mentioned above. However, I will give a temporary weak reject due to its limited novelty.\n\nI have read the reviews and the authors' response. The authors acknowledge some general issues and point out  differences with earlier works, though I do not think is significant enough. Syncing with other reviewers, I decide to keep my initial rating.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a differential neuro-symbolic visual reasoning framework that combines an object representation extractor and a forward reasoner.",
            "main_review": "Pros:\n1. A whole system pipeline is developed for neural-symbolic reasoning. If the authors can open source their code, this will be a good contribution to the community.\n2. The system's advantage (mainly in term of accuracy) is demonstrated in 2 selected datasets.\n\nCons:\n1. I can see how this work is different from a few other works on neural-symbolic visual reasoning, but I cannot see how this work innovate on previous ideas. The authors say that previous work (e.g. Amizadeh et al., 2020) either does not have a differentiable forward reasoner, or an object-centric module. But I think works such as Amizadeh et al., 2020 and neural-symbolic concept learner have both of these modules? I would appreciate if the authors can state more clearly the difference between their work and the previous works.\n2. I feel the evaluation is not enough to gain more insights into the strength of the work. First of all, only two synthetic datasets are used for evaluation. And for the CLEVR-dataset, the authors only reported on CLEVR-Hans dataset, which has very few baselines to compare against. Results on the standard CLEVR datasets will allow us to gain more insights into the model's capability. Experiments on real-world datasets (such as GQA) will also boost confidence in the work. The evaluation should also go beyond simple classification accuracies. One strength of neuro-symbolic model is its capability to generalize beyond the training distributions. I would encourage the authors to at least performs some experiments on compositional generalization. ",
            "summary_of_the_review": "A technically solid paper that lacks a bit of novelty and falls shorts in evalaution.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper tackles the problem of how to incorporate symbolic reasoning into a differentiable, deep learning architecture. The authors present an architecture comprising a pre-trained, slot-based encoder, and a differentiable clausal reasoner. They apply it to visual, relational reasoning problems, evaluating the architecture on two datasets: Kandinsky patterns and CLEVR-Hans, achieving promising performance figures.",
            "main_review": "The paper tackles an important problem, using an approach with great promise (neurosymbolic approach). However, I do have serious misgivings about the paper.\n\nIf the authors had achieved the same performance without so many hand-designed features and hand-coded rules - in short, with fewer built-in priors - it would be much more impressive. But the proposed system appears to carry out very little learning (apart from the pre-training of the encoder), and incorporates a large number of hand-designed elements tailored for the benchmarks. Most glaringly, as far as I can tell, the forward-chaining reasoning module imports a hand-written set of clauses that essentially encode the solution to the problem at hand. For example, in Section C.2, the authors give the clauses used by their system for the CLEVR-Hans3 dataset (Stammer, et al (2021)), which include:\n\nkp1(X):-in(O1,X),in(O2,X),size(O1,large),shape(O1,cube),size(O2,large),shape(O2,cylinder).\n\nNot only does this directly encode class 1 of the dataset, it also, by construction (ie: by “cheating”), solves the confounding issue that the CLEVR-Hans dataset was designed to bring out. In this class, the confounding factor in the training set is the colour of the large cube. In the training set, it happens to be always grey. But in the test set, it is sometimes another colour. There is no in-principle way to work out the “correct” rule from the training set alone, which is why Stammer et al incorporate user feedback to revise their model after training. By contrast, the present authors’ rule omits the confounding factor (colour) from the outset. So it’s little wonder they do well on the evaluation. Regrettably, this seems to me like a fundamental flaw to the paper - although, of course, I might be missing something, and I’m keen to hear the authors’ response here.\n\nRelatedly, on p.4, the authors write “We make the minimum assumption that the perception function takes an image and returns a set of object-centric vectors, where each of the vectors represents each object. For simplicity, we assume that each dimension of the vector represents the probability of the attributes for each object.” And in Section F.2, the authors give a table showing the “output format of the slot attention model”. This format perfectly represents the features needed for the CLEVR-Hans benchmark. I believe the output from the slot attention module is obtained in this format, following Locatello et al, thanks to a supervised learning stage that depends on the availability of ground truth labels corresponding to shape, size, colour, etc. (Is this correct?) It seems to me that this builds in half the solution to the problem from the outset. (I grant that Stammer et al make the same move, but their paper solves a harder problem; see above.) In effect, it means that the low-level object attributes (eg: “sphere”), which correspond to atoms in the clauses (eg: shape(obj1,sphere)), are hand-designed rather than learned.\n\nIt might be helpful to see a (qualitative, at least) comparison with other architectures that tackle similar problems (relational reasoning about images), using architectures that are also to some degree neurosymbolic. Many of these have far fewer built-in priors than the present architecture, such as Santoro et al (2017) (not so neurosymbolic), Asai (2019), or Shanahan et al (2020). The latter two architectures have a number of similarities with the present one, but learn objects, features, and relations end-to-end from scratch. The architecture of Stammer et al (2021) also learns the reasoning part from scratch (using a set transformer), without domain-specific knowledge of the relevant rules.\n\nAsai, M. Unsupervised grounding of plannable first-order logic representation from images. In International Conference on Automated Planning and Scheduling, 2019.\n\nShanahan, M., Nikiforou, K., Creswell, A., Kaplanis, C., Barrett, D. and Garnelo, M. An explicitly relational neural network architecture. In International Conference on Machine Learning, pp. 8593-8603, 2020.\n\nSantoro, A., Raposo, D., Barrett, D. G., Malinowski, M.,Pascanu, R., Battaglia, P., and Lillicrap, T.   A simple neural network module for relational reasoning.  In Advances in Neural Information Processing Systems, pp. 4974–4983, 2017.\n\nI was initially a bit puzzled why the authors repeatedly refer to the batch dimension, both in their formal presentation and in characterising their contribution, since it’s customary to omit the batch dimension for simplicity. However, a sentence at the end of section 3.4.3 states that the “value function computes probablility in batch”. In other words, the batch matters, and can’t be simply omitted for clarity. Is this correct? As I was also wondering what the justification was for interpreting value functions probabilistically, maybe the authors should make this point a bit more prominently, and give a bit more explanation. Indeed, I’m still confused by the role of probability in the architecture.\n\nIt seems that the architecture is not end-to-end differentiable, as the algorithm for converting object-centric representations to probabilistic facts appears not to be differentiable. This isn't necessarily a problem, although it precludes fine-tuning the pre-trained encoder for a given task. But maybe the authors could confirm my understanding here.",
            "summary_of_the_review": "This is an important line of research, in my view, so the work is of potential value. However, I have major concerns about the extent to which the architecture relies on hand-designed features and rules. As well as limiting the value of the work, in its present state, I believe this issue also invalidates some of the evaluation (on CLEVR-Hans). But I am open to having my mind changed by the rebuttal.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}