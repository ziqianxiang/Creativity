{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes an early exit method that uses class means of samples that is gradient free and is aimed for low compute cases such as mobile and edge data. The idea is novel in this setting (though class means have been used for other settings such as few shot classification) and empirical results show that it works well. There are two main concerns from reviewer concerns that were not addressed by the author rebuttal. First, applicability of the model in real world due to its memory requirements and two, experiments that show performance on more realistic datasets such as Imagenet. The reason the latter is required is the promise of mobile application for the proposed method. I suggest the authors explain the first concern more and add the requested experiments in the upcoming version of the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper introduces a novel mechanism to add early exiting on existing networks without re-training the original network. The paper mentions the reason for this is that this can be done on lower-powered devices on-the-fly, where retraining is not wanted. The method does so by calculating output means, and comparing new examples with that, by running a distance function through a softmax, and early exiting if the prediction is above a certain threshold.",
            "main_review": "I think the idea of the paper is cute, and it surprised me that this worked. A pleasant surprise, and I at least learned something new. For the given setup/data, it also seems to work well. Being able to add early-exiting to any network would be a very welcome addition to any framework that runs neural networks on devices.\n\nHowever, when reading the paper there are some glaring holes and potential problems with the method that the authors don't comment on.\n\nFirst of all is the actual computational overhead. This is definitely non-trivial. Take imagenet for example, one would have to do a lot of extra compute for each layer. As for every layer, we'd have to calculate a 1000 feature-map-size subtraction, matrix square and a large sum. Afterwards a softmax is taken, which for large numbers of classes (in e.g. language models this is very relevant), can be expensive as well. The authors seem to not really comment on this, nor calculate the actual overhead per layer. Only in the comparison to other methods do the flops come up. Second is the parameter overhead. The method is not parameter-free. Likely far from it. For every feature map, num_classesfeature_map size parameters have to be stored for comparison. For e.g. Imagenet with a 1000 classes, this is akin to storing a full forward pass of activations of a batch-size=1000. This can run into the gigabytes of memory! Given the author's focus on mobile devices, this doesn't seem very feasible.\n\nThe authors seem to avoid this problem by not considering datasets/tasks with a lot of class labels. For MNIST/CIFAR10, your overhead is not going to be too big, but scale this up to language models or sizeable classification models and the overhead is likely prohibitive. The second reason why these drawbacks seem to not show up in the paper is the usage of really really large models for MNIST and CIFAR10. Not only is the usage of these oversized models at odds with the idea of running them on mobile devices, there are far more efficient networks than those mentioned in the paper nowadays, as we can see from figure 1, after 30 layers the network has already classified the examples properly. One could just use that 30 layer network, stick a single softmax at the end, and use that for classification. I wonder what happens to the gains of this method when comparing on such more efficient models.\n\nIn terms of comparisons, I'm missing multi-scale dense nets. This is the de-facto paper in this area, and can't be missed from a comparison perspective. I also think the authors could have noted comparison with other methods that do something similar, like methods that drop features, networks that do anytime routing etc. (Adaptive neural networks for efficient inference, Deciding how to decide: Dynamic routing in artificial neural networks, Anytime recognition with routing convolutional network, Resolution Adaptive Networks for Efficient Inference, Skipnet: Learning dynamic routing in convolutional networks)\n\nOn a fun note, I wonder what happens if the euclidian distance metric is switched to a cosine one. This would turn the problem into a flattened weight tensor such that we have x.dot(c) for each class. Essentially this is a single neural network layer, with a softmax afterwards. This could work just as well, and make it easier to 'retrain'. Alternatively, you could consider what happens if you had more cluster centers than num_classes. Perhaps you'd be cheaper off mapping the entire feature map to some smaller sub-space, and then projecting back to the softmax output with e.g. an SVD method :)\n\n",
            "summary_of_the_review": "All-in-all, the paper is interesting, but the practicality of the method is very problematic. \n\nI think I could increase my score if:\n\n- The authors commented far more on the added complexity and parameter count\n- Had comparisons to methods that have large num-classes\n- Had comparisons on networks that were actually representative of mobile use-cases, or at least something that's more reasonably sized than a 150 layer network for CIFAR10\n- Compared their method to multi-scale dense nets, and other mentioned methods",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a new method for performing early exits on a pre-trained architecture. Authors use support set to calculate class prototypes at intermediate representations. Then at evaluation execution on the network is stopped whenever the representations are significantly closer to a class at any level. Results on small datasets (cifar-mnist) using very deep networks show that proposed method achieve better results compared to some of the previous work.",
            "main_review": "I think studying early-exit is a very important research direction and the focus on simplicity is important (if not more than final performance) for achieving real world impact. As far as I am aware using nearest centroid classification (NCC) for early exit classification is novel (though not super original as NCC is used widely for classification) and the experimental study provided can be useful for future research. \n\nThis work is solely experimental, and is potentially interesting to the ICLR community. However, the experimental study is limited to some small datasets and lacks in-depth explorations. It is not clear how would the method scale to ImageNet or similar larger datasets/resolutions.\n \n# Concerns\n- Even though the authors of [1] focused on NLP, I think their method should be included as a baseline and possibly use the combination method to increase E2CM performance even further. Shallow-deep+e2cm is kind of similar to [1] as it uses 2 decision to decide to skip to the next layer (compared to using 2 decision to stop). Not sure how easy to do this during rebuttal, but regardless this would be a useful addition to the next version.\n- Is there a reason why authors choose to experiment with ResNet-152 and WideResNet-101 on Cifar/Mnist instead of repeating experiments done in previous work; let's say Shallow-Deep experiments. Using same benchmarks help researcher compare methods easily and therefore accelerates research. Moreover, Shallow-Deep experiments seems to be more diverse in terms of architecture and data. I recommend authors to have (at-least-some) matching experiments. Imagenet experiments would be appropriate, too; given the relatively small cost of the method.\n- I think many of the experimental details can be shared in appendix creating space for some new experiments. It would be nice to see some ablations and in-depth studies in the paper. some ideas: (a) effect of different normalizations: how does different normalizations used in architecture affect results. (b) How about the depth of the network? How does E2CM work on resnet-22? (C) How does the width of the network effects the utility/comparison.\n- Authors should give more details on binary search used to choose thresholds. An algorithm box in appendix would be nice. Similarly sharing the final algorithm for Shallow-Deep+E2CM would be nice. Reading the text it is not clear how does shallow-deep+e2cm works. Specially there are 2 ideas (1) concat of class means with activations (2) using 2 stage early-exit strategy. How do you do both of these?\n- Authors should compare their class-mean approach with prototypical adaptation (it is different), but readers would appreciate a short discussion on similarities and differences.\n- In Figure-2, why is the Branchy net curve flat? Furthermore Branchy-net uses 2 intermedieate activations. Do e2cm and shallow-deep methods use same number of intermediate layers? If e2cm uses all and shallow-deep uses 6, this might not be a fair comparison. Ideally all methods should be able to exit at same layers.\n\n# Minor\n- $c_4^kj$ is not defined. Is it a typo?\n- Authors mention 1000 points in text, however Figure-2 has much less points for each curve. How do you choose the points? Are they chosen on test set? \n- Using different symbols for different curves would make it easier to read in print.\n- \"One of the primary reasons behind traditional deep learning modelsâ€™ high computation demand is their tunnel-like design\" It is the fact that they bring better performance. Their tunnel like design prevents parallelism, though helps finding more complex functions.\n- \"One practical scenario for E2CM may be transfer l\" Here e2cm is not defined yet. Possibly remove this paragraph.\n- \"which suggest our method is more agile and powerful yet simple\" -> agile and simple.\n- \"Given a fixed training time budget\" limited training time\n\n[1] Bert Loses Patience: https://arxiv.org/abs/2006.04152\n[2] https://arxiv.org/pdf/2106.05409.pdf\n\n# After Rebuttal\nI read authors' response. I thank authors for sharing their preliminary results on Resnet-56, I think adding rest of the experiments would be a great addition to the paper. Similarly, adding [1] to the experiments would be great (I don't think the algorithm requires any domain specific adjustments, the idea is relatively simple). In short, most of my concerns stay, thus I keep my score. Hope the authors find the feedback useful.",
            "summary_of_the_review": "I think studying early-exit inference is a very promising research direction and I appreciate this work due its simplicity, which is important for achieving real world impact. As far as I am aware using nearest centroid classification (NCC) for early exit classification is novel and the experimental study provided can be useful for future research. However the potential impact of this experimental work is currently limited due to its choice of datasets/architectures and lacks in-depth investigations/ablations. Without these, I am concerned that the impact of the work would be limited. For example, it is not clear whether the method would scale to larger datasets and perform better than other alternatives. \n",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a method for early exit during inference of DNNs. The method does not require fine-tuning or gradient computation. Rather, it uses the learned embeddings from intermediate model layers to decide where to terminate the execution. Specifically, for a trained model, the method calculates the per-class means of intermediate activation. The distance between the activations of new samples is then computed with the per-class means and inference is exited once the distance becomes lower than a set threshold.",
            "main_review": "Strengths:\n- The paper is well written and easy to follow.\n- The proposed method is very simple and intuitive. It can be adopted in bare DNNs or in combination with previous early exit methods.\n\nWeaknesses:\n- Since the paper assumes that the DNN is pretrained, the early exit setup, e.g., the training of the sallow nets in (Kaya et al 2019) or the computation of per-class means and setting the threshold, can be done on the same HW platform where the DNN is originally trained. This step can thus be done prior to deploying the DNN in inference mode on a constrained embedded device. As such, it is unclear why it is important to reduce the training overhead for the early exit strategy? The evaluations in the non-constrained training time mode show that the proposed method cannot compete with prior methods unless it is combined with them.\n- Regarding the experiments in section 4.1, is the time overhead of finding the optimal threshold for a given FLOP constraint considered within the timing budget? For a fair comparison, the total allocated time to all methods should be equal, rather than just matching the number of forward passes. That being said, it would be great if the authors can comment on my previous question as to why the training budget needs to be limited.\n- The evaluated datasets are fairly simple, therefore it is not clear how well the method can generalize in face of more complicated data patterns such as ImageNet. Specifically, since the method depends on per-class means, it is important to evaluate on datasets with more than 10 layers, e.g., CIFAR100, or ImageNet. It is possible that as the number of classes increases, the intermediate representations, particularly in the earlier layers can no longer discriminate between the classes as easily, thereby increasing the FLOPs of this method.\n",
            "summary_of_the_review": "Please see the above review.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a method for early-exit-based conditional computation for reducing the inference time of a given neural network. The approach works as follows: E2CM saves the mean activation of each class after each layer and then, during inference, compares the activation of a given example to the recorded mean activations to get a prediction and its certainty. If the certainty is high enough, this prediction is returned, otherwise, the next layer is queried instead.  The authors show empirically that the method outperforms other approaches in a setting with a very limited training budget, and is sometimes able to improve the performance of other methods when applied in combination. Finally, the method is also presented in an unsupervised learning setting where it allows for significant computation savings.",
            "main_review": "Strengths:\n- The proposed approach is novel, and, to the best of my knowledge, early exits have not been explored from the perspective of non-gradient training, which is a new research direction. \n- The empirical evaluation shows that the proposed method does outperform the baselines in a setting with a very limited computational budget. Additionally, the experiments showing how E2CM performs in combination with existing methods are an interesting way to engage with the related work.\n- The authors show how their method performs in an unsupervised learning setting, which is an interesting new application of early exits, which were so far mostly considered for classification problems.\n\nWeaknesses:\n- The empirical evaluation, in general, is lacking and should be extended, as in the current form the performance and use-cases of the proposed method are not clear. By this, I mean not only considering more complex datasets or different architectures, but also more in-depth analysis of the memory usage and providing statistics to gauge the significance of the results. In particular:\n  - The empirical evaluation in the supervised learning setting should be extended. The most complex dataset used in this paper is CIFAR-10, which is currently not considered particularly challenging by the community. The additional results on MNIST variants are not enough to weigh the importance of the method, especially when used with powerful architectures such as ResNet-152 or WideResNet-101. Large-scale experiments on datasets such as ImageNet would be most convincing, but even increasing the scope to SVHN, CIFAR-100 or mini-ImageNet would be beneficial. Since the method does not require editing the base network, would it be possible to adapt it to a wide array of pre-trained models available online (e.g. via websites such as PyTorch Hub or Model Zoo)?\n  - The problem of additional memory expenses is not discussed well enough. Although the authors consider the additional memory footprint, they do so for the case of CIFAR-10, where the number of classes is fairly low (10). In practice, many real-world datasets consist of a larger number of classes. For example, for CIFAR-100 the overhead would be 10x higher than for CIFAR-10, which, if I understand correctly, for ResNet-152 means ~100% increase in the memory usage. Similarly, for a dataset with 1000 classes, we have 1000% memory usage overhead. This is quite significant, especially in application to devices with low computational resources. The authors consider adding early exits only at specific points to reduce the memory overhead, but do not show experiments on well it would work in practice.\n  - The supervised learning experiments do not include standard deviation - have they been trained with more than one seed?\n- The practical motivation for the proposed method is not clear to me. Since the proposed method outperforms other approaches significantly only in the setting with a very limited training budget, the possible applications of the method seem to be restricted. The authors propose some potential applications, but the methods are not tested in settings related to these applications. In particular, the paper claims that \"A practical use case for E2CM is when a large, expensive-to-train model is broadcast to edge devices with limited and heterogeneous computation capabilities.\" (5th paragraph of the Introduction), which I wanted to discuss:\n  - \"In such a scenario, different devices may train the E2CM model at different FLOP operating points depending on their computation capabilities\" - Most early exit methods (e.g. Shallow-Deep Networks considered in this paper) can adapt the FLOP threshold after training by choosing the confidence threshold, denoted by $T_j$ in this paper. So one could train the early-exit model along with the expensive-to-train model, broadcast it to the edge devices, and set the FLOPs as needed.\n  - \"Another practical scenario for E2CM may be transfer learning, where the training has to be done on a low power edge device with a local dataset that is different from the dataset the base network was trained on\" - this use-case is intriguing and more convincing to me, but not tested empirically in the paper and it is not immediately obvious if the method would work (maybe the class means won't be as well separable if we change the domain/task).\n  - In the light of application to edge devices, a more thorough discussion of memory usage is even more important.\n- The authors do not provide enough theoretical considerations of the method to justify accepting the paper without convincing empirical results. This is not a weakness but itself, but rather highlights the fact that the work has to be judged on the merit of the empirical evaluation, which is lacking.\n\nTo sum up, the actionable feedback I propose is providing a more thorough empirical evaluation of the method, clearly stating its limitations, and exploring in-depth the settings the method can be applied to (like transfer learning).",
            "summary_of_the_review": "Although the main idea of the paper is intriguing, I think the empirical evaluation needs to be extended and the possible application scenarios should be more thoroughly discussed. In the current version, I cannot recommend accepting this paper for ICLR.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}