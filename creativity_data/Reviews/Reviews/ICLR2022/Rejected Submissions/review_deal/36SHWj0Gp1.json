{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper presents a transformer model for learning representations of assembly code blocks, trained using a variant of the masked language modeling objective that encodes the full code block token sequence into a single bottleneck vector and then uses that vector to decode all the masked out tokens.  Overall reviewer assessment for this paper is on the rejection side, mostly due to the not so novel model architecture and training objective.  Experiments show that this variant of the MLM performs significantly better than the standard MLM objective without the bottleneck, which surprisingly is even worse than the simple TF-IDF in many tasks.  This raises questions and it is unclear from the paper why the variant with a sequence-level bottleneck should perform better than the standard MLM.  Binary code similarity detection has many implications in security, so this is a good domain to explore more in, and I encourage the authors to continue to improve this work and send it to the next venue.\n\nOne related work also published in the ML community comes to mind that the authors might not be aware of: Graph matching networks for learning the similarity of graph structured objects by Li et al., ICML 2019, which also looked at binary code similarity detection, but works at the function level.  It would be good to also take a look for other potentially missing related work."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes an ML-based binary similarity detection technique that uses a skip-gram to encode the instruction sequences and then uses a transformer to encode the whole code fragment. The paper evaluated in cross-compilation and obfuscation settings.",
            "main_review": "+ The paper is well-written and described very well. \n+ The topic is important, especially for binary code analysis and reverse engineering. \n\n\n- The task of binary similarity detection is not super novel. The paper claims that they are the first to propose block-level binary similarity. However, there are many such works including InnerEye, DeepBinDiff, PalmTree, etc. The authors did not compare with any one of them. In fact, they did not even mention these papers.\n\n-The modeling is also not novel. It is a combination of skip-gram and transformers.\n\n- Although the paper compared with many alternate model choices, they failed to compare with any baselines.\n\n- The authors did not evaluate how well the technique will work on sophisticated obfuscation. More interestingly, can it work on unseen obfuscation given they are using denoising skip-gram? \n\n\n",
            "summary_of_the_review": "The paper works on an important topic. However, they assume a weak setting (did not use sophisticated obfuscation) and did not compare with the state-of-the-art baselines. Thus, I am not convinced with the advancement reported by the proposed technique.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "==+== A. Paper summary\n\nThis paper proposes a new model architecture to encode assembly code. By using a new reconstruction loss with the new architecture, the paper achieves better fine-tuning results on downstream binary code similarity detection tasks. ",
            "main_review": "\n==+== B. Strengths \n\nThe performance of the new architecture is much better than baseline MLM and its variants. \n\n\n==+== C. Weaknesses  \n\nThe novelty of this paper is not very exciting. Also, only a single subtask is given (i.e., binary similarity check), which can hardly prove the generalization of the model architecture over the assembly code.\n\nSome important implementation details are missing.\n\nThe paper writing is quite confusing.\n\nThe evaluation is not very interesting.\n\nQuestions and concerns:\n\nThe idea of encoding the instructions together was used across multiple assembly applications (ASM2Vec, https://proceedings.neurips.cc/paper/2019/hash/093b60fd0557804c8ba0cbf1453da22f-Abstract.html ). This paper adopts this idea into transformers and masked the entire instruction for pretraining.  The idea is not very exciting.\n\nThere is a lot of typos in Sec 2.2 and Sec 3.3. I can hardly understand your derivation. For example, 1) I guessed out the meaning EM/TE/H in eq.2 and eq.3. But it never appears anywhere else and also in Figure 2.  2) I am confused about how you obtained eq.10 from eq.9. Also, what is K? What do you mean by `using an arbitrary function'? Is that a trainable matrix? These notations are broken. \n\nMany details are missing, e.g., your training hyperparameters for baseline MLM. This is important because pretraining a transformer using assembly code can easily run out of GPU memory.  According to your maximum sequence length ( which is 512 given in Sec.4), I don’t think this is enough for a moderate-size assembly code. Can you plot the histogram of the token sizes using your pretraining dataset? \n\nAlso, your baseline explanation is confusing (Sec 4.1, also a lot of typos and a lot of notations that are never explained). What is IDF? You should at least explain how TFIDF works. The paper constantly mentions `MLM outperforms all previous methods and machine learning baselines’, but no experiments are given. You can try out some naive machine learning baselines (e.g, a simple bag-of-words method given in https://arxiv.org/pdf/2006.05265v6.pdf). \n\nRegarding your experiment results, I don’t know why O2-O3 matching is easier than O0-O0 matching. Intuitively, O0 with disabled optimization contains more information regarding the code and may be easier to identify its cross-compilation counterparts. \n",
            "summary_of_the_review": "The paper has some novelty but needs a lot of improvements.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes an enhanced transformer for dealing with the generation of assembly code. This is achieved by feeding the model with one instruction at the time (as BERT did), and hence each of these is divided into sub-words in a position-preserving way.\nThe authors shows the efficacy of GenTAL against many other work proposed in literature, showing that their solution is able to better grasp all the possible different level of optimization applied by the compiler (O1 to O3).",
            "main_review": "Pros:\n+ domain is very interesting, as assembly language is very difficult to deal with\n+ good experimental section that considers many models from the literature\n+ creating and learning from different compiler optimization technique is a good idea, since compilers might be very aggressive on edit they apply on the code\n\nCons:\n- Obfuscation is lacking\n- Obscure for non-BERT experts\n- Writing can be improved\n\nThe domain is very interesting, since assembly is very difficult to tame. Also, the development of technologies like this could really help in many different domains, like reverse engineering (which is still a daunting task).\nThe issues of the paper are expressed as follows:\n\n**Obfuscation is lacking**\nThis scenario is the most complex one, and it is treated poorly inside the paper. While the optimization levels are treated and described, this is entirely skipped by the authors. The readers that have not much knowledge on the domain will be lost. Also, is this obfuscation applied at assembly or at the source code level? The paper that is quoted seems to point on the second one, but the author should add these details to the paper. \n\n**Obscure for non-BERT experts**\nThe authors could have spent more time in better describing the technology, as it is very difficult to read if there is no or partial knowledge of the domain.\n\n**Writing can be improved**\nThere is no section dedicated to the limitations, and there is a lot of space that could be saved to write them down (like, 4.1 is a wall of badly spaced text). ",
            "summary_of_the_review": "I am marginally above the acceptance, because some details were obscure to me (lack of my knowledge on the domain, and dense technical details on the paper), but I see the contribution.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}