{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper studies the problem of estimating the trajectory of a linear dynamical system when the covariances for the process and observation noise are unknown. The standard solution is to estimate these covariances from data, and this paper instead suggests an optimization procedure. They show promising experimental results. However there are two shortcomings: In terms of theoretical guarantees, they can only show convergence to a local optimum. Moreover they assume they have access to the ground-truth hidden states. Although this is an assumption that has appeared in earlier works, it seems to limit the applicability."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper studies the problem of using Kalman filter for estimating the state of a dynamical system when the noise covariance matrices, for both state dynamics and observation, are unknown. The paper assumes access to trajectories of both state and observation. In this setting, a natural approach to solve the problem is to use the data to form an estimate for the covariance matrices. However, the paper argues that an optimization procedure to find noise covariance matrices to minimize the MSE is favorable and should become the \"new standard procedure for KF tuning\". With several numerical experiments, the paper illustrates that the optimization based KF tuning provides much better and robust result compared to standard KF based on estimation, and the comparisons made in the advanced neural network based estimation literature is not fair.\n\n\n\n\n ",
            "main_review": "The paper is nice to read. I think the message that the paper is trying to convey is very clear, important and impactful. I appreciated the simplicity of the message, the style of the paper, and the effort in addressing its limitations and connection to related work. \n\nHowever, I am not convinced with the main message that the  optimization based procedure is better than noise estimation. In particular, I am not sure how do the authors evaluate the noise estimation for nonlinear setting, since the formulas given in paper are only for linear observation model. If, for nonlinear observation model y = h(x) + w , the noise covariance is estimated with Cov[y-h(x)] from data, then this is indeed the optimal estimator for the covariance matrix in mse. For a trajectory (x_k,y_k), the difference y_k - h(x_k) = w_k, where w_k are i.i.d, and empirical covariance of w_k is the best mse estimate for cov(w). So I would appreciate if the authors explain the noise estimation for nonlinear observation model and theoretically why is it bad. I read appendix E, but I did not understand the need for defining \\tilde{H}, why not use the full nonlinear observation model to estimate R.     \n\nMoreover, it is not clear if the proposed optimization procedure has a unique solution. For example, in (Formentin and Bittanti, 2014), it is shown that the KF estimate depends only on the ratio of noise covariances in scalar case. Therefore, the MSE optimization problem can only recover the ratio between noise covariances. So, it would be great if the paper have a clear and precise statement of the optimization problem it aims to solve, and analysis on why the minimizer is unique.    \n\nAlso, a main disadvantage of the proposed procedure is that the optimization problem is highly nonlinear and nonconvex, specially after Cholesky decompostion. The paper does not provide any guarantee for its convergence, unlike estimation which simply follows from law of large numbers. \n\nSome minor questions/issues:\n1- I could not find the result in (Humpherys et al., 2012) that the paper refers to. \n\n2- It will be good to add some classical adaptive filtering papers by Mehra and Carew and Bélanger.\n\n3- I don't think KF requires F and H to be time-invariant. \n\n4- I was confused by polar coordinates in 3d. Does it mean spherical? \n\n5- Does the proposed procedure have any advantage for linear Gaussian setting? \n\n\n   \n\n \n\n   ",
            "summary_of_the_review": "I recommend marginally below acceptance threshold. I think the paper is well-written, but it should be much more precise mathematically, explaining the estimation procedure for nonlinear setting, and address some fundamental questions about the optimization problem.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper claims that optimization of the Kalman filter parameters are needed in cases where the filter assumptions are violated.",
            "main_review": "The main contribution of the paper is not clear to me.\nIt looks like that the paper tackles the problem of noise estimation of the Kalman filter in an alternative method. However, the paper is not written in a cohesive way which makes it very difficult to follow what it narrates. It is not clear how different parts of the paper are related to each other.\nThe paper contains many well established concepts that looks unnecessary to include in the paper. \nThe title of the paper does not explain what has been tackled in the paper.\n",
            "summary_of_the_review": "The paper is not written well and does not have a flow. The rationale and objective of the paper is not clear. Therefore, I can not recommend it for publication. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper targets the design of a classical filtering method - the Kalman filter (KF). The linearity assumption is a strong limitation of KF models although a wide range of variants have been demonstrated for non-linear systems. Different from these studies, the authors focused on the estimation of the noise models in the KF(-class) models, in order to improve the accuracy and robustness of the KF estimates, through an optimization method. The proposed approach was assessed on a benchmark dataset, and the results demonstrated the superiority of the proposed method. Also theoretical analyses are provided in the appendices.",
            "main_review": "(+) The paper is well-organized and well-written.\n\n(+) Theoretical analyses of the proposed optimization method.\n\n(-) Noise estimation in KF models is a classical and open issue. The authors should carefully claim their contributions on this point.\n\n(-) The proposed method is a supervised learning method - ground truths are required - this is the major downside as compared to the regular KF methods.\n\n(-) Cholesky decomposition-based gradient descent is not novel for SPD matrices estimation.\n\n(-) Only benchmark datasets were used to demonstrate the superiority to the baseline KF. The data from practical applications are expected. Although a few samples of the filtering results over the real-world data were given in the appendices, no statistical comparisons were shown.\n\n(-) It is not clear on the explanation of the benchmark results - what is the scale of the errors obtained? They should be relative or percentage values, to better understand the improvements.\n\n(-) As ground truths are involved, it is interesting to see how the machine learning methods other than KF-related perform. It is reasonable to compare the proposed approach to the KF-related methods only. However, the ground truths are enforced, which are the most costly and most difficult to obtain in practice. From this perspective, it would be necessary to demonstrate the proposed KF approach performs comparably against these supervised machine learning methods other than KF-related.",
            "summary_of_the_review": "Please refer to the weaknesses in the above section.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes using gradient-based optimization to tune the state and noise covariance parameters defining a Kalman Filter via supervised learning (i.e., assuming access to ground truth state measurements during training).  The need for this approach is motivated by the stringent assumptions under which optimality of the Kalman Filter is shown, and that these assumptions often fail in practice.  It is shown through several case-studies that the Optimized Kalman Filter (OKF) not only significantly outperforms a baseline KF implemented with estimated noise covariances, but also matches or outperforms Extended Kalman Filters and an LSTM-based \"neural KF\" introduced by the authors.",
            "main_review": "Strengths\n\n+ The problem of estimating state from noisy observations for nonlinear systems is an open and important challenge, and the proposed method is intuitive and simple to implement, and appears to yield good empirical performance.\n\n+ The paper is clearly written and easy to follow.\n\n+ The empirical evaluations are comprehensive, in that many scenarios and baselines are considered.\n\nWeaknesses\n\n- Some of the more minor contributions are in fact well established.  While I completely agree that highlighting the shortcomings of the Kalman Filter in an expository manner provides excellent motivation for this work, the idea that state-estimation was a solved problem since Kalman and that the paper is reopening this solved problem is incorrect and a bit over the top.  That the Kalman Filter is only optimal under strong assumptions, and that it can fail spectacularly when these assumptions are violated, is very well known and hardly a contribution. Similarly, using a Cholesky parameterization for optimizing over PSD matrices is standard.  In light of this, I would recommend that the grandiose tone taken in the introduction perhaps be pared back a little bit, as I found it distracted from the otherwise nice insights of the paper.\n\n- Definition 2 is formulated under the assumption of a linear-time-invariant system; however, Kalman Filtering approaches in the time-varying setting also exist, see for example the textbook \"Linear Estimation\" by Sayed, Hassibi and Kailath.  That no such time-varying approach was used as a baseline was also disappointing, as this could likely have compensated for some of the errors introduced by nonlinear dynamics.\n\n- There has been a recent line of work on regret minimization based Kalman Filtering, which updates parameters/estimates online to compensate for the fact that \"effective noise\" is in fact rarely Gaussian or as modeled.  That no comparison to these baselines is presented is also disappointing.  These methods are further completely online, and require no labeled datasets for supervised learning.  Relevant papers to look at include:\n\n@article{goel2021regret,\n  title={Regret-optimal estimation and control},\n  author={Goel, Gautam and Hassibi, Babak},\n  journal={arXiv preprint arXiv:2106.12097},\n  year={2021}\n}\n\n@article{tsiamis2020online,\n  title={Online learning of the kalman filter with logarithmic regret},\n  author={Tsiamis, Anastasios and Pappas, George},\n  journal={arXiv preprint arXiv:2002.05141},\n  year={2020}\n}\n",
            "summary_of_the_review": "While the paper highlights and addresses an important problem, and proposes a simple yet effective solution, I believe that without a comparison to *online Kalman Filtering* approaches that minimize regret, it is impossible to determine if this truly represents an improvement over the state of the art in terms of empirical performance.  From a theoretical perspective, I see little to no novelty in the paper's contributions.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}