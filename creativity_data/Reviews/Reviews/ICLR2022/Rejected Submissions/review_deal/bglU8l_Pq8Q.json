{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "Strengths:\n* Well-written paper\n*Theoretical analysis demonstrates that dual encoder models have similar capacity as CA models\n*New distillation algorithm for learning DE students from CA teachers\n\nWeaknesses:\n* No reviewer seems particularly excited about this work \n* Theoretical analysis doesn’t provide actionable insight -- it does not directly motivate the suggested distillation methods\n* Empirical results are lacking -- reviewers asked for qualitative examples of improvements from their distillation method"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In information retrieval problems, there are two kinds of models: dual-encoder models and cross-attention models. Cross-attention models generally outperform dual encoder models by a large margin. Due to the latency requirement of real search systems, cross-attention models might not meet the requirements. So one ideal solution is to transfer the knowledge from cross-attention models to dual encoder models. This paper follows this line of research ideas by reducing the gap between the two models. \n\nFirstly, the paper theoretically analyzes the effects with sufficiently large embedding dimensions. Secondly, the paper empirically verifies the theoretical results. Thirdly, the paper proposes a new method that further bridges the gap between the two methods. The writing is clear and well-structured. I like the idea and logic of the paper. I have some questions about the experiment part.\n",
            "main_review": "\n\nPros:\n\n1. Good theoretical analysis on the power of dual encoders. I like this analysis as the starting point of the paper. \n2. The generalization gap of dual encoders makes sense to me. The empirical results in Figure 2 are interesting. However, I would like to see more deep insight. For example, analysis on special query-document cases might be useful.\n3. The two modification methods on knowledge distillation are interesting. The empirical results show the benefits of the proposed methods. However, I have some questions about this part (refer to the Cons part).\n.\n\n\nCons:\n\nMy main concern of the proposed approach is the experimental design parts. For information retrieval models, only utilizing the human-labeled dataset for knowledge distillation seems to be a weak baseline.  There has been some work on constructing datasets to improve the dual encoders (also called data distillation). For example, we can easily construct hard negative samples (cited in the paper) to improve the dual encoders. My feeling is that adding these results might make the paper even stronger.\n",
            "summary_of_the_review": "Overall, I like this paper. I would like more empirical results on data distillations.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper aims to narrow the performance gap between cross-attention BERT and dual-encoder BERT for re-ranking task. The authors empirically and theoretically analyze the underlying reasons of the performance gap. The gap could be mitigated by the proposed knowledge distillation method, where the original cross-attention BERT model acts as the “teacher” and the more efficient dual-encoder BERT model is used as student. Comprehensive experiments confirmed the effectiveness of the proposed KD method for various re-ranking tasks.   ",
            "main_review": "Closing the gap between cross-attention models and dual-encoder models is an essential research problem for industrial applications. The main contribution of this work lies in revealing the underlying reasons of gap between CA and DE models, and proposing a new KD method for re-ranking tasks. The whole paper is clear and well-written, \n\nBelow are some questions or confusions I had: \n1)\tI am a bit confused by the relationships between capacity, overfitting and generalization ability (or better OOD performance) as discussed in this work. It is obvious that a model with relatively large capacity tends to easily overfit the training data, while to get better OOD performances we need to establish new learning methods or design better model architectures. The BERT model has a large number of parameters, so it is not surprising that a dual-encoder model, twice size of a CA model, can overfit the training dataset, which is widely accepted in practice. To narrow the gap, the proposed is a modified KD loss, and it would be more interesting to give some suggestions on the network architectures, as the well-known ColBERT.   \n\n2)    The main contribution of this work is the Dual-encoders(DE) model based distillation, and this topic has been studied a lot. The proposed M3SE loss substantially outperforms previous distillation techniques on both re-ranking and full retrieval settings. However, M3SE seems to be a natural extension of previous work MarginMSE by adapting to multiple positive and negative documents. Considering that there is almost one positive doc for each query in MS MARCO and NQ, Equation 4 actually reduces to $(s_{+}-s_{j}^{*})^2+\\sum_{j \\in N} \\[s_j-s_{j}^{\\*} \\]^2_{+}$  in the experiment. This means that in M3SE, the teacher provides an benchmark for student model that what should the score of positive document be close to and what should its negative ones be lower to. It makes sense but seems to be a natural extension of marginal MSE.\n\n3)\tIn this work, 6-layer BERTs are used as student model. The authors are suggested to further study the question of how to find the optimal student network architectures, e.g. a relatively deep and thin network, under the constraints of inference time.\n",
            "summary_of_the_review": "This paper tries to reveal the underlying reasons of performance gap between CA and DE models, and a KD method is further proposed to improve the performance of dual-encoder model, while the analysis on the gap cannot guide the design of new student matching architecture or new distillation method.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper investigate the gap problem between cross-attention (CA) and dual-encoder (DE) models for document reranking. In general, cross-attention models perform much better than dual-encoder models while consuming more computational cost. The authors first leverage Mercer's theorem to prove that dual-encoders models are sufficiently expressive to model a broad class of scores using countably infinite dimension, which mainly the gap between CA and DE models should not exist from theoretical analysis. They further identified the gap is caused by the generalization ability of the DE models. To reduce the overfitting on the training data, they proposed to use distillation algorithms. Specially, they design the loss function which matches the margins between the teacher (CA) and student (DE) models. They also extend the margin to probability matching by using softmax cross-entropy loss. Through the experiments, they demonstrated that their proposed distillation approaches can further reduce the gap between DE and CA models.\n\nThe main contributions of this paper is that they show there should not be a gap between DE and CA models from theoretical analysis. They found that the gap is caused by the generalization ability of CA model and proposed distillation approach to reduce the gap.",
            "main_review": "Strengths:\n* They did a theoretical analysis which shows that DE models are sufficiently expressive to model a broad class of scores and there should not be a gap between CA and DE models.\n* They identified that the gap is mainly caused by the generalization ability of the DE models. They have similar performance as CA models on the training data while performing much worse on the testing data.\n* They proposed distillation algorithms that aim to match the margins between the teacher (CA) and student (DE) models. The experimental results verify that the proposed approach can further reduce the gap.\n\nWeaknesses:\n* It remains unclear to me why the DE models are less generalizable. Do the authors have some any explanations about this?\n* The theoretical analysis can not guide the algorithm design to reduce the gap.\n* They did not compare with other distillation algorithms.",
            "summary_of_the_review": "They have theoretically analyzed that the DE models are expressive enough and the gap should not exist between DE and CA models. They found the gap is because DE models are less generalizable and proposed an effective distillations algorithm to further reduce the gap. Taking these advantages into consideration, I think it is good paper to be accepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper contrasts cross-encoder vs. bi-encoder architectures for re-ranking in information retrieval.  The authors argue that bi-encoders under-perform cross-encoders, not for lack of capacity, but due to poor generalization, and back this claim with some theory and empirical results.  The authors then propose distillation of the cross-encoder model into a bi-encoder, which is shown to improve bi-encoder results.  While this has been proposed before, the distillation loss function the authors propose is slightly different than the standard cross-entropy (which the authors show is closely related).",
            "main_review": "Strengths:\nThe paper is well written and easy to follow.  Good analysis throughout, comparing the score distributions of the two architectures.  The experiments are sound and proofs are correct.\n\nWeak points:\nThe theory presented is correct, but not really informative in practice.  Prop. 1 has no bounds on the dimension of the embeddings.  Without such bound, it's easy to see how bi-encoders might represent any function, however this is not really useful.  I don't think this theory deserves the central treatment it currently has in the presentation, I would remove or relegate to the appendix.  On the other hand, it's interesting to see empirically that bi-encoders have similar training accuracy with cross-encoders.\n\nNovelty - there are multiple recent works which have demonstrated the usefulness of distilling a cross-encoder into a bi-encoder for retrieval.  Authors have cited some of them, but here are two more:\nhttps://arxiv.org/pdf/2010.10999.pdf\nhttps://arxiv.org/pdf/2010.08191.pdf\n\nAll of these works use the standard cross-entropy / KL-divergence loss for distillation, which the authors have shown to work as well or better than their proposed M3SE loss.  From a practical standpoint, the novel contribution of this work is pretty weak.  \n\nAs mentioned before, some of the empirical analysis is certainly interesting, but I don't think it is surprising enough to warrant publication by itself.",
            "summary_of_the_review": "This is a well written paper, however the contribution is not significant, due to multiple recent works already having covered this idea pretty well.  The theory presented is also neither informative nor surprising.  Some of the empirical analysis is certainly interesting, but I don't think it is surprising enough to warrant publication by itself.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}