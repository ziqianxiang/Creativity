{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes a meta-learning method with a latent feature space with a special structure of orthogonality and low-rankness. This paper is well written, and the use of the orthogonal low-rank embedding for meta-learning is interesting. The experimental results (including additional experiments in the author response) demonstrate the effectiveness of the proposed method. The author response addressed some concerns of the reviewers. However, the novelty of the proposed method is not high enough."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes Meta-OLE, which imposes a low-rank orthogonal geometry in feature space for few-shot learning. An adaptive orthogonal low-rank transformation is introduced for efficient adaptations to novel tasks with unseen classes. Also, a geometry-motivated classifier based on subspace projections with adaptive dimension selection is presented for fast and robust class inference\n\nThe proposed method is composed as follows: \n\n1.　Samples within a FSL task is first mapped by a universal feature extractor trained with the OLE loss.  (Actually, with softmax loss as shown in Eq.(9))\n\n2.　To exploit the support set samples, the proposed method meta-learns a lightweight adaptive transformation $\\Psi$ with OLE loss as shown in Eq.(4).  \n\n3.　The proposed method projects each data to subspace, and its lengths are used for classification as shown in Eq.(7). (But it is effective only using one leading direction for each class.)\n\n4.　The proposed method uses pseudo labels for using query samples for training. \n",
            "main_review": "\nStrengths. \n\n+The proposed method, which imposes class-wise orthogonalized distribution in feature space for generalization to novel classes, is elegant. \n\n+Good writing. The paper is well structured and easy to understand. \n\n+The proposed method outperforms the state-of-the-art methods in few-shot learning. \n\n+Ablation studies, which show the effects of OLE, meta-learned lightweight adaptation transformations, adaptive subspace projections, and the leverage of unlabeled samples, are satisfactory.\n\nWeakness. \n\n-The OLE loss is the same as the existing work (Lazama et al., 20018). The contribution of this paper is only the combination of OLE and meta-learning for a few-shot learning problem.  \n\n-The difference between the adaptive subspace (Simon et al., 2020) should be highlighted, and performance should be compared in Table.1.\n\nMinor problems. \n\n-Figure 4 is not referred to in the main text.\n\n-What is the number of mini-batch samples in each iteration of the Eq.(4)? \n",
            "summary_of_the_review": "The proposed method exploits the existing OLE loss to the few-shot learning problems. The method, which imposes class-wise orthogonal distribution in feature space, is elegant for generalization to the novel classes, and the evaluation is satisfactory. \n\n==\nPost rebuttal\n==\n\nMost of my concerns are addressed. \nHowever, I have degraded my score slightly because the novelty is incremental. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper mainly targets few-shot learning. The authors leverage the former work OLE in FSL to let novel class features more discriminative. The key contribution lies in the modification to make OLE robust enough to be extend to novel classes. To solve this, the authors propose to meta-learn a light-weight module ",
            "main_review": "pros:\n1.\tThe proposed modification of OLE in FSL is interesting. \nCons:\n1.\tThe experiment results on single-domain FSL cannot prove the effectiveness of the proposed method. The authors provide both inductive and transductive results on several datasets. However, almost all results are far beyond the state-of-the-art method, especially the transductive ones.  \n2.\tThe inner loop in Eq.(4) is similar to that in MAML, except that no classification loss is used. It would be better to explain why this term is missed in inner update.\n3.\tThe authors need to provide more detail on the subsection ‘Adaptive subspace projections as classifiers’, from which several questions arise:\na)\tThe proposed projection is only usable when K>1. It is not clear how the proposed method can help the original OLE to better adapt to 1-shot setting.\nb)\tAs I can understand Eq.(7), the classification results depend on the norm of the projected vector of original feature on each subspace. I wonder if the feature is normalized before projection.\nc)\tWill including SVD in training process lead to low training efficiency?\n4.\tMore ablation study can be provided, for example, the result of directly using nearest neighbor instead of projection after training with Meta-OLE; result of combining Meta-OLE with other regularization such as S2M2 [1], BF3S [2], etc.\n[1] Mangla P, Kumari N, Sinha A, et al. Charting the right manifold: Manifold mixup for few-shot learning[C]//Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2020: 2218-2227.\n[2] Gidaris S, Bursuc A, Komodakis N, et al. Boosting few-shot visual learning with self-supervision[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 8059-8068.\n",
            "summary_of_the_review": "Despite interesting perspective, this paper lacks detail on the methodology and ablation study. Meanwhile the performance is not convincing enough. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper aims to address the few-shot image classification problem. The authors propose a geometry-regularized method via meta-learned orthogonal low-rank embedding. The technique first employs an orthogonal low-rank geometry imposed feature extractor for extracting image-level feature vectors and then followed by parameter-adapted orthogonal low-rank transformation to achieve higher intra-class similarity and inter-class orthogonality. Finally, each class employs an adaptive subspace projection to directly classy images. In sum, the performance of the proposed method is somewhat weak compared with the existing FSL methods. ",
            "main_review": "[Strengths] \n+ The paper is well organized and easy to follow.\n+ The visualization of the feature space of z_p is good.\n+ The leveraging of meta-learning is interesting. \n\n\n[Weaknesses] \n- The level of technical contribution and novelty is incremental.\n   1) The proposed universal feature extractor is directly borrowed from OLE.\n   2) The meta-learning strategy of the proposed meta-learned adaptive transformation\tis similar to the MAML algorithm.\n   3) The proposed leveraging-query-samples is similar to [A,B] by employing the unlabeled query images to transductive learn the low-rank transformation with the predicted pseudo labels.\n- It is better to analyze the effect of the proposed adaptive subspace projection. For example, which cases of images benefit from such a classifier? \n- The model performance is left behind some existing FSL methods:\n\n   [A] Peyman Bateni, Jarred Barber, Jan-Willem van de Meent, Frank Wood: Improving Few-Shot Visual Classification with Unlabelled Examples. CoRR abs/2006.12245 (2020).\n\n   [B] Malik Boudiaf, Imtiaz Masud Ziko, Jérôme Rony, José Dolz, Pablo Piantanida, Ismail Ben Ayed: Transductive Information Maximization For Few-Shot Learning. CoRR abs/2008.11297 (2020).\n\n   [C] Da Chen, Yuefeng Chen, Yuhong Li, Feng Mao, Yuan He, Hui Xue: Self-Supervised Learning for Few-Shot Image Classification. ICASSP 2021: 1745-1749.\n\n   [D] Peyman Bateni, Raghav Goyal, Vaden Masrani, Frank Wood, Leonid Sigal: Improved Few-Shot Visual Classification. CVPR 2020: 14481-14490.\n\n   [E] James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, Richard E. Turner: Fast and Flexible Multi-Task Classification using Conditional Neural Adaptive Processes. NeurIPS 2019: 7957-7968.\n\n   [F] Imtiaz Masud Ziko, Jose Dolz, Eric Granger, Ismail Ben Ayed: Laplacian Regularized Few-Shot Learning. ICML 2020: 11660-11670",
            "summary_of_the_review": "The primary concerns of this paper are the weak performance and limited novelty. Though the usage of meta-learning is interesting, the proposed learning strategy is borrowed from the existing method, and the final model performance does not surpass the existing techniques.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}