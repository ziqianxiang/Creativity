{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper considers the problem of path integration in cognitive maps, where combining proprioception with visual inputs is required to estimate the displacement.  The paper proposes a small mechanism (a resetting path integrator) that extends a conventional LSTM for this purpose.  The resulting networks demonstrate better performance and interpretability than a conventional LSTM on tested problems.\n\nThe reviewers raised many issues with the paper.  One concern was whether the problem was to model biological, artificial, or robot problems (reviewer hpxs, PuPV), which the authors successfully addressed by stating that it is a minimal model. Many other minor concerns were also addressed. However, significant concerns remained.  One is the emphasis on the cognitive map (reviewer CUYu, hpxs) for which path-integration is a small part.  Another major concern is the significance of the results, with reference to the baselines and $R^2$ (AjJt, CUYu).  A third is on the generalizability of the method beyond single small examples (AjJT, hpxs, CUYu).\n\nAll reviewers indicate reject due to concerns that the paper is not ready for publication.  The paper is therefore rejected."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper considers the problem of path integration for the purpose of determining an agent’s position in space. The authors propose a recurrent neural network architecture that fuses information from two sensory streams and integrates the representation to predict total displacement from a starting point. Their main innovation is a gating network that “resets” the hidden state. The paper claims this helps to reduce the accumulation of errors compared with other non-resetting systems. To validate this claim, they compare prediction accuracy in a two-dimensional spatial domain. The paper also makes the following additional claims\n\n\n1. They claim this architecture is a good candidate for a cognitive map.\n2. The proposed architecture consistently shows better performance than baseline LSTM architectures on the same problem.\n3. The proposed architecture’s internal dynamics are more interpretible than a baseline LSTM.\n4. The proposed architecture produces higher quality representations.",
            "main_review": "### Main Review\n\n\nStudying spatial navigation within biological systems is interesting, especially from the computational learning perspective taken in this paper. To make an impactful contribution in this space requires a strong grasp of the cognitive science and the statistical machine learning literature, which the authors clearly have demonstrated. Critically, contributions in this space need to show that they can both fit within an existing conceptual framework of biological research, and provide a computational / statistical benefit.   \n\nThe ideas in this paper are conceptually well-grounded. However, in the manuscript's current form, the empirical support is too weak to sufficiently support its claims. For this reason and several others I expand on below, I believe this paper is not ready for publication. However, I am open to changing my score depending on the responses, and the degree to which my concerns can be addressed. \n\n### Detailed review\n\n\n**The architecture** described in Figure 2 fuses information from two sensory streams. A visual stream is encoded using a convolutional neural network then concatenated with information coming from a proprioceptive stream that has been encoded with a fully-connected network. Architectures that encode then concatenate multiple sensory streams has been extensively studied. See [1] for a recurrent architecture that fuses visual and auditory streams, see [2] for an instance combining two visual streams, see [3] for an architecture that combines visual, tactile, and proprioceptive streams, and [4] for a comprehensive survey of other examples. \n\nThe paper claims that \n> naive attempts at performing PI using the concatenation of proprioceptive and visual signals yield very unsatisfying cognitive maps which depend on position along the trajectory, but not on absolute position. \n\n\nSince the proposed architecture (RPI) concatenates proprioceptive and visual signals, I think the paper is taking issue with the way these signals could be encoded, or rather how they would not be encoded in a naive approach. Can the authors please clarify the point they’re trying to make? Based on prior work mentioned above, any approach that concatenates encoded signals would be natural.  What are the qualities of a cognitive map that would be absent from this approach?\n\nFinally, the joint encoding is used as input to a recurrent neural network then trained to predict total displacement. \n\n**Training the network** is accomplished with two loss terms that enforce a reconstruction of encodings to respect the temporal structure of a Markovian observation process.  A couple things about this confused me:\n\n\n1. The observation process is formalized as a Markov decision process. However, the paper does not mention anything about a reward signal, which is a critical feature of this formalism. Is the reward signal relevant to this work? \n2. The paper describes some shortcomings of seperately training their encoding networks. However, it was never clear how the separate loss terms (4) and (5) were combined so the full network could be trained end-to-end. \n\nOne of the main points of novelty in this work is **a resetting mechanism**—introduced to control the internal recurrent state and allow the system to use predicted encodings from the transition model or the direct encoding of the next state. Resetting is implemented with a gating network that computes a convex combination of the prediction and the next state's encoding. The paper implicitly claims that this mechanism is beneficial in reducing the accumulation of prediction errors, presumably whenever the prediction is more accurate than the direct encoding. A few questions:\n\n\n- Why would the model's prediction of the next state encoding be more accurate than the encoding that is grounded in direct experience? Please correct me here if I’m misunderstanding the significance.\n- How is the gating network trained to impose beneficial resets?\n\nAnother point made in this section gave me pause.\n> We therefore expect the internal state of the network to strongly depend on the current value of the position, but not on the trajectory used to get there\n\n\nHow can you guarantee that the internal recurrent state doesn’t depend on the history of states? Isn’t the purpose of recurrence to account for the past? \n\n\n### Empirical Results\n\n\nThe main hypothesis claims the proposed RPI architecture can integrate long paths with less error than architectures without a resetting mechanism. The experiments consider a two-dimensional domain and compare path integration errors in two data regimes, with over trajectories of 5 and 100 time steps, respectively. Results are tabulated with uncertainty quantified over eight trials.\n\nIn addition, the experiments compare performance of each system trained on different losses. In one case, an inverse model is used, and in the other case the inverse model is absent. \n\nGenerally speaking, the experiments seemed to ask most of the right questions. There appears to be some positive support for the utility of resets within Table 1. However, it’s not clear if the results are entirely positive because most of the confidence intervals overlap. In addition, I struggle to understand the significance of the second experiment (comparison of losses). I also have a few issues involving the evaluation, baselines, and significance of some results which I describe below. \n\nIn general, more information is needed to understand the baselines. The significance of this experiment is less clear to me\n\nThere are two baselines that are necessary to establish the utility of the proposed architecture.\n\n\n1. RIP with [\\mathcal{G}=0](tex://\\mathcal{G}=0). The recurrent state always uses the direct encoding.\n2. RIP with [\\mathcal{G}=1](tex://\\mathcal{G}=1). The recurrent state always uses the prediction.\n\nComparing to the first baseline establishes that the system can benefit from prediction. In the second case—which I believe is similar to the Vanilla LSTM—the comparison implies something about the benefit of resets (i.e. direct experience). To fully support the main hypothesis, the data would need to show the learned RPI system achieves a lower path integration error than these two extremes.  Does the Vanilla LSTM reflect the [\\mathcal{G}=1](tex://\\mathcal{G}=1) case?\n\nIn both cases a new observation is provided every five steps. This is different from the standard Markov environment described in the formalism; where the system gets an observation at each step. I have a feeling like this property of the observation process is critical to the utility of resets. Can the authors comment on this and describe how they believe the system would perform as this hyperparameter varies? \n\nI have several issues with Table 1. and how it’s presented. It is unclear what the errors represent and what sources of randomness are reflected between experimental trials. The experiments only used eight trials, and it is apparent from the way confidence intervals overlap that more should have been used. I suggest this experiment is rerun with thirty or more trials to improve significance. \n\nAdditional claims regarding the significance of the [R^2](tex://R^2) comparisions were less clear. What conclusion should be drawn from these results?\n\nIn Section 4.2 an experiment was performed to in situations with ambiguous observation information. The line of questioning here was not explicit or clear from the writing. Can the authors elaborate on the purpose and significance of these results (Figures 6 and 7.) What does a positive result look like in Figure 7?\n\n### Additional Comments and Questions\n\n- The mathematical notation was effective, even though it was ambiguous at times.\n- Clarity could be improved if variables such as [\\mathbf{\\Delta r}_t](tex://\\mathbf{\\Delta r}_t) were mathematically defined.\n- How is the average over trajectories computed? Are errors first summed then averaged? \n- Consider adding a requirements.txt file to make it easier for others to install code dependencies and run your code.\n- There are many references to the Appendix; it would help if more information was brought into the main text or the additional material was excluded.\n- The related work gave sufficient coverage of the cognitive sciences, but perhaps less to the ML audience members of ICLR.\n\n\n> We insist that our observations, while processed by a convolutional network, are not standard images in a ”human-readable” format, but rather a spatially organized array of sensors.\n\n\nCan you expand on the difference between your “spatially organized” observations and the observations coming from a standard pixel grid?\n\n### References\n\n\n[[1] Crossmodal Attentive Skill Learner](https://arxiv.org/pdf/1711.10314.pdf)\n\n[[2] Multimodal Deep Learning for Robust RGB-D Object Recognition](https://arxiv.org/pdf/1507.06821.pdf)\n\n[[3] Making Sense of Vision and Touch: Learning Multimodal Representations for Contact-Rich Tasks](https://arxiv.org/abs/1907.13098)\n\n[[4] Recent Advances and Trends in Multimodal Deep Learning: A Review](https://arxiv.org/pdf/2105.11087.pdf)\n",
            "summary_of_the_review": "I am currently **leaning to reject the paper**, because in its current form, the empirical support is too weak to sufficiently support its claims. \n1. I believe the experiments need to include an additional baseline\n2.  The writing should have more details about the domain and methodology. \n3. Some line of empirical questioning seems disconnected from the main claims described in the motivation.  \n4. I cannot be sure the proposed architectural components make a statistical difference, because the reported confidence intervals overlap.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose a network that combines propioception information with visual inputs to estimate displacement along a trajectory. The  new proposed network uses an inverse dynamic model and it is trained to do path integration. Such architecture and training regime result in a network that learns to reset and presents an internal state which correlates with absolute position in the environment.",
            "main_review": "Pro:\n- Apart from some confusion with the terminology in the introduction, the paper is clear to read and there is an abundance of analysis performed. \n- The idea of learning state that encode the absolute position using path integration is interesting and potentially of great impact \n\nCons:\n- In the introduction the authors assert that previous memory trajectories are part of proprioception, as currently written this is very inaccurate. Also, vision inputs are not allocentric, but egocentric. Broadly speaking allocentric representations are found in the hippocampal–entorhinal regions after visual stimuli have been converted in the retrosplenial cortex where it’s possible to find egocentric representation. For these reasons, I found the first paragraph poorly written and I would encourage rewriting, maybe after having read some appropriate papers that have not been cited (e.g. Spiers and Barry, 2015. Moser et al., 2018) \n- Again, in the last part of the “Path integration task” section, allocentric information is used in the wrong way.\n- When presenting the environment the authors describe the retinal inputs using a parallel with a monkey; however, most of the previously cited literature is on mice/rodents, this is awkward. To mitigate this I would cite Akam and Kullmann before.\n- When explaining the network architecture the authors claim: \"In practice, naive attempts at performing PI using the concatenation of proprioceptive and visual signals yield very unsatisfying cognitive maps which depend on position along the trajectory, but not on absolute position.\" It’s unclear if this is a statement about what has been tried by the authors or what is present in the literature. In case of the latter, I would recommend the author to look at the citations I'm pointing out below.  Also it is not clear what they mean by naive. \n- Why H_0 is initialized with a concatenation of v_(s_0) with itself? is it one for the state and one for the cell of the LSTM? This should be clarified or motivated.\n- What happens if the images are provided at irregular intervals?\n- The authors used R^2 to determine whether the absolute position of the agent could be predicted from the artificial neurons. If this is applied directly on the embeddings then the authors should adjust it to account for the number of predictors in the model, which in this case  is high-dimensional. It’s not clear from the text  whether this was done or not so it should be clarified. Another option would be to use a linear decoder.\n- I couldn’t find anywhere the details of the hyperparameters used in the work\n- As a general comment there have been several recent papers that used unsupervised signal to learn allocentric representations (e.g. Buria et al, 2020;  Bicanski and Burgess, 2018; Whittington et al, 2018; Whittington et al, 2020) none of these work is actually cited\n\nMinor\nAuthors claim that a follow up direction could be to extend this work in 3D environments. However using inverse dynamic models in 3D could be more tricky due to an increased state aliasing. \n",
            "summary_of_the_review": "The authors claim to have obtained stable cognitive maps in a network trained to do path integration. However this claim is not supported by the results, as they show only a correlation between learnt embeddings and location in the environment. A cognitive map is a spatial knowledge about the environment, that could be used to guide behavior in a flexible manner. From the neuroscience literature when know that this knowledge is based not just on absolute position (place cells), but also on \n- an allocentric sense of direction (head direction cells)\n- boundary position (boundary vector cells)\n- and cells that support coding of metric distances as the animal moves through the world (grid cells). \n\nThis if we only report the most well known. So there is a lot more than just absolute location, but none of these features are present in the paper. For this reason I don’t think the authors can claim to have learnt cognitive maps.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces several artificial models of path integration and discusses their relationship with biological systems. They train agents that receive noisy proprioceptive information and nearby visual input to minimize their path integration error when navigating in a continuous 2D environment. They compare two different architectures for path integration, based on a vanilla and modified LSTM (the resetting path integrator) that both receive proprioceptive and local visual input processed by two separate MLPs. Both architectures received intermittent visual feedback and used a gating mechanism to replace the internal state when feedback was present. They assessed what loss functions realized improved performance on the task, and found their RPI model showed improvements compared to LSTM variants. Finally, they analyzed the structure of internal representations in the network, showing networks capable of path integration showed high correlation with the absolute position in the environment, indicating that an internal representation of position was learned. ",
            "main_review": "In principle, a study of this type  is well suited to ICLR, and I think some of the analysis of internal dynamics in particular could be interesting. However the manuscript was difficult to follow at parts, and was missing critical details that make it impossible to evaluate. I also have reservations about the significance of problem being studied, as a full discussion of contemporary RL agents that can perform path integration is lacking. I discuss major and minor issues below. I would be happy to take a look at a revised version, as I feel some of the issues with the manuscript are clarity, but my gut feeling is the project may take more time to mature before acceptance is warranted. \n\n\nMajor issues\n(0)\tMy central issue with this work is whether the RPI architecture that is the main focus of the paper is interesting because it reaches state of the art precision on a key benchmark (path integration), or whether it is interesting because it is tractable to analysis. I am not convinced that this is a state of the art network for an important task, or conversely that the results of the network analysis are interesting enough to warrant acceptance on their own. This should be made clear. \n\n(1)\tInsufficient description of past work in reinforcement learning. While there is ample discussion of the problem of path integration in nervous systems, there is very little context on approaches in artificial systems, which is the main focus of the present manuscript. Autonomous naviation is a rich field, and while some references to recent work are given in the discussion, little is given in the introduction. This makes it difficult to ascertain the novelty of the proposed architecture, and assess how the RPI fits into other contemporary models for navigation. \n\n(2)\tMissing critical details about network training and task setup. I found it difficult to evaluate many of the claims in the paper. I could not find critical details about how the network was trained using RL and associated hyperparameters. I am missing an exact definition of the action space – can both the direction and angle be arbitrarily chosen? Relatedly in Figure 5 the network is described as using “specific actions, chosen to force exploration” this process is unclear, making it difficult to evaluate the proposed claims. In Figure 2 the output of this network is unclear. Is it outputting the final action? Is it also trained to output the state? I understand that Figure 3 is demonstrating the use of forward and inverse models, but I am not exactly sure how it fits in with Figures 2 and 4. \n\n\n(3)\tAnalysis of internal network properties. I am a bit confused about the coefficient of determination results in Table 1. I am not actually sure how these are computed. For instance with the activity-position correlation, how are these combined across the x and y dimensions? For the visual correlations, why do those vary in the LSTM cases? Wouldn’t one expect the visual networks to be similar across all the architectures? It is hard to evaluate because we are not given information on how the agents were trained, i.e. does this positional tuning just arise because these agents are heavily overtrained in a single environment? The R2 values are also very high. Even though it is clear that Figure 7 units are spatially tuned, it is not obvious that they would show an R2 of 0.99. \n\nThe link to cognitive maps is also a bit unclear here and overstated. The suggestion is that a global ‘place field’ like cell that is learned is necessary for a cognitive map (i.e. the RPI units with high R2 to position), but many other representations, include those in egocentric coordinates (ie grid cells) are also suitable as cognitive maps. As the authors note, decoding spatial position would be more convincing, as would showing that these representations could be utilized in different tasks, i.e. the classic tolman shortcut experiments. \n\n\n\nMinor points\nPg 19 – table reference in appendix G is missing\nPg 9 – table reference is missing\nI am confused about the term retina and the idea of this as a visual circuit. The information being received is highly local, making this more like a whisking system or some sort of tactile system. \nFigure 7 – how were these examples chosen?\n",
            "summary_of_the_review": "In principle, a study of this type  is well suited to ICLR, and I think some of the analysis of internal dynamics in particular could be interesting. However the manuscript was difficult to follow at parts, and was missing critical details that make it impossible to evaluate. I also have reservations about the significance of problem being studied, as a full discussion of contemporary RL agents that can perform path integration is lacking.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper focuses on fusing external signals and proprioceptive signals for navigation tasks. They propose a direct-inverse model of environment dynamics to fuse image and action related signals. They propose an architecture, namely Resetting Path Integrator (RPI), that can easily and reliably be trained to keep track of its position relative to its starting point during a sequence of movements. RPI updates its internal state using the proprioceptive signal, and resets it when the image signal is present. The architecture outperforms LSTM in performance and interpretability, when benchmarked on a 2D navigation/localization problem. \n",
            "main_review": "#### Strengths\n\n- The design and motivation of the proposed method is well grounded in cognitive science, which is quite interesting.\n- The paper is well written, and there are sufficient details about the experiment design in the appendix.\n\n\n#### Weaknesses:\n\n- I am not exactly sure if ICLR is the right venue for this paper. Particularly, the problem that the authors study is a toy problem for navigation, it is 2D navigation with obstacles. Maybe it is common in the cognitive science community to use such environments to study the emergence of meaningful representations of the spatial structure of the environment. It seems in the AI community the focus is on complex 3D navigation problems[1]. I would recommend the authors to run the experiments on [1] so that the contribution of this work can be better evaluated. \n- Secondly, the network design is rather simplistic, it is a recurrent neural network with a single gate for resetting with external signals. It is not clear if this architecture will work beyond simple 2D navigation tasks. \nIn the abstract the author mentioned “state-of-the-art” LSTM for 2D navigation/localization problems, I am not sure where is this coming from. For example in [1], CMP performs better than LSTM. \n- In the embodied AI community, there are a lot of works on cognitive mapping (see [1][2][3]), but this work didn’t mention those, or compare with those. That’s also why I am not sure about the scope and audience of this paper. In other words, this paper might be aiming at understand biological systems, but how they can be used in AI agents or robotics is not immediately clear. \n\n#### References\n\n- [1] Gupta, Saurabh, et al. \"Cognitive mapping and planning for visual navigation.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.\n- [2] Zhang, Jingwei, et al. \"Neural slam: Learning to explore with external memory.\" arXiv preprint arXiv:1706.09520 (2017).\n- [3] Chaplot, Devendra Singh, et al. \"Learning to explore using active neural slam.\" arXiv preprint arXiv:2004.05155 (2020).\n",
            "summary_of_the_review": "This work proposes a simple yet effective way to fuse external signals and proprioceptive signals. It is benchmarked on a 2D maze-like environment and shows better performance than LSTM. I am not able to fully understand the contribution of the paper since it doesn't compare with other state-of-the-art cognitive mapping methods, and it doesn't use standard benchmarks. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a model for path integration that combines visual and proprioceptive information, i.e. a model that keeps a prediction of where an agent is located, given the past sequence of actions and observations. What is proposed is to learn paired direct (forward) and inverse models (Wolpert, 98), where the direct model predicts perceptual changes due to actions and the inverse model the actions responsible for an observed perceptual change. Through a gating mechanism information is selected from either the current location or from the path towards the location, which in effect resets path integration when unique locations are visited while overcoming ambiguities that might appear elsewhere.",
            "main_review": "The system relies on three losses; the path integration loss and the losses for the direct and inverse models respectively. The most interesting contribution is the gating network that selects either the visual state or forward predicted state for integration. In the experiments, it is shown how the gating network helps the system to reset the state when the visual input is unique to a particular location while relying on forward prediction in case of ambiguities. This is similar to the use of so-called keyframes in SLAM that are used for similar purposes.\n\nThe PI loss assumes that the true sum of all actions, or equivalently the location at each point, is available during training, which seems to be a critical assumption in practice. Given the system’s ability to reset, errors cannot accumulate more than what is done between each reset. In a practical scenario, whether it is a biological or an artificial agent, it is hard to see how this would work. In that sense, this is different from what is assumed in SLAM.\n\nInterestingly, the learned state depends not just on the particular location, but also on how the agent ended up there. When there are no ambiguities the gating allows the state to depend more directly on the visual input, while the opposite is true when there are ambiguities. The key component here seems to be the inverse model that is trained both to predict the action between states and the sum of actions back to the starting point, which forces the state to include this information.\n\nThe fact that the inverse model is supposed to work from the current space all the way back to the starting state, suggests that the corresponding neural network should have little opportunities to generalize beyond the starting states used during training, at least as long as there is not enough training data to avoid memorization. Since no information on how much data is used for training, one cannot really tell. However, since not only the PI loss matters, the inverse and direct models seem to introduce regularization that prevents pure memorization.\n\nIt is a bit unclear whether the proposed system is intended to model biological systems or to be used for artificial or robotic ones. Given how the conclusions are written and how noise is added to both the proprioceptive and visual signals to better emulate a real system, the biological connection seems to be most relevant. However, the simulated system is only shortly evaluated with respect to the degree of noise in Figure 6, which is left without much comment. It is also hard to tell whether the added noise really makes the simulation more realistic since no other data is used to support that. \n\nA weakness with the proposed formulation is that both direct and inverse models are deterministic, even if a probabilistic formulation would be more suitable, where the variance could be used as an indication of confidence which could affect gating. However, a probabilistic model would most likely be harder to analyse and require more training data. \n\nSomething that strikes one as odd is why P(a) maps an action a to the same dimension as V(s) does for a state s. Is this really necessary? Is P needed at all, since it is only two-dimensional? After all, P is used in (4), but not in (7). In most cases, the perceptual space is much larger than the action space, and there is nothing in the direct model D that should require the representations to be of the same dimensionality.\n\nA point that needs to be clarified is whether (9) is correct or whether Figure 4 is so. Currently, they do not show the same thing. By the way, there seems to be one circle missing to the left in Figure 5.\n\n",
            "summary_of_the_review": "The author should make it clearer what the expected target audience is, those who are interested in modelling biological systems, those interested more in artificial systems or those who search for inspiration in biological systems to build artificial ones. The paper does have its merits for each group. It might not be directly applied for e.g. robotics, but the fact that you can learn a state space that combines local and global information as suggested in this paper is worth reading about and could possibly lead to more applicable solutions in areas beyond biological modelling.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}