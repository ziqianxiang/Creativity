{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "After going over the reviews and the rebuttal, and skimming the paper, I feel like unfortunately this paper is not ready to be accepted.\n\nMy reasoning is as follows. I feel the comparison with A2C and PPO is not and should not be the main target of the work. Of course they are good to have as reference points, and they should be in the paper. But the work is not trying to claim that the distilled symbolic policy is more data efficient (or outperforms these methods). If that would be the point, that one has questions similar to reviewer cXsw about these baselines maybe underperforming (compared to other published work). Maybe this is due to a change in setup as argued by the rebuttal, nevertheless this makes comparison and understanding the results difficult. The other argument is that  A2C / PPO are not the most efficient DRL methods for atari.  Lastly, is the question of distilled symbolic policy having access to an expert, making this not an apples to apples comparison. \nBut as I said, and I think this is the point of the authors as well, this is not the point of the paper. But then I find the results not being sufficiently contextualized either by comparing to other methods in this space, or various ablation studies to motivate the choices taken by the authors. Similar points were raised by other reviewers (wezQ, cXsw). Some of these ablations have been brought forward in the rebuttal, but I think they should be a more central part of the work and implies considerable edits to the paper. \nI think the stance that the object identification is decoupled from the symbolic policy is also a bit dangerous. I.e. a learned object identifier (particularly in a visual more complex setting) will have different failure modes, which will affect the policy. I think having a paragraph discussing the issues raised by reviewer AL2N would actually strengthen the paper, and being open about open questions/weaknesses. Alternatively additional ablation or experiments in either other kind of environments (e.g. 3D or environments with occlusion) or just assuming some form of failure at segmentation the visual stream into objects to show robustness would be of interest. \n\nOverall I urge the authors to resubmit their work after properly integrating some of the feedback. In particular focusing on ablation studies or having baselines that are more similar in spirit or at least being more explicit of how it compares with existing work and what aspect of that existing work is trying to fix. For e.g. part of the approach is that it relies on distillation rather than dealing with the RL objective (as other methods might try to do). Now if you take those methods, but you phrase them in a distillation process how would they do? I don’t know if all of this needs to be done, but it just feel as a work to be less grounded and sufficiently far to other existing methods to trivially understand the relationship, while directly only comparing to non-symbolic methods in a way that is not in some sense in the advantage of the non-symbolic methods. \n Additionally,  being more explicit about the potential weaknesses of the method, maybe empirically showing what happens with imperfect segmentation. The work is interesting, and I agree that this is a young field and the goal is *not* to produce state of the art results or outperform DRL methods. And is *not* to solve all the problems with symbolic methods at once, but to improve our understanding in this space. But I think the framing is not the right one in the current manuscript."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents a method for distillating potentially any policy into a symbolic policy (in prepositional logic + basic maths operations), applied in 2D video games based on the Atari, NES, and Genesis emulators. The procedure (RoundTourMix) consists in applying random search to iteratively i) select the features of interests, and ii) solve for a symbolic policy (finding conditions / FSM induction) that works with those features. The teacher policy in all experiments in a neural network trained with PPO. The conversion from pixels to objects (\"geometric features\") is assumed solved. Experimental results are presented on 7 games.",
            "main_review": "The paper can be hard to follow at times, and could be simplified and clarified (e.g. 4.1 could be a figure, 4.2 could benefit from adding one). Another example is that Step 2 \"Guess and Observe\" of RoundTourMix is not defined in enough details (how many geometric features? picked how?) for the results to be reproduced. \n\nThere is a bit of missing related work. Specifically, \"State of the Art Control of Atari Games Using Shallow Reinforcement Learning\" (Liang et al. 2015) that trains linear models on higher level (almost symbolic) features extracted similarly as in this paper. In fact, those features are from the origin of Atari Learning Environment. A similar work is \"Planning From Pixels in Atari With Learned Symbolic Representations\" (Dittadi et al. 2020). Additionally, the litterature of inductive logic programming and case-based reasoning applied to (video) games is relevant, as inferring case-based policies from demonstrations has been an active area of research (e.g. \"Case-Based Planning and Execution for Real-Time Strategy Games\" (Ontanon et al. 2007)).\n\nThe experimental results show that a more robust policy than the PPO and A2C learned policies gets extracted for most of the subset of games that the authors picked (also exemplified in Figure 4.b.). But, this does not necessarily justify the choices of the RoundTourMix algorithm (central contribution of the article). Indeed there is no justification why one has to make a random selection (2. Guess and Observe) of the geometric relations, and why we can't simply be exhaustive for those games (I believe we can at least for Pong and Seaquest). This is my biggest criticism of this paper: there is very little justification and/or empirical validation for the many choices of the RoundTourMix heuristic. \n\n\"Transferability validation\" is an interesting experiment, that shows that the symbolic policy (distilled from a CNN trained with PPO on AdventureIsland 3) transfers to a similar but different game, AdventureIsland 2. Sadly, there was no apples to apples comparison by having a PPO trained policy simply plugged onto the geometric symbol representation. This would have allowed to show how much of the transferability comes from the features and how much comes from the \"if-this-then-that\" policy.",
            "summary_of_the_review": "The central contribution of the paper (the RoundTourMix distillation procedure) seems ad-hoc and many of its decisions are not well-enough justified. The experimental validation has some flaws that need to be addressed to make it a convincing contribution.\n\nFinally, this is honestly not a great fit (topically) for ICLR and would be better suited in a games (e.g. AAAI AIIDE, IEEE CoG) conference or workshop. In its current form, the contribution is too \"heuristic-y\" for publication at ICLR.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to bridge the gap between black box deep reinforcement learning (RL) and more interpretable symbolic RL by distilling learned deep RL policies into a symbolic decision tree. While the symbolic entities for each task are predefined, their algorithm is able to extract out a symbolic representation of the visual input as well as a symbolic policy in terms of a decision tree. The authors show that the distilled policy is highly human interpretable and even generalises better than the original teacher policy to tasks that are visually different but symbolically identical to the original training task.",
            "main_review": "**Review update**\nAfter reading through the authors' comments, I've decided to raise my score to a 6. I am willing to support acceptance of this paper. I believe it offers an original and definite contribution to an important but underdeveloped problem. It by no means solves all the problems need to bridge symbolic and black box deep RL, but having this work out there will allow others to build upon it.\n\n=========================\n\nThis was an interesting paper that shows the potential of symbolic RL, both in yielding better generalisation and in offering more human-interpretable policies. The authors' approach of distilling a black-box teacher policy into a symbolic one is quite interesting. The approach by no means bridges the gap entirely between black-box and symbolic RL, and the approach add some complexity by introducing extra hyperparameters including the maximum depth of the decision tree, an entropy threshold, and the set of symbolic building blocks that the symbolic policy can work with. The main strengths and weaknesses of the paper are as follows.\n\n**Strengths:**\n* original approach to distill black-box policies into symbolic ones\n* the symbolic policies found are human-interpretable\n* the symbolic policies are able to de-noise teacher policies, leading to qualitatively better policies (e.g. the symbolic agents take fewer harmless but wasteful actions at each time step; this means that we see, e.g., less paddle jitter in Pong when ball is far away, which we often see in PPO and other deep RL agents instead.) Perhaps the de-noising can help induce more efficient gaits in the Mujoco tasks.\n* the symbolic policies show superior transfer compared to their original teacher policies, showing the promise of symbolic policies in offering better generalisation\n\n**Weaknesses:**\n* some of the empirical results could be cleaner. This is the main weakness of the paper and is what makes me hesitate in fully recommending the paper for acceptance at this stage. Some of these issues, I believe, can be remedied by the authors during the discussion phase.\n    * how many random seeds were used for PPO, A2C and the distillation for all tasks? I see variances for PPO and A2C but not the distillation (which does use stochastic steps in e.g. Algo 1). It'd be helpful to know how robust is symbolic policy's superior performance to the choice of random seeds.\n    * the Atari results (Pong and Seaquest) are non-standard, making comparison with baselines in the literature difficult. First of all, they only allowed the agent a single life, and they train for 10M steps only (whereas 200M is more standard for published Atari results). Because of this, the paper's scores for PPO and A2C are far lower than what is in the literature.\n    * moreover, why is PPO’s learning curve for Pong far slower than that in Schulman et al, 2017 (https://arxiv.org/pdf/1707.06347.pdf). They seem to get near the max score of 20 points within 10M time steps.\n    * it would also be helpful to know whether the authors are using an open source release of PPO and A2C or a reimplementation. If it is an open source release, could the authors reference which implementations they used?\n* the authors gave a nice demonstration of the superior generalisation of their symbolic policies. The authors could also look to the Procgen Suite of games for further tasks to demonstrate generalisation. These were designed explicitly to train agents on one set of levels and test on held-out levels to evaluate agents' ability to generalise.\n* the authors mention tasks with continuous action spaces but offer no results on these (all reported results were on video games, which have discrete action spaces). What is the reason for this? If their approach was not as successful on continuous action tasks, it'd be helpful to report this and offer possible explanations why in the Discussion.\n* it'd be helpful to have some metric of the training complexity needed to distill a symbolic policy.\n* there were steps in the distillation algorithm that were unclear:\n    * what drives improvement in each iteration of Step 2 (Guess & Observe)? Particularly, what drives improvement of the geometric relations?\n    * how are candidate conditions for policy’s condition nodes chosen? It would seem like the list of possible conditions to pick is infinite, making search difficult.\n* the authors reference Botvinick et al (2009), but this reference seems completely irrelevant to the authors' discussion. The Botvinick et al paper is about hierarchical RL (hRL) in neuroscience. First of all, hRL does not seem relevant to what the authors are doing here -- hRL concerns hierarchical structure in policies where a higher level policy is defined in terms of macro actions, each of which defines a sub-policy in terms of primitive actions. This seems irrelevant to the authors' work here. Secondly, it is unclear why the authors chose to cite this particular hRL papers and not others, including Sutton, Precup, Singh (1999) on the options framework, Dietterich (1999) on MAXQ, Dayan & Hinton (1992) on feudal RL, or any of the many papers on deep hRL that have been published since.\n* minor point: the writing, while still highly readable, could benefit stylistically from the help of a proofreader more fluent in English. There were small grammatical errors (that did not hinder comprehension or ease of reading), but there were also awkward word choices. Here are just a few examples:\n    * p5, line 1 - “the learning target 1 rules the [geometric symbols→numerical state] step”: perhaps say \"1 determines the learning target for the [geometric symbols→numerical state] step\". Likewise for \"targets 2 and 3 rules the ...\" on the line below.\n    * p5, 2nd paragraph of the subsection \"The geometric operator search space\" - \"the nearest class-$i$ ($i$ is given) object from the **protagonist** in the current observation\": usually, \"protagonist\" refers to the main character of a story. Perhaps reword this to \"from **a reference object** in the current observation\"\n    * p6, last paragraph - \"father nodes\": the more common terminology is \"ancestral nodes\".\n",
            "summary_of_the_review": "I believe this paper offers sufficient contributions to the field to potentially be accepted. Although it by no means solves all problems in this domain, it does help toward bridging the gap between black-box deep RL policies and more interpretable, symbolic RL policies. For me, the paper's primary contributions are the use of an original distillation procedure to translate black-box policies into symbolic ones, which offer the benefit of being more human interpretable and generalising better to tasks outside the training set. So this paper does have promise. But what prevents me from fully supporting it just yet are issues with the empirical evaluation. I would, in particular, like to ensure that the PPO and A2C baselines are accurately generated and that the results are robust to the choice of random seeds before I can fully support the paper's acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a novel method for finding symbolic policies for image-based RL environments. The method consists of training a traditional DRL algorithm, and then \"distilling\" this policy by searching for a symbolic policy that mimics the DRL teacher agent. This can result not only in interpretable, lightweight policies, but also policies that generalise better than the original teacher policy.",
            "main_review": "The high-level procedure is relatively straightforward. Train a DRL agent directly on the environment, and then, given symbols extracted from each state, \"distill\" the policy into a symbolic policy (note that supervised learning applied in the context of imitation learning is known as behavioural cloning [1], and what is being done here also falls into the category of imitation learning/learning-from-demonstration). It is difficult to find symbolic policies from scratch, so this procedure makes sense, and results in policies that are (generally) more interpretable and generalise better. Without knowing the literature on symbolic regression/symbolic RL too well, to the best of my knowledge the technical contributions of this work are very good.\n\nA weakness is the strong reliance on object detection and geometric priors for extracting/forming symbols. As it is, the algorithm cannot be applied to more complex visual domains. However, I think we should not discourage research along these directions, as the general idea of distilling black-box policies into symbolic policies has good use cases.\n\nSome questions:\n- How important is the entropy threshold? Any way for setting/tuning this automatically?\n- Are there any (quantitative) ablation studies on the denoising procedure? It would be useful to know if the algorithm fails without this or just has worse performance.\n\nConsidering that the authors' method incorporates imitation learning, they may find methods such as DAgger [2] of interest. Assuming access to an expert policy that can be queried interactively, interactive imitation learning methods benefit from reducing the shift between the teacher's state-action distribution and that of the student (which is a known problem in behavioural cloning, and is potentially an issue for this method as well).\n\n[1] Pomerleau, D. A. (1989). Alvinn: An autonomous land vehicle in a neural network. CARNEGIE-MELLON UNIV PITTSBURGH PA ARTIFICIAL INTELLIGENCE AND PSYCHOLOGY PROJECT.\n[2] Ross, S., Gordon, G., & Bagnell, D. (2011, June). A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics (pp. 627-635). JMLR Workshop and Conference Proceedings.",
            "summary_of_the_review": "I believe the authors tackle an interesting and relevant problem in RL, combining both deep learning and symbolic approaches to good effect. To the best of my knowledge, there are solid contributions in this work, and I would recommend accepting this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces an algorithm called RoundTourMix to distill a trained neural network policy into an executable symbolic form. The algorithm is gradient-free and denoises the original end-to-end policy. Results are shown on Atari games and AdventureIslands, which demonstrate that the distilled symbolic rules retain the original performance level while being more resistant to domain shift. ",
            "main_review": "**Strengths**\n\n* This paper is well-written and easy to follow. The proposed algorithm is also clearly explained and contains enough details to reproduce. \n* To my knowledge, the proposed method is a novel approach compared to prior works. In Figure 4, the distilled symbolic rule recovers and even occasionally outperforms the original teacher NN policy, which demonstrates the effectiveness of RoundTourMix. \n* I appreciate the increased interpretability of the distilled rules.\n\n**Weaknesses**\n\nWhile the results on Atari games and AdventureIsland are strong, I am primarily concerned with the applicability and effectiveness of the method on domains beyond toy 2D pixelated games. The paper makes many simplifying assumptions that may not hold in more complex and useful domains. To elaborate: \n\n1. Section 3 discusses the \"geometric symbol representation\" of the game pixels, which requires a semantic parser to preprocess the image. The exact approach (described in Appendix A) is template matching + FlowNet, which overfits to the toy game domain. This can be very brittle and result in compounding error in the downstream rule selection. Even worse, it will not be compatible with more realistic domains, such as:\n\n\t* 3D environment where the embodied agent can move freely and change perspective. The simple geometric shape parsing will fail, as the background changes dramatically with egocentric motion. \n\t* Occlusion, overlapping objects, deformable objects, etc. The current symbolic representation cannot handle any of these complex scenarios. \n\t* Locomotion tasks with a complex robot morphology that is unclear how to parse from pixels. \n\nMeanwhile, blackbox approach like PPO will be able to handle all of the above without the hand-engineered preprocessing pipeline. \n\n2. Section 4.2 discusses the geometric operator search space, which contains elements like `velocity_extension` and `static_line_drawer`. These are hand-engineered primitives tailored for simple 2D environments that cannot be easily applied to more sophisticated tasks. For example, how to handle complex contact interactions between the robot and its surroundings? What about deformable objects? Objects that change state (like cooking tasks)? Navigation in 3D world? Mobile manipulation? Once again, approaches like PPO can adapt to all these tasks _without case-by-case modifications_. This severely limits the usefulness of the proposed symbolic rule system. \n\n3. The symbolic rules do not seem to have any mechanism for long-term memory. How can it be applied to tasks that require long-horizon planning and reasoning, rather than short-term \"muscle memory\"? PPO + LSTM solves this out of box. \n\n4. The \"transferability validation\" experiment in section 5 gives an unfair advantage to rule-based policy, because it makes the strong assumption that there exists a perfect object recognition system for all game levels, even with very different visual appearance. This certainly holds for AdventureIsland, but does not hold for any real-world tasks. The same invariance can be achieved for ConvNet if we pretrain the early layers of an NN policy to be domain-agnostic, using techniques like data augmentation, domain randomization, or contrastive representation. \n\n5. The authors claim that \"smoothing and denoising\" help make the distilled rule more robust, and show the performance boost in Fig. 4 for certain tasks. However, similar \"denoising\" techniques can be applied to PPO as well, in the form of robust representation learning. For example, the following works all contribute to better neural representations and more robust policies. It is unclear if symbolic rules can consistently outperform these approaches:\n\n    * Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels. Yarats et al. ICLR 2021. \n    * Reinforcement Learning with Augmented Data. Laskin et al. NeurIPS 2020. \n    * CURL: Contrastive Unsupervised Representations for Reinforcement Learning. Srinivas et al. \n    * Learning Invariant Representations for Reinforcement Learning without Reconstruction. Zhang et al. ICLR 2021.\n\nFurthermore, I strongly disagree with the general claim that \"our symbolic policy distillation approach captures the causal structure through data-driven experience\" (introduction section). \n\nFirst, the symbolic rule distills from an NN trained policy, which necessarily means that it will inherit any spurious correlation learned from the teacher. Even though the authors introduce tricks like \"denoising\", it will not be able to recover the true causal structure without any explicit mechanisms like do-calculus or counterfactual reasoning. \n\nSecond, the learned symbolic rules develop hard-coded constants that are _also very brittle_. For example, what if the Pong game is slightly stretched and the `X_pong` coordinate threshold is 45 instead of 40? Humans will have no problem generalizing, but the symbolic rule will fail even though the causal mechanism of the game remains the same (and visual appearance is almost identical). ",
            "summary_of_the_review": "While the paper is overall clear and easy to follow, it makes many strong assumptions that are tailor-made for simple 2D games. It is unclear how the symbolic rule framework can scale to more complex domains. I am unconvinced that the distilled rules are more robust or causal than the PPO counterpart, especially when trained with robust representation learning techniques. \n\nI give my rating due to the limited usefulness and problematic assumptions of the proposed approach in realistic tasks. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a novel algorithm that distills expert experience in the Gym Retro Games environment into an interpretable and symbolic policy. The main steps in their method are the following: first gather experience from a trained network and pre-process with pre-trained and frozen networks to extract a list of objects, their type, position and velocity for every image observation. Then these are passed to the next module that computes numerical states, such as auxiliary lines between the objects and computing relations between the objects. Finally, their novel algorithm makes use of genetic mutation to discover rules that consume these relations and take actions. The last part is trained to match the data from the expert and only then deployed for evaluation in the environment. Their empirical results show that their method can perform well and even outperform the expert policy used to collect the initial experience. Moreover, they show that their proposed method transfers much better compared to the end-to-end baselines (PPO and A2C) from AdventureIsland3 to AdventureIsland2, making it a more robust method. Importantly, they show how easy it is to interpret the decision making that selects the action from the current state.",
            "main_review": "Strengths:\n\n- To my knowledge this is a novel algorithm that shows promising results on a small number of games and shows to be more robust to changes in the environment.\n- Resulting decision making is easily interpretable by humans\n- The work is very clearly presented\n\nWeaknesses\n- Minor: “Abstracting pure symbolic rules from data is not new. The family of symbolic regression (SR) methods (Cranmer et al., 2020; Runarsson & Jonsson, 2000; Gustafson et al., 2005; Orchard & Wang, 2016) have been studied to to directly.” Remove double “to”.\n- Being a purely offline learning method it seems like the maximum possible performance of this method is closely related to the expert policy and the types of levels that training experience is gathered on. It would be very interesting to know whether the authors have thought about whether their proposed method could be extended to learn from online data generated by itself.\n- Another concern is that thinking about their proposed method, it seems that the method thrives on levels where the optimal action can be inferred from the current observation and that a predominantly reactive policy can perform well. I am curious about how their method would perform in a game like sokoban that requires careful planning, or others, where the agent needs to remember information that is no longer visible in the current observation. It seems to me that their proposed symbolic method does not have any memory and would therefore not achieve similarly impressive results.\n",
            "summary_of_the_review": "I recommend a weak acceptance for the paper, due to the novelty of their algorithm and the interesting empirical results presented, where the final policy is very interpretable. My main concerns are with the wider applicability of the method.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}