{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper presents a method for training neural networks with belief propagation-based algorithms. The approach is to set a fully factorized prior over weights, compute a forward and backward pass of messages on a minibatch, then set the new prior to be a slightly higher temperature version of the minibatch approximate posterior. This new prior is then used for the next minibatch, and training iterates.\n\nThere is a huge range of opinions amongst reviewers. The main thing that reviewers appreciate is the novelty of using belief propagation instead of backpropagation for training neural networks. Finding alternatives to backprop with favorable properties could be hugely impactful, so even small gains in this direction are valuable. The posterior-as-prior update is interesting, and the authors have clearly put in care to getting things working. The main weaknesses are that some of the experiments aren’t always reasonable and fair, the paper is framed to overstate its contribution, and there’s not a clear advantage over standard approaches (e.g., MNIST error rates for a two hidden layer network are >2%).\n\nIn the end, this is a very borderline paper, but I find Rev nNGL’s position to be most informative. In particular, the paper frames the main contribution to be message passing as an alternative to SGD for training neural networks, but this is too broad of a framing given the existence of other closely related approaches like Soudry et al pointed out by Rev nNGL. I’d recommend that the authors frame their work as an advance over other message passing-based approaches to training neural networks, and to focus on piecing apart precisely why the proposed approach improves over EBP and alternatives."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors describe a \"focused\" belief propagation strategy for learning NNs.  They argue that it continues to exhibit some of the same nice properties that stochastic gradient methods have in practice with the added benefit of allowing the computation of approximate marginals to improve the accuracy of predictions.",
            "main_review": "Clarity of writing:  The manuscript contains a significant number of typos that, while irritating to read, do not inhibit understanding.  The larger issues are the plethora of undefined or underdefined terminology, e.g., channel functions, damping, etc., the imprecise mathematical formulations, e.g., dimension of vectors, z^l, etc., and the lack of motivation for mathematical formulations, e.g., BP is never really defined and equations (8)-(19) are unmotivated.\n\nNovelty:  The general approach (with the prior) seems novel to me, but it is a bit unclear exactly what other pieces are novel here compared to existing work.  \n\nSignificance:  While it is interesting that a BP style message passing approach can achieve comparable levels of performance to SGD, the overall significance of the works seems somewhat limited.  In particular, I'm not sure that the authors really present a compelling example of when this approach would be preferred over pure SGD based solutions.\n\nSpecific comments:\n\n- The introduction makes vague claims without support, e.g., no citations for applications of BP, \"additional approximation turns out to be benign\", etc.  Consider making it more precise.\n- The paper conflates factorizations with factor graphs (which aren't really discussed at all).\n- \"non-homogeneous\" -> inhomogeneous\n- What is a scalar channel function?  It needs a definition.\n- \"The training error is usually lower for optimized configurations from message passing schemes, suggesting that these algorithms are able to achieve higher capacity than SGD-based algorithms (given the same test error)\"  While, looking that the plots, the convergence of some of the BP variants is certainly better than SGD, I'm not sure that your statement would continue to hold if you ran for another 100 epochs.\n- Do you use adaptive step size methods like ADAM for SGD?  \n- The experiments make claims about deep neural networks, but most of the experimental results are on shallow networks.\n- The number of repeated runs in the experiments (5 in some cases) might be a little small.",
            "summary_of_the_review": "An interesting approach that is hampered by a poor presentation, somewhat limited experimental results, and only minor justification for why it should be seriously considered as an alternative approach to more well-studied approaches for training neural networks.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "[Summary] \n\nThis paper develops a class of fBP-based message-passing algorithms by adding a \n“reinforcement “ term to the BP equations and shows equivalent performance to the binary networks in experiments.",
            "main_review": "[Main Strengths] \n\nThis paper's main strength is that the authors' method of using message-passing to train NNs is pretty interesting, the introductory section is well-written, and the implementation GitHub repo is intended to be provided.\n\n================================================================= \n\n[Main Weaknesses] \n\nThe fundamental shortcoming of this study is that, while the basic concept is intriguing, it does not appear to make a significant impact, much like the various flavors of SGD. I invite authors to make the further improvement suggested in Section 4.4, which is to show that message passing is inherently less prone to catastrophic forgetting issues, which will be highly intriguing and will undoubtedly require more clear justification.\n\n================================================================= \n\n[Technical Comments] \n\n1) Is it possible to expand this method to multi-class scenarios, such as extending message-passing decoding algorithms from binary linear codes to non-binary codes?\n2) Furthermore, substantial message passing successes have occurred in the past, particularly for sparse factor graphs. As a result, future applications of Graph neural networks or sparse Transformers will be quite fascinating.\n\n================================================================= \n\n[Typographical comments]\n1) To ensure perfect anonymity, erase the name displayed in Acknowledgement.\n2) On page 3, the term \"PasP rule\" is not defined until it is used in Section 2.\n",
            "summary_of_the_review": "Using message-passing to train NNs is intriguing, but, like the various flavors of SGD, it does not appear to make a significant difference.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This manuscript provides an interesting try on alterative training algorithms for deep neural networks, based on (approximate) message-passing algorithms based on the well-known belief propagation (BP) algorithm.  In particular, the binary neural network is considered and four algorithms (BP, three variants of BP, i.e., BPI, MF, AMP) are proposed within a unified Posterior-As-Prior Update framework. Experiments are conducted on standard supervised classification tasks and continual learning settings, which shows comparable performances as standard SGD based methods. \n\n==========================\nAfter rebuttal:\n=========================\n\nI have read the authors' feedback (many thanks for the detailed point-to-point feedback) and other reviewers' comments and modified the score accordingly. Overall, the proposed scheme is interesting, though strictly speaking the results are not very advantageous (at least from its current results) compared to traditional ones, and some of the comparisons seem not very reasonable/fair. \n\n",
            "main_review": "**Strengths of the paper**\n-  A new paradigm of deep neural network training is prosed based on well-known belief propagation, which is a very interesting and positive try. I do think that such kind of exploration itself is meaningful for future research. \n- The resultant algorithms achieve comparable performances as the SGD based methods. In particular, the mini-batch implementation running time is also about the same order as SGD, although apparently, it is still slower than the standard SGD algorithms. \n\n**Weaknesses of the paper**\n-  **The first weakness is that there seem no apparent advantages of the performances of the BP-based methods** (even for most complicated BP/BP-I), both in terms of the generalization error and running time (or implementation simplicity). Given this fact, then, one might doubt the practical usage of BP-based methods. As a result, it would be really helpful if the authors could find some scenarios that BP-based methods that are favorable. \n\n Indeed, it is noticed that the authors had already made an effort in this direction, i.e., the evaluation of local Bayes error and local energy, and the application in continual learning,  similar to the Bayesian neural networks. Some comments are as follows:\n\n1. Using more standard benchmark metrics to quantify uncertainty under distributional shift\n\nAlthough local Bayes error and local energy are interesting merits, other standard benchmark metrics to quantify uncertainty under distributional shift such as Expected Calibration Error (ECE) and out of distribution (OOD) entropy might be of more interest [1]. It is thus suggested to add experiments on such standard benchmark metrics [1] and compare with other Bayesian deep learning methods.\n\n2. Adding comparisons with previous methods for continual learning (CL) \n\nAlthough the BP-based method is shown to be well-adapted to continual learning,  this might not be viewed as a special advantage since there are a variety of simple methods for the SGD-based method to enable continual learning [2,3].  All kinds of Bayesian methods (including BP-based) such as variational Bayes, Laplace method et al, are all naturally well-adapted to continual learning within the variational continual learning (VCL) framework[4] and others.  In particular, for binary neural networks considered in this manuscript, there also have been some studies on SGD-based Bayesian training algorithms well-adapted to continual learning, e.g.,[5]. The authors only compare BPI with standard BinaryNet and it is suggested to add some comparison with these previous continual learning algorithms to see if there is any improvement of BP-based methods. \n\n-  **The second weakness is that only binary neural network is considered and there is a lack of evaluation of the continuous weights of BP-based methods**. From my own understanding, the proposed BP-based methods are readily applicable to deal with continuous weights, e.g., AMP was firstly designed to deal with continuous weights. So, why continuous weights are not considered here? It would be helpful if several results for continuous weights can be added with a comparison with SGD based methods.  Otherwise, please add some discussions of the differences between continuous weights and binary weights. \n\n- **The third weakness is that there is a lack of comparisons with several closely related works, in particular, the Expectation Backpropagation (EBP) algorithm [6]**\n\n1. Several closely related works on previous attempts in training deep neural networks using BP-like algorithms are missing. In particular, **[6] proposed one Expectation Backpropagation (EBP) algorithm**, which is applicable to **train deep neural networks with not only binary weights but also continuous weights**. EBP also has a forward pass and backward pass as the paradigm in the current manuscript. Moreover, **EBP has been shown to have similar efficiency and update form as the SGD based method, while achieving competitive performances**.  Although it is based on EP rather than BP in this manuscript, the two are very closely related [7]. In particular, for the AMP variant considered in the current manuscript, **AMP has been proved to be an approximation of EP in [8,9]**. Then, does it imply that the proposed AMP training version is similar to EBP in [6]?  Given such similarity, it is highly suggested to discuss the differences and intrinsic relationships between EBP with the proposed BP-based method, as well as adding comparisons in the experients. \n\n2. Another related method is the proximal mean-field (PMF) method in [10], which is also one training algorithm for deep binary neural networks. **PMF  is proved to be equivalent to a proximal version of the mean-field (MF) method**. As a result, what is the difference between the MF version in the current manuscript from the PMF method in [10]? \n\n3. Moreover, in the Bayesian deep learning community, there are various Bayesian training algorithms using mean-field variational inference methods (eg., [11,12,13]) but are based on SGD methods. Then, what is the key difference between the MF algorithm in the current manuscript with those MF-VI methods? It seems that they (including AMP, BPI, BP) optimize the same variational objective (ELBO bound) but the only difference is the specific optimization methods, previous ones (e.g.,[11,12, 13]) used the SGD to optimize the ELBO bound, while the MF in current manuscript used message passing updates. If so, then maybe the results are expected to be similar or the same? And possibly the BP-based method is not fundamentally different from the SGD method as stated?  Another interesting point is that, from current experimental results, BPI and AMP are basically the same as MF, while apparently BPI and AMP use more accurate (tree-structured, or Bethe) approximations. Can the authors provide some insights into the negligible differences between them? \n\n\n**Additional Technical Comments**\n\n1. It seems that the so-called Posterior-As-Updates (PasP) is simply the core of the Bayesian theorem in the sequential updates setting (here different mini-batch corresponds to different observations, though possibly with an overlap), which I think is also used in [7] similarly. \n\n2. In the experiment parts, e.g, Figure 1 and Table I, why results of BP are missing? \n\n3. After training, how to perform the prediction using the results of BP-based methods? Are they obtained by sampling from the posterior distribution first and then computing the average output of different samples? Please illustrate clearly in the main text. Also, can the authors plot the posterior distribution of the learned weights? \n\n4. Similarly, how the continual learning is performed? Is it using the learned posterior of the previous task as prior over the next task to learn, similarly as [4,5]? \n\n5. Why the training error of BP-based methods are much lower than SGD while the test error is high or about the same? Is there any intuitive explanation? \n\n6. It is unclear of the so-called _additional reinforcement message_ and how the resultant BP-based algorithms are different from the original versions, e.g., MF, AMP. It would be better to give some explanations in the main text if this point is important. \n\n7. The submitted code of this manuscript seems unavailable. \n\n8. It is suggested to add a discussion of the limitations of the BP-based methods from current results. It seems that although it is comparable (or slightly worse with slightly higher complexity) to SGD methods, additional advantages (Bayes, continual learning) are not apparent either, given that a variety of Bayesian deep learning methods can do the same in a (presumably) simpler way.\n\n\n**References**\n\n[1] Ovadia, Yaniv, et al. \"Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift.\" NeurIPS 2019. \n\n[2]  Zenke, Friedemann, Ben Poole, and Surya Ganguli. \"Continual learning through synaptic intelligence.\" ICML, 2017.\n\n[3] Parisi, German I., et al. \"Continual lifelong learning with neural networks: A review.\" Neural Networks 113 (2019): 54-71.\n\n[4] Nguyen C V, Li Y, Bui T D, et al. \"Variational continual learning.\" ICLR 2018.\n\n[5] Meng X, Bachmann R, Khan M E. \"Training binary neural networks using the bayesian learning rule\". ICML 2020.\n\n[6] Soudry, Daniel, Itay Hubara, and Ron Meir. \"Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights.\" NeurIPS. Vol. 1. 2014.\n\n[7] Minka, Tom. Divergence measures and message passing. Technical report, Microsoft Research, 2005.\n\n[8] Meng X, Wu S, Kuang L, et al. \"An expectation propagation perspective on approximate message passing\". IEEE Signal Processing Letters, 2015, 22(8): 1194-1197.\n\n[9]  B. Cakmak, O. Winther, and B. H. Fleury, “S-AMP: Approximatemessage passing for general matrix ensembles,” in IEEE Information Theory Workshop (ITW), Nov. 2014, pp. 192–196.\n\n[10] Ajanthan T, Dokania P K, Hartley R, et al. \"Proximal mean-field for neural network quantization[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 4871-4880.\"\n\n[11] Alex Graves. \"Practical Variational Inference for Neural Networks\". In NIPS, 2011\n\n[12] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight Uncertainty in Neural Networks. In ICML, 2015.\n\n[13] Osawa, Kazuki, et al.. \"Practical Deep Learning with Bayesian Principles\". In NeurIPS, 2019.\n\n",
            "summary_of_the_review": "This paper applies the well-known belief propagation (BP) algorithm and several variants to train deep neural networks with binary weights. Overall I like the topic of this manuscript and it is a good start to explore the potentials of BP-based methods for deep learning. While this is indeed a very interesting try, there are several aspects to be improved for the current manuscript, such as a systematic evaluation of the potential advantages of the proposed BP-based methods, a comparison with previous closely related algorithms especially the EBP algorithm and other Bayesian deep learning methods, as well as clarifications of some related technical points, as detailed above. In addition, given the comparable (or slightly worse performance with higher complexity) but not competitive results of the proposed BP-based algorithms, it is worth a discussion of the intuitive reason behind it. This is not saying that it is not useful to study an algorithm without SOTA performance, but rather on the opposite, it is very useful to explain the (intuitive) underlying reason why it works this way even it is not SOTA, as well as its limitations. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a belief-propagation message-passing training algorithm for multi-layer neural networks. This algorithm is adapted to mini-batch training and biases distributions toward high entropy solutions. Empirical results show that neural networks with discrete weights and activations trained with this algorithm achieve comparable performance the same networks trained with SGD (BinaryNet), and can make approximate Bayesian predictions that have higher accuracy than pointwise solutions.",
            "main_review": "Correctness:\nNo correctness concerns.\n\nTechnical novelty and significance:\nOne of the ideas of the paper---to use belief propagation to estimate the marginals of the weights of a neural network (NN) in order to make probabilistic predictions---is a novel paradigm allowing to attach confidence to NN predictions. This is a research direction with a lot of potential which can lead to better NN performance in many tasks.\nHowever, the paper's theoretical contributions are somewhat limited as they seem to be very similar to the contributions of (Baldassi et al., 2016). Although the authors claim that the novelty of their approach is that their results apply to mini-batch training and deep neural networks, it is unclear how the results derived for deep neural networks differ from those derived for shallow ones in (Baldassi et al., 2016). Can the authors provide a more detailed explanation?\n\nEmpirical novelty and significance:\nThe second numerical experiment (on continual learning) provides empirical justification to use the proposed algorithm to avoid catastrophic forgetting. The downside of the numerical results is that, again, the neural network architectures that are considered are not very deep. They are wide, but this wasn't as emphasized in the text. In the first numerical experiment, the proposed method does not have any advantages with respect to SGD other than a lower training error. The authors mention that this could suggest that their algorithm leads to better NN capacity, but this is not explored any further in the text. I suggest running simulations with deeper neural networks and expanding on the advantages and disadvantages of the proposed BP-scheme.\n\nOther concerns:\n- What is the quality of the approximation in eq. (3), i.e., how much is lost by making this approximation?\n- It would be helpful to have a formal definition for the variables in eqs. (6) and (7).\n- In the caption of Fig. 2, the authors state that \"the training hyperparameters in the two cases are independently selected and generally differ\". How are they selected? By cross-validation?\n\n\n",
            "summary_of_the_review": "Although the paper is a close extension of (Baldassi et al. 2016), the idea to use belief propagation to estimate the marginals of the weights of a neural network (NN) in order to make probabilistic predictions is a novel paradigm and the empirical results on continual learning seem promising. I recommend acceptance subject to the clarification of the concerns discussed in the main review.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}