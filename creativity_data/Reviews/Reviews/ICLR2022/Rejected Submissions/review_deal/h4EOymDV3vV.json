{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes extracting multiple-scale features using denoising score matching. Reviewers pointed out the limited novelty in the work and that it does not cite various previous work and how it connects to them.  The paper needs some further  polishing on the writing, and in making the use of lambda divergences more rigorous and principled as explained in the comment of Reviewer VdM1 ."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a revised version of score-based models for representation learning. Specifically, they add an additional input to the score network which is a latent code encoded from the clean sample by an encoder. The encoder can be further conditional on time. Using the pertaining algorithm they achieve improvement of SOTA semi-supervised learning methods. They also show how adversarial training and tuning of the initial noise schedule can improve the performance of score-based models. ",
            "main_review": "The paper is easy to follow and well-motivated. The idea of incorporating a latent code into the score network looks promising and interesting.  However, I have a few concerns:\n\n* A few important formulations and implementation details are missing. For example, what is the exact formulation of VDRL? It is only briefly discussed at the end of section 2.3 but without further introduction. For the semi-supervised learning experiments, what is the value of $t$ used to extract the latent code in inference time? Did you use the adversarial training and initial noise schedule tricks that are introduced in section 3?\n\n*  From the limited experiment description, I suppose that for the semi-supervised learning, the encoder is first pretrained by the DRL objective and then finetuned by the original LaplaceNet algorithm. In that case, it seems that the most contribution leading to the performance is still from the LaplaceNet, and the improvement is tiny. A more informative experiment is to extract features directly from the learned encoder and train an L-2 SVM classifier on the top. \n\n* Section 3 and the corresponding experiment subsections look like a completely different topic that is not relevant to the paper title \"representation learning\". I would suggest removing those content and including more details and variants of representation learning experiments.\n\n",
            "summary_of_the_review": "Overall I appreciate this paper's idea of using score-based models for representation learning. However, I feel the writing and experiments can be improved before it gets accepted. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes to leverage the recently proposed mixture of denoising score matching losses of score-based generative models for representation learning. Since the representation encoder is conditioned on time, it can be viewed as an infinite dimensional representation. Results demonstrate improvement over previous semi-supervised learning techniques by pre-training classifiers with the method proposed in this paper.\n",
            "main_review": "## Strengths\n\nThe method proposed in this paper is simple and easy to use. Experimental results also demonstrate improvement over baselines.\n\n## Weaknesses\n\n1. The idea of representation learning with multi-scale denoising score matching is not new. Very similar ideas have already been discussed in at least [2][3][4].\n\n1. Using \"diffusion-based\" and \"score-based\" interchangeably in the paper makes it very confusing. I highly recommend sticking to one name. Given the dependency of the proposed approach on denoising score matching losses, I feel \"score-based\" might be a more appropriate naming.\n\n2. Proposition 1 is quite straightforward. Everyone who understands the proof of denoising score matching should know clearly that it is equivalent to the Fisher divergence up to a constant. It feels inappropriate to write down the expression of the constant as if it were your own contribution.\n\n3. The second sentence in section 3 is misleading. The work of Song et al. 2021a didn't claim diffusion models are trained to minimize KL-divergence—it is only true for a particular weighting of the score matching losses called the likelihood weighting. Also, the statement holds for the likelihood weighting even if the score model is not curl-free.\n\n4. In the second paragraph of section 3.2, authors claimed that the maximum pairwise distance of images is 170 for CIFAR-10. This is incorrect according to song and ermon 2020, where the maximum pairwise distance is reported to be around 50 for images with pixel values in [0, 1].\n\n5. The adversarial training of lambda divergences is not principled because the unknown constant of denoising score matching can depend on lambda. Also authors should cite and discuss the related work [1] which discusses adversarial training for score matching losses.\n\n## References\n[1] Jolicoeur-Martineau, Alexia, et al. \"Adversarial score matching and improved sampling for image generation.\" ICLR 2021.\n\n[2] K. J. Geras and C. Sutton. Scheduled denoising autoencoders. arXiv preprint arXiv:1406.3269,\n2014.\n\n[3] B. Chandra and R. K. Sharma. Adaptive noise schedule for denoising autoencoder. NeurIPS 2014.\n\n[4] Q. Zhang and L. Zhang. Convolutional adaptive denoising autoencoders for hierarchical feature\nextraction. Frontiers of Computer Science, 2018.\n\n\n",
            "summary_of_the_review": "Authors propose a simple idea to extract multiple-scale features with denoising score matching. However, very similar approaches have already been proposed in previous works and authors fail to cite and discuss.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose a new scheme for training representations using diffusion probability models (DDPM). In detail, the estimator $s_\\theta$ receives the encoded original image $E_\\phi \\left(x_0\\right)$ (or $E_\\phi \\left(x_0, t\\right)$) as additional input - the DDPM loss function causes the $E_\\phi$ encoder to extract the features needed for image reconstruction. Remarkably, this representation can be deterministic or stochastic, pointwise or infinite-dimensional (with time variable $t$).\n\nQualitative experiments on MNIST and CIFAR-10 show that the encoder can learn some meaningful features. A quantitative comparison with LaplaceNet [1] on CIFAR-10, CIFAR-100, and MiniImageNet, demonstrates an improvement in the quality of semi-supervised classification.\n\n[1] Sellars et al. LaplaceNet: A Hybrid Energy-Neural Model for Deep Semi-Supervised Classification. 2021.",
            "main_review": "1. Strengths.\n\n    The idea, presented in this paper, is novel.  The motivation behind the method, taking its origin in the rethought conditional score matching objective, is clearly explained.\n\n\n2. Weaknesses.\n\n    a. Firstly, the submitted manuscript misses the detailed review of related works on representation learning. Therefore, it is rather difficult to realize which context the authors put their work in. The restriction of limited computational resources was mentioned, but it is still unclear what the formal restriction is.\n\n    b. In the absence of context, it is difficult to understand the choice of a baseline for the quantitative evaluation. The last two years were very successful for representation learning in the domain of images. Such methods as SimSiam [2], SimCLRv2 [3], BYOL [4] et cetera may be considered as reliable models for comparison. However, they are not even mentioned in the paper. Also, the evaluation protocol used in those works could be applied to the proposed DRL approach.\n\n    c. Though the idea of infinite-dimensional representations looks appealing, I did not understand how it can be applied for the downstream tasks. During the experiment, described in Sec. A5, the authors chose the specific timestamp according to the best classifier accuracy. Thus, the final representation is still point-based. Am I right, that for any other task, e.g. image retrieval, one should again test several values of $t$ and choose the best one?\n\n\n3. Questions and suggestions.\n\n    a. It is claimed in Sec. 3.1. that $\\lambda$-divergences can express any $f$-divergence. However, as far as I can understand, paper [5] has shown that $\\lambda$-divergence can bound the KL-divergence. Could you please put the proof sketch or the reference to one of the previous results?\n\n    b. What is the motivation behind the sum of a Uniform r.v. and Gaussian r.v. (Sec. 4.3)?\n\n    c. In Eq. 23 the decoder should be defined as $D_\\theta \\left(E_\\phi \\left(x_0, T\\right)\\right)$ instead of $D_\\theta \\left(x_T, E_\\phi \\left(x_0, T\\right)\\right)$.\n\n\n4. References\n\n    [2] Chen et al. Exploring Simple Siamese Representation Learning. 2020.\n\n    [3] Chen et al. Big Self-Supervised Models are Strong Semi-Supervised Learners. 2020.\n\n    [4] Grill et al. Bootstrap your own latent: A new approach to self-supervised Learning. 2020.\n\n    [5] Song et al.  Maximum likelihood training of score-based diffusion models. 2021.",
            "summary_of_the_review": "1. Pre-rebuttal rating.\n\n    Despite I admit the idea, presented in the paper, is interesting and novel, the evaluation does not seem conclusive for me. I ask the authors to motivate the choice of the baseline and explain what the use case of the presented approach is. Currently, I tend to rate the submission below the acceptance threshold.\n\n2. Post-rebuttal update\n\n    Having read the authors' feedback and other reviews, I keep the initial rating. While the presented ideas may be of some interest to the community, the evaluation of the approach should be more thorough.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper presents a novel technique for representation learning by means of generative models. In particular, non-adversarial score-matching methods are improved to control the level of detail in the representations and learn a infinite-dimensional code. This is particularly useful for the semi-supervised learning setting, where it is shown that pretraining via the proposed model allows strong class separation and leads to state of the art results.",
            "main_review": "Overall, the paper is well written and the method seems sound. The topic of score-based generative models as an alternative to GANs and VAEs is worth exploring and the authors propose an interesting way to generate powerful representations. \n\nThe fact that pretraining with such method allows to improve the state-of-the-art in the semi-supervised classification setting is an important contribution, even though the improvement seems less significant on datasets with more classes than CIFAR-10. It would also be desirable to have standard deviations associated to the classification accuracy values to better assess statistical significance of the result.\n\nResults on the generation task do not seem particularly impressive but it is fine for it not to be the main focus. \n\nA couple of typos should be fixed: page 1 the acronym SDE has not yet been defined; page 3 Densoising --> Denoising.",
            "summary_of_the_review": "Pros:\n- Interesting extension of current non-adversarial generative models, with expanded theoretical results\n- Powerful representation learning capabilities and SoTA on semi-supervised classification\n- Adequate experimental assessment and ablation\nCons:\n- Results on image generation are not very impressive\n- Statistical significance of the classification results could be better assessed\n\nI think that overall the paper is a valuable contribution and expands the knowledge on representation learning techniques in a meaningful way. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}