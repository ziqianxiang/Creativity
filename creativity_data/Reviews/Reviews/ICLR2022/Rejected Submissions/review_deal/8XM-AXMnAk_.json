{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes a novel strategy for deep active learning based on the training dynamics of the underlying deep model, defined as the derivative of the loss of the ultra-wide NTK. All reviewers enjoy the clean story and motivation of the proposed acquisition/objective function and appreciate the authors’ effort in providing theoretical justification and analysis. \n\nOne note is that -- as Reviewer 8dDv highlighted -- part of the analysis pertaining to the incompatibility between the generalization bound of NTK and the non-iid nature of active learning rely on numerical evidence: the MMD under the covariate shift setting (i.e. assuming that the conditional distributions P(Y |X) remains consistent) is shown empirically to be smaller than the dominant term of the generalization bound. This serves as a reasonable empirical motivation/ justification of the dynamicAL heuristic under the AL setting, but I would suggest the authors be more precise in the abstract / intro (e.g. abstract) that this is an empirical result. \n\nWhile the theoretical results are interesting, not all reviewers are convinced that the experimental results are sufficiently compelling. In particular, Reviewer YgGb points out that the significant performance boost reported in the main paper was mainly due to the non-retraining (i.e. not retraining the model (from scratch)) constraints imposed by the problem setup. Reviewer p3z9 shares the same concern that such a setting would be far from realistic at least for the data sets/labels considered in the experiments. The authors refer to Ostapuk et al, 2018 as a justification of the non-retraining setting; yet they assumed a high budget, e.g., up to 50% of all labels of datasets. \n\nIn summary, this is a theoretically well-motived work, but the empirical components need to be further clarified and supported with more realistic experiments to merit acceptance for the proposed solution."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors address the problem of active learning in the context of deep learning. Instead of querying new examples based on the decision boundary (which in nonlinear models can be tricky or even ill-defined), as it is usually done for linear models, they rely on the train-faster--generalize-better paradigm. Thus, they propose to optimize the \"training dynamics\", which is the time derivative of the loss function in the ultra-wide limit. By using pseudo labeling and subset approximation, their method allows for fast selection of the examples that should be added to the dataset.\nThey justify their active learning strategy through an analysis in the limit of very wide models, through the neural tangent kernel, which is particularly convenient since the NTK does not depend on time in the ultra-wide limit. \nThey study a quantity that they call \"alignment\" (which measures the correlation between input and output in the NTK space) and show that a higher alignment is related to better generalization (a better bound) and faster training (--> larger training dynamics). They then show that this is also true in the active-learning setting, where the data is no more iid, with the help of some empirical evidence. Finally, they test their method and compare it with other active learning strategies, which they systematically outperform.\n",
            "main_review": "\nThe work is very timely, since it leverages on recent results to provide a better objective for deep active learning, namely the time independence of the NTK and the train-faster--generalize-better paradigm. From there, everything descends smoothly, providing a clean story and a method that seems to work well. Even though the theoretical analysis is only justified in the ultra-wide limit, the empirical evidence provided (on satisfactory models and datasets) suggests that the method is still good for typical use cases.\n\nAs the authors truthfully recognize, with active learning the data is not iid, and this needs to be addressed. In particular, it is not clear that the dominant term of the population risk is given by B (which then goes as 1/Alignment). To do so, they upper bound the difference of the population risks using the two distributions (from population and from active learning) by the Maximum Mean Discrepancy (MMD). Then, they show empirically that MMD<B for several values of b, on the first query round:\n\n- They state that  the MMD is always much smaller than B, but the blue and yellow bars in figure 1 look similar. It took me a long while to realize that the bars have different axes. There is no legend nor visual aid that helps the reader. This must be changed.\n\n- They only show this for round R=1. Could one expect that the two quantities approach each other for larger R? My intuition would say that the MMD increases with R (though I don't know by how much). \n\n- Practitioners would be often interested in quite large R, but results do not seem to go in this sense.\n\n- From figure 1 we see that bigger b implies a smaller B/MMD. This can be expected, since with a bigger b the two distributions are more different. Would it be more interesting to compare at constant budget size, as done in table 1?\n\n- If we then look at table 1, we see that the best performing schemes are those with many smaller query rounds. Is there some intuitive reasoning that could lead to this expectation? What is the recommendation that the authors would give to a practitioner who wants to implement their method? \n\nDid the authors observe a deterioration in their active learning scheme as models become narrower? Should we expect this? Does this mean that dynamicAL should not be used for \"deep and narrow\" models?\n\nWhy only 4 query rounds for the CIFAR and Caltech datasets? It doesn't seem that the performance has converged as for SVHN (which got 9 rounds), and the performance is not close to those that are obtained in practice.\n\nFigure 3 might be more readable if different symbols are used for each method. I had a hard time identifying each curve.\n\nThere's a couple of typos and expressions that need some English polishing (nueral, convergence faster, ...).\n",
            "summary_of_the_review": "The paper looks good. Some aspects of the presentation can be improved. Some of the results rely on approximations or numerical evidence which might not be universal, but I find the method quite interesting, and it paves the road to new possibilities in deep active learning.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper provides a theoretically-motivated active learning algorithm based on maximizing convergence speed, which is justified by the \"train faster, generalize better\" intuition. The paper has limited but favorable empirical results. ",
            "main_review": "This paper has a nice derivation of the dynamics of the training loss and provides an algorithm to maximize the rate of convergence.\n\nMy biggest concern is the empirical evaluation. In the training regime on CIFAR10, only 37% test accuracy is attained which is really really low. Secondly, it is strange to have different experimental settings (batch size, number of rounds) for different datasets. I think the experiments require a larger amount of data (at least up to 10k labeled points for CIFAR10 and SVHN).\n\nOther comments:\n\nIn equation (15), is the gradient squared supposed to be in the summation? It doesn't depend on the summation so it seems like the summation is a factor of |S|.\n\nShould the inequality in equation (23) be in the other direction? How does maximizing over a restricted (smaller) set result in a larger value?\n\n",
            "summary_of_the_review": "The theory in this paper appears good, though I am not very familiar with NTK analyses which makes me hesitate to say more. The empirical evaluation lacks quality however.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper suggests a new method to perform active learning by choosing prioritizing examples that lead to faster convergence during training. ",
            "main_review": "This paper proposes a new active learning strategy that samples examples which would help the network converge faster during training.\nThe paper suggests that faster convergence during training typically leads to better generalization performance. Therefore it should be beneficial for and AL algorithm to select examples that will speed up training convergence. \nThis speed-up is measured by approximating the derivative of the of the loss with respect to the training iteration t (which they call training dynamics). The authors propose many relaxations to be able to compute this derivative in practice, notably: (i) when computing the loss, assign weak labels to unlabeled samples, and (ii) instead of computing the derivative on all possible subsets, compute it separately for each unlabeled example then approximate the training dynamics of a subset using the sum of the training dynamics of each sample in that subset. \n\nStrengths:\n\n1. The paper is well written and well organized. \n2. The idea of linking training dynamic to active learning is novel and potentially influential. \n\nWeaknesses:\nThe main weakness of this paper (and the reason I am not recommending it for acceptance) is its experimental section. The theoretical section is well written, but the problem is completely non-tractable and cannot be theoretically analyzed in a realistic setting so the experimental section is instrumental and necessary to support the paper's claims. However, I don't think the experimental section is thorough at all. \n\n1. Coresets AL is a very strong baseline on Cifar10 with a ResNet18, so it is a warning sign that in your experiments it underperforms compared to Random. If you look at Fig 4 in the coresets paper [1], all  AL methods (including Random sampling) hover between 60% and 65% accuracy on Cifar10 with budget 5000 using a VGG-16. I have also run many experiments in that exact setting using a ResNet18 instead and it's easy to reproduce these accuracies. \nIn fact, figure 3)a) in [2] (which you cite in your paper), runs the same CIFAR10 experiments and reports around 78% for random sampling using 5k samples. However, in the middle-left and top-right plots of your Figure 6 of Appendix E.2, the best accuracy on CIFAR10 when the budget is 5000 using a ResNet18 is below 55%. [2] provides the exact training hyperparameters to reproduce their results so you can try copying them and rerunning your experiments. \nThe only difference between your experiments and those in the CoreSets paper is that you don't retrain your models from scratch at each round and instead you continue training from the previous checkpoint. So either (a) not resetting the network weights makes a big difference or (b) your training hyperparameters do not allow the network to train to saturation. I don't think (a) is the culprit here because if you look at figure 3)a) in [2] they report 45% on CIFAR10 using 1k randomly selected images while your number is less than 30% (this is when you train from scratch at round 0). \nIf (b) is true then those plots are inconclusive, since you're not training to saturation and your method is specifically targeting examples that allow faster convergence. \nI know that it can be very hard to find good hyperparameters for these experiments because you're trying to use the same learning rate schedule at round 0 as you are on round 9, but the learning rate schedule that works with 1000 training examples is likely less than optimal when training with 9000 examples. \nI also understand that it saves computation time and cost to continue training where you left off at the previous round but running at least one experiment where you train from scratch at each round is again necessary to at least show that this isn't the reason your method stands out. Also these experiments can take less than one day when using the machine you mention in appendix C.1.\n Obviously in my comments above I focus on ResNet18 on CIFAR10 because I am familiar with this setting and could find apples to apples comparisons to point you to in [2] , but you should revisit the rest of your experiments on other datasets to verify that they do not suffer from the same problems. \n\n2. I understand that you use small budgets in your experiments to save compute. However, using a small initial budget benefits methods that don't rely heavily on the classifier's feature extractor. If you choose a small initial budget, the feature extractor will be messed up and again methods such as Coresets will completely mess up. \n\n\n[1] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In International Conference on Learning Representations, 2018.\n\n[2] Zhuoming Liu, Hao Ding, Huaping Zhong, Weijia Li, Jifeng Dai, and Conghui He. Influence selection for active learning. arXiv preprint arXiv:2108.09331, 2021.",
            "summary_of_the_review": "I found the paper's experimental section to be insufficient. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work proposes DynamicsAL, a novel AL criteria that selects new training example base on its ability to maximize the training dynamics $\\frac{\\partial}{\\partial t} l(f, y)$. \nThe authors supplied a practical algorithm (Section 3.2) and compared the proposed criteria with existing methods (Section 3.3).  \nThe authors also derived theoretical result on faster converges leads to better generalization (Section 4) under the assumption that the network has infinite width, and also empirically verified their result based in the non-i.i.d. case. Experiments on standard benchmark and architecture (CNNs on CIFAR/SVHN/Caltech101) are conducted, where DynamicsAL is shown to outperform existing approaches.",
            "main_review": "Strength:\nThe paper is technically clear and well-written, with good result on toy benchmarks (i.e., clean, small images with mostly balanced distribution). The method seems to be a good continuation of Ash et al., 2020; Liu et al., 2021 and built a theoretical bridge between these objectives with the training dynamics and generalization in deep learning theory justification.\n\nWeakness:\n* The infinite-width assumption: The main theoretical result (Theorem 1) is derived under the assumption of infinite width model. However, it is well known that the infinite assumption does not hold in practice (e.g., [Fort et al, 2020](https://arxiv.org/abs/2010.15110)), which has led the community to derive better approximations such as NTH ([Huang and Yau, 2019](https://arxiv.org/abs/1909.08156)) or their more tangible lower-order approximations ([Chen et al, 2020](https://proceedings.neurips.cc/paper/2020/file/b6b90237b3ebd1e462a5d11dbc5c4dae-Paper.pdf)). While I understand the authors are building their results based on classic NTK theory, it is still worth addressing (can be non-rigorous or based on small empirically verifications) cases where the infinite-width assumption does not hold. Mostly if the high-level conclusion of the theory (faster training -> better generalization) still hold?\n\n* The Pseudo Labeling approach: While I understand this is a technique that is being used in the previous works, I wonder what would  would happen if model prediction is wrong, such that the training dynamics is computed against the wrong label? Some comments about this in either Section 3 or in the conclusion section would be helpful.\n\n* Clarity: This is a minor comment: To make sure the deposition is maximally accessible, please provide derivation for equation (13) and (15) (e.g., in the appendix).",
            "summary_of_the_review": "A paper that continues the investigation of active learning using gradient/influence function signal (Ash et al., 2020; Liu et al., 2021). It builts a theoretical bridge between these objectives with the concepts of training dynamics and generalization, and supplied a algorithm that is practically effective. I personally find this work to be a nice value add to the community, both in terms of the theoretical contribution and the practical utility of the algorithm. Hence recommend accept.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}