{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper analyzes the gradient behavior of RNNs in terms of the Lyaponuv exponents of its trajectory/orbit, showing that RNNs with cyclic or stable equilibrium dynamics have bounded gradients, but if the dynamics are chaotic the gradients will explode. From these insights, the authors propose an algorithmic remedy for this pathology, which is essentially a teacher-forcing method that periodically projects the observation onto the hidden state during training. A thorough empirical investigation is performed showing the utility of the proposed approach for modeling chaotic data.\n\nThe reviewers had split opinions on this paper. Some reviewers found value in the theoretical contributions and the connection between Lyapunov exponents and behaviors of the dynamics of recurrent neural networks, while others thought the theoretical framework may have limited practical utility. Several reviewers found the initial experiments to be lacking, though many of their concerns were alleviated after the substantial additions the authors provided during the discussion phase.\n\nI believe the observation that exploding gradients are unavoidable when modeling chaotic data is important and would be of significant interest to the broader ICLR community. However, the practical implications of this observation have not been thoroughly described or investigated, and without this perspective, the theoretical results by themselves are much less impactful. In practice, it is usually the case that the ground-truth function is not learned exactly, the time horizon is finite, the gradients are noisy, the data-generating process is opaque, etc. Do these caveats have any bearing on the conclusions? The experiments address some of these questions, but only indirectly, and a more explicit discussion of the practical implications would broaden the impact of the paper.\n\nAlong the same lines, the practical utility of the theoretical framework could be further supported if there were some analysis of more varied or additional RNN use-cases. As one reviewer mentioned, I think the ICLR community in particular would appreciate any theoretical or algorithmic insights that might yield improvements on a standard baseline task like seqMNIST, which has served as a point-of-comparison for many alternative methods and which would facilitate comparisons to prior work.\n\nOverall, this paper does make some nice and potentially important theoretical insights about training RNNs on chaotic data, and it does include an extensive battery of empirical evaluations, however the practical implications remain largely unconvincing, and I believe the paper falls just short of the bar for acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper connects the gradient behavior of recurrent neural networks with its dynamics through the maximum Lyapunov exponent of a trajectory, which is generated by the respective RNN. In particular, several connections between RNNs which produce specific dynamics (e.g. with stable fixed points or limit-cycles) and the mitigation of the exploding gradient problem are proved. Moreover, the paper shows that RNNs generating a chaotic trajectory always suffer from the exploding gradient problem. \nThat being said, the paper emphasizes that in order to approximate chaotic systems, the RNN has to be able to produce chaotic trajectories, which is ruled out by design by several state-of-the-art RNNs that are constructed to solve the exploding and vanishing gradient problem. However, given that these RNNs - at least the moment the optimizer finds parameters such that the resulting RNN produces chaotic trajectories - will always run into the exploding gradient problem, the paper suggests an algorithmic fix for that, by forcing the hidden-states of the RNN to be the pseudo-inverse of the underlying ground-truth trajectory at equidistantly distributed points in time during training. This cuts off the gradients at these points in time and forces the output states to be close to the ground-truth trajectories. The interval for that is chosen based on the maximum Lyapunov exponent.\nIn the paper, three experiments are provided, where two of them are based on approximating a chaotic dynamical system (Lorenz and Rössler), while one is based on real-world data from temperature forecasting.",
            "main_review": "## Pros:\n* The paper is very clear and well written\n* I very much like the idea of providing a systematic and general overview to analyse the gradient behavior of RNNs\n* Very thorough literature review\n\n\n## Concerns:\n\n* In reality, an RNN trajectory heavily depends on the external input (forcing). The paper also states in the very end of the discussion that this forcing can significantly change the dynamics. Hence, it would very interesting to see some sensitivity analysis with respect to changes in the input for popular architectures (one with stable fixed points and one with limit-cycle behavior for instance) and a discussion on how this might change the provided theory. \n    \n* My biggest concern is regarding the experiments. Given the cut-off interval $\\tau$, I understand the suggested approach to be equivalent to simply splitting the input sequence into smaller windows of length $\\tau$ (in a mini-batch manner), and setting the initial hidden-state(s) to the pseudo-inverse of the ground-truth of the last entry of the window before. Hence, the only contribution is using specific initial hidden states and a theoretically motivated length for the windows (which in most cases has to be found anyway by hyperparamter optimization, as in many real-world applications the max. Lyapunov exponent cannot be efficiently computed). I would like the paper to acknowledge that the provided approach is in fact equivalent to windowing, which is a very standard practice for training RNNs. Moreover, I would like to see results using the same window length $\\tau$, as in the suggested approach, but using a standard initial hidden state (set to zero). I suspect, that the results will be quite similar to the reported results of the suggested method (given that LSTMs for instance have been very successfully applied on chaotic time series prediction). But I could be mistaken.\n    \n* It feels very natural to choose the predictability time to be the length of the intervals $\\tau$. However, following the reference provided in the paper (Bezruchko \\& Smirnov, 2010), predictability time is a way more complex concept and can even be defined without appealing to the Lyapunov exponent, meaning systems with the same max. Lyapunov exponent can have very different predictability times. Therefore my question: Why was the predictability chosen as $\\tau_{\\text{pred}}=\\ln(2)/\\lambda_{max}$? Was it simply a fit to the provided experiments? In my opinion, more experiments need to be done in order to have a valid empirical support for that, e.g. what about a high-dimensional chaotic systems in say $>50$ dimensions. I suspect that the chosen predictability time won't be an accurate choice anymore.\n    \n* It would be interesting to compare the suggested training method to other approaches suggested in the context of training RNNs (for instance LSTMs) on chaotic systems, e.g. (Vlachas et al., 2018) in the paper.\n    \n* Unfortunately, the empirical investigation is very limited and I would have liked to see results on other tasks, such as NLP tasks, health-care applications and others (as paper claims in the introduction: chaotic behavior can be found in many sequential data sets), to see if this approach actually increases the performance of RNNs in widely-used RNN applications.\n    That being said, language modeling (such as the Penntree bank (PTB) word-level and char-level task) would have been an excellent application, as the training of the models is already based on cutting the very long tokens stream into smaller windows, but in contrast to the suggested method by the authors, the initial hidden states are not pseudo-inverse of the ground-truth but are set to the last hidden state of the window before.\n\n### Minor comments:\n* The pseudo-inverse introduced in eq. (15) needs more discussion: Why can it be constructed like that (i.e. assumption of linearly independent rows)?\n* Please define things like basin of attraction (first used in Theorem 1)\n",
            "summary_of_the_review": "At this point, I cannot vote for acceptance, as \n\n* the provided theory is helpful to get an overview over the various general ideas of tackling the exploding and vanishing gradient problem, however, it is often prohibitively hard to prove (or it is simply not true) that the RNN exhibits specific dynamics (for instance limit-cycle behavior), due to the role of the external input, for which preferably no prior assumption should be made. That being said, the provided framework might be very limited in its usefulness, when it comes to constructing new RNN architectures which mitigate the exploding and vanishing gradient problem.\n* the suggested training method seems to be a minor deviation of very standard approaches\n* the empirical evidence is not sufficient (more diverse datasets should be considered) and the presented results lack more useful comparisons. \n\nI am happy to increase my rating, if the authors can resolve my concerns.\n\n\n=====POST-REBUTTAL COMMENTS========\n\nIn general, I'm satisfied with how the authors addressed my minor concerns, in particular I appreciate the additional experiments.\\\nHowever, my main concern that the theoretical framework provided is very limited in its usefulness remains.\\\nI raise my score to (very) marginal acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the exploding and vanishing gradient problem (EVGP) in the sequential modelling with chaotic dynamics. Theoretical analysis of the EVGP, based  on the relationship between loss gradients during RNN training and Lyapunov spectrum of RNN-generated orbits, is provided. Inspired by this analysis, an alternative to BPTT training algorithm is proposed, named sparsely forced BPTT, that forces the diverging dynamics to conform to the true trajectory, at regular time interval provided by the  Lyapunov spectrum.",
            "main_review": "**Strengths**\n\n- Analyzing chaotic behaviour of RNNs (vanilla, GRU, & LSTMs) based on the connection between loss gradients and Lyapunov spectrum is novel.\n\n- Although ideas similar to sparsely forced BPTT exists in the literature, its interesting to see there exists a natural notion of the interval at which the teacher should be forced onto the student. Although its a little disappointing to note that its not easy to find such an interval and would live as a hyper-parameter in the algorithm.\n\n\n**Weaknesses**\n\n- Proposed sparsely forced BPTT provides feedback in the training process at regular intervals and such an idea has been explored in the literature ( see https://arxiv.org/pdf/1803.00144.pdf, https://proceedings.mlr.press/v139/kag21a.html, and references therein. )\n\n- Paper claims that RNNs based on the fixed point idea or the restricted eigenspectrum of the recurrent matrix do not allow chaotic dynamics. No evidence for this point has been provided. \n\n\n- In all the theorems, there have been assumption on what kind of dynamics is being followed by the RNN (be it fixed point, chaotic, quasi-periodic). One has no hold of any quantity that regulates the dynamics. In essence, the theory only characterizes how the gradients will behave knowing the dynamics and essentially under the hood just boils down to the eigenspectrum of the jacobians, which has been explored numerous times in the literature (starting with Bengio et. al. as to why gradients explode or vanish). This work does not help any practictioner to carefully choose one architecture over other given that the time-series dynamics is chaotic. Theoretical anaylsis given the dyanmics maybe one another way to characterise the EVGP and upto a certain extent has novelty in its own right.\n\n\n**Questions for Authors**\n\n- The predictability time for any other problem would indeed be treated as a hyper-parameter. Do you have any sense of what values might be a good place to initialize the search from?\n\n- Sparsely forced BPTT looks a bit close in spirit to some alternative training schemes where auxiliary supervision is provided in the learning scheme (see https://arxiv.org/pdf/1803.00144.pdf, https://proceedings.mlr.press/v139/kag21a.html). Did the authors try out similar training scheme in their experiments? \n\n- Is there any reason behind not using ODE-RNNs as a baseline, since they are designed to be operating with stable transitions?\n\n**Writing Clarity**\n- After Eq.1, For RNNs, you use $h_t$ to indicate the hidden states and the transition function $F_\\theta$ for standard RNN, uses $h$ as the bias. Consider changing the symbol for bias for readability. \n- Missing references\n\t- Practical Real Time Recurrent Learning with a Sparse Approximation ( https://openreview.net/forum?id=q3KSThy2GwB )\n    - RNNs Incrementally Evolving on an Equilibrium Manifold: A Panacea for Vanishing and Exploding Gradients?  (https://openreview.net/forum?id=HylpqA4FwS)\n\t- Time Adaptive RNNs ( https://openaccess.thecvf.com/content/CVPR2021/papers/Kag_Time_Adaptive_Recurrent_Neural_Network_CVPR_2021_paper.pdf )\n\t- An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories ( https://ieeexplore.ieee.org/document/6797135 )\n\t- Learning Longer-term Dependencies in RNNs with Auxiliary Losses ( https://arxiv.org/pdf/1803.00144.pdf )\n\t- Training Recurrent Neural Networks via Forward Propagation Through Time (https://proceedings.mlr.press/v139/kag21a.html)",
            "summary_of_the_review": "Paper has two main contributions: (a) theoretical connections between loss gradients and Lyapunov spectrum that help in analyzing various RNN dynamics including chaos, and (b) sparse forced BPTT. The latter has been explored in the literature and the paper fails to mention these works in the literature review. Theoretical contribution is of some novelty, but it would be another way to explain EVGP, albeit with dynamics in mind. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper looks at the asymptotic behavior of the Jacobian of various RNN variants (standard, LSTM, GRU, PWL-RNN) when realizing stable vs chaotic dynamics. In particular, the paper shows that in the chaotic setting, the gradients asymptotically explode, i.e., when learning chaotic dynamics, the gradients have to explode (asymptotically).\nThe paper proposes to overcome this limitation by truncating the BP length and applying a teacher forcing method that periodically projects the observation onto the hidden state during training.",
            "main_review": "**Pros:**\n- The paper provides a fresh look at how learning chaotic dynamics is difficult for established RNN architectures. \n- The paper proposes to fix this issue by relying on existing architectures and techniques (teacher forcing), i.e., in contrast to proposing obscure architectures, which, unfortunately, seems to be standard in the research community nowadays. \n\n**Weakness:**\n- The Lorenz and Rossler systems used in the experiments seem to be deterministic and fully observable, i.e., the initial state + dynamics function defines the entire trajectory. Thus there is no need for using an RNN with a hidden state. One may directly learn a $x_{t} \\mapsto x_{t+1}$ mapping using a feedforward network. What are the advantages of using an RNN, which suffers from gradient issues, compared to feedforward networks? Experiments on partially observable chaotic systems would have been appreciated. \n- What happens in the \"inverting the output mapping step\" of the in teacher forcing when the output dimension is much lower-dimensional than the hidden state, and the output mapping is not uniquely invertible? Please elaborate.\n- No code was provided",
            "summary_of_the_review": "Overall solid paper. Please elaborate on the weaknesses mentioned above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors provide a study of gradients of the recurrent neural networks using Lyapunov exponents. They claim to offer a simple and effective training for the chaotic data. Some simulation results are provided.",
            "main_review": "Strengths: The authors establish a connection between Lyapunov exponents and behaviors of the dynamics of recurrent neural networks. Some derivations for particular recurrent neural networks are provided.\n\nWeaknesses: In general, Lyapunov exponents are one of the most important and well-known techniques to characterize the dynamics of the chaotic systems. One of the concerns when training with time-varying signals the dynamics becomes essentially time-varying and the results as stated do not hold. Beside long calculations in the appendix, that are mostly straightforward I am not convinced that the approach will be numerically efficient. Lyapunov exponents are known to lead to primarily numerical computations that are very expensive. They also highly depend on initial conditions, and I failed to find any analysis related to this. In addition, the RNN dynamics are mostly bounded stable and show chaotic behaviors only in bounded regions and can be divergent only locally. Finally, I failed to find any comparison results with any other technique to illustrate and show the claimed effectiveness of the technique.\n",
            "summary_of_the_review": "Based on what is provided it is difficult for me to believe the claims are valid.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}