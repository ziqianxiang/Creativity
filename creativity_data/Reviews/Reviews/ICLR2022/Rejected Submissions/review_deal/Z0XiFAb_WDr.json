{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper presents the Language-complete Abstraction and Reasoning Corpus (LARC): a collection of natural language descriptions by a group of human participants who instruct each other on how to solve tasks in the Abstraction and Reasoning Corpus (ARC).\n\nOverall, the reviewers found the LARC benchmarks to be well-motivated. However, there were concerns about whether the value of the dataset to downstream tasks. Results from additional program synthesis systems (like Codex and GPT-Neo) would also make the paper stronger. I agree with these objections and am recommending rejection this time around. However, I encourage the authors to continue pursuing this line of work and resubmit after incorporating the feedback from this round."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents the language-complete ARC to study how human language can affect abstraction and reasoning capability.  It collects the natural program described by human annotators and applies program synthesis techniques to analyze the gap between communicating to humans and machines. The motivation is sound and the contribution is solid with the effort of annotating LARC and benchmarking the program synthesis approaches.",
            "main_review": "Strengths:\n\n1. The paper is well-motivated with the goal of studying the gap between communicating natural programs to humans and machines. The existing reasoning benchmark shows there still exists a large gap in reasoning and problem solving between them. It's a critical problem to be addressed.\n\n2. The LARC dataset construction is by collecting verifiable instructions from a describer and builder, which ensures the task finish rate. The dataset is valuable for the community to study how natural program boosts the abstraction and reasoning process. The linguistic analysis shows how concepts differ in computer and natural programs.\n\n3. It uses existing program synthesis techniques to study how to execute the natural programs as humans. Showing solid qualitative and quantitative findings.\n\nWeaknesses:\n\n1. The technical contribution of this paper is weak since it mainly uses previous approaches to analyze the program synthesis results. I would like to see a new learning method to tackle this challenge.\n\n2. The improvement from 16/183 to 22/183 is not trivial but still very poor. One main reason is the DSL-based program synthesis is limited by the scale and range of the DSL. The analysis of how the scale and coverage range of DSL affects the performance is required for a better understanding of how to close the gap.",
            "summary_of_the_review": "I like the idea and solid contribution of this paper. The main concern I have is the technical contributions. Please refer to the above for details. I would consider raising the score if the concern is properly addressed.\n\nWhile I am not totally familiar with all the literature, my assessment of the novelty and originality might not be accurate enough.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper extends the ARC dataset by adding human-written instructions that they term as \"natural programs\".  The extended dataset is called LARC. They draw parallels and cite differences between computer programs and natural programs. They draw some linguistic insights based on the natural programs and finally study the performance of a program synthesis model on the LARC dataset.",
            "main_review": "*Strengths*\n- The paper addresses an important problem: creating a benchmark aimed towards creating machine learning models that can help understand and reason concepts. Adding language annotations ( which is the way humans communicate) to ARC is the right direction to progress especially keeping in view the surge of large langauge models for both natural language and code. \n- The paper is well written in most parts ( especially introduction and motivation). However, there are significant improvements that can be done in terms of improving the clarity ( see comments below).\n\n*Weaknesses*\n- Even though the motivations are well set, I think just adding langauge to an existing dataset might be considered less novel. The linguistic analysis is interesting but I would have liked to see the results of more program synthesis systems especially the large models like Codex [1], GPT-Neo [2], GPT-J [3]. I feel the addition of language naturally makes the set of tasks in LARC more suitable for such language models that makes it essential to have this comparison. Using a program synthesis system that is not catered towards accepted natural langauge instructions (the system used in the paper) will not lead to significant improvements.\n- I felt that the clarity of the paper needs to be improved ( see concrete suggestions below). On a high-level including more examples will really help in understanding the ideas put forth in the paper.\n\n\n*Suggestions/ Comments*\n\n1. An example in each category of the tag will be useful. Just describing the results with these tags used directly in the sentences seemed a bit abrupt to me.\n2. More details should be provided about the exact program synthesis model employed.\n3. I felt the bandit algorithm for data collection was interesting and if more details about it are placed in the main paper, it will definitely add much value.\n4. More description of how psedo-annotations are produced along with examples.\n5.  More description of how exactly the distant supervision algorithm works.\n6. I found the brief section on suggestions quite interesting. Personally, I would like if the authors could expand this section with more observations drawn from other program synthesis systems and evaluation settings.\n7. Some sample cases showing in which cases the program synthesis system succeeds and why will be extremely useful. In fact, I will say it is needed to understand the role and contributions brought in by adding language. It might be possible that the DSL itself is not defined properly or the language annotations or pseudo-annotations are not good enough to accomplish the set of reasoning required to solve the particular task. Basically, more analysis of why the systems fail so terribly through sample cases needs to be added.\n\n[1] https://openai.com/blog/openai-codex/\n[2] https://github.com/hendrycks/apps\n[3] https://arxiv.org/abs/2009.03393",
            "summary_of_the_review": "The paper studies an important problem and the motivation is well laid. However, I feel the paper needs some work in terms of improving clarity as well as more extensive evaluation of large language models. They also need failure case analysis elucidating some sample cases.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Authors construct a dataset of natural language programs for solving ARC tasks by using a population of Mechanical Turk workers to validate that a turker can correctly communicate how to solve an ARC task to another turk purely via the natural language used. The collected dataset is then analysed by the author and evidence is gathered for the richness of the programming primitives implied by natural language, the use of refining language, and the diversity of terms. The authors use this analysis to motivate the need for an improved DSL for solving ARC.",
            "main_review": "### Strengths:\n- The authors are attempting to tackle an important program in artificial intelligence: how to tackle open-ended puzzle learning, communication, and resolution. Past approaches have selected fixed goal posts via DSLs, implying there exists a known solution to the problem, while here the authors build off of ARC which has no DSL, and thus could prove to be unreachable via any standard DSL. Fortunately the authors show that in 88% of the cases human descriptions of a solution can correctly communicate a solution to another human. This single observation is one of the key achievements of the paper, and is interesting enough in that it constructs a new artifact that machines could try to approximate instead of tackling ARC directly.\n- The author methodology for obtaining the samples is cost efficient, and provides good coverage over the dataset.\n- The proposed approach is original and could prove valuable to researches in AI communication, planning, knowledge representation, linguistics, and meta or multimodal learning.\n\n### Weaknesses:\n- While the contribution of a natural language caption over the ARC dataset is compelling, is it not yet clear whether this data can prove useful for others downstream. The inclusion of a seq2seq model accuracy at predicting these descriptions, or some other form of evaluation would help highlight whether these annotations are within reach of current approaches, or too diverse or unpredictable to be effectively captured by present conditional language models.\n- Existence proof of the usability of a description certainly proves that some humans can save a task, but does not demonstrate the robustness of a description to all human receivers — in this sense it might be useful to either describe this limitation more extensively, provide an example, or a counter-example of this shortcoming.\n",
            "summary_of_the_review": "Paper presents a useful dataset complementary to the ARC effort for understanding the limits of DSLs and capabilities of natural language for expressing programs. The dataset appears to be adequately constructed, however a lack of downstream applications and proof of utility make the effort harder to contextualise, and the dataset can be seen as incremental over the original ARC effort.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}