{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper focuses on zero-shot capability of BERT-like models. The key contribution boils down to a novel prompting techniques that effectively ensembles predictions made for [MASK] tokens inserted at different places.\n\nReviewers B5Rv and 9k3X voted for rejecting mostly on the grounds that the contribution is not significant enough for ICLR. In particular, there are already existing works show that null prompting works, and other works that suggest that using multiple prompts works. While these insights have not been combined before, it is to some extend incrementally.\n\nOn the positive side, the multi-null prompting strategy is a genuinely useful tool. I think it is likely to find applications in different NLP applications as an effective way to generate ensemble predictions. The paper has also many carefully carried out experiments that will likely help guide future efforts in designing effective prompting strategies.\n\nOn the whole, I am recommending rejection. I know this is a disappointing result. Thank you for your submission, and I hope the reviews will help improve your paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "### What is the Problem / Question?\nHow effective are pre-trained language models (PLMs) in zero-shot settings across diverse prompting strategies and tasks?\n\n### Why is it impactful?\nRecently, there have been a battery of studies examining the extent to which PLMs can serve as few- or zero-shot learners, finding that they can perform surprisingly well given minimal to no fine-tuning data simply by framing problems in LM settings via cloze-filling or prompting strategies. Gaining a better understanding of the utility of LMs in this setting, and how to best exploit their abilities, could therefore be very impactful, provided the insight gained offers sufficient inductive value.\n\n### Why is it hard? Why have previous approaches failed?\nPrompting is a relatively recent development in the study of PLMs, and while there have been studies examining various aspects of prompting (e.g., https://arxiv.org/pdf/2105.11447.pdf), comprehensive studies of the effect are limited given its novelty.\n\n### How do the authors solve / answer it?\nThe authors profile 3 different BERT models over 20 different datasets using several prompting strategies. They perform several targeted analyses to better probe some of their findings (such as delving more deeply into the multi-null strategy) and also offer commentary on several overall takeaways. \n\n### How do the authors validate their solution / What is the answer to the question?\nUltimately the authors find several properties:\n  1) Using the RoBERTa model as a base, across datasets the authors find that prior reported results for the power of prompt based zero-shot systems are very sensitive to the choice of prompts used, and that using other prompts may yield dramatically worse performance. This is consistent with prior literature, such as https://arxiv.org/pdf/2105.11447.pdf, which is not cited in this paper but likely should be.\n  2) Using a variety of somewhat odd strategies in which prompts are not used but instead the representation fed to the MLM head is obtained through other means (either the CLS token, the SEP token, or mean pooling), performance drops dramatically vs. any prompting methods. This is not at all surprising, as those representations are never fed to the MLM head during PT, so this usage method is effectively untrained.\n  3) Using a simple, unrefined prompt method of \"[X] [MASK]\", with the answer inferred from the [MASK] token is surprisingly effective, often matching the unspecialized manual prompts. However, as the authors note, this strategy overall is not a new strategy; [Logan et al.](https://arxiv.org/abs/2106.13353) explore it previously, so the impact of this contribution is limited. That said, in what I think is a new finding, it can be further improved by using multiple mask tokens at different prositions in the sentence and ensembling them together. This is a nice finding, as it simplifies the process of defining a prompt for new tasks for future researchers, though the fact that methods like KPT still take first undercuts its value somewhat.\n  4) The authors further profile other methods which use unlabeled data or external knowledge to refine their learners, and find that these robustly outperform all simple prompt-based strategies. The authors further adapt some of the ideas from these other methods to their Null-prompt method, finding that using additional label words in their prompt resolution step improves performance significantly. Though such strategies have been previously explored (e.g., in the knowledge-based methods they cite), I'd argue this is their most significant finding; that by taking a method like Null-prompt and expanding the labelspace via synonyms, they can significantly boost performance. However, this value is somewhat undercut by the fact that the existing knowledge based methods still significantly outperform null-prompt based settings. It would therefore be very interesting to try to port this discovery into other prompting strategies -- e.g., do manual prompts improve if multiple [MASK]s are included? What about the KPT method?\n  5) The authors further analyze how different variations on the multi-null method offer benefits, finding that placing up to 3 [MASK]s at both the start and end of the sentence and mean-pooling the results improves performance by up to ~5%. Inserting the [MASK]s randomly into the text is also explored, though not unsurprisingly this greatly reduces performance from the traditional start or end of sentence methods. The authors analyze a few other niche cases here, such as whether other words beyond [MASK] can be used and how to aggregate the [MASK] tokens, but in general I find these analyses to be less compelling as the questions answered seem less impactful to me; it is unsurprising that using random tokens doesn't hurt performance too dramatically given the setup of the MLM task (a fact which the authors note), and I would only normally conider mean-pooling or majority vote to be natural ensemble methods in this context, and they both generally perform comparably.\n  6) The authors also analyze prompting strategies on a battery of other kinds of natural language understanding tasks, finding that models do much worse on these than on the simpler text-classification tasks they've already explored. The authors argue this is due to the difficulty of identifying high-quality label words, but I suspect a simpler explanation is more likely; that one is much less likely to find pre-training MLM examples in the PT corpora for these PLMs that resemble GLUE tasks than one is that resemble simple text classification tasks. The success of more recent approaches like T0pp also seem to contradict their intuition here, though of course T0pp had much more directed training to solve these sorts of tasks. Ultimately, as the authors do not find many instances of success on these more involved tasks, and there analysis of the reason behind the poor performance is limited to simple qualitative arguments, I'm not sure of the impact of this finding.\n",
            "main_review": "### Key Strengths\n  1. I think prompting is a very exciting and promising area, and obtaining new transferrable insights into how it works would be very valuable.\n  2. I think there are some interesting preliminary findings here, most notably (a) confirming that expanding the labelset words improves performance and (b) in finding that expanding the number of [MASK] tokens improves performance.\n\n### Key Weaknesses\n  1. Unfortunately, this field is fast moving and there are both other works that need to be cited (e.g., https://arxiv.org/pdf/2105.11447.pdf) and other works that have since come out that undercut the value of this work (e.g., https://arxiv.org/abs/2110.08207). I don't expect you to have profiled the latter -- I know it came out after the ICLR submission deadline. Nonetheless, its existence does undercut the impact of your work.\n  2. Even ignoring T0pp, I'm not sure how much there is that I can take away from this work that will materially impact future research. Of the 6 major findings I identified in my summary above, most are either completely expected, undercut by other existing models which further improve performance, or already known. The direction that seems like it has the most potential of improving future research is the finding on multiple [MASK] tokens and the more general idea of ensembling different prompts / label-words together. I think a more focused analysis on that, and how it impacts other various prompting strategies, could have a higher potential of providing transferrable insight, but as it stands now I question the significance of this work.\n  3. I think there are a few things you assess that seem not well-motivated, and those just clutter your work. In particular, the [CLS], [SEP], and mean-pooling rows in Table 1 and associated commentary on those methods doesn't seem to offer value here. I also question some of the ensemble strategies used for the [MASK] study, but as comparing mean-pooling and majority vote does seem well motivated, I'm not as confident you'd need to remove the other methods studied there.\n\n### Minor issues\n  1. I think the work would benefit from more clarity (perhaps some examples) of how each of the tested prompt strategies work (including previously published works)\n  2. I'd love to see the knowledge systems also tested on GLUE or other NLU tasks.\n",
            "summary_of_the_review": "Unfortunately, as it stands while I think the work is well exectued, I question the impact it is likely to have on future research. Therefore, I'm not sure it is suitable for ICLR, and might recommend another venue, such as a workshop submission until further research is done that focuses on isolating and expanding these studies in areas that would offer transferrable insight for future research.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper has a full span of empirical results on BERT models trained by both methods of fine tuning and prompt. The study was on zero shot learning task, and the authors proposed two new methods -- multiple null prompts and the search for label words. The fine tuning was worse than prompt BERT models for zero shot setup. The new approaches from the paper have large F1 gains compared with single null prompt. The author extensively conducted empirical study on many setup, e.g. the number and combination of the null prompt, zero shot and few shot, the classification and GLUE task.",
            "main_review": "Strength\n- Proposed new method of search for label words, the multiple null prompts\n- Abundant study\n- Proposed multiple null prompt demonstrates very big enhancement\n\nMaybe weakness\n- The variants of the [mask] token for Section 4.2 seem to have only limited values to the work.\n- The multiple null prompt may seem sort of simple technique and causes the concern whether the paper seems to meet the standard of novelty.\n- For the subword token as from Schick & Sch¨utze (2021a), can we combine the multiple null prompt from that paper w/ the multiple null prompt from current work? Please let me know -- suppose the authors know such results.\n",
            "summary_of_the_review": "The result would clearly worth publishing to researchers working on prompt BERT model. The paper may be around the borderline for acceptance of the conference unfortunately, from the novelty concern that has been talked before.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the zero-shot capabilities of the BERT family of models. The paper seeks to establish that simple variants of NULL-prompting can provide a competitive alternative to few-shot fine-tuning of BERT family models for text-classification. The paper analyzes these variants of NULL-prompting to establish principles behind design choices. The paper finally establishes limitations of NULL-prompting for zero-shot NLU tasks.",
            "main_review": "Strengths\n1. Extensive exploration of the different design choices that went into creating Multi-NULL\\*. \n2. Exploration of the limitations of the zero-shot capabilities of BERT family models. It is good to see intuition confirmed about zero-shot NLU capabilities of BERT models\n3. Paper is well written with the inclusion of substantive background information\n\nMajor Issues\n1. The breadth of the contributions of the paper are questionable if you account for the fact that zero-shot has been explored as a baseline in [2] (Line 1, 2 of Table 1)  and the efficacy of NULL prompting has been pointed out in [3]. \n2. Missing important comparisons\n\n      2.1 NULL prompting based methods benefit greatly from using the expanded vocabulary set. But the other methods being compared to are not evaluated with this expanded vocabulary set. I suspect that the other methods would benefit from this too. This is an important missing baseline because it would affect the paper’s conclusions (about methods requiring no human input and auxiliary data) if any of the representation aggregation approaches with the expanded vocabulary set outperform \\*-NULL\\* approaches\n\n      2.2 Multi-NULL seems to benefit from having multiple [MASK] tokens. Manual prompts with multiple mask tokens should be explored. This baseline, if it performs as well as Multi-NULL would be a more principled approach.\n\nMinor Issues\n 1. Some important details are missing. Please see the questions below\n\nQuestions for Authors\n 1. How were the tokens in Table 3 that replaced [MASK] chosen ? It is curious to see that all the mask words chosen lead to better-than-random performance, are there any words that this fails on - ie - worse than random ?\n 2. Table 1 - why does \\*-NULL\\* methods have error bars but \\*-NULL method don’t ? Is the correlated label set dynamic ? If the correlated label set is dynamic, how is the set dynamically chosen ?\n 3. Which tasks are you evaluating against in Figures 1 and 2 ?  Would be easier to contextualize the results knowing the task. If this is an individual task, do the trends hold across other tasks ?\n\n\nRelevant Related Work\n1. Zero-shot text classification with generative language models [https://arxiv.org/pdf/1912.10165.pdf]\n2. Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference [https://arxiv.org/pdf/2001.07676.pdf]\n3. Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models [https://arxiv.org/pdf/2106.13353.pdf]\n\n\n",
            "summary_of_the_review": "Whilst the paper performs extensive experimentation to establish the limits of zero-shot capabilities of BERT family models, the comparisons made are questionable and some experimental details are missing. The novelty/significance of the paper’s premise is questionable when situated in the context of past work.\nI am open to updating my score if my concerns about the missing baselines and experimental details are addressed.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper conducts a theoretical study of zero-shot learning in BERT-style pre-trained LMs over 20 datasets. They test different methods of prompting as well as repurposing the MLM head classifier. They propose a different prompting method, Multi Null Prompt, which is a simple method where multiple MASK tokens are inserted at random locations in the prompt and the outputs with respect to the multiple MASK tokens are aggregated. Multi Null Prompt seems to perform better than the other prompting methods. They also propose to incorporate more similar words to the label words (cosine similarity to the label words used for zero shot learning), which also helps. They do some experimental analysis of MASK tokens in Multi Null Prompt, including their position, whether other tokens can be used instead of MASK, different ways of aggregating the outputs from the multiple MASK tokens.  The paper also tries GLUE tasks and shows that zero shot performance on these tasks is comparatively worse.",
            "main_review": "Strengths\n- The paper conducts a thorough empirical study with many datasets and many methods. The results are pretty intuitive - the prompting methods are better than \"tuning -based\" methods, engineered manual prompts are better than non-engineered. \n- The proposed Multi Null prompting and label word search seem to improve performance steadily and the approach is simple.\n- After proposing Multi Null prompt, the paper empirically analyzes the components of the method, showing why certain decisions were made.\n- Improving prompting methods is an important question, although it's unclear why exactly we want the zero shot setting if few shot gives substantial benefits and its not very hard to get a few examples.\n\n\nWeaknesses\n- It's unclear why / with what insight from the experiments the new strategy of Multi Null Prompt was proposed - there isn't a clear issue that comes out of the general empirical study -- Multi Null Prompt just seems to appear without much motivation. More motivation / delineation of a concrete problem that comes from observations in the empirical study could make the logic of the paper more clear.\n- In Table 1, it seems that only some of the results have error bars?\n- In Table 3, it's unclear how the alternative words for MASK were chosen. Are they random words? It's not quite clear what the purpose of this experiment is, though it is an interesting exploration. The word to use for Multi Null Prompt doesn't seem like an obvious hyperparameter to ablate. Overall, the results show that other words may be worse or similar to using MASK, so we may as well use MASK.\n- One would hope that in Fig 2, adding more similar words helps more, but this isn't really the case. Perhaps it would help to show how much it improves initially from zero additional words.\n- As mentioned in a footnote, Multi Null Prompt cannot be used with some types of tasks that need multi MASK positions. Is there a proposal for how to deal with this limitation? \n\nQuestions\n- In the intro, why is zero shot referred to as a more general setting than few shot? It seems the other way around.\n- In Fig 1, interestingly after about 3 MASK tokens, the performance tends to go down for Multi Null prompt. Any intuition for this? \n- In Table 4, why does ALBERT have such a different behavior on IMDB? \n- In Fig 2, what does number 1, number 2 number 3 mean? ",
            "summary_of_the_review": "The paper provides an empirical study of BERT models for zero shot learning with interesting results, including two simple proposed methods that improve performance. The motivation for the multi null prompt method isn't very clear, and some of the experiments were not completely motivated, but overall it seems like a good empirical study.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}