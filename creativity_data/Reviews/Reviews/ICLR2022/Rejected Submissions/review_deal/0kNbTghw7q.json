{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "To improve the generative adversarial nets, the paper proposes to add an implicit transformation of the Gaussian latent variables before the top-down generator. To further obtain better generations with respect to quality and diversity, this paper introduces targeted latent transforms into a bi-level optimization of GAN.  Experiments are conducted to verify the effectiveness of the proposed method. The paper is highly motivated and well-written, but the experiment part still needs to be strengthened because the goal of the paper is to improve the GAN training, comprehensive and thorough evaluation of the proposed method is necessary. \n\nAfter the first round of review, in addition to the clarification issue and missing reference issue, two reviewers point out that the method is only tested in small-scale datasets, and suggest authors evaluate the performance of the proposed method in more complex datasets. Two reviewers point out that the experimental validation and comparison to prior approaches are insufficient. During the rebuttal, the authors provide extra experiment results to partially address some issues.  However, most of the major concerns from other reviewers, such as (i) how are the performance of the method in large scale datasets that have complex latent space manifolds, (ii) non-convincing performance gain, and unclear problem setup, still remain. After an internal discussion, AC agrees with all reviewers that the current paper is not ready for publication, thus recommending rejecting the paper. AC urges the authors to improve their paper by taking into account all the suggestions provided by the reviewers, and then resubmit it to the next venue."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper applies two existing techniques derived from adversarial example literature and MSGAN to improve the quality and diversity of generated samples by GAN. The former idea from [Goodellow et al., 2014b] is used to shift a latent vector z originally sampled from the Gaussian, which is different from the original adversarial attack paper that transforms the input image. The direction to move the latent vector is calculated by using I-FGSM with a standard loss for the generator proposed in the vanilla GAN paper. The experiments show that this technique can improve the quality of generated samples. Furthermore, this paper also consider how to improve the diversity of generated samples by transforming the latent vectors before putting them into the generator network. The approach for that is based on MSGAN's idea that mines hard latent vectors for the discriminator. The authors combine these two techniques and achieved better results compared to DCGAN, WGAN, WGAN-GP, and SNGAN models on several relatively small scale datasets such as CIFAR-10, STL-10, etc.",
            "main_review": "This paper introduces their motivation convincingly with some easily comprehensible figures. However, the experiments to show the effectiveness of the proposed method are basically performed only with small scale datasets, so that it makes difficult to figure out how complex manifold in the target space can be handled by this approach, in particular, it is unclear how it can scale to the ImageNet dataset. The discussion to compare the proposed method with other work seems to be not enough because there are some more research that shed light on the importance of latent vector transformation. For example, the series of StyleGAN work is continuously improving the performance by precisely analyzing the relationship between the latent space and the pixel space. They also firstly adopted to transform the latent vector z to another latent space W using a mapping network. That should be discussed as one of the possible approach to transform the latent vector, i.e., using a trainable function instead of I-FGSM. \n\nSome figures to show the effectiveness of the proposed method seems difficult to interpret as the captions claim, e.g., Figure 5. It was difficult to find the clear difference between the results of \"Anti-diverse\" and \"Original\", and the \"Diverse\" results look worse than other two in terms of the quality (if it is intended because the figure tries to show the increase of diversity, but it still unclear that the \"Diverse\" row in the Figure 5 has higher diversity compared to other two.)",
            "summary_of_the_review": "The paper is well organized and easy to follow the authors' claims but the qualitative comparison results seems difficult to be interpret as the captions state. And from the perspective of transformation of the latent vector z, there can be more discussion in the comparison with other methods such as StyleGAN that transform z before putting it into the generator.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper looks at the problem of improving the generative quality of GANs. The paper makes improvement along two dimensions: a) The paper adjust the samples distribution of GANs using adversarial attacks (I-FGSM), and thus effectively samples from a potentially multi-modal distribution. b) The paper improves the diversity of generated samples using using adversarial attacks on the mode-seeking objective of Mao et al 2019.",
            "main_review": " + The paper is generally well written and quite easy to understand. It does however mix preliminary work (I-FGSM) with the proposed contribution in 3.2.\n + The paper is able to improve vanilla GAN models using the presented objectives.\n - The paper does fully compare to prior work. Yes Table 8 highlights some of the similarities to prior work, but the main evaluation does not compare to alternative approaches that consider the interplay between GANs and adversarial attacks or latent space exploration. In order to see the efficacy of the presented method, the paper should experimentally compare to the majority of methods highlighted in Appendix C. The paper would be a lot stronger if it could show that the design choices made here are better than other attacks (i.e. perturbations on the generated or real images, instead of latent features, e.t.c.)\n- The visual quality of the presented examples is somewhat underwhelming. Does the presented method work on larger GAN architectures such as StyleGAN2 or BigGAN? Does the presented method actually address the issues highlighted in Fig 3?",
            "summary_of_the_review": "The paper explores an interesting idea of adding adversarial robustness into GAN training to improve latent distribution sampling and diversification. Unfortunately, the paper falls a bit short in the experimental validation of the approach, and comparison to prior approaches.\n\n---\n\nPost rebuttal. The rebuttal makes a good case for their final algorithm using additional results. However, I still do not see what the paper adds on top of baselines, or how the problem setup in Figure 3 (interpolation artifacts) is actually addressed. The rebuttal mentions some experimental evidence that seems to indicate latent-space sampling can helps. However, I would need to see these results in an actual paper submission for review to feel comfortable about accepting it. As is the paper seems interesting, but not ready for publication.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work proposed a sample shifting method in GAN, formulated as adding intermediate latent space to generated pixel space. Such method is based on observation of continuous mapping limit: image quality in pixel space is not as continuous as latent space; limited latent space will incur mode collapse thus poor image diversity. The main contributions are: a new optimization problem as sampling method to improve image generation by quality and diversity, propose to use I-FGSM optimization method to achieve this sampling optimization problem. The experiment showed improvement on public dataset of STL-10, CIFAR-10.",
            "main_review": "Pros:\n1. The paper is well organized, with clear sub-titles and clear logic flow;\n2. Ablation study on various baseline GAN architecture (DCGAN, WGAN, WGAN-GP, SNGAN) is conducted to show generalizability of such sampling method.\n\nCons:\n1. Compare with baseline method MSGAN: (1) why it only compare the div+ with MSGAN; (2) improvement over baseline method MSGAN is very limited.\n2. If generator trained with better quality regularization, the latent space after mapping should have better continuity? Comparison like Fig 3 after training would be needed to prove that.\n\nSome minor issues:\n1. The paper is a bit redundant on algorithms and figures. For example,  it seems lengthy to include both algorithm 1 and 2 in the main paper\n2. Fig 1. It’s not obvious what the latent space did in this figure\n",
            "summary_of_the_review": "The paper is overall well written, and the idea is very clear and elegant. Meanwhile, I have some doubt on the sufficiency of experiment to show the improvement from quality. Also the improvement over baseline method MSGAN seems minor for me. Expect the rebuttal to clear my doubt.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors demonstrate that the generator in a GAN is a continuous function two latent codes that are close in the latent space are mapped to two images that are close in the pixel space. However, the quality of the generated images is not preserved as quality is not a continuous function in pixel space. To address this issue, the authors propose to transform the original latent codes and demonstrate that it results in better generation quality and diversity.",
            "main_review": "1.    Related works have not been cited. For example, the following paper performs a similar optimization technique with a different objective.\n\na.       Yan Wu, Jeff Donahue, David Balduzzi, Karen Simonyan, Timothy Lillicrap. LOGAN: Latent Optimisation for Generative Adversarial Networks. (https://arxiv.org/pdf/1912.00953.pdf)\n\nb. https://arxiv.org/abs/2005.02435\n\nc. https://arxiv.org/abs/1809.03627\n\nPlease do a thorough literature review to incorporate any such missing work.\n\n2.       The authors propose five methods viz (i) AdvLatGAN-z, (ii) AdvLatGAN-qua, (iii) AdvLatGAN-qua+, (iv) AdvLatGAN-div, and (v) AdvLatGAN-div+. \n\nEach method either focuses on improving quality or aims to enhance diversity. \n\nIt would be interesting to see what happens when the proposed objectives in equation (5) and equation (8) are combined. Does the hybrid method outperform both AdvLatGAN-qua+ and AdvLatGAN-div+?\n\n3.   Another limitation of the work is that they compute FID and JSD. FID, although a widely used metric, is not able to quantify the quality and diversity as it is a unidimensional score. Therefore, it would be nice to quantitatively verify the claims of enhancement in quality and diversity in AdvLatGAN-qua+ and AdvLatGAN-div+ respectively. Comparing other metrics such as precision/recall or density/coverage will be more meaningful towards such goals.\n\na.  Density/Coverage: Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo. Reliable fidelity and diversity metrics for generative models. In International Conference on Machine Learning, 2020. (https://arxiv.org/abs/2002.09797)\n\n\nb.   Precision/Recall: Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, Sylvain Gelly. Assessing Generative Models via Precision and Recall. In NIPS, 2018. (https://arxiv.org/abs/1806.00035)\n\n4. The number of baselines used in the experiment section is too less for the CIFAR-10 case?\n\n5. It would be good have experiments on large-scale datasets such as FFHQ.\n\n6. I wonder if FGSM is the only method this can be applied with or any other can be used as well? If so, how does the method depend on the adversarial training method employed?\n\n7.  I am not sure why MSGAN was chosen as the baseline for regularization?\n\n8. There is no theoretical justification on why should the proposed method work? There is some empirical evidence but there would be better to have some theoretical backing on why this method should aid in avoiding mode-collapse.\n",
            "summary_of_the_review": "Please refer to the above comments. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}