{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "Thank you for your submission to ICLR.  The paper proposes a simple method for improving calibration performance using a loss based upon a Dirichlet KDE.  The method is appealing in its simplicity, but several reviewers (and myself) have concerns simply about the fact that the method ultimately seemed to give rather marginal improvement over the standard cross-entropy baseline.  The authors attempted to address this point in the rebuttal, with their additional example on the Kather domain.  And while this is a nice addition, I'm still not fully convinced that the improvement here is _that_ significant, to the point where I think it would be important to consider much broader sweeps of hyperparameters, etc, for all methods (which I believe should be reasonable here given the data set sizes).  I believe this has the potential to be a nice contribution, and its simplicity can be a positive, but ultimately I think a bit of additional effort is required to show the full empirical advantages of the method."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a new approach for calibrating neural network outputs. The idea is to train the neural network with a regularized loss function that is a linear combination of prediction and calibration errors. The calibration error is measured as the Lp norm of the difference between predicted class probabilities and the expected true class probabilities given the predicted class probabilities where the latter term is computed using a kernel density estimate with a Dirichlet Kernel. The approach is evaluated in terms of accuracy and expected calibration error on CIFAR-10 and CIFAR-100.",
            "main_review": "Strengths:\n\n1. A novel and technically sound approach for regularizing neural networks in terms of the canonical calibration error is proposed with competitive empirical performance.\n\n2. The use of the Dirchlet Kernel Density Estimate to measure the calibration error is an interesting side benefit of the approach.\n\nWeaknesses:\n\n1. The technical novelty is a bit limited since it mainly consists of directly plugging in the expression for the Dirichlet/Beta Kernels into the regularized loss functions. The consistency results are a good starting point but without a discussion on convergence rates they are not very useful in practice.\n\n2. The  approach seems a bit computationally expensive especially since the simple CRE approach has competitive marginal L1 ECE performance (both pre and post T) and is much less expensive. I would suggest either exploring a more computationally efficient version of the approach (For eg. using approximations to speed up the kernel computations) or including an application (in experiments) where the canonical L1 ECE is more important (such as the medical diagnosis application mentioned in the introduction) so as to clearly illustrate a setting where the proposed approach would be preferrable to CRE. Also given the difference in computation cost I would like to see inference time measurements to exactly illustrate the overhead w.r.t the CRE baseline.\n\nAdditional questions:\n\n1. Is it possible to consider f-divergences instead of the Lp error in CE under your approach? If yes, have you already looked into it?\n\n2. I think there is some issue with the subscripts in (19). If $z_i$ is a softmax output (vector) then shouldn't the individula $\\alpha$'s be labeled as $alpha_{ij}$?\n\n3. Why are values of $\\lambda$ different fro MMCE and KDE-CRE? Does the $\\lambda$ in MMCE correspond to the value used in the original paper?",
            "summary_of_the_review": "I am leaning towards accepting the paper because the approach is technically sound and the results are competitive. However I would like to see more of a discussion on the comparison of the proposed approach with the less expensive CRE baseline (see my recommendations above) to be completely convinced.\n\nComments after rebuttal: I am satisfied by the additions made to the paper. The experiments on the Kather (medical) dataset do illustrate the superior L1 ECE (canonical) of the proposed approach, and the time measurements show that the overhead introduced by the proposed approach is not significant. I am therefore increasing my score to reflect the same.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a regularization term to augment loss functions, where the regularization term effectively minimizes the calibration error of the model. The term itself is a kernel density estimator over the K-simplex space (hence Dirichlet kernel is the natural choice).\n\nThe authors claim the estimator is consistent (but not unbiased, though they partially debias it) and empirically verify that their method yields tradeoff between accuracy and calibration that is near Pareto optimal.",
            "main_review": "Strengths:\n-  Idea is conceptually simple\n- The work focuses on the stronger notion of calibration (canonical)\n\nWeaknesses:\n- The method claims to be well suited for differentiable training. Yet, these are mostly deep learning scenarios with large datasets. The scalability of the O(n^2) estimator can be a challenge. The paper does not provide any data on training times. A breakdown of \"training without CE term\" vs \"training with CE term\" would be very helpful in assessing the utility of this method, especially in evaluating whether the improvement in calibration error relative to other methods is worth the additional computational budget.",
            "summary_of_the_review": "Justification of the recommendation is based on the strengths and weaknesses stated above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper the authors address the problem of calibration in classification tasks, proposing to use kernel density estimation methods to estimate calibration, and then add a penalty during model training to regularize the model toward having better calibration.  They apply this technique to CIFAR-10 and CIFAR-100 and consider several measures of calibration.   ",
            "main_review": "I found parts of this paper, particularly the exposition to be quite clear.  I also find the proposed method to be appealing in its simplicity and how easy it would be to apply to generic classification tasks (e.g., regardless of architecture).  I do find the results to be a bit lackluster (improvements seem marginal compared to existing methods) and some of the results are a bit cursory (e.g., the authors mention the application to any method that uses stochastic gradient descent, but the penalty requires looking at a large number of data points to estimate the density, and there is no guidance on how to set batch size or whether to use the whole dataset.  Does using smaller batches incur more bias? Is the bias large enough to be a concern for reasonable sample sizes?  The authors leave this for future work.) Otherwise, I have a number of mostly minor comments below.\n\nMinor Comments:\n- It would have been nice to go into depth about how to choose the various hyperparameters introduced by this method.  The authors describe a method for choosing the kernel bandwidth, but do not discuss how to choose $p$ in the $MCE_p$ regularization term, or how to set $\\lambda$.  I suppose $p$ should be chosen based on which canonical calibration you want, and $\\lambda$ might be chosen using cross-validation, but it would be good to provide users some guidance.\n- Given that histogram density estimators are also consistent estimators of densities (given that the number of bins grows as sample size grows) Figures like Figure 2, Figure 3, Figure 8 and Figure 9 are statements about whether the Dirichlet KDE or the histogram is a more sensible estimator for a particular finite sample.  While it seems totally reasonable that the Dirichlet KDE should do much better (it's visually striking in Figures 2 and 8), Figures 3 and 9 are a bit difficult to interpret.  It's clear that the two estimates are highly correlated, but it's not obvious that one is better than the other.  One easy way to visualize this, would be to compare the estimates from using different numbers of points (as in Figure 9) but compare large samples to small samples: given the consistency of the KDE ECE, one could take the KDE ECE from 1000 points are being close to the ground truth, and then by plotting either the Binned ECE from 100 points or the KDE ECE using 100 points against this \"ground truth\" and see which estimator is better able to match the large sample size result with a smaller sample size.\n- From a writing standpoint, I did find the overall organization of the paper to be a bit lacking which made it hard to follow the main thread -- for instance section 5.2 is interesting, and a nice use case of the Dirichlet KDE, but then it's not mentioned or used again (e.g., in the Tables etc...) and does not really belong in the \"Results and Discussions\" section.  Section 5.2 also serves to separate the details of the empirical exploration, from the results, making the paper feel disjointed.  Lastly, the \"Mean squared error in binary classification\" section is nice, but given that some \"tricks\" are used to reformulate the optimization problem, it feels a bit disconnected from the framework presented at the beginning of Section 3. \n\nTypos:\n- I believe that in the top paragraph on p. 6, $f(x) \\in [0, 0.2)$ should be $f(y | x) \\in [0, 0.2)$ of perhaps $f( \\cdot | x) \\in [0, 0.2)$.\n- The abbreviation \"ECE\" is used in the last paragraph on p. 2, but is not defined until the section \"Metrics\" on p. 6.",
            "summary_of_the_review": "The paper presents a conceptually simple approach to improving the calibration of classification models.  The paper is interesting, but could use some restructuring.  It is not totally clear how to use the method in practice (especially on large datasets where $O(N^2)$ would be prohibitive) and it is not clear if the proposed methodology substantially improves calibration over existing approaches.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors proposed  the Beta/Dirichlet kernel density estimator, which is differentiable (trainable) and asymptotically converges to the true Lp calibration error. Empirical results demonstrated that such estimator is beneficial for both training and evaluation tasks. ",
            "main_review": "This is an easy-to-follow paper and the proposed kernel estimator is simple to implement. The main idea is not very technically novel, but the introduction of beta/dirichlet kernels for classification is interesting, since they are differentiable and fit into the probability simplex.\n\nMain concern:\nThe idea of differentiable ECE is not entirely new. Although histogram ECE are non-differentiable, some work have proposed remedy for this issue [1], and the kernel density ECE proposed in [2] also requires trivial change to be differentiable (has been explored by [3]). Considering this, I recommend the authors to discuss and compare with those approaches in this work.\n\nAs for the experimental results, I'm also not convinced that the proposed KDE-regularized training significantly outperformed the cross-entropy baseline (Table 1-2), as it seems that on many NN architectures the CE one obtained comparable or even better ECE. A major contribution of this paper is the introduction of Dirichlet kernels, but the authors missed an opportunity to demonstrate the benefit of using Dirichlet kernels over existing kernel density ECE method [2,3] or advanced histogram estimators (such as [4]) in the experimental section, such as their better capability to handle the multi-class cases, other than showing that they are \"closely correspond to\" the histogram ones. I believe this can be improved.  \n\n1. Bohdal O, Yang Y, Hospedales T. Meta-Calibration: Meta-Learning of Model Calibration Using Differentiable Expected Calibration Error[J]. arXiv preprint arXiv:2106.09613, 2021.\n\n2. Zhang, Jize, Bhavya Kailkhura, and T. Yong-Jin Han. \"Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning.\" International Conference on Machine Learning. PMLR, 2020.\n\n3. Ma, Chunwei, Ziyun Huang, Jiayi Xian, Mingchen Gao, and Jinhui Xu. \"Improving Uncertainty Calibration of Deep Neural Networks via Truth Discovery and Geometric Optimization.\" arXiv preprint arXiv:2106.14662 (2021).\n\n4. Kumar, Ananya, Percy Liang, and Tengyu Ma. \"Verified uncertainty calibration.\" arXiv preprint arXiv:1909.10155 (2019).",
            "summary_of_the_review": "Overall, I think this is an interesting paper, but not ready to publish in ICLR in its present form. I hope that the authors can improve the empirical aspects for this paper. ",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}