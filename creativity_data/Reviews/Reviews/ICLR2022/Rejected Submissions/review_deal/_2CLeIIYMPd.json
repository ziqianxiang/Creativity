{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper introduces a technique for randomised dynamic programming and uses it to scale a latent variable model that enables interpreting the hidden states of large pre-trained models for text representation and generation.\n\nThe current version needs to be improved with regards to scope, which can be seen by the various confusions that it triggered, and which the authors tried to address in the rebuttal phase. It is somewhat unclear to all of us (myself included) whether the paper is about i) randomised dynamic programming (RDP), or ii) RDP's role in a particular LVM (with a CRF posterior approximation), or iii) RDP+LVM's ability to interpret deep Transformer models? Empirically, the paper is much more about (iii), somewhat about (ii, e.g. Table 1), very little about (i, e.g. Figure 2).\n\n*Because the scope is now confusing*, the current version sometimes comes across as relatively incremental or even incomplete:\n\n* Should the authors embrace interpretation. The overall strategy is *very interesting*, and it scales a neat model precisely in the way it needs to be scaled to do what it's meant to do, but this would change the focus of the paper, RDP would be all but a means to an end, and perhaps other techniques for interpretation would be needed.\n\n* Should the authors embrace RDP itself (disentangled from its application to model interpretation). Some of us felt like the randomisation technique on its own is not too surprising (given the work of [Liu et al](http://proceedings.mlr.press/v97/liu19c/liu19c.pdf), for example), and, regardless of that, to push for RDP's significance, the paper would need more comparisons. The only alternative to RDP investigated in the paper is a heuristic top-K gradient. There are deterministic gradients that are less heuristic, and which may become unbiased eventually as training progresses, see for example [[1]](https://aclanthology.org/D18-1108/) and [[2]](https://papers.nips.cc/paper/2020/hash/887caadc3642e304ede659b734f79b00-Abstract.html).\n\nIn the first round of reviews there were some comments that questioned the paper's fitness to ICLR, I would like to remark that this has been clarified, and the paper targets a problem of clear relevance to the conference.\n\nI would personally like to add a minor comment: it would be nice to acknowledge some older literature on randomised DPs (see for example [[3]](https://papers.nips.cc/paper/2009/hash/e515df0d202ae52fcebb14295743063b-Abstract.html) and [[4]](https://aclanthology.org/N10-1028/))."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Contextual embeddings of words are mapped to a sort of cluster of word embeddings with each one corresponding to one possible state in a linear CRF model. The idea is to build a network between the cluster of words (states of CRF) for analysis the manifold of language. To reduce quadratic cost of CRF, a sampling based approach is proposed to compute the partition function. \n\nEmpirical analysis is not very insightful, and the contributions are incremental.\n",
            "main_review": "To put it simply, BERT or GPT-2 like models learn continuous contextual representations of words, so sentences. In this paper, linear-chain Conditional Random Field (CRF) model is considered with a number of hidden states which is relatively small w.r.t. number of words, with each hidden state representing a set of words. Just like word indices are discrete, one can think of the indices of the states of CRF as discrete even though the hidden state vectors are high dimensional continuous vectors, hence the authors project this approach as of  discrete latent structures. Essentially, all this approach is doing is clustering words based upon their contextual embeddings, and then building a graph of transition between those states. In the experiments, the number of states is 2000 while the number of words in BERT is 32k, so there isn’t a significant reduction in the number of states if one were to consider each word as a state. There isn’t any novel contribution in this idea, nor a value of such analysis, at least from the perspective to advancing science. Even empirically, I don’t see any interesting insights from the analysis. \n\nFurther, since CRF models are trained using dynamic programming which is efficient in itself but it has quadratic cost in the number of states. It is proposed to sub-sample upon some of the sequences with low weights, while retraining the ones with high weights. This approach is just a heuristic, with many possible flaws. For instance, it is proposed to compute the sequences with high weights absolutely without recursive formulation, which should be expensive. Sampling strategy is not explained well, and I believe challenging to obtain for NLP like high dimensional problems. Besides, the approximation is applied at coarser level, not accounting for tree like structure emerging from recursion, big sequences as child nodes and subsequences as parent nodes. Intuitively, something like brand and bound could have helped if there was sampling accounting for tree structure. It is also worth noting that the proposed sampling approach is not novel but explored in related problems. Theoretical analysis on zero bias is not enough, and there isn’t sufficient analysis for ensuring low variance. ",
            "summary_of_the_review": "Probably, this is a preliminary work, and it could become a good paper with lot more research efforts put into it.\n\nAlgorithmic contribution of sampling is interesting but incremental and not throughly studied. Empirical analysis is not very insightful either. I don’t see much justification in mapping 32k contextual embeddings to 2k latent embeddings to build a so called latent representation.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns. \n",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new way to greatly reduce the memory requirements of inference on discrete latent variable models (LVMs). It does this by estimating the key inference steps in dynamic programming through enumerating a small subset of nodes and using importance sampling for the rest of the nodes. This technique is generally applicable to many dynamic programming algorithms and enables scaling LVMs to much more states by reducing memory requirements. Empirically, this work shows that the proposed approach gets better partition estimates than existing methods using top-k approximations. This work also demonstrates an application of the proposed approach on discovering meaningful latent codes and latent trajectories from pretrained language models.",
            "main_review": "Strengths:\n1. The paper is well written and does a good job connecting the method with prior work while also explaining the phenomenon that initially inspired the development of their method (the bias of top-k).\n2. The potential connection to randomized automatic differentiation is both intriguing and I think will be useful in many applications. This paper does a good job of highlighting the limitations of our current tools and the authors did a good job documenting how these appear in the implementation.\n3. The visualizations in this paper have strong execution and do a good job providing the reader with intuition regarding the structure of the discrete latent space the algorithm produces.\n\nWeaknesses:\n1. The main reason for approximating partition calculation is to scale LVMs to more states, but from experiment results, it doesn't seem that scaling LVMs to more states helps performance. For example, in table 1, the PPL of RDP-2k is only 0.19 better than RDP-100 which doesn't seem very significant, and in table 2, RDP-2k gets worse BLEU and iBLEU than RDP-50. Based on existing results in the paper, I cannot tell if it's because the approximation is bad, or because LVMs with more states do not get better performance, but either case is evidence against this work (in the former case, the proposed approximation is bad even though it's better than top-k; in the latter case, why would we want to scale LVMs then?). Another concerning result is that RDP-100 outperforms FULL-100 in table 1, and RDP-50 outperforms FULL-50 in table 2 other than self-BLEU. How could an approximation be better than exact inference?\n2. The paper only focuses on a single application of the proposed randomized dynamic programming technique. When introducing such a broad technique, it seems natural to explore a range of applications to showcase the generality. It is unknown how it will perform when applied to other dynamic programming problems found in natural language processing or in machine learning.\n3. I don't think the application of analyzing the representation space of pretrained language models makes sense: the CRF is only used to parameterize the inference network, but the generative model is trained on raw sentences (x) and does not use representations from the pretrained model. This setting contradicts the claim that \"the discovered latent network is intrinsic to the representation manifold\" (section 1, last paragraph), since the generative model is trained on x instead of representations r (Eq. 12). Even with a BERT with random weights, I think it is possible to learn meaningful clusters since the generative model is trained on x (so the posterior can still reveal meaningful structures). To me, a more natural setting would be to use an HMM to parameterize the generative model and perform exact marginalization over latent states using the proposed approach, which does not directly use raw words x but only uses representations r and would be truly \"intrinsic to the representation manifold\".\n4. The paraphrasing experiment is not very clear. Where is bag-of-words used? What is the generative process?\n5. Another issue with the paper is the lack of exploration of different sampling strategies. This is another place where I would be interested in results from a different task or domain. A stronger analysis of how the selection of K1 and K2 changes the result might also give insight into improving the method.\n\nMinor issues:\n1. It would be nice if you can compare to existing unsupervised clustering works such as Brown clustering which has a similar probabilistic formulation (https://github.com/percyliang/brown-cluster).\n2. While the algorithm is inspired by randomized automatic differentiation, a method that provides an unbiased approximation of gradients, this method uses biased gradients of an unbiased approximation of the partition function?\n3. Page 5, For experiment (2), I assume you meant (3)?\n\nQuestions for authors:\n1. \"we extract CRFs on-the-fly from different LVM training stages.\" Would the network learn log potentials to adapt to the approximate partition calculations so evaluation in this way becomes preferable to your approach?\n2. Table 1 PPL, I assume you meant exp -ELBO for your models since true PPL is intractable under the generative model?  Or did you estimate it?\n3. Fig. 2, how are the density curves drawn? Did you run your algorithm multiple times?",
            "summary_of_the_review": "This paper is well-organized, has a novel contribution and I believe it will be of interest to various subgroups in the ICLR community. However, the experiment setting does not make much sense to me, and the empirical results are not convincing. I understand that I might be asking for too much for a single paper, but I think this paper would be much stronger with better experimental settings and more thorough analyses. I'd recommend the authors to split this paper into two papers. For example, the first paper can be an in-depth analysis of the proposed randomized dynamic programming algorithm with applications in different domains and different sampling strategies, and the second paper can be about structure discovery in pretrained models with a correct setup. In its current form, I am leaning towards rejecting this paper due to the issues mentioned before.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a latent network analysis, using a proposed a Randomized Dynamic Programming (RDP) approach, to study the topology of the representation space of contextual embedding models. By constructing an unbiased estimator for the partition function with subsampled DP paths, the authors are able to reduce the memory complexity of forward-backward computation on Linear CRFs by 2 orders of magnitude. Using this RDP formulation, inference with structured latent variable models is scalable to 1000s of latent states. Latent states are modeled in a linear chain CRF with a long-tailed prior on transitions and learned using amortized variational inference similar to Fu et al. 2020, Li and Rush 2020. State-word connections in the representation space provide linguistic information that encapsulates syntactic and semantic roles and state-state connections correspond to the construction of phrases. The authors find anchoring states in the representation space that provide enough information to perform unsupervised paraphrase generation. ",
            "main_review": "Strengths:\n\n- The proposed Randomized Dynamic Programming approach provides a simple, well-founded approximation to the log partition function, as evidenced by Figure 2, especially in the case of dense distributions. \n- Empirical analysis of State-Word connections [Figure 3] show meaningful linguistic information on semantic and syntactic roles; analysis of State-State Relations [Figure 4] provides information about language compositions such as infinitives and prepositional phrases.\n- Generalizations to different graph structures are discussed in the Appendix, I believe the proposed approach could be extended to tree graphs straightforwardly\n- As expected, memory and runtime for RDP from Appendix section E.4 are low (equivalent to TopK which sets $K_1 = K, K_2=0$)\n- The proposed approach provides insights into linguistic roles and compositions without supervision unlike many preceding works which utilize supervised probing \n\nWeaknesses:\n\n- To the best of my knowledge, the RDP mechanism detailed in this paper combines two well studied approaches (i) importance sampling (i)TopK budget [Sun et al. 2019]. As a result, the methodological novelty of the sampling scheme is somewhat limited.\n- Additionally, the latent network modeling approach is quite similar to standard well-studied approaches (linear-chain CRF, long-tailed prior on proposals, amortized variational inference) and provides limited novelty.\n- As discussed in the paper, most linguistic information seems to be captured by 500 states. It would be helpful to compare experimental results for state-word and state-state relations among the Full, TopK, and RDP approaches which should all be feasible for 500 states. In particular, as seen in Figure 2, the benefits of RDP seem especially beneficial in Dense distributions. How does this translate to differences in the learned state-word and state-state connections?\n- Marginal performance benefits in terms of Test PPL on MSCOCO [Table 1] and on Paraphrase generation on MSCOCO [Table 2] when compared to TopK and Full (for small # of states). \n\n",
            "summary_of_the_review": "Interesting approach for inferring large-scale structured latent variable models in contextualized representation space. Empirical results are insightful but methodological contributions seem a bit limited in the context of prior literature.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Paper utilizes large language models, which are known to have potential biases, but the analysis is not ethically concerning.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors address the issue in the computational and memory efficiency in Dynamic Programming when it is used in the Forward Algorithm for Linear Chain CRF. The author purpose  a Sample Forward Algorithm by randomizing the summation. The authors proves that such randomization is an unbiased estimator for the true sums, and, using Amortized Variational inference for optimization, shows that it empirically reveals meaningful latent topology with much less required resource.\n",
            "main_review": "# Summarization\n\nIn this paper, the authors address the issue in the computational and memory efficiency in Dynamic Programming when it is used in the Forward Algorithm for Linear Chain CRF. The author purpose  a Sample Forward Algorithm by randomizing the summation. The authors proves that such randomization is an unbiased estimator for the true sums, and, using Amortized Variational inference for optimization, shows that it empirically reveals meaningful latent topology with much less required resource.\n\n# Strong Points\n\nGenerally this paper is an interesting one, well motivated and easy to follow. It combines the randomized summation, an idea used in [1], to the problem of Forward Algorithm, by viewing Forward Algorithm as a sum-product algorithm applied over the consecutive time steps. Such randomization is proved by the authors to be an unbiased estimator for the true sums.  The experimental section focuses on showing the meaningfulness of the learned latent space, mostly through state-world/state-state relations.\n\nNote that the author uses recently amortized variational inference [1] for optimization and LSTM for parameterization of the forward algorithm [2], which are good to have, yet not central to the main contribution.\n\nAlthough the authors frame it in Linear Chain CRF problems with pre-trained language model for contextualization, I think it could be conceptually expected to a broader spectrum of sum-product algorithms (the author briefly covered that in the Appendix), although this may be beyond the scope of a single paper.\n\n# Weak Points:\n\nThere are a few missing pieces in this work which should be better addressed.\n\nThe first is the difference between the true proposal $\\~{a}$ that is proved by the authors to lead to an unbiased estimator but cannot achieve improvements in computational efficiency, and the practically used proposal as in Eq. 10.  Although Fig 2. shows some distributions by using the practically used prospal that show no bias, it remains unclear how much it differs from the true proposal. Maybe a study of distances vs. memory presentation (e.g., $K_1$) could show a better picture of the trade-offs here.\n\nThe second issue is that, the conceptual benefit from the proposed method is being able to deal with larger $N$ (the number of latent states) where full Dynamic Programming could not run efficiently. However there is no such experiment showing what would be happen with larger $N$.\n\nFinally, the experiments visualizing the latent space are for the proposed method only. It would be interesting to see how it compares with methods that only takes top $K_1$ and the vanilla Forward Algorithm (which takes the full $N^2$ transition matrix) \n\n# Questions for the Authors\n\n  - How are interpretations of states in experiments (e.g. in Fig. 3 and 4) coming from? Are they chosen in some particular way? \n  - Maybe “the more $q_i$ correlates with $a_i$, the less variance $\\hat{S}_2$ has“ can be explained in more detail ?\n  - In Eq 11: $[s_{z_{t-1}; x_{t-1}}]$ should be $[s_{z_{t-1}; r_{t-1}}]$?\n  - In Figure 3, subfigure B1 and B2, states and words are shown in the same space. What are exactly the features for words and states ($r$? $s$? or something else?)?\n\n\n# Assessment\nI think this is an interesting paper and is above the threshold of acceptance since the presented method is somewhat novel and inspiring. I’m willing to increase my score if my concerns are addressed\n\n# Refs\n\n[1] Kool et. al. 2020: Estimating Gradients for Discrete Random Variables by Sampling without Replacement\n[2] Fu et al. 2020:  Latent Template Induction with Gumbel-CRFs \n[3] Li & Rush, 2020: Posterior Control of Blackbox Generation \n\n\n# Post-rebuttal \n\nI thanks the authors for the responses which improves the manuscript a lot. Still, I would like to keep my recommendation \"6: marginally above the acceptance threshold\", since (1) the contribution on RDP and that in linguistic application are entangled and do not provide a clear message, and (2) as issues in my reviews and other reviews remain.\n",
            "summary_of_the_review": "I think this is an interesting paper and is above the threshold of acceptance since the presented method is somewhat novel and inspiring. I’m willing to increase my score if my concerns are addressed",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}