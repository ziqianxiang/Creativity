{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper presents a locally connected spiking neural network model trained to do classification of MNIST using spike-timing-dependent plasticity (STDP) and reward-modulated STDP. The authors show that this model can learn to classify MNIST images (though not at a very high accuracy) and that it can engage in classical conditioning. The reviews were initially all in the reject range. The common theme in the reviews was concerns about the weak and limited nature of the results. After a good amount of author response and reviewer replies to the authors, one reviewer increased their score to a borderline accept, but the other reviewers did not change their scores, producing scores of 3,3, and 6. Given these scores, and the reviewers' remaining concerns, a reject decision was reached."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper discusses learning in biological networks, and proposes a model that combines spiking networks, local connections (convolutions without weight sharing) and reward-modulated spike-timing dependent plasticity. All three components are chosen to be more realistic compared to artificial neural networks. The model is tested on MNIST and on a classical conditioning task. ",
            "main_review": "### Major comments \n\n**Strengths.** The paper aims at combining multiple biologically plausible features of deep networks: spikes, local connections and RL-based error signals. This is a reasonable direction, and it’s been tried by multiple papers as shown in Tab. 3. \n\n**Weaknesses.**\n\nHowever, I struggle to see the novelty in the author’s approach: spikes and local connections alone have been tried many times (Tab.3 and also [1]). Training the output layer (rather than the whole network) with an RL-based rule is somewhat new, but I find this approach unreasonable for the following reasons:\n1. The last layer is usually trained with SGD + cross-entropy to assess the quality of representations built by previous layers. So the performance of R-STDP in any case would be limited by the representations it gets from earlier layers, which are arguably more important for training networks. (This paper tries to do that too with SVM, however.) \n2. There’s no reason for this approach to scale beyond MNIST, as the hardest part of training is done by a simple STDP rule. Maybe some layer-wise R-STDP can be a valid approach (akin to [2]), or a backprop-like RL error [3].\n\nAs a side point, I couldn’t run the code in colab (with PyTorch 1.8 and Bindsnet installed). Running `image_classification_experiment.py` gives `record() got an unexpected keyword argument 'n_labels'`. Disabling recording makes it go away, but then there’s a shape mismatch. And if you make the running time 256 instead of 256*3 to fix it, the accuracy doesn't improve at all.\n \nThe main result of the paper -- MNIST accuracy (Tab. 3) -- is very weak. It’s pretty straightforward to achieve 95%+ test accuracy with spiking networks, local connections and unsupervised pre-training (using SGD for predictions) [1] (Tab. 2 there). Therefore, even ignoring the potential weakness of the R-STDP in the final layer and concentrating on the STDP + SVM result (87.5%), it is clear that the network does not learn useful representations. There are multiple potential reasons for that:\n1. The STDP in the first layer is at fault, which would be a bit surprising given the clarity of filters in Fig.3A. As a sanity check, you can train an SVM on the hidden layer without any pre-training, and see if it improves the results. \n2. The decoding scheme is ill-fitted for SVM. I’d suggest using SGD with cross-entropy like in [1] and probably many papers in Tab. 3 of your paper. If you see a large improvement, then R-STDP needs some rethinking to properly make use of the pre-trained layer.\n3. Local connections make it harder. Some works in Tab. 2 of [1] successfully use LC layers, however. I would test performance with the same architecture, but using convolutions. Another thing I noticed is really large filters -- 15x15 filters for a 28x28 image are not too far from a fully connected layer.   \n4. When the winner-take-all in the LC layer makes a mistake by activating the wrong “digit” (and the filter weights do look like digits in Fig.3A), the readout layer can’t fix it. \n\nFinding the root of poor performance would improve the paper, but the overall approach (hidden layer STDP + last layer R-STDP) is still unlikely to scale to harder problems and deeper networks.\n\n**Recommendation**\n\nDue to limited novelty and unsatisfying results, I would recommend rejecting the paper. \n\n### Minor comments\n\n> It is noteworthy that in many learning problems, we do not have direct access to the explicit label of the data. Consequently, we may need to abandon gradient-based methods, and utilize reinforcement and reward-modulated learning rules\n\nGradient-based doesn’t mean it uses labels. See VAEs, self-supervised learning, etc. that all use backprop. \n\n> The proposed network is … the first locally-connected SNN with a hidden layer\n\nThat’s not true. See [1] and references therein.\n\nVarious problems:\n1. Eq. 3 has extra underscores. It has to be dg/dt.\n\n1. P_ij^+- in Eqs. 7-8 only need one index, j for Eq. 7 and i for Eq. 8.\n\n1. Eq. 12 is confusing. Where does the reward come from at each trial? Is one of the r_i taken from Eq. 11?\n\n1. Explaining the network model in Sec. 4.2 with equations would greatly improve clarity.\n\n[1] https://www.sciencedirect.com/science/article/pii/S0893608019301741\n\n[2] https://www.frontiersin.org/articles/10.3389/fnins.2018.00608/full\n\n[3] https://proceedings.neurips.cc/paper/2020/hash/1abb1e1ea5f481b589da52303b091cbb-Abstract.html\n",
            "summary_of_the_review": "The paper presents a somewhat novel combination of biologically plausible features of deep nets. However, the overall approach is too simple to scale to hard tasks (STDP in hiddden layers + R-STDP in the last layer), and it performs poorly even on MNIST. The paper should be rejected. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors proposed an SNN model, dubbed BioLCNet, for image classification. The proposed network consists of three layers: 1) input layer, accepting spike trains as input;  2) locally connected (LC) hidden layer; 3) decoding fully connected (FC) output layer. The LC layer is for feature extraction and is trained first using STDP in an unsupervised manner. The decoding output FC layer is trained second with R-STDP in a semi-supervised manner. The proposed SNN is validated with the MNIST dataset to show its effectiveness. ",
            "main_review": "**Strengths:**\nThe manuscript proposed a training scheme mixed between STDP and R-STDP and proposed reward functions for training the 3-layer SNN with R-STDP. I think it is unique. However, unique does not mean good or better. \n\n**Weaknesses:**\n1. The authors proposed a dedicated 3-layer SNN network for image classification. The validation is only limited to MNIST but with worse performance than other wildly used SNN approaches. From my point of view, it is too limited and it is not convincing enough in the effectiveness of the proposed method.\n2. The authors claimed, “The proposed network is the first to employ RL … for image recognition”. Even it is true, why the community should care? From my point of view, the motivation for utilizing RL for classification is not very clear, especially since the performance is worse than other commonly used SNNs. \n3. It is not clear to me why the authors mixed STDP and R-STDP for training the network? I think it would make sense to show the performance with pure STDP and pure R-STDP training. \n4. The last sentence of the first paragraph of the introduction, “Spiking neural networks also require fewer labeled data….”, does not make sense to me. \n     * Why SNNs require fewer labeled data?\n     * Fewer labeled data and operations can also be compatible with regular GPU, right?\n5. I appreciate the authors tried to bring the bio-plausible concepts to the SNN domain. However, not all bio-plausible concepts revealed by biology/neuroscience research can be directly leveraged and meaningful for the SNN research as SNN neuron models themselves are just bio-inspired and they are super simple compared to the neurons in the nervous system. I do not think combining a few bio-plausible components together without clear insights could be a meaningful research project.  \n",
            "summary_of_the_review": "The main concern on my end is the proposed approach lacks clear motivation why the developed SNN network and corresponding training approach are meaningful. As an SNN researcher, I did not really see the significant value. The validation is only with MNIST, and I think it is too limited. \n\nBut, my rating is not firmed at the moment. Based on the authors' feedback, if I realize I indeed miss something important, I am flexible to change my recommendation. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a spiking neural network (SNN) architecture for\nimage classification, designed to be more biologically plausible than\ncomparable existing architectures. In particular, it eschews\nconvolutional connectivity in favour of local connectivity, and uses\nSTDP (vanilla and reward-modulated) instead of gradient-based\nlearning. The paper describes in detail the proposed architecture\n(providing introductory material on topics such as spiking artificial\nneurons and STDP), and then compares its performance on an MNIST\nbenchmark to other SNN architectures. Finally, the paper presents the\nperformance of the architecture on a conditioning task, which is used\nto highlight the capacity of the network to track a dynamically\nchanging reward.\n",
            "main_review": "## Strengths\n- The paper is adequately written, and the research is motivated and\n  presented clearly.\n- To the best of my knowledge, the proposed architecture is a novel\n  combination of existing approaches, and the reported results\n  convincingly show that it achieves reasonable performance on the\n  benchmark task.\n- The problem of devising flexible architectures that can perform\n  local (Hebbian) learning is one of significant theoretical and\n  practical interest, especially with an eye to potential neuromorphic\n  applications.\n\n## Weaknesses\n- The paper seems rather incremental, in the sense that its main\n  innovations seem to be tweaks or variations of existing\n  approaches. The Introduction section ends with this sentence: *\"The\n  proposed network is the first to employ reinforcement learning with\n  Poisson rate-coded inputs for image recognition and the first\n  locally-connected SNN with a hidden layer.\"* Even if taken at face\n  value, being the first paper to \"employ reinforcement learning with\n  poisson rate-coded inputs for image recognition\" does not seem such\n  an impressive primate, if it means that there already exist papers\n  where reinforcement learning with poisson rate-coded inputs is\n  applied to a different domain, or where rate-coded inputs for image\n  recognition are used with a different learning scheme. Moreover, a\n  cursory search brought up https://doi.org/10.3389/fncom.2021.543872,\n  where reinforcement learning and Poisson rate-coded inputs are\n  applied to an MNIST classification benchmark. I haven't searched\n  more, but this should suffice to show that the claim may not even be\n  true (I'd like for the authors to comment on this). As for being the\n  first locally-connected SNN with a hidden layer (again assuming that\n  the claim is true), I'm similarly unsure how significant an\n  achievement that is, if there already exist convolutional and\n  fully-connected spiking networks with hidden layers, or\n  locally-connected networks without hidden layers.\n- Some of the arguments of the paper seem oddly fixated on the\n  importance of developing spiking networks that are biologically\n  plausible. For instance, on page 1: *\"although convolution has shown\n  great potential in solving any translation-invariant task, its use\n  of weight sharing is biologically problematic\"* . I agree that\n  biological plausibility is important if the goal of the study is to\n  learn something about biological vision, but this doesn't seem to be\n  the goal of the present paper, which is instead more aligned with\n  the interests of the computer vision field. In other words, several\n  passages in the paper present biological plausibility as something\n  desirable per se, but this is not a self-evident truth. Relatedly, the paper seems very\n  quick to dismiss or ignore alternative (and very successful) methods\n  for training SNNs, such as surrogate gradient-based ones (see for\n  instance https://doi.org/10.1109/MSP.2019.2931595 and\n  https://doi.org/10.1038/s41467-020-17236-y), because of their lack\n  of biological plausibility (see for instance on page 2: *\"the\n  majority of works in this area use derivations of the Hebbian\n  learning rule where changes in connection weights depend on the\n  activities of the pre and post-synaptic neurons\"* - is there a\n  reference for this statement?).\n\n## Minor points\n- The introduction contains a few surprising and unsubstantiated\n  statements that are likely to elicit surprise and confusion in the\n  reader. For instance, *\"in many learning problems, we do not have\n  direct access to the explicit label of the data. Consequently, we\n  may need to abandon gradient-based methods, and utilize\n  reinforcement and reward-modulated learning rules\"*: this seems\n  completely gratuitous, and it is not clear how going gradient-free\n  is a necessary implication of not having access to labels, as the\n  wealth of existing gradient-based unsupervised learning methods\n  shows.\n- When discussing biological plausibility, I do not understand the\n  insistence on the primate brain specifically. What is special about\n  the primate brain that is relevant to the arguments made in this\n  paper, and that does not apply, say, more generally to mammal or\n  vertebrate brains?\n- On page 3, please clarify in what sense *\"Spike-timing-dependent\n  plasticity is a type of biological Hebbian learning rule that is\n  also aligned with human intuition\"*. Is this supposed to imply that\n  other types of biological hebbian learning rules are not \"aligned\n  with human intuition\"? What does \"aligned with human intuition\"\n  mean?\n\n## Typos/clarity\n- It would help clarity if the final paragraph of the discussion\n  started with something like \"In the future...\", to signpost that\n  further developments are now being discussed.\n- Throughout the whole paper, please use the appropriate latex\n  citation commands - I believe that in most cases you will want to\n  use \\citep instead of \\cite or \\citet. In the current state of the\n  paper, many sentences are made needlessly hard to parse because the\n  name of the authors of some reference is just inserted into the\n  sentence without any parenthesis.\n- page 1: \"although convolution has shown\" → \"although convolutional\n  networks..\"\n- page 2: \"online on-cheap\" training → \"on-chip\"\n- page 2: \"this differs with artificial neurons\" → \"differs from\"\n- page 3: \"adapt the learning rule with the excitatory to\n  inhibitory...\" → \"adapt the learning rule to\"\n- page 4, above eq 5: I believe \"post-synaptic\" neuron should be\n  \"pre-synaptic\".\n- page 4, below eq 5: \"pre-syanptic\"\n- page 4, below eq 10: \"can be considered a special CASE of R-STDP\"\n\n",
            "summary_of_the_review": "This paper presents a novel combination of existing approaches. It is\nin my opinion incremental in nature, but reasonably consistent in\nachieving its goals. In its current state I can't recommend it for\nacceptance, but I may revise my score if the confusing biological\nplausibility angle is softened and if better connections are made with\nrelated work, especially gradient based methods which at the moment\nseem unjustly penalized.\n\n\n---\nUpdate after discussion: I am updating my recommendation to \"marginally above the acceptance threshold\" to reflect the effort made by the authors to engage at least with some of my concerns. See my message downthread for more details.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}