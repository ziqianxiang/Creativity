{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper presents a debiasing technique that modifies a model's attention mechanism by equalizing attention across social groups.  The authors show that their approach (which is perhaps the first of its kind to look at transformer based models and debiasing instead of fixed word representations) work well in debiasing across certain social group indicators while maintaining overall performance.  However, there is disagreement between reviewers in terms of acceptance of the paper (especially Reviewer 7L6Q wants the paper to be rejected and points to recent critiques such as https://aclanthology.org/2021.acl-long.81.pdf that point out pitfalls with the benchmarks used in this paper).  I agree with said reviewer that a lot of these benchmarks are toy-ish and finding real impact of bias in NLP models is quite elusive.  Hence, I am recommending the paper be rejected for ICLR 2022 and the suggestions below be incorporated towards a better draft for the future."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a new debiasing method for contextualized word embeddings, specifically for attention-based text encoders. At a very high level, the proposed method tries to calibrate the attention scores of words from different groups, e.g. to reduce gender bias, the method forces the model (text encoder) to allocate same attentions to word \"man\" and \"woman\". Experimentally, the paper also demonstrates relatively good results on both likelihood-based evaluation (StereoSet and Crows-Pairs) and inference-based evaluation (NLI).",
            "main_review": "Strengths:\n1. While we have seen a number of interesting papers focusing on debiasing static word embeddings, debiasing (even measuring) contextualized word embeddings is underexplored.  As pretrained text encoders become more and more powerful, this paper is approaching a very challenging and important problem. \n\n2. The way that the proposed method to correct the attention scores is smart. More specifically, it attaches a sequence of words from different social groups to the original training sentence, which makes it very controllable to adjust the attention scores assigned to specific words. \n\n3. The proposed method also has an advantage of reducing multiple types of biases (e.g. gender, race, age, ...) simultaneously and there can be more than 2 groups for certain biases (although in Table 1, examples are all binary).\n\nWeakness:\n1.  The experimental results, especially in Table 4, are not very convincing. The proposed method seems only work well on reducing race bias. Gender bias seems to be the hardest one to mitigate across all methods and Kaneko is doing better on reducing religion bias.\n2. The paper also need to provide more details on hyper-parameter tuning, e.g. lambda in Eq (3). And also more details and ablation studies on negative sampling and layer selection.\n3. I am not fully convinced about the motivation (intuition) in Sec. 3.1. Figure1 definitely presents some good examples of bias in encoders but it is hard to conclude that bias is mostly from the encoder. The paper needs more quantitative analysis on bias in encoders and also decoders to support this claim.\n4. The paper mentioned that bias measurements like WEAT have been questioned. However, Stereo-set and Crows-Pairs have also been criticized [1]. \n5.I am wondering that the positions of words in s_g may also affect the attention scores. I may have missed something here but has any analysis on the relative positions has been done? E.g. \"man\" comes first or \"woman\" comes first. If not, it would be interesting to see if we need to randomly change the order of words in the tuples.\n\n[1] Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets\n",
            "summary_of_the_review": "A paper with a novel method to reduce biases in text encoders and also reasonable empirical results. I would recommend to accept this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This papers propose a attention-based debiasing method for pretrained text encoders. The paper shows reduction in bias on few datasets and a small degradation to the GLUE benchmark. \n",
            "main_review": "Strength \n- The debiasing based on attention is a new method.\n- The empirical results seem good for likelihood based and inference based evaluation. \n\nWeakness\n- While the work has its merit in decreasing the biases in some metrics, It is bad using those dataset as leaderboard without looking into them. [1] elaborates on the few problems with CrowS-Pairs and StereoSet. \n- In [2], it criticizes all the work focusing on word level debiasing and those are really not removing them, but hiding them. To me, this paper is no different from prior work in this aspect. Also given most of the differences are quite small, I am not sure whether the paper is making meaningful progress. \n- The proposed approach is quite limited in the setup. How does it handle multiple token words, different synonyms? Also, as described in section 5, it is not scalable for words that need not be debiased. \n- The proposed approach is also not very novel. It is an extension of counterfactural data augmentation in the attention with regularization with original attention. \n\n\n[1] https://aclanthology.org/2021.acl-long.81/\n\n[2] https://arxiv.org/abs/1903.03862\n\n",
            "summary_of_the_review": "Overall, while there are merits in proposing attention based debiasing, the evaluation methodology is weak. It is another paper treating fairness issues as benchmarking without questioning on the actual effect, differences, and whether the datasets are meaningful in the first place. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper addresses potential biases introduced by attention models\nby re-weighing the attention weights. First, the paper provides a few\nexamples that demonstrate attention weights correlated with social\nstereotype (e.g., doctor attending to he and nurse attending to\nshe). Then, it proposes to reduce this type of bias by \"calibrating\"\nthe attention weights. To do so, each sample is augmented with pairs\nof words corresponding to the groups for which the model is intended\nto mitigate bias. The augmented samples are used in training, and,\nthus, each token in the input will attend to tokens in the augmented\nportion of the sample. The weights are changed such that the weights\nattending on the augmented, group-relevant tokens are similar. This\ntype of re-weighing can lead to some of the semantic meaning encoded\nin the network to change. To prevent this change, the attention\nweights corresponding to the tokens from the original sample are\nforced to follow the weights in an unaltered model that is trained in\nparallel. Last, the paper suggested also utilizing negative sampling:\nusing random words for augmentation and forcing all attention weights\nto follow the ones of the teacher.\n\nFor evaluation, the paper shows the performance of several\ntransformer-based models.  The evaluation of the method includes three\ndifferent benchmarks: two benchmarks that measure bias intrinsically\n(Crows-Pairs and StereoSet) and an NLI task designed for measuring\nbias. To make sure that the semantic strengths of the model are not\nlost, the models are also evaluated using the GLUE tasks. The model\nobtains competitive performance for the GLUE tasks, while reducing\nbias compared to original models and a couple of related works.",
            "main_review": "The method for bias mitigation introduced by the paper is intuitive\nand relatively simple. In particular, I liked the idea of sample\naugmentation as opposed to trying to identify parts of the original\ntext that refers to social groups and trying to alter the weights\nattending to those tokens, which is much higher to achieve.\n\nI have a couple of concerns:\n\n1. The method requires a teacher, unaltered model to be trained in\nparallel, which can be computationally involved. I would have liked to\nunderstand how much the performance of the model (perhaps on the GLUE\ntasks) is changed if such a teacher model is not used.\n\n2. I am concerned about the evaluation of bias. As recent studies\nshowed, intrinsic measures of bias do not necessarily correlate with\nbias measures on a concrete, down-stream task that can be used in an\napplication. https://arxiv.org/abs/2012.15859 The paper includes the\nNLI task, however, I perceive that task very similar to the intrinsic\ntasks. It doesn't provide insights in what it would happen wrt bias in\na down-stream task. I would have liked the evaluation of the model to\ninclude a task for which group annotations exist and the bias for the\noriginal and modified model are presented. For example, the Jigsaw\ndataset could be used for such an evaluation\n(https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data),\nor, for a smaller dataset, HateXplain could be used\n(https://github.com/hate-alert/HateXplain). Without such an evaluation\nfor an application oriented task, it is hard to understand whether the\nfew points gained by the new model will lead to significant bias\nimprovement in a downstream task.\n\nYou may want to take a look at this paper that lists some of the problems with benchmarks such as StereoSet:\nhttps://www.microsoft.com/en-us/research/uploads/prod/2021/06/The_Salmon_paper.pdf\n\nI am not sure if I understood the need to separate the original input\nfrom the artificially constructed one. Do you think that the model\nwill get confused if no separated is used and, thus, its semantic\nperformance would decrease?\n",
            "summary_of_the_review": "The method presented in the paper is intuitive and simple, with some,\npotentially considerable computation overhead. While the semantic\nperformance is preserved and there are modest improvements on\nintrinsic measures of bias, the effects of the model on the bias\nresults of a downstream, application task are not addressed and,\nhence, not understood.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}