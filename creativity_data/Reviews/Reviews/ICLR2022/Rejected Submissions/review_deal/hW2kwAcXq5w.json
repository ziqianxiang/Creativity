{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The authors introduce a method for offline imitation learning in the presence of optimal and non-optimal data. In particular, they propose to learn a discriminator that can be then further used to modify the behavior cloning loss which leads to performance improvements over baselines. The reviews mention that the idea is novel and most sections of the paper are well written and self-explanatory. They do point out, however, several flaws such as the clarity of the derivation and  the thoroughness of experimental evaluation. While the paper has significantly improved during the rebuttal, its significant changes warrant another round of reviews. I encourage the authors to continue improving the paper, addressing the reviewers' feedback and resubmitting it as it has a potential to be a strong submission."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors consider the problem of offline imitation learning in the presence of suboptimal datasets. In the presence of suboptimal data, classical baselines like behavior cloning suffer performance hits, the drop in performance often correlates positively with increase in number of suboptimal trajectories. In this work, the authors propose a novel learning objective inspired by the min-max formulation in GANs. Particularly the agent learns a discriminator to distinguish between samples from the expert and the suboptimal demonstrator. Building on prior work in cost-sensitive learning, this discriminator is used to reweight loss per sample in the offline buffer. \n\nThe proposed algorithm is evaluated on standard offline RL benchmarks, across multiple environments. In many environments, e.g Hopper-v2 style environments, the policy improves against strong imitation learning benchmarks. The discriminator (which takes action probabilities as input) is also evaluated in the context of offline policy evaluation. On Hopper-v2 datasets, the discriminator output is compared with true reward accumulated by multiple policies.",
            "main_review": "Strengths:\n+ The paper is well written and clearly communicates the motivation and contributions.\n+ Sufficient empirical experiments which support the core thesis of the paper. The baselines are selected carefully to provide meaningful comparison to the proposed approach.\n+ Using the jointly learned discriminator seems like a novel idea for policy optimization and evaluation. \n\n\nWeakness:\n+ The experiments on very similar datasets (e.x changing the fraction of true positives). While I agree that it is strong experiment, From Figure 1. it is clear that the performance of DWBC is not too sensitive to fraction of positive samples. It would be useful to have similar experiments on datasets collected from a random mix of optimal and suboptimal policies (instead of buffer of single learning policy). For example, mixing expert and random policies, or Adroit environments with human demonstrations.\n+ Offline Policy evaluation is evaluated on Hopper-v2 only. It would be useful to have a diversity in environments to evaluate this contribution more objectively.",
            "summary_of_the_review": "Overall, the paper is well motivated and provides promising results for leveraging suboptimal datasets for effective offline imitation learning. The problem is well motivated and the authors provide some novel insights into better objectives for behavior cloning. While the authors demonstrate some improvement over the provided baselines, I would encourage the authors to consider adding couple of more ablations, particularly across datasets with a mix of human and random trajectories. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concern.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new offline imitation learning algorithm, DWBC, for datasets that combine both optimal and suboptimal demonstrations. The approach is based on a modified behavioral cloning loss that weighs expert and non-expert data based on a learned discriminator. DWBC is compared against prior methods in OpenAI Gym tasks, and it is shown to yield better policies compared to the prior work. As a by-product, the method learns a discriminator that can be used to estimate the relative performance of any policies without rolling them out in the environment.",
            "main_review": "Overall, the results are quite promising, especially the fact that the algorithm can learn to mimic the expert from a really small number of expert demonstrations. For example, Figure 1 shows that it is possible to learn HalfCheetah from just 4 thousand expert samples (=4 episodes?). However, I do feel like that both the method and the derivation need some more clarity, and I also have some concerns regarding the experiments.\n\nThe derivation includes several inaccuracies and vague statements. The main innovation behind DWBC is to learn a discriminator and condition it on the policy that is being learned. The conditioning is motivated by the fact that it makes discrimination easier (given the optimal policy, one can discriminate the actions based on their probabilities only). It is then argued (after Equation 6) that learning the disciminator becomes more robust, if at the same time, the policy is optimized by minimizing the amount of useful information it can provide to the discriminator. This observation is the most important contribution of the paper, yet the justification seems insufficient and vague. Further, it is not obvious that having the derivative of the discriminator loss w.r.t the policy equal to zero (Equation 7) will be desirable, and that minimization of the final loss in Equation 8 will in fact lead to that condition to be true. That said, the results do indicate the algorithm performs well, so I'm mostly curious seeing a more rigorous derivation and justification that could help shed some light why the DWBC works well.\n\nMoreover, there are several other inaccuracies that make following the derivation hard. For example: \n* The loss $L_d$ should not depend on $s$ and $a$ as it does in Equation 6.\n* I think in Equation 8, the gradients need to be stopped from flowing to the discriminator.\n* How is the discriminator trained? Equation 8 is only minimized with respect to the policy. Is the same loss used for learning the discriminator?\n\nI also have several smaller comments on the experiments:\n\nThere seems to be something not quite right with the shading (standard deviation) in Figure 1. For some of the curves, the shaded region is really narrow compared to how much the curves change between iterations. Can you comment on that? Is the training set  different for each seed? \n\nPlease add axis labels to Figure 2.\n\nThe datasets used in the experiments have a really particular form as they are collected during online training and thus the expert and the sub-optimal data are highly related. It would be good to see a comparison where the datasets come from two completely different policies: one optimal expert policy, and another fixed but suboptimal policy. This would be a more realistic setup (i.e., if the data comes from policy that is trained online, then why do we need offline learning?).\n\nIn the offline policy selection experiment, do you use separate datasets for training the discriminator and evaluation? If not, then perhaps the discriminator is simply memorizing the training data. \n\nWhat is the return of the expert policies for the experiments in Figure 1?\n",
            "summary_of_the_review": "The results presented in the paper are promising, but there are several issues with the clarity of the derivation and some with the experiments, and thus the paper is not yet of sufficient quality to be published as is.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes an offline imitation learning framework that incorporates both optimal and suboptimal datasets to learn decision-making tasks, without requiring any reward annotations. To leverage high reward transitions from the suboptimal dataset, the authors formulate a discriminator that optimizes a positive-unlabeled learning objective, where positive samples come from the optimal dataset and unlabeled samples come from the suboptimal dataset. This discriminator is trained in an adversarial fashion along with the policy, resulting in a behavior cloning objective where samples from the optimal and suboptimal datasets are weighted differently according to the discriminator’s predictions. Experiments demonstrate that on a set of simulated locomotion domains, the proposed algorithm can leverage the suboptimal dataset to learn more performant policies compared to vanilla behavior cloning objectives and prior offline IL/RL baselines.",
            "main_review": "The problem studied in this paper is quite relevant and important — the ability to re-use large, noisy offline datasets to solve new tasks. This paper motivates the problem well in the introduction and lays out the preliminaries clearly. The related work section also makes references to much of the relevant work, though the subsection on offline RL could be enhanced by expanding on the limitations of offline RL (something the authors already did in the introduction). The method appears to be novel and the discussion framing this method as a weighted behavior cloning objective connects nicely to prior work. Alongside these strengths, there are a number of concerns which I enumerate below:\n1. The derivation presented in section 3.2 is a bit confusing and unintuitive. First, I’m not sure I agree with the justification for the why policy outputs needs to be part of the discriminator input. The authors state that it helps the discriminator distinguish between expert and non-expert actions better, but I would like to see this design choice verified empirically — ie. comparing to a discriminator that only takes the state and action as input, possibly without the adversarial formulation. In addition, the discussion on the adversarial policy learning objective also can benefit from additional motivation and details. In particular, why is “providing as little information in $\\log \\pi$ as possible” the best way to learn a discriminator? Why is the latter necessarily equivalent to “maximizing $L_d$ for $\\pi$”? This subsection is, in my opinion, the weakest part of the paper. The discussion here can be improved by a combination of providing a more formal framework for the adversarial learning problem, complementing the formal framework with intuitive explanations, and connecting the ideas here to prior literature.\n2. I am having trouble understanding the derivation details in Appendix B. Specifically, why does $\\frac{\\partial F}{\\partial \\log \\pi}$ equal to $\\frac{\\partial L_d}{\\partial d} \\cdot \\frac{\\partial d}{\\partial \\log \\pi}$? And also I don’t see where the $\\frac{\\partial d}{\\partial \\log \\pi}$ term in incorporated into the final derivation at the end of the section. Can you please clarify these steps?\n3. One critically missing baseline is to train the discriminator defined in equation (4) once, without all the adversarial learning machinery presented in section 3.2 — and use the discriminator weights to weigh different samples when training the policy. This can help justify all of the additional complexities presented in section 3.2 of updating the discriminator and policy in an alternating optimization scheme.\n4. While the proposed method appears to work well in locomotion domains, it remains unclear how the approach would scale to more complex settings, such as the robotic manipulation datasets for the kitchen and adroit tasks in D4RL [1] and the manipulation tasks in robomimic [2]. In principle, it should not be too difficult to run experiments on these datasets as well and such experiments would certainly enhance the scope of this paper. That said, given that ICLR is primarily focused on core machine learning methods and less so on strong empirical evaluations, this is not the primary concern in this review. \n5. It is unclear to me why BC-all and BCND are constant lines, while the other baselines are curves. Shouldn’t all the baselines be shown as curves — ie. where the performance is changing across training iterations? \n6. While it can be implied from the paper as is, pseudocode or a text description of the full training scheme would be nice to have. In particular, I was wondering how often the discriminator is updated relative to the policy — does one update more frequently than the other? \n\n[1] Fu et al., D4RL: Datasets for Deep Data-Driven Reinforcement Learning, 2020\n\n[2] Mandlekar et al., What Matters in Learning from Offline Human Demonstrations for Robot Manipulation, CoRL 2021",
            "summary_of_the_review": "My reaction to this paper is mixed. On one hand, the introduction, preliminaries, and related work are well laid out. On the other hand, I had several confusions about the method and have some concerns about the experiments (see the main review for specific details). As it stands, I think this paper is marginally below the acceptance threshold. I hope that the authors can diligently address the concerns that I raised, at which point I will reconsider my recommendation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper deals with the following setup: offline imitation learning in the presence of both an expert dataset and a non-expert dataset. More precisely, the goal is to learn a policy as close as possible to the one(s) that generated the samples in a dataset $D_e$, while making the most of samples in a non-expert dataset $D_o$. The “reward” information is not present/used in the dataset.\nThe authors draw inspiration from the positive-unlabeled classification as well as the adversarial imitation learning literature to propose a new algorithm to tackle this problem. They interleave the training of a discriminator and a policy. The discriminator is trained to discriminate between expert and non-expert dataset (using a positive/unlabeled loss) and takes as input the state, the action and the logit of the policy $\\pi(a | s)$. The policy is trained to imitate the expert on $D_e$ and to “fool” the discriminator. \n\nThe authors present results on four environments from the Gym Mujoco suite with datasets extracted from the D4RL datasets.\n",
            "main_review": "**Strengths**\n- The paper is overall well written and easy to follow. The justification of the setup makes sense and is well explained.\n- The setting is very interesting and has a great potential impact on the community.\n\n**Weaknesses**\n- Method\n    - The authors fail to mention a very close work that already applied positive-unlabeled learning to the Imitation learning setup: PUGAIL (https://arxiv.org/abs/1911.00459). This greatly limits the novelty of this work.\n    - I find the theoretical derivations confusing. In particular, in appendix B, proposition 1, is “when” an “if and only if” condition or just an implication?\n    - I also don’t understand why the authors need to introduce the function F. Overall the mathematical derivations are unclear and would benefit for more details (keeping them shorter in the main text but developing them correctly in appendix for example).\n\n- Related Work\n    - I think a few more works would be worth mentioning. In particular, some previous work already considered having access to two datasets (on expert, one non-expert), like CSI  (https://hal-supelec.archives-ouvertes.fr/hal-00869804/document)  or MILO (https://arxiv.org/pdf/2106.03207.pdf)\n- Evaluation\n    - Authors write “the proposed algorithm can learn behaviors that are much closer to the optimal policies” yet they fail at providing experiments that support this claim as they only study the return of the learnt policies. I would recommend that they checkout this work (https://arxiv.org/abs/2105.12034 ) that provides insights on how to evaluate models in the context of imitation learning. What is more, the plots don’t show the average return of D_e which makes it impossible to use the return as a proxy to study how “close” the policy is from the demonstrations.\n- Experiments\n    - The experimental setup is good but a bit “light”. As written by the authors, their method is quite fast to train, so why not provide more results on different setups, e.g. 1) with very little data (e.g. like 1 or 5 trajectories only)? with very random data in D_o? with human expert data in D_e? with more complicated environments like Adroit? (All these environments/datasets are available in D4RL).\n    - Please report the average return in D_e as a horizontal bar in the plots. Otherwise it is impossible to calibrate the results as a reader.\n- Writing\n    - In the abstract, the authors say “both optimal and non-optimal expert behaviors”. I find this expression a bit confusing. It suggests that the algorithm will only work if the D_o is actually made of expert but slightly suboptimal trajectories.\n    - nit: In 3.2, “much hard” -> “much harder”, “which we denote it as” -> “which we denote as”\n",
            "summary_of_the_review": "I believe this is an interesting idea in an interesting setup yet it lacks novelty and it would deserve more work, notably on the experimental part, before being published.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}