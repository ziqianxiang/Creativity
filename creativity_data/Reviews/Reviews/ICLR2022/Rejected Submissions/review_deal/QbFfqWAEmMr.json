{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes a novel method for improving domain generalization based on the idea of learning different subspaces for each domain. Authors provide theoretical analysis related to their proposal and further evaluate their proposed method on a subset of DomainBed benchmark.\n \n \n**Strong Points:**\n \n- The paper is well-written.\n \n- The proposed method is novel.\n \n- Authors provide theoretical analysis in support of their proposal.\n \n- The theoretical results seem to be correct.\n \n- Empirical evaluation shows that the proposed method improves over baselines on a subset of datasets included in the DomainBed benchmark.\n \n**Weak Points:**\n \n- The complexity of the theoretical results makes it very difficult for the reader to get any intuition about the underlying mechanisms at play.\n \n- The theoretical analysis is disconnected from the proposed algorithm. It is hard to see how one could end up proposing such an algorithm following the theoretical results. I suggest that authors would consider reorganizing the paper with less emphasis on the theoretical part, perhaps simplifying the theoretical results and pushing the rest to appendix.\n \n- The empirical evaluation can be improved significantly. Domain generalization is a very well-established area at this point. WILDS is a carefully designed and well-known benchmark and showing improvement in that benchmark would be very convincing but unfortunately authors do not discuss or even refer to it. They instead report their results on a subset of datasets used in DomainBed benchmark. The DomainBed benchmark is less challenging than WILDS but even following DomainBed closely and reporting the 3 evaluation metrics on all 7 datasets would have been satisfying. However, authors only report the results on 3 datasets. Reporting the results on a diverse group of datasets is particularly important in the case of Domain Generalization because we know that many methods are able to show improvements on a few datasets but it is challenging to beat the baselines on a significant majority of datasets.\n \n**Final Decision Rationale**: \n \nThis is a borderline paper. On one hand, the proposed method is interesting and novel. On the other hand, the theoretical contributions are very limited and the empirical evaluation is not strong enough for acceptance. Given that all weak points mentioned above can be addressed, I recommend rejection and I sincerely hope that authors would strengthen their paper by addressing them before resubmitting their work."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents LASSO, a Latent Sub-Spaces Orientation to tackle domain generalization problem. The paper involves both theoretical and empirical results. \n\nTheorem 1 says the target domain loss is bounded by terms involving source domain losses, label shift, and the latent data shift (which reminds me of the theorems from the seminal work by Ben-David et al. for domain adaptation). Theorem 2 extends Theorem 1 but with latent subspaces. Theorem 3 says that for a given subspace indicator \\Gamma, the mutual information between the sub-space and label is lower bounded by the sum of negative source domain losses.\n\nThe proposed method essentially introduces the subspace-indicating binary variables in the neural network. This seems to work similarly to the dropout, but with more structure and with learned and data-dependent drop rates. Inference is also done by generating multiple subspace-indicating binary masks and ensemble them. \n\nExperimental results are provided on standard DG benchmarks. The trend seems clear that for shallow network (e.g., AlexNet, ResNet-18) the performance is not necessarily better than previous methods, but with deeper network (e.g., ResNet-50) the improvement is  somewhat significant. ",
            "main_review": "- Strength\n  - The proposed method is solid and novel.\n  - The empirical results show solid improvement over baselines with a large network backbone. \n\n- Weakness\n  - Theoretical arguments are not particularly strong. For example, Theorem 1 and 2 are rather for domain adaptation when one has access to the target domain, so it is unclear how it is relevant to the domain generalization context. While formulating a bound with latent space might be new, but it does not seem to add any better understanding for domain generalization.\n  - \\Gamma is optimized based on Equation (7) and it is unclear what authors mean by \"our principle is to encourage \\Gamma_{d} to becomes more independent\". Are there any part of the optimization problem that promotes independence?\n  - Might be good to add a baseline without updating \\Gamma (i.e., adding a dropout layer).\n\n- Misc\n  - It is unclear what it means by \"but the gains shrink with ResNet50 since larger ResNet backbones are known to generalize better\" in Section 4.1.3.\n  - The trend observed in the paper on the performance w.r.t. the network size seems interesting. Might be good to add more experimental results with deeper networks to see if trend holds true.",
            "summary_of_the_review": "While there is a question on the significance of the theoretical results, overall the paper proposed a new domain generalization algorithm that is both technically and empirically solid.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "\nAuthors point out that the assumption of existence of fixed domain-invariant features and common hypotheses learned from a set of training domains could be overly strict and sub-optimal. Authors propose a new method which doesn’t have a single hypothesis shared among domains and give theoretical analysis of the proposed method. Authors also give results on benchmark datasets. \n",
            "main_review": "1) Authors correctly point out overly strict and sub-optimal assumption that is popular in the literature currently. \n\n2) Authors’ claims are backed both by theory and experimental evidence. \n\n3) Some benchmark papers in the literature (which don’t have an assumption of domain-invariant features) are not considered [1-2]. Both these papers don’t have an assumption of domain-invariant features and are similar in spirit as the current paper.  \n\n4) Theoretical analysis in the given paper helps to come up with a loss but not actually analyze the method. Like in [1-2], can authors comment on the learning theoretic study? How does risk bound change with the number of available domains and number of examples in the training? \n\n5) As we have a large number of domains and a large number of training examples then one should be able to have optimal error or loss.  I could not draw this conclusion from the theory that the authors provided. Can authors comment more on this? \n\n6) How were hyperparameters tuned for the baseline method? \n\n[1] Blanchard, Gilles, Gyemin Lee, and Clayton Scott. \"Generalizing from several related classification tasks to a new unlabeled sample.\" Advances in neural information processing systems 24 (2011): 2178-2186.\n\n[2] Blanchard, Gilles, Aniket Anand Deshmukh, Ürün Dogan, Gyemin Lee, and Clayton Scott. \"Domain Generalization by Marginal Transfer Learning.\" arXiv preprint arXiv:1711.07910 (2017) J. Mach. Learn. Res. (JMLR) 22 (2021): 2-1.\n",
            "summary_of_the_review": "Paper has rigorous set of experiments and is backed by clearly explained motivation. Addition of suggested literature could further improve the paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper critically re-examines the rationality of domain-invariant based method for DG (generalize to unseen domains without re-training). It highlights that, tackling all source domains equally without taking the underlying relationship between them to learn domain-invariant features can lead to a sub-optimal solution. To deal with this, it introduces a latent space decoupling-based method called Latent Sub-Space Orientation (LASSO) to eliminate some irrelative representations for prediction. By this, the relationship between latent space and label space can be strengthened, thus the model can generalize better. The results show that LASSO (especially with threshold) can commonly lead to a marginal performance boost for popular DG benchmarks. ",
            "main_review": "In the abstract, the author claimed that exploring diverse latent sub-spaces for individual hypotheses learning is superior compared with learning single hypothesis shared among domains, however, it is not clear to me what is the significance of doing so.  The authors try to give an explanation by delivering an upper bound target general loss, which to me is quite questionable since the target domain is not available during the training phase. Simply using the mixture sources domain may not reflect the true distribution of the target one.\nBesides there are some other weaknesses as follows:\n1. Is Theorem 2 strictly lower than Theorem 1? I can only find the author’s explanation rather than strict prove. This part is essential for this paper, besides, this is closely related to my given score. \n2. In Section3.4.2, the author proposes to use sampling for model training. I wonder about the performance of doing this in a deterministic manner, just like using threshold at the testing phase.\n3. Experiments are somewhat lacking, 1) challenging datasets (e.g., medical imaging for CV, reinforcement learning in MLDG) should be considered since the proposed method is more related with general machine learning,  2) only RSC is considered as the latest stoa baseline, which is not desired.\n\nMinor:\n4. For the experimental part, why not keep the same manner to select the validation set? It would be better to keep this unchanged. \n",
            "summary_of_the_review": "Though I am not an expert in this area, I have discussed my comments with my colleagues with rich experience in this area. Thus, I am still confident in my comments. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a latent subspace orientation algorithm for domain generalization, which is built on diverse latent subspaces and individual hypotheses with label-informative features. Extensive theoretic and experiments show the effectiveness of the proposed method with two variants.",
            "main_review": "Instead of using a single consistent latent space, the authors propose to employ multiple latent spaces by capturing the label-informative features from the available source domains. Some theoretical analyses are given to support their learning scheme, and a very simple latent subspace orientation algorithm is proposed to tackle the domain generalization challenges. Experiments validate the usefulness of the proposed method on different datasets.\n\nSome concerns should be considered for rebuttal.\n1. Generally, the proposed method alternatively optimizes an indicator function and subspace learning. Although there are three theorems to support your claim, the reviewer still has concerns about the standpoint and update of the subspace indicator. Apart from these claimed theoretic analyses on the subspace indicator \\Delta, the authors should give a more intuitive explanation of why it is useful, which can help the readers understand your work, rather than lots of equations. Moreover, the reviewer is wondering if we fix these indicator updates after some iterations, what will happen for the performance because the review doubt such an update is meaningless for this task in practical systems.\n2. Why is maximizing the mutual information useful in your framework? From the reviewer's view, using other adversarial learning or metric learning also can achieve your objective.\n3. The presented performance is unsatisfactory when compared to other works published in 2021, although this paper is very simple and easy to implement. \n4. The authors should clarify why the performance of using Threshold outperforms those of using ensemble. Intuitively, using an ensembling strategy has a tendency to have better performance. Moreover, how could you set the value of \\tau? Is it a hyper-parameter? Is there any adaptive learning strategy to determine it or empirically setting?\n5. Many grammar mistakes are shown in the work, and the authors should pay attention to your presentation.\n",
            "summary_of_the_review": "This paper proposes a simple latent subspace orientation algorithm for domain generalization based on some theoretical and experimental observations. The technical parts with some complex theoretical analyses are satisfied, yet the model construction and experimental parts could be improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "See the above Main Review Section.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper introduces a new method for domain generalizable classification. Given a feature representation of input, the proposed method first samples binary masks from a Bernoulli distribution whose parameters are determined by the feature, projects the input feature to multiple sub-spaces by using the masks, and applying classifiers associated with each of the sub-spaces. The parameters of the Bernoulli distribution are predicted by a module that is learned together with the feature representation and classifiers. \n\nAlthough the paper tries to provide mathematical backgrounds and underlying theories of the proposed method, unfortunately, this reviewer finds no clear justification for learning and using the sub-spaces. Also, the improvement by the method seems marginal (or it even underperforms sometimes) when coupled with small backbone networks.\n",
            "main_review": "Although the paper presents lots of theorems, this reviewer could not find a clear justification for the proposed method. \n\n(1) How the diversity of latent representations could reduce the latent data shift issue: \n\nTheorem 1 and the first paragraph of page 4 suggest that to achieve domain generalization one has to reduce the latent data shift, which is unfortunately not straightforward since no target data is available in the task. The second paragraph argues that learning a single domain-invariant model is not enough to resolve the issue, but such an argument is not theoretically proven. Moreover, in the third paragraph, the authors claim that modeling and utilizing diverse latent representations are the key to the success, but the connection between the diversity of multiple latent representations and the latent data shift issue is totally missing. (Further, considering the technical details in Sec. 3.4, there is no guarantee of the diversity in the proposed method.)\n\n(2) How the sub-space hypothesis could reduce the latent data shift:\n\nIn page 5, the authors argue that the compactness of the sub-space can reduce the latent domain shift, which is however not proven theoretically either.\n\n(3) How the mutual information maximization is linked to reducing the latent data shift:\n\nTheorem 3 derives a lower bound of the mutual information between ground-truth label and data projected to the sub-space (i.e., masked feature representation), and accordingly, the masking module is learned to maximize the mutual information in Eq. (6). However, there is no justification for this approach nor the connection between the mutual information and the latent data shift (or domain generalization in general).\n\n(4) How the independence of the masking weights leads to better generalization:\n\nThe argument in page 6 about the advantages of such independence is not rigorous; the reason following the argument seems indecipherable.\n\nDue to these, this reviewer feels the theorems derived in this paper are not very useful to justify the proposed method, and cannot find clear reasons for its advantages. \n\n\n[Other comments]\n- Do not understand why z is a 2D tensor and what each dimension stands for.\n- Would recommend drawing a figure to better illustrate the overall pipeline.\n- The first theorem resembles the well known theorem in literature of domain adaptation. It would be useful if their differences are well explained in the paper.\n",
            "summary_of_the_review": "This reviewer feels the theorems derived in this paper are not very useful to justify the proposed method, and cannot find clear reasons for its advantages. Also, the practical value of the method seems limited as its performance is not impressive or even inferior to previous work when it is incorporated with small networks. \n\n-- post-rebuttal --\nI greatly appreciate the kind responses and the update of the manuscript and would like to upgrade my rating accordingly, but since my major concern has not been resolved yet, I am still leaning towards rejection.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}