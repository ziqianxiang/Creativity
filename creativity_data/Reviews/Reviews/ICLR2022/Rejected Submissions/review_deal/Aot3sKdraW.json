{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "All four reviewers agree that the paper should be rejected in its current form, but make numerous suggestions for improving it. The main points of concern were the motivation of the proposed method, novelty and the quality of the presentation of the work. The authors did not provide a response. The AC agrees with the reviewers and recommends rejecting the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors consider Navier-Stokes equation in 2D and the 1D Burger's equation.\nUsing a PINN approach to model the solution with neural networks, they propose an architecture that alternates spatial and channel attention blocks to convolutional blocks. \n",
            "main_review": "\n\nThe presentation is very poor, experimental details are missing, and it is not clear \nagainst which models the authors are benchmarking. They report increased performance\naccording to a metric they don't even define. \n\n- Missing references in the introduction\n\n- Open problems in the field are not discussed and \nmotivation for the proposed model is missing. \n  \n- Poor English throughout the paper and generally a poor presentation. \nThe first sentence\n\"Computational Fluid Dynamics (CFD) has become the core technology behind almost every fluid\nsimulation.\" is almost tautologic. Many ill-composed sentences such as\n\"A thorough study of data-driven methods using machine learning approaches for\nmodeling the turbulence was studied by ...\".\n\n- tilde and bar variables not defined in Eq. 4\n\n- No outline of the paper in the introduction. Not clear what is the purpose of some sections like 3.2 and 3.3.\n\n- The author should care to explain what is the output of the network. \n\n- Not clear what models the authors are benchmarking against, as\nthey don't provide details and references.\n Possibly they should apply their attention augmentation to  State of the art\n architectures such as the ones analyzed in the NSFNets paper.\n\n- The authors say they evaluate on a few different metrics, but all I can see reported is\n\"accumulated errors\", which I have no idea what it is since it has not been defined. \n\n- A figure is not a detailed explanation of a NN block but only a visual help, please provide definitions in formulas.",
            "summary_of_the_review": "The proposed model may have some selling points to it, but the presentation\nis so poor and so many details are missing that it is impossible to tell.\nThis manuscript is far from being up to ICLR standards. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This manuscript introduces the attention module into the framework of physics-informed neural networks. The contributions are: (1) proposing a network architecture that marries the popular attention mechanism to physics-informed neural networks. (2) separating the large attention module by operating on the channel and spatial dimensions individually. The authors empirically studied the efficacy of this architecture on PDEs including Navier-Stokes fluid simulation and Burgers equation.",
            "main_review": "I appreciate the idea of combining attention mechanisms with physics-informed neural networks, but I expect more than merely an addition. Also, there are a few weaknesses I have to point out:\n\n1. The author did not provide any explanation on the intuition behind refactorizing the channel and spatial dimensions. Is it for memory efficiency? How does it compare to non-local neural networks?\n2. Physical process usually comes with tons of hyperparameters, which is understandable because of the complexity. However, a lot of the hyperparameters are left unexplained or unexperimented. For example, how were $\\alpha$ and $\\beta$ selected? What does the overall network architecture look like? Does the number of attention modules stacked matter to the result?\n3. One key question to physical simulation with neural networks is generalizability. It will be very helpful to demonstrate how this method generalizes to unseen boundary conditions and initial fields.\n4. Attention mechanisms intrigue me by not only a superior performance on multiple tasks but also the explainability. It will be interesting to see the visualization of them so that it might shed some light on what and how the networks learn physics.\n\nIn addition to these technical unclearnesses, my main concern is the novelty of mixing multiple popular topics. I expect more solid theories or more exciting demonstrations.",
            "summary_of_the_review": "This manuscript talks about two popular topics: attention mechanisms and physics-informed neural networks. I believe something significant will happen in this particular overlap but not in a \"1+1=2\" form. I expect more explanatory theories or experiments and domain-specific modifications.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a new architecture and loss function for training physics-informed neural networks (PINNs) on fluid flow problems. The idea is centered around augmenting a basic residual network with two additional attention blocks that are placed before and after residual blocks. These two new blocks aim to introduce channel and spatial attention into the model. In addition, they propose a new loss function that is tailored to solving PDEs such as Navier Stokes and Burgers Equation.",
            "main_review": "PINNs are a vibrant research area that combine ideas from deep learning and numerical methods and scientific computing. Advancing this area is of interest for the scientific machine learning community and work that advances the state-of-the-art can have significant impact. Designing new architectures that are better suited for certain scientific problems is a plausible research direction since (a) standard PINNs rely on very simple fully-connected architectures and (b) architectures that are designed for CV and NLP problems might not be optimal for scientific problems.  Hence, the research question that the authors address is of interest. \n\nHowever, the paper has several weaknesses and there are several open questions after reading the paper.\n\n1. Method:\n\n* The proposed spatial and channel attention models are insufficiently discussed. It is not clear to follow the logic and what the motivation for the design of these modules is, in the specific context. The concept of attention is not explained. \n\n* It would be helpful to have more details about the 1x1 conv layers to understand whether the dimensions are increased or reduced. I assume that the authors use zero-padding and stride of 1 (details are not provided). Feeding a given tensor x of shape (B, K, H, W) into the proposed spatial attention model that applies twice a 1x1 convolution layer with F_1 and F_2 filters should return a new tensor of shape (B, F_2, H, W), i.e., the spatial attention model is changing the filter dimension from K to F_1 to F_2. Since this output is added to the input tensor x, F_2=K. But, details about F_1 are not provided and it is not unclear whether the number of channels are increased or decreased here, i.e., if this module is introducing some bottleneck. \n\n* It is not clear what is meant by `reshape' and what the addition operation is doing in Figure 1 and 2. \n\n* The ablation study does indicate that there is some effect of using both modules together, but the discussion of the ablation study is insufficient. On which dataset has the study been performed? Why does the model with both modules has less parameters? Does this mean the modules essential reduce the dimensions? Also a baseline model that does not use any additional module is missing.\n\n* Why is the new loss function more robust? How do you tune the many new parameters that are introduced? A detailed ablation study is missing. \n\n\n2. Experiments\n\n* Following the reasoning behind the experiments and understanding the content in the tables is difficult. First, it is not clear where all the other models are coming from. For example, has a GAN been trained on the specific dataset by someone else, or by the authors? In the former case, some references are needed; in the latter case, more information about the setup is needed.\n\n* Next, I don't know what the authors mean by `accumulated errors'. The author introduce other metrics for evaluation, but I cannot find any table that shows, for instance, the RMSE. \n\n* In Figure 6, what is the difference between the second and third row?\n\n\n3. General comments\n\n* It would be helpful to clearly state and discuss the model / PINN framework that the authors consider.\n\n* The related work jumps between general deep neural networks for scientific applications and PINNs. It is confusing to follow the related works.\n\n* What do the authors mean by `running at good enough speed'?\n\n* The background section can be moved to the appendix. It is not relevant to understand other parts of the paper.\n\n* The additional provided materials seem not relevant for the general discussion. \n\n* I strongly suggest that the authors expand the discussion and provide additional details to clarify the working mechanisms of both the spatial and channel module. ",
            "summary_of_the_review": "The authors introduce a new architecture for PINNs, which can be seen to be novel up to a certain extend. However, modules of similar flavor have been proposed in previous works and simply proposing a new architecture that augments an existing architecture with additional modules or layers is not sufficient. The experiments need to clearly study and demonstrate the advantages on a range of problems, in particular if no theoretical justifications are provided. The provided experiments do not live up to the standards that I would expect from an empirical paper. Further, the presentation and discussion of the ideas is insufficient and many details are missing. Also, no research code is provided to reproduce or better understand the proposed architecture. The overall quality of this paper is below the acceptance threshold for ICLR. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The manuscript entitled “AA-PINN: ATTENTION AUGMENTED PHYSICS INFORMED\nNEURAL NETWORKS” use a combination of channel and spatial attention module in addition to the normal Physics Informed Neural Networks. Using evaluation metrics like RMSE, divergence and thermal kinetic energy, AA-PINN network outperforms previous PINNs for modelling Navier Stokes and Burgers Equation.\n",
            "main_review": "This paper is not well organized. In some sections e.g. BACKGROUND, there is a whole page describing the MOMENTUM EQUATIONS and PRESSURE POISSON EQUATION, however later in the AA-PINN experiment, these equations are never used. Also in the Results section, the authors just display the outcomes from the AA-PINN without any discussion about why AA-PINN works. The ablation study of the channel and spatial attention module on the performance looks interesting, but again there is no comment from the authors at all.\n\nI would not recommend this paper to be published in the current form.\n\nMy addition comments are as follows:\n\n1.\tOn page 4, the BURGER’S EQUATION Eq. (18) doesn’t show f(t, x) at all. Later in the loss function Eq. (22) on page 5, f(t, x) is used. This will cause confusion. \n2.\tThe definition of t_u_i is given right after Eq. (18), but it is not used in that part. Instead, it is used later in Eq. (22). This will confuse reader as well.\n3.\tThe terms of t_u_i, x_u_i, u_i, N_u, etc. should be explained respectively instead of as a whole.\n4.\tOn page 4, Figure 1 contains two operations 1) reshape with x, 2) reshape with +. What are they about, respectively?\n5.\tOn page 8, Figure 6 has three rows of plots, what are the last two rows about, respectively?\n6.\tOn page 7, Table 1 shows the performance comparison of the proposed network with previous state of the art. What equation are those networks solving? Where is the data for those neural networks, e.g., TF-net, from?\n",
            "summary_of_the_review": "This paper proposed attention augmented physics informed neural networks (AA-PINN). It uses a combination of channel and spatial attention module in addition to the normal Physics Informed Neural Networks. This paper would be an interesting paper however it is poorly organized in the current version and also has a lot of flaws as I pointed out above earlier.\n\nIn summary, I would not recommend its publication in ICLR until all my comments are addressed.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}