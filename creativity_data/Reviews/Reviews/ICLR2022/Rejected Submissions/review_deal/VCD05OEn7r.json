{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes the framework CAGE (causal probing of deep generative models) for estimating counterfactuals and unit-level causal effects in deep generative models. CAGE employs geometrical manipulations within the latent space of a generative model to estimate the counterfactual quantities. The estimator is written in potential outcome language and assumes unconfoundedness, positivity, stable unit treatment value assumption (SUTVA), and linear separability in semantic attributes of the latent space. Furthermore, the framework considers only the case of binary treatments.  \n\nOne major concern raised by reviewers TgM5 and xP5d is that the method is based on a trained generative model, which may not be the true data-generating model. In this case, the paper appears to address statistical dependencies instead of the actual causal relationships in the real world. The authors claim to empirically show that their framework can probe unit-level (individual) causal effects. However, the reviewers are concerned that no theoretical support for the correctness of the method is provided. In other words, the problem is assumed away once a probabilistic model is assumed to be equal to the true generative model, which is almost never the case in practice and is well-known in the field. We want to encourage the authors to provide a more detailed theoretical justification, perhaps with proofs and/or references, that the proposed method can infer causal and counterfactual relationships given the underlying assumptions. \n\nAfter all, reviewers were interested but somewhat skeptical about the method's ability to learn causal and counterfactual relationships. Unfortunately, the paper is not ready for publication yet. Still, we would like to encourage the authors to take the reviews seriously and try to improve the manuscript accordingly."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a novel framework CAGE (causal probing of deep generative models) for inferring the causal-effect relationship in deep generative models. The treatment is implemented by moving linearly along the hyperplane normal. Then the attribute treatment effect can be quantified via a proxy classifier. The authors evaluated the CAGE on low-dimensional synthetic and high-dimensional datasets. Results show the ability to infer causal relationships of the proposed CAGE.\n",
            "main_review": "Pros: The paper is well-structured. The idea is novel. The authors established a new method for both counterfactual manipulation and treatment evaluation. The paper also designs corresponding experiments and evaluation methods. The generated counterfacual images and the causal-effect relationship seem reasonable. \n\nCons: Since the hyperplane that encodes the classification boundary is obtained by linear classifier, it means that the learned attributes should be linearly separable in the latent space. How to guarantee or to proof this, like some theoretical analysis or qualitative feature visualization?\n\nOther comments:  \n1)  The CAGE performs the binary treatment, i.e., manipulating attributes with binary labels. Is it possible to extend to other attributes on multi-category situations or even continuous situations?\n2)  How to obtain the normal vector to the hyperplane given the high-dimensional latent encoding z?\n3)  The text of histograms in Table 1 is not clear enough, please check.\n4)  Some typos, please check.\n",
            "summary_of_the_review": "This paper is novel on the interpretability by diving into the deep generative models. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This manuscript proposes CAGE, a method for determining causal relationships between attributes based on the learned representations of deep causal models.  The method is based upon relatively standard causal inference methods after structuring the deep latent representations between them.  The method is straightforward to apply and estimate the Generative Average Treatment Effect (GATE).  Results are shown on both synthetic and real datasets, and are used to infer relationships.",
            "main_review": "A strength of this paper is that it clearly presents the methodological ideas and formulations.  If you have a background in causal inference, the approach is relatively straightforward to apply in the latent space, as the methodology is well-laid out in Section 3.  The visualization in Figure 2 is quite nice to illustrate the idea and the utility of the linear classifier.  There are many results and visualizations, allowing the reader to do a great deal of visual evaluation.  The ideas are highly relevant to ethical considerations in modern applications of deep learning (although I would like to see this expanded further in the discussion!).\n\nWhere this becomes tricky is on the interpretation of the results. 4.1 and 4.2 are relatively straightforward to evaluate, but on a small and simplistic scale. It is very difficult to evaluate whether the results from this model are good or not on the real data (4.3), which is a common problem in the evaluation of causal models.  In particular, the different results between the different models are very challenging to evaluate, and they need to be discussed at length.  Why do the 3 baselines and the proposed method differ at times?  They are all analyzing the same data, and the \"prior\" relationship is not necessarily true for the generative model (as is noted by the authors). The authors should discuss at greater length their rationale for the proposed causal relationship, and whether that appears true in the system.  Additionally, I would like greater discussion of the visual samples, which don't always match what I would expect from the system.  For example, adding a mustache to a female example without changing the gender seems like a strange example, as essentially no similar samples exist in the training database.\n\nMany of these causal relationships are evaluated with semi-synthetic data setups, as was done in 4.1 and 4.2.  It does seem like you selectively choose the training images in Section 4.3 to imply specific causal relationships and see if you method picks them up in the latent space, rather than subjectively choosing priors, which may or may not be true in the generative latent space.\n\nThe results presented in Table 2 are strange.  Notably, the $G\\rightarrow H$ strategy is significantly better on test but significantly worse on validation than $H\\rightarrow G$.  Any idea why there is this discrepancy?  Given these results, it is unclear how strong the evidence is for the claim that using the causal direction is better than the anticausal direction or if it is just statistical fluctuation.\n\n",
            "summary_of_the_review": "Interesting and potentially useful method in a relevant topic area.  The evaluation approach could be improved.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose to discover causality for a trained man-made model. This is very unusual, as normally we would be interested in discovering causality the true underlying process that generates the true data.    \n\nMore specifically, they propose a framework called CAGE to estimate generative average treatment effects (GATEs) for probing cause-effect relationships in the so called latent space of deep generative models. They use the average treatment effects (ATEs) to infer causal directions for deep generative models' attributes and use CAGE to generate counterfactual data. They evaluate the ability of CAGE both synthetic (a variant of MNIST called MorphoMNIST) and high-resolution face dataset (CelebaHQ).\n",
            "main_review": "I would like to thank the authors for the interesting work they proposed, and tried to explain my concerns below. \n\n1. My first concern is that the authors try to probe causality in a trained model, instead from the data. The trained model may not represent the data well nor capture the causality in the true underlying process that generates the data. In fact, the styleGAN model used is not even designed to capture causality. It is unclear why probing causality in such a trained model is useful at all. \n\n2. Even for a causal discovery model that aims to estimate causality for latent variables from the data, they often reply on heavy assumptions. Whether recovering the true parameter or the causal structure is possible is known as identifiability problem. I suggest that the authors to check some particular identifiable generative models, e.g., CausalGAN (Kocaoglu, Murat, et al. \"Causalgan: Learning causal implicit generative models with adversarial training.\" arXiv preprint arXiv:1709.02023 (2017).), not “any existing pre- trained latent variable model” shown in the abstract. In addition, I recommend the authors read some papers relevant to identifiability in latent space, e.g., Khemakhem, Ilyes, et al. \"Variational autoencoders and nonlinear ICA: A unifying framework.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2020. \n\n3. The term 'latent' variables used in this manuscript is misleading. My understanding is that the authors used StyleGAN which produces style vectors. Once produced, these vectors are observed, thus are not really latent. Discovery causal relationship among observed variables have been well studied in the existing literature.   \n\n4. I am curious about the definition on the equation in ‘step 3, page4’. In causality, do’ operator has strict mathematical definition, it is unclear whether ‘do’ operator can be replaced by the RHS in the equation in theory. Or what the different between the RHS and the true ‘do’ operator. What is more, by using this definition, is the true potential outcomes equal to the ‘potential outcomes’ obtained by the equation? This is a very strange equation, it would be better if the authors can show that the potential outcomes obtained by the equation is consistent with the true outcomes in theory. \n\n5. Counterfactuals require to recover the exogenous noise variables first, which is different from intervention. I don't see the noise variables were recovered in the manuscript.  \n\n---Post-rebuttal review----\nThe counterfactual is different from intervention. What the authors did in the paper is intervention, but claimed as counterfactuals. The authors seem to point out that they followed \"section 4.1 of [9]\", where I didn't find the support. Counterfactual reasoning requires recovering the exogenous variables (a.k.a. disturbance/noise variables) first, and then performs intervention. \n\nIt remains unclear what is the benefit of probing causality in a pre-trained model which is not designed to capture causality. \n\nI am happy with the rest of the responses, thus have lifted up the score from strong rejection to rejection. ",
            "summary_of_the_review": "The task is not well motivated. There are several technical issues mentioned above. I think this paper is not ready to publish. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The goal of this paper is to inspect causal relationships learned by a generative model. In particular, the paper studies how moving in the latent space with respect to a putative cause variable (e.g. age) affects the probability of the putative effect variable (e.g. baldness). This is done by training two linear classifiers in the latent space, one for each variable. The counterfactual for a given cause variable and datapoint is defined by changing the location of the data point in the latent space in the direction perpendicular to the linear classification boundary for the cause variable. Then, the paper inspects what happens to the putative effect variable, using its respective decision boundary. A null distribution is defined using permutation. The usefulness of techniques is demonstrated for inferring the causal relationships (including direction) for pairs of variables and for generating data augmentations for robust classification.\n",
            "main_review": "PROS:\n\nThe goal of the paper is fresh, I am not aware of other works trying infer causalities from the latent space in a similar way as done here.\n\nThe use of the method for data augmentation seems promising.\n\nThe presentation is clear and technically correct, although some more theoretical justification for calling the relationships causal, and not just statistical, would be beneficial.\n\nCONS/QUESTIONS:\n\nMy main concern is that this paper appears to be more about the statistical dependences of two variables in the latent space - and while some of the results seem promising - it is not clear to what extent these findings have to do with causality, and to what extent they are just conditional probabilities. It is quite possible that causality leaves some trace that can be detected in the latent space, but it is not clear what that trace could be.\n\nI would suggest the following, at least, to improve this:\n1) Visualizations of the latent space and the decision boundaries to provide intuition about what's happening in the latent space, and where does the causal signal come from.\n2) Many more experiments with pairs of variables, including (but not limited to) such variables that are correlated but not causally related. This would allow the reader to see whether the method can truly distinguish causality from association.\n\nPermutation removes not only causality, but any association altogether. Therefore, it can not be used to test the statistical significance of \"causality\".\n\nThe results regarding pairs of variables are too limited to convince a reader about the ability of the method to infer causal relationships in general (and not just statistical). Many of the pairs appear to be of the kind where the putative effect is present in a subset of data items where the putative cause is present (e.g. gender->mustache, those who have mustache are a subset of male) - I'm wondering what role this asymmetry plays in the results? With the current results, the reader is left to wonder if there are other similar asymmetries that could lead to a (potentially non-causal) signal that can be picked by the method?\n",
            "summary_of_the_review": "The paper presents interesting ideas to investigate the relationships between variables learned by a deep generative model, but to make the claim that these relationships are causal, more theoretical justification, intuition, and experiments would be needed.\n\nPOST-REBUTTAL UPDATE:\n\nI thank the authors for their attempt to improve the paper. Nonetheless, I don't find my original concerns resolved, nor answers to my suggestions. Especially the framing of the approach using causal concepts remains obscure and unconvincing.\n\nAs a reply to the authors' specific question on permutation: if two variables are strongly associated but not causal, and the method incorrectly detects a causal signal between the variables (based on the presented experiments it is not inconceivable that this may happen in some cases), then the permutation test will highlight this as highly significant (because the association is removed in the permuted dataset).\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}