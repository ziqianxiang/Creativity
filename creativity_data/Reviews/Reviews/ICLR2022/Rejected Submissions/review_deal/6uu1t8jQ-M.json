{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The authors consider the problem of unconditional image generation in the low-data regime, such as learning from frames of a single video or even from a single image. The main idea is to apply GANs with a specific two-branch discriminator architecture such that the content features and layout features are handled independently. Secondly, to improve the variability of generated images the authors apply diversity regularization. The authors show that the proposed model is able to, to a certain extent, generate diverse high quality samples.\n\nThe paper is well-written, the authors described their method and the evaluation protocol thoroughly and clearly. The reviewers felt that this submission was borderline, with questionable novelty and significance. In an extensive rebuttal and discussion phase the authors addressed several raised challenges and improved their paper. However, two points remain:\n- **Technical novelty**: content and layout separation as well as diversity regularisation previously appeared in many contexts and papers.\n- **Motivation and practicality**: One of the main arguments for the utility of the proposed method is to use it for data augmentation. While it may indeed result in content-based augmentations, it nevertheless necessitates training of a GAN for every single image, which is severely limiting in practice.\n\nAfter reading the manuscript, reviews, and the rebuttals, my view is that the paper is below the acceptance bar and I agree with the points on novelty and significance. In particular, the main application to data augmentation seems to be \"unexciting\" and the proposed method impractical. At the same time the proposed method is a combination of already known techniques, albeit in a different setting. I suggest the authors condense the arguments in the extensive rebuttal to improve the points raised above and resubmit."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an approach to train an image generator using only a short video clip or a single image. The overall framework follows the design of sinGAN that has an unconditional generator that generates new images from random noises, and the generated images and the input real images/frames are fed to discriminators to enforce similar distributions. The key contribution is to disentangle the image into content and layout features, and use different discriminators, which is claimed to reduce the memorization issue.",
            "main_review": "This paper improve the sinGAN on image generation using very few training images. Technically speaking, the proposed solution is interesting, but the technical contribution is not super significant. It is not new to disentangle the image into content and layout parts by restricting the feature dimension -- deep vector for content without spatial information and shallow map for layout without semantic information, similar to the papers such as Swapping Autoencoder. The results look better than sinGAN, but still suffer from large duplicated content and distortion artifacts, but that's understandable for such an extremely challenging problem.\n\nFor the technical details, the paper is written well and easy to understand. I just have a few small questions and comments:\n1. In feature augmentation, what exactly does \"For the single image setting this is done by mixing the features of two different augmentations of the original image\"? What is the different augmentation here?\n2. In feature augmentation again, I am a bit concerned about the proposed augmentation. Take layout as an example, to crop a rectangle of feature to another image does not make a reasonable real image in my understanding, which is equal to crop a random patch from one frame and paste to another frame, the composted image will look strange. I am either not sure about the content augmentation. A discussion could help understand the intuition.\n3. No visual comparisons for many ablation analysis, including DR and FA.\n4. Would it make sense to add another reconstruction loss by combining content and layout features through another decoder to form an autoencoder? It might help enforce feature completeness and make training easier? But I'm not sure.\n5. Are those editing applications in sinGAN (Fig.12) supported?",
            "summary_of_the_review": "Overall I like this paper, which is solid, complete, with sufficient experiments and evaluations. I am okay with acceptance, but I am a bit concerned that the contribution might not be significant enough for acceptance, since it is essentially the sinGAN setting plus content/layout separation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposed a new method for generating new scene compositions using a single video. The method proposes a two-branch discriminator, one for content discrimination and the other for layout discrimination. It proposes a diversity loss for generator to encourage images generated from different latent codes to be perceptually different. Experiments show the method generates diverse and high-quality scene images.",
            "main_review": "The proposed method generates good quality results. However, I think the technical novelty is limited and the task of learning from a single video is much simpler than learning from a single image.\n\n1. The method proposes to learn new scene generation from a short video. Although a short video takes similar access effort as an image, it literally contains at least tens or hundreds images. This task is much easier than learning from a single image. \n\n2.The paper didn't use the temporal information in the video, so what's the difference from a video and hundreds of frames? Also, hundreds of images are often enough for image-to-image translation, e.g. [A] uses hundreds of  frames to learn a render-to-realistic GAN.\n\n[A] Yi et al. Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose. Arxiv.\n\n3. The proposed two-branch discriminator assumed content information is spatial independent, which is strange since spatial distribution also provides much content information. And the method mainly aggregates spatial information and channel information separately, which has also been proposed in previous methods, e.g. [B].\n\n[B] Fu et al. Dual Attention Network for Scene Segmentation. CVPR 2019.\n\n4. The diversity loss is also quite simple, i.e. given two latent codes z1, z2, encourage the generated images G(z1), G(z2) to be different. Shouldn't the distance between two latent codes affect the distance between generated images? In current setting, if z1 and z2 is very close, the loss also encourages the generated images G(z1), G(z2) to be different.",
            "summary_of_the_review": "In summary, the statement of learning from a single video is basically the same as learning from hundreds of images, and is thus much easier than learning from a single image. The proposed contributions, i.e., the two-branch discriminator and diversity loss, lack technical novelty (see above). So I think the paper is not good enough for acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Potentially harmful insights, methodologies and applications"
            ],
            "details_of_ethics_concerns": "The method can generate high-quality images given few data. This method if abused would produce false news and rumors.",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper presents a new GAN model that generates new scene compositions from a single training image or a single video clip.  This is achieved by a two-branches (content-layout) discriminator and a diversity regularization technique. The paper also claims as the first work learning from a single video.",
            "main_review": " \n##########################################################################\n\nPros:  \n1. The paper overall is well-written and easy to follow. \n2. The authors have conducted extensive experiments (quantitative and qualitative) in the paper. \n \n##########################################################################\n \nCons: \n1. [Motivation] The authors worried about training GANs with limited data will lead to memorization of training images (claimed in the abstract). However, the results shown in Figure 1 look like a kind of repetition of patches of the training samples, which conflicts with the motivation. \n \n2. [Motivation] The application of training from a single sample (in the first paragraph on Page 2) is not convincing. Are there any other practical examples to show the practical value of learning from a single sample?\n \n3. [Motivation] The first contribution of learning from a single video is not convincing. Similar to generating a single image by learning from a single image, readers typically expect learning to generate a video by learning from a single video. The authors should provide video results that show temporal coherence. Otherwise, learning from a single video is actually learning from multiple frames.\n \n4. [Approach] Two-branch-like discriminators have been widely explored in GAN. For example, local and global discriminators (Globally and locally consistent image completion. Iizuka et al., TPOG 2017). I think the novelty can be limited. \n \n5. [Approach] Since there is only one real sample during training, I don’t think this two-branch discriminator is able to prevent memorization as claimed on Page 5. I think the effective technique is feature augmentation. Please show the results without feature augmentation and regularization to verify the effectiveness of the discriminator.\n \n6. [Approach] I have trouble understanding the regularization technique in Section 3.2. What does “be more or less equally different from each other” mean? In my view, the regularization technique is actually a maximization of the feature distance of different samples. \n \n7. [Experiments] The results in Figure A is confusing. The results of ”no content branch” also show a worse layout, which is hard to verify the effectiveness of the layout branch and content branch respectively. \n \n8. [Experiments] I suggest ablation studies (by visual comparisons) on the feature augmentation and the diversity regularization, to verify the effectiveness of the proposed model.\n",
            "summary_of_the_review": " \nOverall, I vote for rejecting. I am especially concerned about the novelty of the two-branch-like discriminator, the effectiveness of the discriminator, and the motivation of learning from a single video. Please address the concerns listed in Cons during the rebuttal period.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a single-image/single-vide GAN model that obtains both high quality and high diversity in the generated images, as opposed to previous models that only achieve one of the two. To achieve this, the discriminator consists of two branches that evaluate global layout and quality independently and a novel regularization approach for the generator is introduced. Comparisons to SinGAN, ConSinGAN, and FastGAN show improved performance.",
            "main_review": "This paper introduces a new GAN architecture for unconditional image synthesis after being trained on a single image/video. Based on the observation that previous models either fail to generate realistic layouts (SinGAN, ConSinGAN) or collapse to the training data without diversity (FastGAN) the paper proposes a new discriminator architecture to address this.\nAfter a couple of layers to extract image features, the discriminator splits into two independent branches. One branch is trained to judge the global image layout while the second branch is trained to evaluate individual object quality. Aditionally, a feature augmentation is introduced to further increase the diversity.\nAt the generator side, a diversity regularization is added which forces to generator to generate different images for different latent samples.\n\nThe resulting model outperforms all baselines when comparing the image quality and diversity. Compared to single-image GANs it obtains a much better quality by generating realistic layouts and compared to few-shot GANs it obtains a much higher diversity. A detailed ablation study shows the effects and improvements of the individual novelties.\n\nWhile the approach is intuitive and does seem to lead to clear improvements I wonder how well it translates to applications such as image harmonization/editing/animation/retargeting/etc. While unconditional synthesis is interesting I think many of the more obvious direct applications do not involve pure unconditional generation. Since at least the single-image GANs seem to be able to perform those tasks without or only minor changes in the training pipeline it should be relatively simple to evaluate this approach on some of these tasks, too.\n\nRelated to that: while the diversity regularization in general makes sense I wonder if really forcing the features to be different regardless of the distance in z is the best way of doing. This may affect tasks such as animation or latent space interpolation. Have you tried the \"normal\" way of just forcing the features to be different dependent on the distance in z? I don't see a compelling reason why this shouldn't work, too.\n\nAlso, it would be helpful to have a user study to compare your approach with the baselines. Since the main metrics (to a degree) oppose each other (e.g. SIFID vs diversity), having humans compare the models directly would be helpful. You could either reproduce the user studies from SinGAN/ConSinGAN (compare against training image) or compare the models against each other directly.\n\nFinally, do you have any numbers on the model size compared to the baselines, training time/effort, etc? It seems like your model is trained for many more iterations than SinGAN/ConSinGAN and not in a progressive manner, i.e. I assume it takes longer to train your model?\n\n",
            "summary_of_the_review": "The proposed novleties make sense and seem to lead to clear improvements. However, the evaluation is only based on unconditional image synthesis and ignores many more practical applications for single-image models such as animation, harmonization, retargeting, etc. A more broad evaluation on some of these tasks coupled with a user study would be more appropriate.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}