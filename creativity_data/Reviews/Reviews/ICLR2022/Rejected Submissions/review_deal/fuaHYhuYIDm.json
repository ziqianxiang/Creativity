{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes a global model-agnostic explanation method. The method relies on a neural model that learns to predict which input features are important for the original model’s predictions. Using experimentation, the authors demonstrate that their approach outperforms LIME and Integrated Gradients. They compare the explanation methods in terms of the faithfulness of explanations and computational complexity. While the premise of the work is interesting, reviewers have suggested several areas for improvement: i) writing can use more clarity. writing seems verbose and hard to understand especially in intro and methods section ii) several points have been raised about empirical evaluation including lack of user studies, a more extensive set of baselines and data modalities. Given this, we are unable to recommend an acceptance at this time. We hope the authors find the reviews helpful."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a post-hoc explainability algorithm that is global and model-agnostic. The global comes from training from batches of data while the model-agnostic property is achieved by a gradient estimator.  Experiments show the proposed method outperforms IG and LIME from aspects of sparsity, sufficiency, and time. Some case studies also validate its effectiveness.",
            "main_review": "**Strengths**\n1.\tThe proposed explainer is model-agnostic and fairly efficient, which has great potential in applications. Moreover, such universality is validated by various models, including Random Forest, BERT, and CNN.\n\n**Weakness**\n1.\tI find the paper hard to read, and lots of its parts seem to be wordy and lack clarity. I suggest the authors thoroughly reorganize the lines of writing, especially for the introduction and method section. \n2.\tAs suggested in the introduction part, the novelty of this works partly comes from the global feature of datasets instead of a single instance, which is not new in the literature, e.g., [1]. \n3.\tRegardless of the variety of models in the experiments,  more competitive baselines are necessary to justify the superiority of MAGNEX.  \n4.\tI think more clarifications for the experimental conclusions are needed. For example, why would the sparsity values be the same for all the methods? Plus, it would be nice to provide a more in-depth study about the performance gain of MAGNEX.\n\n\n[1] Parameterized Explainer for Graph Neural Network. Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, Xiang Zhang.",
            "summary_of_the_review": "I think this work is not ready for publication, thus I lean towards rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a global explainers for black-box models. Original\ninputs/features are fed in parallel with the black-box model to an\nexplainer model that is in charge of deciding important scores for\neach feature. Depending on the scores, features are dropped and the\nremaining input is also bed to the original model. The outputs for the\noriginal input and the trimmed version with the explainer's help are\ncompared. The explainer is trained such that the original input and\nthe trimmed version produce similar scores when fed to the original\nmodel. In addition, the sparsity of the trimmed input is maximized.\n\nThe new explainer is compared to LIME and Integrated Gradients in\nterms of how faithful the produced explanations are (how close the\nresults of the original input and the trimmed one are when passed\nthrough the black-box model) and the execution time on three different\ntasks: image classification, sentiment analysis and question\nanswering. The new explainer produces the results much faster and with\nsimilar and sometimes higher quality (as measured by faithfullness).\n\n",
            "main_review": "I think the approach in this paper is fairly intuitive. The results\nare considerably better wrt execution time. One thing that was not\nclear to me: is prediction time for the trimmed input going through\nthe original model included in the total execution time?\n\nIn the introduction, there is a statement that I'm not sure I agree\nwith. Last phrase of the first paragraph: \"Furthermore, explainability\nis an important mechanism to ensure black-box models act fairly and\nwithout bias\". I'm not sure that once we understand model predictions\nwe understand its fairness characteristics. Perhaps undersstanding\nmodel predictions could help with understanding its fairness.\n\nIn Figure 4, it's not clear what the question is.\n\nI think the paper would have a much stronger contribution if a small\nsample of the results for the explainer and LIME would be judged by\nhumans for how helpful they are for the human to appreciate the\nresults of the model. As it stands now, the execution time is the main advantage (albeit, a strong one). \n",
            "summary_of_the_review": "I think this paper introduces a straightforward technique for global explainers that has clear execution time advantages over existing explainers. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a global model-agnostic neural network-based explainer.",
            "main_review": "The proposal of the paper is interesting. However, it has some weaknesses especially in the experimental part that should be fixed before publication. In particular, the evaluation is not convincing because: a) the sparsity metrics takes always the same values for all the methods tested and there are not tests of statistical significance; b) the evaluation is done with respect to two measures optimized by the MAGNEX (it would be odd that it get low results on them). However, it should be tested also with respect to other evaluation measures; c) the main advantage of MAGNEX seems to be the low runtime but this aspect is not the focus of the paper; d) there are no examples of explanations for images or question-answering; e) the approach should be tested also on the most simple data type, i.e., tabular data; f) I recommend adding a large array of baselines such as surrogate decision trees and random forests returning as explanation the features importance, SHAP, Trepan. Finally, I recommend investigating how changes the performance of these baselines when varying instead of assigned to it the same retrieve by MAGNEX.\n\nMinor. Some mission related works:\n- Setzu, Mattia, et al. \"GLocalX-From Local to Global Explanations of Black Box AI Models.\" Artificial Intelligence 294 (2021): 103457.\n- ElShawi, Radwa, et al. \"ILIME: Local and Global Interpretable Model-Agnostic Explainer of Black-Box Decision.\" European Conference on Advances in Databases and Information Systems. Springer, Cham, 2019.\n- Spinner, Thilo, et al. \"explAIner: A visual analytics framework for interactive and explainable machine learning.\" IEEE transactions on visualization and computer graphics 26.1 (2019): 1064-1074.",
            "summary_of_the_review": "Experiments are not sound nor convincing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a global model-agnostic explanation method. The method relies on a neural model that learns to predict which input features are important for the original model’s predictions. Through substantial experimentation, the authors demonstrate that their approach outperforms LIME and Integrated Gradients. They compare between explanation methods \nin terms of the faithfulness of its explanations and in computational complexity.\n\nThe main contribution of this paper is MAGNEx, a neural model-agnostic explanation method that is substantially faster than popular methods on high-dimensional inputs.\n",
            "main_review": "This is an interesting paper that discusses an important and timely topic, generating fast and reliable model-agnostic explanations. The authors propose a novel and interesting approach that compares favorably with popular methods, and present experiments across many modalities and tasks. They also discuss and analyze the methods performance, and provide a detailed review of their technical approach.\n\nWhile I do find the paper useful and clear, I have some issues that I would like to see addressed for it to be published. \n\nFirst and foremost, I find the lack of any discussion about causal explanation and especially counterfactuals to be highly problematic. Masking parts of the input and observing the model’s output does not causally explain the model’s decisions, even if the prediction of the model is the same.\n\nThe authors note that they intentionally do not compare to humans as we should care only about what the model is doing. However, they do not extend this reasoning to trying to causally estimate how decisions are made. I recommend the authors address the causal explanation literature, and explain why masking tokens or replacing existing pixels with black pixels is sufficient in terms of counterfactuals [1,2,3].\n\nSecond, I don’t see how is Figure 3 helpful for analysis. As I read it, this seems to be just the authors’ opinion regarding the model’s internal state, without any comparison to counterfactuals. Also, MAGNex mostly highlights “great” in this particular example, so I don’t fully agree with the intuitive explanation. Same critique stands for Figure 4 as well.\n\nThird, the empirical results are not as convincing as the authors claim. Apart from time, MAGNEx doesn’t seem to be improving much in the first three experiments, and the fourth has huge standard deviations. In the BERT experiment, if tested on significantly shorter sentences, even the time advantage might not be true. Also, the CNN experiments should be done on a substantially more challenging dataset\n\n\n[1] Explaining Classifiers with Causal Concept Effect (CaCE)\n[2] Causalm: Causal model explanation through counterfactual language models\n[3] Amnesic probing: Behavioral explanation with amnesic counterfactuals\n",
            "summary_of_the_review": "While I do find the paper interesting and find some of the contributions meaningful, I have some issues with the paper in its current form. First and foremost, I find the lack of any discussion about causal explanation and especially counterfactuals to be highly problematic. Second, the qualitative analysis of the output of MAGNEx compared with LIME is unconvincing. Third, the empirical results are not as strong as the authors claim, apart from the important and substantial gain in time efficiency.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}