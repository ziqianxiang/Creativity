{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes the notation of DB variability, which is essentially prediction variance. It is also closely related to algorithmic stability which is a theoretically more sound notation to derive generalization bounds. The paper is a mixed bag of empirical observations and \"theory\". However, looking at the \"theoretical results\" in the paper, it is clear that the authors lack adequate theoretical background. \n\nI'd be more positive if the paper has been focused more on the former, and could be judged by the empirical part only. While the reviews were positive, I looked at them and realized that similar to the authors, the reviewers also lack theoretical backgrounds. \n\nFirst, the fact large variance implies a generalization lower bound is trivial, to the degree it is not worth stating as a \"result\". Second small variance implies a generalization upper bound isn't true. One can have a predictor that perfectly overfits the training data and predicts class 0 everywhere else. This has small prediction variance but poor generalization. In this context, the upper bound analysis of the paper is clearly misleading. Usually one compares training error to generalization error, where the estimator depends on the training set. In such case, one cannot use the simple argument of the convergence of the empirical mean of sum of independent random variables to the mean due to the dependency of estimator on the training set, e.g. in Thm 3. One needs to use uniform convergence and exponential probability (instead of Chebyshev) inequality to obtain such results. The right hand side of Thm 3 (the theorem itself is also very poorly stated. and shouldn't be allowed to be published) could not be interpreted as training error as should usually be the case for such bounds, but only as validation error. Such a result (comparing validation and test error when distribution isn't changed) has no value. \n\nI would not elaborate on other similar issues.  My recommendation is to focus on the empirical study if the authors are not familiar with theoretical analysis."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Decision boundary variability is measured in two ways. One method depends on the algorithm, or the random seed and reflects how much the boundaries change wen retraining the same network on the same data. The other technique reflects the variability across different amounts of training data. Generalization bounds derived from these two quantities are presented.",
            "main_review": "The notions introduced are rigorous and prove useful. The theoretical claims are supported by some empirical work as well. The bounds do not depend on network size and apparently reflect the behavior of practical and popular networks.\n\nSeveral bounds are presented, and while it is stated that they do not depend on model size -- implying other existing bounds do -- there is no comparison presented. When would these bound be more useful than existing ones? When are other be more appropriate (or tighter)? If these questions seem misguided, perhaps a short discussion of why the bounds derived do not need to be compared to existing ones would be helpful.\n\nI admit I am not so familiar with this domain and may have overlooked something.",
            "summary_of_the_review": "The derivations of new bounds that utilize the proposed decision boundary variability measures are strong. The claims are supported by experimental results where appropriate. This paper would be even stronger with more detail/discussion about existing work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors introduce the notion of decision boundary (DB) variability and develop generalization bounds in terms of this.  They consider two notions, that of 'algorithm DB variability' and 'epsilon-eta DB variability', the latter a generalization of the former.  They show upper and lower bounds for the risk of classifiers in terms of DB variability under a variety of assumptions, as well as experimentally verify some of their claims.  ",
            "main_review": "\nOverall, I think the paper introduces a number of interesting potential ideas for explaining generalization in neural networks.  But my initial read leads me to a number of questions and concerns before I can more confidently back this paper for acceptance; I've listed a number of them below.\n\n\n(1) For the central 'algorithm DB variability' claims, the primary experimental validation relies upon the three algorithmic variations of \"standard training w/ data augmentation\", \"adversarial training\", and \"standard training without data augmentation\".  My understanding is the primary purpose of these set-ups is to produce a plot like in Figure 2(c), comparing generalization of fixed models but changing algorithmic procedures, so as to discern a relationship between generalization and DB variability.  I feel the strength of this message would be improved if a small number of additional experiments further tested this hypothesis.  Some possible ideas: using different early-stopping schedules, or using progressively larger data subsets (e.g. using 10,000, 20,000, 30,000 training samples).  This would allow for a fixed model but a change in other parameters that can affect test accuracy.\n\n(2) The usage of BigGAN images to estimate variability seems tolerable but it would be best if there were some additional confirmation that the obvious alternative -- data subsetting, say keeping 5,000 random samples from the training set as a new test set and not using these in the training process -- would result in similar trends.\n\n\n(3) What is the intuition for why Assumption 3 is needed?  Can the authors give a verbal description of how this places a role in the proof in the main section?\n\n\nSome minor points/typos etc.:\n\np.1 and a few places: the notion of \"margin\" seems a bit muddled throughout.  The margin of a classifier f(x) is the largest distance gamma s.t. y * f(x) >= gamma.  The authors seem to be referring to the margin at a point x in terms of the largest rho s.t. y * f(x') has the same sign as y * f(x) for all ||x-x'|| <= rho; these are closely related only if f has a small Lipschitz constant as a function of x.  It is definitely the case that large margins in the former sense have good generalization performance (vs. authors' suggestion in p.1); this is shown in standard statistical learning theory.  But adversarial robustness  (the latter sense of margin) does not imply good generalization. \n\n\np.4, Sec. 4.1: Soudry et al. (2018) isn't really about neural networks; better references would be Lyu & Li (2020) and Ji & Telgarsky (2020) not 2018a (\"directional convergence and alignment\").\n\np.4: in Fig. 1, the authors say (c) plots DB variability as a function of sample complexity, but x axis is nonlinearity level?\n\np.7: Assumption 2 is missing an \"=j\" inside the second expectation\n\nAppendix A.1: what adversarial training method is used?\n\nAppendix B.2: how does Def B.1 differ from eta, epsilon DB variability?\n\nAppendix C: the E^2() notation is weird; does this mean [E()]^2 or E[()^2]?\nHow does one go from (50)-(51)?   Not clear on first read.\n",
            "summary_of_the_review": "Overall, I think the paper introduces a number of interesting potential ideas for explaining generalization in neural networks.  But my initial read leads me to a number of questions and concerns before I can more confidently back this paper for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies that the smaller variability of prediction can provide a better generalization in empirical and theoretical foundations. Considered examples and theories are solid, and the intensive studies reveal the relationship (correlation not causation) between the variability of prediction and generalization.\nThe paper considers two types of variability with respect to algorithm and training data. In any case, lower variability ensures higher performance in generalization.    ",
            "main_review": "The proof and empirical results are sufficient to show the claim. All claims are clarified and clear to understand, the organization of the paper looks good. \n\n(1) I have some issues concerning the theoretical aspects.\n \n1. Assumptions 2--3 can be sound. These conditions are essential for the proof. When these conditions break down, what will we observe? In the binary classification, assumption 3 means the correct classification on average for any $(x, {\\bf y}) \\sim \\mathcal{D}.$  \n2. Can you clarify the situation when the assumptions 2--3 are weakened? \n\n(2) In addition, simulations studies to validate the claims are limited since we cannot know the causality between prediction variability and generalization due to various confounding effects. If you consider the causal graph to address this issue, the claim can be strong. In this case, I can only know the correlation, not causation.  \n\n    \n    ",
            "summary_of_the_review": "There is a valuable empirical analysis for the prediction variability. However, the more realistic cases should be considered.\nAdditionally, to validate the claim of the proposed, assumptions 2--3 should be checked in experiments.     \n",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes to characterize the generalizability with the variability of decision boundary instead of the margin. They show empirically that generalizability is negatively correlated with the variability and theoretically bounded by it. ",
            "main_review": "My main concern about this paper is how well the findings hold in a more realistic setting, given the strong assumptions made in the limited experiments and several confusions in the theoretical part. \n\n1. Although the authors see a certain level of negative correlation between the decision boundary (DB) variability and generalization, the variability may not be a reliable characterization of generalization unless the following confounding factors are well addressed:\n\n   * Training steps: models trained with different numbers of steps may have different variabilities but the same generalizability. for example, the claim that algorithm DB variability is zero on the linearly separable data is not true unless SGD takes infinite steps and other assumptions hold [1]. So training with different numbers of training steps, the variability would change. While after a certain amount of steps, which is often observed empirically, the generalization gap stops changing. In this case, the variability does not well reflect the generalization - more experiments are needed to understand this.\n\n    * Label noise: continue with the training steps, if there exists some level of label noises and consequently epoch-wise double descent [2], the connection between training steps and generalization is even more complicated. Does the DB variability still characterize the generalizability?\n\n    * Factors that may increase the DB variability while improving the generalization: there are cases where instabilities of the training process leading to better generalization, for example, dropout and large learning rate [3]. Do such instabilities also lead to larger DB variability? If so, why is better generalization achieved? \n\n    * The current observations on the negative correlation between DB variability and generalizability can probably be both explained by over-parameterization. And in the other cases like those listed above, it is unclear whether the correlation still holds.\n\n2. The quantification of the DB variability depends on the data generating distribution, which is however vaguely defined in the paper. For the two-moon dataset, the data generating distribution seems to be the $\\mathcal{R}^2$, and the authors empirically selected the rectangular grid in their experiments. But on the Cifar-10 dataset, the author picked the images generated by BigGAN as the data generating distribution, where the decision boundaries off the data manifold are excluded. \n\n3. The authors propose two factors, simplicity effect, and constraint effect, to account for the algorithm DB variability without rigorous definitions which have led to confusion. For example, when discussing the adversarial training under section 4.1, the paper says \"which (adversarial training) substantially decreases the inter-class distances and makes the training set more complex\", the authors seem to confuse the two factors into the same phenomenon. It is not convincing to me that significant the difference between these two factors is.\n\n4.  The motivation of the data DB variability is not clear to me. The authors motivate the data DB variability as \"the algorithm DB\nvariability hardly shows the decision boundary variability caused by changes in training data\". Why is that important in characterizing the generalizability? And the paper lacks the comparison between the bounds built upon the two variability metrics. It is not clear to me when one is more useful / tighter than the other and should be used.\n\nMinor:\n\n1. \"Remark 1. (1) If a input...\" -> \"Remark 1. (1) If an input...\"\n\n[1] Soudry, Daniel, et al. \"The implicit bias of gradient descent on separable data.\" The Journal of Machine Learning Research 19.1 (2018): 2822-2878.\n\n[2] Nakkiran, Preetum, et al. \"Deep Double Descent: Where Bigger Models and More Data Hurt.\" International Conference on Learning Representations. 2019.\n\n[3] Li, Yuanzhi, Colin Wei, and Tengyu Ma. \"Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks.\" Advances in Neural Information Processing Systems 32 (2019).",
            "summary_of_the_review": "For the reasons listed above, I think substantial experiments are required to better understand the phenomenon and confirm the empirical findings, rigorous definitions and clarifications are needed in the theoretical sections, and thereby my current rating.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}