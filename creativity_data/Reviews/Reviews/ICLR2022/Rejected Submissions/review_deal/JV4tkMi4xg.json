{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper considers the problem of black-box optimization and proposes a discrete MBO framework using piecewise-linear neural networks as surrogate models and mixed-integer linear programming. The reviewers generally agree that the paper suggests an interesting approach but they also raised several concerns in their initial reviews. The response from the authors addressed a number of these concerns, for instance regarding scalability and expressivity of the model. However, some of these concerns remained after the discussion period, including doubts about the usefulness for typical applications in discrete black-box optimization and some concerns about the balance between exploration and exploration. \n\nOverall the paper falls below the acceptance bar for now but the direction taken by the authors has some potential. I encourage the authors to address the problems discussed in the reviews before resubmitting."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a method using piecewise-linear neural networks as the surrogate model (and the acquisition function) in a black-box optimization framework. At each step, the acquisition function is optimized by casting the learned neural network (and a set of constraints excluding the already-visited data points) into a mixed integer linear program, which can be then solved using off-the-shelf solver. The proposed method is empirically evaluated on a number of unconstrained and constrained tasks. \n\n\n\n\n\n\n\n\n",
            "main_review": "### Strong points\n- The declarative nature of the proposed method eliminates the need for developing an algorithm for solving the inner loop optimization (as long as the structural constraints can be represented in MIP formalsim). It also facilitates to adpating an existing method to similar variations by simply modifying the constraints. \n\n### Aspects to be improved\n- The choice of a piecewise linear neural network for both the surrogate model and acquisition function is not properly motivated. In particular, it is not clear how the proposed approach maintains an exploration-exploitation balance. \n\n- The proposed approach does not demonstrate significant improvements in empirical evaluations. In the unconstrained setting, it is outperformed by the baseline, and in the constrained setting, solving the inner loop problem to optimality does not seem to provide an advantage. The argument of \"ease of implementation\" would have been convincing if implementing the alternative (evolutionary) approach was prohibitively difficult. But this does not seem to be the case. \n\n- The proposed approach relies on solving a MIP problem repeatedly. This does not allow the method to be applied to problems with more than a certain number of variables. The problems studied in the  experiments have few number of variables. In particular, *TfBind(8, 4)* can be probably solved by simple enumeration. It is not clear to what extent the proposed method is applicable to larger problems. \n\n### Question\n\n- When comparing different methods, each algorithm is evaluated in terms of the best reward observed after 1000 queries. At each iteration, the MIP solver is given a 500 seconds timeout, and the evolutionary aglorithms is given a budget of 10k queries. Does this give the inner loop algorithms equal opportunities for finding good solutions? Isn't it more fair to compare the methods subject to an equal overall time budget?\n",
            "summary_of_the_review": "The paper presents an interesting direction for using a declarative framework (i.e. mixed integer linear programming) to encode combinatorial structures in black-box optimization. The proposed method lacks a theoretical motivation, and the empirical evaluation does not demonstrate significant advantages over existing methods. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper develops a technique to optimize an unknown, black-box function \"f\" by leveraging a combination of neural networks with mixed-integer linear programming (MILP) methodology. More specifically, authors encode an approximation of \"f\" using a neural network with piecewise-linear activation functions, which is optimized using its associated reformulation as an MILP with no-good cuts. Numerical results evaluate the approach with respect to other baselines and neural network optimization mechanisms. ",
            "main_review": "Overall, the paper is well-written and suggests an interesting and relevant approach to address black-box optimization problems equipped with discrete domains. I found the basic framework to be well thought-out and, in my view, of potential value for a large array of settings within this field.\n\nMy major concern, however, is that many design choices are somewhat unclear, and the paper often feels that lacks depth in more specific areas. In particular:\n\n(1) Optimizing inputs with neural networks is challenging with MILPs, often requiring more sophisticated implementations as that of Anderson et al., 2020 (as nicely emphasized by the authors). I wonder whether the fact that the authors were limited to a very simple network, with only a single layer, did not hinder some key insights on the numerical evaluation? For instance, wouldn't the function approximation be quite poor for larger instances? Also, is this approach really scalable?\n\nPerhaps my suggestion here is for authors to consider encodings that are much more efficient/scalable for MILPs (and other model-based approaches). For example, many black-box functions have a natural discrete structure, and authors could consider a decision tree, which is much \"simpler\" to optimize.\n\nSimilar reasoning applies to the \"no-good\" constraints, which in this case are quite simple and only eliminate one solution at a time. This could really impact MILP performance (e.g., similar to combinatorial Benders cuts). Would there be cases where it is possible to eliminate several points from the acquisition domain? For example, suppose \"x\" are binary and represent subsets. If you eventually learn all subsets of size <= N, you could eventually replace those constraints by \\sum_i x_i >= N + 1.\n\n(2) The numerical experiments do not seem to reflect well the benefits of the approach. To the best of my understanding, the final conclusion is that problems are \"easier\" to model with NN+MILP, but performance improvements are marginal (if any). I believe this is indeed the case, but the paper lacks more concrete evidence of this statement. \n\nIn particular, when comparing NN+MILP and NN+ConEvo, why not consider a problem class that the MILP has clear benefits? For example, a set-packing/set-covering acquisition domain, perhaps with other side constraints, or a scheduling feasible set (e.g., \"f\" could encode a weighted completion time with unknown weights, and the feasible set are the valid sequences). The authors could compare with any global optimizer (as opposed to RejSample and ConEvo)  because it is well-known that MILP is one of the state-of-the-art techniques for these problems.",
            "summary_of_the_review": "The ideas are novel and significant, especially given the modeling expressivity provided by MILPs. However, the paper lacks some justification concerning the scalability of the approach and the fact that neural networks had quite a limited size. Moreover, in my view, the numerical experiments do not explore well the benefits of their approach.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper solves a constrained discrete black-box optimization problem that employs a surrogate model in modeling an unknown objective function. Unlike the formulation of standard Bayesian optimization, it constructs a surrogate model using a piecewise linear neural network. Under the assumption that a randomly-initialized neural network is able to produce an uncertainty for exploration (against exploitation), the proposed method optimizes a surrogate model directly with mixed-integer linear programming. It follows a spirit of Thompson sampling. Finally the authors conduct their method on several experimental circumstances and show the validity of their method.",
            "main_review": "### Reasons to Accept\n\n+ This paper is well-written and well-organized.\n+ It solves a very interesting problem that contains constraints on discrete search space.\n\n### Reasons to Reject\n\n- I do not think that a piecewise neural network models an uncertainty of unknown objective appropriately.\n- Following the point described above, even though we train a neural network every iteration, it tends not to reflect a factor for exploration, which implies that regression results are almost same where the same observations are given.\n- GP is a popular choice for a surrogate function, since it has sufficient expressiveness, which is defined on a RKHS space. I am curious that the surrogate function used in this paper is sufficiently expressive of unknown function.\n\n### Questions to Authors\n\nPlease answer the comments described in Reasons to Accept and Reasons to Reject.\n\n1. Can I ask what the difference between no-good constraints and generic equality constraints is? If they are similar, it can be considered in a continuous search space?\n1. Is there any specific reason not to compare the proposed method to COMBO? In my experience, COMBO can be thought of as the state-of-the-art model now. To cope with a no-good constraint, you can just apply a rejection strategy in a step of acquisition function optimization (i.e., local search with rejection sampling).",
            "summary_of_the_review": "It solves a very interesting topic which is defined on a discrete search space with constraints. However, the choice of surrogate model is not convincing and the baseline is missing. Thus, I would like to recommend rejection.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors present NN+MILP, a framework for the optimization of an expensive to evaluate blackbox fuction with a discrete combinatorially constrained domain. The acquisition problem of finding the surrogate minimum is solved to global optimality by solving a MILP formulation of the acquisition problem. The MILP formulation limits the considered neural network surrogate class to networks with piecewise linear activation functions. However, it provides a simple declarative language for integrating the problem-specific constraints. \n\nThe experiments cover both the cases of unconstrained and constrained optimization of the acquisition function. \nThe unconstrained case compares NN+MILP to general purpose algorithms for unconstrained discrete blackbox optimization. It shows that the global optimization often achieves better results than a local search evolution based method for solving the acquisition problem, while also solving the inner loop problem faster. Additionally, it demonstrates that even with the restrictive function class of a single layer neural network, comparable performance to better-suited surrogate hypothesis classes can be achieved, thanks to the global optimization. \nIn the constrained case artificial subset-equality constraints are used to create a combinatorial domain for the blackbox function. NN+MILP performs similar to NN+ConEvo, a manually adjusted local search method that ensures feasibility of the proposals at every step. Other methods employing random search for inner loop optimization or local search with a different surrogate hypothesis class perform much worse.\nFinally, a case study for the NAS-Bench-101 benchmark is provided. A novel MILP formulation for cells of a valid architecture design is described, which consists of an MILP formulation for directed acyclic graphs with added null operations to allow DAGs with a reduced number of cells. Despite the generality of NN+MILP it outperforms a strong evolution based baseline.",
            "main_review": "# Strengths\nThis paper tackles an interesting problem of optimizing expensive blackbox functions even in the case of complex combinatorially constrained domains, by using a MILP formulation of a piecewise linear neural network. Even though such formulations have been used before (the authors mention neural network certification as an example), it offers a very flexible way of easily integrating combinatorial constraints, without handcrafting a local search method to maintain feasibility of solutions to the acquisition problem. The experiments show, that\n1) solving the acquisition problem to global optimality can provide benefits over local search methods even in case of unconstrained domains\n2) performance of handcrafted methods to ensure proposal feasibility in the constrained case can either be matched (NN+ConEvo in RandomMLP with subset constraints) or surpassed (RE in NAS-101), with a simpler way to implement the constrained domain in a general framework.\nWith the flexibility of the framework this can be used for various other future problems that involve MBO in the presence of combinatorially constrained domains.\n\n# Weaknesses\n### Limitations of surrogate model\nThe surrogate model is limited to a) neural networks with piecewise linear activation functions, this is inherent to the MILP formulation and restricts the framework from using potentially better-suited surrogate hypothesis classes such as Random Forests.\nThe surrogate model is also limited to b) a relatively small number of neurons in the network due to runtime limitations of the MILP. The dimensionality and number of constraints in the MILP formulation scales linearly with the number of nonlinearities in the network and as mentioned in the paper, the runtime scaling of the MILP solver is then often unpredictable. \nThis means that the runtime comparison between methods is a crucial factor in the comparison, which leads to some questions.\n\n1) In section 4.4 the authors reported the inner-loop optimization runtimes for the unconstrained case. To me it is very surprising that the local evolutionary search RegEvo has a larger average runtime than the global MILP optimization. It would be great if the authors could provide intuition on why this is the case. Additionally, is this also the case when comparing MILP to ConEvo in the constrained case, where those two methods are the main competitors? As the constrained case is the one this framework is designed for, this would be the more important comparison than in the case of an unconstrained domain.\n2) In the experiments a single hidden layer with 16 neurons is used, and ablations with two slightly bigger architectures (32 and 16+16 neurons) are provided. While the slightly bigger architectures are still within the computational limits, the network probably cannot be scaled to much larger (deep) networks. In the ablations it is stated that for the TFbind8 experiment larger architectures did not improve performance, which suggests that the small surrogate model is already expressive enough in this case. It would be interesting to know whether this is also the case for a larger experiment such as the NAS case study. \n\nIn any case, the runtime restriction could be a limiting factor in future applications, where more complex surrogates could be required, giving faster but less optimal local search methods an advantage over the global optimization.\n\n### Integration of continuous variables\nIn the current stage the framework doesnâ€™t support mixed-integer domains because the no-good constraints do not naturally extend to this case, as stated in the conclusion. Therefore, in its current stage, the method should be called NN+ILP instead of NN+MILP, as the name otherwise suggests that mixed-integer domains can be used.\n\n\n## Typos\nThese do not affect my rating.\n1) Before section 2.2: â€˜areâ€™ should be removed",
            "summary_of_the_review": "Strengths: The presented framework is flexible and easier to implement than other methods in the presence of combinatorially constrained discrete domains. The experiments show that the performance is also competitive with other approaches.\n\nWeaknesses: The MILP formulation limits the hypothesis class of models to neural networks with linear activation functions, runtime limitations restrict the number of neurons in the network. Mixed-integer domains are currently not supported by the framework, even though the name suggests otherwise.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}