{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes a multi-scale fusion self attention model for phrase information, which incorporates convolutional models into self-attention to explicitly handle word-to-phrase correlation. This is paired with a sparse masking strategy to balance between word-to-word attention and word-to-phrase attention. The model achieves good performance on downstream tasks.\n\nWhile the proposed method is simple and looks effective, reviewers have expressed concerns with lack of novelty (see the suggested missing references), lack of clarity in the experimental details, and unclear writing. Unfortunately, there was no response from the authors, which makes me recommend rejection. We urge the authors to follow the reviewers' suggestions in a future iteration of their work."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper incorporates convolutional models into self-attention to explicitly handle word-to-phrase correlation, paired with a sparse masking strategy to balance between word-to-word attention and word-to-phrase attention. The model achieves good performance on GLUE and RE tasks.",
            "main_review": "Strengths:\n\nA simple method that uses convolutional model to enhance the localness of self-attention and phrase-level learning.\n\nWeaknesses:\n* Adding convolution into self-attention and capturing phrase information have been well studied before;\n* The proposed dynamic masking strategy has flaws;\n* Writing needs improvement;\n* Experimental details are not always clear;\n* Ablation study is incomplete and comparison should be enhanced.\n\n\nDetails:\n1)\tAdding convolutional models and capturing phrase information for self-attention has been well explored in the context of machine translation. Take the following two papers as an example:\n[1] Yang et al., Convolutional Self-Attention Networks [2] Hao et al., Multi-Granularity Self-Attention for Neural Machine Translation These existing studies reduce the novelty of this paper. Also, a direct comparison with these studies is required.\n2)\tThe masking strategy is essentially another attention layer, but its formulation indicates that no gradient will be back-propagated into W^{Q_1} and W^{K_1} as shown by Eq. 10, 11, 12, 13. The authors don’t explain how to optimize them.\n3)\tWriting needs improvement. In particularly, logistic should be improved.\n4)\tHow did you implement your convolutional model? Did you apply a vanilla convolution or depth separable convolution, or dynamic convolution?\n5)\tThe proposed model is used for fine-tuning after BERT encoding. One noticeable thing is that, although BERT outputs representation for each word, its encoding is fully contextualized. To large extent, BERT encoding contains sentence-level information rather than simply word-level. The authors need more stronger motivation to tell the necessity of convolution in self-attention.\n6)\tOne ablation is missing. What if you stack another vanilla attention layer for finetuning without convolution? This should be added for comparison.\n",
            "summary_of_the_review": "In short, the authors propose convolutional model for self-attention, which is lack of novelty. The method also has flaws, and the experiments are not very convincing.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The manuscript presents a multi-scale self-attention method for NLP tasks. The aim is to better extract phrase- and word-level features. The main contribution of the proposed method is to apply different kernel sizes for feature extraction and multi-scale attention fusion. Additionally, a mechanism called dynamic sparse module is applied to adjust the weights of the obtained attention matrix.",
            "main_review": "The contribution of the manuscript is minor and the novelty of the proposed method is marginal. The motivation of the proposed method is not convincing. There are many existing methods that can extract phrase or context information for NLP tasks. \n\nFor multi-scale feature extraction or attention fusion, there are also many existing studies. The proposed method just simply use different convolutional kernels for the task. It does not provide many new insights to the community.\n\nThere is not a deep analysis of the existing literature. The introduction and related work section should be completely rewritten. The information provided in the current shape is very shallow. Many state-of-the-art studies are missing. \n\nThe experimental results cannot fully support the proposed method. The authors should compare the proposed method on more NLP benchmarks with a comparison with the SOTA methods. For example, the most recent methods used for comparison were published in 2020 in Table 1.\n\nAs the proposed method is a generic module. It should be evaluated on various NLP tasks.",
            "summary_of_the_review": "Overall, the proposed method is not good enough for publishing in top tier conferences such as ICLR. The contribution and novelty of the proposed multi-scale self-attention method are marginal. The experimental results cannot fully support the proposed approach.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a new network architecture based on a transformer. The basic idea includes\n1) Extract phrase information using a convolution operator and compute the attention between phrases and words\n2) Learn to predict a mask for word-word attention to turn off word-phrase attention when word-word attention is high\nEmpirical results show that the proposed method improves the baseline BERT model when being applied to the features extracted by BERT.\n",
            "main_review": "Despite being motivated by the attention between phrase and word, the empirical and technical novelty is limited. \n1. The proposed method essentially adds additional transformations (and therefore parameters) in the transformer architecture, so the performance improvement is expected. It is unclear whether the proposed method is indeed better when it has the same number of parameters and operations as the baselines. \n2. The idea of combining convolution and transformer architecture has been widely adopted in prior works.\n3. It is not clear whether modeling phrases explicitly is indeed helpful, especially in a deep neural network. The model may already capture the phrases implicitly.\n\nTo improve the paper, the authors should try to\n1. Verify that the proposed architecture is indeed superior to alternative design when having similar number of parameters and operations (e.g. what's the performance w.r.t model size, can the proposed model be applied to the backbone network to achieve better performance?)\n2. Show that the superior performance is not simply the result of more parameters (e.g. what happens if we set k=1 and increase the number of filters)\n3. Show evidence about why modeling phrases explicitly is important and the proposed method indeed capture the phrase information\n4. Explain why existing model is not sufficient for capturing phrase information\n",
            "summary_of_the_review": "This paper falls short of showing the benefit of the proposed method. It is unclear whether the new architecture is indeed better than existing alternatives both theoretically and empirically.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work proposes a multi-scale fusion self-attention module to help extract phase-level at different scales. It utilizes convolution operations with kernels of different sizes to achieve this end. The authors conduct experiments on the relation extraction and GLUE tasks to demonstrate its effectiveness.",
            "main_review": "I think the novelty is limited. Using convolution operation with different kernel sizes to extract information of different scales is quite general, e.g., googlenet [1]. It seems it merely transfers the idea from multi-scale context information modeling on some vision tasks to sequential modeling tasks. \nBesides, I think simply convolution operation with different kernel sizes can help to extract contextualized information. However, it seems it can not guarantee extract phase information. \nAs claimed in the paper, the dynamic sparse module is proposed to realize the end that if a word chooses to pay attention to a phrase, it should reduce the attention to each word in the phrase. However, this work simply sets the attentional values that are smaller than a threshold to 0 to realize this end. I do not why this mechanism can realize the above-mentioned end. Besides, the contribution of this point is not verified.\n\n[1] Szegedy et al., Going deeper with convolutions, in CVPR, 2015.",
            "summary_of_the_review": "I think the novelty is limited and some explanation is not clear.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}