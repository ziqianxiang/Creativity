{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "While I understand and have empathy with the authors' viewpoint of their work and novelty, this unfortunately has not reached reviewers' hearts in the way that they intended. There has been no strong support for acceptance, as the questions about the amount of novelty piled up. Some interactions between authors and reviewers happened. It is clear that the authors made an effort to show the differences with respect to other tree-related models and algorithms, as well as to highlight the strengths of the approach which was claimed to be too similar and simplistic. I believe the interactions helped with improving the first viewpoint of reviewers, however this improvement has not been enough, as reviewers did not significantly changed their stances. This is a short process and indeed it is not easy to change first impressions. If anything to add is that I hope that the impressions can be used to give a new presentation to the work that will enhance the work and its view by others."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper studies the task of network embedding. The authors propose a Graph Tree Networks (GTree), a deep graph neural network architecture that originates from the tree representation of the graphs. In particular, the proposed approach conduct message passing upward from the leaf nodes to the root node to update the root node’s hidden features. Experiment validates the effectiveness of the proposed model,",
            "main_review": "### Strength\n\n- The idea of utilizing the tree representation is generally sound.\n\n\n### Weakness\n\n- From the second paragraph, the authors seem to make their tale by pointing out the complexity issue in the existing models (e.g. \"time complexity of O(k|E|)\", \"could be a too large number\" ). However, there is no interpretation on how their proposed model can address this issue. Particularly, the complexity of the proposed model is still in the same order as the vanilla GCN and GAT. And in the experiment, they only evaluate their proposed model in terms of effectiveness, which is hard to give a good end to the tale in the introduction.\n- The approach section is pretty chaotic. I am really hard to follow the key idea of their proposed model, and the model formulation is not tidy enough. E.g. Equation (5) and (6) could be more concise.\n- In the conclusion, the authors further propose the Heterogeneous Graph Tree Attention Network (HGTAN) with its formulas. But this proposed model hasn't been validated by any means, and should not be in the conclusion section. Instead, the conclusion section should focus more on the Graph Tree Network and the conclusion should connect to the experimental result, rather than some subjective statements, such as '\"We believe xxx\".\n- The writing is a critical issue for this paper. There are too many grammatical issues throughout this paper. And many statements lack their justification. E.g.Section 2, the first sentence, \"Generally graph learning based on direct neighbors only does not yield the best performance.\" needs a justification.\n- I don't think message passing through tree structure nodes would be a general scheme for all the nodes, since some nodes may not have any hierarchical structure. The analysis on which types of nodes could benefit more from the proposed tree-based message passing scheme should be added to further validate the generalization ability of the proposed model.\n- \"self-interpretive \" has not to be interpreted and justified in the paper.",
            "summary_of_the_review": "- The writing of the paper should be significantly improved.\n- The motivation is not well stated in a good tale and the proposed model cannot address the issues stated in the motivation.\n- Many statements are too subjective and are not be well justified.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a message-passing formula derived from the tree representation of a graph.\nHowever, the proposed method has no innovation.",
            "main_review": "I see no difference between the classical message passing scheme and the tree structure proposed in this paper.\nThe classical message passing scheme can also be described by a tree where each node's neighbors are its children.\n\nThe specific formula proposed by the paper is almost the same as APPNP [1]\n\n[1] Predict then Propagate: Graph Neural Networks meet Personalized PageRank. ICLR 2019",
            "summary_of_the_review": "This paper has no novelty.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors propose a novel neighborhood aggregation technique for GNNs based on the tree representation of the graph. Their proposed methods GTCN and GTAN pose comparable time complexity to that of GCN but empirically yield better results on various popular benchmarks. ",
            "main_review": "Strenght: \n1. The model architecture allows the network to encode path information crisply.\n2. The Graph tree network is more expressive than APPNP.\n\n\nWeakness\n1. I find the empirical evaluations are somewhat limiting.",
            "summary_of_the_review": "Questions/ Clarification/ Suggestions:\n1.  I understand that the time complexity is comparable with GCN, however, I am still curious to know about training time (per epoch) for al the proposed models. \n2. Open Graph benchmark (https://ogb.stanford.edu/docs/leader_nodeprop/) provides a wide variety of tasks and associated datasets that are more interesting than these benchmarks. I would encourage the authors to apply their model to some of them.\n3. Why does the performance degrades significantly when a nonlinear layer after each propagation layer is added? The authors have never addressed that in the paper. I think pressing a bit more along that line might lead to a better interpretation of the message passing of the model.\n\nI would be happy to increase my score if some of the above-mentioned concerns are properly addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In the paper, a self-interpretive graph learning architecture is presented based on the tree representation of graphs.The message passing scheme is natural and intuitive. Two models, Graph Tree Convolution and Graph Tree Attention Network, are given with SOTA performance. ",
            "main_review": "This paper provide two deep learning models to dealing with the tree representation of the graphs. \n\nStrength: 1. The message propagation is clear and explanable, which is dfifferent from conditional vanilla CCN， GAT and other dervatives.  2. The method is easy to follow and intuitive. 3. The performance reaches that of SOTA.\n\nWeakness:  1. The proposed method is intuitive and straightforward. So the contribution of the paper is unclear. ",
            "summary_of_the_review": "This paper provides a simple and intuitive way to carry out graph learning based on the tree represnetaion of graph. And the performance of the presented models looks convincing. Source code is give for evluation. But I still have the following concerns:\n1. What are the major contributions of the paper? \n2. The time complexxity should include the transfering time from graph to tree structure? Especially for the large scele graph data? ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}