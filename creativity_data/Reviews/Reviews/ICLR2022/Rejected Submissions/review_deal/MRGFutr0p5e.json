{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes to use the recently introduced \"Barlow-twins\" contrastive learning objective, to the case of graph networks. The main concern raised by reviewers was the limited novelty of this work, which they argued mostly combines existing lines of work, and does not introduce sufficiently new concepts. This was also discussed between the authors and the reviewers.\nHaving read the paper and the reviews, I tend to agree with the reviewers that this paper is more of a combination of existing works, and their relatively straightforward application to the graph network domain. Thus, although the empirical results are encouraging, I agree the paper has limited novelty, and falls below the ICLR acceptance bar."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper applies the recently proposed self-supervised learning method Barlow-Twins to graph structured data. For constructing the augmented version of a graph, previous methods such as edge-dropping or feature masking are used. The paper conducts experimental evaluation on datasets of various scales on both transductive and inductive setting.  ",
            "main_review": "Strenghts:\n\n1. Paper is clearly written and easy to follow.\n2. The experimental evaluation is robust and experimental details are clearly stated.\n\nWeakness:\n\nThe paper is the straightforward extension of previous work Barlow-Twins for graph structured data. The paper does not has any technical novelty in my opinion. I am willing to increase the score of the paper if in the rebuttal, the authors can clearly state the novelty of the paper w.r.t to the Barlow- Twin paper. ",
            "summary_of_the_review": "Although the paper is clearly written and many experiments are presented, the paper does not meet the bar of the top conference such as ICLR because of it being a very direct application of a previous paper.\n\n\nMy review is rather short for this paper because based based on the lack of novelty of this paper, I do not have many questions to ask or suggestions to make.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors study symmetric self-supervised graph representation learning without negative samples, inspired by the Barlow Twins method previously proposed in the image domain. They illustrate that using their method, it is possible to achieve competitive performance to state-of-the-art methods such as BGRL, at a fraction of the training cost.",
            "main_review": "Disclosure: I have reviewed a previous version of this paper. The authors have included thorough full-batch experiments as well as larger scale datasets such as ogbn-products, which is much appreciated.\n\nThe paper is clear, well-written and easy to follow. The authors propose a simple but meaningful extension of the Barlow Twins idea to the graph domain, and demonstrate its effectiveness on relevant experiments. I think allowing for symmetric loss is a very important direction for graph representation learning, and the proposed solution is an elegant way of achieving that.\n\nMy main concern would be with the reported BGRL results on ogbn-products. I understand that the authors have ran BGRL under the same computational budget as G-BT, but it appears clear that BGRL needs more time to reach peak performance. Would it be possible, just to avoid muddying the waters for future work, to run BGRL for longer and report how the performance is affected? It is OK if this number is higher than G-BT's reported performance -- the authors are optimising for a different metric.",
            "summary_of_the_review": "I think that sufficiently many of my previous concerns have been addressed, and I am now leaning on the side of acceptance. The authors have presented a useful extension of Barlow Twins into the graph domain, and now have experiments in support of the industrial relevance of their method. The novelty is somewhat limited (as is the case for most of the recent graph SSL papers that adapt image domain techniques) but it is useful in and of itself that the gains observed in images transfer well to the irregular domains.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposed a self-supervised learning framework for graph representation learning based on a cross-correlation-based loss function. In the proposed framework, two views of the input graph obtained by augmentation methods are passed through the same encoder to compute two embedding matrices, then Barlow Twins loss is used to compute the loss according to the embedding matrices.\nThe main contribution of this paper lies in that it adapted Barlow Twins from vision to graph representation learning field and evaluated the performance of this self-supervised framework in multiple node classification tasks. The proposed method achieved analogous results compared to SOTA methods with lower time and space complexity.\n",
            "main_review": "This paper is easy to follow, well-written and organized. This paper is a heuristic attempt to apply Barlow Twins in graph domain. Both transductive and inductive experiments are done to evaluate the performance. The proposed method achieved results on par with the SOTA methods while the time and space complexity are lower.\n\nI have some minor concerns below for the authors to address.\n\n1. The novelty of this paper is limited. The challenges to be tackle of the application about Barlow Twins in graph domain is unclear to me. \n\n2. Data augmentation in vision tasks comes from strong human prior, e.g., random resize, cropping and horizontal flipping would not change the semantic of an image. While the graph data augmentation methods used in this paper is borrowed from previous literatures, it makes no sense to me. For example, applying edge dropping to a protein would obviously lead to different bio-molecules.\n\n3. The experimental results are on par with baseline methods on the most tasks. Considering that the low time and space complexity is coming from previous literature, i.e., Barlow Twins, the experimental contribution is limited.\n\n4. In terms of the encoder network and augmentation hyperparameter design, the paper did not provide comprehensive analysis or ablation studies.\n\n5. The authors carefully describe the downstream datasets. Maybe I miss it, but I don't find the pre-trained dataset used in the experiment.",
            "summary_of_the_review": "This paper adapted the recent Barlow Twins to self-supervised graph representation learning and provided some informative empirical experiment results. With such interesting trials, the reviewer expected to see the concerns are well addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}