{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "Four reviewers have reviewed this manuscript and two found it borderline leaning towards acceptance, two other reviewers scored it below the acceptance threshold. While the authors *identify the key challenges and bottlenecks in 3D point cloud model*, the most positive two reviewers notice that the depthwise kernel and the attention mechanism (and similar tools) are well-known in the literature and that this work is *more of an engineering improvement than a technical contribution*, which erodes the novelty of the proposed idea on that front. While authors noticed some discrepancies in the numbers quoted by Rev. 3, the model gains are also nonetheless modest compared to other models. Overall, the feeling amongst the reviewers was that  the presentation of NK attention could be further improved and that the paper uses very heavy machinery to achieve results comparable with SOTA.\n\nOn this occasion, the manuscript is below the acceptance threshold with even the borderline positive reviewers having their doubts about clear cut technical novelty."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper addresses point-based methods for classification and semantic segmentation in 3D applications. It aims to improve the computational efficiency of this kind of methods to make it better fit the applications of limited resources such as mobile scenarios. To achieve this, this paper conducts the following works. First, it develops a mobile attention kernel point convolution (MAKPConv) scheme to improve the performance of existing kernel point convolution. Second, it utilizes neural architecture search (NAS) technique to design the MAKPConv-based network. In this part, this paper proposes a wide & deep neural predictor for the NAS process. Experimental study is conducted on benchmark datasets and tasks to demonstrate that the resulted deep networks can achieve higher computational efficiency and improved performance. Ablation study is also conducted to illustrate the key components in this work. ",
            "main_review": "Strengths:\n\n1. The point-based 3D application focused by this work is of importance and has recently attracted much attention as a new research area;\n2. The motivation on computational efficiency makes sense and the experimental result demonstrates that this is well achieved by this work;\n3. The idea of changing KPConv to a depthwise kernel and then using NK-Attention to compensate the degraded performance caused by the limited representative power of the latter is sound, and its effectiveness is verified by Table 1. \n4. The overall performance of the resulted MAKPConv-based network is validated in the experimental study. \n5. The ablation study on the kernel contribution via the NK-Attention is helpful for illustrating the function of the proposed attention mechanism. \n\nWeaknesses:\n1. Although the ideas of using a depthwise kernel and the attention mechanism work well in this paper, they are well-known techniques in the literature. In this sense, the contribution of this work in this part is mainly on innovatively using them to improve the computational efficiency of KPConv. The theoretical contribution of this work seems to be limited. \n2. The part on the proposed wide & deep neural predictor and the idea of carrying feature engineering on searchable features in NAS can be made clearer. Firstly, the key differences and novelties with respect to the existing related work can be further clarified. Secondly, can the proposed feature engineering idea be regarded as a new principal in NAS that can be generally applied? Clarifying these issues will make the contributions in this part easier to be understood. \n\nMinor issues\n1. \"N\" in Figure 1 shall be explicitly defined;\n2. Kenrel --> Kernel in the caption of Figure 1;\n3. \"...my our novel Wide & Deep ...\" in Conclusion. ",
            "summary_of_the_review": "This work is well motivated. The proposed idea on reducing computational cost is neat, and it works effectively as experimentally demonstrated. The proposed wide & deep predictors for NAS helps to efficiently find the high quality architecture for the proposed network, achieving promising performance. Meanwhile, the potential issues are i) the idea of MAKPCon is mainly based on existing well-known techniques and does not seem to bring significant theoretical insight and ii) the novelty and contribution of the Wide & Deep neural predictor part can be better clarified.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns. ",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper aims to accelerate the inference of 3D point cloud neural networks. The authors first borrow a few designs from efficient 2D neural networks: depthwise convolution and inverted residual bottleneck. They then propose to reweight the weights of kernel points with attention to boost its representation power. Finally, the authors adopt the predictor-based neural architecture search to explore the best model under a resource constraint. They have evaluated their proposed solution on the small-scale ModelNet40 dataset and the large-scale SemanticKITTI dataset and have achieved reasonably good empirical performance.",
            "main_review": "The problem studied in this paper is very important. 3D point cloud neural networks are the core of many real-world applications (AR/VR and autonomous driving). At the same time, most existing solutions are still not efficient enough to provide a satisfactory user experience and guarantee user safety.\n\nThe paper is well-written and relatively easy to follow. Furthermore, the proposed solution is technically sound and has achieved fairly good empirical performance on small-scale and large-scale benchmark datasets.\n\nMy primary concern of this paper is its limited technical novelty. This paper is a combination of existing solutions:\n* Depthwise convolution and inverted residual bottleneck are originally proposed in MobileNet-V1/V2.\n* The reweighting mechanism adopted in this paper is essentially the same as in SENet [Hu et al., CVPR 2018].\n* The neural architecture framework used in this paper is almost identical to Neural Predictor [Wen et al., ECCV 2020].\n\nThe only thing new to me is the unified dense and sparse neural architecture representations, which, however, is more of an engineering improvement than a technical contribution. The authors should highlight their core contributions apart from applying existing techniques to a new modality. Besides, the existing solution (SPVNAS) already adopts the one-shot weight-sharing NAS framework to explore the best neural architecture. It is unclear why the authors follow a different approach.\n\nThe authors claim that MAKPConv is 52% faster than SPVNAS, which is inaccurate. Having fewer MACs does not necessarily translate into measured speedup on the actual hardware. The authors should directly compare the latency of MAKPConv with SPVNAS (and other baselines) in Table 4. Furthermore, the techniques proposed in this paper are tailored for #MACs reduction but not necessarily for latency reduction. Point-based neural networks are usually bottlenecked by the KNN computation rather than the convolution (while most techniques in this paper aim at reducing the computation of convolution). It is unclear whether these modifications are also helpful to reduce the latency. The authors should include latency numbers in Table 1.\n\nThe authors have only reported the performance on the validation set of SemanticKITTI. They should also include some numbers on the final testing set. Besides, it would be better to verify whether the searched neural architecture is transferrable to other datasets (such as ScanNet and nuScenes) and/or tasks (such as panoptic segmentation and object detection).\n\nReferences:\n* Hu et al., \"Squeeze-and-Excitation Networks\", CVPR 2018.\n* Wen et al., \"Neural Predictor for Neural Architecture Search\", ECCV 2020.",
            "summary_of_the_review": "The paper has relatively weak technical contributions and lacks latency comparisons with state-of-the-art solutions. Therefore, I would like to see the authors' response before the final decision.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper builds a memory and computation-efficient version of the KPConv model, which is a point cloud processing model. The paper uses many techniques to achieve this including, depthwise kernels, attention over kernels, and also neural architecture search (NAS). The paper conducts experiments to demonstrate the effectiveness of their techniques.",
            "main_review": "Strengths\n- Usage of a depthwise kernel reduces the model size of KPConv and the computation cost involved.\n- The paper can effectively search for a MAKPConv block that is better than a hand-crafted one using NAS.\n\nWeaknesses\n- It is unclear why the paper decides to start with KPConv architecture, which is a particularly parameter inefficient architecture. The objective of building a lightweight point cloud processing model would have been better served by starting with some already lightweight architecture like [1] (explained later).\n- Even after elaborate techniques like NAS, the final architecture is not very impressive in terms of memory and accuracy. The paper does not compare to [1], which is a very simple baseline method that already achieves better performance (93.0 vs 92.2) than achieved in the paper while using a similar memory footprint (0.8 vs 0.56 M params). Hence, the end-product of the work is not very useful in the reviewer's opinion.\n- The paper is missing details about the protocol used for the evaluation of the ModelNet40 experiments. This is particularly important as [1] shows smaller changes in protocols can have a drastic impact on performance. \n- The architecture of the proposed model MAKPConv is very complicated. The ablation studies donâ€™t appear to justify the utility of the complicated structures used in the model design.\n\n[1] Revisiting Point Cloud Shape Classification with a Simple and Effective Baseline, ICML 2021\n",
            "summary_of_the_review": "Overall, I appreciate the paper's efforts in making lightweight point cloud processing models. This could be very useful in many applications. However, the method is weak in terms of the final performance (parameters vs accuracy) and is overly complicated. Some important details are also missing (see weakness). Hence, I would recommend rejection for the current version of the paper.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper aims to address the point cloud analysis task by:\n\n1) improving the existing KPConv operation via considering the kernel relationship;\n\n2) designing the networks via a predictor-based NAS approach.\n\nThe improved KPConv operation. i.e., MAKPConv, can model the local structure efficiently, and the searched network has fewer parameters while performing better than the baseline on two datasets, including ModelNet40 for classification and SemanticKitti for segmentation.",
            "main_review": "STRENGTHS:\n\n1. This paper is well-written, in general.\n2. The motivation behind MAKPConv is reasonable.\n3. The networks have fewer parameters, and outperform previous works, especially on Semantickitti.\n4. The analysis on NAS for block structure search is enough.\n\nWeakness:\n\n-- I am confused about the introduction to MAKPConv.\n1. What is the connection between MAKPConv and KPConv. In KPConv, the correlation between each neighbour and each kernel point is calculated, which I do not find in MAKPConv.\n2. In NK Attention, the NK Distance indicates the distance between the neighbour and the kernel point or between every two neighbours?\n3. It seems that MAKPConv is an improved version of KPConv, but I cannot associate them in Figure 1. For example, it seems that the NK Attention is similar to the h(.) function in Eq. 5 in KPConv [1]. If so, the Depthwise Kernel in the upper-left part of Figure 1, should be Eq.5, right? Then, which part indicates Eq. 4 in KPConv[1]. Maybe I misunderstand the figure. Please clarify this.\n4. In Eq. 1, the dimension of F is $D_{in}$, while $i\\in[1,...,D_{out}]$, which makes me confused. Does $D_{in}$ equal to $D_{out}$?\n5. In Eq. 1 and Eq. 2, since the points are unordered and we do not know which kernel point corresponds to a certain neighbour, how can we have $F_{\\*,k,\\*} \\cdot W_{k,\\*,\\*}$? In addition, I also cannot find a similar formulation in KPConv.\n6. In Eq. 2, the dimension of $F$ and $W$ is $D_{in}$, so how can we get $F'$ with dimension $D_{out}$? \n\n-- Experimental results\n1. Currently, previous works on point cloud analysis usually conduct experiments on several datasets, such as S3DIS, ShapeNetPart, and ScanNet, while this paper only considers two datasets, including ModelNet40 and SemanticKitti, which cannot demonstrate the effectiveness adequately. \n2. The OA of KPConv is 92.9\\%, which is higher than the OA of the proposed method (92.6\\%), while in this paper the authors do not report that result and instead report the result of their implementation.\n3. On ModelNet40, currently there are many works yielding OA larger than $93\\%$, but the authors do not report.\n\n[1] https://arxiv.org/pdf/1904.08889.pdf\n\n",
            "summary_of_the_review": "I have two concerns,\n\n1. The introduction to the proposed MAKPConv is unclear.\n2. The experiments are not enough.\n\nAs a result, currently, I vote for borderline reject. But I might change my score if the authors can address my concern or other reviewers can give more positive reviews.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}