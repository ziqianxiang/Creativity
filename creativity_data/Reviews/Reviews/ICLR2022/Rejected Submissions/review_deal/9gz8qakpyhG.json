{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper builds on ideas in test-time adaptation and test-time normalization to improve performance under covariate shift. Concretely, the paper proposes (i) alpha-BN, a method to calibrate batch statistics by mixing source and target statistics and (ii) test-time adaptation using the CORE loss (which was proposed by Jin et al., 2020). The authors compare the the proposed approach to existing approaches on multiple benchmarks. \n\nThe reviewers found the idea interesting and appreciated the additional ablations. The main concerns were around novelty (as the idea is closely related to prior work in test-time adaptation and normalization) and hyperparameter selection (e.g. how to choose alpha in practice).  Overall, the reviewers and I felt that the current version falls slightly below the acceptance threshold. I encourage the authors to revise and resubmit to another venue. \n\nMinor comment about Appendix C (this didn't affect the score, just a suggestion for future revisions): \nI think it might be interesting to include other alternatives to cross-entropy that downweight easy examples, cf. focal loss https://arxiv.org/abs/1708.02002 and https://arxiv.org/abs/2002.09437. I'm curious to see if CORE and focal loss consistently outperform cross entropy."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents several modifications over test-time normalization and test-time adaptation, which swap the statistics of the BN layer during test-time to handle the distributional difference. Specifically, they propose alpha-BN to balance the source statistics and target statistics, and CORE to further refine the transformation parameters of BN, which is similar to prior test-time adaptation methods. \n",
            "main_review": "**Strengths**\n\n1. Both proposed modifications are technically sounds. \n\n2. Empirical evaluation on many experimental settings show reliable (yet not revolutionary) performance improvement by the proposed methods. \n\n3. This paper is well written and easy to follow. \n\n\n**Weaknesses**\n\n1. The proposed method is technically sound but not a revolutionary idea. Regarding CORE, difference from (Jin et al., 2020) is not clear from the manuscript. \n\n2. Table 1 is confusing, since DG usually assumes the existence of source data, and does not adapt online usually. Similarly, protocol DG in Table 6 is a little all methods listed in the first block of Table 6 does not use any test-time adaptation. I think it is better to call it optimization-free test-time adaptation to clarify the difference with optimization-based TTA like TENT. \n\n3. From my personal experience, I am a little bit  surprised that simple T-BN significantly worsened the performance in Table 2. In my past implementation, I also observed slight performance degradation on VLCS (say 3%) yet slight performance improvement on PACS (say 1.5%) by T-BN. \nWhat is the batch size in the experiment? Is there any class imbalance among test bach in your evaluation? Any other detail that could change the results?\n\n4. Table 6 shows that using Tent in DomainNet worsens the performance yet CORE stable increases the performance. They claim that this is because the domain gap in DomainNet is extremely large, which might be a true yet too casual statement not suitable for the academic paper. It should be justified qualitatively or quantitatively why Tent fails to handle the huge distribution gap, and DomainNet has a huge distribution gap. \nBesides I am suspicious that the performance gain is caused by the hyperparameter selection. It should be better to include the hyperparameter sensitivity of Tent and CORE to fully show the CORE is indeed superior one. \n\n5. Table 7 lacks comparison with T-BN, therefore it is not clear whether the performance gain caused by the modification or the nature of test-time normalization. Besides, If I correctly understand it, CORE can not be used in semantic segmentation (as with the Tent can not), which should be elaborated in the manuscripts.  \n\nA minor comment: In the last sentence in 6.4, I think Tent should be CORE. \n",
            "summary_of_the_review": "Overall I think this is well-written and an acceptable paper with solid empirical results. I am happy to increase my score if the response clarifies my concerns or I misunderstood some points. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes a test-time batch normalization method for domain adaptation. They present a formulation $\\alpha$-BN to calibrate the batch statistics by mixing up the source and target statistics with a fixed hyper-parameter of alpha. And, they propose to use the Core loss [ Jin et al. (2020)] to optimize the affine parameters of beta and gamma in BN layers instead of the entropy minimization used in the Tent method. The paper provides comprehensive experiment results.    ",
            "main_review": "Strength: \n\nThe paper uses the same setting of the test-time BN methods to perform test time adaptation on unlabeled target mini-batch data. The method has been well-motivated by pointing out the limitations in SOTA methods, i.e., BN adaptation alleviates the domain statistics shift, but perturbs the discriminative structures. The paper provides comprehensive experiment results.    \n\nweakness:\n - They present the $\\alpha$-blending formulation to mix up the source and target statics instead of directly using the target statistics. They empirically set the weight $\\alpha$ =0.9 for image classification and 0.7 for image segmentation tasks. There is no discussion on how to select $\\alpha$. \n\n - The paper proposes to update beta and gamma on the mini-batch with the Core loss. The main concern is that the proposed Core loss in EQ 5,  $L_{core}=  1 - \\sum_j p_j p_j$,  is actually very similar to the entropy loss $L_{ent} = - \\sum_j p_j log(p_j)$. The experimental results in the papers show the performance is better. However, there is no theoretical proof or explanation that the Core loss can necessarily get better results than the entropy loss. And, it has the same problem as the entropy loss: the proposed loss cannot prevent trivial collapsed solutions where all unlabeled samples are assigned the same one-hot encoding (which is the actual minima).",
            "summary_of_the_review": "The proposed method can be considered as an extension of Tent. Essentially, the optimal BN parameters ( $\\mu$, $\\sigma$, $\\beta$, and $\\gamma$) can be obtained by optimizing only the BN affine parameters ( $\\beta$, and $\\gamma$). The proposed alpha-BN actually provides a better initialization of the BN parameters for DA, which is why it can obtain better performance than T-BN. But, there is no proof that the proposed $L_{core}$ can necessarily achieve better results than Tent. I consider the proposed method is incremental. The authors should provide more explanation & discussion of the $L_{core}$ in the rebuttal. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a domain adaptation algorithm for the recently introduced test-time adaptation setting by adapting batch-norm parameters. The algorithm has two key components: first, it adapts batch-norm statistics using a linear combination of source and estimated target domain statistics; second, it uses a class correlation optimzation loss to optimize batch-norm parameters on the test batches. These design choices differ from previously described methods. The resulting algorithm is evaluated on a range of datasets and tasks including common corruptions and the DomainBed benchmark for image classification, as well as a benchmark for image segmentation.\n\n\n",
            "main_review": "Strengths:\n- Proposed method can be easily applied on top of any base model for performing test-time adaptation\n- Strong empirical results across a range of datasets and tasks, especially image segementation, outperforming recent methods\n- Some insight into the failures of T-BN is described \n\nWeaknesses:\n- It is unclear how the parameter $\\alpha$ should be set for new datasets. The chosen values happen to achieve the best performance according to Figure 3 (for segmentation), but how was this chosen and how should these be chosen in practice for a new dataset when the test performance is unknown? How $\\alpha=0.9$ was selected for classification tasks was also not described in the text. \n- A major contribution in the paper is the $\\alpha$-BN method of combining source and target batch statistics, which the paper argues improves upon T-BN. However, the precise effect of this procedure versus the orthogonal class correlation optimization (CCO) was not examined in the experiments. In particular, each of the $\\alpha$-BN and CCO components can be incorporated/compared with the previous TENT method that uses T-BN and entropy. An ablation study swapping out each respective component will show the significance of each part (e.g. T-BN + CCO, $\\alpha$-BN + Entropy).\n- There appear to be some inconsistencies with the experimental evaluation as detailed below:\n  * For segmentation experiments, it appears not all of the methods share the same backbone model, which has a large impact on performance, e.g. SW (Pan et al. 2019) reports using \"DeepLab-v2 [2] model with VGG16 [25] backbone\", but this paper uses DeepLabV3 with ResNet-50 as stated on page 8. Moreover, the result for SW in the GTA5 -> Cityscapes setting is 29.9 in Table 7, which differs from the result reported in the paper (Table 5; AdaptSetNet-SW - mIOU 35.7). These differences should be clarified during the discussion period. It should also be clarified whether the same backbone is used for comparisons in all other experiments as well.\n  * TENT is included in other comparisons but not in the segmentation results - is there some particular reason why? The original TENT was also not included in Table 6 - how does it perform?\n- Comparison to recent DG for segmentation baseline is missing - FSDR: Frequency Space Domain Randomization for Domain Generalization, CVPR 2021.\n- Presentation wise, some parts of the text should either be supported by evidence or toned-down in terms of the claims, e.g. \"guaranteed discriminative representations\" (this suggests there will be a proof), \"suppresses the confident false predictions ... providing a more robust optimization\" (some empirical evidence of this should be provided). Also, Section 4 seems a bit tangential to the rest of the paper as it does not directly motivate $\\alpha$-BN. Does the proposed $\\alpha$-BN method perform better than T-BN in evaluations provided in Section 4? This will provide better motivation for the $\\alpha$-BN method and improve the flow of the paper.\n\nOther comments/suggestions:\n- The justification for comparing the proposed method with other DG methods is that Pandey et al 2021 does some optimization at test time. However, they do not modify the model parameters unlike the proposed method and other test-time adaptation approaches. Moreover, the method of Pandey et al 2021 was not included in the comparisons.\n- It would be useful to provide some insight into why the proposed method does well in some cases compared to others.\n- Statistical tests for significance should probably be done to compare all methods rather than just the baseline, and on all tasks.",
            "summary_of_the_review": "Overall this paper proposes a simple and effective method for test-time adaptation that seems to outperform the previously described TENT method. However, key questions about how the $\\alpha$ parameter should be set in practice and issues with the experimental evaluations should be resolved before the paper is ready for publication. A more thorough examination of the effect of $\\alpha$-BN and CCO components will also provide more confidence in the contribution. \n\n**Post response update:** I have read the other reviews and responses from the authors. The additional ablation studies address my concerns about the effect of the individual components, but as pointed out by the other reviewers as well, technical novelty is somewhat limited as both components have existed in the literature although they are not applied to this particular setting. Also, it is unclear how the $\\alpha$ parameter can be set in practice. Thus, while this work presents an interesting empirical study, I think it is borderline and I lean towards rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a method to calibrate batch normalization statistics at test time to improve a model’s cross-domain generalization ability under covariate shifts. The authors validate the effectiveness of the method on several datasets and tasks. The authors also conduct some interesting analyses to help better understand the method.",
            "main_review": "Strengths\n1. The authors conduct a lot of experiments to validate that simply calibrating the batch statistics at test-time can improve the model performance in different settings, such as robustness to corruptions and domain generalization.\n2. The analysis in Figure 1 is inspiring and interesting.\n\nWeaknesses\n1. Lack of technical novelty.\n- The idea of mixing source and target statistics has been proposed by Schneider et al. (2020). I don’t see how the so-called alpha-BN differs from it.\n- It seems that the class correlation minimization loss is directly taken from Jin et al. (2020). I think the authors should not claim to use it as a contribution.\n2. Method\n- The logic flow from Section 4 to Section 5 is a bit weird to me. We can know replacing source statistics with target statistics can indeed reduce covariate shift if we use that representation to train a classifier (Figure 1(b)), but simply replacing the statistics will hurt the performance (Table 2). Do these two results intuitively conclude that we should mix the statistics from both source and target domains? I don’t really think so. Also, the selection of the alpha values seems so ad-hoc, without any theoretical analysis. What’s more, the alpha value of the semantic segmentation task is kind of overfitting on the test set (Figure 3). Isn’t this cheating?\n- While in the classification problem we know that directly replacing batch norm statistics with target statistics is bad (Table 2), it actually works quite well in semantic segmentation (Figure 3(b)). I wonder why. I think we need more analysis to understand this phenomenon.\n- Can the class correlation minimization loss be applied to the semantic segmentation task as well?\n3. Some claims are wrong\n- In Table 1, not all the domain generalization methods support online adjustment (actually, only one recent method supports this). I would suggest adding another term called test-time batch statistics update (or something similar) for better clarity.\n- In Section 4, the authors mention test-time normalization. But in the related work section, the authors actually use the same term to refer to both Schneider et al. (2020) and Nado et al. (2020). However, these two works apply test-time normalization differently. The former one does a very similar thing as what this paper does, while the latter one replaces batch norm statistics with target batch statistics. It is confusing to use a single term to refer to two different methods.\n4. Some details are missing\n- In Section 4 “Error of ideal target hypothesis”, the author claims that “we train a new classifier over the target representations with corresponding labels”. Please define “target representations” here.\n- In Section 7 “Discovering a better pre-trained model”, how to use alpha-BN to get a better pre-trained model for fine-tuning remains unclear to me.\n5. Other comments\n- The ERM baseline on semantic segmentation is too bad. Currently, GTA5 -> Cityscapes only gives 29% mIoU by using a ResNet-50 DeepLabv3. However, I know that we can easily get a 35~37% mIoU by using a ResNet-50 DeepLabv2, which is actually a weaker model. I think there must be something wrong with either the model setting or training schedule. Showing great improvement over a bad baseline is not that meaningful.\n\n",
            "summary_of_the_review": "- The main concern is the lack of technical novelty. I would say both of the two main components in this method have been proposed by others previously. In this case, this paper over-claims its contributions. It will be better to reshape the paper as an empirical finding paper instead of a method-proposing paper. \n- Also, semantic segmentation is so different from image classification, so that I am not sure if the conclusions we get from image classification can safely transfer to semantic segmentation. For example, the mismatch between Table 2 and Figure 3(b), as I mentioned in the weakness section as well. And it is also quite weird that another important component, class correlation minimization, is only applied on classification but not segmentation. So I would suggest the authors either discard the semantic segmentation and fully focus on image classification, or conduct additional experiments and analysis on semantic segmentation to have a deeper understanding.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}