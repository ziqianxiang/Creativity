{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper examines a sum-over-paths representation of ReLU networks, for which learning can be broken into two parts: learning the gates, and learning the weights given the gates, the latter of which being described by the Neural Path Kernel. The paper introduces a dual architecture, Deep Linear Gated Networks (DLGN) that parameterizes these two processes separately. The DLGN is argued to aid in interpretability of ReLU networks, with a main conclusion being that the neural network is learned path-by-path instead of layer-by-layer.\n\nThe reviewers generally found strength in the motivation and perspective and thought that the DLGN could serve as a useful architecture for aiding interpretability. Some reviewers found the presentation hard to follow, and others were not entirely convinced by the ultimate conclusions. Overall, the reviewers opinions were mixed.\n\nI believe the ICLR community would generally find interest in the DLGN and the interpretations it might afford to deep ReLU networks. However, the number and strength of the conclusions obtained in the current analysis are rather weak. The conclusion that networks learn path-by-path instead of layer-by-layer was emphasized but the implications were not highlighted, and it remains unclear to me and at least some reviewers what the concrete significance of this observation actually is. Another major claim is that the DLGN recovers more than 83.5% of the performance of state-of-the-art DNNs, but a priori it is not obvious what this number means, or if it is even good or bad performance. A more detailed analysis with additional common baselines, ablations, etc., would really help readers understand the significance of the performance gap.\n\nOverall, this is an interesting direction with significant potential, but for the above reasons I cannot recommend the current version for acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors of this paper proposes a DLGN (Deep Linear Gated Networks), a novel class of deep networks, inspired by a recent dual view where the computation in DNNs is broken into two parts: learning in the gates and learning in the weights. The DLGN disentangles the computations into 2 parts: (1) \"primal\" part between input and the pre-activations in the gating network, and (2) \"dual\" part in the weights network, conditioned on inputs and gates. DLGN’s performance recovers 83.5% of SOTA DNNs. This development may lead to more interpretable deep network models that are also highly performant.  ",
            "main_review": "* This work provides an alternative perspective on understanding and interpreting how deep networks work. In particular, the current study overcomes the limitations of NTK framework, by proposing NPK (Neural Path Kernel) for DNNs with gating and dual pathways. \n\n* Beyond interpretbility, what is the functional role of gating? Is there any computational benefit of using a gating network? Recent work in gated linear networks [1,2] have provided functional examples where the usage of gating pathways gives rise to computational benefits such as superior performance in continual learning tasks [1] and implementation of local learning rules [2]. Perhaps the connection to these lines of work can be discussed more clearly.  \n\n* Does the separation of gating network and value network introduce twice as many parameters? Is there any way to reduce the number of parameters in gating network, and can this be used to explain why the performance of the DLGN is a bit worse than SOTA? \n\n* Description in Figure 1 is a bit confusing and not self-contained. Can Fig 1 be more detailed? For example, illustrate \"Gating Signal\", with a small figure of the functional form of G, to illustrate GaLU. (I understand that it becomes clear in Figure 2, but perhaps it can be drawn in Figure 1 too)  \n\n* I’m not sure if the entanglement of linear and and non-linear operations of DNNs being uninterpretable is a strong motivation for developing a new (interpretable) class of networks, especially given many ongoing studies on interpretability methods that can be used directly on deep nonlinear networks. Perhaps, the paper could benefit from an additional clarification on the role of interpretable models (as opposed to interpretability methods on uninterpretable models). \n\n* Concluding a paper with a meta-note that this paper concludes with the question if DLGN a universal spectral approximator makes the paper sound a bit like a long introduction to the next paper. Perhaps it should be stated as an interesting direction for future study. Admittedly, exploring the expressivity and capacity of DLGN is an interesting question in its own right and it's unclear why this is  emphasized only at the end of the paper. \n\nReferences \n\n[1] Veness, J., Lattimore, T., Bhoopchand, A., Budden, D., Mattern, C., Grabska-Barwinska, A., Toth, P., Schmitt, S. and Hutter, M., 2019. Gated linear networks. arXiv preprint arXiv:1910.01526.\n\n[2] Clark, D.G., Abbott, L.F. and Chung, S., 2021. Credit Assignment Through Broadcasting a Global Error Vector. arXiv preprint arXiv:2106.04089.",
            "summary_of_the_review": "I think this paper continues an interesting line of research. The goal and the motivation of the paper, and the connection to the existing prior work could be made more clear, but once those are addressed I am willing to change my score. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper extends the framework of the deep gated network to the deep linearly gated network. Based on this extension, the paper investigates the separate part of the network. By theoretical analysis and empirical experiments, the paper argues that the neural network is learned path-by-path instead of layer-by-layer. Also, it present that the neural path kernel has several interesting properties.",
            "main_review": "## Strengths:\n1. Disentangling neural networks in the path space is a really interesting perspective, which may give new insight to understanding neural networks.\n2. By theoretical analysis, the paper proves some novel propositions (e.g., ResNets have an ensemble structure, NPK is rotationally invariant) which might be useful to understand deep learning models.\n\n## Weakness:\n1. Since the paper is largely based on the previous work DGN (Lakshmi-narayanan & Singh (2020)), it would be better to provide more details and formal statements about the prerequisites. Otherwise, it would be hard to follow for readers who are unfamiliar with DGN. (e.g., Definition 2.1 should be a more formal definition with mathematical symbols and formula.)\n2. The paper claims that \"the learning in the weighted network happens path-by-path\" with experiments and theoretical analysis. However, the \"input 1\" experiment and the \"layer permutation\" experiment on DLGN only prove that **DLGN can** be learned path-by-path, and this conclusion doesn't necessarily hold for the general neural network.\n3. The paper claims that \"Destroying structure by permuting the layers\", but actually, according to Figure 3 (C4GAP-DLGN), I think it's somewhat overclaimed. Let q_1, ..., q_4 denote the pre-activation which generate G_1, ..., G_4. And we have q_i = (C_i^f C_{i-1}^f ... C_1^f)(x^f). Since C_i^f C_{i-1}^f ... C_1^f is linear, we can merge (C_i^f C_{i-1}^f ... C_1^f) to a single weight matrix/convolution kernel, which is denoted as P_i. Then P_i is just a common linear operator, and different q_i has its own unique P_i. It's obvious that we can permute them and get similar performance. I don't think it can be called a \"destroying structure\".\n\n## Questions:\n1. In Table I, why is the performance of models whose layers are permutated slightly higher than the original models?\n2. Can the linear network functions replace the non-linear network?  If not, why try to design the network in a linear manner?",
            "summary_of_the_review": "The paper gives an in-depth theoretical analysis of neural networks. But some of its claims are not well-supported or well explained. The writing of the paper is ok, but the prerequisites of the paper can be more comprehensive.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper deals with the entanglement in the DNN through two steps. First, replacing the rectified linear units (ReLU) in the traditional DNN with Deep Linearly Gated Network (DLGN). Second, demonstrate the weighted network is disentangled in the path space.",
            "main_review": "Strengths:\n1. The view of this paper is interesting since the success of the DNN is its non-linear ability. And the paper proposed to reduce its non-linearity.\n2. And both the theory and experiments are sufficient to support their claims.\n\nWeakness:\n\nWay too theory. It is a little bit hard to read for me.",
            "summary_of_the_review": "The problem of this paper deal with is important, and the results show the effectiveness of their proposed method.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed deep linearly gated networks (DLGN) for interpreting DNNs with ReLU activations based on a dual view. The proposed framework is able to completely disentangle the ‘gating network’ and the ‘weight network’. Finally, the experiment results demonstrate that DLGN can achieve good performance for classification on two benchmark datasets compared to the existing DNNs.",
            "main_review": "Pros:\n1. It is a good idea to adopt deep linearly gated networks (DLGN) for interpreting DNNs with ReLU\n2. This work also offers some theoretical analysis to better understand the proposed method.\n3. Compared to the prior work on dual view, this paper presents more insights and new results for convolutional networks with global average and ResNet\n\nCons:\n1. Did not conduct experiments on complex tasks, such as ImageNet and CelebA. I am curious about what experimental results they will be.\n2. This work did not discuss the impact of $\\beta$ in the soft gate on the experimental results\n3. It is hard for the audience without background to understand some theorems.",
            "summary_of_the_review": "The idea of using deep linearly gated networks (DLGN) with dual view looks novel and interesting. I think this paper should be above the acceptance threshold.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}