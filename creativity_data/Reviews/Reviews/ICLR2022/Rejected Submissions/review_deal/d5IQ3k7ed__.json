{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The authors propose a deep multi-agent RL framework to compute equilibria in a economics problem. Several reviewers raised issues with the presentation, as well as issues with evaluating the impact of the work, partly because the novelty of the approach is made insufficiently clear. While the authors have resolved some of the confusions arising from the presentation in their rebuttal, resulting in 2 out of the 4 reviewers to increase their score, the concerns regarding novelty mostly remain. For these reasons, I don’t think this work is ready for publication at ICLR at the moment and recommend rejection."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper uses a multi-agent reinforcement learning algorithm to simulate an economic environment and solve the general equilibrium of the induced game. The authors propose to use a structured learning curriculum that runs only on GPUs. The authors conduct experiments to show that their algorithm converges fast and that the solution represents an epsilon Nash equilibrium.",
            "main_review": "Using a multi-agent reinforcement learning algorithm to solve the complicated game considered in the paper is natural and reasonable. The structured learning curricula runs in multiple stages and could potentially ease the problem of instability in the training process. Also, it is interesting to see that the GPU-only framework is able to significantly accelarate the training process. The experiment results look convincing and interesting. \n\nThere are a number of notations that are used before they are defined (for example, p_i, J, \\tau), making the paper hard to follow. I have to read back and forth, and keep guessing the meaning of these symbols. The readability can be significantly improved.",
            "summary_of_the_review": "In general, I find the paper interesting. But the readability can still be improved.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a multi-agent deep reinforcement learning (DRL) method to compute general equilibria in economics. The main contribution is hence algorithmic. The method uses a combination of DRL, structured learning curricula, and suitable annealing of action space and of some penalty coefficients (which are useful to make some problems easier to solve, but distort the computed solution). The authors apply their method to an example of real-business-cycle model, which involve three types of agents: firms, consumers and a government. The problem is of Stackelberg (or leader-follower) type, with the government acting as a leader. Computing equilibria for such problems is generally very challenging.\n",
            "main_review": "As pointed out by the authors, there are relatively few works on reinforcement learning for economics. This direction seems promising. However, I have a few questions:\n\n1. In the abstract and the introduction, could you please clarify what you mean by finding optimal agent policies “explicitly” versus “implicitly” (top of page 2)?\n\n2. In section 3, several ideas are described but there is no clear description of the algorithm. Since the problem considered here is rather complex, I recommend adding a description (e.g., pseudo-code) of the MARL method. It would be helpful for the reader to see how the ingredients described in this section are combined.\n\n3. In section 4, at the bottom of page 6, how do you check the local equilibrium property for the leader? The text seems to indicate that you fix all the followers and train the leader alone. However, I thought that the leader's reward depends the followers' reactions. Are the followers' policies functions of the leader's policy? Here again, I recommend adding a clear description of the quantities that are computed for the plots.\n\n4. Figure 4, left plot: Even at the end, there is still a significant improvement for the firms (roughly equal to the improvement at the middle of the training). How can you explain that?",
            "summary_of_the_review": "The application of multi-agent reinforcement learning for economic problems seems very promising, particularly given the high complexity of these problems. The main contribution is algorithmic, but without a precise description of the algorithm, I find it hard to really understand the method and to assess the impact this work could have.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed a deep reinforcement learning framework for finding dynamic general equilibrium, which is one of the most fundamental problems in economics. As the dynamic general equilibrium is a special case of the Markov game, this problem has also been recognized as a significant topic in machine learning. The proposed scheme is tested in a real-business-cycle model with 100 worker-consumers, 10 firms, and a government, the scale of which is much larger compared with the numerical examples in most related works.",
            "main_review": "The most interesting point in this paper is that the proposed scheme breaks the curse of many agents. In the numerical example, the total number of agents is more than 100. As in economics models, a single agent usually represents a large population of homogeneous agents, 100 is a sufficiently large number from an application point of view. The success of the deep RL algorithm studied in this paper highly relies on WarpDrive, a recently developed framework that supports running both simulation and RL on a GPU.\n\nYet, as far as I'm concerned, running the algorithm successfully is just a first step for studying this fundamental problem. Although the authors reported their empirical results, they failed to provide enough theoretical guarantees for supporting the results. \n\n(1) Can the proposed algorithm converge to the ($\\epsilon$-)Markov equilibrium almost surely? If so, can you provide sufficient conditions for the convergence?\n\nIn this paper, the convergence to the equilibrium is checked empirically (p6-best responses and local equilibrium analysis). Yet, a specific case could not provide the convergence guarantee for the general setting. This specific case is relatively special: the 100 consumer-works and the 10 firms are homogeneous, respectively. Therefore, it is similar to (but not the same as) a potential Markov game, which is not significantly different from a single-agent MDP. In the study of dynamic general equilibrium, the heterogeneity of agents has received more and more attention. The convergence of the algorithm in the heterogeneous setting (e.g., the 100 consumer-works are different from one to another) could be more difficult. \n\n(2) Can the proposed algorithm learns a real Stackelberg strategy?\n\nIn the proposed model, the government has a first-mover advantage as in a Stackelberg game. In the optimization literature, it is well known that finding the Stackelberg equilibrium is an NP-hard problem. When both the leader and the followers become MDP agents, this problem would definitely become more complicated. Yet, in this paper, no theoretical or empirical results could support that a (locally) Markov Stackelberg equilibrium is found. The only numerical result for this issue is provided in p7-comparing with baseline government policies: it is reported that the RL government policy improves the social welfare by 15% compared with the case with fixed tax rates. However, it doesn't mean that it is really a Stackelberg strategy unless sufficient results could support that the government has fully realized its first-mover advantage.\n\nMeanwhile, the algorithm itself is not described clearly in this paper. For example, the trick for running both simulation and RL on a GPU is from WarpDrive, yet the details of the WarpDrive framework are not even introduced in this paper. The structured learning curricula and the action space annealing procedure, as another example, are introduced using textual description completely without providing any equations and technical details. ",
            "summary_of_the_review": "The paper studied a fundamental problem and tested an efficient algorithm for solving it. However, I do not think the paper can be accepted in the present form because (1) the success of the algorithm highly relies on a trick proposed in another paper; (2) besides running the algorithm empirically, no theoretical analysis of the proposed scheme is provided in the paper. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work investigates the use of Deep MARL in order to study economies. They propose and implement an RBC (economic market model) and propose a reward shaping schedule to bias agents into learning non-degenerate joint-strategies. In their setting, low-welfare equilria and provide a brief quantitative and qualitative analysis of discovered equilriba in open- and closed-economies. ",
            "main_review": "**Review**\n\nWIth recent advancements in Deep MARL, the door has opened to allow it to be brought out and applied to real world scenarios. In this submission, the authors look at economics based on an Real-Business-Cycles (RBC) model. I find this to be an exciting direction for research as it promises to help inform and improve economic policy design. That being said, the RBC model presented here is not motivated nor verified (why should it capture the important economic mechanisms of interest?). However, this may be because I am not adequately familiar with this literature (as I would expect the same of the average ICLR reader). The core contribution of this work is a reward shaping schedule to prevent discovery of \"uninteresting\" equilibria. Very little is said about the implications of the different shaping choices and there are no ablations or analysis of their component effects. As the conclusions, and thus take-aways for economic governance, are critically impacted by the bias imposed from this reward shaping I would have preferred to have it treated more rigoursly. Finally, I found the author's use of empirical game theoretic analysis to be essentially absent despite it being labelled as underpinning the results. In total, I am excited to see interest in this area; however, I do not think this particular unit of research is ready for publication at this time.\n\n**Major Comments**\n\n- Reward shaping is used in this work to bias equilibrium selection towards \"non-trivial\" solutions. Why are these non-trivial solutions not worthy of discussion if they are afforded by the mechanism? How is an equilibrium identified as trivial, and which qualities of solutions where the authors biasing against through their reward shaping?\n- The ICLR community is not economics focused, and as a result it would be beneficial to include additional motivations and explanations when bringing in outside ideas. In particular, a large portion of this paper describes the implementation of a particular RBC model. It would be helpful to either punt a justification of its design to related work that uses the same mechanism. Or, if this is a novel mechanism, compare and contrast it with a pre-existing model include motivation and justification for deviations. \n- As a cross-disciplinary work, I would like to see additional connections back to what is known about RBCs from econonmics. Are there known equilibria for this model? How do the results compare with economics solutions? This would allow an appreciation of the results for non-experts.\n- Empirical game theoretic analysis is a method for solving games by constructing meta-games covering restricted strategy sets of the underlying-game, and solving the meta-game as an approximation of the underlying-game. In this work, convergence is measured by calculating the regret against the opponent's policy by freezing it and using Deep MARL to learn an approximate best-response. If all player's have no regret, then an e-NE is found. While this is all technically true, the quanlity of the analysis is weak, and as a result its likely that e is extremely large. Notably, the meta-game being analyzed in this setting is an incomplete 2x2 (abstracting to 2 players) NFG. The top-left cell being the payoffs being the pre-frozen joint-strategy performance, and the off-diagonal being the the approximate best-response performance against fixed policies. Could the authors provide evidence why such as imple empirical game is sufficent to ensure that e is small, and thus the meta-game's solution is at all relevant to the underlying game?  \n- The ethics statement does not seriously consider the ramifications of this work. Please discuss the biases and assumptions baked into this work and how this may lead to downstream issues instead of making blanket staements \"should not be used to make deicsions in real-world systems\" --- if this statement is true, then what's the point of this work?  It is important to think of the unknown-knowns that readers or future practitioners may overlook in the future. \n- Sec 5, Par 1, \"Empirical game-theoretic analysis .... with heterogeneous agents.\" This is not true. A meta-game is a choice of the practioner and through that choice they add restriction such as role-based meta-games often to gain performance benefits. EGTA is not restricted in analysis to any class of games, and can be analyzed through the lens of any game -- where certain choices, ala NFG, scale poorly (although this is a limitation of game theory that is simply inherited by empirical game theory).\n\n**Minor Comments**\n\n- Consumer reward has very large error bars in Fig 3, is there a particular explanation?\n- The authors note that their method finds only low-welfare equilibria. What does the discovery of these equilibria in RBC suggest to policy-makers? Do the authors suspect this is an artifical resultant of reward shaping?\n\n",
            "summary_of_the_review": "I recommend rejecting this paper in a large part because (a) the main contributions (RBC model/reward shaping) have unclear/unspecified novelty, (b) weaknesses in the analysis described in the main review, and (c) need for contextualizing this work in both the MARL and economics literature (help the MARL community understand that this work is important!). ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Other reasons (please specify below)"
            ],
            "details_of_ethics_concerns": "The authors propose and study an economics model of a realtime business cycle. This model needs to be vetted to understand potential issues that might stem from economic-policy recommendations that it suggests. \n",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}