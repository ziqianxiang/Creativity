{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This submission receives mixed reviews. One reviewer leans positively while two reviewers are negative. They raise several issues upon improper evaluations, insufficient experimental analysis, baseline and sota network comparisons, presentation unclarity, and technical motivations. In the rebuttal and discussion phases, the authors do not make any response to these reviews. After checking the whole submission, the AC agrees with these two reviewers that there are several drawbacks to the aspects of the technical presentation and experimental configurations. The authors shall take these suggestions into consideration and make further improvements upon the current submission."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors propose a novel method to unify the top-down and bottom-up attention together for recurrent visual attention. They also propose two constraints in the bottom-up recurrent neural networks for better balancing the trade-off between exploration and exploitation when searching local regions.",
            "main_review": "Strength: The idea is interesting and new. The writing is good and easy to understand.  Sufficient experiments also demonstrate the effectiveness of the proposed model.\n\nWeakness: The authors should give some visual examples to show the difference of the searched regions between the proposed method and other compared methods, especially the RAM model. Such visualization can also serve as qualitative evidence to support the effectiveness of the proposed method.",
            "summary_of_the_review": "Overall, this paper proposed an interesting idea and successfully demonstrated its effectiveness. Only a few visualizations should be done to better justify the proposed ideas.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This submission proposes to offer a better initialization by using image pyramids and Q-learning (top-down manner) for the original recurrent visual attention (RAM) model (bottom-up manner). Two new constraints are also proposed for better exploration for RAM. The proposed method has been tested on several image classification datasets, including MNIST, cluttered translated MNIST, SNHN (sequential multi-digit recognition). They also test the robustness to adversarial attack (PGD attack) on CIFAR10.",
            "main_review": "### Weaknesses\n\n- Any direct evidence to show that the initialization and attention trajectory are better than the original RAM method other than final accuracy? Can you propose some quantitative metrics and offer more visualizations? Now I do not know where the improvement comes from. Is there any possibility that the improvement simply comes from larger model capacity?\n- Tab. 2: On this task, the proposed method brings significant improvement, which is amazing. But there is not any in-depth analysis about it. BTW, I think a fair comparison to CNN-based methods would be a spatial transformer network, or any other CNN with spatial attention.\n- Tab. 3: Seems to me that RAM-based methods are very suitable for sequential digit recognition since they have the concept of glimpse. But it performs worse than a CNN method proposed in 2014. Why is that?\n\n- better report run-time speed and computation cost for all the comparisons. Accuracy is not one major advantage for RAM-based methods compared to CNN-based methods (see Tab. 3)\n- Tab. 2 is too difficult to read. Maybe you can report RAM and the proposed method under different settings pair by pair.",
            "summary_of_the_review": "The motivation looks OK but there still remain many questions. A bunch of experiments have been conducted and some of them look very promising, but more in-depth analysis are required to address my concerns.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper extends the recurrent attention model(RAM) with another extra top-down attention. Specifically, they exploit image pyramids and Q-learning to select regions-of-interest first in the top-down attention mechanism, and then follow RAM to use policy gradient to find the patch in the bottom-up attention. Meanwhile, they also propose two loss constraints to further boost the performance of bottom-up recurrent neural networks. The proposed framework is an end-to-end framework. Experiments on three datasets (MNIST, CIFAR 10, and SVHN) have demonstrated the effectiveness of the proposed model.",
            "main_review": "Strength:\n1. The whole paper is well-written and well-organized.\n2. The idea of combing top-down attention and bottom-up attention in the recurrent attention model is interesting and reasonable.\n\nWeaknesses:\n1. From Algorithm 1 and implementation details, the top-down model is only a one-step prediction, which makes the  whole model less convincing. In contrast, I suggest the authors can discuss multiple-step top-down predictions, and how to (dynamically) control the step of top-down prediction or trade-off the steps in two types of attention.\n\n2. The comparisons with existing works are unfair. Since the proposed model has extra top-down steps, it is unfair to compare with the baselines at some fixed steps of glimpses. Instead, I suggest including the number of steps of the top-down steps into evaluation.\n\n3. Motivations of the proposed context constraint are not clear. Based on my understanding, \\alpha_{ty} is the normalized action probability distribution in y-th class at t-th step. Thus, based on Eq.(11), the meaning of C is the averaged hidden state of all T steps, and I am confused about the exact meaning of C.\n\n4. The proposed entropy constraint is limited. For the MNIST-like dataset, it is reasonable to skip all black or white regions. However, for other more complex datasets or visual scenes, image patches with higher entropy seem to have no direct relation with the final prediction.\n\n\nMinor:\n1. Eq.(10)c seems wrong. It should be L_{t+1} instead of L_t?\n\nWriting Suggestions:\n1. It would be better to change the sequence of \"context constraint\" (Eq. (11)) and \"entropy constraint with better exploration\" (Eq. (12) - (14)) in Page 6 to make it consistent with the introduction section and article part.",
            "summary_of_the_review": "The idea itself is interesting, and but I think these are several limitations of existing versions (cf. the weaknesses part), and the submission can be further improved by solving my concerns.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}