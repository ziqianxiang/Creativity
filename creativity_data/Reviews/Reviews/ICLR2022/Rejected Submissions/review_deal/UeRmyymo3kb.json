{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes a method to change the graph structure for better robustness against adversarial attacks. The reviewers commend the authors for a clearly written paper and promising results. Several reviewers expressed concerns about experimental validation (specifically, comparison to truncated SVD and choice of baselines), complexity, and novelty. The rebuttal and follow-up discussion alleviated some of the concerns, but the reviewers still have outstanding issues, therefore the AC does not recommend accepting the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes GARNET, a spectral approach to depend adversarial attacks on graph structure. GARNET consists of three steps, low rank approximation of adjacency matrix, adaptive filtering on node signals and label propagation. All the 3 steps of GARNET achieve low computation complexity and thus can be applied to large graphs. Extensive experiments on both homophily and heterophily graphs show that GARNET achieve significant accuracy improvements compared with baselines, when the structural perturbation on graphs are stronger. ",
            "main_review": "Strengths\n\n+ The paper is clearly written. All the technical steps of GARNET are easy to understand. \n+ The paper presents extensive empirical evaluation on the proposed framework. The datasets cover both homophily and heterophily graphs, and from small to large scales. \n\n\nWeaknesses\n\n- Although the three main steps of GARNET are all reasonable, they lack some technical significance. For example, \n    - The low rank approximation step follows a standard algorithm and the description over-complicates the algorithm. Although the authors call the reconstructed graph a \"k-NN\" graph, it is indeed just obtained by row-wise preserving the largest elements in the rank-r approximate adjacency matrix -- according to Thm 1, the similarity score used by k-NN is exactly the element value of the rank-r adj matrix. The spectral embedding matrix is also well-known (e.g., used in spectral clustering), and the authors are not really utilizing much property of the spectral embedding except deriving the low-rank approximation. \n    - The adaptive filtering step is also not novel. Multiple existing works follow similar form of such filtering (e.g., SIGN [a]). \n    - Theorem 2 does not seem useful, since the upper bound is way too loose. If we compare the upper bound with the scale of the original perturbation, then the derived upper bound is even orders of magnitude larger. If the upper bound is achieved, then it would tell us that adaptive filtering can enlarge the effect of perturbation. In addition, the upper bound grows quadratically with filtering depth P, indicating that heterophily graphs could be problematic (from theoretic perspective). \n- While the experiments show significant accuracy gains, further clarification could make the improvements more convincing. \n    - In Table 2, 3 and 4, we can see that GARNET achieves higher accuracy even on the clean graph. I suppose this is due to the adaptive filtering step. However, since adaptive filtering is a known design, it would be better to add one unvaccinated baseline model following Eq 2 and 3 in these tables. \n    - From Fig 2, it is clear that reducing the rank is the primary reason for successful defense. If this is the case, then it is not clear why similar rank-reduction based method, GCNSVD, achieves much lower performance than GARNET in Tables 2 and 3. More explicitly, since in Fig 2, GNNGuard achieves much lower accuracy than GARNET, I would imagine GCNSVD to achieve much higher accuracy than GNNGuard and slightly lower accuracy than GARNET in Tables 2 and 3. Yet this does not seem to be the case. \n\n\n[a]: SIGN: Scalable Inception Graph Neural Networks, 2020.",
            "summary_of_the_review": "I think the paper is lower than the acceptance threshold due to the lack of technical significance and question in empirical evaluation, as detailed above. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a reduced-rank and sparse approximation method to purify the graph structure for better robustness against adversarial graph attacks. The approximation is accelerated through spectral embedding and sparsification. Moreover, adaptive graph filter and label propagation are considered for further improvement on low homophily graphs.\n\n",
            "main_review": "## Strengths\n\n1. The paper introduces an efficient reduced-rank approximation method to purify the graph structure that is scalable to large graphs. \n\n2. It provides theoretical justification for the matrix sparsification method.\n\n3. The paper presents lots of experiments to demonstrate the robustness of the proposed method in different settings, such as targeted and untargeted attacks, homophilic and heterophilic graphs, small and large graphs, etc. \n\n\n## Weaknesses\n\n1. The proposed GARNET combines several techniques proposed in previous works such as Low-rank approximation in GCN-SVD and ProGNN, adaptive filter learning in GPR-GNN, as well as correct and smooth in C&S method. Overall, the novelty of the proposed method is quite limited, and the contribution of this paper is unclear.\n\n2. The comparison with the baseline models doesn’t seem to be fair. From my understanding, although the baseline methods (such as Pro-GNN), use classic 2-layer GCN as the backbone model, according to their formulation, their ideas can be generally applied to more advanced GNN models such as APPNP and GPRGNN as well. In essence, GARNET in this paper chooses a more advanced and deeper backbone model that adopts adaptive filter as in GPRGNN and adaptive label propagation as in C&S to compare with the baselines which only use the basic GCN as their backbone models. This doesn’t seem to be a fair comparison since the depth (e.g., number of propagation layers) and filter design typically have significant impacts on the robust performance. Therefore, the performance improvement is not convincing.\n\n3. As far as I am concerned, the low-rank approximation idea is essentially similar to existing works and this paper proposes a more efficient way to achieve this. Therefore, regardless of the efficiency and scalability, the robust performance should be similar to existing works if appropriate backbone models are chosen in the comparison. It is better to have a discussion on this.\n\n4. From the ablation study in Section 4.4, Figure 2 only compares GARNET and GNNGuard. However, GNNGuard is less relevant in this context and the performance is much worse than Pro-GNN in this setting as showed in Table 2 or Table 3. It would be more interesting to see how the low-rank approximation in this work improves over Pro-GNN instead of GNNGuard.\n\n\n",
            "summary_of_the_review": "Overall, the contributions of the paper are unclear and the novelty seems to be limited. In addition, the comparison and ablation study in the experiment is not totally convincing. I would like to suggest a rejection. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "There is no ethics concern.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a spectral approach towards robust and scalable graph learning, namely GARNET. There are three main components inspired by handling low-homophily graphs in GARNET: low rank and sparse approximation of the adversarial graph, a variant of spectral GNN with an adaptive filter, and label propagation with special interest to the learned filter. GARNET is claimed to be nearly-linear so that it can be scaled to very huge graphs.\nExperiments against both targeted and untargeted attacks under the poison setting over a collection of graph benchmark datasets demonstrate that GARNET could increase the adversarial accuracy more than state-of-the-art defending methods.\n",
            "main_review": "Strengths:\n- The paper is easy to follow with clear motivation and is well written. The design of GARNET is clearly motivated by handling the low-homophily graphs.\n- Due to the common assumption on the homophily of underlying graphs, it is of great importance to go beyond these datasets and propose a universal solution to different types of graphs.\n- The acceleration of TSVD is remarkable and the scalability of the proposed method is clearly a strong contribution to the research on the robustness of GNNs.\n\nWeaknesses:\n- The main concern of mine is the contribution towards scalability is a little bit over-claimed. In the major advantages of GARNET, the authors claim that GARNET has a nearly-linear runtime/space complexity. However, the total space complexity of GARNET is not analyzed, which could result in a dense matrix after the adaptive filter learning then is not nearly linear anymore. \n- Another concern is that the proposed approach is heuristic and can be easily broken by an adaptive attacker (see e.g. [1] for a detailed discussion). An adaptive attacker can easily circumvent them by changing their attack to account for the defense. For example, it is relatively straightforward to add an additional term in the computation of Nettack's scores that discourages adversarial edges that have a large impact on the high-rank spectrum. Furthermore, if the attacker knows what proposed defense is used, they can specifically target the largest singular values to potentially cause even more damage.\n- For experiments, the experimental setup should be more specific for better reproducibility since no code is available for examination. Several questions include: what package do the authors use to conduct truncated SVD? I have experience in having the top eigen-pairs of the whole OGB graph with scipy, which takes hours to finish. Thus the run time that is less than 7 minutes is interesting to me. For the training on ogbn-arxiv and ogbn-products, do the authors use sampling techniques or just training with full batch? The difference of evaluation between small graphs and OGB graphs should also be specified in the detail of settings.\n\n[1] On adaptive attacks to adversarial example defenses, NeurIPS 2020\n",
            "summary_of_the_review": "My concerns are mainly from three aspects:\n- The complexity analysis is vague while it is one of the main contributions of this work. \n- No adaptive attacker is evaluated.\n- The reproducibility of empirical evaluation is a little weak.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}