{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes a new federated learning method which uses the recently developed PAGE gradient estimator in the local updates, and provides convergence analysis for both convex and nonconvex loss functions. There are several technical questions raised by the reviewers that are not addressed by the author rebuttal. Given such technical issues and limited novelty and empirical evidence, I cannot recommend acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a new federated learning method, called FedPAGE, which uses the PAGE gradient estimator in local update and provides convergence analysis for both convex and nonconvex setting. The paper shows that the convergence rates achieved in both cases are better than existing results. Numerical experiments are included to illustrate the performance of proposed method.",
            "main_review": "The paper contains following strengths and weaknesses.\n\nStrength:\n- The use of PAGE estimator in federated learning is new. However, this idea is rather incremental as it is a direct application of the PAGE estimator to local method in federated learning.\n- The convergence results for nonconvex setting is better than existing results in terms of dependence on the number of workers.\n\nWeaknesses:\n- The result in convex setting is not rigorous enough. See details below.\n- The method requires to have communication to all clients for every $\\frac{1}{p_r}$ iterations on average. This is similar to the full participation requirements for methods like FedSplit or FedPD. This appears to be not realistic as clients in federated learning might get disconnected from the server. It is better to just use a subset of clients to update as in FedAvg.\n- Numerical experiment only consider one nonconvex objective while theoretical results include both convex and nonconvex. Also, only SCAFFOLD and FedAvg.\n\nRegarding the theoretical results for convex setting, in the proof of Theorem 4, page 28, first the authors need $\\eta_g \\leq O\\left( \\frac{(1+\\delta)p}{L(1+\\sqrt{1/(Sp)}} \\right)$ and select some small constant to satisfy the condition that $w$ is non-positive. However, when deriving the rate for communication round, they actually use $\\eta_g \\leq \\Theta\\left( \\frac{(1+\\delta)p}{L(1+\\sqrt{1/(Sp)}} \\right)$, i.e. $\\eta_g$ has the same order of the term in the bracket, and plug in the choice of $\\delta$ and $p$. How can you guarantee that when $\\eta_g$ is in the same order of $\\frac{(1+\\delta)p}{L(1+\\sqrt{1/(Sp)}} $, $w$ is still non-positive.\n\nSome minor comments:\n- In equation (1), it should be $\\frac{1}{M}\\sum_{j=1}^M$.\n- Appendix, end of page 23, the first equality should be $\\frac{\\sigma^2}{Nb_1} \\mathbb{I}[b_1 < M]$.\n- Appendix, page 28, there are 3 cases for $\\eta_g$ under different range of $S$, instead of $\\Theta(\\cdot)$, they should be $\\Omega(\\cdot)$ as it should be least in the order of $(\\cdot)$ or higher.",
            "summary_of_the_review": "The paper does have contribution towards the development of FedPAGE and its analysis for convex and nonconvex setting. While the nonconvex result is shown to be better than existing ones, the convex result is not rigorous enough to me. Numerical experiments appear to not fully verify the theoretical results as they only contain nonconvex example.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper provides a new federated optimization method: FedPAGE, based on the recent variance-reduction based PAGE method. Latter is designed to solves smooth non-convex optimization problem. FedPAGE improves over variance reduction-based SCAFFOLD algorithm in both smooth convex and smooth non-convex settings. Finally the paper provides some numerical experiments to compare FedPAGE, PAGE, FedAvg, and SCAFFOLD.",
            "main_review": "### Strengths\n1. FedPAGE can handle larger effective learning rate.\n2. Theoretiacally FedPAGE improves over variance reduction-based SCAFFOLD algorithm in both smooth convex and smooth non-convex settings. \n3. Empirically FedPAGE is better than FedAvg. It is also comparable or better than SCAFFOLD.\n\n### Weaknesses\n1. Table 1: What should we focus on total communication bandwidth (S*R) or number of communication rounds (R)?\n\n2. Table 1:  Assumption $S \\leq \\sqrt{N}$ should be explicitely stated. Otherwise, FedPAGE may even only match SCAFFOLD’s round complexity.\n\n3. Not a lot of discussion on the limitations. \n\ni. SCAFFOLD never samples all clients at the synchronization rounds. That could be a too expensive waiting for this many clients to communication. Is this a necessity for FedPAGE? Is it possible to weaken this requirement?\n\nii. b1 and b2 are large. Negative effects of these are not discussed in this text. For example in convex, since b3=1, effective K=K+b2=K+ min(M,N^3/2/S^2). Even for S=\\sqrt{N}, this is equal to K+min(M,\\sqrt{N}). It is expected that authors discuss all such  cases.\n\n4. Experimental results are not comprehensive enough.  \n\ni. even though non-convex rate seems to be the main highlighted theoretical results, experiments are for convex optimization. \n\nii. It is not clear if comparing the algorithms at the same effective LR is appropriate, ideally they should all be tuned separately and the best stepsize choices for each algorithm should be compared against each other.\n\niii. Can the authors please provide comparison with more standard benchmark datasets like EMNIST.\n\niv. Since, in theory, FedPAGE is not improving PAGE, more comprehensive experiments could be useful for making a case for the former.\n\n5. Assumption b3=1 should be explicitly stated in the theorems.\n\n6. There are no rate comparison to non-local method like Accelerated SGD, PAGE, etc. From the results, FedPAGE and PAGE seems to have the exactly same round complexity. So it is not clear if the given theory is non-trivial. But the limited experiments seem promising since they show a constant but diminishing improvement when increasing K.\n\n7. It would useful if the authors can provide a discussion of any technical novelty in their analysis.\n\n### Other\n1. Table 1: What is this expectation $\\mathbb{E}[f]$ over?\n\n———————————————————————— \n### FINAL:\n\nI thank the authors for the response. However, after reading all the reviews, author response and public comments carefully, I have concluded that in this current state this paper cannot be publishable at this venue. Overall it seems there are many holes in the arguments of the paper. These are some additional comments detailing my reasoning.\n\n1. I acknowledge that the authors want to position this as a theoretical paper. However, in my opinion most of the theory of FL algorithms fail to explain their success in practice. Because of this it is hard to accept the usefulness of the algorithms or the theory without extensive experimental results. A prime example for theory not explaining practice is present in this paper itself! PAGE (FedPAGE with K=1) has the same convergence rate as FedPAGE (K != 1). This is possibly due to the fact that the proofs mainly show that FedPAGE is not worse than PAGE. However, experiments show that FedPAGE (K != 1) may be faster than PAGE. In fact this has been pointed out by other reviewers too. This is never addressed by the authors. \n\n2. Most of my concerns about the experiments are not addressed? Authors promise new experiments but doesn’t provide plots or data.\n\n3. I can agree with the fact that large(could be full)-batch local computations may be okay when optimizing number of communication rounds. But ideally this limitation should be discussed upfront in the paper. Public commenter “Durmus Alp Emre Acar” also raises an important point where SACFFOLD (with full-gradient and K=1) gets better rate than what is given in Table 1 for convex problems! Therefore the experimental and theoretical comparisons may not be fair.\n\n4.  Critical concern of Reviewer Gwh9 about the proof is not addressed. \n\n5. New concern: It is not clear were “Assumption 2” is being used in the proofs.\n\n6. Thanks for pointing out the technical novelty, although they are a bit limited.\n",
            "summary_of_the_review": "Results seems promising. However there isn’t enough discussion of limitations, novelty, and comparison to some baselines.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new federated learning algorithm to reduce the communication cost on both federated convex and nonconvex optimization by applying optimal PAGE method into the federated setting and running multiple local update steps on clients before communicating to the orchestrating server. Experiments verifies the effectiveness of multiple local update steps and its superiority over other methods.\nContributions:\n1)  Adapting PAGE algorithm into federated setting with probabilistic local updates reduces the communication complexity on both federated convex and nonconvex optimization.\n2) Numerical experiments demonstrates the effectiveness of multiples local update steps.\n",
            "main_review": "Strengths:\n1)  Adapting PAGE algorithm into federated setting with probabilistic local updates reduces the communication complexity on both federated convex and nonconvex optimization.\n2) Numerical experiments demonstrates the effectiveness of multiples local update steps.\n\nWeaknesses:\n1) Lack of analysis on test performance. Numerical experiments show that FedPAGE converges much faster than the comparison methods. How is the performance on test settings? Does the proposed optimization method achieve comparable or even better performance beyond convergence speed?",
            "summary_of_the_review": " Reducing the communication cost  with multiple local updates by applying PAGE in federated optimization.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes FedPAGE, a federated variant of PAGE, which is claimed to attain the SOTA communication efficiency theoretically.",
            "main_review": "1. Table 1: the convergence round does not account for the full-participation round. a) This communication round should be included as communication rounds b) the comparison with other algorithms is *unfair* since the other algorithms never allow for a full participation round.\n    - To make the above point clearer, let us consider Distributed-SVRG (with, say only 1 local step). Distributed-SVRG only requires O(1/ε) rounds of full participation, and O(L/ε) rounds of sampling. The total communication round is only O(L/ε).\n\n2. The paper assumes a strong, non-standard assumption (in FL study) that each client can access the *deterministic* gradient oracle of the local objective. The table 1 compares their results with FedAvg / SCAFFOLD, but all these methods are tailored for stochastic gradient oracle.\n\n3. Significancy: The main Theorem 1 and Theorem 2, are trivial applications of the PAGE with no local steps. The argument follows by the statement that running K > 1 of local steps are not worse than running K = 1 step. As a result of this trivial application, running K>1 local step gives no improvement.\n\n===\n\nI read the response and decided to keep my initial score.",
            "summary_of_the_review": " My main concern of this work lies in the significancy of the results, and improper comparison with other algorithms.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper considers a federated variant of the PAGE algorithm, called FedPAGE that utilizes local steps. The authors analyze the convergence behavior of FedPAGE algorithm and show that it is very similar to PAGE. And as PAGE has good convergence guarantees, compared to other distributed optimization methods, FedPAGE shows good convergence results compared to other federated learning methods with guarantees. The authors have a limited set of experiments to compare FedPAGE with other federated methods and also PAGE.",
            "main_review": "- The theoretical part of the paper is relatively well-written and clear.\n- The overall contribution of the paper is very clear. The paper takes a recent distributed algorithm with a good convergence rate, and adds local computation to the algorithm (which is very common in FL), and shows it still achieves a good convergence rate. And because the original algorithm has superior convergence results, the FedPAGE also enjoys the same superiority over other analyzed FL methods. But if one steps back a bit here, actually the convergence guarantee of the FedPAGE algorithm is not better than the PAGE algorithm. This is clear from the approach in the proof (Lemma 1) and Table 3, where K=1 would give the best convergence guarantee for FedPAGE. This is important because adding local updates is at the heart of most FL algorithms despite the fact that there is little theoretical evidence that they improve the method. Unfortunately, these points are not very clearly conveyed in the main text. Also, due to this fact, it is important to accompany the theoretical results of the paper with solid experimental results that can support the use of local steps in FedPAGE under different circumstances (heterogeneity levels) and against many solid FL algorithms. As I will point out later, this part is very much lacking in the paper.\n- It seems that the paper assumes the same number of data points per client and I am not sure if this can be easily relaxed. Such an assumption is unrealistic because in FL clients are heterogeneous.\n- One part of the FedPAGE that in my opinion makes it basically impractical (for large N practical FL problems) is the need for rounds that communicate with all the nodes. In spite of the fact that these rounds are very rare (due to low p) during the run of the algorithm, they are not possible in practice mainly because 1) not all devices are available at the same time for an update (update usually happen only when devices are on wifi and connected to power).  And 2) the delay for the responses has a very long tail; meaning that waiting for all the devices to respond in a round takes a long time. And 3) the devices can drop out during any round which means that you cannot count on all devices for a round. This means that the few rounds that need complete device participation might never finish (or take a very long time to do so).\n- It is not very clear to me why in FedPAGE the effective step-size is $\\eta_g$ (as opposed to FedAvg and SCAFFOLD). It would be great if the authors can elaborate.\n- Regarding the experiments (For the importance of the experimental section for this paper see my first point):\n  - No comparison is done at different heterogeneity levels of data (which are known to have a great effect on FL algorithms).\n  - Not a single result is reported for the achieved test accuracy or other metrics that are relevant to ML\n  - For the comparison against other FL methods, no comparison is done for state-of-the-art methods such as the one that uses momentum, ... ! SCAFFOLD and FedAvg are not empirically the state-of-the-art methods.\n  - I had a very hard time understanding how the HP tuning has happened. What parameters were tuned for each algorithm and how (what were the metrics and possible values). This is important in all the results but esp important for Fig 2 and 3 where there are other algorithms and their HPs are also involved and the comparison needs to be fair.\n\nIt is worth noting that I did not check the proofs of the paper thoroughly, but the claims and results seem reasonable and correct.\n\nMinor comments:\n\n- last paragraph of the introduction is not very well connected to the rest of the intro.\n- The algorithm does not summarize how a solution (x) is returned. I had to go through the appendix theorems and proofs to understand that the authors assume the return of one of the x^r (probably at random based on the convergence guarantee). \n\n===== After rebuttal =====\n\nI read the responses and other reviews/comments and have decided to reduce my score to 5. I agree with other reviewers that the paper has flaws and issues that have not been addressed and it is not ready to be published. I encourage the authors to strengthen the theoretical results by looking at the possible benefits of local steps and also include more informative experiments.\n\n",
            "summary_of_the_review": "Based on the above points, I would say the current version of the paper is slightly above the acceptance threshold. I would be willing to increase the paper's score based on the authors' responses/updates to my comments/concerns (mainly around novelty, assumptions, practicality, and experiments).  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}