{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper introduces a defense method (gradient broadcast adaptation) against backdoor attacks on pretrained language models. It proposes to utilize prompt tuning to guide the perturbed weights back to a normal state and thus helps avoid the degradation of model's generalization ability. \n\nStrengths:\n- Experiments are conducted across multiple datasets with different types of backdoor attacks, demonstrating the effectiveness of the proposed approach\n- The proposed idea is well motivated and intuitive\n\nWeakness:\n- Improvement on experiment results seems marginal\n- Some technical details of the attack setup are unclear\n- Writing of the paper needs improvement"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a defense against backdoor attack on pre-trained large language models. The proposed defense computes the average of the gradients per input sentence to contribute to updating all tokens in the sentence. The approach is empirically shown to outperform two baselines.",
            "main_review": "Strengths.\n- S1: The simple proposed approach is shown to defend backdoor attack targeting rare tokens.\n- S2: The approach is evaluated on 5 datasets against 4 attacks.\n\nWeaknesses.\n- W1: The approach seems to be based on the idea that the semantics of the tokens in the same sentence are similar. But this might not be true for more complicated tasks. Unfortunately, the experiments only consider tasks with only few classes, mostly two classes, and just one dataset with 4 classes. In this case, the task becomes learning if a word is related to the target class. More complicated tasks would show how this defense would perform in the real world.\n- W2: The description of the attacks used is not sufficient. While the core algorithms can be learned from the cited papers, the experiment setting doesn't explain how the triggers are chosen, what the portion of backdoored samples in the test dataset, or how they are constructed. Thus, it cannot be inferred if the evaluation was done fairly and properly. For example, BadNets (Gu et al., 2017) doesn't discuss poisoning of text models at all, and this necessary information cannot be found.\n- W3: The paper needs major improvement in writing. There are many errors (e.g., we usually takes, state-of-the-art, We are the first ... method, safely adaptation method, a method which do not ...), unnecessarily repeated sentences or words (e.g., all whole vocab, Sec 3.1), and missing explanations/definitions (e.g., V, \\theta^*). Missing information can be understood by a domain expert, but the paper should be self-contained as much as possible if not citing a prior work. Also, many statements are over-generalized.\n- W4: If the trigger does not appear in the training data, the embedding still wouldn't be updated as Q is computed and applied per sentence according to Eq 4. So, it is unclear how this update is significantly different from just updating the token with its own gradient only. Comparing the gradient and Q can be interesting. It almost looks like the effect is using a larger learning rate, but there was not sufficient analysis on this.",
            "summary_of_the_review": "This paper proposes a simple defense against backdoor attacks on pre-trained language models. Aside from the large room to improve writing, this paper has many other issues. While the simplicity is fine, this paper failed to show the exact effect of the defense as the rare tokens would be still updated only when it appears in the training data. Also, the evaluation was done for similar tasks where words can be directly grouped into each class. Moreover, the exact poisoning procedure was not explained. Thus, it is not possible to confirm the benefit of the proposed approach.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a method to defend against NLP backdoor attacks. The authors propose to calculate the global direction of gradients of loss with respect to input word embeddings and update word embeddings using the global direction. By doing so, rare words can be updated to a \"normal state\" and are expected to be no trigger of attacks anymore. The authors also empirically show the effectiveness of the proposed method.",
            "main_review": "==========After Rebuttal==========\n\nAfter reading all the comments, I tend to retain my score.\n\n================================\n\n\nStrengths\n1.\tThe proposed method is well-motivated and novel to me. \n2. The proposed method is easy to plug in fine-tuning or prompt pipeline.\n3.\tThe authors conduct experiments and show that the approach can help defend against backdoor attacks, with only a negligible generalization drop.\n\nWeaknesses\n1.\tThe empirical results may be only marginally significant. For example, in Table 2, the proposed method cannot surpass SOTA under several settings. Plus the current version only conducts experiments on bert-base-uncased. It would be helpful to validate the proposed method using at least one more pre-trained language model like RoBERTa.\n2.\tActually I like simple but effective methods. But given that the empirical results are only marginally significant, I am worried that the proposed method might be too simple.\n\n\\\nPlus, some technical details are not clear to me. See my questions below.\n\nQuestions:\n1. Should the probability ratio in Eq 4  be inside the $\\sum$? Or shall we use $w'$ inside the $\\sum$?\n2. For each minibatch, does the proposed method update all the embeddings of words in vocab or just update words present in the current batch? \n3. The proposed method can help defend against backdoor attacks with only 1% of clean training data, while the SOTA method NAD needs more. I am wondering whether this is only because of the few-shot property of prompt, or it is credited to the proposed gradient broadcast.\n4. What is the proposed soft template optimization for prompt? \n\n",
            "summary_of_the_review": "Given the points listed, I give the current rating here. It would be helpful if the authors can address my concerns.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper identifies an emerging threat for the prevailing pre-trained models -- the inheritance of backdoor attack, and proposes a simple yet effective defense approach: gradient broadcast adaptation (GBA). Instead of the traditional “erasing triggers”, GBA utilizes the “prompt-tuning” as a tool to guide the “perturbed weights” back to the normal state, which helps avoid the degradation of generalization ability. It provides an exciting and novel analysis of why backdoor attacks could be inherited during the pretraining and tuning procedure. Meanwhile, the authors perform an empirical evaluation of the proposed method against four state-of-the-art backdoor attacks.",
            "main_review": "In the security of pre-trained models, the inheritance of backdoor (adapt backdoored models to various downstream tasks) is an important challenge. Injected with the backdoor, the existing model could be further developed for a long life-circle with backdoor survival. This paper examines this interesting and under-explored topic. It locates the most vulnerable component of pre-trained language models – word embeddings, which is the reason for the ever-lasting backdoor. It shows that by carefully optimizing the word embeddings, and the malicious backdoor could be erased or repaired, taking no effect on further adaptation.\n\nBy taking inspiration from the “erasing backdoor” viewpoint of “Neural Distillation” prior work, it paves a new way of defense instead of “abandon redundant weights”, which obeys the generalization purpose of pre-training. The tool of “prompt-tuning” is also more natural to pre-trained models.\n\nThe paper proposes a much more intuitive optimization strategy that guides “those perturbed weights” back to the normal state by joining the adaptation stage, thus erasing backdoors while preserving the generalization ability of pre-trained models. Additionally, as a plugin of the optimizer, the usage scenarios of the proposed approach are unlimited.\n\nExperiments show that just by focusing on the word embeddings, one can disable nearly all backdoor attacks. I suggest the author consider this as a “firewall” to the standard adaptation operations.\n\nComments:\n\n1. The paper is well organized, and the motivation is clearly written.\n2. The authors try to unleash a wave of security concerns in pre-trained models. Many existing works focus on improving the performance of pre-training and adaptation while neglecting the threats of backdoor attacks/adversarial attacks. Following this paper's problem setting and experiment results, it may be easy to plug GBA into the current pipeline. One of the benefits is that we do not need to worry about performance degradation or computation efficiency.\n3. After reading this paper, I can hardly think about a better alternative for the proposed GBA approach. To the best of my knowledge, the root of backdoor attack inheritance only could be the lazy update of rare tokens. This is consistent with the authors’ claim. However, there may be other non-parameterized ways, such as information-entropy smoothed or neighbor cluster, in the repair of backdoored tokens. I am wondering if it can work, but it deserves a trial. I also have some additional questions to discuss with the authors:\nAre all the current backdoor-erasing methods or backdoor-outline methods (mainly proposed in CV) not useful in the case of pre-trained language models?\nCan we view the GBA method as a variant of neural distillation or pruning? So if I were to turn this into a method that outlines the triggers first and then erases them in a blacklist.\nFigure 2 shows a clean acc drop when increasing the data size of clean data from 0% to 1%. Does GBA hurt the model performance under few-shot settings more than the former distill-based method?\n",
            "summary_of_the_review": "This paper targets an under-explored fundamental challenge in pre-training and adaptation trends. This is a brave and valuable step in the security research of pre-trained models. I enjoy reading this paper, and this is the first time I have seen a reasonable solution to the inheritance of the backdoor phenomenon. Although some problems need to be corrected, I believe authors should be able to take them in hand.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}