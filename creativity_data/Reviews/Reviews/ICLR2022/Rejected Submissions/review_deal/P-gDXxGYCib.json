{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper received a majority voting of rejection. During the internal discussion, all reviewers insisted their original scores. I have read all the materials of this paper including manuscript, appendix, comments and response. Based on collected information from all reviewers and my personal judgement, I can make the initial recommendation on this paper, *rejection*. Here are the comments that I summarized, which include my opinion and evidence.\n\n**Research Problem**\n\nIn this paper, the authors consider a novel scenario that feature selection in the contrastive setting, where an extra *background* dataset is utilized to remove the background noisy features. However, this problem can be easily handled with a fully supervised feature selection method, where the samples in the *target* datasets are annotated as 1 and the samples in the *background* datasets are annotated as 0. Therefore, the research problem addressed in this paper is not novel. Reviewer UFq8 and ft7b held the same opinion. \n\n**Technical Points**\n\nThe technical part could be more informative. The whole framework is based on auto-encoder based self-reconstruction, where the feature selection is finished by the recent CAE model. In my eyes, the major contribution of this paper lie in learning $g_z$, the background representation function. To achieve this, the authors proposed three strategies, *joint*, *pretraining* and *gates*. The *pretraining* idea does not involve any information from the target dataset, where the background representation function is a general one and it has no relationship with the target dataset. I believe the concept of background should be defined based on the target dataset. The *joint* idea suffers from the information leak, which was pointed by the authors. We can also see the inferior performance of the joint model, comparing with two other models. Unfortunately, the philosophy of *gates* is unclear.\n\n**Experimental Evaluation**\n\n(1) The authors only compared with one supervised method on the semi-synthetic dataset. No results of supervised methods on real-world datasets were reported. (2) The performance with different numbers of selected features were not reported."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper is about feature selection in a weakly supervised setting using a background and target dataset and the goal is to learn features that are specific to the target dataset. The paper formulates the problem using a latent vector for data generation. There is a background latent vector z and a salient vector s. For the background model, the salient vector s is fixed where it has variations specific to the target. The general idea of this paper is to learn an encoding function that takes the input data and generates a low dimensional embedding g_z to captures the variation in z and not the variations in s. We define a loss function that takes this z along with the features to reconstruct the original target points. The main challenge here is that some of the salient features may leak through g_z and we want to avoid this. The paper presents different methods to learn these functions, and the validation is done on a semi-synthetic dataset and a biomedical dataset.",
            "main_review": "Pros:\n\n1) Modeling the latent vector as salient and background latent vector is interesting. \n2) The proposed algorithm does not use too much supervision and only relies on the knowledge of whether a data point belongs to background or target datasets.\n\nCons:\n\n1) One of the main weaknesses of the proposed method is the choice of k. It is not clear as to how one would choose the number of salient features. \n\n2) I have doubts about the basic problem formulation. The paper talks about contrastive analysis, but completely ignores the recent work on contrastive learning in ML community that uses strong data augmentations and ranking loss functions to generate visual representations that work well for a wide variant of downstream tasks, e.g., Chen et al. 2020 SimCLR.\n\n3) The paper also has some similarity with the notation of isolating factors of variation and the disentanglement literature, e.g., Locatello et al. 2019. In some sense we can think of the underlying problem as identifying the factors of variation in the target dataset that is not present in the background dataset. There is a large literature on disentanglement and representational learning that is relevant to this work. \n\nFrancesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Ratsch, Sylvain Gelly, Bernhard Scholkopf, and Olivier Frederic Bachem. Challenging common assumptions in the unsu-pervised learning of disentangled representations. InInternational Conference on MachineLearning, 2019.\n\n4) There is no error bars or statistics for the results reported in Fig. 4. The baseline used is just a concrete autoencoder. Stronger baselines would make the paper strong. \n\n5) The experiment to measure disentanglement is a bit inconclusive. The main idea is that the target features should do poorly in generating the background dataset since it captures both the background and the salient latent variables. The poor reconstruction could be due to many other reasons in addition to capturing salient variables.\n\n6) For grassy MNIST, I am surprised that the authors use a simple MLP architecture. We typically use CNNs and ResNets for imaging data. The use of simple MLP seems like it may not learn much about complex factors of variation. It would be good to provide details on the experimental parameters such as batch size, learning rate, and number of epochs. \n",
            "summary_of_the_review": "Overall the paper addresses and important problem, but the proposed methods and the experimental results need significant improvement and clarification. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a weakly supervised feature selection algorithm which selects features which have different amounts of signal compared across two datasets (a background or control dataset, and the dataset of interest). This allows it to sidestep the requirement for labels to select features for a downstream task or group of tasks. It uses a concrete selection layer to find relevant features with three different approaches for separating them from the background features.",
            "main_review": "Comments:\n- The evaluation only uses extremely randomized trees as the classifier, which is not ideal for a feature selection evaluation as the randomness means that it's hard to know if the whole feature space was useful. Some kind of regularized gradient boosted tree, or other non-linear but less randomized classifier would also be helpful in evaluating the selection procedure. Or even an MLP given the feature selection system is in pytorch.\n- There are fairly simple filter baselines for contrastive analysis once the problem has been introduced. For example selecting features which have different variances or entropies as compared to the background dataset, or selecting features which maximise the KL divergence between p(x_b) and p(x_t).  Without these baselines it's hard to say whether it's the additional supervision from the background/target difference or the specific techniques proposed in the paper which improve performance.\n- Could the authors expand on the difference between the pretrained and gated systems with reference to the experimental results? It's not clear in what circumstances one technique should be preferred over the other, neither experimentally nor theoretically. Presumably the pretrained method took longer as there were two separate training runs, but there are no runtimes reported either.\n- Overall the presentation of the contrastive analysis problem is well motivated, the dataset choice and descriptions are helpful and the paper is well written.\n\nQuestions:\n- How big was the background set for grassy-MNIST? \n- Are the grass images sampled with replacement or without replacement, and if without replacement is it the same sample across both background and target datasets? Are any grass images repeated?",
            "summary_of_the_review": "The problem the paper investigates is interesting, the algorithm seems like a sensible extension of unsupervised techniques to the weakly supervised setting.  The experimental study has weaknesses, but still justifies the proposed technique.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "A new formulation of selecting the top k informative features, when two sets of samples are available: one containing background patterns and patterns of \"potential\" interest, and the other having background patterns only. Proposed objective is quadratic error minimization between the original example and vector obtained using reconstruction function applied on background embedding vector concatenated with k selected original features. Three different ways of learning the reconstruction and background functions was proposed and evaluated on several datasets (synthetic and real world ones) against unsupervised baselines.",
            "main_review": "The article is communicating the proposed approach and conducted exercises in a very clearly and concise manner. Mentioned related work is relevant and credits to prior studies are given appropriately. Both the formulation and solutions proposed appears novel.\n\nEmpirical evaluation is performed on augmented MNIST problem (synthetically fused digits with images of grass), where accuracy is measured for different hyperparameter settings, as well as on 4 unaltered biomedical datasets. Proposed approach (in two out of three flavors) showed superior performance in terms of accuracy. Baseline feature selection methods used are exclusively unsupervised, while proposed approach has \"unfair advantage\" of being trained in a \"Contrastive Analysis\" setup. That is, proposed Contrastive Feature Selection had a certain insight into downstream problem of classification. It would have been interesting (and highly relevant) to see how would the method compare to the supervised feature selection approaches (e.g using target and background sets as a binary classification problem).\n\n",
            "summary_of_the_review": "Even though I think the approach can be valuable, my main concern is the motivation to use it when background and target sets are already known. How does its utility compares to supervised feature selection methods? Can Contrastive Feature Selection be applicable to the cases when the target and background samples are not given (or clearly separated)?",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Summary. The paper considers the problem of feature selection in the contrastive analysis setting. In particularly, the authors are interested to discover features that reflect salient variations enriched in some target dataset compared to a background dataset. A new method, called Contrastive Feature Selection, is proposed.",
            "main_review": "Strengths. The reported accuracy of the experimental results is extremely good. \n\nWeaknesses. In general, there are a number of unclear points in the paper. Fig. 3: \"captures variations due to our uninteresting latent variables z\", what do you mean by that? Do you mean that $g_z$ is not useful?\n\nNot clear sentence: \"A diverse array of prior work exists on unsupervised feature selection, with one major differentiating factor between methods being how to define the “usefulness” of a feature.\" \n\nThe feature selection is performed using the existing stochastic gating layers, so, the feature selection novelty is absent. The final framework is quite complex, is not extremely clearly described. \n\nIn the experimental section, it is mentioned that to assess the quality you train an extremely randomised tree classifiers. Am I right that this part is not drafted on Figure 3? You perform the feature selection (the framework on Fig. 3), and then a classifier (in your case randomised trees) are applied to learn the predictive model?\n",
            "summary_of_the_review": "The paper presents a combination of an existing feature selection method (called stochastic gating layers) and a classifier (to test the models performance). There is a lack of novelty in the paper. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}