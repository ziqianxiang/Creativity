{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper aims to scale transformers to large graphs. In this regard, authors propose to first obtain a \"coarse\" version of the large graph using existing algorithms. With reduced number of nodes in the coarse graph, we can employ the transformer efficiently to capture the global information. To capture the local information, GNNs are employed. Finally, authors carry out extensive experiments on a range of graph datasets. Also, reviewers do appreciate reporting the confidence intervals. We thank the reviewers and authors for engaging in an active discussion. Unfortunately, the reviewers are in a consensus that novelty of the proposed method is limited: it is combination of existing techniques and similar ideas have been widely used in the literature. Also, the empirical results are not very significant. Thus, unfortunately I cannot recommend an acceptance of the paper in its current form."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In this work, the authors mainly combine the GNN and Transformer to learn both local and global information in large-scale graph.",
            "main_review": "strengths\n\n1. The paper is easy to follow, the ideas combine the GNNs and Transformer is interesting, take benefits of the GNN, local aggregation and Transformer, global aggregation.\n\n2. The coarsening part is good, to reduce the complexity of the self-attention in the Transformer.\n\n3. A sampling techniques are also adopted for mini-batch training.\n\n\nWeakness:\n1. The novelty of this work is not very high, it combines a lot of existing of techniques, for example GNN, Transformer, Graph Coarsening, Sampling, PPR etc. These methods are widely explored[1, 2].  The overall design is very straightforward. Somehow I feel the model is a little heavy with more model parameters.\n\n[1] https://openreview.net/forum?id=uxpzitPEooJ\n[2] https://arxiv.org/pdf/2106.05150.pdf\n\n2. The idea of the global and long range dependecies is also widely expolited, For example \n[1] Geom-GCN: Geometric Graph Convolutional Networks\n[2] Improving Breadth-Wise Backpropagation in Graph Neural Networks Helps Learning Long-Range Dependencies\n",
            "summary_of_the_review": "In summary, I think the idea of combine GNN and Transformer is very interesting. However, the novelty of this work is very straightforward, a mixture of many existing work.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to use the transformer architecture on the coarse graph obtained from the graph coarsening algorithm.\n\nTo be more specific,\n* Since directly using the transformer architecture to the large-scale graph is computationally prohibitive, the authors use the existing graph coarsening algorithms to the large graphs, and then use the transformer on the coarse graphs, which can capture the global information of the given graph in contrast to the GNNs capturing local information.\n* To simultaneously use both local and global structures of the graph, the authors first separately capture the local and global information of the given graph with GNNs on the original graph and transformers on the coarse graph, respectively, and then propagate information of local and global to each other with the cross-view propagation scheme.\n* The authors show that the proposed Coarformer outperforms baselines on node classification tasks, and also show that the proposed model is highly efficient against the global transformer models for graphs.",
            "main_review": "### Strengths\n* Application of transformer on large-graphs with graph coarsening is a relatively undiscovered problem (though not entirely novel, see below weaknesses).\n* Information propagation across GNNs and transformer to capture both local and global information on graphs is novel and valuable.\n* The paper is well-written and easy to understand.\n\n### Weaknesses\n* Using transformer on the compressed graph is already discovered idea [1]. Specifically, [1] captures the local information on the original graph with GNNs, and then further captures the global information on the compressed graph with transformer. Also, the idea of alleviating computational complexities, both memory and time, in the paper is similar to the mentioned previous work [1].\n* In Table 2, it seems the authors do not incorporate the time for graph coarsening. If the authors compress the large graph, then the resulting computational cost for coarsening is likely to be large. What relative cost does the proposed method needs for graph coarsening?\n* In Figure 2, the authors only display the GPU memory usage of small size graphs, namely Cora and Chameleon. I think the extra memory usage required for the proposed method is relatively increasing, when we use the large-size graphs that authors tackle. Thus, since the authors target the large graphs, I recommend the authors to change the datasets in Figure 2 to larger ones, such as OGB or PubMed.\n* In Table 4, I think the experimental comparisons against baselines are not enough or entirely fair. The authors exclude the powerful transformer-based baseline, namely Graphormer, but also it seems the authors use a small number of batches for baselines (Correct me if I misunderstand).\n\n### Typos\n* In Section 3.3, I think the $\\beta$ in the sentence \"we adopt $\\beta=1$ in our experiment\" should be changed to the $\\gamma$. The $\\beta$ is already defined as the learnable parameter for the PPR matrix in Section 3.1.\n* In the caption of Table 1, \".\" is missing at the end.\n\n---\n[1] Baek et al., Accurate Learning of Graph Representations with Graph Multiset Pooling, ICLR 2021.\n\n---\n**-------------------- After rebuttal --------------------**\n\nSumming up all my reviews and follow-up comments, I think this paper is above the borderline, and I lean towards an acceptance of this work.",
            "summary_of_the_review": "I think this is a borderline paper, as graph coarsening to use the transformer on large graphs is not entirely novel, however, it also has clear merits: a novel scheme of information propagation across original and coarse graphs; an almost complete piece of work except for some minor issues in the weaknesses above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes Coarformer, a two-view architecture that consists of a standard GNN learning pipeline, and a Transformer over a coarsened graph.\n\nThis paper represents an interesting idea that scales Transformer to large graphs.",
            "main_review": "Overall, this paper tackles an important question on scaling Transformer to large graphs.\nThe proposed method is technically sound. \nThe experimental results are not very strong (performance gain over baselines are marginal).\n\nWith that being said, I have the following concerns for the paper:\n\n1 The proposed Coarformer is not well motivated and seems overly complicated. \nIt seems that the main contribution is to operate the Transformer on a coarsened graph. \nThe rest of the method is quite standard.\nI think a simpler model to discuss is to use the GNN node embeddings as the input to the coarsened graph, or directly combine the predictions from both models.\n\n2 Proposition 1 is not well explained. I'm not sure why it can lead to the claim that \"Coarformer can cooperate with mini-batch training and thus is viable on large graphs\".\n\n3 How important is the graph coarsening algorithm? How scalable is the algorithm being used?\n\n",
            "summary_of_the_review": "Overall, I like the motivation of the paper. The proposed methodology should be improved though. And the experimental results are not very strong.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes hierarchical neural network model for processing graphs. The lower level can use any GNN model suitable for processing smaller subgaphs of the original graph while the higher level part realized by a transformer operates on an abstracted 'coarse' graph.",
            "main_review": "Strong points\n* I find the empirical part of the paper solid. Training is performed over many splits. The model is tested on several datasets and many baselines and ablations are presented.\n* The model is more memory efficient than other baselines which might be important in practical applications.\n\nWeak points:\n* Most of the results where Coarformer outperforms other baselines are not statistically significant (e.g. in Tab 1 and 3), there is usually some baseline that has mean value inside the confidence interval of Coarformer. However overall the model performs strongly and it is great that authors report the confidence intervals. \n* The idea of using hierarchical graph representations to bridge long range dependencies isn't new, however I feel that prior work isn't properly discussed, other hierarchical graph models are missing in the Related Work section. Papers like DiffPool https://arxiv.org/abs/1806.08804 are highly relevant in my opinion. Other relevant papers are e.g. https://aclanthology.org/2020.emnlp-main.710.pdf or https://arxiv.org/pdf/2105.03388.pdf \n\nQuestion:\n* Sec 3.1 --- Is use of PPR as attention bias in graph transformers new? No citation is in that paragraph.\n* Loss on coarse graph is just auxiliary, correct?\n* How does the number of Coarformer's parameters compare to other compared models? Is lower memory consumption just a function of fewer parameters or is it solely due to the different structure of the computation graph?\n* Why are you testing just transformer based architectures on the global scale? E.g. GraphSAGE can be used for both local and global level.\n\n\nPresentation improvements:\n\n* BERT (Kenton & Toutanova, 2019) is a strange citation for BERT, I was expecting Devlin et al.\n* Cite PPR.\n* Sec 3.1 says that \\Beta is a learnable param, however Sec 3.3 says it is set to 1 for simplicity. I would omit \\Beta completely since I don't see any value in it. In every formula one can add similar hparams that are set to 1 and they don't add any value to the model.\n* Related Work (no S)\n* Define HPO, I get that it is hyper param optimization but it isn't explicitly stated.\n* Cite SPD.\n* Add at least one sentence describing VN, VE and Alg JC and cite them.\n* Instead of Tab 5 a graph with 'c' on the x axis would be more appropriate. It will be easier to interpret the data and see possible trends in it.\n* Table 12, Cornell column: Coarformer is in bold even though GT has better result.",
            "summary_of_the_review": "I am slightly leaning towards acceptance. While I think that related work should be better discussed I think that many people can be interested in low memory overhead of the model and the whole community can benefit from that.\n\n--------------- Post Rebuttal -------------------\nAfter the latest round of improvements I believe the paper is above the acceptance bar.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}