{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper describes a new testbed to evaluate Bayesian techniques in the context of joint predictive distribution.  Since this is not the first paper that considers marginal vs joint distribution evaluation, the paper should include a thorough discussion of the differences with prior work.  The paper simply states that it refutes Wang et al.'s previous observation that joint distributions do not distinguish techniques much more than marginals.  However, the paper does not really explain why their observation is correct and Wang's observation should be discarded.  Since this is the core contribution of the paper and it is doubtful, this is problematic.  The discussion of epistemic/aleatoric uncertainty also seems superfluous and therefore distract the reader."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work advocates evaluating predictive distributions via joint predictions (rather than the standard practice of evaluating marginal predictions) and introduces The Neural Testbed, an open-source software which includes the testing suite, along with implementations of a handful of methods in uncertainty quantification (UQ). The core evaluation metric used is KL divergence (cross-entropy loss) between the predictive distribution and the true likelihood of the data-generating process, and this work proposes an algorithm to compute this metric with joint distributions. The empirical evaluations compare numerous standard UQ methods with the Testbed, with both synthetic and real datasets.",
            "main_review": "Strengths:\n\n- This paper is generally well-written and clear. Evaluating predictive distributions is an important and relevant topic to the general UQ community.\n- As far as I can tell, the idea of evaluating predictive distributions via joint distributions over the numerous predictions is quite novel, as well as the proposed algorithm.\n- The experiments section does a good job in demonstrating the discrepancies that can arise between marginal and joint predictions during evaluation, and these findings are interesting.\n\nWeaknesses:\n\n- As this work propose an evaluation metric, I believe it's important to have a discussion on the suite of metrics that are currently used in UQ and how the proposed metric provides an advantage over existing ones. This paper only touches upon KL-divergence (equivalently, cross-entropy, or likelihood), and doesn't mention other widely used metrics, such as calibration, or other proper scoring rules.\n- In fact, would calibration (+multiples notions exist, e.g. ECE, classwise-ECE, adaptive ECE, ...)  of the marginal distributions also fail to differentiate between the different methods in Figure 2?\n- What is the significance of using $\\tau=100$ for the joint predictions? Would there be discrepancies in the ranking of methods based on other $\\tau$'s? In practice, if this metric was used for model selection, which $\\tau$ should the practitioner base the metric on to choose a model?\n- On the same note, I believe there needs to be more demonstration/elaboration that a model with better joint predictions is \"necessarily better\" than another model that has worse joint predictions, but possibly as good or better marginal predictions. In the second paragraph of page 2, the authors hint that some Bayesian models are not performant in sequential decision tasks because they have poor joint predictions. The proposed metric would be more convincing if this point was followed up with, or if other downstream tasks were demonstrated in the experiments.\n\nQuestions on content:\n\n- In Section 4.1, what does the environment variable $\\varepsilon$ represent, concretely? Is it the choice of SNR and the initialization distribution over the data generating NN? Let me know if I am missing something here.\n- In the last sentence of the second paragraph of Section 4.1, I don't understand what is meant by \"agent is provided with the data generating process\"? How is this done, concretely?\n- In Step 1 of Algorithm 4, what exactly is the distribution, $\\mathbb{P}(\\hat{\\varepsilon} | f_{\\theta_{T}})$? How is this distribution produced?\n\nOther points:\n\n- The font and spaces between lines for this submission are quite different from the default style. However, as far as I can tell, it seems to reduce space for content, so I did not flag it.\n- Table 1 protrudes beyond the side margins — I'm not sure if this isn't a formatting violation.\n- In Figure 6(b), which $\\tau$ setting was chosen each of the blue points?",
            "summary_of_the_review": "As noted in the section above, while I do think this is interesting work with interesting findings, I am not fully convinced of the merits of the metrics proposed. Further comparisons and discussion of other existing metrics, along with demonstrations of the benefits of better joint predictions in a downstream task would be helpful in making a stronger case for this work. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a simulation based framework to evaluate different techniques proposed for uncertainty estimation of predictive models. The approach relies on simulated data to control effects such as environments where data is collected, data and model uncertainty. This control enables generating interesting insights about various techniques. \n\n The paper also go beyond evaluating marginal posterior predictive distributions and extend their benchmarking work to joint distributions capturing sequential decisions that can be made with such models. Some of the highlights from the results: 1) Their results show that Bayesian deep learning is impactful for capturing the joint predictive distributions. 2) Priors used in ensemble+ help with diversity and therefore enable better predictive distributions. 3) Bootstrapping help with robustness of predictions if the models hyperparameters are not tuned.",
            "main_review": "Pros:\n\nBayesian deep learning is an important research area but the evaluations of the progress made in the field has not been consistent so far which makes it difficult to evaluate different techniques. Furthermore some of the common practice on evaluations on out of distribution data provides insight but lacks providing a quantitative benchmark. The paper addresses an important problem in Bayesian deep learning and provides a framework and tools to evaluate different techniques. The framework is based on simulations which helps provides control over variables and enables gathering insights about various techniques. Furthermore, the paper extend the evaluations to sequential decisions which makes the contributions unique.\n\nOverall I found the contributions of the paper very important for the field. I also think the paper is very written and organized.\n\nCons:\n\nEven though in the abstract the authors mention that the proposed method provides insights into aleatory and epistemic uncertainty, the manuscript does not elaborate on this point. Even without this point, I think the paper is interesting and removing this point will not hurt its reach.\n\nBayesian deep learning researchers are in general interested in out of distribution generalization. I found the discussion on OOD a bit limited. I believe the proposed framework can also enable evaluations with OOD and I would recommend authors adding a discussion on this.\n\nMinor comments:\n- Abstract: Please explicitly define what you mean by \"joint predictions\".\n- Figure 6. Would be interesting to color code the agents in the Figure.",
            "summary_of_the_review": "Overall, I vote for accepting. I like the idea of using simulated settings with explicit control on variables to perform benchmark analysis and this paper enables this through a novel framework as well as open source tools. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new benchmark for approximate inference methods, with emphasis on comparing the quality of joint predictions instead of marginals. The proposed testbed is comprised of synethetic test functions generated by simple MLPs, and authors show results on the synthetic benchmark are well correlated with results on small scale real world datasets as well.",
            "main_review": "I appreciate that benchmark emphasizes joint predictions as an important component in evalauting uncertainty estimates. I agree that these joint predictions are very important for certain applications of uncertainty estimation, especially in sequential d\necision making settings. However, I am not convinced this benchmark will be a significant and useful contribution to the community in its current state, for the reasons below.\n\n**Existing benchmarks**: Wang et al 2021 [1] already emphasize the differences between joint and marginal uncertainties and propose various ways of evaluating joint predictions, including a downstream transductive active learning task. Interestingly, Wang et al find that directly evaluating joint likelihoods gave little more information than simply marginals, while this paper concludes that different approximate inference methods can produce very different joint likelihoods with similar marginal performance. I suspect the difference here is in the sequence length $\\tau$ used to evaluate joint predictions; Wang et al appear to only evaluate over 5 data points, while this paper uses 100 to show major performance differences. It would be good to include additional results at different $\\tau$'s between 1 and 100 to see how the difference between joint and marginal predictions grows.\n\nAnother benchmark proposed by Wilson et al 2021 [2] also focus on evaluating the faithfulness of Bayesian posterior approximations, though they only evaluate marginal uncertainties. Nontheless, I find their benchmark has several advantanges: they study much larger scale networks (albeit this implies much higher computational cost) and also consider real world input distributions instead of only synthetic ones.\n\n**Regarding likelihoods as the evaluation metric**: While (joint) likelihoods are certainly a very natural choice for comparing predictions, what ultimately matters is how useful the predictions are in making downstream decisions. In addition to just measuring likelihoods, I think the benchmark would be a much stronger contribution if it also emphasized comparisons on downstream tasks, which can be synthetically generated similarly to the existing settings.\nExamples of possible downstream tasks that really test the quality of joint predictions could include active learning, contextual bandit, and Bayesian optimization tasks, plugging different posterior approximations into standard representative algorithms for each task (though Wang et al 2021 already do propose transductive active learning as a benchmark task for joint uncertainties). \nAnother set of potential tasks focusing more on the quality of marginal predictions could include tasks like selective classification, possibly in conjuction with different levels of synethetic covariate shift. Being able to accurately gauge the (relative) trustworthiness of predictions (especially with distribution shift) is certainly an important task for uncertainty estimation, even if it does not rely on the quality of joint predictions. \nWithin the scope of synethetically generated tasks, I think the inclusion of additional evaluations on a variety downstream tasks into a single consolidated benchmark would be a much stronger contribution, and emphasize how different applications of uncertainty estimation can have different needs and be more amenable to different algorithms.\n\n**Regarding trying to approximate true Bayesian posteriors:** Another hesitation I have is that the focus is that the target is given by quality is being measured against a ground truth consisting of random MLPs, whlie it is hard to say whether these pri\nors actually perform well when faced with real world data and tasks. For example, in Izmailov et al [3], they find that (in larger scale settings), exhaustively running HMC, with presumably better approximations of the true posterior, actually performs worse than approximate methods like ensembling when faced with distribution shift, suggesting simply trying to faithfully approximate exact inference is not necessarily the right (or at least only) goal we consider. \n\n**Scale and synthetic nature of the benchmark**: I also have reservations about the scale of the proposed benchmark, as it is focused on a small data regime with very simple models (MLPs). While the authors state that the synthetic results are predictive\n of real world data, the real world datasets evaluated are also quite toy in nature, and extending insights to larger scale models and datasets like in [2] is quite important.\n\n**Regarding likelihood evaluation in Algo 4**: I believe it would be helpful provide more explanation of Algorithm 4 in appendix A.1. Currently, there is little motivation for the different steps in the algorithm, and I would appreciate greatly if the authors could summarize the relevant concepts from high-dimensional geometry as applied here. Given the paper is proposing a new set of evaluations, it would be especially important to make sure that users have confidence that the metrics used are meaningful. If no formal analysis of the accuracy of the approximation is feasible, there should at least experimental comparisons with the direct Monte Carlo estimate (run with exhaustively many samples to estimate the ground truth) to compare the sample complexities of the proposed estimate at different sequence lengths, as well as documentation of how hyperparameters of the estimator were or should be determined.\n\nCitations:\n\n[1] Wang, Chaoqi, Shengyang Sun, and Roger Grosse. \"Beyond Marginal Uncertainty: How Accurately can Bayesian Regression Models Estimate Posterior Predictive Correlations?.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2021.\n\n[2] Andrew Gordon Wilson, Pavel Izmailov, Matthew D Hoffman, Yarin Gal, Yingzhen Li, Melanie F Pradier, Sharad Vikram, Andrew Foong, Sanae Lotfi, Sebastian Farquhar. \"Evaluating Approximate Inference in Bayesian Deep Learning.\" https://izmailovpavel.github.io/neurips_bdl_competition/.\n\n[3] Izmailov, Pavel, et al. \"What Are Bayesian Neural Network Posteriors Really Like?.\" arXiv preprint arXiv:2104.14421 (2021).\n~                                                                                                                                   ",
            "summary_of_the_review": "I believe the scope of the benchmark is quite limited, and the paper does not show any strong benefits provided by this benchmark over other already existing benchmarks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors discuss whether it is sufficient to consider the marginal posterior predictive vs considering a joint posterior predictive when evaluating Bayesian deep learning approaches. Introducing a set of experiments (_The Neural Testbed_), they demonstrate that the performance of common approaches can differ greatly depending on which of these predictive distributions are evaluated. Additionally, extensive code is provided for efficient implementation and evaluation of new models.\n",
            "main_review": "_Edit: Thank you for the rebuttal and detailed discussion. I consider the results an interesting contribution that is relevant to the field, which is why I vote for acceptance. The limited score is due to the presentation and discussion of topics in the paper not directly related to the theoretical contribution (see discussion below for details). A proper rewrite of the related sections to guide the reader to focus on the main topic marginal vs joint posterior predictive would strengthen the paper a lot._ \n\n___________\n\n## Strengths\n- The paper is overall well written and structured\n- The experiments demonstrate the difference between marginal and joint posterior predictive the authors ask the community to focus on\n- An extensive library with code is provided, which allows for the implementation and test of further methods\n\n## Weaknesses\n- The title reads a little bit too attention-grabbing, especially given that the paper itself does not actually target the question of whether Bayesian deep learning (BDL) works, but rather whether evaluating a marginal posterior predictive is sufficient, of whether one should consider joint posterior predictive. \n- The abstract claims to solve \"what roles epistemic versus aleatoric uncertainty play\", and the end of the introduction also claims to \"resolve a point of philosophical debate\" on this topic. However, can the authors comment to in which respect they do that? As far as I read it, the authors claim that epistemic and aleatoric uncertainty are highly dependent on the model (e.g. they differ depending on the model, yet can lead to the same predictive distributions for distinct models), and that to compare models, one should rely on the posterior predictive. (To be more precise in the rest of the paper, the joint posterior predictive.) But is that really a point that is being debated and needs to be resolved? That what is reducible and irreducible uncertainty is always conditioned on the model, and that we, therefore, need to focus on the posterior predictive to compare different models with each other properly seems to be completely obvious and not challenged by anybody to my knowledge. (Unfortunately, the supposed philosophical debate lacks all references.)  \n    The realization that we cannot usefully compare ir/reducible uncertainties between models does, of course, not at all tackle the question of whether it can be useful to distinguish between them within, i.e. conditioned on a specific model, to gain a deeper understanding of why the current model performs as it does, or how one can improve it (taking e.g. Depeweg et al. (2018) as an example of an active learning context). \n- Section 2.1 and 2.2 read as if they are due to the paper. However, both the $\\tau$th-order KL-loss as well as a minor variation of the example are due to Lu et al. (2021), which are only cited in passing without highlighting this fact.\n- Given the small nature of the nets further comparisons against an HMC giving access to  the _true_ posterior seems feasible and relevant in the set of models discussed, and if it is infeasible after all an SGHMC (Chen et al., 2014) could give a second point similar to the Langevin `sgmcmc` to have a richer exploration of the posterior. \n- Given the topic, Izmailov et al. (2021) seem highly relevant with similar results concerning ensembles and `sgmcmc` performances (abbreviated SGLD in their case). This prior work is missing completely in the discussion.\n\n\n## Specific additional questions to the authors\n- Can the authors clarify the differences/similarities with Lu et al., 2021?\n- Can the authors comment in greater detail on the claimed different observations between the current work and Wang et al. (2021)?\n- The nets used in the experiments are rather small. Do the authors have any indication that the findings remain stable with deeper nets?\n\n\n## Minor\n- Several parts of the paper are \"confusing\", \"puzzling\", \"surprising\", unclear as to what is \"really Bayesian\", and seem to be formulated with the primary goal of teasing \"Bayesian purists\". While this is a nice structure for a poster/conference talk to provoke some good discussions in the next break, it feels somewhat unnecessary in a paper. _(Feel free to ignore this complaint, as it is only my highly subjective prior)_\n- Similar to the comment on the teasing structure of the paper and the earlier comments on aleatoric/epistemic, the XKCD comic seems somewhat out of place. Especially given that it is introduced as if the deep learning community is its cause. While I do not know the original story behind Munroe's comic, the discussion of different types of sources of uncertainty is a lot older than our current deep learning popularization.\n- \"We see that ensemble ... actually provide much better approximations to the Bayesian posterior than 'fully Bayesian' VI approaches like `bbb`\". I would claim that this finding is not too surprising. Given that most VI approaches take a unimodal, mean-field approximation as their starting point, it is directly clear that the approximation to a highly multimodal posterior is terrible. It seems reasonable that an ensemble approach gives a richer signal as long as its members do not collapse to the same optimum. This has also been extensively evaluated and demonstrated by Izmailov et al. (2021). \n- ~~I might have overlooked it, but the hyperparameter details in the appendix seem to miss the number of samples from the posterior for the BNNs.~~ _Edit: All details are provided, I had just overlooked them. See the author response below_\n\n### Typos\n- The datasets in Sec 5.1 give (TFD) as a reference, while the references only contain the long-form TensorFlow Datasets.\n\n\n\n______\nDepeweg et al. Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning, ICML 2018  \nIzmailov et al. What Are Bayesian Neural Network Posteriors Really Like?, ICML 2021",
            "summary_of_the_review": "An overall interesting paper in a new direction (after the contribution of Lu et al. is clarified) to evaluate BDL models that tries a bit too hard to be controversial. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}