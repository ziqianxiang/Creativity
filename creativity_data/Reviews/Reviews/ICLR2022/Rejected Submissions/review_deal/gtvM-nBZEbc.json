{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes a framework for novel object captioning by combining BERT and CLIP.  The model improves fluency, fidelity, and adequacy of generated captions. However, as reviewers mentioned, the novelty is limited, combining large models and big data to solve a downstream task does not make useful insights at this moment."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The papers presents an approach for the task of novel object captioning (NOC). NOC requires the model to describe novel objects appearing in test images that were not described in the training captions. The authors claim that existing methods are not good at the task because they don't tackle three concepts required to produce a good caption — \n\n1) Fluency: generating human-like captions, \n\n2) Fidelity: whether the caption is describing the novel object in the image or not, \n\n3) Adequacy: whether the caption covers all the salient visual concepts in the image. \n\nThe authors propose a model Visual-Linguistic Adequacy, Fidelity, and Adequacy (VLAF2) to address these issues. The model utilizes BERT to sample different variations of the captions by substituting random words from the caption. It uses CLIP to assess which of these are more relevant to the image. The model also uses CLIP score and combine it with CIDEr based reward to further train the captioning model via reinforce. \n\nThe model is evaluated on the nocaps benchmark on which it achieves improvements over existing sota methods like VINVL+VIVO. The authors also show that apart from captioning metrics, the model also does better on fluency (B@4, C), Fidelity (Precision), Adequacy(Recall).",
            "main_review": "### Strengths\n\n1. Overall, I concur with the key insight from the paper which is that models for NOC should tackle fluency, fidelity and adequacy. It has been observed in Agrawal et al 2019 (nocaps dataset) that methods suffer from these issues. Captions which are fluent often aren't accurate, and captions which are accurate often aren't fluent.\n2. The authors take reasonable steps to fix these issues. Using a LM to fix fluency is a good idea. The authors address one of the main issues with such an approach — BERT like language models can produce fluent captions but it's hard to judge their accuracy. The authors use CLIP to score these and select the best method. Such an approach makes sense.\n3. Another novel contribution in this work is using uncaptioned images (images in the OpenImages dataset which do not have associated training captions) during training. Since these don't have training captions, they modify the reward to use CLIP as well penalize repeated words. Indeed, they show that CLIP + repeated penalty is helpful for overall performance (Table 3).\n\n### Weaknesses\n\n1. One of the biggest and most important weakness in the paper is that the paper doesn't follow the guidelines from the nocaps paper. As mentioned in the nocaps paper (section 3.3), \"Do not use additional paired image-caption data collected from humans. Improving evaluation scores by leveraging additional human-generated paired imagecaption data is antithetical to this benchmark – the only paired image-caption dataset that should be used is the COCO Captions 2017 training split. However, external text corpora, knowledge bases, and object detection datasets may be used during training or inference\". The paper heavily relies on CLIP which has been trained on alt-text data for images (which are human written) for improving the captioning performance. This makes the method proposed in the paper not comparable to existing methods in my opinion. The authors should have used the nocaps (XD) for evaluation.\n2. Related to the second point, I would have liked to see if you can replace CLIP with other approaches (ViLBERT) which also outputs a score indicating how well the caption is aligned with the image. This would form a fair comparison.\n3. Since CLIP is trained on alt-text data for images, it would be interesting to see if joint-training on Conceptual Captions (or a similar web-scale data) works better instead.\n4. CLIP is being used for checking the relevance of the caption with the image. I wonder how will other similar metrics for caption relevance (VIFIDEL) compare against scores from CLIP.",
            "summary_of_the_review": "Overall, I think the paper presents an interesting insight (breaking down caption quality into fluency, adequacy and fidelity) and presents an approach which improves all the three aspects on the nocaps dataset. I raised a couple of concerns (use of CLIP which has been trained on human-generated alt-text data) which goes against the guidelines of the benchmark. It nullifies some of the claims in the paper and I'd be interested in hearing from the authors regarding that. I would have also liked to see comparison with other caption relevance metrics like VIFIDEL which would make the contributions stronger. Overall, I am borderline on this paper and will update my thoughts based on the author response.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a framework that combines masked language models (BERT) and image-text embedding models (CLIP) for novel object captioning. The proposed model significantly improves the fluency, fidelity, and adequacy of the generated images. Extensive experiments are conducted and the comparison between the proposed model and existing models shows that the proposed model performs relatively well.",
            "main_review": "Strengths:\n\nS1: This paper pays attention to three important properties of captions -- fluency, fidelity and adequacy and proposes a formwork to generate fluent, faithful and adequate captions to describe images.\n\nS2: Extensive experimental results show that the proposed model outperforms existing models on novel object captioning.\n\nWeaknesses:\n\nW1: The proposed model combines some existing models, like BERT, CLIP, so one of my concerns is the novelty of the proposed framwork. Plus, the reward to train the model is not novel, which is used in Rotian's work [1] and distilling from BERT for text generation was proposed by Chen [2]. Also, the observation that repeating the objects that occur in an image can improve retrieval score is not new [3].\n\nW2: Neigher BERT nor CLIP is trained using the dataset used in the paper, so how to mitigate the problem of domain shift when applying BERT and CLIP? I do not think you can assume that BERT and CLIP are always correct.\n\nW3: It seems that BERT is only used during training. I am wondering whether BERT can be used in the inference stage, for example, using the captioning model to generate a caption with beam search and then employing BERT to generate more captions. In this way, we can see whether BERT can improve fluency.\n\nW4: Lack of human study.\n\n[1] Rotian Luo et al.. Discriminability objective for training descriptive captions. CVPR, 2018.\n\n[2] Y-C Chen et al.. Distilling Knowledge Learned in BERT for Text Generation. ACL, 2020.\n\n[3] Qingzhong Wang et al.. Describing Like Humans: on Diversity in Image Captioning. CVPR, 2019.",
            "summary_of_the_review": "This paper proposed a strong baseline of novel object captioning via combining BERT and CLIP models, however, the novelty is limited.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to improve novel object captioning i.e. describing objects/contents that are not seen during training. Specifically, the authors propose to correct the captions generated from captioning models using pre-trained BERT and CLIP models by rewarding the captions with precise and rich visual content. They show qualitative and quantitative improvements on CiDER and SPICE scores compared to previous work and propose to measure the fluency, fidelity and adequacy of the generated captions for a more comprehensive evaluation of novel object captioning.",
            "main_review": "### Pros:\n- Leveraging prior visual-linguistic knowledge from pre-trained BERT and CLIP models for the task of novel object captioning. The association between fidelity, fluency and adequacy with the training tasks of BERT and CLIP presents a good motivation for the method adopted. \n\n- The reward based loss functions designed for encouraging caption novelty and fluency and to discourage repetitive captions is interesting and novel. \n\n- The authors design fidelity/fluency and adequacy quantitative evaluation to measure the efficacy of their approach compared to previous work and show improvements. \n\n### Clarifications:\n- Is the BERT model finetuned with the captions from the nocaps dataset in eq 2?  How is the CLIP condition satisfied in eq2, what happens if both the captions are unreliable ? Is there a threshold above which the CLIP scores are compared ?\n\n- The BERT equations also could fulfil the fidelity objective because of the gating function. Is this observed?\n\n- Are the improvements from Eq5. coming because of the r_CIDER term in the labelled caption data? Are the results in Table 1 compared with CIDER optimization for baselines? \n\n- What is observed if the reward is not calculated over the labeled training samples X_l?\n\n- How will  the model perform if Eq 2 is performed during the inference stage only from a pre-trained captioning model?\n\n- In fig 1, it is not very clear where the second caption “two glove hands...” comes from, is one from greedy decoding and other from sampling  ? \n\n- Are the novel words mostly objects/nouns?\n\n### Cons: \n\n- While the method is effective and simple, there are certain ablations which could have been performed qualitatively and quantitatively to draw where the improvements come from. \n\n- The improvements in the method over the SOTA are via CIDER optimization on labeled examples in Eq 5. If the baselines are trained with CIDEr optimization, it might improve overall performance as well. \n\n- The motivation of improving fluency or adequacy is probably not only related to novel image captioning but for image captioning in general. Could this method help to address those problems there? \n\n- As stated in the paper, that previous methods produce grammatically incorrect captions for NOC, that does not seem to be the case (eg in fig 4). There are details missing from baseline captions but they are still fluent. \n",
            "summary_of_the_review": "The paper is well written with detailed experimental results to support the claim. The paper proposes interesting ideas to improve NOC and is marginally novel but there are still some concerns as stated in the main review. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper tries to generate novel object captions that meantime satisfy three aspects: fluency, fidelity, and adequacy. To achieve this goal, it exploits two large-scale pertaining models which are BERT and CLIP. Specifically, given a novel object caption, the method first masks some words (expect nouns) in the sentence and then use BERT to generate these masked words. After that, they compare the association degree between two captions (the original caption and BERT re-generated caption) and the image. Such association degree is computed by inputting the image and the caption into CLIP. Based on the association degree, the method decides whether the BERT generated caption should be used as training supervision. ",
            "main_review": "1. Novel object captioning is one sub-task of image captioning and generating captions that are more fluency, fidelity, and adequacy is one sub-requirement of captioning. However, this method exploits two large-scale models BERT and CLIP to solve this small task, which is not an economic solution. In this way, I think the motivation of the research is not quite significant. Actually, not only novel object captions are not fluency, fidelity, and adequacy, even the captions generated by standard captioning settings (trained by MSCOCO) may also have low fluency, fidelity, and adequacy (Or say that standard captions' fluency, fidelity, and adequacy can be further improved). It would be better if the authors could apply their method to standard captioning or more settings which have limited training data other than only one novel object setting.\n2. In Sec 3.1, the authors mention that they mask certain words instead of nouns since nouns may appear in the image, which should not be generated by BERT. However, the attribute and relation words may also appear in the image, which should not be masked. For example, some horses may be white and some others may be dark. Or in some images, a man ''ride'' a horse while in another image a man ''stand near'' a horse. Then these attributes and relations should also not be masked.\n3. At the end of the first paragraph in Sec 3.2, the author claims that ''word-by-word supervision tend to imitate the sentence patterns of the training images instead of relating caption data to the visual content'' while do not provide suitable evidence. \n",
            "summary_of_the_review": "The major concern is about the significance of the motivation and the applied solution is not very economic by using two large scale pre-training models to solve a small task. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}