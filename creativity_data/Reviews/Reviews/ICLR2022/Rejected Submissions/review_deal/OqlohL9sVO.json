{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes a new method to combine global and local image features, targeted at image retrieval applications. The main idea is a model branch where both spatial and channel attention are used. The local feature branch undergoes supervision directly and this branch’s output is concatenated to the global feature branch’s output in order to eventually produce the final image embedding. \n\nThe reviewers appreciated the care in the evaluation (ablative analysis) and the promise of the approach compared to existing baselines. The reviewers also expressed concerns about several claims, for instance that the proposed approach is able to learn homography transformations, the quality of the exposition, and missing baselines. The reviewers also pointed out that several parts of the paper were hard to follow and important details were missing. \n\nThe authors submitted responses to the reviewers' comments. After reading the response, updating the reviews, and discussion, the reviewers considered that ‘the concatenation of local features with global ones works does not mean at all that some geometric transformation is learned’ and the justification provided for omitting baselines (suggested by the reviewers) were unconvincing. The feedback provided was already fruitful, yet major issues still remain.\n\nWe encourage the paper to pursue their approach further taking into account the reviewers' comments, encouragements, and suggestions. The detailed feedback lays out a clear path to generate a stronger submission to a future venue.\n\nReject."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper addresses the problem of image retrieval and proposes a method that tries to learn better representations through the use of local feature attention and reduces memory and latency overheads by not requiring re-ranking with local features and only using global one.\n\nFor this, the paper proposes a module called Local Attention Learning Module (LALM) that performs both spatial- and channel-wise attention on local features and that it's plugged between two consecutive blocks of the backbone architecture. The output of this module is then concatenated to the input of the next block. The rationale behind this is that the following blocks will act as an homography transformation of the local features and will produce a better global representation. The method also uses intermediate supervision by computing a cross-entropy loss on top of the output features produced by the LAL Module.",
            "main_review": "Strengths:\n- The paper proposes an interesting approach that obtains very good results using only global features, outperforming two-stage methods that use local features for re-ranking.\n- Experimental evaluation is also complete and thorough, with a very extensive ablative study.\n\n-----\nWeaknesses:\n- The main weakness of the paper it has to do with its presentation: it has numerous syntax and grammatical errors and typos, which makes for a challenging read sometimes. Some paragraphs are difficult to follow and in my opinion should be rewritten. The main problem with this is that it affects the motivation of the paper: I feel I couldn't fully grasp the rationale behind some design choices or the conclusions drawn from some of the experiments. It is difficult to provide a complete list of all the things that should be addressed by the authors but I'll discuss in section \"Presentation\" below what I think are the most importante ones. Nevertheless, I highly encourage the authors to do another pass on the writing.\n- Slightly related to the point above, I'm missing a deeper and more detailed explanation about the claim that the last convolutional blocks act as an homography transformation because there's no experimental or theoretical study that proves that in the text. Overall, the contributions of the paper are not clear enough.\n- Not too important: I understand that the focus of the paper is removing the re-ranking step, but it would have been interesting to see if the local features produced by the LALM module could have been used for this purpose to further improve the results in applications that can afford the extra memory and latency costs.\n- I wondered why the authors decided to use average-pooling and cross-entropy loss for the intermediate supervision instead of GeM or ArcFace as for the global features. Any insight about why this shouldn't work better?\n- Apologies if I missed this in the text, but how are R50-UGALR* and R50-UGALR in Table 2 are different? I couldn't find a reference in the text to the dimensionality reduction mentioned here.\n\nPresentation:\n- The caption of Figure 1 only half describes the diagram there so I would suggest to extend it to give a better overview of the method. It is also missing the legend/explanation of what is \"A\", \"X\", \"M\", and \"C\".\n- \"At present, there is such a problem in image retrieval: although the results of global feature-based retrieval are correct in the category, they are not sorted in detail\" -> technically, the problem in instance-retrieval datasets is that they're not correct in the category so that's why re-ranking is performed.\n- \"But if we don’t differentiate the importance of attention and features, it’s hard to make sure that the learned attention is not part of features, which is not an ideal way for attention learning.\" -> this sentence should be re-written\n- \"However, these methods have not been well modified yet to solve image retrieval problems.\" -> this sentence should be re-written\n- \"in the next small section.\" -> in the next section\n- The nomenclature used in Figure 2 does not match with the text nor with the formulas in Eq 1-4\n- In page 4 the text mentions \"D\", which I'm assuming it's the output of \"X\" in Figure 1, but it has never been defined before.\n- The previous to last paragraph in page 4 is difficult to follow and it should be re-written more clearly. This is a crucial paragraph to properly understand the rationale behind the LALM module and the intermediate supervision and in its current shapes it loses impact.\n- \"We assume that there is an identical average target available for every category of objects\" -> I couldn't understand this assumption\n- Eq 6 makes reference of La but it was never defined in the text before (only in figure 1)\n- \"Obviously, UGALR completely outperforms previous global features based methods\" -> \"As we can see, UGALR...\"\n- \"The most amazing two improvements we made are\" -> \"The largest two improvements...\"\n- \"Amazingly, our approach using only global features outperforms\" -> \"Surprisingly, our approach...\"\n- Spaces are usually missing just before parenthesis",
            "summary_of_the_review": "Overall, even though it doesn't introduce any impactful contribution, I think that the paper proposes an interesting method that is able to obtain very good results using only global features, outperforming methods that also combine re-ranking with local features. However, I find it difficult to recommend this paper for its acceptance in its current shape since its presentation is not adequate. Introduction and Method sections are sometimes difficult to follow and I couldn't fully grasp some of the ideas and rationale behind the design choices. Another pass in the text is required, and figures and formulas should also be updated and improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this work, the authors propose a Unifying Global and Attention-based Local Features Retrieval method (referred to as UGALR). UGALR accelerates extraction speed and reduces memory consumption by removing the re-ranking process and learning local feature matching with convolutional neural networks instead of the RANSAC algorithm. In addition, UGALR learns more accurate and semantic local information by combining spatial and channel attention with intermediate supervision.",
            "main_review": "The proposed method is feasible. However, its novelty is pretty limited. Also, many points are confusing for readers.",
            "summary_of_the_review": "Due to the limited contributions, I do not recommend this paper to publish on ICRL.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "1. Main contributions are not clear. The main structure is simple and normal, where the attention techniques are adopted to extract the local information.\n2. I do not think re-ranking is an indispensable part of an image retrieval system. Thus, the motivation of this manuscript is not valid enough.\n3. Section 3 is described as shallow, which prevents readers from understanding your main idea.\n4. The influence of free parameters should be studied.\n5. More latest image retrieval models should be added to testify your model. \n",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a new pipeline architecture for image retrieval. The main purpose of the proposed architecture is to overcome the time and space consumption due to the images' re-ranking that exploits local features (after the search using global features). \nIn order to do that, UGALR is proposed as a single-stage pipeline able to combine global and local features to speed up the retrieval process, removing the re-ranking step. This is achieved using a CNN able to learn the homography transformation in local feature matching.\nMoreover, the architecture combines spatial and channel attention with the aid of intermediate supervision, obtaining better performances than the SOTA models presented in the analysis of the results. This is achieved with the introduction of a Location Attention Learning Module (called LALM).\n",
            "main_review": "Strengths:\n* the proposed architecture is deeply described, justifying each of its parts;\n* the experiments are performed using two datasets and the proposed model is compared with a lot of other models, providing a serious and robust discussion;\n* the ablation analysis is complete and justifies the use of each component of the architecture, describing numerically why each macro-layer (e.g. LALM) helps to improve general performances;\n* all the results against the SOTA models are complete and well described;\n* one of the main (well-described) goals of the architecture is to speed up the retrieval process reducing the used space. A comparison of these aspects has been clearly done, demonstrating the great advantages of UGALR with respect to the other models.\n* the benefits in terms of time and space do not preclude an improvement in accuracy performance, which is higher than in the baseline models.\n\nWeaknesses:\n* even if complete, the description of the architecture seems a little bit confusing. For example, the use of capital letters in figure 1 (and then in the text)  is confusing;\n* the comparison in terms of time and memory seems ok but is not very clear if all the models were tested on the same GPU (using the same environment). If not, this is a real problem and the results cannot be taken into account;\n* there is no error analysis. The accuracy is higher with respect to baseline models, but not that much; in order to study why, an error analysis would be helpful, maybe providing a comparison with the common errors of the other SOTA models.\n\nMinor grammar errors:\n* in figure 2 you use $\\varphi$ (varphi) but in the text $\\phi$ (phi) is always used;\n* lack some spaces between words and brackets (but this could be a template prerogative)\n",
            "summary_of_the_review": "The paper describes the proposed architecture proving a complete explanation of all of its parts (with the help of a complete ablation analysis). The comparison with a lot SOTA models on two datasets confirms the robustness of the proposed model. Even if the improvements in terms of accuracy are not so high (not a negative point, there is however a clear improvement) there is a huge improvement in terms of speed and memory used. An improvement for the paper would be the addition of a complete error analysis that now is not present and a better description of the entire architecture that, at present, is not very clear. Moreover, is not clearly specified if all the models have been tested on the same GPU (by the authors) to test speed and memory usage, this is an important point to specify because is one of the main goals of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a new method to combine global and local image features, targeted at image retrieval applications. It designs a new local feature model branch where both spatial and channel attention are used. The local feature branch undergoes supervision directly (in the paper called “intermediate supervision”), and this branch’s output is also concatenated to the global feature branch’s output in order to produce a final image embedding at the end. ",
            "main_review": "Strengths:\nS1) Combining local and global image features is a promising research direction.\nS2) The paper provides several experiments, with many ablations.\nS3) Some performance improvements seem to be observed compared to previous work.\n\nWeaknesses:\nW1) The authors naively claim several times in the paper that their method is learning homography transformations (Introduction, Methodology, Experiments, Conclusions). However, one can clearly see that this is not the case. The authors are simply concatenating local feature maps with the ones from the global feature branch and supervising by ArcFace classification loss. The authors seem to be assuming that homography learning is implicitly happening, which is naive. Similarly, the abstract claims that the proposed method is “learning local feature matching with convolutional neural networks instead of RANSAC algorithm” – this is incorrect.\nW2) The paper is not well written, and several parts are hard to understand. For example, in Fig 1 the paper has blocks named A, X, M, C – what do they refer to? Similarly, in page 4 the text start referring to “D” (“D is then followed (...)”), but D had never been introduced before. D (and O, which is also later introduced) should be added to Fig 1, and A/X/M/C should be explained in the caption/text. It is also very difficult to understand the way that local and global feature maps are combined – the paragraph “Backbone Network” and the one before it are quite confusing.\nW3) The authors seem to have suspiciously excluded results from previous work. For example, Tab 1 does not show numbers for “R50-DELG (global+local)” in the “+1M” dataset setups, which are available in the exact same paper where the other numbers from the table were taken from. I suspect that they were not included because in half of the cases the proposed method underperforms compared to “R50-DELG (global+local)” – specifically, the ROxf+1M cases.\nW4) In Tab 2, the authors simply take latency numbers reported in previous work and compare to their own for the new method, using a different GPU. The comparison does not seem appropriate.\n",
            "summary_of_the_review": "The paper addresses an important technical direction of fusing local and global image features. However, it has several major flaws, such as: claiming learnable matching/homography when none of that is happening, suspiciously excluding previous work’s performance numbers in some cases; poor writing.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}