{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "In general, the reviewers were lukewarm about the paper. They all acknowledged the strength of the paper: it is well written, HCCL showed (somewhat) improvements over previous methods, and it is easy to implement. However, it still feels incremental, and the improvement over the full training setting is small due to the natural limitation of consistency assumption. The AC feels that while there is merit of the proposed method, the impact seems to be limited to specific scenarios such as limited epochs."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a hierarchical cross contrastive self-supervised learning framework for learning visual representation. This paper proposes to project the representations of an image and its augmented version to multiple latent spaces and also make predictions on each of the latent spaces.  A contrastive loss between the features of different projection levels is minimized to learn the parameters. Experiments on the image classification and detection benchmarks are evaluated. A comparison between important existing methods is also done in the paper. ",
            "main_review": "This paper starts with the motivation of self-supervised learning methods followed by the summarisation of the important methods. Afterward, the paper observes the trend of the existing methods being imag-level predictions utilizing global representations. Paper argues that the gap between the image-level pre-training and downstream tasks is not filled yet. It sounds interesting but it is not clearly mentioned how this gap exists in a more detailed manner.  As this paper advocates the need for hierarchical cross contrastive learning, a clear insight on the issue with the methods relying only on a global representation learning would make this paper stronger. \n\nThe method is straight forward and it is clearly present in the paper. Pseudocode has made even easier to understand the pipeline. \n\nAnother strong point about this paper is the extensive evaluations. Although the performance improvement over the existing methods is marginal in most of the cases, the pair have done evaluations on multiple benchmarks and also present the ablation studies.\n\n",
            "summary_of_the_review": "The research problem is interesting.  Novelty on the idea is modest and the performance compared with the existing method is also modest.\n\nThe presentation of the paper is good but it lacks a clear insight on the motivation for hierarchical architecture and also the way cross-correlation is established. What would be the performance when cross-correlation has been established in a different manner than that present in the paper? As there is potential to have different combinations of cross-correlation dependencies between the layers.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors proposed a Hierarchical Cross Contrastive Learning (HCCL) method for Self-supervised learning (SSL) of visual representation.\nThe proposed method include a design of a hierarchical projection network that produces multi-level latent representations. A cross contrastive loss is also introduced to learn invariant visual representations.\nHCCL is validated on several downstream tasks including classification, segmentation and object detection.",
            "main_review": "Strenghs\n- The paper writing is clear.\n- The cross-level contrastive learning shows the improvements in comparison to previous methods.\n\n\nWeaknesses\n\nIn general, the contributions of the paper is incremental from previous works as most of the building blocks of the proposed method are adopted from SimSiam (Chen & He, 2021) and BYOL (Grill et al. 2020).\n\n1. Novelty: the main difference between the proposed and SimSiam approaches are the hierarchical projection heads and the cross contrastive loss between each head in a branch with the head of previous level in the other branch. \nAlthough the experimental results show some improvements, the intuition of cross contrastive loss to make improvement is unclear.\nFor example, as the encoder f already extract high-level features, which information is further embedded in the hierarchical projection heads?\nWhich aspect (i.e. feature distributions, embedded information, etc.) can be enhanced in cross contrastive loss to help the improvements?\n\n2. Experiments: As illustrated in ablation study (Table 5, 6, 7), the improvements seem to be the results of several tuning steps (i.e. number of additional levels, predictor learning rate). In particular, the adaptation of Hierarchical and Contrastive loss only cannot help to improve the performance as presented in Table 5.\nThe authors should analyze in details to show the actual improvements obtained by the proposed approach.\n",
            "summary_of_the_review": "Although the experimental results show some improvements, the contributions of the paper is incremental from previous works and the intuition for the improvement is unclear.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose an extension to the contrastive learning based representation learning approach. The proposed method, call Hierarchical Cross Contrastive Learning(HCCL), leverages the features in different levels and views for more consistent features over the standard CL using only the features of the final layers. The effectiveness is validated in classification, detection, segmentation, and few-shot learning tasks. ",
            "main_review": "The consistency assumption over the representation in different views and different layers has been partially explored in many feature regularization approaches. In most cases, this assumption was directly implemented with a consistency loss between feature pairs.  While the consistency across views is well established as the basis of contrastive learning, most practices showed that the features are only partially similar among the last few layers. For this reason, researchers have to carefully select the layers and parameters in tuning the performance. see examples of related work in [1-4].\n\nThe authors address the limitation of the assumption above with several transformation modules between compared features. The performance did show good improvement in the cases of limited training data and short training epochs. The improvement for the full training sets and full training epochs are, however, limited. This may lead to the concern that the information introduced by the proposed approach has been naturally learned with the existing datasets and training settings.\n\nReference:\n[1] Hao Guo, Kang Zheng, Xiaochuan Fan, Hongkai Yu, and Song Wang. Visual attention consistency under image transforms for multi-label image classification. CVPR, 2019. \n[2] Yuenan Hou, Zheng Ma, Chunxiao Liu, and Chen Change Loy. Learning lightweight lane detection CNN by self-attention distillation. ICCV, pages 1013–1021, 2019. \n[3] Lezi Wang, ZiyanWu, Srikrishna Karanam, Kuan-Chuan Peng, Rajat Vikram Singh, Bo Liu, and Dimitris N Metaxas. Sharpen focus: Learning with attention separability and consistency. ICCV, 2019.\n[4] Zeyi Huang, Yang Zou, Vijayakumar Bhagavatula, Dong Huang, Comprehensive Attention Self-Distillationfor Weakly-Supervised Object Detection, NeurIPS, 2020.",
            "summary_of_the_review": "The approach proposed in this paper is straightforward and well implemented. The DNNs trained by this approach benefit mostly in the case of small training sets and epochs. The advancement over the full training settings is still small due to the natural limitation of the consistency assumption mentioned above. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This manuscript proposed a new contrastive self-supervised learning approach (HCCL). Compared with exiting work such as BYOL and SimSiam, HCCL introduced (1) multiple hierarchical projectors and predictors; and (2) contrastive loss calculated across different layers of projectors. HCCL is empirically evaluated on several self-supervised learning benchmarks (e.g., iNat18, Place-205, and COCO instance seg, etc.) and achieves noticeable improvement compared with previous states of the art (e.g., SWAV, BYOL, SimSiam, and Barlow Twins). Additionally, ablation studies are provided to verify the significance of hierarchical projectors, cross contrastive loss, and higher learning rates of the predictor.",
            "main_review": "Strengths\n1. The proposed approach, especially the cross contrastive loss, is novel and interesting.\n2. The proposed approach is fairly easy to implement.\n3. The proposed approach is evaluated on multiple benchmarks and achieves noticeable improvements compared with previous states of the art.\n\nWeaknesses\n1. Although empirically evaluated, the intuition behind the cross contrastive loss is still not clear. More theoretical analysis or interpretation should be provided to understand why cross contrastive loss works better. Specifically, what is the difference in features learned with and without cross contrastive loss? Will the features across different projector layers become closer after trained with cross contrastive loss?\n2. The training protocol for serval downstream tasks is not consistent with previous work. Since the authors simply report the performance from existing literature, it would be more critical to use consistent training protocols for a rigorous comparison. For example,\n    - Table 2 in this manuscript reported the results of Table 1 in Barlow Twins [1]. However, Barlow Twins uses a learning rate of 0.002 for the conv-net weights and 0.5 for the final linear layer, while HCCL uses 0.02 and 300, respectively.\n    - Similarly, Table 3 reported the results of Table 3 in Barlow Twins. But the training epochs for Places-205 is 14 for Barlow Twins, but 30 for HCCL.\n3. On page 6, the authors pointed out that \"BYOL has a greater training cost, because It[it] requires 4 times forward in one iteration.\" However, HCCL has much more trainable parameters by introducing more projectors and predictors. Taking two projector heads as example, HCCL has about 35% more parameters compared with BYOL (~49M for HCCL and ~36M for BYOL). It would more reasonable to compare the memory footprint and time during training instead of forward-propagation times.\n4. In Table 7, the \"Fixed\" learning rate is confusing. Does it mean the predictors us the same learning rate as the rest of the network?\n5. In Table 8, what is the performance of HCCL+Momentum Encode for 400ep and 800ep? Can it outperform BYOL? If not, what is the possible reason that HCCL cannot benefit more from longer training time compared with BYOL and Barlow Twins?\n6. There are several typos in the manuscript:\n   - In the caption of Figure 1, ’̸=’ should be `=’;\n   - Page 6, \"because It requires 4 times...\" should be \"because it requires 4 times...\";\n   - Page 7, 2nd line, missing space in \"Table2\";\n   - Page 8, 3rd paragraph, missing space in \"section4\".\n\n[1] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Ste ́phane Deny. Barlow twins: Self-supervised learning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021.",
            "summary_of_the_review": "The authors proposed an interesting hierarchical comtrastive learning approach that achieves noticeable improvement over serval benchmarks. \n\nHowever, it would be more convincing if some theoretical analysis or empirical interpretation could be provided to help the readers understand why hierarchical project heads and cross contrastive loss works better. Additionally, some justification regarding downstream task training protocols should be provided for the sake of fair and rigorous comparison with previous work.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}