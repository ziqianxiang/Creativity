{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The authors present a method to learn representations of 3D atomic structure. They consider two cases: \"small\" and \"large\" molecules based on a metric that takes the spatial extension and number of atoms in the molecule into account. Small molecules are represented by an interatomic distance map. Large molecules are represented by a \"sinusoidal function-based absolute position encoding method\". Both settings make use of a transformer architecture on top of the initial representation. The authors also introduce a subsampling step to select a subset of points/atoms and aggregate information from these.  Experimental results are shown for datasets relating to small molecule property prediction, protein-ligand binding and a dataset from material science on metalorganic compounds.\n\nStrengths:\n\n- interesting modification of transformer architecture dedicated to the chemical compound.\n\nWeaknesses: \n\n- Poor presentation of methods with respect to prior work\n- Limited technical novelty. The distance map representation for small molecules and the sinusoidal function-based absolute position encoding method for larger molecules have previously been proposed. Many components are built upon the design of PointNet++(Qi et al., 2017b) without significant modifications. The proposed \"3D-Transformer\" is very similar to an attention-based PointNet++ that is specially designed for molecular data.\n- Experiments are applied only on classification tasks.\n\nAll reviewers voted for rejection. I recommend the authors to address the limitations listed above by improving the presentation with respect to prior work, clarifying more the novelty of the methods and including a more diverse range of experiments."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors target the molecular property prediction problem and propose the 3D-transformer to utilize 3D geometric information of molecules and take advantage of the advanced (graph) transformer architecture. Specifically, the authors propose three key components of the 3D-transformer, namely, the adaptive position encoding, the multi-scale self-attention, and the attentive farthest point sampling. ",
            "main_review": "The paper studies an emerging problem of learning representations from 3D structure of molecules. Several interesting methods taking advantage of the molecular graph transformers and molecule 3D information are proposed. The experimental results seem solid. However, there are several concerns and questions regarding the methods and presentation.\n\n1. The current presentation needs improvement. The description of the entire computing pipeline is not clear, and many details are missing or confusing. For example, it’s unclear that what the notation \\boldsymbol{A} stands for in Eq.(4) and the notation of input/output are inconsistent for Eqs (4) and (5). The description “A are accumulated along column … computation by maximizing \\tilde{A}_i” and Eq. (6) (a “max” is missing?) are confusing. Also, according to the description, line 1 in Algorithm 1 seems incorrect (\\tilde A <- sum of A along column?). I suggest the authors examine the writing again and make the presentation more clear.\n\n2. The motivation of absolute position embedding for large molecules is reasonable to me, i.e., it aims to distinguish chiral molecules. However, it is not clear that whether the absolute position embedding still satisfy the roto-transformation invariance, as it is still an important property for even large molecules. Besides, could the authors explain why chirality is not that important for small molecules?\n\n3. Is the threshold \\tilde{\\phi} a trainable parameter or hyper-parameter tuned in the grid search? If it is a hyper-parameter, then “Adapt” seems an over-claim. Also, the discrete condition separating large molecules and small molecules does not make sense to me, unless there is a pivot point on the molecule size and their properties. Are the thresholds the same for all datasets or tuned individually (I suppose the threshold would be the same according to the motivation)? Is there any insights on using the discrete condition instead of soft conditions such as a linear combination of CPE and APE? It would be better the authors also provide the learned/tuned threshold values for each dataset or other results to support the claim.\n\n4. The idea of AFPS is interesting as a novel sample-based global pooling (readout) operation. However, there might be a potential issue with the operation during optimization. The selection of the subset is based on the representation of each node, and the optimization of node representations requires gradient back-propagated through the node. If a node is not initially selected, the optimization of the node may lack of supervision (even with the gradient from attention, which can be weak), preventing the node to be further selected. Could the author compare the effect of different pooling methods?\n\n\nOther suggestions:\n1. Could the author provide all tuned values of hyper-parameters or constant numbers (threshold \\tilde{\\phi}, \\epsilon in AFPS, distance threshold \\tau_s, etc.) in the appendix for better reproducibility and a deeper understanding of the proposed methods.\n2. It would be better to move equations describing the computation of proposed methods to the main text (e.g., the computation of APE).\n3. As the proposed 3D-transformer consists of multiple novel components/operators, it would be better to conduct more ablations like Table 8 in C.1 (on more datasets) to help understand the effectiveness of each component.\n",
            "summary_of_the_review": "The paper studies an emerging problem of learning representations from 3D structure of molecules. Several interesting methods/operations taking advantage of the molecular graph transformers and molecule 3D information are proposed. The experimental results seem solid. However, there are several concerns and questions regarding the methods and presentation.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents an approach to represent 3D atomic structure in a machine learning context. \n\nThe paper distinguishes between representations for \"small\" and \"large\" molecules based on a metric that takes the spatial extension and number of atoms in the molecule into account. Structures of small molecules are represented by an interatomic distance map. Large molecules are represented by what the authors refer to as a \"sinusoidal function-based absolute position encoding method\". In both cases, a transformer architecture is used on top of this initial representation. The authors further introduce a subsampling procedure to select a subset of points/ atoms and aggregate information. \n\nThe authors demonstrate quantitative results on datasets relating to 1) small molecule property prediction (QM datasets), 2) protein-ligand binding (PDBbind dataset), and 3) a dataset from material science on metalorganic compounds (CoRE-MOF dataset) \n\n",
            "main_review": "The question of how to efficiently represent and learn from 3D atomic structure has attracted a lot of interest in recent years due to a wide range of promising applications. Different representations might be adequate for small molecules (with the defining characteristic of the chemical formula) compared to structures of larger molecules (such as proteins). \n\nMy main concern regarding this paper are statements referring to prior work that I think are incorrect. \nAn example of this is the first bullet point listed unter contributions \"To the best of our knowledge, 3D-Transformer is the first Transformer variant that incorporates 3D spatial information for molecular representation learning.\". The authors of Fuchs et al (2020) and the recent Alpha Fold paper will likely disagree. There is also a body of work of hierarchical learning from 3D molecular structures that is not referenced in relation to the introduced subsampling operation (see for example Eismann et al, Proteins, 2020). \n\nThe methodological novelty in terms of molecule representation is limited, the distance map representation for small molecules and the sinusoidal function-based absolute position encoding method (Li et al., 2019) for larger molecules have previously been established. \n\nThe adaptive representation is in general an interesting idea. However, it would be great if the authors could clarify how they encoded the protein-ligand complexes in the PDBbind dataset. In section 5, the authors state that their proposed metric Phi is \"a good indicator of small or large molecules\". The protein-ligand complexes contain both a large molecule (or parts of it in terms of the binding pocket) and a small molecule (the ligand). \n\nThe quantitative results for the three datasets indicate that the method is performing well.\n\nIn the future, I would not include a Github link as part of the manuscript but provide the code as part of the submission. \n\n",
            "summary_of_the_review": "My recommendation is based on the 1) methods' representation with respect to prior work and 2) limited technical novelty. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes 3D-Transformers, which is a variant of Transformer that incorporates the 3D spatial information.\nThe authors develop a multi-scale attention module, an adaptive position encoding module, and a farthest sampling scheme for the 3D-Transformer.\nThe authors validate the performance of 3D-Transformer on three different domains.",
            "main_review": "My major concern regarding this paper is the contribution and the experiments.\nThe paper proposes a \"multi-scale attention\" module and a \"farthest point sampling\" module. These are all built upon the design of PointNet++(Qi et al., 2017b) without significant modifications. Another contribution, namely ATPE, is just thresholding based on existing techniques. Most importantly, the effectiveness of these designs is now properly validated in the experiments (quantitatively). Without these comparisons that clearly show the difference for each module, the proposed \"3D-Transformer\" is very similar to an attention-based PointNet++ that is specially designed for molecular data.\n\nAlso, I don't like the title 3D-Transformer. 3D deep learning is a very large research field. Because the paper only focuses on molecular representation, the current title would cause confusion to readers in the actual 3D computer vision area, such as point cloud and mesh. I suggest the author reconsider the title that could better describe the proposed method in the corresponding research domain to avoid overblown.\n\nOther minor concerns are:\nIn table 5, why there are two 3DGNNs? what is their difference?\nWhy the performance of the 3D-Transformer is close against MPNN on QM9, but shows a very huge gap on QM7 and QM8?\nThere are many typos and grammar mistakes in the paper. I recommend the authors carefully polish the writing.\n\n\n",
            "summary_of_the_review": "Overall, there are several components proposed in this paper. But they are not properly evaluated, which makes it hard to verify the significance of these contributions.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In the paper, the authors proposed a new Transformer dedicated to working with molecular representation. It is a modification of classical Transformer using the information of 3D coordinates of atoms. 3D-Transformer operates on a fully-connected graph with direct connections between atoms.",
            "main_review": "In the paper, the authors proposed 3D-Transformer. It is a modification of classical Transformer with using the information of 3D coordinates of atoms. 3D-Transformer operates on a fully-connected graph with direct connections between atoms.\n\nThe paper is hard to read since the authors concentrate on the abstract, introduction, and listing contribution. Some important information was moved to the Appendix (like intuition behind the measure of molecular structure complexity in the 3D space). Unfortunately, it makes it not consistent. \n\nThe main contribution is problematics since in literature, existing papers uses 3D coordinates in transformer architecture dedicated to  molecular representation:\n[1] Molecule Attention Transformer Ł. Mazurka at all.\n[2] Relative Molecule Self-Attention Transformer Ł. Mazurka at all.\nIn my opinion, authors should add to related work comparison to [1,2] and numerical comparison to [1]. \nThe methods seem to be different, but the main contribution is problematic. \n\nIt is not trivial to understand Fig. 1. Therefore, the caption should better describe the image. \n\n3D-transformer is misleading name since it suggests that the authors use 3D point cloud representation. In the title, the authors should add some information that the graph was used. \n\nIf the main contribution is the application of 3D coordinates, the author should compare such a solution with the naive method, which adds such coordinates to the label of the graph.\n \nAt the QM7 dataset, 3D-transformer outperforms other algorithms. Do you have some idea why it happens, what caused such difference? \n\nCompetition between baselines is applied only on regression tasks. Therefore, I think that authors should use classification tasks from [1,2] or from (cited in the article) “Self-Supervised Graph Transformer on Large-Scale Molecular Data,” where authors use BBBP, SIDER, ClinTox, BACE, Tox21, ToxCast datasets.",
            "summary_of_the_review": "The paper proposes an interesting modification of transformer architecture dedicated to the chemical compound. Unfortunately, the method is not well situated in the existing literature. In my opinion, the Abstract, Introduction, and list of contributions are too extended. On the other hand, some critical pieces of information are moved to the appendix. Furthermore, experiments are applied only on classification tasks. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "1. How important is the information from 3D coordinates? Maybe you can compare such a solution with the naive method, which adds such coordinates to the label of the graph.\n2. The competition on classification tasks should be added. \n3. The Abstract, introduction, and list of contributions shud be changed.  I recommend leaving the three most important contributions.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}