{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes a more generalized form of certified robustness and attempts to provide new results on applying randomized smoothing to semantic transformations such as different types of blurs or distortions. The main idea is to use an image-to-image neural network to approximate semantic transformations, and then certify robustness based on bounds on that neural network. The authors provide empirical results on standard benchmark datasets like MNIST and CIFAR showing that their method can achieve improved results on some transformations compared to prior work.\n\nThe review committee appreciates the authors taking the time to attempt to respond to the concerns of all reviewers, and for updating and improving their work during the rebuttal process. The committee is glad to see that they do provide empirical evidence of improvement to common-corruption robustness, compared to AugMix (one of the state-of-the-art approaches for standard common-corruption robustness) and TSS.\n\n\nHowever, the reviewers still have concerns about the novelty of the paper. The main novelty is not improvement for resolvable transformations (prior works that the authors cite perform about the same or better), but rather, is the ability to handle non-resolvable transformations. The reviewers agree that robustness to non-resolvable transformations is important; however, the reviewers think certified robustness to non-resolvable transformations is not meaningful, because they are only being certified with respect to a neural network that is trained to approximate those non-resolvable transformations. Without MTurk studies to confirm how good the neural network's non-resolvable transforms are, the reviewers do not find certified robustness here meaningful."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a more generalized form of certified robustness and attempts to provide new results on applying randomized smoothing to semantic transformations such as different types of blurs or distortions. The main idea is to use an image-to-image neural network to approximate semantic transformations, and then certify robustness based on bounds on that neural network. The authors provide empirical results on standard datasets like MNIST and CIFAR showing that their method can achieve improved results on some transformations compared to prior work.",
            "main_review": "Overall, I would recommend a reject for this paper.\n\nThe paper discusses an important topic (robustness to semantic transformations), but does not tackle the problem from a perspective that I feel is significant. In particular, it is unclear to me how useful certified robustness to semantic transformations will be, especially for non-resolvable transformations. What does it mean to be robust to rotational blur for attack range ||alpha||_2 < 20 for CIFAR-10, for example? Does this correspond to any guarantees on real datasets, or at least empirical improvements on benchmark datasets?\n\nI appreciate that the authors are thorough with comparing to various prior works in their experiments. However, their method usually performs very similarly or slightly worse than prior works when comparisons are possible; thus, the main experimental improvement is that it is now possible to achieve results on certain non-resolvable transformations that prior works are incapable of handling.\n\nStill, in my view, a key question is unanswered - how can we be sure that the generative model is capturing all the possible non-resolvable transformations that we may care about? Because if we can not answer this question, then it seems that certified robustness is not particularly meaningful, and instead, empirical robustness on benchmark datasets (for example, measuring accuracy on CIFAR-C as well) would be a better measure of success.\n\nAdditionally, the paper could be presented better; in particular, the important components should be explained more clearly (e.g. the question I asked above about what ||alpha||_2 < 20 means), whereas other parts of the paper can be moved to the Appendix. For example, Theorems 1 and 2 and Corollary 1 are primarily based on prior work and use up a lot of text, but there is not much description or explanation of how these theorems can be applied to get certified robustness to semantic transformations. The best example of a useful explanation that I found is the short paragraph just under Figure 1 on page 4, which was helpful for me.\n\nAdditionally, I believe the authors are missing very relevant prior work. The work of [1] discusses a similar idea, where semantic transformations are modeled via generative models, and Lp-robustness in the latent space of the generative model is used as a proxy for robustness to semantic transformation.\n\nSome more specific comments and questions:\n- Can the notation section be simplified any further?\n- How large is the error term epsilon in Theorem 3 look for the generative models used in the experiments?\n\n[1] https://arxiv.org/abs/2005.10247 Model-Based Robust Deep Learning: Generalizing to Natural, Out-of-Distribution Data (Alexander Robey, Hamed Hassani, George J. Pappas)\n\n\n\n\n=======================================================================\n\nAfter the rebuttal period, I have read the author's responses, and changed my score from a 3 to a 5. I appreciate the authors taking the time to attempt to respond to the concerns of all reviewers, and for updating and improving their work during the rebuttal process.\n\nI am glad to see that they do provide empirical evidence of improvement to common-corruption robustness, compared to AugMix (one of the state-of-the-art approaches for standard common-corruption robustness) and TSS, although I can not tell how the authors derived their baselines (I can not find references to the AugMix accuracy numbers that the authors provided in their rebuttal in the TSS or AugMix paper).\n\nStill, in my opinion the paper's novelty is limited. As the authors and I agreed upon in our discussion, the main novelty is not improvement for resolvable transformations (prior works that the authors cite perform about the same or better), but rather, is the ability to handle non-resolvable transformations. I agree that robustness to non-resolvable transformations is important; however, certified robustness to non-resolvable transformations is not meaningful to me, because they are only being certified with respect to a neural network that is trained to approximate those non-resolvable transformations. Without MTurk studies to confirm how good the neural network's non-resolvable transforms are, I do not find certified robustness here meaningful, because it does not necessarily correspond to anything concrete that we can understand. On the other hand, empirical improvements on non-resolvable transformations would be meaningful.\n\nThus, the main reason I increased my score to weak reject is due to slight empirical improvements over baselines on CIFAR-10C.",
            "summary_of_the_review": "The paper leaves some key questions unanswered, and can be presented much more clearly. It is also missing very relevant prior work. Thus, I recommend a reject.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose a randomized smoothing based certification algorithm  for general semantic transformations. The key idea of the work is to use a neural surrogate for the semantic transformations, and add noise in the latent space of the surrogate for randomized smoothing. The neural surrogate appears to convert the non-linear, multiplicative transformation into an additive operation in the latent space, thus allowing for randomized smoothing methods to be applied. Their approach proposes to decompose the resolvable and unresolvable parts of the semantic transformation by lifting the data+transformation parameters into a larger augmented latent space defined by an image-to-image network. The resolvable parts of the transform can then be certified similar to previous works (Yang'2021, Salman'2019) using the Lipschitzness of the smoothed transforms. The non-resolvable part of the transform however are assumed to be Lipschitz in the latent space. The authors provide theoretical and empirical evidence for GSmooth and show improvement on contemporary methods.",
            "main_review": "Strengths:\n1. The topic of certifying generalized semantic transformations is extremely relevant. The paper is well motivated and the authors clearly point out differences with existing work. \n2. GSmooth is able to certify several non-trivial semantic transformations which include difficult combinations such as rotational and defocus blur. \n3. While the certified accuracy values are not that significant for the more complex transforms, the algorithm is good first attempt which succeeds as compared to other methods.\n\nWeaknesses:\n1. I am not convinced that an $\\ell_2$ bound in the parameter space for compound transformations makes sense. A real world attacker instead would have $\\ell_\\infty$ or $\\ell_2$ type bounds for parameters of each transform. For example in the case of rotational blur, an attacker could independently increase rotations and blur to the max value. Given the very different ranges of the two tranformations, an $\\ell_2$ bound may emphasize one transform over the other. The current algorithm does not seem to take this into consideration.\n2. The idea of augmenting the latent space to decouple resolvable and non-resolvable components is interesting. However, from the text, it is not clear why such a decoupling would occur without additional training objectives for the surrogate. Could the authors clarify this?\n3. It's not clear to me why H would not reflect in the M* term. The proof seems to ignore the effect of H by considering it to be a concatenation of an identity and a zero matrix. However, the decoder itself may not be Lipschitz due to the non-trivial nonlinearities involved. A more clear explanation here is warranted.\n4. The paper lacks a empirical confirmation of the presented bound. Specifically, the GSmooth classifier should be tested against an adaptive attack that has access to the surrogate and the parameter space to find if the bounds do actually hold. The certificate should also be tested against standard Expectation over Transformation attacks. This would also help quantify the tightness of the proposed bound. \n\nMinor comments:\n1. The explanation regarding the dimensionality augementation is confusing and can be presented better-- \"Specifically, we introduce the augmented data $\\tilde{x}$ ... additional dimensions to 0.''. Also, Eq. 9 should include the expression for augmented $\\tilde{H}$ for better clarity. \n2. Table 2 does not mention the radius for the certified accuracies.\n3. Section 3 needs to be more clear. The notation for the augmented parameter space needs some clarity on the dimensions used. \n4. While the authors present a nice simplification for F_1, how about explicitly forcing lipschitzness for F_1, F_2, and H? ",
            "summary_of_the_review": "Overall, the paper presents a novel and interesting addition to the randomized smoothing literature. This appears to be the first attempt at certifying complex semantic transformations, and therefore has merit. The paper still needs some additional work however, specifically in terms of quantifying the tightness of their proposed bound against semantic attacks. I also believe the authors should take a more careful look at the actual certificates for the compound attacks. An $ell_2$ bound for the entire parameter vector may not capture the worst case for each component of the transform. \n\nThe paper in its current form is marginally below the acceptance threshold. However, if the authors show empirical evidence of their certificates against semantic attacks, I will be glad to improve my score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper generalizes randomized smoothing to certify robustness against complex semantic transformations. To do that, they construct a surrogate neural network mapping images to images, to approximate complex semantic transformations and certify robustness with respect to this network. ",
            "main_review": "In this work, the authors first construct a smoothed classifier $G(x)$, based on smoothing the parameter of the parametrised transformation $\\tau$. Based on the compositional properties of $\\tau$, they define conditions to obtain a robustness radius (Theorem 1). Then they proceed to construct a surrogate image to image translation model approximating the transformation $\\tau$. Then the parameters of $\\tau$ and inputs $x$ get augmented to $\\tilde{\\tau}$ and $\\tilde{x}$ respectively to divide the transformation into a resolvable and a non resolvable part. The surrogate transformation then leads to a more concrete form of $M^*$ (Theorem 2), which is further simplified by assuming that $F_1$ is an affine form (Corollary 1). Finally, in Theorem 3, the authors provide a correction for the robustness radius $R_r$ for the case where surrogate transformation $\\tilde{\\tau}$ is approximating $\\bar{\\tau}$ dependent on the maximum $\\ell_2$ approximation error. \n\nThe main ideas of the paper are interesting and to the best of my knowledge novel, particularly the surrogate network. However, this paper needs to be written much more clearly to convey the central ideas to the reader. Particularly Section 3 and Section 4 are hard to understand and required considerable effort, after which some aspects remain still unclear. On page 5: What are the dimensions for $x’$ and $\\theta’$ or $\\tilde{x}, \\tilde{\\theta}$ and how is $x’$ chosen? How do you decompose the transformation to get a resolvable and a non resolvable part? Do i guess correctly from Eq. 9, that the resolvable part is the sum between $\\theta$ and $x’$, and the non resolvable part is the sum between $F_1(\\theta) + \\theta’$ and $F_2(x)$? \n\nThe related work Li et al 2021 and Fischer et al 2020 do evaluate on ImageNet. Can the same be done for GSmooth? Table 1 indicates that VeriVis is not able to certify against translations. However, VeriVis can certify translations. Was the certification accuracy 0? What about the other works? Can the other methods potentially be adapted to handle Blur or the Pixelate transformation? Further, the runtimes need to be stated in order to judge the trade off between performance and accuracy. Where do the authors show how they calculate their $\\epsilon$ for the surrogate transformation networks as needed for Theorem 3?\n\nThe authors claim multiple times (abstract and later in the paper) that existing work can not handle complex semantic transformations. Can the authors substantiate this impossibility? I could not find a proof or some further justification for this. Further, in the related work, section 2.1: Apart from Madry et al. 2017, these works do not attempt to certify some simple geometric transformations but actually do certify them. Mardy et al 2017 on the other hand focuses on attacks and defenses. The authors clarify the difficulties the approach of Li et al 2021 presents for generic transformations. Can the authors also clarify the difficulties the approach of Fischer et al. 2020 presents? In Section 3.3. the authors imply that existing methods based on convex relaxations and randomized smoothing require the development of a specific algorithm for each individual transformation. To what extend is this true for Balunovic et al 2019 and Fischer et al 2020? \n\nMinor comments:\n- Eq. 1: It is slightly confusing to write $\\theta \\sim g(\\theta)$. \n- Just before Eq. 8: the dimensions for $\\tilde{\\tau}$ seem off. ",
            "summary_of_the_review": "While the ideas in this paper are to the best of my knowledge interesting and novel, many questions remain unanswered and key points in the paper remain unclear. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed GSmooth, a generalized randomized smoothing method for semantic transformations. \n\nThe main technical contributions are:\n(1) Introduce the use of an image-to-image translation network to provide a unified framework for the analysis of non-resolvable semantic transformations.\n\n(2) Theoretical proof on the certified radius and the approximation error from the image-to-image translation network. \n\n(3) The empirical performance is superior to existing methods on most transformations. More importantly, the method can certify many new transformations that are hard to analyze based on existing methods.\n",
            "main_review": "In my opinion, this work shows ground-breaking results in certified robustness, by providing an elegant and general framework for robustness certification with arbitrary and possibly complex semantic transformations with performance guarantees. I believe it achieves an important milestone in adversarial robustness.\n\nThe paper is also well-written and easy to follow. The comparisons to existing methods are thorough and convincing. The ablation studies (especially the noise distribution part that uniform distribution is quite competitive) also shed some new insights into randomized smoothing.\n\nI have two suggestions that I hope the authors can incorporate into the revised version:\n\n1. In Table 1, since the considered transformation has very few parameters, I would like to see the comparison of empirical robustness by running the actual attack on the certified value to understand the gap between certified and empirical robustness.\n\n2. Since the proposed method can handle composite semantic transformations, can the authors some case studies to demonstrate such an advantage?\n\nMinor comment\"\n1. In Sec. 3.1, \\nabla should be defined, and I believe \\psi function needs to be differentiable\n",
            "summary_of_the_review": "1. Strong theoretical and empirical results\n2. The contributions are significant for expanding randomized smoothing to more complex semantic transformations",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}