{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes a modification of the Dreamer method to account for safety constraints.\nThere is agreement among the majority of reviewers that the paper lacks in novelty (with respect to Dreamer), in the safety analysis, and in convincing experiments.\nI encourage the authors to take the detailed reviews into account when improving their work."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a safe Reinforcement Learning approach using Latent Shielding. The agent learns a  data-driven recurrent state-space model representing a world model based on an existing approach (Hafner et al., 2019b). The learned latent world model allows agents to foresee various trajectories arising from different actions and avoid unsafe states. In their setting, the environment also provides binary labeling over states indicating an occurrence of violation in each step, formulated as syntactically co-safe Linear Temporal Logic (scLTL) (Kupferman & Vardi, 2001). The proposed approach is evaluated on two environments (1) Visual Grid World and (2) Cliff driver and compared against Dreamer agent with no shielding (Hafner et al., 2019a; 2021) and a Dreamer agent with Bounded Prescience Shield (Giacobbe et al., 2021).\n",
            "main_review": "The main contribution is minor. The proposed approach is mostly is a straightforward combination of existing methods. The experiments are limited in complexity and thoroughness. The baseline methods such as Dreamer (Hafner 2021) have been evaluated over a wide range of test scenarios. Therefore two environments for evaluation are not sufficient. Furthermore, for fair comparison methods from Safe Reinforcement Learning should also be included. ",
            "summary_of_the_review": "The paper contribution is minor. Their experiments are limited in terms of test environments and benchmark methods.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a safe model-based reinforcement learning algorithm that trains a classifier to predict whether or not a state is unsafe, and plans trajectories that avoid unsafe states. The key idea is to train the classifier on latent representations of states given by the latent dynamics model in a recurrent state-space model. Experiments in simulated navigation tasks show that in some environments, the proposed method achieves higher reward and violates safety constraints less often than an ablated variant that does not constrain planned trajectories to be safe.",
            "main_review": "Overall, this was an interesting paper, but I have several concerns regarding the experiments.\n - One issue is that the difference in performance between “Latent” and “Unshielded” does not seem significant (i.e., the error bars are substantially overlapping in Figure 3 and in several rows of Table 1). \n - It’s difficult to interpret the numerical values of the test rewards. It would be helpful to report more interpretable performance metrics, such as the fraction of targets reached in the VGW environment, and the time-to-edge-of-cliff in the CD environment.\n - “Unshielded” does not seem like a fair baseline, since it does not make use of the labelling function (while the “Latent” method has access to the labelling function at training time). A stronger baseline might be [Trial Without Error](https://arxiv.org/abs/1707.05173), which also trains a classifier to predict whether or not states are unsafe using labels from an oracle (in this case, a human supervisor), and trains an RL agent to avoid unsafe states.\n\nI’m also concerned about the novelty of the method: it is essentially Dreamer, but with a safety constraint on the planned trajectories. I may have missed the algorithmic contribution of this paper, and am happy to revise this comment during the discussion period.\n\nUpdate\n----\nThank you to the authors for the thoughtful response. I have increased my score.",
            "summary_of_the_review": "Weak experimental results and lack of comparisons to prior methods",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper introduces shielding technique for RL safety into world model RL agents, like the Dreamer model.\nThe shielding technique works on the latent space and is trained from violation information only, i.e. without the hand-crafted design of safety rules.\nAn experimental evaluation is performed on two environments and compared against the unshielded version and BPS from the literature.",
            "main_review": "The paper is well-written and structured and accessible to the reader.\n\nThe new latent shielding has one major drawback and weakness, which is of course mention and acknowledged in the paper, but still a concern to some degree: The guarantees that other shielding techniques have are lost as the method only works on the latent model.\nThis is advantage and disadvantage at the same time. The advantage is the non-requirement of hand-crafted rules. However, violation information is required during training which also needs to be modeled from somewhere, so the experts are still needed, although it might be easier to identify a violation than to mitigate it from rules. The disadvantage is that the model cannot be deployed alone for RL safety, but at least for other applications or as the main safety technique with an additional external shield as fallback.\n\nI'm a bit disappointed by the alternative action in case of a violation. Here I would expect that the world model would allow to perform better planning than just selecting any non-violating action.\n\nThe experimental evaluation is okay. A more complicated environment could have been chosen to really highlight the advantage of not needing hand-crafted rules could have been chosen, e.g. car racing or also one from Alshiekh 2018 for comparison.\n\nThe method itself is sound, although it is based on many existing contributions and it is a judgement call whether the contributions over the existing techniques are sufficiently high, but I think they are and it is an interesting contribution and basis for future work.",
            "summary_of_the_review": "The paper explores an interesting direction for RL safety and shielding and even though it cannot provide performance guarantees, it is helpful to the agent's performance and reduces some violations.\nThe contribution will be helpful to the community.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a framework for safe model-based RL through latent shielding. The key contribution is extension of an existing MBRL algorithm Dreamer with shielding such that the agent is penalized for taking unsafe actions during exploration; and during planning, the agent can sample multiple plausible futures through the learned dynamics model and avoid taking actions that lead to unsafe states.",
            "main_review": "The proposed approach is simple, and intuitive, which is a major advantage. Fig. 1 clearly describes the contribution of the paper on top of Dreamer, by incorporating a safety constraint violation predictor. The violation predictor is learned based on an approximate shielding approach inspired by classical notions of a shield. The shield checks the probability of constraint violation given the policy outputs a certain action, and if this is the case, then an alternate policy based on rejection sampling is activated. The paper is well-written and easy to follow, with detailed algorithms mentioning the flow of the approach. \n\n\nI have a few questions and several concerns regarding the paper which I have listed below:\n\n- An important limitation of the approach is that since visiting unsafe states are discouraged during exploration (by penalizing the agent with -negative rewards), the agent is unlikely to visit unsafe states, which is good for being safety but bad for learning an accurate dynamics model in those regions. Hence, dynamics model rollouts are likely to be very inaccurate around unsafe parts of the state-space, thereby making the planning process inaccurate. \n\n- Another limitation of the approach is that it necessarily requires visiting a lot of unsafe states during training, for learning the world model, and hence can never be completely safe. This is also reflected in the results in Table 1 where the training violations are significantly high. It would be helpful to have a discussion about acknowledgement of this limitation in the paper. \n\n- There are no guarantees of how accurate the safety shield is given errors in the learned dynamics model. It will be helpful to formally quantify the safety constraint satisfaction as some function of model-error, or provide intuitions about the extent to which inaccuracies in the model are likely to affect safety violations. \n\n- Since $\\pi_{\\text{alt}}$ in equation 4 outputs a \"safe\" action by rejection sampling, there is no guarantee that a safe action if it exists will be found. This is because the learned model is inaccurate, so at any given time, when a safe action is not found, we can't be sure whether this is because a safety violation is inevitable, or because there exists a safe action which is not feasible under the current (inaccurate) dynamics. \n\n-  There are no comparisons to model-free safe RL approaches (for example the constrained optimization and safety critics papers cited in the 2nd paragraph of the introduction). In comparison to prior safe RL works that learn a safe policy without learning a safe dynamics+rewards model i.e. in comparison to safe model-free approaches, the proposed approach will likely suffer from a compounding of errors. This is likely to happen because the proposed method has a large number of learned components (parts of the model) and relies on the model being accurate for planning. Hence, a comparison with some prior model-free safe RL methods is important. \n\n- A very important limitation is the lack of sufficient experiments. Experiments are only on toy settings. It is necessary to evaluate safe behavior on more complex dynamical systems (for example robot manipulation, locomotion, navigation environments). Many of the prior safe RL papers cited evaluate on such environments. \n\n- [clarification] Is the unshielded variant (in blue in the plots), normal Dreamer? If so, then how does the proposed method compare with Dreamer in the normal DeepMind Control Suite environments? It is necessary to see this in order to understand whether the proposed method achieves safe behavior while compromising on task performance in environments where safety is not explicitly important. \n\n- [minor] I am curious if the title has any relevance to the contents of the paper? I understand that it is a spin-off of \"Do Androids Dream of Electric Sheep\" and the connection to the paper in my understanding is: Dream --> Dreamer; and Fences --> Shield. If this is the case / there is more to it, then a one line explanation of it somewhere in the paper / in the appendix might be helpful.",
            "summary_of_the_review": "This paper modifies a prior model-based RL framework, Dreamer to incorporate a safety violation predictor based on latent shielding. While this idea is simple, easy to implement, there are several issues with the paper with regard to severe safety violations during training, compounding errors due to model error and policy error, and lack of experiments + comparisons. As such, I don't think the paper is ready for publication, and does not provide concrete reliable takeaways. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}