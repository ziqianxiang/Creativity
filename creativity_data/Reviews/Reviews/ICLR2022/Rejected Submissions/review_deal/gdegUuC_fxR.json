{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper considers the high resolution continuous limit of Nesterov's Accelerated Gradient (NAG) algorithm and its connections to sampling (MCMC methods). The paper develops a Hessian-Free High Resolution (HFHR) ODE and injects noise into it to obtain an accelerated sampling algorithm. Further, the paper provides a discrete-time variant of the algorithm by appropriately discretizing HFHR using simple discretization schemes. For strongly log-concave potential functions (log-densities), the paper proves convergence of the order $\\tilde{O}(\\sqrt{d}/\\epsilon)$ in Wasserstein-2 distance. In the asymptotic sense, the result matches the convergence of the underdamped Langevin algorithm; however, the paper argues that the constants in the proposed algorithm are smaller and empirically shows that the proposed algorithm is faster in practice. The main contributions of the paper are theoretical; however, the theoretical results are supplemented by numerical experiments.\n\nOverall, the reviewers found the contributions interesting and the theoretical contributions of the paper technically sound. The main concerns that were not completely addressed were related to the presentation of the results and reproducibility of some of the numerical experiments. While both seem minor and possible to address, ultimately there was not enough support to recommend acceptance. However, the paper is solid and merits acceptance after suitable revisions. Thus, the authors are encouraged to revise the paper and resubmit it to one of the conferences in the equivalence class of ICLR."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes an accelerated MCMC method for sampling, motivated by Nesterov's Accelerated Gradient (NAG) method. Starting from the high resolution ODE of NAG obtained in Shi et al, the paper applies a two stage mechanism to remove the Hessian-dependency. The obtained first order ODE system serves as the backbone of the proposed diffusion process, Hessian-Free High-Resolution (HFHR) dynamics. Discretization the HFHR dynamics leads to the proposed sampling method. Theoretical convergence are provided for both the continuous and the discretized variant, showing acceleration to existing underdamped Langevin method (ULD). ",
            "main_review": "The paper is very technical and not easy to follow. The main contribution of the paper is to provide an accelerated first order diffusion process in continuous time, yielding a novel sampling method after discretization. I have a few questions/concerns to be clarified regarding the claims in the paper:\n\n1 Acceleration in continuous time \nComparing to the rate obtained in Dalalyan 2020, the proposed algorithm improves by a factor of $\\kappa$. However, I am confused as no improvement is achieved for the function $f(x,y) = mx^2+Ly^2$, this seems contradictory to the claim, do I misunderstand anything? Is it related to the time scaling? \n\n2 Discretization \nWhen discretizing, the paper applies a second-order symmetric composition. What is the motivation behind this step? What would be the benefit compared to applying forward Euler discretization? \n\n3 Acceleration in discrete time \nUnless we show that the constant term is improved by a non-trivial factor (L, or $\\kappa$ for example), I wouldn't call it an acceleration as those are only upper bounds in the analysis. \n\n4 ULD = HFHR(0,$\\gamma$)\nI understand that the continuous ODE are equivalent by taking $\\alpha = 0$, is the discretized algorithm still follows the same equivalence? (for instance, different discretization method may be applied)\n\n5 Comments on Experiments \na. In figure 2, it seems surprising to me that only 2 iterations is enough to reach $\\epsilon$ closeness, which may suggest the problem is too easy\nb. In Figure 3, why does some figure only has $\\alpha=0.1$ and the others has $\\alpha = 0.1, 0.5, 1$\n",
            "summary_of_the_review": "Overall, I believe the theoretical result of the paper has merit but the presentation need to be improved ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces Hessian-Free High-Resolution SDE inspired by Accelerated Gradient (NAG). The author shows that continuous solution achieves an acceleration over the underdamped Langevin. A discrete algorithm also has speed-up with a constant factor.",
            "main_review": "This paper has solid theoretical analysis. \nHowever the author missed an important comparison with randomized midpoint method (Shen & Lee, 2019). \nI think the author proved an iteration complexity that is worse to the method in that paper. This invalidates author's claim on constant factor speed-up over underdamped Langevin dynamics.\n\nSpecific Comments:\n1) In Assumption A1, $m \\lVert y-x \\rVert \\leq \\lVert \\nabla f(y)-\\nabla f(x) \\rVert \\leq L \\lVert y-x \\rVert$ seems insufficient to establish m-stronly-convex and L-smooth.\n\n2) Does this paper consider \"log-concave\" case? In the introduction, author said \" will consider the setup of log-concave / log-strongly- concave target distributions\". However, it seems Assumption A1 require $m > 0$. \n\n3) On page 9, author wrote \"(strongly) log-concave assumptions required in Theorem 5.2\", why there is a bracket? Does it imply that Theorem 5.2 could be applied in log-concave case?\n\n4) In Remark 5.5, at the last line, the author claimed \"steps needed by ULD can be halved by HFHR\". Which algorithm is the author referring to for \"ULD\". Is it algorithm in (Cheng et al. 2018) or HFHR with $\\alpha=0$? It seems these two algorithms are actually different.\n\n5) In section 6.1, author proposed to use error of mean as a surrogate of 2-Wasserstein distance. Although error of mean is a lower bound, how tight it is? In figure 2, the mean value at $\\alpha=0.5$ in the Figure 2 seems close to 1, which means HFHR achieves $\\varepsilon\\leq0.1$ accuracy in just one iteration. Clearly one iteration is not enough for burn-in. Does it means error of mean is a bad indicator of mixing?\n\n6) In figure 1, could the author also add lines for ULD and randomized midpoint method?",
            "summary_of_the_review": "I thinks this paper could be improved substantially, but currently would like to recommend reject. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This stuyd focused on developing a Hessian-free high-resolution Nesterov’s Acceleration approach which proves converging faster and being  non-trivial (not a time-rescaling trick).",
            "main_review": "This paper introduces a Hessian-free high-resolution Nesterov’s Acceleration approach which proves faster convergence than both Underdamped and Overdamped Langevin Dynamics. In addition, it also proves that the acceleration cannot be achieved by time-rescaling. Experiments results support the argument in optimization acceleration as well as the theoretical results. \n\nSome questions the reviewer minds:\n\n-The theorem 5.1 depends on the assumption on \\gamma and \\alpha. If such an assumption often holds in real applications.\n\n-It is will be better to demonstrate the acceleration superiority in with a more complex network and a large-scale dataset.",
            "summary_of_the_review": "The reviewer thinks this paper contributes a new and effective gradient based optimization solver. Overall, the reviewer would like to recommend acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The author proposed an accelerated-gradient-based MCMC method based on Nesterov's accelerated gradient (NAG). In continuous time, the algorithm is able to achieve a tremendous acceleration over the underdamped Langevin algorithm. Numerical schemes can propose a speed-up with a constant factor.\n",
            "main_review": "This paper builds a NAG-based MCMC sampler, which is named as Hessian-Free High-Resolution (HFHR($\\alpha, \\gamma$)) and generalizes the underdamped Langevin (also can be written as HFHR($0, \\gamma$)) with an additional gradient drift and Brownian motion for the update of the position variable. \n\n**Pros**: The most prominent feature of the extended algorithm yields much faster exponential convergence with **rate $O(L)$**  in continuous time than the standard underdamped Langevin algorithm  **(Cheng'18, rate $O(\\frac{m}{L})$, Dalalyan'20, rate $O(\\frac{m}{\\sqrt{L}})$)**. Although the acceleration becomes less significant in numerical algorithms due to an increase of discretization error, some speed-up by a constant factor can be still achieved.\n\n**Cons**:  the writing is not very clear, which affects the readability experience and made me hard to check the details. For example 1) the derivation of Formula (6) is slightly ad-hoc and more related work or motivations are suggested; 2) at the end of section B, some link on the definitions of notations may be suggested; 3) page 24, $H_t$ is derived based on Taylor expansion instead of saying nothing is better for presenting to the readers.\n\nRemark: Theorem 5.1 is checked carefully; Theorem 5.2 is not; the logic seems to be reasonable.\n\nMinor issues: \n\n1. Experiments conducted on section 6.2 is non-convex and doesn't match the theory.\n\n2. Missing part in related works: Replica-exchange (a.k.a parallel tempering): \n\n[1]. Accelerating Nonconvex Learning via Replica Exchange Langevin Diffusion. ICLR'19\n\n[2]. Non-convex Learning via Replica Exchange Stochastic Gradient MCMC. ICML'20\n\n\nCheng'18: Underdamped Langevin MCMC: A non-asymptotic analysis. COLT'18.\n\nDalalyan'20: A. S. Dalalyan and L. Riou-Durand. On sampling from a log-concave density using kinetic Langevin diffusions. Bernoulli\n\n",
            "summary_of_the_review": "The NAG-based MCMC sampler extends the underdamped Langevin with a drift term and Brownian motion and is proved faster than the alternative. I would recommend this paper to be accepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethic concerns.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": " This paper discuss the accelerated-gradient-based MCMC method and propose some rigorous proof.",
            "main_review": "I do not think the problem addressed by this paper does not make sense.  The gradient-based algorithm added by the noise in the convex case does not play the essentially different role with that without the noise.  ",
            "summary_of_the_review": "Please the authors refer to \n\nBin Shi, Weijie Su and Michael I. Jordan,  On Learning Rates and Schrödinger Operators, https://arxiv.org/abs/2004.06977\n\nBin Shi, On the Hyperparameters in Stochastic Gradient Descent with Momentum, https://arxiv.org/abs/2108.03947\n\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
        },
        {
            "summary_of_the_paper": "This paper introduces a stochastic process called HFHR. The SDEs of HFHR is derived by rewriting Nesterov’s Accelerated Gradient (NAG) into phase-space representation, formulating it as ODEs, and then injecting noise into both position and momentum variables. HFHR can be used for sampling because it has stationary distribution as the target distribution. A discretization of HFHR is given by operator splitting and Euler integration. A mixing time bound of $\\widetilde{O}(\\sqrt{d}/\\varepsilon)$ is obtained for sampling log-strong-concave-and-smooth target distribution with extra third-order growth condition.",
            "main_review": "Strengths:\nThe HFHR process seems novel and has interesting connections with NAG. The paper is mostly clear and easy to follow.\n\nWeakness:\n1. HFHR has noise injected into position directly\n\nThis property is actually harmful for discretization because directly coupling Brownian motion into $q_t$ makes the trajectory of $q_t$ extremely non smooth and make it hard to estimate $\\nabla f(q_t)$.\n\nThe ULD avoid directly inject noise into position to achieve acceleration compared to the overdamped LD.\nHFHR, unlike from ULD, has noise term $dW_t$ in position. However it still achieve same asymptotic convergence as certain discretized ULD at the price of an extra condition on higher order derivative.\n\n2. Rely on additional third-order growth condition\n\nAlthough this condition is weaker than Hessian Lipschitz and has been used in prior art to accelerate the convergence of LD [Li et al., 2021], it still makes the convergence guarantee weaker than many discretized ULD, which have same or better asymptotic convergence speed and don't require third-order growth condition.\n\n3. The condition number $\\kappa$ dependence and third-order growth constant $G$ dependence in iteration complexity is unclear\n\nThe main paper doesn't specify the asymptotic dependence of $\\kappa$ and $G$ but hide it in the constant $C$. Including that information in the main paper would be appreciated.\n\nAccording the appendix page 20, where C is defined, I guess that the $\\kappa$ dependence is $O(\\kappa^{3/2})$ and $G$ dependence is $O(G)$. Could the author help to verify the above statements?\n\n4. Iteration complexity is worse than certain discretization of ULD\n\nAlthough the authors cited [Shen & Lee, 2019], they didn't discuss methods in this paper at all.\n\nThe randomized midpoint method is a special discretization of ULD, which achieves $\\widetilde{O}(d^{1/3}/\\varepsilon^{2/3})$ iteration complexity and doesn't require third-order growth condition.\n\n5. The method is not well motivated\n\nAccording to the paper, HFHR is motivated by one question \"how to appropriately inject noise to NAG algorithm in discrete time\". We should note that, although there are underlying connections between optimization and sampling, injecting noise into a good optimization method doesn't necessarily yield a good sampling algorithm.\nUnfortunately, this paper seems like this is the case. More specifically, discrete HFHR with $\\alpha \\neq 0$ have worse iteration complexity compared to certain discretization of ULD.\n\n6. Points in Figure 2 is biased estimation\n\nIf I understand correctly, the y-axis value in Figure 2 is smallest number $k$ such that $\\lVert \\mathbb{E} {\\mu_k} q-\\mathbb{E} \\mu q \\rVert \\leq \\varepsilon$.\nIn order to estimate $\\mathbb{E} {\\mu_k} q$, $\\frac{1}{n} \\sum_i^{n} q_k^{(i)}$ is calculated for $n=1000$ samples (according to line 3 in generate_config.py) which are generated and run in parallel for $k$ iterations.\n\n$\\frac{1}{n} \\sum_i^{n} q_k^{(i)}$ is indeed unbiased estimation of $\\mathbb{E} {\\mu_k} q$. However \n$\\min k$ $s.t. \\lVert \\frac{1}{n} \\sum_i^{n} q_k^{(i)}-\\mathbb{E} \\mu q \\rVert \\leq \\varepsilon $\n is a biased estimation of \n$\\min k$ $s.t. \\lVert \\mathbb{E} {\\mu_k} q-\\mathbb{E} \\mu q \\rVert \\leq \\varepsilon$.\n\nTypo:\n\ndt is missing in eq 6.\nBracket error on 3rd-to-last line on page 8. ",
            "summary_of_the_review": "The method is novel in the sense that it is derived based on the idea \"inject noise into Nesterov’s Accelerated Gradient\".\nHowever the derived theory is not significant as it rely on extra assumptions and has worse iteration complexity.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}