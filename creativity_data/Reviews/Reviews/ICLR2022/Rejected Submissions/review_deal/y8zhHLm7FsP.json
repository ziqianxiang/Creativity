{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper presents the use of the Ensemble Kalman Filter (EnKF) to solve the linear quadratic Gaussian (LQG) optimal control problem. After reviewing the paper and taking into consideration of the reviewing process, here are my comments:\n- The related work is limited and needs more improvements to contextualize the problem and the solution.\n- The reinforcement learning paradigm is not really appreciated in the proposal.\n- The results are rather limited, so more experiments are needed to clearly validate the solution.\nFrom the above, the paper does not fulfill the standards of the ICLR. I suggest improving the paper accordingly and submitting it to a control systems venue."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a method of calculation for filtering and control in continuous-time linear systems based on using a simulator to generate many trajectories and use their statistical properties to empirically calculate the needed estimation and actuation signals.",
            "main_review": "The current version of the manuscript is not ready for publication. \n\n- The problem understudy is not well motivated: \nThe proposed method is not capable of providing a performance better than the classical approach of Riccati equations, and it makes sense computationally, only if the dimension is large. However, in that case the statistical properties enforce requirement of many trajectories, because of hight sample-complexity of covariance estimation, which defeats the purpose. For example, see (14).\n\n- Incomplete literature review: \nMost of the existing references focus on discrete-time settings (for which the literature review is still incomplete, just search adaptive linear systems in google scholar), and none of the modern works on reinforcement learning, estimation, and control in continuous-time linear systems are reviewed in the manuscript (again, google scholar helps). Further, on p7, the rich literatures of single trajectory RL and adaptive stabilization are skipped, and less relevant approaches are emphasized.\n\n- Multiple incorrect or inaccurate statements, including but not limited to:\n\"In LQR, the bottleneck ...\" (a big and inaccurate claim without sufficient reasoning), \nFootnote 1 and related material on p2 (generality and optimality are mixed up in a confusing way, and are inconsistent with (3) and prop 1), \nIn (3), the expected value and the covariance of the initial state are assumed known, but it is not mentioned or justified,\n\"In summary, ... any RL algorithm\": such strong statements need strong justifications,\nIn footnote 3, it is not clear how the oracle is used, since access to an oracle does not imply access to its minimizer,\nThe first para on p7.\n\n- Several unclear statements and/or unsupported claims:\nFirst para on p2, \n\"a forward (in time) DRE\" and \"a backward (in time) DRE\", \nThe control signal in (4),\nComputation of \\bar m_t and \\bar \\Sigma_t in (4),\n\"statistics of the process\",\n\"This term has a gain ...\",\nDiscussions after (9),\nComputation of (10),\nNotation in (11), (12),\nDiscussions in \"Arrow of simulation time\",\nThe quantity m on p6.\n\nMoreover: \n- There are frequent typos and writing errors, and the citation format is set incorrectly.\n-The terms 'particle' and 'mean-field' are commonly used and have rich literature. So, borrowing them needs connection to the existing approaches and semantics.\n-Adaptation of control signal to the filtration Z excludes a class of policies and is not required (re-stating based on information etc might help). The same applies to assuming that control signal cannot depend on previous values.\n- It is unclear why in the context of RL, Markovianity of Step 2 is a good assumption.\n- \"Backward Ito-integral\" is undefined.\n- RL terminology is misused and connections to RL look artificial (see item 3 on p6).\n- An incorrect benchmark is adopted on p6 in Relationship to RL: the reasonable benchmark is to solve DRE/ARE.\n- In the motivating application from weather prediction, what is control input?",
            "summary_of_the_review": "The current version of the manuscript is not ready for publication. \n- The problem understudy is not well motivated.\n- Incomplete literature review.\n- Multiple incorrect or inaccurate statements.\n- Several unclear statements and/or unsupported claims.\n- and, further items discussed above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes how the ensemble Kalman fillter (EnKF) can be used on the LQG problem for both state estimation and control. The algorithm is evaluated on a linear system and a linearized nonlinear system.",
            "main_review": "The paper looks at the control as inference duality and proposes the EnKF as a replacement for a Riccati-based solver. Sample based inference, such as the eKF or sequential Monte Carlo, offer greater accuracy especially for complex nonlinear systems.\nThe connection between data assimilation methods and RL is an interesting one and the paper was well written and demonstrated a lot of mathematical rigour. \n\nUnfortunately, this paper has several weaknesses.\n\n1) Scholarship. \nThe duality between Kalman smoothing, LQG and Riccatti equations is well established. This paper barely touched on this literature, and when it did it was only in the appendix. \n\nProbabilistic Inference for Solving Discrete and Continuous State Markov Decision Processes. Toussaint et al. ICML 2006\n\nRobot Trajectory Optimization using Approximate Inference. ICML 2009\n\nOn Stochastic Optimal Control and Reinforcement Learning by Approximate Inference. Rawlik et al R:SS 2013\n\nSampled DIfferential Dynamic Programming, Rajamäki et al. IROS 2016\n\nLinear Optimal Control on Factor Graphs - A Message Passing Perspective, Hoffman et al. 2017 IFAC.\n\nRegularizing Sampled Differential Dynamic Programming Rajamäki et al. ACC 2018\n\nStochastic Optimal Control as Approximate Input Inference, Watson et al. Corl 2019\n\nAdvancing Trajectory Optimization with Approximate Inference: Exploration, Covariance Control and Adaptive Risk, Watson et al. ACC 2021\n\nStochastic Control through Approximate Bayesian Input Inference Watson et al. Arxiv 2021\n\nWhile the Rawlik paper is cited, it is misrepresented by saying it is solving the linearly solveable MDP problem like a path integral method, when the posterior policy iteration algorithm actually uses approximate Gaussian message passing (linearization in the paper). Moreover the R:SS paper is not properly listed in the references.\nThe papers cited above use the duality between Riccati equations and Gaussian smoothing using a range of approximate inference methods, including linearization, quadrature and Monte Carlo.\nNot only is it strange for this large body of work to not be referenced in the paper (when path integral methods were) but it weakes the contribution of applying the eKF to LQG, especially when the eKF isn't well motivated for this setting. Section 2.2 it listed as the main contribution of the paper, but does not seem particularly novel given prior unreferenced work. While the continuous time derivation does seem novel, I did not understand the motivation for this direction since RL is primarily discrete time. \n\n2) Motivation for the eKF. For my understanding, the EnKF is motivated for high-dimensionality linear systems where the number of samples is lower than the dimensionality. While this setting is covered in 3.2 for a synthetic toy task, it was not clear to me why this setting was interesting for actual control problems and did not seem especially relevant to the ICLR community. Perhaps there are more specific problem domains where this is the case.\nA nonlinear extension is teased in the appendix, and in my opinion it is this setting that would be of interest to the ICLR community.\n\n3) Does not seem to perform RL. While RL is mentioned in the title and abstract, it seems the eKF is actually applied in an optimal control setting rather than RL. While it may only use a model implicity and not require linearizing the dynamics, the same could be achieved using quadrature Gaussian message passing (e.g unscented, cubature, Gauss-Hermite). The paper needs to be heavily revised because of this. \n\n4)  Limited experiments. Given the linear setting, the experiments a small scale for a venue like ICLR. In particularly for 3.2, it was not clear why the baselines chosen were used, especially when LQG has a closed-form solution and 80 dimensions are not too large for modern mathematical libraries.\n\n\nMinor comments:\n5) The paper title does not need to include abbreviations\n",
            "summary_of_the_review": "The paper does not reference a large body of existing work on the Kalman fitler / LQG duality which the paper claims is their contribtion. The EnKF is also not well motivated for this setting and the experiments are very limited and seemingly not reinforcement learning tasks. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper is about designing a simulation-based ensemble Kalman filter algorithm for learning the optimal control policy for the Linear Quadratic Gaussian (LQG) control problem.\nThe main contribution is that the paper extends the existing ensemble Kalman filter algorithm for filter (state estimation) design to the optimal control policy learning for the LQG control setting. And it is shown to be exact in its mean-field limit (N=infinity). ",
            "main_review": "Strengths:\n  The motivation for the proposed algorithm is that when the simulation-based models are available and the control problem's state d is high- \n  dimensional, the current differential Riccati equation (DRE) or algebraic Riccati equation (ARE) based algorithms is O(d^2). The proposed \n  ensemble Kalman filter based algorithm computation complexity is O(Nd), where N is the number of particles used for the estimation of the \n  optimal control policy. The proposed algorithm would be more computationally efficient for the above case where N<<d.\n\nWeaknesses:\n  1. Contribution of the extension of ensemble Kalman filter idea to control policy estimation is questionable. Based on the paper's result, it is not that difficult to construct a mean field process in Eq.(9) from backward DRE in Eq.(7) and (8) comparing with the mean field process from existing Kalman filter design. Also, the exactness contribution in Proposition 2 is a straightforward extension of the Proposition 1.\n  2. The claim on the reduced computation complexity from O(d^2) to O(Nd) is debatable. Although N is the number of particles, and we could use N<<d. But will that make the estimated control policy worse than the algorithms with O(d^2)? Should the N depend on the problem size as well? Based on the error bound in Eq.(14), the first term in RHS, ||\\bar{S}_T||_F may also depend on the problem size d as well, which makes the error in Eq.(14) depend on d and thus the particle number N.\n  3. The evaluation metric used in Section 3.2 may not be desirable. The MSE w.r.t. the true matrix is not what the Fazel et al. (2018) tries to minimize. The ideal metric might be using the same loss function to evaluate the performance.\n",
            "summary_of_the_review": "Based on my above comments, I think the paper is interesting but not good enough to be accepted at this time.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}