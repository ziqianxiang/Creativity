{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The manuscript introduces a taxonomy for organizing continual learning research settings and a software framework that realizes this taxonomy. Each continual learning setting is represented by as a set of shared assumptions (e.g., are task IDs observed or not) represented in a hierarchy, and the software is introduced with the hopes of unifying continual learning research.\n\nThe manuscript identifies a clear issue in the field: settings and methods for continual learning have proliferated so that there is little coherence in benchmarks, making progress difficult to judge. Reviewers generally agreed that the motivation of building software to help unify continual learning research was a positive.\n\nHowever, reviewers also pointed to many concerns with the manuscript and software package (Sequoia) that comprises its main contribution. In particular, there is concern that the software is at an early stage of development and makes heavy use of existing libraries to function (e.g. Avalanche and Continuum). This makes it unclear what Sequioa offers over using its dependencies directly. As well, there is concern that multiple standard benchmark tasks and common methods are missing from the implementation — particularly for large scale experiments with, e.g. ImageNet-1k. In theory, the library allows extension and these might be implemented by others in the community. However, this would require that the original manuscript+software are strong enough to draw buy in from other researchers. \nIn sum, the manuscript+software does not yet offer a convincing starting point for researchers looking for a starting point to begin their continual learning research."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper attempts to unify all CL research with a single formalism. Next, they present a software implementation of their framework. Finally, experiments are ran which demonstrate that Sequoia can be used to evaluate CL methods.",
            "main_review": "Pros:\n\nP1) Creating a software package which unifies RL and supervised learning approaches seems like a step in the right direction.\n\nCons:\n\nC1) CL hierarchy - I think there are missing nodes in the hierarchy. For instance, it is important to also consider problem-incremental learning where the task is the same, but the input spaces are different (e.g. the dimensionality of the input changes, not just the input distribution).\n\nC2) Related work: You mention that your framework doesn’t compete with others. It is not clear to me why someone else would not just use the other frameworks. A more in-depth discussion of the previous work is necessary. This way, it would be easier to distinguish your contributions are.\n\nC3) Experiments: The text does not give us information on which methods are implemented by the authors. It gave me the impression that the evaluated methods have all been implemented by another software package.\n\nC4) Experiments: The results are presented but not discussed.\n\nThe provided hierarchy of CL methods might contain an interesting insight, which relates RL and supervised learning approaches to CL. However, due to the lack of comparison to related work, I am not certain of this. \n",
            "summary_of_the_review": "Overall, I don’t see a significant technological or conceptual contribution.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors try to establish a unified framework for different continual learning settings. They also provide a Python library, which includes different related methods. Extensive experimental results are also provided. ",
            "main_review": "### Strengths \n\n&nbsp;\n\n- The continual learning research tree (Figure 2) is well organized and makes a lot of sense to me.\n\n&nbsp;\n\n- I think the motivation of building a unified framework for continual learning is meaningful. Many different continual learning papers evaluate their methods in different benchmark protocols. It is difficult for the following researchers to compare these related methods. \n\n&nbsp;\n\n- Detailed explanations and experimental results are provided in this paper. The authors also provide the results on wandb. It makes obtaining the detailed results for each setting very convenient for the following researchers. \n\n&nbsp;\n\n### Weaknesses\n\n&nbsp;\n\n- Some important continual learning baselines are not included, such as iCaRL (Rebuffi et al., 2017). \n\n&nbsp;\n\n- The authors only provide experiment results on small-scale datasets, such as MNIST and CIFAR. Most continual learning papers, such as iCaRL (Rebuffi et al., 2017) and BiC (Wu et al., 2019), provide the results on large-scale datasets (e.g., ImageNet-1k). As the authors trying to establish a new benchmark protocol, it is important to provide the results on large-scale datasets.\n\n&nbsp;\n\n- This project includes plenty of different methods, but the authors don’t include detailed information (such as the licenses) about the related open-source resources they use. \n\n&nbsp;\n\n- The definition of “incremental learning” in Section 2.1 is ambiguous. I think it is better to use class-incremental learning (or domain-incremental learning) directly. The reasons are as follows. (1) “Incremental learning” is often considered as another name of “continual learning”. It is weird to use it to denote a specific setting of continual learning. (2) You include class-IL and domain-IL in “incremental learning”, but you exclude task-IL. It is not reasonable.\n\n&nbsp;\n\n- The authors include too many details about the code implementation in the paper. I think it is better to move these parts to the appendix and include more experimental results and analyses. ",
            "summary_of_the_review": "Overall, I think this project will be a useful tool for the following continual learning researchers. I will recommend acceptance if the authors can address my concerns in the “weaknesses” part. \n\n&nbsp;\n\n### === Post-rebuttal Comments ===\n\nI thought the authors aimed to establish a unified software framework that makes running continual learning experiments easy.\nHowever, after reading the rebuttal, I think Sequoia has the following major issues and the authors failed to address them in the rebuttal:\n\n- ***Sequoia heavily relies on the previous libraries, such as Avalanche and Continuum.*** I don't think this design is very friendly to the researchers. In my personal view, I prefer a framework that is easy to be understood and includes the most popular baselines. I think your framework should be designed for a researcher instead of a software engineer.\n\n- ***Sequoia hasn't been evaluated on large-scale datasets (e.g., ImageNet-1k).*** If I need to use this framework, I need the framework can reproduce the results of the previous baselines correctly. \n\nSo, I don't think Sequoia is very useful to a researcher like me. According to my personal experience, I tend to reject this paper. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
            ],
            "details_of_ethics_concerns": "This project includes plenty of different methods, but the authors don’t include detailed information (such as the licenses) about the related open-source resources they use. ",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a theoretical framework to organize research problems in the continual learning (CL) domain according to a hierarchy. This theoretical framework is used as the basic foundation for Sequoia, a software library designed to reuse methods (i.e. training algorithms) across different research problems (settings).",
            "main_review": "The goal of the paper is very ambitious. There exist many open source libraries implementing CL methods for supervised learning and reinforcement learning settings.\n\nFRAMEWORK:\nAccording to the paper \"each setting is described as a set of assumptions. A tree-shaped hierarchy emerges from this view\". This shows a limitation of the theoretical framework. Defining settings as a set of assumptions does not result in a tree-shaped hierarchy (it would be a lattice). Therefore, using a tree-shaped hierarchy means that some methods will be compatible in principle but not in practice (in Sequoia) because the tree lacks the connection. This a strong limitation of the framework that is not discussed.\n\nIMPLEMENTATION:\nThe framework heavily relies on several dependencies to implement the heavy lifting (datasets, RL environments, Methods). This is not a big problem by itself, but it is not clear what Sequoia is adding compared to the original libraries.\n\nEXPERIMENTS:\nThe experiments show several baselines in the supervised and reinforcement learning settings. Most methods are not implemented by Sequoia and are inherited from avalanche for SL and stable-baselines and continual-world for RL. One thing that I expected from this section was how the same method could be easily applied to different settings, which is the main claim of the library. Instead, SL and RL settings share zero methods. For example, the EWC implementation is different in the SL and RL settings. Does this mean that users have to implement every method twice? It seems that RL and SL methods are completely separate, which is against the entire spirit of the library. At this point, what is the advantage that Sequoia brings compared to its dependencies (continuum, gym, avalanche, stable-baselines, continual-world)?\n\nADDITIONAL COMMENTS:\n- \"constraints often relate to memory, compute, or time allowed to learn a task\" -> is Sequoia able to check time and memory constraints?\n- is it possible for the end users to define new settings or modify the hierarchy? it would be interesting to see an example.\n- \"We note that some Avalanche methods achieve lower than chance accuracy in task-IL because they do not use the task label to mask out the classes that lie outside the tested task\" -> can you fix this problem? Otherwise a user needs to know the internals of each library to debug the code. This adds a lot of complexity.\n",
            "summary_of_the_review": "I believe the objective of this paper of merging together continual SL and RL is very important and ambitious. However, the paper in the present form has several weaknesses. It is unclear whether the theoretical framework really achieves the paper’s objective. Sequoia (the software) seems to be still at a very alpha stage in its development cycle. I see very little unification in the methods, which is the main scope of the paper. This strongly limits the methods' reuse between the different settings. It is unclear what advantages sequoia is bringing compared to using its dependencies directly.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper introduces a new continual learning framework that aims to boost the research in the field. This framework is based on a taxonomy of all possible assumptions that are common to CL methods. Moreover, this taxonomy helps in putting supervised and reinforcement methods in a unified framework.\n",
            "main_review": "- In section 2, the authors propose a Markovian process that is being ignored in the later parts. It is not clear why this hidden-mode Markov decision process is useful to the proposed framework.\n- While I do appreciate the effort in developing this framework, I am lacking the novelty in the paper. The work is mainly an engineering effort that's appreciated but might not fit this conference.\n",
            "summary_of_the_review": "While I do appreciate the effort behind this work, I doubt the match between this paper and the scope of ICLR.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}