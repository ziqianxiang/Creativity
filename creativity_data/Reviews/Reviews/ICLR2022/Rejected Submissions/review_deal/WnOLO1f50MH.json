{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The authors study separable convolutions in the group-convolutional setting, and describe experiments showing them to be more computationally efficient without loss of performance in the setting of some group-augmented MNISTs, and show some promising results on un-augmented CIFAR10, CIFAR100, and Galaxy10.  The reviewers are mixed; some of the reviewers have concerns about the completeness of the experiments and the novelty of the work, and in particular to what extent the experiments support the specific novelties claimed.  The authors have made some updates to address this in the revision, but my opinion is that the authors should resubmit to the next venue after further experiments and exposition to clarify."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper adresses redundancy in group convolutional filters and the scalability of group ConvNets. It is achieved by introducing a separable group convolution for Affine Lie group which allows to share the kernels for translation elements.",
            "main_review": "The strengths of the paper are as follow :\n\n1/ Clarity of the theoretical and empirical parts\n\n2/  Experimental results are promising\n\n3/ The use of SIREN  to parametrize convolutional kernel is a tricky idea and quite original since it is originally proposed for coordinates space data. As a result, a given layer has a fixed number of parameters regardless the resolution of data\n\n4/ Approximating the group convolution integral through random sampling shows a clear advantage over discretization schemes. It allows to mitigate some artifacts of raw data \n\nThe weaknesses are as follow :\n\n1/ The proposed model is quite generic. In order to show its scalability, one may wonder if the proposed model could be extended to large scale vision tasks : 3D rendering, Video classification and also to other domains like physical processes where group of symmetries are important.\n\n2/ Lack of comparison with state-of-the art models\n\n3/ Need a further theoretical study \n\n\nSmall typos in appendix, before equation (9) \"the group group\"\n\n",
            "summary_of_the_review": "Following the aforementioned consideration, l recommend to accept the paper. The proposed research direction is promising and could open perspectives to several domains, mainly physical process tasks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper discusses considerable parameter redundancy in regular group convolution networks and then proposes separable convolution kernels to share the weights over the subgroups. Besides, the authors explored the equivariance of three different groups and presented a continuous parameterization scheme. Evaluations on several datasets show that their proposed method gains improved performance and computational efficiency.",
            "main_review": "The reviewer is very interested in this research work. This paper explores the shortcomings of existing work in depth, discusses separable convolution kernels, and does the parameterization of different groups more comprehensively, which  is a good research guide. In addition, the authors also conducted experiments on many vision datasets to illustrate the superiority of the proposed method and achieved performance improvements.\n\nHowever, there exist some confusing parts or weaknesses to be further strengthened.\n1. The biggest shortcoming of this paper is its limited innovation. As stated in this paper, the proposed method is more like a promotion of Cohen2016, by using a strategy similar to ‘depthwise separable convolution’ in Xception. It seems that this work depends on the existing method heavily and its original parts bring weak contributions.\n\n2. The theoretical foundation of this paper is not solid. It is worth mentioning that the authors explored arbitrary affine Lie groups (three different groups: SE2, dilation +translation, and dilation +Sim(2)). But as far as the reviewer knows, this is not entirely applicable. For example, correlation is an equivariant map for the translation but NOT for the rotation group. Why can the authors directly consider SE(2) and Sim (2) in convolution (both containing Rotation)? In addition, when proving translation equivariant, it is necessary to extend f and k to group G, otherwise g^(-1)x does not belong to group G and cannot go on. The reviewer has not seen relevant explanations. Regarding Sim(2), it has a totally different Lie algebra expression from SE(2)’s due to the scaling, and there has not existed a discussion on specific forms. \n\n3. The motivation of the proposed method comes from parameter redundancy, but from Figure 1, the reviewer found that the metric is F-test (variance ratio), which is usually used to analyze statistical models that use more than one parameter to determine whether all or part of the parameters are suitable for estimating the population. And the robustness of F-test is extremely dependent on the samples with a normal distribution. In the case of limited samples, the distribution is often biased, which tends to reduce the power of such a test, leading to incorrect statistical inference.\n\n4. The authors used SIREN as kernel parameterization in the Lie algebra. But as far as the reviewer knows, this work proves the superiority of using periodic activation functions (such as sine), which is, however, irrelevant to this paper from the perspective of the reviewer. Much more details should be provided here.\n\n5. Another concern of the reviewer is that evaluation results of this paper are not convincing to the reviewer. Traditional CNN-based methods have achieved 95+% accuracy on CIFAR10, and regular GNNs (p4m & Z2) can also reach 90+%. The accuracy results of this paper are still inferior to 90% with a 77% baseline. It violates the claim that \"G-CNNs usually improve upon regular CNNs\" stated in this paper.\n\n6. Some parts are not novel at all, such as the section “Depthwise separability”. The reviewer suggests the authors to include more relevant references (Xception or more). \n",
            "summary_of_the_review": "The research field of this paper is very interesting, but unfortunately there remain many shortcomings in theoretical and experimental results, which need to be forcefully addressed. The reviewer therefore thinks that the paper is not good enough to be published on ICLR.\n\nAfter reading the authors' rebuttal, I increase my rating a bit. But I am still not contented with the stuff in this submission, which should undergo a thorough revision to convince and be easily understood by the readers.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper builds group convolutional neural networks based on the depth-wise separable convolution operations, which are commonly seen in modern CNNs. The authors demonstrate that the $Sim(2)$-equivariant can be achieved in such separable convolution operations. In the implementation, the authors borrow the SIREN approach for parameterizing the proposed group convolutions. Finally, the experimental analysis shows good improvement over other types of group convolutions.",
            "main_review": "This is a solid paper to explore the combination of group convolutions and techniques employed in popular CNN architectures. The ideas of how we can achieve $Sim(2)$-equivariant on separable convolution operations are clearly explained, they are helpful in designing kernels with better performance. Besides, the paper itself is educational to readers not familiar with this field, I recommend accepting this paper to let more researchers benefit from the ideas behind G-CNNs.\n\nI have a few questions to ask:\n\n1. The motivation is explained as the group convolutions learn redundancies. Is this only found in G-CNNs or all types of CNNs? Do separable convolution operations address this problem finally? Can we get a figure similar to Figure 2?\n\n2. Although the utilization of SIREN is the highlight of the paper, the motivation behind is unexplained and unclear. Why the group convolution kernels can be represented implicitly? Is SIREN the only choice?  Do larger SIREN networks provide better parameterization?\n\n3. Why does random sampling over subgroups provide better performance? I am baffled by these results, if we can design a discretization sampling method that has similar anti-aliasing effects?",
            "summary_of_the_review": "The bad thing of such mathematical ideas inspired paper is that the experimental performance are always far from sota. But I think the performance is not the only thing, it is not bad to accept this paper for me.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes to use separable convolutions along the group dimension in the type of group CNNs proposed by Finzi et al [1]. The motivation is the same as the popular depthwise separable convolutions for conventional CNNs: reducing parameter redundancy to increase efficiency and accuracy. Experiments on rotated MNIST, CIFAR10/100, and Galaxy10 show that the method outperforms the non-separable versions both in accuracy and speed.\n",
            "main_review": "*Strengths*\n\nS1: The paper is well-written, the method introduced makes sense and seems to work well in practice. \n\n*Weaknesses*\n\nW1: The submission has somewhat limited novelty. It can be seen as an application of depthwise convolutions to group-CNNs. Depthwise convolutions are quite popular since at least 2017 (Xception, Mobilenets); and the (non-separable) group-CNN utilized seems the same as Finzi et al [1], with sinusoidal activations. More importantly, the idea of separating the group-convolution along the group dimension has also appeared in Lengyel and van Gemert [2], although for a different flavor of G-CNN. \n\nW2: I see several issues with the experimental section. The most problematic is that all comparisons are against the paper's own baselines. While this makes for fair comparisons in the sense that the data pipeline, architectures and training schedule are all the same (although I have reservations about the number of parameters, see W3), I'm not sure if the conclusions hold in general. For example Weiler and Cesa [3] seems to show significantly better performance than the proposed method on rotated MNIST and CIFAR10/100; can the proposed method improve those results? If not, what would be the applications where the method is useful? Finzi et al [1] also show favorable results on a molecular property prediction, how does the submission fare in that task? When comparing with Finzi et al, I believe there should also be an ablation to disentangle the effects of the separable convolutions and the use of SIREN instead of a regular MLP.\n\nW3: Please report the number of parameters for each model trained. If I understand correctly, the number of channels per layer is kept constant, so the non-separable models have several times more parameters than the separable, is that correct? In that case could the lower performance on non-separable be explained by overfitting or slower convergence? I think both constant number of channels per layer or constant number of parameters are informative and should be reported (Weiler and Cesa [3] do something similar for CIFAR). \n\nW4: In Table 5, I believe a few experiments are missing and would be needed to disentangle the effects of the separation along the group dimension and the channel dimensions. I suggest to show, for each group, the performance when the convolutions are separable over the channel (depthwise) but not the group dimension. And for the baseline, it would be interesting to also see the performance for the separable depthwise version. \n\n*Questions*\n\nQ1: For the rotated MNIST, it is shown that approximating the convolution with random samples is superior, however the other MNIST variations seem to fall back to the discretization. Why is this the case? Do the random sampling approximation perform worse on the higher dimensional groups?\n\nQ2: The SIM(2) MNIST experiment is described as limited to 2, 4, 6, or 8 elements for each subgroup. Does this also refer to the way the dataset is constructed, or is it created with random rotations and scaling sampled from the continuous internal?\n\n*References*\n\n[1] Finzi et al, \"Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data\", ICML'20.\n\n[2] Lengyel and van Gemert, \"Exploiting Learned Symmetries in Group Equivariant Convolutions\", ICIP'21.\n\n[3] Weiler and Cesa, \"General E(2) - Equivariant Steerable CNNs\", NeurIPS'19.         ",
            "summary_of_the_review": "I believe the paper is not there yet, mainly because of limited novelty when compared to [1, 2] (W1), lack of comparison against the state-of-the-art of the tasks proposed (W2), and not very convincing experimental results due to lack of details (W3) and ablations (W4).\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}