{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes a new approach for weakly supervised learning, based on conditional normalizing flows. Reviewers generally found the paper to have an interesting, novel proposal with empirical promise. However, some concerns were raised: to name a few,\n\n(1) _Clarity._ Several reviewers found portions of the technical content hard to follow, e.g., the description of constraints in Sec 4.\n\n(2) _Scalability compared to data programming._ One reviewer was unsure of how the present approach compares in terms of inference time and/or accuracy to a two-stage data programming approach.\n\n(3) _Infeasibility of sampling from Equation 2._ One reviewer suggested the paper discuss and compare to a simpler baseline, which is to perform rejection sampling from the constraint set.\n\n(4) _Suitability of point cloud problem._ One reviewer was unsure of whether the point cloud problem, considered as an experimental setting in this paper, is reflective of weakly supervised learning.\n\n(5) _Practicality of knowing weak labeler error rates._ The paper assumes knowledge of the weak labeler error rates in constructing constraints. Some reviewers raised concerns on the practical viability of this assumption.\n\nFor point (2), the relevant reviewer was not convinced following the discussion. The suggestion is to treat LLF as a label model, which serves as input to a non-MC predictor. The question then is what the predictive performance of this combined approach looks like, as opposed to the LLF's themselves.\n\nFor point (3), the response clarified that the number of constraints might make rejection sampling infeasible. This appears to be true, but it is suggested that the paper at a minimum discuss this, and ideally also clarify claims about the general-purpose need for the proposed approach (since in some cases one might be able to do rejection sampling).\n\nFor point (4), the discussion was somewhat inconclusive. It is suggested that the authors explicitly discuss some of the points brought up in the response.\n\nFor point (5), while the assumption not wholly uncommon in the literature, it would be better for the authors to perform some sensitivity analysis against misspecification of the error rates.\n\nOverall, the paper has some interesting ideas that are well worth exploring. The present execution appears to have some scope for improvement, with the reviews providing a range of suggestions of areas of the paper that could be made clearer or strengthened. The paper would be best served by incorporating these comments and undergoing a fresh review."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes to utilize conditional normalizing flows to exploit weak supervisory signals by converting the conventional likelihood maximization problem into a constrained optimization problem. The paper develops three (very) different constrained problems and demonstrates the method on each.",
            "main_review": "Overall, I like the paper and vote to accept. The conversion of the traditional use of normalizing flows is clever and appears to be effective. I really appreciate the variety of the experiments, e.g., a classification problem, a regression problem, and a generative problem. That the generative problem is additionally performed within the constraints of point clouds/sets is very impressive and really helps to illustrate the versatility of the idea.\n\nThat said, I had to read section 4 several times before I understood how the constraints were constructed for a particular problem and am not sure I could construct them on a different problem set. The paper would be considerably more accessible with a simple toy problem with figures/cartoons. This could also be used to help better motivate \"weak signals.\"\n\nI'm also curious why the paper (exclusively?) utilizes affine coupling layers to construct the normalizing flow when more advanced methods exist (e.g., spline coupling).\nThe use of a GAN as part of the constraints alongside the flow-model appears to be a bit of a contradiction but I think helps to illustrate the variability that is allowed within the framework.",
            "summary_of_the_review": "The paper proposes a clever twist on conditional normalizing flows and shows the versatility of the idea on several different weakly supervised problems. An illustration would help to make the paper more accessible which would increase it's impact.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new method for weakly supervised learning where the true label of a data sample is unknown but there are known constraints on the label. The proposed method is based on conditional normalising flows, where the inverse flow is used to model the conditional label distribution and optimised with the label constraints. The proposed method is used in three applications including classification, regression, and point cloud completion. The proposed method is shown to have better performance in these applications in the comparison with several baseline methods.\n\n",
            "main_review": "Strength:\n\n1. This paper presents a new method based on normalising flow for weakly supervised learning problems. It seems to be novel in this specific setting.\n\n2. The paper has a comprehensive set of experiments in a variety of applications.\n\n3. Overall, the paper is well written and easy to follow.\n\nWeakness:\n\n1. It is a bit unclear why normalising flow is used. Specifically, in the weakly supervised learning setting of the paper, $y$ is unobserved but we know a set of constraints of $y$, i.e., $C$. It is unclear why directly parameterising $\\log{p(y|x)}$ and optimising with $C$ is impossible and why normalising flow makes it possible. I feel that the context around Eq (4) needs more explanation.\n\n2. It seems that there are not many existing methods that exactly follow the setting of the paper. Therefore, it's better to have a more comprehensive comparison with the latest related methods, such as PGMV and its variants in [1] and AMCL and its variants in [2].\n\n[1] Mazzetto, Alessio, et al. \"Semi-supervised aggregation of dependent weak supervision sources with performance guarantees.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2021.\n\n[2] Mazzetto, Alessio, et al. \"Adversarial Multi-Class Learning under Weak Supervision with Performance Guarantees.\" International Conference on Machine Learning. PMLR, 2021.",
            "summary_of_the_review": "The idea of the paper is interesting and seems to be effective. But more explanations and comparisons are needed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This article proposes **label learning flows (LLF)**, a general framework for weakly supervised learning tasks. The modelling framework is primarily driven by 1) conditional normalizing flows (Trippe & Turner, 2018), which is used as a flexible conditional likelihood model, with an affine coupling flow layer, and 2) constrained optimization formulation of maximum likelihood, where weak signals/error rate bounds enter as constraints (Arachie & Huang, 2021). The main practical formulation uses Lagrange multiplier to define an unconstrained stochastic loss function that requires Monte-Carlo samples during training (Equation 6). Ultimately, they obtain a trained inverse flow, which conditional on a covariate, can produce prediction samples.",
            "main_review": "Section 3 gives a theoretical underpinning to the procedure, claiming that learning LLF is analogous to dequantization, i.e., learning LLF optimizes the likelihood of dequantized true labels. While I can see the similarity in Theorem 1, I fail to see how this motivates the use of LLF as an appropriate method for learning the true label's generating process, which ultimately should be our target in weakly supervised learning. Dequantization is typically considered to simplify the fitting of discrete observations, whereas in this paper we do not have any observations from the true generating process.\n\nSection 4 gives instantiations of LLF to three different scenarios: weakly supervised classification, weakly supervised regression, and unpaired point cloud completion. The formulation for classification is from previous work (Arachie & Huang, 2021), with LLF as the solution instead of min-max optimization (Equation 8). For regression, the paper assumes the existence of M rule-based weak signals (Equation 9), which again enter the optimization problem as constraints (Equation 10). Lastly, the formulation for unpaired point cloud completion is presented, which is set-up differently from the classification and regression cases. It is more similar to a VAE-GAN formulation (Larsen et al., 2016) where the encoder is the conditional normalizing flow. Alternating steps are required during training as per traditional GAN.\n\n### Pros\n\n- Creating a unified framework to solve both (inaccurate) weakly supervised classification and regression is an important research question as collecting data sampled from the true data generating process can be an expensive process and while a lot of research has been done on solving weakly supervised classification, the regression case is under-studied. Having one paradigm can reduce the entry barrier for users who have to solve both problems.\n\n- Formulating weak supervision, where there is no access to true labels, as an inverse flow which sidesteps the need to estimate the labels prior to training the model is an interesting formulation. This work makes use of the same weak supervision setting as ALL (Arachie & Huang, 2021), but the training is simpler as it does not require min-max optimization.\n\n### Cons\n\n- In unpaired point cloud completion, the complete clouds, which are true observations from the true data generating process, are considered as weak supervision, and the partial clouds are the covariates that we seek to map to being complete. In typical weak supervision scenarios, weak signals are generated in a paired manner to their covariates. Even though I can see how Equation 12 can be cast as Equation 6, I fail to see how unpaired point cloud completion is a weakly supervised learning problem. I was wondering if we can reformulate this problem such that the labels that are being used are truly weak, i.e., not coming from the true conditional p(y|x)? Moreover, just like the typical weakly supervised learning set-up, we should not tune on true validation sets.\n\n- The paper states \"However, these methods ignore this uncertainty between x and y\", and \"LLF uses a conditional flow to define the conditional distribution p(y|x), so that can model the uncertainty between input x and all possible y\". However, the experiments only measure test set accuracies and not goodness-of-uncertainty metrics like NLL or calibration. If we want to make this claim, can we show that LLF performs good uncertainty quantification?\nWe are comparing approaches that only use weak signals against LLF (and ALL) which require the error rate bounds of the weak signals, an extra piece of information. In this paper, it is not clear to me how the error rate bounds are computed — in order to keep things fair, error rate bounds should be determined solely based on the weak signals. Is it possible to clarify this?\n\n- Inference requires sampling and summarizing which can be slow and limits the use of LLF as a final predictor. On the other hand, two-stage approaches like Data Programming (DP) uses the label model to output probabilistic labels, which is then used to train a downstream neural network. As it is right now, if we compare their inference speeds, DP seems to be better. Can we also use LLF in a two-stage manner like in DP? How does it perform then?\n\n### Additional Points for Discussion:\n- Can you handle the case where you know of the dependence/correlational structure between the weak signals?\n\n### Additional Experiments:\nSee Cons\n",
            "summary_of_the_review": "Conceptually, constrained flow formulation of LLF is interesting and novel. However, its numerical results are not too convincing and there are some questions related to the practicality of this method.  I also do not think that unpaired point cloud completion should be an example of weakly supervised learning.  As it stands right now, I do not think that this paper meets the bar of \"powerful and effective tool for weakly supervised learning problems\". The paper can be much improved (especially for weak supervision practitioners audience) by addressing the questions under Cons and additional points above.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work focuses on the problem of learning predictive models where no ground truth labels are provided.  Instead a set of weak constraints over instances that restrict the space of possible target values are provided.  The proposed method, LLF, is based on the core idea that instead of determining a single true weak label for each instance, a model can be learned by considering the expectation over all possible labels in the feasible set defined by the given constraints.  The authors utilize a conditional normalizing flow model as a generative process for labels and constrain it's output by imposing regularization terms on the training objective that represent different constraints imposed by the weak supervision.  The authors show how their framing of weakly supervised learning can be applied go specific settings in classification, regression, and unpaired point cloud completion.  In their evaluation, the authors compare LLF to a number of weakly supervised baselines and show superior performance on classification and regression tasks, while largely being outperformed by one method in point cloud completion.",
            "main_review": "Major Points:\n- Clarity regarding formal definition of the model.  Overall, the paper does not sufficiently describe some important technical details, especially with regards to prior work.  In many places, informal language is used instead of specific, formal definitions of key aspects.  This makes it difficult to fully grasp the work without having read numerous papers on the subject if certain points can be gleaned at all.  I am not suggesting that the work has to extensively review the methods LLF is based on, but the paper could be greatly improved simply by fully elaborating on the construction of the method and taking the time to formally define important concepts.  Some examples include:\n1. 3. “In this paper, we use affine coupling layer (Dinh et al., 2014; 2016) to form normalizing flows. It splits the input to two parts, and force the first part only relate to the second part, so that the Jacobian is a triangular matrix”.   The equation below this elaborates further by introducing a “split” function, but it isn’t clear 1) How the vector “y” is split 2) Whether the particular split is important (can you split them any way?) 3) How this results in a triangular Jacobiam 4) Why a triangular Jacobian is important here.\n2. “Previous methods only look for one possible y within Ω.” This needs a citation.  What methods?\n3. Theorem 1:\n    A.  “tight enough space” is too informal.  What is meant here?\n    B. It is stated that for two different true labels their constrained spaces are “non-overlapped”:\n        I. Does this mean “disjoint”?\n        II. It is stated without direct relationship to the rest of the theorem.  Is this assumption?  Is it for all pairs of labels or just two?  \n        III.  I think the main takeaway from this theorem is to directly compare LLF to a technique that fix labels derived from weak supervision, namely the LLE objective lower bounds the traditional cross-entropy objective, which would be used in the case where weak supervision is used to obtain hard labels for training.  Not only does  I think not only does this point need to be more explicit, but if there is a technique in the literature that does the latter, it should be cited.  Further, the intuitive benefit or insight this theorem shows should be stated more explicitly and clearly.  Later it is written: “learning LLF is analogous to dequantization (Theis et al., 2015; Ho et al., 2019).”.  This is helpful, but elaborating on what dequantization is and why this relationship is important would be better.\n4. It’s not clear to me how Equations 2 and 4 result in the constrained optimization in Equation 5.  Is it that you’re simply replacing the sampling assumption in Equation 2 with a constraint?\n5. “L” is introduced without definition.  Later, through context, I was able to understand it as the number of samples of z ~ p_Z, but it should be made clearer.\n6. Equation 7 and Equation 8: What does the inequality mean between the two vector operands?  Is it element wise?\n7. The problem definition of the unpaired point cloud completion problem (Section 4.3) is rather informal.  What specifically does it mean to “restore” an incomplete point cloud?  How is it measured?\n8. It is unclear from Section 6 what the various hyper parameters for LLF are set to.  Namely, the paper states that that “\\lambda = 10”.  Does this indicate that for all experiments and all different \\lambda hyper parameters across all learning problems (this encapsulates 9 different hyper parameters) are set to 10?  If this is true, this is rather remarkable due to the variety of influences each constraint term has on the proposed objectives (discussed later)\n\n- Practicality. Some of the motivation for key assumptions made in the paper lack argumentation for their practicality.  In my mind this hurts the potential impact of the paper.\n1. Why is it a practical assumption that the error rate for a weak labeler to be known a priori in the weak classification case?  How does one obtain this in practice?\n2. There should be some discussion on the kinds of problems the weakly supervised setting proposed in this work applies to.  The case studies help, but those seem highly specialized and weakly motivated.  For instance, the weakly supervised regression seems somewhat impractical. In general, humans struggle to provide real valued labels such as those suggested int he given example.    I think such single-variate, rule-based assessments make sense for classification problems, where humans can more easily make class assessments based on thresholds (such as “patient has a disease if lab test x_i is over threshold e).   Even in the given example, the underlying problem is better modeled as classification problem (disease versus not-disease), where the labels are noisy assessments of confidence in class membership.\n\n- Other technical questions that need to be addressed:\n1. “Since y is unobserved, directly optimizing the conditional likelihood, i.e., Equation 2, is impossible.”  I don’t understand this.  If p(x) and \\Omega are given, you should be able to compute Equation 2.  Sampling from p(x) for the outer expectation gives you, x.  Sampling from U(\\Omega) for the inner expectation gives you y.  The optimization variable is \\phi.  Why can’t I compute p(y|x,\\phi)?  Is it rather that sampling within the constraint set is impossible or that it’s simply computationally infeasible in practice?  Clarification is needed.\n2. In Equation 8, it seems very important to ensure g(x) is producing valid soft-labels in the simplex and, similarly, in Equation 9 it seems important to ensure the regression labels are in [0.1] (which is an assumption I would like to see explicitly stated in the paper).  The reason I believe this is that other constraint terms could easily push the objective to infinity (namely, the lambda_1 terms).  How is this managed practically?  Would projected gradient ascent be more appropriate to ensure model outputs are on the proper simplex to alleviate this issue?\n3. It is difficult to interpret the performance of the proposed methods as well as the baselines if there is assumed to be no ground truth available for validation and all hyperparameters are default.  How can one be sure that the performance of LLF relative to the baselines cannot be explained simply by assuming bad default parameters for the baselines?  \n\nMinor Points:\n1. “attaining” in the second sentence in the abstract seems like the wrong word.  Perhaps “obtaining”?\n2. “Therefore, this model captures all possible relationships between the input x and output y”.  Seems dubious.  Certainly, this is subject to the class of functions used to learn p(y|x).\n3. Consider using a different function name for “f” (constraint functions) in section 3, as “\\textbf{f}” is used as a the normalizing flow vector function above.  Having them be the same character might make readers try to understand them as directly related, when they are not.\n4. I believe in equation 2 “p(y|x,\\phi)” is the same as equation 1’s p(y|x).  If so, they should be consistent (either they both should be conditioned on \\phi, or neither)\n",
            "summary_of_the_review": "Overall, the idea of modeling the range of possible labels for instances based on weak supervision instead of simply using a single estimate is a very interesting one.  However, the paper suffers from a number of major issues.  1) The experiments use default hyperparameters for all techniques, thus it is difficult to tell whether the relative performance of methods is indicative of their quality or the quality of the particular default values of their hyperparameters. 2) The normalizing flow component is not well motivated nor is it clearly presented in the context of learning from weak supervision. 3) Some of the proposed used cases lack strong practical motivation.  Many of the major issues I present in the main review seem difficult to remedy in a short period of time, though, if they are the result of a misunderstanding on my end and small edits to the paper could alleviate future readers from similar confusion, I would be willing to change my score.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Different from many existing weakly supervised learning methods, which learn a deterministic function that estimates labels given the input data and weak signals,  this paper proposes label learning flows (LLF), a general framework for weakly supervised learning problems, and it is a generative model based on normalizing flows. The main idea of LLF is to optimize the conditional likelihoods of all possible labeling of the data within a constrained space defined by weak signals.  The proposed method is applied to three weakly supervised learning problems. Experimental results show that this method outperforms many state-of-the-art alternatives.",
            "main_review": "This paper has some positive aspects as follows:\n1. The proposed method is novel. This method proposes to optimize the conditional likelihoods of all possible labeling of the data within a constrained space defined by weak signals, which can alleviate the issue that the existing methods ignore the uncertainty between x and y.\n2.  The experiment is abundant. The proposed method is validated in three different tasks, including weakly supervised classification, weakly supervised regression and unpaired point cloud completion, which shows its effectiveness.\n3.  The paper is well written, the arguments are clear, and the methodology is well presented.\n\n The authors need to deal with some issues as follows:\n\n4.  In Table 1,  for OBS Network, the performance of LLF is poor than other methods, while on other datasets can obtain the obvious improvement.  the authors should provide an explanation for this result.\n5. For the results in Table 4, it is better, if the authors can give a comprehensive evaluation protocol, which can mix MMD, TMD and UHD together, thus it can obviously highlight the superiority of LLF in unpaired point cloud completion.\n",
            "summary_of_the_review": "The proposed method is novel and the experiment is sufficient. More experimental analysis can further improve the quality of this paper.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}