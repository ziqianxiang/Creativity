{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper explores the usefulness of intermediate layers for linear probing, aiming at improving out-of-distribution transfer with significantly less cost than fine-tuning. Two reviewers recommended borderline acceptance, while two others recommended borderline rejection as final rating. The main concerns raised by the reviewers were the limited novelty of the proposed method (e.g., compared to Elmo), unconvincing results in the natural and structure categories of VTAB, and lack of experiments to justify the claims, as well as the demonstration of the method in other tasks beyond image classification. The rebuttal has clarified several other questions. The AC really likes the simplicity of the approach, and also finds the problem of improving the efficiency of transfer learning very important. In addition, the paper is very well-written and easy to follow, as acknowledged by all reviewers. However, the AC agrees with R2 and R3 that the paper, in its current form, does not pass the bar of ICLR, unfortunately. First, the novelty is limited, as pointed out by R1, R2, and R3. In addition to the related works mentioned in the reviews like Elmo, note that the idea of selecting intermediate features, concatenating them, and running a linear classifier for OOD transfer has also been explored in [Yunhui Guo et al, A broader study of cross-domain few-shot learning, ECCV 2020]. Second, while the approach has advantages in terms of efficiency, the accuracy drop (compared to fine-tuning) for in-domain tasks limits its applicability. Finally, even though the AC agrees with the authors this is not a requirement, a more comprehensive set of experiments on more tasks would make the paper stronger, especially given that the novelty is incremental. The authors are encouraged to improve the paper for another top conference."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper explores the utility of intermediate layers for linear probing in transfer learning. Specifically, authors proposed Head-to-Toe\nprobing (HEAD2TOE), that selects features from all layers of the source model to train a classification head for the target-domain. Experiments on the VTAB benchmark shows that Head2Toe matches performance obtained with fine-tuning on average, but critically, for out-of-distribution transfer, Head2Toe outperforms fine-tuning.",
            "main_review": "Strengths:\n- The paper is very well-written and easy to follow. \n- The overall idea of Head2Toe for transfer learning is interesting.\n- Competitive performance on VTAB benchmark.\n\nWeaknesses: While the main idea and experiments presented in the paper are very interesting, there are several important weaknesses in the paper that need to be throughly addressed to improve the quality of the work (mainly missing experiments, analysis and comparisons).\n\n- While authors empirically demonstrate the utility of intermediate features in experiments, there are no feature visualizations to support the claims. How about visualizing the selected intermediate features per target task? Is it possible to analyze the weight matrix W across different target datasets to better understand what type of features being selected for transfer learning? Are the selected features complementary go each other?\n\n- In current experiments, features are selected specific to one target task. How about selecting a common set of intermediate features for all the tasks? How is that global baseline comparable to individual selecting selection of features per task?\n\n- I would suggest the authors to rearrange or rewrite the main contributions of the paper. Points 1-5 in introduction section are not really the contributions. In other words, authors should merge them into 2-3 key contributions along the directions such as, problem definition, methodology and experiments.\n\n- How the proposed method comparable to finetuning on other tasks besides image classification, e.g., video classification? I believe the main conclusions will still be the same but a small experiment on this will strengthen the paper.\n\n- Authors compare with layer wise selection by computing the mean relevance score of each feature in a layer. Can it be not done through the L21 optimization directly where group of features belong to one layer will be made zero?\n\n- Authors mention that one of the properties of the proposed method is key for transfer-learning methods to be practical with billions of parameters such as GPT-3. I wonder how is the proposed method applicable to transfer learning in NLP. Experiments and analysis on text classification or other standard NLP tasks should be included in the paper to validate the effectiveness of the proposed method beyond image classification.\n\n- Is it possible to include some of the tweaks used in ViT paper to further improve the performance of Head2Toe (say Head2Toe+), e.g., different aggregation schemes as authors mentioned in the main paper?\n\n- How is the proposed method comparable to Linear probing and Finetuning with different percentages of labeled data in the target domain? Is it still effective in low data regime, e.g., 1% of labeled samples?\n\n- How is this method comparable to existing transfer learning methods besides Finetuning on VTAB benchmark, e.g., selective finetuning or some of the methods mentioned by the authors in the related work?\n",
            "summary_of_the_review": "Given the interesting idea and competitive performance, my initial recommendation is borderline accept. I would like to see authors response on new experiments and discussions during the rebuttal as I think the experiments are limited and somewhat unconvincing in the current version of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper propose Head2Toe, a method that exploits intermediate representations of DNNs to improve performance, and OOD generalization of transfer learning. The key idea of the Head2Toe is to augment traditional linear probing with intermediate representations. It uses group lasso as regularization to learn weights of different features, and then select features based on the learned weights and validation performance.Experiments on VTAB benchmark show that Head2Toe outperforms linear probes and is competitive compared with fine-tuning. An important finding is that Head2Toe improves OOD generalization.",
            "main_review": "Strengths:\n1. This paper investigates how to use pre-trained models for better transfer learning, which is an important problem.\n2. The paper is generally well-written and easy to follow. \n3. The proposed method is simple, technically sound, and makes sense. \n4. The authors conducted extensive experiments on VTAB benchmark with both ResNet and ViT backbones, and conducted comparisons with several baselines. The overall performance of Head2Toe shows some improvement compared to linear prob and even fine-tuning.\n5. The preliminary experiments and analysis is informative and helps the readers understand to motivation of the paper.\n\nWeaknesses:\n1. The idea of combining intermediate representation from a pre-trained model to improve fine-tuning (transfer) performance is already exploited by ELMo. They use a learnable weight to control the relative importance of features from different layers. The key difference between Head2Toe and ELMo's approach is that Head2Toe selects a subset of intermediate features after learning the weights. However, I'm not sure this makes the novelty, or technical contribution, enough.\n2. The experimental results is not convincing enough. In the VTAB benchmark, in natural and structured category, the performance of Head2Toe is only comparable with linear prob but worse than fine-tuning. The overall improvement of Head2Toe is because it performs well on Specialized category and it has more tasks. It will be better if some analysis on why Head2Toe perform particularly on Specialized category but not on others.\n3. The claim in the title and abstract that Head2Toe improve OOD generalization are not well supported by the experimental results. The main experiments only show that Head2Toe improves transfer performance. However, the fine-tuned models' performance on OOD samples is not tested.",
            "summary_of_the_review": "The paper is generally well-written and the method makes sense. However, given that the novelty of the method and some concerns about experimental results, I think the paper, in its current form, is slightly below the bar. I believe this paper can be improved largely if theconcerns about experiments are resolved.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I think there is not any ethics concerns.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The main focus of this paper is the study the use of intermediate layers in a deep pretrained model on downstream tasks.\nAuthors argue that the the fact that the good performance while fine tuning on downstream tasks even if data scarce is due to the prior existence of useful representations deep in the model which are brought up during training.\nAuthors propose a new approach. Head2toe, which consist in utilizing intermediate representations, while freezing the weights of the network for training for downstream tasks\nExperimentation is carried out starting with a ResNet-50 and ViT-B/16 models on a variety of datasets collected on the VTAB collection, showing that the approach where Head2toe  outperforms linear finetunning (training a classification head on top of the model, while freezing the rest of the weights) and matches the performance of finetunning the whole model.",
            "main_review": "Overall, an interesting paper. The proposal is indeed interesting, although utilizing intermediate representations has been seen in the past.\nMy main concern regarding authors claim \"We conjecture that FINETUNING\nbetter leverages existing internal representations rather than discovering entirely new representations; FINETUNING exposes existing features buried deep in the net for use by the classifier\". Even if Head2Toe manages to reach comparable performance to finetunning, that doesn't proves that existing good representations where present in intermediate layers and where \"brought up\".  An analysis on the resulting weights of both methods might be a better way to prove that.\nIn addion, Table 1 shows a gap between the best performance reached by Head2toe and the best performance using finetunning. That result seems to imply that is more than a clever usage of existing representations what finetunning the whole model is doing.\n",
            "summary_of_the_review": "An interesting idea,\nNumbers hold\nClaims are not completely justified with experimentation.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "# Summary\n\nThis paper proposes a method for using intermediate representations from a pre-trained model for better transfer to other tasks. A linear classifier is trained on features from multiple layers. A feature selection strategy is chosen, where first a linear classifier is trained on all features with group-lasso regularization, and then another classifier is trained only on features whose regularization score exceeds some threshold. This approach is evaluated in a low-resource transfer scenario of image classification, and compared both to training on all features without the two-step feature selection but with regularization, and to the standard fine-tuning approach. The proposed method works slightly better than fine-tuning with a ResNet model, and slightly worse with a ViT model, although the comparison is not completely fair, as discussed. There are different performances on different categories of images. \n\n",
            "main_review": "\n# Evaluation\n\nThis is a well-written paper with a simple but clearly motivated idea. The preliminary experiments and discussion are helpful in motivating the approach. I am not familiar with the benchmark chosen for experimentation, but the experiments appear to be sound as far as I can assess. There is a comparison with several reasonable baselines. \n\n\n# Questions and comments \n\n1. While I'm not very familiar with the transfer learning literature in image classification, the idea seems common enough (as discussed in the related work) and pretty straightforward. This isn't necessarily a shortcoming, but I think it would make sense to discuss similar work in more detail, and perhaps draw more direct comparisons. Others have studied intermediate representations -- have they used the exact same approach? A different one? How would the results of the present approach compare to others that use intermediate representations? What exactly is the novelty? \n\n2. The name Probing is not so intuitive. I'm familiar with this term from the interpretability literature, especially in NLP, where a probing classifier is used on frozen features to assess their quality w.r.t some property. In the present context, however, the goals are different, and a more familiar term (at least to me) is to refer to frozen transfer, or feature-based transfer learning as opposed to fine-tuning. \n\n3. In figure 2, what is Scratch trained on? If it's a randomly-initialized ResNet-50 trained on low-resource datasets, it seems like a problematic case of over-parameterization. Would a simpler network perform better? the comparison with Linear is maybe problematic here. \n\n4. The Control reported in section 3.1 is helpful in assessing the extent of improvement that can be expected from adding intermediate representations. Oracle is than 1% greater than Control on average. \n\n5. The group Lasso regularization is applied once, and then a second final classifier is trained on the remaining features. I'm confused as to why this retraining is necessary, and why the group-lasso-regularized classifier can't be used as is. The experiments show that this second step works better, but why would it be so? Is it basically an optimization problem? \n\n6. In group Lasso, it's important to a-priori select the groups. In one case, features are grouped by layer, but I'm confused about the \"feature-wise strategy\". What are the groups there? And, would it make sense to consider other groupings? \n\n7. Related to the above, [1] performed feature selection with intermediate representations in an NLP context, and found different layers and different features inside a layer. They ahd a different kind of regularization, the ElasticNet. Even though their motivation is different (analyzing redundancy), they still get some pretty good results, so it would be interesting to compare the approaches. \n\n8. In table 1, I would have liked to know which differences are statistically significant and which aren't. The text mentions standard deviations as appearing in Appendix 1, but I couldn't find them. \n\n9. Could you discuss the different performances on the three groups (natural, structured specialized), of Head2Toe vs fine-tuning, in ResNet and ViT cases? Do the differences make sense in light of the nature of these categories? \n\n10. From an interpretability standpoint, I would have liked to see more discussion of which features are selected, in which layers, and how this relates to different tasks and what we might expect. \n\n[1] Dalvi et al., 2020. Analyzing Redundancy in Pretrained Transformer Models. \n",
            "summary_of_the_review": "# Evaluation\n\nThis is a well-written paper with a simple but clearly motivated idea. The preliminary experiments and discussion are helpful in motivating the approach. I am not familiar with the benchmark chosen for experimentation, but the experiments appear to be sound as far as I can assess. There is a comparison with several reasonable baselines. See the main review for more questions and comments. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}