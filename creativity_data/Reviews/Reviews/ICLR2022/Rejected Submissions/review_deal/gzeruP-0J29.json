{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper investigates fast adversarial training methods as a bilevel optimization problem. The proposed algorithm compares well with the existing techniques in overall runtime (obtaining better clean-test accuracy, which is not the goal, and) matching the robust accuracy of existing adversarial training methods. The proposed framework, however, is more general and flexible and is theoretically grounded. The problem studied here is exciting and the approach the authors take is interesting. \n\nThe current version, unfortunately, has some serious shortcomings. The empirical comparisons are a bit lacking — in general, the wall clock time is not a very good measure, it depends heavily on the implementation and various optimizations therein. A more suitable comparison would be in terms of floating-point operations, or in terms of iteration complexity. \n\nThe paper reports other interesting findings such as how the proposed method avoids robust overfitting. However, there is little theoretical evidence or insight for how the proposed method avoids it. \n\nThe writing can be improved with more emphasis on the novelty and significance of the contributions — some of the statements regarding improvements over prior work are somewhat misleading given the incremental gains (e.g., see Table 1). I believe the comments from the reviewers have already helped improve the quality of the paper. I encourage the authors to further incorporate the feedback and work towards a stronger submission."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work focuses on the problem of speeding up adversarial training in the $\\ell_\\inf$ threat model. The work first describes two previous works designed to speed up adversarial training:\n- Fast-AT: carefully perform single-step PGD (otherwise known as FGSM) to train $\\ell_\\inf$ robust models - from Wong, Rice and Kolter 2020\n- Fast-AT-GA: single-step PGD + a gradient alignment loss function - from Andriushchenko and Flammarion 2020.\n\nThe work then lists a problem with Fast-AT:\n- Fast-AT experiences catastrophic overfitting (i.e. provides no robustness on held-out data) for $\\epsilon = 16/255$ (Note: Fast-AT-GA does not have this catastrophic overfitting issue)\n\nFinally, the authors present a new method based on bi-level optimization to perform fast $\\ell_\\inf$-robust learning. They evaluate it and compare with Fast-AT-GA using a 20 epoch schedule, finding that for $\\epsilon = 16/255$, the technique can provide the same robustness except with standard accuracy ~68% instead of ~59%. ",
            "main_review": "Strengths: the work outperforms Fast-AT-GA at $\\epsilon=16/255$, obtaining better standard accuracy in less time while matching robust accuracy (standard accuracy ~68% instead of ~59%).\n\nWeaknesses:\nBaselines and experimental evaluation: \n- When comparing performance on a system that has evaluation metrics of both accuracy and training speed, the evaluation must take both into account. Currently it does not: all we see is a table with times and accuracies. One standard way of comparing training speed and accuracy is to measure either the \"maximum accuracy in a fixed amount of time\" or \"minimum time to attain a fixed accuracy\" while varying the metric of interest (time or accuracy respectively). Using this type of evaluation, a reader can see the speed/adv. accuracy frontier necessary to compare across algorithms. Currently the evaluation is not sufficiently detailed to compare across algorithms.\n- As a suggestion, one natural way of trading off time for accuracy is to treat the number of epochs as a free parameter; an illuminating here would be adv. performance vs training time as the number of epochs changes for each method.\n- The performance analysis is currently not informative; it uses a fixed training configuration for each training method (according to the Table 1 description, all the algorithms use early stopping yet according to Figure 1 this heavily disadvantages Fast-AT compared to no early stopping). One must choose the pareto (trading off speed and accuracies) optimal configurations for each training algorithm. The analysis must ablate across different training configurations to be fair.\n- The time does not have a standard deviation.\n- How is it that $\\epsilon=8/255$ and $\\epsilon=16/255$ trained models always have the same runtime, as reported in Table 1? Does early stopping always exit the same epoch for both?\n\nCharacterization of previous work: \n- What is the difference between problem (i) and problem (ii) on page 2? Robust performance is always properly evaluated against the strongest possible adversary.\n- The authors claim to solve an issue with Fast-AT stemming from its tendency towards catastrophic overfitting. The authors should give more background on when Fast-AT fails to catastrophic overfitting; how often does it happen? How does the $\\epsilon$ impact Fast-AT's convergence?",
            "summary_of_the_review": "While the work shows a promising approach (as measured by outperforming Fast-AT-GA at $\\epsilon=16/255$ in terms of standard accuracy while beating it in speed and in robust accuracy), the evaluation is lacking. In particular, the evaluation does not give a speed/accuracy tradeoff for each training routine, and does not properly hyperparameter search for each compared algorithm in the comparison. Finally, the work has some minor issues in its characterization of previous work.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies adversarial training as a bi-level optimization problem. The authors show that Fast Adversarial Training can be viewed as a bi-level optimization problem where the lower-level problem is a linearization of the loss in the direction of the sign of the gradient. Motivated by this observation, the authors propose Fast Bi-level Adversarial Training, where the lower-level optimization linearizes the loss in the direction of the gradient (as opposed to the sign of the gradient). The authors then show analytically how to obtain the gradient of the upper-level problem under the assumption that the loss Hessian is equal to zero. This gradient is then used in an iterative manner similar to standard adversarial training. The proposed method is evaluated on CIFAR10 and ImageNet against multiple baselines including Fast-AT, Fast-AT-GA, and PGD-2. The empirical results show that compared to the baselines, within the same order of computational cost, the proposed method enjoys improved stability and mitigates the catastrophic overfitting present in other baselines.",
            "main_review": "Relevance: the paper focuses on the important problem of designing computationally attractive algorithms for adversarial training -- this is an important problem that is relevant to ICLR.\n\nResults: The main strength of the paper is the empirical study. Given that within the same computation budget as the baselines, the proposed method has lower variance and resolves catastrophic overfitting in several settings, I found the results important. The results are only averaged over 5 random experiments, therefore, given the complexity and the size of the task, I’m not sure how reliable the estimates of mean/variance are. I think the empirical results can be much more compelling if the authors include more experiments.\n\nPresentation: Although the paper is well-written for the most part, there is much room for improvement in sections 3 and 4. In particular, some claims are not well-justified, there are some issues with the notation and the presentation lacks mathematical rigor. Here are some examples:\n\n            - in page 3, the authors state that “FAST-AT-GA yields improved robustness but has a poor accuracy-robustness tradeoff (e.g. Table 1)“. I do not see how FAST-AT-GA has a poor accuracy-robustness tradeoff in Table 1. Can you elaborate?\n\n            - the description of set C = {delta | || delta ||_infty < epsilon,  delta in [0, 1]} is very confusing. What does delta in [0, 1] mean? attacks are always point-wise positive? How is it not vacuous since epsilon < 1?\n\n            - in page 4, the authors say “Even if we set ell_atk = - ell_tr, problem 2 does not reduce to 1 due to the presence of lower-level constraint”. Regardless of whether this claim is true or not, the justification is far from satisfactory. In order to argue two optimization “problem” are equivalent, or if one reduces to another, one needs to prove/disprove that solutions to one constitute solutions to the other. The fact that gradients are different — as argued in the paper — does not a-priori inform us of anything useful about the equivalence of the two. In particular, when ell_atk = - ell_tr, then delta* in (2) is very well a maximizer of the inner attack problem in (1). Again, whether two optimization problems are equivalent has nothing to do with certain optimization procedures (such as PGD being considered here) generating different trajectories.\n\n            - the bi-level interpretation of Fast-At is trivial — this is how gradient updates are motivated in the literature of first-order optimization.\n\n            - in page 5, the derivation of the implicit gradient in (6) seems reasonable but lacks mathematical rigor. I suggest authors include careful proof here.\n\n            - The Hessian-free assumption in Theorem 1 essentially means that the attack loss ell_atk is linear. I do believe that this is not true for most interesting cases, and I do not buy the argument “the rationale behind the Hessian-free assumption is that neural networks commonly lead to a piece-wise linear decision boundary”. Are you suggesting that the loss landscape in deep learning is almost linear?\n\n            - in page 6, remark 1, the authors state that “Clearly, if alpha_2 = 0, then it reduces to the standard Fast-AT”. How is this true? here the updates will be based on the gradient of the loss whereas in standard Fast-At the updates are based on the sign of the gradient.\n",
            "summary_of_the_review": "To summarize:\nStrengths:\n          - the paper studies an important and relevant problem.\n          - the experiments are well done. \n          - the proposed adversarial training approach has nice theoretical motivations.\n          - the paper is well-written for the most part.\n\nWeakness:\n          - several ideas and claims are handwavy and lack mathematical rigor \n          - the assumption behind the main theoretical result is too stringent\n          - statistical significance --> only 5 random experiments",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a bi-level optimization problem for training models robust to norm constrained adversarial attacks that can be optimized via argmin/implicit differentiation by linearizing the inner problem (the search for the worst perturbation). The new technique is then compared empirically to other existing approximations of the robust training problem (referred to in the paper as Adversarial Training) and shown to provide improved or comparable results in terms of standard and robust accuracy, for different attacks.",
            "main_review": "Rewriting (robust) AT as a bi-level optimization problem is a nice formalization that allows the use of implicit differentiation for training robust models. This contribution seems however straightforward and the deployed techniques for optimizing the problem (linearized proximal form, differentiable projection) are standard in the literature.\nThe other advantage of rewriting the problem in this way is in making a distinction between training loss $\\ell_{tr}$ and attack loss $\\ell_{atk}$, but this does not seem to be explored in the experiments.\n\nThe paper presents some inaccuracies when reporting the related work and in the derivations:\n\nContrary to what stated in the paper, Adversarial Training was proposed before [Madry 2018] in the seminal work of [1]. In [Madry 2018] adversarial training was formulated as a Robust Optimization problem. The attribution should be rectified.\n\nTo write Problems (5) and (7) it is never explained how the term $\\frac{\\lambda}{2}$ squared norm 2 of $\\delta - z$ appears. It does not come directly from the linearization otherwise this term would be multiplied by the second order derivate of $\\ell_{atk}$ and there wouldn't be any $\\lambda$ hyper-parameter. Instead it seems that the $\\lambda$ proximal operator is applied, but this is never mentioned.\nThe assumptions on the loss functions needed in order to linearize the BLO problem are not reported.\n\nIn order to compute the implicit gradient of FAST-AT, $\\mathcal{P}_{\\mathcal{C}}$ does not need to be differentiable as the derivative of the sign function is 0 null on the dimensions that need an update. The statement \"the projection function is not smooth, and thus the use of chain rule is not legitimate\" is somehow incorrect, as this projection is still differentiable as it is shown later in page 6, it just gives a piece-wise derivative depending on the box constraints.\n\nIn Theorem 1, it is not explained why the so-called Hessian-free assumption is needed. Moreover, its naming seems inaccurate as not all the components of the Hessian should be null (in particular those w.r.t $\\theta \\delta$).\n\nThe following notions essential for understanding the paper are not defined: robustness catastrophic overfitting, random corner linearization, gradient alignment.\n\nThe methods used as baselines in the experiments are not sufficiently described, namely FAST-AT-GA and PGD-2-AT. Also AutoAttacks should be described and the reason why this type of attack is chosen to measure robust accuracy should be provided. In the tables, the standard deviations should be used to determine which score differences are significant. At the moment, it seems that the result marked in bold is the one with the best average score, but a significance test should be used instead. \nWhy does PGD-2-AT generally achieve better results than FAST-AT and FAST-AT-GA and comparable with FAST-BAT? \n\n(Minor) The following reference is defined twice: Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018b.\n\n[1] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks, ICLR 2014",
            "summary_of_the_review": "I'd tend to reject the paper because of the highlighted inaccuracies and unclear points, and because the contributions seem straightforward applications of BLO theory to adversarial training. Additionally, the new linearization does not seem to significantly improve state-of-the-art results.\n\n## UPDATE\nMy final evaluation is based on the current version of the paper, as the authors had the chance to update it.\nI increased my score to 5, as the inaccuracies have been corrected, namely:\n- the attribution of adversarial training has been rectified\n- the additional $\\lambda$ norm_2 term in the linearized inner problems has been explained\n- the phrasing for the Hessian-free assumption and projection's derivative has been corrected\n \nI still tend to reject the paper for the following reasons:\n- definition of gradient alignment and autoattacks is not provided\n- justification for using autoattacks is not provided. \"everybody is doing it\" is not a sufficient justification per se\n- comparison wrt baselines is based on mean values, hence the reader cannot judge the significance of the improvements. Doubts about the significance of the empirical results has been raised by other reviewers as well.\n\nFinally, I still have some doubts about the contributions of the paper to the BLO literature. In the rebuttal the authors claim \"(1) we show that the choice of lower-level linearization (gradient sign-based vs. non-sign case) could be a key to simplifying BLO with lower-level constraints, and (2) we derive the closed-form of the implicit gradient of lower-level constrained BLO assisted by lower-level linearization.\"\nHoewever the two linearization schemes are not novel, neither the derivative of the projection onto a convex set.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper aims to interpret the fast adversarial training methods from the perspective bi-level optimization. Though the discovery is straightforward, it is worthy of letting the community know this important connection. And then the authors proposed a new linearization of the lower-level optimization problem and introduced a new FAST-BAT approach for improving both accuracy and robustness. Various experiments were conducted to verify the effectiveness of the proposed method. ",
            "main_review": "In general, I like this perspective due to the realizing of the implicit gradient term since actually the adversarial perturbation depends on the network weights, and this is also related to some versions of GANs’ training. However, in the current version of the paper, there exists several key points that requires careful justification to make it as a much stronger piece of work. I list them in the below. If these questions could be handled properly, I will increase my score. \n\n1.\tIn the Remark 1, it seems an very important conclusion the authors want to claim on the role of the second term associated with alpha2. Then what is evidence for the claim that “the choice of alpha2 could affect the tradeoff between accuracy and robustness…alpha2-associated term plays a positive role in boosting robustness, especially for training with large epsilon”? \n2.\tWhy does FAST-BAT produce better gradient alignment? Is it just an empirical observation or is there any good theoretical justification?\n3.\tIs there any connection between FAST-BAT and standard PGD adversarial training? \n\nOther questions: \nAbout the Hessian-free assumption. What about the non-ReLU activation function for which the function is not piece-wise linear wrt to input? \n",
            "summary_of_the_review": "Good perspective for interpreting fast adversarial training, but in the current version, there exists several key points that requires careful justification to make it as a much stronger piece of work.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}