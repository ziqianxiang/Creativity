{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes an FL framework that optimizes the performance of a subset of clients. Reviewers did appreciate the value of several contributions, but unfortunately consensus is that it remains below the bar, even after the discussion phase. Concerns remained on correctness issues, motivation of assumptions, and distinction of personalized vs selfish FL, even after the author feedback.\n\nWe hope the detailed feedback helps to strengthen the paper for a future occasion."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposed a new client reweighting algorithm for selfish federated learning where the goal is to optimize the performance of a few internal clients not all clients.",
            "main_review": "For strengths: this paper studied a new selfish federated learning setting (i.e., the objective is the loss on a small (internal) subset clients instead of on all clients) and propose a VaRSeL algorithm for solving them.\n\nFor weaknesses: \n- Regarding the selfish-FL model in Table 1. Here are some concerns: 1) the datasets are shared with $M$ clients, not stored on their own clients;  2) **if the objective is only to optimize the loss of $M$ internal clients, why all other $N$ clients will participate? what is the incentive/benefit for them?** Note that in Section 3 (Problem formulation), the authors even assume that **$M\\ll N$**. \n \n- Regarding the proposed algorithm VaRSeL, in each communication round, they need to communicate with all clients (see Line 3, 4, 5 of Algorithm 1) to compute the reweight parameter ${w}$, which is impractical compared with classical federated learning algorithms such as FedAvg (which only communicates with a subset of clients for each round).\n\n- Regarding the Theorem 2, the step size $\\eta \\leq \\frac{\\frac{1}{L}-\\frac{A}{\\mu}}{2\\sqrt{A}}$ is likely unachievable, i.e., $\\frac{A}{\\mu} \\geq \\frac{1}{L}$, since the strongly convex parameter $\\mu$ usually is much smaller than the smooth parameter $L$, and $A$ is related to the bounded gradient/similarity and also can be very large.\n\n- Similarly, regarding the Theorem 3, the step size $\\eta \\leq \\frac{2(1-\\sqrt{A})}{L(A + \\sqrt{A})}$ is also likely unachievable, i.e., $A$ can be larger than $1$.\n\n- In the proof of Theorem 3, i.e., the bound for $T_3^{(t+\\frac{k}{\\tau+1})}$ (on Page 20), it uses Lemma 1 for the last inequality. However, Lemma 1 only holds for $w_*$. Thus the last inequality of bounding $T_3^{(t+\\frac{k}{\\tau+1})}$ requires the $w_*$ for $\\overline{\\theta^{t+\\frac{k}{\\tau+1}}}_{w}$ \n\nof Eq (4), but $w_*$ is not available for $\\overline{\\theta^{t+\\frac{k}{\\tau+1}}}_{w}$ at that time. Also the weighted average of $\\theta_i$ is not equivalent to weighted average of $\\nabla l_i(\\theta)$.\n\n- Regarding the experiments, for Table 2, I did not see the results for local steps $\\tau =1$ while the authors wrote the local steps are 1, 3, 5. Also, how do you choose/tune the learning rates?\n\n\n\nSome typos:\n- Page 14, in Eq (10), $\\sum_{j=1}^N \\mathbb{E}||\\nabla l_j(\\theta)-\\nabla l(\\theta)||^2$ should be $\\sum_{j=M+1}^{M+N} \\mathbb{E}||\\nabla l_j(\\theta)-\\nabla l(\\theta)||^2$.\n- Similarly, for Page 15, these summations should be $\\sum_{j=M+1}^{M+N}$.\n- Page 20, the first inequality is wrong, i.e., both terms within $ \\mathbb{E}||\\cdot ||^2$ in the second term are the same. It should be changed to $\\nabla l_i(\\theta_i^{t+\\frac{k}{\\tau+1}})$.",
            "summary_of_the_review": "The model is not very appropriate (see the comments above). Besides, there exist some issues in their proofs, and also the conditions for their theorems may not hold. Thus I suggest a reject.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work studies a variant of the federated learning (FL) problem where the stakeholder (i.e. central planner) only cares about the performance of a trained model over a small proportion of local (internal) clients. The work proposes a first-order federated learning algorithm that weights external clients to obtain a variance-reduced gradient estimate of the ultimate objective (over internal clients), and presents convergence analysis for their algorithm under certain settings. Finally, the work evaluates their proposed algorithm by comparing against existing FL algorithms using synthetic and real datasets. \n",
            "main_review": "This is a well-written paper, and in particular I think the paper does a good job in the following aspects: 1. The paper positions its proposed methodology well with existing works, and highlights in a rather concise way some key differences in motivation and methodology; 2. The paper presents many interpretations for/insights into relevant results and claims, which makes the proposed methodology rather interpretable from both a theory and practical perspective;3. The paper includes a wide range of experiments that demonstrate the practicality of applying the proposed algorithm in various settings.\n\nThe main weakness, from my opinion, is that the methodological and technical contributions seem insignificant. In particular, the convergence analysis for the reweighting strategy relies heavily on the second half of Assumption 1, i.e. the difference between the gradient for external clients and the gradient of the ultimate objective can be bounded by the gradient norm of the ultimate objective. In other words, this basically means the gradient estimates from external servers are rather sufficiently accurate to serve as estimates for the  ultimate objective’s gradient. This seems like a rather stylized assumption that says external servers essentially give us more (somewhat accurate) estimates of the gradient of the ultimate objective, which in turn reduces overall variance of the ultimate objective’s gradient estimate. The proof techniques in the paper for convergence results are rather standard and, to the best of my knowledge, have been introduced in previous works. Also, it is stated at the beginning of Section 3 that the distributions over the data in external clients are very different from those of internal clients. Intuitively, this seems to be against (if not contradicting) the second half of Assumption 1, because why would the gradients be rather close while the distributions are very different? I might be missing something here and it would be nice if the authors can clarify.\n\nIn addition to the above, I feel the paper would be much stronger if among the external clients, there exists \"bad clients\" whose gradient estimates are very different from that of the ultimate objective. In practice, these bad clients may represent, for example, malfunctions of hardware, or adversarial attacks from strategic clients who are indeed \"selfish\" with the aim to optimize their own local objectives. In this more complex setting, it would be interesting to see if the weighting strategy would eventually underweight the bad clients. That being said, even in the current setting of the paper, it would be nice if the paper can provide insights into the eventual weights for external clients; e.g. do the weights represent some notion of external client importance? Which external clients have more weights and why? What key characteristic of an external client would impact its weight? I believe that such insights would be beneficial to the overall message of the paper.\n\nOther comments:\nI am curious why the paper does not present a theoretical convergence analysis for the VaRSeL algorithm (i.e. solving for the approximate weight $\\tilde{w}$ in Eq.(8) instead of the actual optimal weight $w_*$)? Intuitively under Assumption 1, the individual gradient differences are bounded by the norm of the ultimate objective’s gradient, so showing convergence for VaRSeL should not be much more difficult than the reweighting strategy. Is there something that I am missing here that prohibits us to obtain a convergence bound for VaRSeL? If not, it would be nice if the authors can also discuss convergence of VaRSeL, since this is the main algorithm of interest.\n",
            "summary_of_the_review": "The paper is well-written, explains ideas in a concise manner, and includes a wide range of experimentations.\nThe methodological contribution may be a bit underwhelming, due to the restrictive assumption that external clients’ gradients are bounded around the ultimate objective’s gradient. The paper would be much stronger if it can deal with \"bad\" external clients, and also provide interpretations for/ insights into the weights of external clients.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes selfish federated learning, an FL framework that optimizes the performance of a small subset of clients. The authors further propose a new selfish-FL algorithm, termed VaRSeL, based on minimizing the variance between the surrogate gradient $\\nabla l (\\theta; w)$ and the ultimate gradient $\\nabla l (\\theta)$. Under certain conditions, the authors prove the convergence of an idealized variant of VaRSeL. The performance of VaRSeL is demonstrated in a range of simulations and real-data experiments.",
            "main_review": "Strengths: The idea of minimizing the variance is interesting and the performance looks promising.\n\nWeaknesses and questions: \n- The idea of only optimizing the performance of a subset of clients is not really new. If we let $M=1$ (i.e., there is only one internal client), then this is precisely the formulation in [1, 2, 3]. If the authors assume that $D_1, ..., D_M$ are all approximately equal to some $D_I$ (the display before Eq. 2), then there is only a marginal difference between $M \\neq 1$ and $M = 1$.\n- Eq. (2) looks a bit confusing for me. What is the meaning of $(x, y) \\sim D_1, ..., D_M$? Are you taking the expectation over a mixture of $D_1, ..., D_M$?\n- Please correct me if I am wrong, but isn't optimizing either $\\Phi$ or $\\tilde \\Phi$ a very hard problem, as the gradient involves the second derivative of $l_j$? \n\n[1] Cheng, Gary, Karan Chadha, and John Duchi. \"Fine-tuning is fine in federated learning.\" arXiv preprint arXiv:2108.07313 (2021).\n\n[2] Chen, Shuxiao, et al. \"A Theorem of the Alternative for Personalized Federated Learning.\" arXiv preprint arXiv:2103.01901 (2021).\n\n[3] Hanzely, Filip, and Peter Richtárik. \"Federated learning of a mixture of global and local models.\" arXiv preprint arXiv:2002.05516 (2020).",
            "summary_of_the_review": "The proposed algorithm is interesting and its performance looks promising. However, considering the fact that the proposed framework bears similarities with earlier works on personalized FL, my score is 5: marginally below the acceptance threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new setting for federated learning -- Selfish Federated Learning -- where all clients (internal and external) participate in the training, while the trained model needs to give low error only for the internal clients. The main idea is to reweight the loss functions and give small weights to external clients so that the overall objective is closer to the true risk of internal clients. The paper provides some theory for methods where the optimal weights are known and proposes a heuristic method VaRSeL to estimate the optimal weights on the fly.",
            "main_review": "**Motivation.** The motivation of the proposed new setting *Selfish Federated Learning* is not well substantiated. There is only one short paragraph on two examples, which are not quite convincing. For instance, what are the incentives of external clients to participate in the training (provide data in the form of gradients, spend resources on computation and communication) if they are not going to benefit from the trained model? To be clear, I am not criticizing the setting and claiming that is unfruitful. Quite the opposite, I suggest devoting more space, building intuition and motivation behind this setting at the very beginning. As an example, I could imagine some online services, which allow training (to some limited extent) on their data without viewing raw data.\n\n**Internal clients' distribution.** On page 5, it is written that \"Since the internal gradients are unbised estimators of the ultimate gradients, we have\"\n$$\\mathbb{E}[\\nabla l_i(\\theta)] = \\mathbb{E}[\\nabla l_{I}(\\theta)] = \\nabla l(\\theta), \\qquad 1\\le i \\le M.$$\n\nFirst, I presume that the expectation here is with respect to local subsampling $S_i \\sim \\mathcal{D}_i$, which is not clarified in the text. Second, to have those equalities, it is necessary to have all internal clients' distributions $\\mathcal{D}_1 = \\mathcal{D}_2 = \\dots = \\mathcal{D}_M$ ***the same***.\n\n\n**Ultimate objective.** What does $\\mathbb{E}_{(x,y)\\sim \\mathcal{D}_1, \\dots, \\mathcal{D}_M}$ mean in (2), the definition of $l(\\theta)$ ? The wording up to (2) suggests that\n\n$ l(\\theta) = \\frac{1}{M}\\sum_{i=1}^M \\mathbb{E}_{(x,y)\\sim \\mathcal{D}_i} \\mathcal{L} (f(x,\\theta),y).$\n\n**Claims before section 4.** *\" we will show both in theory and in experiments that it is probable to improve the estimation of $\\theta$ by leveraging additional knowledge from external clients without privacy leakage.\"*\n\nWhich part of the theory supports this claim? Do you have privacy guarantees to prevent privacy leakage?\n\n**Strategy (section 4.1)** *\"make our surrogate objective (3) simulate the ultimate objective (2).\"*\n\nObjective (3) includes empirical loss functions of external clients, whose distributions are expected to be very different over the internal clients'. On the other hand, objective (2) consists of true risks (not empirical losses as in (3)) of only internal clients. So, you assume that the external clients' distributions are largely different but somehow want to use them to improve the objective composed of only internal clients' data. I failed to see the intuition behind this.\n\n\n**Proof of Lemma 1**. How do you justify the inequality in (9), namely $\\sum_{i=1}^M \\mathbb{E}\\\\|\\nabla l_i(\\theta) - \\nabla l(\\theta)\\\\|^2 \\le M^2\\sigma\\\\|\\nabla l(\\theta)\\\\|^2$ ?\n\nWhat if $\\lambda_1=0$ ? Then the function $h(\\omega)$ is monotonically decreasing and $\\omega_* = +\\infty$. Does this mean that internal clients should be ignored in those cases ?\n\n\n**Theorem 2.** I presume that the method analyzed in Theorem 2 is of theoretical interest only since it uses the optimal weights $w_*^t$ in each iteration, which depends on $\\nabla l(\\theta)$ (full gradient of true risk). Even so, the rate (5) is very weak. First, the upper bound on $\\eta$ implies $A<\\frac{\\mu}{L}$, which is quite restrictive. Second, the noise term has additive part $\\frac{B}{\\mu^2}$, which is fixed and independent of how we choose step size $\\eta$ or sample $S_i\\sim\\mathcal{D}_i$.\n\n**Theorem 3 (Main convergence).** No discussion of what the main convergence result means and implies. It is claimed that $\\Gamma_1, \\Gamma_2, \\Gamma_3$ are constant, but in the proof, they depend on both $\\eta$ and $\\tau$.\n\n\n**VaRSeL method.** I think, the terminology \"variance reduction\" is not appropriate here. The method is a heuristic, which approximates several steps discussed in the paper (e.g. $\\nabla l(\\theta)$ is approximated by $\\nabla l_I(\\theta)$).\n\n\n\n\n\n",
            "summary_of_the_review": "The proposed setting is not well motivated. There are several issues and inconsistencies in theoretical arguments.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}