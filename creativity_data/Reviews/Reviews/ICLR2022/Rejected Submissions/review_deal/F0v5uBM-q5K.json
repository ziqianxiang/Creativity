{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper argues that the existing approaches for reducing power consumption do not model the precise power usage of each model. To remedy this an approximate power usage model is proposed using bit flips and a simple approach called PANN is introduced that relies on tricks such as unsigned arithmetics and implementation of multiplications with addition. The reviewers have found the overall direction of this paper in modeling power consumption important and have acknowledged the clarity of presentation. However, they have also raised serious concerns regarding (i) the efficacy of modeling power consumption with bit flips and ignoring memory power, (ii) its relevance to modern hardware, and (ii) the efficacy of replacing multipliers with repeated additions. Unfortunately, the paper in it is current form does not provide a compelling answer to these concerns. Given these criticisms, we don't believe that the paper is ready for publication at ICLR."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper argues that power consumption is a major obstacle in deploying DNNs to end devices and that current quantization approaches do not take power consumption directly into account and therefore are not optimal in reducing it. Using an approximate power model based on the average number of bit flips, they make two observation that are frequently overlooked by existing quantization approaches: \n1) A significant portion of the power consumption of the MAC operation is due to the usage of signed integers and using unsigned integers instead can significantly reduce the power consumption.\n2) The multipliers power consumption is dominated by the larger bit widths (weight or activation) and therefore using lower bit for one of the two (e.g. weights) is not power efficient.\n\nBased on this the authors introduce a new weight quantization approach (PANN) which removes the multiply operation and replaces it with additions. This allows PANN to efficiently reduces the power consumption. For the same power budget, PANN achieves significantly higher accuracy (or effective bit width), but comes at the cost of higher latency and memory usage.\n",
            "main_review": "Strong points:\n* The proposed approach (PANN) is original and unlike any previous work in the field of quantization.\n* The paper highlights unique new insights which are valuable for future hardware considerations.\n* The proposed approach shows strong results for a given power budget for which it clearly outperforms conventional uniform quantization (under their power model assumption).\n* The authors are somewhat upfront with some limitations and added the memory and latency overhead to table 2 and discussed them.\n* Overall the paper is well and clearly written.\n\nWeak points:\n* The paper only considers the power consumption of the MAC array. However, the paper ignores the power consumption (and latency) required for the memory movement, which is non neglectable. The proposed approaches, both the switching to unsigned tensors (eq 5 and 6) and the switching to additions (eq 11), can further increase the memory transfer costs making this an even more important part to consider for the power model (and latency considerations).\n* It is unclear whether the provided power simulations generalize to MAC operations in neural networks, and specifically of the proposed PANN approach. The simulation uses uniformly distributed random inputs (see appendix A.1), however in neural networks weights are roughly a normal distributed and similar activations (or a clipped normal distribution in the case of ReLU activations). It is unclear whether the proposed power model would also be a good approximation for such a different expected input distribution. With the proposed splitting of the positive and negative parts of the weight tensor (eq 5 and 6), the weight distribution will be even more different from the assumed uniform distribution. It is important to validate that the power model is valid under the expected distributions as most claims of the paper are based on observations from this power model.\n* While the proposed approach shows good accuracy improvement for a power budget, it also has significant limitations. Namely it leads to memory overhead, latency overhead and requires dedicated hardware implementations to be utilized. For example, the 3 bit power budget model, corresponds roughly in memory and latency to a 6 bit uniform quantization model, and in this case the accuracy of the uniform model is significantly higher. Thus there is a clear trade-off between power and memory/latency. \n* The assumption about uniform distributed weights and activations in section 5.3 is fairly unrealistic. Would the outcome (significantly) change under the assumption of a normal distribution?\n\n\nNotes:\n* Observation 1: I agree that the accumulator can have significant impact on the power usage of the MAC array. Though the required bit width for the accumulator scales with the bit width of the input tensors. The common use of 32 bit accumulator is often based on 8 bit quantization (see Rodriguez et al.), thus the example provided might overemphasis this effect.\n* It is quite common to use asymmetric quantization (unsigned int + zero offset, see Jacob et al. 2018), thus unsigned integers are already frequently used. What is the advantage of the proposed splitting approach compared to standard asymmetric uniform quantization?\n* Sometimes bits and power budget can be confused, especially by a less careful reader (e.g. table 2, 3). It would improve clarity if the tables and text clearly use power and only state comparable bit width where needed in brackets (now it is frequently the other way around, which on a first look seems like N bit quantization, but does not highlight the power aspect).\n\nQuestions:\n* In figure 4a) (Appendix, validation of power model), the results are scaled independently for multiplication and additions. On the other side, in the paper we trade off multiplications with additions. How is this scaling exactly down and can this scaling actually have an influence on the trade-off made between multiplications and additions?\n* How does PANN work during QAT? It is a bit unclear from the paper (e.g. how is the quantization simulated, is the budget set before or after training, how are ranges determined, etc)\n",
            "summary_of_the_review": "The authors propose a novel new approach to reduce directly the power consumption for inference. The originality and good results are the main strength of the paper while the weak points are mostly centered around some assumption and justifications of the power model and the limitations of the proposed approach. Overall both side are fairly in balance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors observed that the power consumption is dominated by the bit toggling at the input of the accumulator and decreasing the bit-width of only the weights or only the activations has limited benefit to reduce the the power consumed by the multiplier. The paper proposed PANN, which uses tricks such as unsigned arithmetic in CNN and implement multiplications via additions to achieve a multiplier-free and power-aware neural network. Experiments on both post-training quantization and quantization-aware training showcased better performance compared to other works under the same number of bit-flips. However, the reviewer believes there are fundamental flaws that need to be thoroughly addressed before this paper is ready for ICLR.\n",
            "main_review": "\n### Strengths\n* Power-aware DNNs are interesting and important that merits exploration.\n* The mean squared quantization error of a regular uniform quantizer and the proposed PANN was theoretically analyzed given first-order approximation, and they are further compared under the same fixed power budget.\n* Low-bit networks in post-training quantization with PANN exhibits only a minor degradation in accuracy compared to floating-point counterparts.\n\n### Weaknesses\n* Although bit-flips can be considered an important source of power consumption, the proposed approach of counting bit-flips and minimizing such counts per image inference is fundamentally flawed. The design of the multiply-accumulate provides an incomplete and oversimplified description of the actual hardware realization of a DNN. For instance, the summation of $N$ values can be implemented as an adder tree instead of working on a single accumulator, and would result in a reduction of the number of additions to approximately $\\log(N)$ and thus reducing the number of bit-flips. In addition, how the circuit should parallelize and/or time-share computations would have great impact on the number of bit-flips. Considering only a long multiply-accumulate chain for each layer is unrealistic and not faithful to the actual hardware designs.\n* The proposed method in Section 5 to replace multipliers with repeated additions by accumulation is equal or inferior to multiplication in terms of bit-flips. Intuitively, counting bit operations, repeated addition consumes $min(O(2^N M), O(2^M N))$ for an $(N,M)$-bit multiplication, whereas multiplication would use $O(MN)$.\n* Some important experimental settings (bit-level details, training epochs, optimizer, etc.) are missing. It would be better to provide such details in appendix to for reproducibility as code is not available in this submission.\n\n### Minor\n* “Our approach can can work in combination with any activation quantization method.” Duplicate “can”.",
            "summary_of_the_review": "Although power-aware DNNs are interesting and this topic is an important research direction, this paper has some fundamental flaws that prevented the reviewer from recommending acceptance.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes to remove signed arithmetics and removing multiplication. The power model is limited to dynamic power, and just the MAC power.",
            "main_review": "Weakness:\n\n* The authors rely on a 2000 publication to support the argument that dynamic power dominates. That's hardly definitive today, especially the power is simulated in the 5nm regime. Stronger support and/or better methodology is needed.\n* The authors didn't consider the memory energy, which usually dominates the total energy, as opposed to MAC energy. In general, the experimental methodology is too simplistic, essentially a python script counting bit flips.There are just way too many other things that affect the energy. At least try some CACTI models; better yet, use a memory compiler.",
            "summary_of_the_review": "The idea is of intellectual merit, but the evaluation ignores too much important aspect of real energy/power modeling. The results are far from convincing in that regard.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a method for reducing the power consumption of deep neural networks called a power-aware neural network (PANN).  The proposed method exploits the entire power consumption of the model to determine the quantization bit widths, so it could achieve higher accuracy than some previously developed methods when the power budget is set to ultra-low bits such as 2-bit or 3-bit.",
            "main_review": "** Strength **\n1. The idea that quantized the model using the power budget is very important to real-world applications.\n 2. The paper achieved superior accuracy than the previously developed quantized methods.\n** Weakness ** \n1. The experimental application is too narrow. Since the authors mentioned that the proposed method can be applied to \"any\" model, a wide variety of applications (BERT for QnA task, etc..) should be included. \n2. If each layer has a different quantization bit-width, the hardware implementation cost could be increased when compared to the fixed bit-width-based hardware.\n\n** Minor Comments **\n- Please fix me if I am misunderstood. The proposed method determines the quantization bit-width using the power budget of the entire network, so each layer or neuron has a different bit-width. If my understanding is right, mentioning this explicitly might help to understand the paper.\n- The phrase \"to quantize the weights and activations at post-training\" in the caption of Figure 1 is a bit confusing. In the current explanation, the readers can be confused why removing the multiplier increases accuracy.\n- In the case of batchnorm, the authors mentioned that \"Batch-norm layers should first be absorbed into the weights and biases\". How can it be handled when using the layernorm?\n- The state-of-the-art models employed activations with negative values. How to convert to unsigned integers when using gelu or elu?\n- The paper can be enhanced if the authors provide how many bits are allocated for each layer for 2 bits (power) in Table 2.\n- There is a typo on page 2, \"Our approach can work in combination\" -> \"Our approach can work in combination\"",
            "summary_of_the_review": "The idea of finding the optimal quantized model in terms of the power budget is an important issue for real-world applications. Although some part of the paper is ambiguous to me, I think this paper could be a bridge to reduce the gap between the model researchers and hardware engineers.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}