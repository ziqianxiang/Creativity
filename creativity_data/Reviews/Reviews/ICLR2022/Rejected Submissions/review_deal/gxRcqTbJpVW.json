{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes a pruning approach that regularizes the gram matrix of convolutional kernels to encourage kernel orthogonality among the important filters meanwhile driving the unimportant weights towards zero. While the reviewers found the proposed method well-motivated and intuitive, they believe that the proposed claims are of limited novelty and are not supported well by the experiments. Analyzing and explaining the effect of different parts of the proposed method, i.e., orthogonalization and regularization of batch normalization parameters, on the accuracy of the pruned models would significantly improve the manuscript."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Dynamic isometry is shown to be a useful property that enable effective gradient propagation through the forward/backward. However, pruning will largely damage such a structure. This paper studies how to maintain the “dynamic isometry” property during pruning. Specifically, after getting an initial assessment of filter importance, the algorithm will maintain the partially kernel orthogonality of the important filters. They also propose to regularize the BN parameters to future boost the performance.",
            "main_review": "Pro:\nThe authors propose a nice way to maintain dynamic isometry property during pruning: just doing a back-propagation considering the orthogonal constraint. The method is quite intuitive and simple, which I think it’s good.\n\nCons:\nI think the imagenet performance is not quite better than previous methods. The improvement is in general 0.1 points except ResNet50 (3.06x) where the improvement is 0.5 (still not significant). If the authors want to claim their method has larger advantage in larger pruning ratio, there should be more experiments to support such a claim.\n\nThe authors argue to use “difference between performance under different learning rate” as a metric to measure whether dynamic isometry is preserved, which in my opinion is not a good measure. Can the author measure the jacobian of each layer?\n\nQuestions:\n\nAbout the claim: “SGD training in finetuning can help recover it”, I’m confused what is the goal of recovery when looking at figure 2. The model seems not to converge to JSV 1, then what information this figure conveys?\n\nWhat is the L1+KernOrth in Table 1 differs from the proposed method? Is the difference just whether regularize BN parameter? Moreover, there is no explanation of what is the difference between L1 + X v.s. X + L1, which is confusing.\n\nWhy this paper only use kernel orthogonality instead of convolutional orthogonality?\n\nComments:\n\nI think the content would be much easier to read if you can create a 2D plot with the x-axis being the speedup and the y-axis being the accuracy. In its current form, all I can see is the proposed method does not have a clear advantage over the previous method.\n\nRegarding maintaining approximately dynamic isometry, this paper is also related:\n[1] deep isometric learning for visual recognition. ICML 2020.",
            "summary_of_the_review": "I like the idea of maintaining dynamic isometry during pruning. But there is still great room to improve in both the presentation and analysis. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a dynamical isometry preserving approach for structured pruning by introducing an orthogonality preserving regularization for weight matrices in each layer. The experiments on cifar and imagenet datasets show improvements over comparable methods.",
            "main_review": "# Strengths\n\n1. Orthogonality preserving structured pruning is interesting.\n2. The results demonstrate the merits of the method.\n3. Overall the paper is well written.\n\n# Weaknesses\n\n1. The role of BN regularization is not clear to me. It is not clear how the BN parameters of pruned filters would make a difference in dynamical isometry (DI) and how the proposed regularization alleviates this issue? More clarity is required.\n\n2. It is not clear that the method indeed improves DI. Considering Fig.2 c, mean JSV increases and is higher than 1. So not clear the method indeed improves DI. \n\n3. The notation inside the Frobenius norm is not clear in Eq.3. Probably a type, please comment.",
            "summary_of_the_review": "Overall the paper is interesting. The connection to BN regularization and DI could be explained better to improve clarity.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a structured pruning method based on kernel orthogonality analysis. The method is straightforward and well-investigated in the network pruning field.",
            "main_review": "Pros:\n1. Clear motivation and writing.\n2. Well-established pipeline.\n\nCons:\n1. Comparisons with the state-of-the-art methods are insufficient, such as [1].\n2. Experiments on ImageNet are insufficient and results are not comparable.\n3. Moderate novelty. This kernel orthogonality idea has been employed in deep networks in previous methods.\n\n\n[1] ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting, Ding et al., ICCV 2021",
            "summary_of_the_review": "This paper has moderate novelty with the well-studied kernel orthogonality method. Besides, the experiment design is not sufficient. The performance gain in ImageNet is incremental and not sufficient compared to the state-of-the-art methods.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes regularizing learnable parameters of NNs to maintain the dynamical isometry during pruning and improve their accuracy. In the experimental analyses, the proposed OPP outperforms baseline methods on benchmark datasets.\n\nThe paper is well written in general, and the proposed methods are intuitive. In the experimental analyses, the proposed methods outperform baseline on various benchmark datasets.\n",
            "main_review": "The paper is well written in general, and the proposed methods are intuitive. In the experimental analyses, the proposed methods outperform baseline on various benchmark datasets.\n\nHowever, there are few major issues with the paper:\n\n- There are three main claims in the paper; achieving structured pruning, orthogonality and dynamic isometry. However, these claims are not explored in detail;\n\n-- Structured pruning is only mentioned as a synonym for filter pruning or channel pruning in the review of the related work. This is achieved in the proposed method by user defined parameters that identify the indices of parameters to be pruned. In the analyses, selection of this set of indices affects the accuracy as well. That is, there is not a major contribution for structured pruning.\n\n-- Orthogonality is aimed to be achieved by regularization on gram matrices of kernels. However, how well/much orthogonalization is obtained is not analyzed. In addition, regularization of batch normalization parameters has much more effect on the accuracy compared to orthogonalization. \n\n-- The paper claims to achieve dynamic isometry by orht. However, it is also stated “We also propose to regularize batch-normalization parameters for better preserving dynamical isometry for the whole network.” That is, either orth. does not enable dynamic isometry by itself, or dynamic isometry does not affect the accuracy remarkably. This claim should be examined theoretically or experimentally in detail.\n\nTo summarize, the relationship between these three proposed major claims should be analyzed in detail. Otherwise, the scope and claims of the paper should be updated accordingly.\n\nAdditional questions:\n\n-\tHow did you calculate speedup? Could you please provide an example of actual running times for training and test for an experiments, such for the VGG19 + Cifar100 case, in comparison with the baseline?\n\n-\tHave you employed regularization of parameters of BN of baseline methods to check how this method also affects accuracy of baseline?\n",
            "summary_of_the_review": "The paper is well written in general, and the proposed methods are intuitive. However, the proposed claims are not explored well. More precisely, the relationship between the proposed major claims should be analyzed in detail. Otherwise, the scope and claims of the paper should be updated accordingly. To this end, the paper proposes regularizing learnable parameters of NNs using different heuristics. Therefore, the novelty and scope can be redefined according to provided analyses and results.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}