{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The authors consider the task of interpretable video classification. First, a set of binary “concepts'' is predicted, and these concept features are then used for classifying a video. The set itself is automatically generated from natural language descriptions, instead of relying on expert annotations. The authors collect two datasets to validate the proposed approach and show that the model can match the performance of a standard video classification model, while being interpretable.\n\nThe reviewers felt that the paper was well written and that the method and empirical results were clearly outlined. They also appreciated the empirical results whereby interpretability doesn’t necessarily come at the expense of accuracy and consider interpretability as a desirable property. The main reason for the borderline results is the heuristic nature of the proposed automatic concept labeling and the empirical evaluation against alternative baselines. In particular, one needs to **show that the proposed method generalises to other datasets**. Secondly, one of the main contributions, namely the automatic **concept extraction, still ends up requiring human annotation in the form of narrations**, and this cost should be quantified and contextualised.\n\nI suggest the authors address these points and resubmit."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper works on the interpretability of video understanding problem. With a set of textual descriptions, the authors propose a pipeline called CoDex to extract the key concepts for explaining the classification, in contrast to previous methods which use the predefined classes. The CoDex method contains clearning, extraction, completion, grouping, pruning and vectorization phases to obtain the final concept matrix. The trained final concept bottleneck model can obtain interpretable explanations aligned with human servey, while retaining the performance",
            "main_review": "Here are the pros and cons of the paper:\n\nPros:\n1. This paper presents a quite interesting method which automatically selects the concepts as bottleneck for explanation. This could benefit other interpretability models which have textual descriptions available.\n2. The authors performed a good amount of illustration on the intermediate results after each phase\n3. The proposed datasets could also benefit the related fields/applications\n\nCons:\n1. It seems that there's a lack of ablation studies on each phase and a lack of some decent baseline models for selecting the key concepts. One simple baseline might be, training an MLP together with the information bottleneck model and selecting the ones with high importance. For each phase, it's not quite clear to me how important they are.\n2. It looks like the CoDex also needs some human efforts to tune and intervene the process. Is there any way to empirically prove that they are general enough to process various texts and can obtain similar results with humans?\n3. The authors mentioned that this system is for videos, but it looks like the whole process is quite general, and there is not much correlation with the mentioned issues such as the time span?",
            "summary_of_the_review": "I think this paper overall contributes an interesting pipeline for automatically extracting relevant concepts for the concept bottleneck model. Several concerns are listed above.\n\n-- after rebuttal --\nThanks for providing the rebuttal and explanations. I've read the authors' responses and other reviews. Overall I still feel the paper is addressing an interesting problem, but agree that the method might limit its applicability. I'll keep my rating (borderline-ish).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors address the task of concept-based video classification: The resulting model is interpretable, as it first predicts a set of binary \"concepts\", and these concept-features alone are then used for producing the final classification. The main novelty of this work is to propose a method for automatically generating these concepts from natural language descriptions (hence avoiding the need for expert annotations). The authors produce two new datasets of roughly 2000 videos each to validate their method. They also show that their concept-based model performs as well as a standard video classification model on these datasets (ie interpretability does not come at the cost of predictive accuracy). ",
            "main_review": "The authors address the timely problem of interpretable video classification. The paper is well-written, and the authors have also described their method, and data collection process, thoroughly and clearly. And I also appreciate the discussion of future work and weaknesses of the current approach.\n\nHowever, I do feel that the main contribution of the paper -- to generate concepts automatically from natural language descriptions provided by the annotators -- is quite heuristic (combination of NLP post-processing and clustering) and not generalisable across different datasets.\n\nIt is not clear to me how the proposed method can be extended to more common video datasets. For example, on Kinetics or Something-Something, the action classes are fairly \"atomic\", and it is not clear to me what natural language explanations there could for these kind of actions beyond the class label itself. In other words, the \"concepts\" that end up being extracted in this paper, are similar to the class labels themselves in other datasets. And so it is not clear to me how the proposed method can develop interpretable models for such datasets.\n\nI think the paper would also be stronger if the authors compared the annotation effort required to generate the natural language descriptions used in this paper, to the annotation time/cost to collect the original dataset in the first place, compared to the annotation effort required for the manually-defined set of concepts in previous work. Although the proposed concept extraction method is automatic, it still requires significant annotations in the form of natural language descriptions for each video in the dataset. And so the annotation time and effort of this needs to be contextualised.\n\nThe paper would also be more convincing to me if the authors augmented existing, popular video classification datasets (ie Kinetics, Something-Something, Epic Kitchens etc) with descriptions and developed interpretable models for such datasets. Not only are these datasets more common in the community, but I believe that the vocabulary of these datasets are more challenging for the proposed method, compared to for example, the baseball dataset used in this work (since sports videos consist of well-defined actions which are explained by the rules of the sport.)\n\nThe proposed attention mechanism helps to improve the concept prediction accuracy, but not at all on the task classification accuracy. It would also be interesting to see how the fully-connected subnetwork, operating on \"oracle\" concept scores (ie the concept ground truth, rather than the concepts predicted by the network before it) performs. This would show the headroom available if the network was able to predict these initial concepts better.",
            "summary_of_the_review": "In summary, the paper is well-written and addresses an important problem of interpretable video classification models. The proposed method does not rely on manual concept labelling, but rather extracts these concepts automatically from natural language descriptions provided by dataset annotators. My concern is that the proposed method does not seem general to me, and I feel that it only works for the specific datasets that the authors have chosen. And although the concept extraction process is automatic, the method requires obtaining natural language descriptions for each video in the dataset, and the annotation cost of this is not discussed at all in the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": " This paper proposed an automatic concept discovery and extraction module based on the label explanations and an end-to-end concept bottleneck model for video classification. The result on two constructed datasets (MLB-V2E and MSR-V2E) shows that the architecture can get explainable results without harming the classification performance.\n",
            "main_review": "Strength:\n1) CoDEx proposed in this paper provides a more comprehensive understanding of the video feature by extracting concepts from the explanation of the labels and constructing a higher-dimensional concept space. \n2) The pipeline can be easily applied to any supervised video classification architecture.\n\nWeakness: \n1) One of the motivations of the CoDEx is to get concepts without domain experts, but only simple cases are reported in the results. In a simpler domain, the label will be highly correlative to the concepts, so the requirement of experts is low. However, in a more complicated domain, the requirement of experts will become higher. Whether the explanation will still be reasonable is not reported. \n2) This work is more like a \"label decomposition\" based on natural language explanations and has low relevance to video modeling. \n3) The annotation quality of explanations is important since the extracted concepts are from them. However, only two datasets with 5 and 10 classes are reported in this paper. It seems that the annotation cost of this pipeline is high in even normal-size datasets.\n4) In the extraction phase of CoDEx, a fixed set of rules is used to extract \"raw concepts\". The motivation for using sentences as the concept is not clear. Will word-level concepts work? \n\nMinor: Both \"black-box\" and \"blackbox\" are shown in the paper, which is inconsistent. ",
            "summary_of_the_review": "This paper proposed an end-to-end pipeline to train an explainable video classification model. The paper is targeted to study three interesting research questions, and I particularly liked the discussion section at the end of the paper. However, the experiments lack in some aspects resulting in a less convincing story. \n\n\n######## Post-Rebuttal ########\n\nI appreciate the authors made a good effort to improve their paper based on review comments. However, I still find that the contribution is weak -- experiments are done on only two video datasets with limited classes but given that the method itself is generic. Therefore, I stick with my original rating. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a method for video classification, which uses textual descriptions of each training video to automatically extract concepts and uses these concepts for classification. For extracting the concepts, the method identifies some phrases in each description. The extracted phrases are grouped based on their semantic similarity and associated labels. Then they are pruned so that only informative phrases are extracted as concepts. For inference, these concepts are identified in a video and based on these identified concepts, an FC layer classifies a video into one of the target classes. The method is evaluated with two datasets MLB-Youtube video activity classification dataset and MSR VTT dataset with extending them with textual descriptions, showing that the concept bottleneck does not impact a lot to the classification performance. The human study demonstrates that identified concepts by their method is better than randomly selected/frequent concepts.",
            "main_review": "## Strengths\n1. The ways the method groups and prunes raw concepts are interesting as these sub-modules are aware of associate labels. \n1. The experimental results are encouraging. Because the concept bottleneck forces to drop much information in the original video, the performance drop seems inevitable; yet the performance degradation is limited. \n\n## Weaknesses\n1. I think the competing approach with this method is the one that uses a set of handcrafted concepts instead of automatically extracted concepts from textual descriptions, while according to the paper, the downside of such handcrafted concepts is that there is no guarantee of having a necessary and sufficient set of concepts. However, this is also the case for textual descriptions, and thus to demonstrate the merit of this method, I think comparison to the approach that uses a set of handcrafted concepts are necessary. One possible comparison is to design such a set of concepts from scratch, and another is to manually extract concepts from textual descriptions. \n1. The necessity of pruning is not clear. According to Figs. 3 and 4, the number of concepts (almost) monotonically increases the performance in senses of both MI and classification. I would say that, perhaps, these less useful concepts do not help a lot, but unless they are harmless (for example, redundant concepts may still increase the chance of correct classification, while if a certain concept is completely useless, the learned FC layer will ignore it), there is no need to remove them. It would be helpful to have some more discussion on the necessity of pruning. \n",
            "summary_of_the_review": "I think the paper is well-written, but the merit of the method is not very clear. Since textual descriptions may require a similar burden to handcrafting a set of concepts, the proposed method may just add noises in the set of concepts. Regarding this point, the merit of the method should be experimentally verified (w1). Also, the necessity of pruning is not very clear (w2), which I guess involves the first contribution of the paper. I think the paper will be stronger if these concerns are resolved. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}