{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper presents multi-agent RL framework that uses the divergence between the learned policies and a target policy as a penalty that pushes the agent to learn cooperative strategies. The proposed method is built on top of an existing one (DAPO, Wang et al., 2019). Empirical experiments clearly show the advantage of the proposed method.\n\nThe reviews for this paper are mixed and borderline. The reviewers appreciate the experiments reported in the paper and that indicate the advantage of the proposed method. But two reviewers do not think that the proposed analysis is sufficiently novel compared to an existing one (DAPO). The responses provided by the authors were appreciated, but did not dissipate these concerns."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "For cooperative multi-agent reinforcement learning, they propose an off-policy(?) learning method that is reqularized by the KL divergence to some target policy, which, in contrast to classic max entropy RL, allows to circumvent(?) the bias that the regularizer introduces (eq18).\nThey give several special cases / tricks of how the learning can be somewhat decoupled between the agent (Lem 3, end of sec4.4). They paper comprises theory about convergence of the method, as well as experiments in toy envs.",
            "main_review": "Regarding related work, it seems that closely related is Wang et al. 2019. I propose the give more details on the differences to the current work, as to why it cannot be applied to the cooperative MARL setting the authors study (because intuitively, cooperative MARL is verly close to the single-agent case).\n\nAm I understanding correctly that the theoretical guarantees (monotonic improvement) only hold for a fixed target policy rho? I.e., not for the additional update rule for rho in 4.5?  If so, that would be OK, it is clear that one cannot understand everyhting in one paper. However, it may be worth making more explicit if there is a theoretical guarantee also for this general method or not, and if there in fact is, then explicitly state this as a theorem.\n\nI suggest to give a bit more overview and motivation of what is being done where in the paper and how things are connected. For instance, it remained unclear to me how rho is defined, or what is intuitively is, until sec 4.5.",
            "summary_of_the_review": "MARL is a very challenging task, and I agree that entropy regularization introduces problematic biases, so they address a relevant and challenging problem. The paper is strong in presenting both theory (I took a brief look at the proofs, the authors seem to know what they are doing, but I did not go through details) and experiments. \n\nWhat I'm not so sure about is how deep the contribution is compared to previous work, in particular Wang et al. 2019, or, for instance, (Haarnoja et al., 2018), which is cited in the proof of Lem 2. Additionally, it seems the guarantees are only for fixed rho, while the full strength (nonbiasedness) of the method seems to come into play only when updating rho as well.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper develops a new multiagent framework with regulated divergence to address the learning of biased policies in the maximum-entropy approach. Specifically, the main contributions include the developments of the divergence policy iteration for general cooperative MARL settings and the off-policy divergence regularized multiagent actor-critic framework (DMAC). Empirical results show that DMAC can improve the performance of existing MARL frameworks in various multiagent domains.",
            "main_review": "**Strengths:**\n1. The paper is well written, organized, and explains the main insights well. \n2. The proposed framework has a very appealing and important benefit that DMAC is flexible and can be easily combined with previous works. This benefit is highlighted well by combining with four representative MARL algorithms in the challenging SMAC domain. \n3. DMAC makes a conclusion in a principled manner, where the benefit of DMAC is thanks to the improved exploration without introducing the bias (Section 4.5). \n\n**Questions:**\n1. As pointed in the paper, the theoretical study in Section 4.2 is assuming the fixed target policy. However, in practice, the target joint policy is non-stationary/updated (see Algorithm 1). Hence, I wonder whether the theoretical conclusions (i.e., monotonic improvement, optimal policy convergence) would still hold with the non-stationary target joint policy.\n2. To address the partial observability, an RNN layer is used. However, it is unclear from reading the main paper and appendix, how the hidden state is handled in the off-policy implementation. Is the hidden state stored in the replay buffer as in Hausknecht and Stone, 2015?\n3. What are the possible future directions of DMAC? Can DMAC be extended to general-sum games (other than cooperative settings), and if not, what are the expected challenges?\n\n**Reference:**\nMatthew Hausknecht, Peter Stone. Deep Recurrent Q-Learning for Partially Observable MDPs. AAAI, 2015.",
            "summary_of_the_review": "Initially, I vote for a score of 6. While some parts of the theoretical studies are based on related works (e.g., SAC, DOP), DMAC shows the appealing benefit that the framework can be easily combined with prior MARL works, so it has much applicability. After reading the authors' responses to my questions, I am open to raising my score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes DMAC, a general off-policy actor-critic framework with entropy-based divergence regularization for multi-agent reinforcement learning to better coordinate multiple agents in a complicated environment. Theoretical derivations and empirical investigations on several state-of-the-art algorithmic benchmarks over the SMAC environment demonstrate the effectiveness of DMAC.  ",
            "main_review": "Strengths:\nTheoretical derivations have been included in the analysis of the algorithm in addition to the experiments. \nExperiments conducted are comprehensive, covering most of the state-of-the-art algorithmic benchmarks. \n\nWeaknesses:\nThe motivation for the target policy rho is not well-explained. What exactly is rho, and why do we need to maintain a rho to make learning more effective? Does it represent prior knowledge of team level policy pre-defined by the user or something else? If we already know what the optimal policy is, why do we need DMAC? If we don’t necessarily know the optimal policy, how can rho be chosen such that rho will help learning? Neither theoretical or empirical evaluations seem to address these concerns.\nThe degree of novelty seems unclear. On a high level, DMAC looks like a slight modification of the soft actor-critic(SAC) algorithm with joint action spaces, particularly in the definition of the objective function. The update rule is also standard in RL literature, and it would be ideal if the authors can further emphasize how their method differs from existing benchmarks. \n\n",
            "summary_of_the_review": "DMAC does provide improvements over existing algorithmic benchmarks, but the level of novelty seems limited as the DMAC method resembles SAC and several components of DMAC has questionable motivation. I’d be happy to change my score, if the authors address these issues accordingly. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper suggests a DMAC learning framework by introducing a divergence penalty between the learning policy between a target policy into the RL framework instead of the commonly used entropy penalty. It can be shown that the proposed objective can be easily combined with various MARL algorithms and can eventually lead to the optimal policy of the underlying unmodified MDP with properly set target policies. The paper rigorously studies the mathematical foundation of the algorithm following a standard workflow and presents a few experiments showing that DMAC could improve the learning efficiency of various MARL algorithms. ",
            "main_review": "### Strong points\n1. In general, I think the proposed framework is at least new in the MARL literature. It is interesting to read a paper with a full deviation of such a divergence formulation of the learning objective.\n2. The mathematical justifications are completely and sufficient for the purpose of presentation, although I do feel some of them could be possible shrank or moved to the appendix to save some space for more discussions and experiments. For example, Lemma 2 & 3 look just the same to me, and the discussions on applying DMAC to COMA look nothing special to the audience familiar with COMA.\n3. I really appreciate seeing that the proposed methods can be really combined with multiple algorithms and gain improvement naturally.\n\n### Weak points\n1. Yes, the method is new in MARL but I'm still a bit concerned about the novelty of all the technical content. \n\nThere are sufficient mathematical discussions but I can still see most of the techniques are somewhat straightforward from existing works. For example, the value decomposition policy improvement theorem seems direct to obtain from the DOP paper; the mirror descent analysis seems very similar to the analysis in the DAPO paper. The authors state in the introduction section that \"DAPO cannot be trivially extended to cooperative MARL settings\". However, I don't really get this argument. It seems that all the mirror decent analysis simply holds directly with the assumption of linear-value decomposition. So, at least from the current form of the paper, the authors would have been done a better job explaining why those methods are challenging to be applied in MARL and what technical ideas really makes this paper distinct (rather simply saying that paper A is working on single-agent RL while paper B is on MARL). I hope the authors make more clarifications and discussions in the next revision. \n\n2. Experiments are not sufficiently convincing.\n\n(1) For Fig.1, why not run the same experiment setting as DOP so that we can have a straightforward comparison? From the curves in Fig.1, the behavior of COMA and DOP are contradicting the curves in the DOP paper. \n\n(2) The paper utilizes COMA as its on-policy baseline. It is a fair choice, but it is been known that COMA has less satisfying performances in many domains. Is it possible to include some strong on-policy baselines like PPO or IMPALA?\n\n(3) Except for FOP, the SMAC maps used in the experiments are less challenging. It would be more convincing if more hard maps can be examined. Also, for FOP, why not perform experiments on those simpler maps? The authors state that \"COMA+DMAC has higher win rates than COMA in most cases at convergence\" and \"DOP+DMAC learns faster than DOP in most cases and finally converges to better performance\". I agree that DMAC enhancement makes learning faster but I cannot really see the performances at convergence ---- at least from the visualization, DOP and COMA are still improving on some maps. \n\n\n### Writing \n1. It would be better if the authors could include some justifications at the beginning of section 4 on why such a ratio penalty is preferred. What is the motivation for this choice? Is this simply due to that we can eventually derive a nice KL-penalized value formulation?\n\n2. \"Besides, DMAC is **benefited** for exploration and stable policy improvement.\" --> beneficial\n\n3. \"**we** find that COMA+DMAC has higher win rates than COMA in most cases at convergence,\" --> capital w",
            "summary_of_the_review": "This paper presents a new algorithmic framework with sufficient mathematical discussions, although I'm a bit concerned about its current form. However, the paper can be improved if more novelty discussions can be presented and more thorough experiments can be conducted. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}