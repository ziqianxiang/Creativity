{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "In this study, the authors propose a new graph transformer network for dynamic graph representation. To solve the challenges of static graphs learning and the temporal information aggregating, this paper introduces a Dynamic Graph Transformer (DGT) which contains three components: (1) a two-tower Transformer-based method, (2) temporal-union graph construction (3) a complementary pre-training task. Extensive experiments on the two datasets of link prediction and node classification demonstrate the superiority of the model. The ablation studies justify the effectiveness of each module in the DGT model. The reviewers has various technical issues with the paper which the authors mostly addressed (e.g., whether the nodes are static or dynamic, whether DGT is robust to noise, whether it scales to larger datasets). Overall, the contributions seem incremental. There is confusion among the reviewers as to whether the proposed model differs from prior art. It seems to me there actual  differences  but whether they major or minor is open to interpretation. Overall there reviewers were not particularly excited about model/results/contributions."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors designed a graph transformer for dynamic graph link prediction tasks. The model provides a scalable solution to graph transformers, and is claimed to be robust to noise.",
            "main_review": "Strength:\nThe proposed method was presented in detail, with many experiments to support the main contribution.\n\nWeakness:\n1. The difference among the three categories classified in \"Dynamic graph representation learning\" could be stated. For example, the second and third classes, according to the authors, only differ from the choice of graph convolution: \"...use self-attention mechanism for spatial and temporal message aggregation. For example, DYSAT (Sankar et al., 2018) replaces the GCN with GAT...\". Rather than the key difference of embedding a dynamic graph, such difference choices are more about neighborhood aggregation (no matter the graph is static or dynamic).\n2. Some sections could be reallocated to make it easier to follow. For example, Section 3.1 could be moved to later sections after introducing basic paradigms, such as \" temporal-union graph\".\n3. It is confusing whether the authors work on static nodes (and dynamic edges) or dynamic nodes. Section 2 defines \"...the t-th snapshot graph Gt(V, Et) is an undirected graph with a shared node set V...\" while Figure 1 (and later presentations) seems to allow different node sets at different time steps.\n4. As the proposed method is claimed to be robust to noises of input data, it would be desired to design experiments to prove this statement. \n5. The scalability of the proposed method could be supported by larger datasets. \n6. As the model complexity relies largely on edge (according to Section 3.1), revealing the consumption on the two (relatively) large datasets, Wikipedia and Reddit, would be more informative (Table 3).  ",
            "summary_of_the_review": "The paper could be more persuasive on their main contributions for providing more empirical evidence, such as the 'robustness to noise' and 'scalable solution'.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the author focused on the problem of dynamic graph representation learning and proposed a temporal-union graph structure and a target-context node sampling strategy. Experiments show that this work performs well in some real-world datasets.",
            "main_review": "Strength: \nThe paper proposed a temporal-spatial encoding to capture implicit edge connections.\nWeaknesses:\n1. This paper investigates the methods for dynamic graph representation learning. However, some closely related methods are not discussed and not compared in this paper, e.g.,\nhttps://storage.googleapis.com/pub-tools-public-publication-data/pdf/19698cbb225b3fc165231dd2fc6f0483ff28a777.pdf,\nhttps://snap.stanford.edu/jodie/\netc.\nThe authors are suggested to compare with or at least discuss these work, which could provide a more comprehensive discussion for the paper.\n\n2. The paper lacks some standard datasets that are widely utilized in this topic, e.g., MOOC and LastFM (https://snap.stanford.edu/jodie/). The authors are suggested to utilize or at least discuss these datasets, which could help the readers to compare the proposed method with the existing ones.",
            "summary_of_the_review": "In this paper, the author introduced a DGT for dynamic graph representation learning to leverage the graph topology and capture implicit edge connections. For the weaknesses, it is suggested to provide more concrete experimental results to better validate the effectiveness of the proposed method.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this study, the authors propose a new graph transformer network for dynamic graph representation. To solve the challenges of static graphs learning and the temporal information aggregating, this paper introduces a Dynamic Graph Transformer (DGT) which contains three components: (1) a two-tower Transformer-based method, (2) temporal-union graph construction (3) a complementary pre-training task. Extensive experiments on the two datasets of link prediction and node classification demonstrate the superiority of the model. The ablation studies justify the effectiveness of each module in the DGT model.",
            "main_review": "Strengths:\n1. The temporal-union graph can significantly reduce the computational costs and memory usage of transformers.\n2. To further reduce the computation on the spatial dimension, a new sampling strategy divides the sampled nodes into two groups called target nodes and context nodes.\n3. The spatial-temporal encoding layer describes a way to encode the spatial and temporal information of a node into the network.\n4. The pre-training method containing a temporal reconstruction loss and a multi-view contrastive loss is proven to be useful to enhance the generalization ability of the model from both theoretical and experimental ways.\n5. Experiment results on two datasets in different applications show the proposed model achieves state-of-the-art performances. Meanwhile, the ablation studies are sufficient to evaluate modules in the DGT. \n\nWeaknesses:\n1. The paper claims its main contributions to the transformers from both its title and introduction section. However, the main body of the transformer in Section 3.4 seems similar to the previous works. Contributions (2) and (3) could be considered as auxiliary methods to improve the transformer. This might be solved by a better paper organization.\n2. An issue of temporal-union graph generation: the method only assumes the dynamic edges but the number of nodes remains static. This may limit the application of the model. Can the temporal-union generation be applied to the above case?\n3. In Section 3.2, this paper divides the nodes into target and context nodes. However, during testing, if all the nodes in the dataset are required to be classified or to predict links (i.e., all nodes are target nodes), it seems that this sampling method and the two-tower transformer will become invalid. \n4. For spatial-temporal encoding, the method is effective but a little complex. It would be better for authors to compare the performance with other straightforward approaches. For example, concatenating the spatial and temporal information to the node, using the raw information as the bias term, or other methods the authors mention in Appendix D.1.\n",
            "summary_of_the_review": "This work proposes a new DGT for dynamic graph representation. All the claims of existing challenges and their corresponding solutions are correct. The novelty of this work is moderate and the experiments on two datasets justify the superiority of the model. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper makes an attempt to study the dynamic graph representation learning. The paper proposes a couple tricks for the combination with graph transformer networks. The tricks include sampling, union graph, pretraining etc. Some experiments have been conducted.",
            "main_review": "This paper makes an attempt to study the dynamic graph representation learning. The paper proposes a couple tricks for the combination with graph transformer networks. The tricks include sampling, union graph, pretraining etc. Some experiments have been conducted.\n\nThis paper makes an attempt to study the dynamic graph representation learning. The paper proposes a couple tricks for the combination with graph transformer networks. The tricks include sampling, union graph, pretraining etc. Some experiments have been conducted.\n\nWhile the topic of the paper: time series modeling of graphs is certainly important and needs to be highlighted, i find the paper doesn't really touch upon the core research question of what models are good for time series modeling of graphs, i.e., how the time domain and cross-section domain should be best combined. Instead, the paper mostly focused on a few engineering tricks for applying the transformer architecture and some pretraining tasks. This makes the paper quite disappointing and not so important for the research community, hence, I recommend rejection. This paper is maybe more suited for engineering or Kaggle style conferences, but not ICLR, which should highlight eye-opening scientific research.\n\n1. While the tricks you propose for Transformer is of interest to some Kaggle audience, it's not clear what's the best way to combine time and cross-section domain information. The simplest you can do is either treat each time snapshot as independent samples, and do basic training, this in practice work quite well. You can also use say RNN or TCN etc but unclear whether these will add value. It's also not clear what's the best ways. In this sense, this paper doesn't really touch upon this question at all. Authors need to work on this.\n\n2. Authors need to implement and compare with the efficient Transformer models such as Liformer etc. It's not clear the tricks proopsed can outperform those.\n\n3. If the graphs are quite noisy, say the financial time series, then will attention fail?Since the main focus is on attention, authors need to validate this.\n\n\n",
            "summary_of_the_review": "While the topic of the paper: time series modeling of graphs is certainly important and needs to be highlighted, i find the paper doesn't really touch upon the core research question of what models are good for time series modeling of graphs, i.e., how the time domain and cross-section domain should be best combined. Instead, the paper mostly focused on a few engineering tricks for applying the transformer architecture and some pretraining tasks. This makes the paper quite disappointing and not so important for the research community, hence, I recommend rejection. This paper is maybe more suited for engineering or Kaggle style conferences, but not ICLR, which should highlight eye-opening scientific research.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}