{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "There was some discussion on this paper, both with the authors and between reviewers. On the one hand, there is a general agreement that the empirical results suggesting that spectral clustering-based method can be competitive with SOTA methods on node classification benchmark is an interesting result. One the other hand, reviewers did not find a significantly novel contribution in the methodology proposed, and found that the empirical evaluation lacks depth and details to be really informative (eg, to understand why some methods work or not on some benchmarks). There is therefore a consensus that the paper is not ready for ICLR in its current form, but we hope that the reviews and discussion will help the authors prepare a revised version in the future."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper puts forth a methodology for supervised spectral clustering, by leveraging existing theoretical foundations of spectral clustering, in particular drawing insights from seminal works such as Lee at al on multi-way Cheeger inequalities.\n",
            "main_review": "The transfer of geometric intuition from Lee at al. leads to an improved pipeline for supervised spectral-based classification, and is a nice insight. It could spark further research in this direction, by building on strong theoretical foundations.\n\nIt is quite surprising to see that such a supervised spectral clustering pipeline attains higher accuracy than a number of sota methods which were thought to be miles away better in performance, which also has lead to spectral methods being dropped from the comparison baselines in most works. This work demonstrates that with the right pre-processing steps in place (like the radial projection onto the unit sphere), spectral methods could be brought back to the comparison board.\n\nThe experimental results are convincing, though they could be made more complete, but it is enlightening to observe such performance of from a spectral based methods.  \n\nIt would be good to see a comparison of performance for synthetically generated data, for example by considering standard stochastic block models. ",
            "summary_of_the_review": "An interesting paper which could open further avenues of research at the intersection of spectral methods and neural-based approaches.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "none",
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a comprehensive analysis of spectral clustering based approaches for node classification in graph-based data. The paper thoroughly analyzes design decisions of using spectral clustering in this supervised setting. The empirical results indicate that even compared to GCN / deeper approaches, the less parametric spectral clustering-based approach can be competitive or better at classification tasks. ",
            "main_review": "**Summary**. This paper investigates the use of spectral clustering-based approaches for node classification in graph data. It extends analysis of spectral clustering in Lee et al (2014) including the key steps of spectral embedding, orthogonal projection, radial projection, geometric partitioning.  Overall this paper provides a clear and thoughtful presentation the spectral clustering-based methods. The clarity and care the authors put into understanding each component of the algorithms is a key merit of the paper. \n\n**Merits**. This paper does an excellent job in its thorough investigation of these spectral clustering-based approaches. Not only does this provide an array of competing methods for future work, but also provides for a better understand of the impact of various components of related models. I think this kind of careful analysis is needed, especially as the number of such methods for node classification / related problems increases. Suggesting such a critical look at past work and introduction of baselines from decades prior (e.g. RandomWalker (Kemp, 2006)), I think is fantastic an important contribution.    \n\n**Concerns**. Despite these merits, I have following concerns about the paper. \n* While there is a careful analysis of the different design decisions/performance tradeoffs, I feel that there is only a limited understanding about what are the properties of the datasets that lead to these decisions/performance differences. I.e., what is about the Arxiv data that makes Conic more effective? Is it just the dataset size? Other properties? Why is Knn competitive on some dataset, but then much worse that sota on others (e.g. cora 1%)?\n* While these spectral clustering methods are simpler / more scalable than some competing GCN-based approaches. I feel the (train or test) efficiency / accuracy trad-offs are not sufficiently explored? I.e. when would be scenarios/considerations that someone would chose one of these approaches? \n* I think the paper would be improved with some discussion of tradeoffs empirically from a wider variety of GCN approaches. \n\n**Minor Notes / Typos**:\n\n* Top Table of Table 1: RandomWalker Cora 4% seems to be better than other methods, shouldn't it be bolded? Same for knn in WikiCS 1%?\n* Intro: \"spectacular comeback\" is perhaps a bit informal/colloquial? \n* 2.1: \"Along it\" phrasing seemed confusing to me.\n* 2.1.2: There seem to be four components rather than \"three\" as noted?\n* Top Table of Table 1: 16 % and 32% seems larger font than others\n* Table 1 has a lot of results. and located at the end of the paper as it is, is somewhat difficult to navigate, especially as reading through the experiments section. Perhaps there is a way to include it at least before the end of the text of the section? \n* Several of the citations in the text are missing the year.\n\n\n\n",
            "summary_of_the_review": "This is a thoughtful investigation spectral clustering in classification problems for graph-based data. However, there are several concerns regarding missing forms of analysis such as deeper investigations into why certain methods work well on certain datasets.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "This paper does not introduce any new ethics concerns. ",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper argues for refocusing the efforts of the graph neural network community onto previous work on spectral clustering. The authors propose the addition of simple, but structured, classification methods to an existing unsupervised spectral clustering algorithm. A battery of empirical results on standard graph benchmarks demonstrate the effectiveness of such simple models and compare favourably to existing, more complex models.",
            "main_review": "Strengths: The paper is mostly well written, and the main thrust of the paper is an important point for researchers in this area: simple, but well structured and theoretically motivated graph models can perform surprisingly well against larger, more complex models, and can even outperform them. Also, the empirical results are presented honestly, which is greatly appreciated.\n\nWeaknesses: The main weaknesses of the paper are: (1) a lack of direction and (2) an insufficient distinction from previous works.\n\nIs the point of the paper to alert the community to the previous work of Lee et al. 2014, or is it to emphasise that such models have impressive empirical performance compared with other methods? If it is the former, then Section 2 can be improved by clearing up notation and errors as well as more firmly delineating where this paper diverges from existing work, which would help with point (2) as well. Is Section 2.1.3 the insight of the authors or does it also summarise previous works?\n\nIf it is the latter, then I do not find the argument of the paper either novel or convincing. The results demonstrate that the addition of correction and smooth, the essential point of Huang et al. 2021, consistently has the largest impact on the accuracy. A counter argument to the paper’s results is that the node classification tasks in Citeseer, Cora, and Pubmed are in a way too simple, being well described by local smoothing of the labels. If that is the case how does this differ from the point of Huang et al.? Recent GNN research has focused on topics such as incorporating additional node and edge information, which the proposed method could attempt to incorporate, but is not included in the empirical section as the authors choose to focus solely on the connectivity information. How do the proposed models compare when such information is considered? How can these data be incorporated into the models in a principled manner? The other direction for more challenging graph problems is in inter-domain settings, where there are multiple graphs (such as the molecule classification benchmarks) or in finding correspondences between nodes in situations where the graph has been modified in some way (by pruning or modifying the ambient embedding). Unfortunately, the proposed methods would fail in these situations. However, this would be a very interesting direction for the authors to pursue in the future. \n\nWhat exactly is ‘neural’ about the additional classifiers? Such a description seems to detract from the main claim, and does not seem justified. Is it simply that there are two steps: the spectral embedding and the classification? Is it the inclusion of the K independent channels?\n\nNotation and definitions: \n\na) the definition of vol(.) should be consistent in its indexing.\n\nb) my preference would be to reserve degree of a vertex for the unweighted count of edges, and call what is in the paper the weighted degree.\n\nc) I am familiar with the definition of conductance, but have not seen the notation of cap(.) before, a brief definition would likely improve clarity.\n\nd) I understand what the authors are attempting to communicate about a partition of the vertex set, but as written the statement is incorrect: the S_i must be disjoint as well.\n\ne) Final sentence of Conic in Section 2.2, the second ‘Conic’ should be ‘Linear’.\n\nEmpirical Results: \n\na) Why are there empty entries for the proposed models in the Products results? Do the methods fail at this scale or are there other problems? \n\nb) Inclusion of the results of GCN, NetMF, and DeepWalk in the Arkiv and Products tasks would aid the paper as these are much larger and could be treated as a more challenging comparison. Is there a reason these are not included?",
            "summary_of_the_review": "I appreciate the main point of the paper, that simple models can perform surprisingly well on standard graph benchmarks, and I find such methods interesting. However, in its current form, the paper insufficiently distinguishes itself either from existing theoretical works such as the cited Lee et al., or the essential empirical point of Huang et al., and so I recommend rejection. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a study about spectral clustering techniques. Given the standard spectral clustering approach, some variations are investigated, also using recent advancements in the field. The experiments show that such a simple approach can produce state-of-the-art results on social graphs.",
            "main_review": "==STRENGHTS==\n\nBACKGROUND\nI have found the theory well written and self-contained. I think a non-expert could find most of the information in the paper, and I appreciate this aspect. \n\nINSIGHTS\nThe geometrical insights are didactical and well communicated. The conclusion given by the experiments looks interesting and valuable for future practitioners, while I think a synthesis would be beneficial for the reader.\n\n==WEAKNESSES==\n\nNOVELTY AND CONTRIBUTION\nThis work does not present any particular methodological novelty, and I see this paper more as an experimental one. I think such kinds of papers are valuable and deserve attention, but in this specific case, I think the contribution provided by the analysis is limited in the following aspects:\na) there are no specific comments for performances on different datasets; for example, why on CORA does the \"Conic\" perform poorly without \"+CS\"? Why on \"arXiv\" outperforms all other methods? Which are the peculiarities of the different datasets? I see that such analysis may be complicated given the dimensionality of the graphs, but I think that even some insights given by further investigation would be important\nb) the experiments lack variety in the applicative domain; for example, spectral clustering has obtained significant attention in the 3D field. It would be interesting to apply the techniques to at least one geometrical dataset that is more prone to have symmetries. These latter are the main limitations of spectral methods that do not consider additional features and should be somehow discussed in the paper to provide a complete overview.\nc) I think that considering only the quantitative performances tells only part of the story, and a deeper analysis should be provided (e.g., error localization, timings)\n\nPRESENTATION\nApart from the background, I find the rest of the paper a bit verbose, and that could be improved in its communication. For example, visualization of spectral embeddings and a pipeline scheme would help a lot in diving into the paper. Also, It is not comfortable to move back and forth to check the results of the final table: I suggest splitting it in two and locate near the discussion text. I think the visual aspect of the paper could be significantly improved and serve as support to the analysis.",
            "summary_of_the_review": "While I like the general idea and I appreciated the effort in explaining the underlying theory, I think there is no significant methodological novelty and also a significant lack in the experimental and analysis settings. I do not consider it ready for publication in the present state.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper reviewed the application of spectral clustering embedding on node-level classification problems. It first revisited the theoretical foundation of spectral clustering, related to the graph NCut problems. Then it introduced neural network-based classifiers which utilize the embedding for classification tasks. Their benchmark results validated that the neural network framework showed certain advantages in accuracy. And the \"correction & smooth\" introduced by Huang et al (ICLR 2021) would also improve the performance in a relative big margin.",
            "main_review": "Strengths:\n1. This paper is very clear and easy to follow and understand.\n2. It conducted solid experiments to show the performance differences for each method.\n\nWeakness:\nIt is very difficult to position and evaluates this paper. Though it concretely introduces lots of methods, the paper itself does not propose any new methods or insights, such that it does not qualify as a \"new-method\" paper. As for the \"review & benchmark\" paper, this paper lacks a more detailed exploration of this topic and the experimental validation is rather poor compared with review papers. Still, I will treat this paper as a review paper and below are some of my unuseful comments.\n\n1. The mathematical background could be more concrete. Instead of briefly introducing the cut and conductance, the author would also show the equation of graph Ncut, which reveals the power and need of conducting spectral clustering for node embedding.\n\n2. While introducing related methods, it would be better to use mathematical equations to showcase their insights and improvement compared with previous methods.\n\n3. The \"orthogonal projection\", \"radial projection\" and \"geometric partitioning\" can be unified in the classifier section. All these projections could be considered as the preprocessing for classifiers. The author could consider introducing them in the neural classifier part altogether. \n\n4. Though section 2.1.3 talks about clustering from the geometric perspective. But it does feel disconnected in this paper. I don't see the benefit here.\n\n5. About the efficient computing of the eigendecomposition, the author could consider eigengame (ICLR 2020). \n\n6. For a more comprehensive experiments. The author could consider compare spectral clustering embedding with other embedding. Like node2vec, and more graph neural network based methods like GAT, GraphSage et al.\n\n\n\n",
            "summary_of_the_review": "This paper is very well written. But it lacks originality and also did not derive new insights.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}