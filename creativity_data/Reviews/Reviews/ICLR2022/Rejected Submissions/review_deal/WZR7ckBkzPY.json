{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes  a min/max reformulation for JKO gradient flows appealing the variational formulation of f-divergences. This would alleviate the need of an explicit density. All reviewers pointed out the limited novelty in the work and the limited experimentation. \n\nWe encourage authors to add a theoretical analysis to their work and further strengthening of the experimental section with high dimensional experiments, and to resubmit the work on an upcoming venue."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a method to solve Wasserstein gradient flows based on the JKO scheme using variational formulations of functional objectives, such as the KL divergence or the generalized entropy (non-linear diffusion). Relying on known reformulations of the JKO scheme as optimization over convex functions, the paper departs from recent related methods in expressing certain objectives as f-divergences, and in turn using the dual formulation of these divergences to circumvent the need to do explicit density computation in these. The resulting method involves parametrizing two types of operators as neural networks (one of them as an input-convex neural network), and solving a mini-max objective. The paper presents experiments on simple PDEs (mostly in 1D or 2D) with known solutions. ",
            "main_review": "Strengths:\n* An ingenious use of f-divergence duality (aka variational formulation) to re-write density-depending functional flow objectives, such as the generalized entropy, as (optimization of) expectations, allowing for computation via finite-sample approximation \n* The resulting method seems to be empirically valid in low- and mid-dimensional settings \n\nWeaknesses:\n* Novelty, the main (and perhaps, only) novelty of this approach compared to (Mokrov et al., 2021; Alvarez-Melis et al., 2021; Bunne et al., 2021) is the use of the variational formulation of f-divergences, something that is itself well known. Note that this is only relevant for functional objectives that depend on the density of the measure itself, e.g., those that cannot be expressed as an expectation over the measure. All other objectives, including the interaction energy considered here, can be tackled with the exact same approach of the three papers above. \n* The novel contributions should be more clearly separated from non-novel ones. E.g., the reformulation of WGF as optimization over convex functions might seem like a contribution of this paper to an uninitiated reader, although it has been explored extensively in other work, most prominently in Benamou et al (2014), which is puzzling not cited here, and more recently in the 3 JKO + ICNN papers cited above.  \n* Some statements are not clear, not correct or too vague. These should all be clarified or corrected. For example:\n    * \"that is not too far away from the identity map\" in Pg 5. \n    * In section 2.2, the $\\delta F / \\delta P$ is not formally a gradient, but the first-variation of a functional\n    * In Section 3.6, it is stated that prior works requires explicit computation of log determinants of Hessians at cubic complexity, but this is not the case. At least one of those methods uses cuadrtic-cost (matrix-vector-product) stochastic estimators of the Hess logdet.\n* The motivation for using the FB scheme for the interaction energy is not clear. Some of the intuition/motivation discussed in sections B.2 and B.3 should probably be moved to the main text. \n* Limited experimental validation, especially with regards to high-dimensional settings, which is arguably the main promise of this work. In addition, all experiments are synthetic. A more compelling evaluation framework should include at least one high-dimensional, realistic dataset. \n* The evaluation is mostly qualitative. The paper purposefuly chooses PDEs with known solutions, but then provides mostly qualitative/visual results. A more compelling evaluation framwork would include quantitative comparison against these known solutions in sections 4.3 and 4.4.\n* While the computational complexity summary provided in the paper is useful, there are some many hidden constants in those bounds that they are hardly useful. These should be complemented with a thorough empirical runtime analysis comparing against exact and inexact methods, such as those cited here as related work. \n\nMinor comments:\n* While the motivation is different, there are deep and unexplored connections between the dualizatin of the f-divergence objectives used here and the dual of the kantorovich OT problem that has been recently explored extensively for learning Monge maps between distributions. Many of these rely on convex conjugacy to reformulate a sup objective as a sup-inf one (see Korotin et al. 2021b for an excellent survey on these). It would have been great (though not obligatory!) to see a discussion on these connections \n* I would suggest moving Table 1 after Proposition 1, so that all objects in the definition of $\\mathcal{A}(T,h)$ have been already introduced. I spent some time trying to figure out what $\\mu(T)$ was before finding it further down.\n\nMissing related work:\n\n* Benamou et al, \"Discretization of functionals involving the Monge-Ampère operator\". 2014.\n* Huang et al, \"Convex Potential Flows: Universar Probability Distributions with Optimal Transport and Convex Optimization\", ICLR 2021.\n* Bunne et al., \"JKOnet: Proximal Optimal Transport Modeling of Population Dynamics\", 2021.",
            "summary_of_the_review": "The paper provides an interesting variation on recent JKO-based methods for computational Wasserstein gradient flows, but its limited novelty and empirical evaluation diminish its contribution, and make it a borderline paper in my view. That being said, if the issues I raise in my review are properly addressed, I would be willing to increase my score. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the implementation of some Wasserstein Gradient Flows (WGF) in discrete time but without discretizing the space. The methods proposed are based on the JKO operator to discretize WGF in time. The implementation of the JKO can be challenging. The strategy of the authors is to first reparametrize the JKO as a minimization over a space of functions (instead of measures) via pushforward. Then, when the objective function is a f-divergence, the objective inside the JKO admit a variational representation and can be expressed as a sup. Conclusion: each JKO is written as a min max over a space of functions. To solve it, they parametrize the functions by neural networks and alternatively maximize and minimize the problem using Adam. An important feature is that the objective in the min max can be approximated with samples of the current distribution (its density doesn't appear, only integrals wrt to the current distribution). ",
            "main_review": "Strengths:\n\nThe paper is well written and high level ideas on the theory are explained. The approach is clear and reasonable. The simulations show promising results, they do not seem to have been cherry picked. They show what is claimed by the authors and there is no surprise given the simplicity of the approach.\n\nWeaknesses:\n\nThe theoretical consistence of the proposed methods is ignored. Are the proposed scheme consistent time discretizations of the WGF? Are the assumptions satisfied? Is the alternative maximization/minimization strategy (Algo 1) consistent? Do all measures considered admit density wrt Lebesgue? The paper mainly provides intuition for Algo 1 without solid theoretical foundations. \n\nMoreover, the technical contribution is rather limited (see the summary of the paper above). One could argue that it is an \"easy paper\" in the sense that only the simulations seem new. The novelty is mainly to have parametrized the functions in the objective of the JKO by neural networks. ",
            "summary_of_the_review": "The approach is reasonable, the simulation promising but I do not see a significant technical contribution (They essentially reparametrized a min max problem over a function space using neural nets and running Adam to alternatively maximize and minimize). ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a variational formulation of each JKO step for optimizing functionals on measures.\nDifferent from existing recent works on emulating JKO steps by training pushforward neural networks (either directly or as gradients of convex functions), the variational formulation involves another inner maximization of a function, without needing density access that typically requires cubic time complexity due to computing the log determinants of the pushforwards. Experiments are done to demonstrate the practicality of the algorithm.\n",
            "main_review": "This paper identifies a crucial challenge in the existing works of emulating JKO steps, namely the expensive computation of the densities of the form $P_k(x)$ for each $k$. The solution the paper suggests is reasonable, but it comes at the cost of adding another inner maximization which makes the optimization a lot more difficult (e.g. unstable due to high variance). Overall I think the amount of contribution is very limited. The variational formulation of the objective functionals considered is all well known, and putting it together with the JKO step is fairly straightforward. The experiments are not convincing enough to demonstrate the practical advantages of the proposed method compared to the alternatives.\n\nDetailed comments:\n- In Eq (9), a distribution $\\Gamma$ is introduced. What is the point of introducing this measure? Is it essentially an importance sampling of $Q$ for which we only know the density? Why is it enough to just choose Gaussians, which could be very different from $Q$?\n- Many sections are very similar to [Mokrov 2021]. For example, there is nothing new in Sec 3.5, and the experiment's setup in 4.1 and 4.2 are exactly the same. Yet the more challenging experiments from [Mokrov 2021] are not reproduced here, such as posterior inference and non-linear filtering. \n- In Sec 3.6, it is common to use the Hutchinson trace estimator to approximate the gradient of $\\log \\det$ (in addition to a linear solve), which could speed up the competing methods. It might be good to include a comparison to that.\n- The results in Figure 2 are visibly worse than those of [Mokrov 2021]. Moreover, here only up to dimension 13 is included, whereas [Mokrov 2021] contains dimension 32.\n- The results in Figure 3 of the proposed method are better than those of [Mokrov 2021]. I'm wondering why this is the case since in [Mokrov 2021] the KL divergence is calculated exactly, whereas in the proposed method additional bias could be introduced due to the failure of maximizing $h$.\n",
            "summary_of_the_review": "The paper studies an important challenge in JKO steps encountered by recent works, but the contributions are incremental without demonstrating convincing practical advantages.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a method to compute Wasserstein Gradient Flows (WGFs) via neural networks and the JKO scheme. In contrast to prior works, to compute WGFs of functionals involving f-divergences, the authors use variational approximations rather than direct computations. It is claimed to work faster and perform better.",
            "main_review": "**Benefits**\n1) The variational approach might be a solution to issues of direct computation such as cubically growing complexity as a function of the dimension;\n\n**Drawbacks**\n\n2) The experiments of the paper are weak and do not sufficiently support the main claims;\n3) The relation to the prior work is not fully disclosed;\n4) The scope of the paper (WGFs) is narrow.\n\n*Detailed comments are below.*\n\n**Relation to prior work.** A large part of the algorithm proposed by the authors matches the previously known art. More precisely, sections 3.0 and 3.1 exploit the reformulation of JKO via functions that have already been proposed in [1] (not cited in the paper!) and extensively used in [2,3]. I think this is not very clear from the text and might add extra (inexistent) value to the current work.\n\nIf I correctly understand, the actual difference w.r.t. the prior work, e.g., [2], is the way that the f-divergence is optimized. The authors use a variational approximation in contrast to direct computation. Here I have two remarks.\n\nFirst, the key claim of the paper is that direct computation scales cubically and is not feasible in high dimensions. I agree, but what about fast approximations? In [3] the authors explicitly state they use a fast estimator based on the Hutchinson method. In [2] the authors state that to speed up the computation fast approximations can be used. It is unfair to ignore this and compare in the experimental section only with [2] using and only by using the direct computation.\n\nSecond, the variational approximation of the f-divergence is not novel, see, for example, f-GANs [5]. I wonder if such approximations have already been used in Bayesian machine learning (BML)? I encourage the authors to include a detailed discussion of this. Why is this important? I am not an expert in BML, but it seems to me that this particular variational approximation can be applied to a dozen of tasks involving KL divergence. The current paper demonstrates that it outperforms direct computation. If this is indeed true, why hasn’t this approach been used for other BML tasks? If it has already been, the authors should include relevant references to support the current experimental findings.\n\nOverall, the paper should more carefully acknowledge the prior work and make it transparent what is new and what is already well known. Paper [4] also seems relevant.\n\n**Experiments.** The experimental section is very weak both in terms of quality and quantity. In terms of quantity, the paper looks poor compared to predecessors, e.g., [2], a paper to which they compare their method. In particular, the results provided in Figure 2 visually suffer from notable artifacts, which is a contrast to that reported by [2] in a similar setup (Figure 2 of [2]). Besides, the dimensions considered in the current paper are lower, which is suspicious.\n\nImportantly, the quantitative results provided in Figure 3 raise questions. How is it possible that on such a toy example (evolving Gaussian distributions -- linear pushforward maps grad psi), the variational methods so drastically outperform the direct computation (up to 10 times)? This is quite unbelievable, in particular, in small dimensions. Did the authors use the same network architectures and other shared hyperparameters for comparison? A discussion here is needed.\n\n**Scope.** If I correctly understand, the authors do not provide any high-dimensional applications of WGFs. Due to this, I currently tend to think the scope of the paper is narrow and the usefulness of the proposed approach for the community of ICLR is questionable. Adding an application would definitely benefit the paper. \n\n**Correctness of the method.** Overall, the method is correct. However, in one of the experiments, the authors approximate the pushforward of the JKO step directly as the neural network, not as the gradient of the input-convex network. In this case, the push forward distribution might not have density damaging the entire JKO scheme.\n\n**Clarity.** What for the FB scheme is introduced (section 3.3)? This is not clear from the text.\n\n**References**\n\n[1] Benamou, J. D., Carlier, G., Mérigot, Q., & Oudet, E. (2016). Discretization of functionals involving the Monge–Ampère operator. Numerische mathematik, 134(3), 611-636.\n\n[2] Mokrov, P., Korotin, A., Li, L., Genevay, A., Solomon, J., & Burnaev, E. (2021). Large-Scale Wasserstein Gradient Flows. arXiv preprint arXiv:2106.00736.\n\n[3] Alvarez-Melis, D., Schiff, Y., & Mroueh, Y. (2021). Optimizing Functionals on the Space of Probabilities with Input Convex Neural Networks. arXiv preprint arXiv:2106.00774.\n\n[4] Bunne, C., Meng-Papaxanthos, L., Krause, A., & Cuturi, M. (2021). JKOnet: Proximal Optimal Transport Modeling of Population Dynamics. arXiv preprint arXiv:2106.06345.\n\n[5] Nowozin, S., Cseke, B., & Tomioka, R. (2016, December). f-gan: Training generative neural samplers using variational divergence minimization. In Proceedings of the 30th International Conference on Neural Information Processing Systems (pp. 271-279).",
            "summary_of_the_review": "My overall impression of the paper is that it is unfinished. While the idea of variational approximation is reasonable, I suppose this paper requires a major revision with a dozen text improvements and experiments. Therefore, I vote to reject this paper in its current form.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}