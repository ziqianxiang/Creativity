{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The authors analyze linear regression with gaussian covariates in an\nasymptoptic setting, where the number of examples and the number of\ncovariates go to infinity together.  They identify conditions\non the covariance under which \"multiple descent\" occurs, and\nconditions under which a regularization removes this effect.\n\nConcerns were raised that the overlap between this paper and previous\nresearch was too substantial for it to be published in ICLR.  These\npersisted after the authors' response and the discussion period."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the generalization risk of ridge and ridgeless linear regression under Gaussian data and Gaussian noise settings. The limiting bias and variance are described in closed-form. The analysis leverages random matrix theory and CGMT. The authors show that sample-wise multiple descent phenomenon can take place or be avoided if optimal regularization is applied.",
            "main_review": "The paper is well-written and technically sound as far as I can tell. I enjoy reading Theorem 1, which uses two different approaches to derive the same limits in a way that can mutually corroborate each other, and believe this closed-form result will have future impact on other topics. The theoretical result is further supported by clean experiments up to high precision. I believe it is a mathematically strong paper that is among top quality in this venue.\n\nHowever, I have reservations regarding the motivation, at least it is not nicely explained.\n1. As the title suggests, the authors are working on sample-wise multiple descents. They made this clear in the INTRODUCTION by citing (Nakkiran et al.,2021;Chen et al., 2020b; Min et al., 2021). However, I believe the authors are working on something else by their setup in section 1.1 'Asymptotic Regime' as $n,d\\to\\infty$. In words, not only the sample size goes to infinity but also the feature size goes to infinity in proportion. First of all, this is not sample-wise descents in prior works, where $d$ is fixed! A better description of this work is on sample-feature-wise multiple descents. Second, this seems to limit the application as in most tasks the number of features is fixed. Whether the theoretical results in this work shed any light on real cases needs at least empirical evidence.\n2. Can the authors give some motivation on why to assume $\\|\\Pi_i\\theta^*\\|_2→\\eta_i$. I understand its use in theory but what does this assumption mean in words?\n\nOther minor points: the paper seems very sloppy, e.g. Page 4 'Bias-Variance Decomposition of Expected Excess Risk', the first line of equations misses a square; in CONCLUSION section, the first sentence should have ridge regression besides ridgeless; in the same section, \"risk curve.Moreover\"->\"risk curve. Moreover\"; there are multiple places where the lines are out of margin, e.g. Equation (8); the appendices are not well-referenced, e.g. after reading Theorem 1, I tried to look for the proofs but there is no hyperlink that should link to Appendix C. Same thing happens to other results wherever appendices should be clearly referenced.",
            "summary_of_the_review": "This paper is technically very strong and well-supported by experiments. I believe it is among top 20% of accepted ICLR papers. However, the motivation of the problem is less clear which requires non-trivial improvement.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Authors study the generalization risk of ridge and ridgeless linear regression. It is assumed that the data follows a multivariate normal distribution and there is also an assumption on the spectrum of the covariance matrix. Among the contributions, the authors show a formula for the limiting bias and variance, show that sample-wise multiple descent happens when the covariance matrix is highly ill-conditioned, and study optimal regularization.",
            "main_review": "Strengths\n* The paper is well written\n* Technical contributions are sound and somewhat interesting\n\nWeaknesses\n* Some assumptions are unclear if they are mild or why they are assumed in the first place. For instance, assuming fixed eigenvalues looks like a rather strange assumption at first glance, and authors do not provide any intuition why this is assumed and how restrictive it is. To me, that assumption is an artifact for the proof techniques, which undermines the contributions of the paper.\n",
            "summary_of_the_review": "After my first read of the paper, I am recommending a weak accept. My concerns are more about the assumptions and the lack of discussions about them. I would appreciate it if the authors can comment on this matter.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper builds on a very recent line of work studying the “multiple descent” phenomenon. \nMultiple descent is known to occur along many axes: (i) number of parameters, (ii) train time, (iii) amount of train data. The current paper focuses on studying the theoretical rates for (iii) in the asymptotic regime where the number of samples and the dimensionality of data tend to infinity at similar rates.  This covers both under and over-parameterized settings.\nIn this limiting regime, the main results of the paper are:\n1.\tComputing formulae for the excess risk of the least squares estimator and the ridge regression estimator in a specific setting.\n2.\tUsing 1, they show that if the covariance matrix of data is ill-conditioned, then multiple descent can happen.\n3.\tThey also show that under an assumption on the scaling of the projections of the true parameter to the eigen-spaces of the eigenvalues of the covariance matrix, an optimal amount of regularization ridge regression estimator avoids multiple descent even in anisotropic settings which is novel.\n\n",
            "main_review": "Strengths of the paper:\n1.\tThe paper tackles an important problem very relevant to the ICLR community.\n2.\tIt provides a deep theoretical analysis of a specific setting and the proofs appear non-trivial. The main contributions of the paper are novel.\n\n\nWeaknesses of the paper:\nOverall the paper is reasonably well-written but the writing can improve in certain aspects. Some comments and questions below.\n1.\tIt is not apparent to the reader why the authors choose an asymptotic regime to focus on. My understanding is that the primary reason is easier theoretical tractability. It would help the reader to know why the paper focuses on the asymptotic setting.\n2.\tIt is unclear in the write-up if sample-wise descent occurs only in the over-parameterized regime or not. Pointing this explicitly in the place where you list your contributions would help. More broadly, it is important to have a discussion around these regimes in the main body and also a discussion around how they are defined in the asymptotic regime would help.\n3.\tThe paper is written in a very technical manner with very little proof intuition provided in the main body. It would benefit from having more intuition on the tools used and the reasons the main theorems hold.\n4.\tGiven that prior work already theoretically shows that sample-wise multiple descent can occur in linear regression, the main contribution of the paper appears to be the result that optimal regularization can remove double descent even in certain anisotropic settings. If this is not the case, the paper should do a better job of highlighting the novelty of their result in relation to prior results.\n\nI am not too familiar with the particular techniques and tools used in the paper and could not verify the claims but they seem correct.",
            "summary_of_the_review": "The problem of multiple descent is an important scientific question which is relevant to the ICLR community. The contributions of the current paper are novel. The techniques used appear novel as well. However the degree of significance of the results is not fully clear. The paper would benefit from a deeper discussion around this.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Derived the asymptotic generalization error (squared loss) of the minimum $\\ell_2$-norm interpolator under general Gaussian design. The precise asymptotic formulas reveal that the risk curve can be non-monotonic and exhibit multiple peaks (in the variance term), depending on the spectral properties of the feature covariance matrix $\\Sigma$. ",
            "main_review": "This authors devoted a lot of effort to rigorously prove the generalization error formulas in the proportional limit.  \nUnfortunately, my impression is that most of the theorems in this submission are essentially re-derivation of existing results from the following papers (some of these prior results actually allow for weaker assumptions):   \n* Dobriban and Wager, Annals of Statistics 2017. https://arxiv.org/pdf/1507.03003.pdf. \n* Wu and Xu, NeurIPS 2020. https://arxiv.org/pdf/2006.05800.pdf. \n* Richards et al., AISTATS 2021. https://arxiv.org/pdf/2006.06386.pdf.\n* Amari et al., ICLR 2021. https://arxiv.org/pdf/2006.10732.pdf.  \n\nNote that all of these papers have been publicly available for more than one year, so I think it is no longer the case that this new submission can be considered as concurrent work. \n\nSpecifically,   \n1. in terms of the risk formula, [Wu and Xu] and [Richards et al.] derived the limiting risk under the same problem setup for general regularization parameter $\\lambda$, whereas [Amari et al.] focused on the bias-variance decomposition in the ridgeless limit. All these works built upon a generalized Marchenko-Pastur theorem, as well as [Dobriban and Wager], which showed that the limiting variance can be related to the Stieltjes transform and its derivative (sometimes referred to as the \"derivative trick\"). \nThese results cover Theorem 1 and 2 in the current submission. \n2.  The two block ($m=2$) case was already studied in Section 3 of [Richards et al.], Appendix A.4 of [Amari et al.], and Section 5 of [Wu and Xu]. In particular, the closed-form expression of the Stieltjes transform (and therefore the limiting variance) has been derived in Appendix A.6 of [Richards et al.] and Appendix D.10 of [Amari et al.], and the resulting \"multiple descent\" has also been reported. These results cover Theorem 3 and Corollary 2 in the current submission. \n3. Theorem 4 in this submission is analogous to Proposition 7 in [Wu and Xu], which states that under an isotropic prior on the true parameters $\\theta^*$, the optimally-tuned ridge regression estimator has monotonically decreasing generalization error with respect to $\\gamma=n/d$, and thus avoids multiple descent; this is basically equivalent to the projection formulation in Theorem 4 (the bias term in the generalization error only depends on the projection of $\\theta^*$ onto the eigenspace of $\\Sigma$).  \n\n\n**Additional Comments** \n\n- At a high level, I personally do not find counting the number of peaks in the risk curve to be that interesting of a research problem, as least in the case of minimum $\\ell_2$-norm interpolation. It is rather intuitive that one can engineer the features such that the sample covariance matrix becomes ill-conditioned once in a while, and it is not clear if this specific phenomenon has wider implications in practical machine learning problems. \n\n- (minor) Note that the Gaussian assumption has already been relaxed to bounded moments in the Stieltjes transform-based analysis, e.g., [Dobriban and Wager] and [Wu and Xu]. Hence I'm not sure if there's any benefit of introducing CGMT to this problem (which limits the analysis to Gaussian features). \n\n- (minor) In Corollary 2, why is $\\varrho\\to\\infty$ a well-defined limit? Intuitively, wouldn't this violate the condition that the eigenvalues do not depend on $d$ which goes to infinity?",
            "summary_of_the_review": "Due to the significant overlap with prior results, I cannot recommend acceptance. I'm happy to adjust my score if the authors can clarify how the current submission differs from (and improves upon) all the aforementioned papers. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}