{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper provides a way for explaining the reasoning of a neural network to humans in the form of a class-specific structural concept graph (c-SCG). The c-SCG can be modified by humans. The modified c-SCG can be incorporated in training a new student model. Experiments show that the new model performs better on classes that their corresponding c-SCG have been modified. While all the reviewers agree that the paper puts forth an interesting idea, some concerns have been raised by reviewers about the scale of experiments and the lack of theoretical guarantee on the fidelity of the SCG. The authors have added two large scale experiments which confirm their previous results as part of their rebuttal. This paper is borderline and needs to be discussed further."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper extended VRX (Ge et al. 2021) with an interface to allow human modify the class-specific structural concept graphs (c-SCG) as well as a procedure to distill the human changes in the c-SCGs back to original task's neural networks. This is an interesting piloting idea. However, the scale of the experiments is not convincing enough to demonstrate the generalizability of the performance to a large number of concepts, classes, datasets and complex scenarios. ",
            "main_review": "Strength\n\n\t1. A new framework to connect the explainable neural net paradigm to a paradigm that can make use of the explanation to allow human intervene the neural net models. A success in this direction will open up a new era of data-driven machine learning in a long run. \n\nWeakness\n\n\t1.  The fidelity of the SCG lacks theoretical guarantee or at least lack systematic after-math accounting means. This is inherited from VRX (Ge et al. 2021) and the authors did not address it in this paper except for human review.\n\t2. Similarly, there is a need for high fidelity accounting  on human-to-nn path regarding whether the distilled NNs truly reflect the  human's modification on the reasoning of NN explanation. Indeed, in page 8, deleting all edges are used to train GRN with the new c-SCG. \n\t3. The experiments are conducted at the scale of 200 images and 20 classes. It is not clear whether the proposal will still observe the same performance gains for a large scale datasets and a large number of classes.\n\t\nDetails:\n\n\t1. Have you tried a large dataset with a larger number of classes? Please do ablation study at scale to understand the scaling properties of your approach. \n\t2. Page 8, how \"deleting\" all edges compares to the original modified c-SCG? Why wasn't it working in the first place? Does it indicate that the c-SCG graph itself did not convey the right format or the right logic in full which can recover the NN behavior in the original tasks? This is worrisome as it might meant that the original VRX (Ge et al. 2021) might have fidelity flaw in its methodology in the first place. \n\t3. For zero-shot experiments, 4 seen classes and 1 unsee classes over a 192 training images per seen class is too small a dataset to reveal the possible success or weakness of the approach. \n",
            "summary_of_the_review": "This might be a game-changing direction for the future however the paper as it is now is lacking solid ground and mature enough progress in this direction. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper provides a way for explaining the reasoning of a neural network to humans in the form of a class-specific structural concept graph (c-SCG). The c-SCG can be modified by humans. The modified c-SCG can be incorporated in training a new student model. Experiments show that the new model performs better on classes that their corresponding c-SCG have been modified.",
            "main_review": "Whenever we train or fine-tune a neural network if it's going to be used in an application that is safety-critical the resulting network must go through a thorough investigation to make sure that it performs as expected in all classes and this would be a time-consuming process. Whenever we train a new student network, there are many things that could go wrong for example a bad learning rate, bad choice of hyper-parameters. Investigating the accuracy and behavior of the neural network for unexpected behaviors, vulnerability to adversarial attacks, etc. is time-consuming. Every time we make a small change in the decision-making process of one of the classes using HNI, such a verification process should be repeated. This sounds a little impractical. \n\nI think the application of the proposed method can be better justified for training neural networks when there are not enough training samples or in zero-shot learning. The paper has provided experimental results related to such scenarios which is good.\nModifying nodes (adding or removing concepts) seems to be much more intuitive than modifying edges. I found that modifying edges is not very intuitive. For example, in section 4.1.2 it is not clear how a human should know that's removing all edges will result in removing the effect of pose.\n",
            "summary_of_the_review": "The neural network-to-human path heavily overlaps with (Ge et. al 201). I think the novelty of the work is mostly in the human-to-NN path. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes an interactive learning algorithm for image processing tasks based on structural explanations. The paper therefore reuses recent advances in creating structured image explanations for image classification and uses knowledge distillation to update the model after end-users manipulated them.\n\nThe approach is evaluated for robustness / predictive improvement and zero-shot learning on known datasets (ILSVRC2012 and iLab-20M, object dataset). The baselines are the non-interactive system and a system with more training data per class - each with the same architecture. The results show that the proposed approach generates better results after human corrections and can be used for novel classes.",
            "main_review": "The paper proposes a sensible approach for the relevant problem of interactive learning for image classification. While there are several interactive methods for CAM-based explanations, it is interesting to explore this direction for structured explanations.\n\nThe related work section covers relevant papers for human-in-the-loop learning, but is quite sparse. There are numerous other works with similar goal as the cited paper [1], also for other data modalities such as text. In addition, several other works exploit scene graphs user interaction (e.g. [3]) for different, but related goals (here image generation), which might be relevant for this section. \n\nGiven that the paper is titled “generic interface”, it would be required to cover more modalities. This might be an argument for the whole introduction / motivation of the paper, as only images are used to explain and evaluate the method.\n\nThe rest of the related work section mostly covers “components” that the approach is built on (i.e. image explainers, GNNs, Knowledge distillation) with the exception of zero-shot learning, which could be remedied for overall readability. A possibility would be a designated background section. The problem continues in the main part of the paper, where the reused explanation method [2] is cited but not clearly distinguished from the contribution (“NN-TO-HUMAN”).   \n\nThe evaluation results are interesting and promising. However, a lot of useful information are buried in the end of the suppl. material. This is cumbersome, as these are basic information about dataset origins or user study setup. More specifically, it would be very helpful to add a short explanation for the origin of the object dataset in evaluation setup and clearly define the user study before presenting the results. To this end, did the mentioned user study only cover the qualitative questions about the experience with the system or also the actual manipulation of structured explanations for the quantitative results? It is not clear to me from the sentence.\n\nSimilarly, it is vital to motivate why the evaluation suffices for the paper claims, which is also not part of the main paper. The suppl. Material contains limitations of the current approach and evaluation, which cover parts of this.\n\nFor the zero-shot learning experiment, it would be insightful to add a representative zero-shot baseline to better assess the added value of human intervention. While it is good to show that the method can generalize in the block example, there are established zero-shot benchmarks which could be used at this point. \n\n[1] Li, K., Wu, Z., Peng, K.C., Ernst, J. and Fu, Y., 2018. Tell me where to look: Guided attention inference network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 9215-9223).\n[2] Ge, Y., Xiao, Y., Xu, Z., Zheng, M., Karanam, S., Chen, T., Itti, L. and Wu, Z., 2021. A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2195-2204).\n[3] Mittal, G., Agrawal, S., Agarwal, A., Mehta, S. and Marwah, T., 2019. Interactive image generation using scene graphs. arXiv preprint arXiv:1905.03743.",
            "summary_of_the_review": "The paper tackles a relevant problem for the conference. While the empirical evaluation is rather insightful for image processing, the paper is motivated for any data modality, but the claims are not backed up by evidence. The paper could also improve the readability / clarity of presentation.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a technique for allowing humans to interact with NNs.  It starts by approximating a NN by extracting concepts from an image, using those concepts to form a graph, and then putting that graph through a GNN.  Then, it allows users to edit these \"concept based graphs\" and uses those edited graphs to retrain the GNN.  Experimentally, this is shown to improve performance in three different experiments.  \n",
            "main_review": "Strengths:\n-  The idea is an interesting one, the paper does a good job of building on prior work, and the methodology seems sound.  \n-  The results show that this method is useful in fairly low-data regimes.  \n\nWeaknesses:\n-  Overall, the experiments don't do a great job of higlighting how the method could be useful:\n\t-  While low data regimes may be interesting, all of the ones studied in this paper are \"artificial\" in some sense:  4.1.1 -> sub-samples ImageNet, 4.1.2 -> sub-samples (in a deliberately biased way) iLAB-20M, 4.2:  uses synthetic data.  \n\t-  The paper focuses on human interaction/intervention, but the interventions in 4.1.2 (deleting all edges) and 4.2 (automatically generate new I-SCGs) are both algorithmic and seemingly could be done (or at least tried) based soley on domain knowledge without seeing the C-SCGs/explanations.  ",
            "summary_of_the_review": "\nOverall, the reviewer think that this is a ok, but not great, paper.  With a more realistic experiment (ie, with a naturally occuring dataset and interventions from users based on the explanations), the paper would be much stronger.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}