{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper introduces a technique to measure the *expected* robustness of a\nneural network by measuring the probability random input perturbations will\ncause the model to make a mistake.\n\nThe reviewers are not convinced by the results in this paper. The methods\nare not carefully evaluated against prior work, and it is not exactly\nclear what lesson one can draw from the resulting statistical evaluation.\nThe experimental setup is not clearly explained in several places, making the\npaper difficult to fully follow.\n\nSince the authors do not respond to the reviewer concerns, there was no\nopportunity to address these concerns."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Present a method to measure the expected robustness of a neural network model, by determining  the probability that a random input perturbation might cause misclassification, providing formal guarantees regarding the expected frequency of errors that a trained model will encounter after deployment. The method can be applied black-box. Applied the approach to compare the robustness of different models, and measure how a model’s robustness is affected by the magnitude of input perturbation. ",
            "main_review": "While the paper appears quite novel and interesting, I did not feel the text reflected the abstract as a whole. The paper was quite opaque, and I remained unconvinced that the method if applied in practice would provide useful insights. \n\n- Needs proofreading, the repeated citations in the related work section is distracting. \n- Additional motivation for Definition 3 is warranted. Can the authors present a practical motivating example, which does, or could believably, occur in reality?\n- Grammatical errors: \"we need to measure how many inputs are in the $\\epsilon$-ball around $x_0$ are adversarial\"\n- \"To illustrate this point, we trained a VGG16 DNN model,\" on what dataset? with what training parameters? These details should at least be put in the appendix. How exactly are the perturbations generated, what attack, what attack parameters?\n- While I appreciate the acknowledgement that most often the adversarial inputs around a datapoint are not normally distributed, the example in Figure (2a) is very close to normal. I would hypothesise that a wide variety of distributions are possible given different parameterisations of  dataset, downstream task, label distribution, architecture, training parameters altogether. On ImageNet, if the perturbations are generated by non-targeted attacks, it is significantly easier to induce misclassification via related categories (different types of dogs) than classes like \"frogs\". Altogether, I am unconvinced that the figures as presented correspond to expected scenarios in reality, and even if so, I expect the variance across datasets and training parameters to be high. \n- Does the Box-Cox transformation preserve the statistics of the distribution, what statistics are and aren't preserved? From a functional perspective, can the transformation be considered bijective, can one reverse the Box-Cox transformation and recover the original distribution. One can transform one distribution to an entirely different distribution, but that isn't interesting if the transformed distribution does not reflect the original distribution in any unique way? This should be clarified in the text. ",
            "summary_of_the_review": "Recommend rejection as the paper did not demonstrate its utility in practice, nor corroborated their abstract in a clear manner. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents a statistical method - Robustness Measurement and Assessment (RoMA) to measure the expected robustness of a neural network model. The robustness is defined as the probability that a random input perturbation causes an incorrect prediction. The presented approach is a blackbox approach. Different output labels are observed to exhibit different robustness values. \n\nThe basic premise of the paper is that the adversarial perturbations are not naturally normal, but a transformation (Box-Cox) can be applied to make them a normal distribution before applying statistical estimation techniques (Anderson-Darling test + z score). ",
            "main_review": "Strengths:\n\n+ The paper addresses the very important challenge of characterizing robustness of deep learning models.\n\n+ The described approach is simple - transform the neighborhood confidence c_i values to a normal distribution  and then estimate the z-score.\n\nWeaknesses:\n\n- Some observations are rather obvious and well-known. Different classes exhibit different robustness. Confusion matrices have been used widely to understand these class-conditional error distributions. The reviewer is not convinced that \"categorical robustness\" is a new notion. \n\n- The decision of normalizing c^i on sampled input appears arbitrary. How do we decide what inputs to sample? The model is likely going to be robust in some part of the input space and not robust in another. Sampling inputs on which the perturbation is applied appears a crucial decision. Evaluating the model in just the neighborhood of the training data is not very useful. \n\n- Could authors provide more details on how lambda was selected? The paper states that \"Selecting the parameter λ in often performed using the maximum-likelihood estimation (MLE) method. In this method, λ is chosen by heuristically maximizing the goodness-of-fit score of the resulting distribution, so that it most closely resembles a normal distribution.\" This appears to describe the usual practice. In this paper how was λ selected? Was a specific tool used to implement this?\n\n- \"An analysis of the results indicated that very small or very large \u000f values more often led to failures, whereas mid-range values more often led to success. We speculate that this is because very small values lead to almost no adversarial inputs — i.e., the resulting distribution of adversarial inputs is close to uniform, and is consequently impossible to normalize. A similar situation occurs for very large \u000f values, which introduce a large number of adversarial inputs distributed uniformly\". Could authors clarify what is meant by uniformly distributed? It is not intuitively obvious why adversarial examples would be uniformly distributed in the input space. The decision of the model would be more sensitive with respect to some features (or pixels) than others. Also, given the observations that the technique only works on some mid-range values of epsilon. Could authors comment on how this limitation would impact its practical utility? \n\n- A minor suggestion - forward references such as \"explained later\" should be avoided. Putting a citation for standard textbook approaches such as goodness-of-fit tests would suffice. ",
            "summary_of_the_review": "The paper presents a simple approach to characterize robustness. Some limitations of the techniques need to be clarified for the reviewer to understand the contribution and limitations of the proposed approach.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes RoMA, a robustness evaluation framework based on local sampling and probability computation.\n\nThe main contributions are:\n1. Proposal of the ($\\epsilon$,$\\delta$) local robustness score for assessing the probability of random local samples that have different predictions than a given data input with a $\\delta$-confined top-1 confidence.\n\n2. Use of Box-Cox transformation for input data to improve statistical estimation.\n\n3. The method can be implemented in a model-agnostic fashion.\n",
            "main_review": "This paper has several drawbacks, which are detailed as follows.\n\n1. There are several existing methods that can provide similar probabilistic robustness measures. Although the authors emphasize that the proposed method uses random sampling to evaluate robustness and obtain a statistical robustness score. Similar ideas are already widely used in the literature. For example, randomized smoothing [R1], which is one of the most widely used certification techniques, already provides probabilistic bounds and confidence intervals for local robustness assessment. Moreover, [Weng 2019] considered the problem setting of a random perturbation to a local data sample. The resulting bound is also similar to the proposed local sampling technique. Since the idea is similar and the authors did not compare to those existing methods, I don't see many insights from the proposed method compared to existing works.\n\n2. The experimental results are not convincing. First, the authors did not provide any empirical robustness results to compare with the robustness evaluations. How can one know the reported results are correct and meaningful? Without rigorous justification and empirical evidence, the results could be questionable. Second, the authors did not compare to existing methods, which makes it difficult to assess the quality of the proposed method.\n\n\n[R1] https://arxiv.org/abs/1902.02918\n   \n",
            "summary_of_the_review": "The current version is quite incomplete. It lacks comparisons and distinctions to prior arts for technical novelty, and it also lacks comparisons to empirical robustness and other baseline methods.\n\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "Summary:\n\nThe paper introduces a statistical method to measure the robustness of deep neural networks. The novel \npart of the method is that it's designed to measure the probability of random points near an input being \nadversarial, instead of probability of adversarial examples existing in the vicinity of an input. To measure it, \nthe authors proposes to use Box-cox transformation to transform distribution of confidence scores to normal, \nthen calculate the probability based on it.\n\n",
            "main_review": "Pros: \n\n \n1. The paper tries to address an important problem in the field of adversarial machine learning: how to evaluate \nrobustness of deep neural networks efficiently, which is of great importance and has wide application.\n\n \n2. Experiments show that the proposed statistic decreases as epsilon (the distortion bound) increases, which shows \nthat the proposed statistic behaves expectedly. Since larger epsilon usually leads to weaker robustness (as the \nprobability of adversarial examples existing improves), a reasonable statistic that measures robustness should \ndecrease as epsilon increases.\n\n \n3. It's nice to see a comparison of robustness of different architectures. \nCode is provided for reproducing the experimental results. \n\n \n##########################################################################\n\nCons: \n\n1. Definition 3 is problematic. Based on the sentence before the definition \"..., which signifies the probability of \nrandomly perturbing $x_0$ into an input which is not a distinct adversarial input\", the robustness score should \nbe $1-$probability of perturbed input being an adversarial input. Thus, the definition of score should be:\n\\begin{align*}\nplr_{\\delta, \\epsilon}(N,x_0):=1-P_{x:\\|x-x_0\\|\\le\\epsilon}[argmax(N(x))\\ne argmax(N(x_0)) \\bigcap c(x)>\\delta].\n\\end{align*}\nThe definition in the paper does not satisfy the claim \"the closer this value is to 1, the less likely it is a random \nperturbation to $x_0$ would produce an adversarial input.\" Based on the original definition, the closer the value \nis to 1, the closer the value of $P_{x:\\|x-x_0\\|\\le\\epsilon}[argmax(N(x))=argmax(N(x_0)) \\bigcup c(x)<\\delta]$ is to 0, \nwhich makes the probability of robustness close to 0.\n\n \n2. The proposed method section is not clear. Based on the description of z-score calculation, my understanding \nis that the method is trying to measure $1-P(c(x)>\\delta)$, which is $1-$the probability of the maximum confidence \nscore of perturbed input greater than a preset threshold. Here're some problems of it:\n\n- In this section, it says that $x_0\\sim N(\\mu=0.473, \\Sigma=0.053^2)$. Isn't $x_0$ the original input? Why suddenly \nit follows a normal distribution? Based on the context, I guess it represents the distribution of maximum confidence \nscores of perturbed inputs. Another possibility is that it represents the distribution of maximum confidence scores of \nadversarial examples out of the perturbed inputs. If my understanding is wrong, please correct me. \n\n- If it represents the distribution of maximum confidence scores of perturbed inputs, why $1-P(c(x)>\\delta)$ can be used to measure robustness? $P(c(x)>\\delta)$ does not represent the probability of $x$ being adversarial. The probability of adversarial should be: $P[argmax(N(x))\\ne argmax(N(x_0))\\bigcap c(x)>\\delta]$. Where is the requirement of $argmax(N(x))\\ne argmax(N(x_0))$ reflected in the method?\n\n- Even if the authors compute the empirical distributions of maximum confidence scores based on only adversarial \nexamples out of the perturbed samples, it's only a conditional probability and the component of $P[argmax(N(x))\\ne argmax(N(x_0))]$ \nis still missing. Besides, estimating $P[argmax(N(x))\\ne argmax(N(x_0))]$ is a difficult problem. The probability cannot simply be estimated by number of adversarial examples in the perturbed inputs divided by total number of perturbed inputs, because we don't know how adversarial samples are distributed in the input space.\n\n3. For experiment 4.1, why not report plr scores of $\\epsilon<0.1$? On CIFAR10, a white-box attack like PGD with perturbation budget $0.03$ can reduce the accuracy to $0$. So, it would be better to report the plr scores for $\\epsilon<0.1$ as well.\n\n4. The paper uses \"adversarial inputs\" many times. In the abstract, \"adversarial inputs\" is defined as: \"small input perturbations that cause the model to produce erroneous outputs.\" However, later on, it is used to describe maximum confidence scores of adversarial examples. For example, in Figure 1, the caption is \"Normally-distributed adversarial inputs\". Based on the description, the histogram is generated with highest confidence scores assigned to any label other than original label. So, a better caption can be \"Histogram of highest confidence scores of adversarial examples of 10,000 perturbed images\". This happens several times, making the definition of adversarial inputs very confusing.\n\n##########################################################################\n\nMinor:\n\n1. \"CONFERENCE SUBMISSION\" should be removed from the title.\n\n2. Page 5, when calculating $plr_{0.6, 0.04}(N,x_0)$, the value $0.008288$ implies that upper tail value is calculated, but the integration represents lower tail.",
            "summary_of_the_review": "Reasons for score: \n\n \nOverall, I vote for rejection. I like the idea of measuring the probability of random inputs being adversarial \ninstead of finding the extreme case. However, I think whether the proposed method is measuring such \nprobability is questionable. Some notations and terms are unclear, so it is possible that I missed some \ncomponents. If the authors can address my major concerns in the rebuttal period, I'm ok to increase \nmy score and accept the paper.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}