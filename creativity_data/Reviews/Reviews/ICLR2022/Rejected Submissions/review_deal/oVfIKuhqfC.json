{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper introduces a new method for diffusion-based generative modeling through a Brownian bridge formulation, where the data and latent variable can be coupled. They extend their method to mixtures of diffusion bridges and spatially correlated processes that go beyond the factorial diffusion processes used in prior work.\n\nWe thank the authors for engaging with the reviewers and addressing many of their detailed concerns. While reviewers agreed that the proposed theory and methodology were novel and interesting, there are no small or large scale experiments or empirical comparisons to the relevant prior work. In the absence of theoretical justification (bound or proof) as to why the proposed diffusion bridge mixture transport method would result in better performance, more empirical comparisons and evaluations are needed. Additionally, several reviewers found the presentation confusing and overly complex, including the notation, writing, and figures. Given the lack of experimental results and concerns over presentation, I’m inclined to reject this paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper introduces a methodological framework for generative modeling through diffusion processes without time-reversal arguments. By utilizing diffusion bridges the authors consider processes that start and end in pre-determined points $x_0$ and $a_{\\tau}$. Then by extending this to mixtures of diffusion bridges they show how to transport a distribution $\\Pi_0 = P_D$ to $\\Pi_{\\tau} = P_Z$ without time-reversal of the diffusion process. For specific classes of SDEs by drawing connections to the paradigm introduced by Song et al 2021 and provide a unified view on the drift adjustment in forward and backward SDEs. This allows for defining a time and state-dependent class probability function (given by conditional expectation) providing further insight into the inner-workings of DTRT and DBMT. The authors then introduce two other training objectives $L_{\\text{FD}},L_{\\text{CE}}$ and discuss their favorable properties. Finally, methods for moving beyond fully factorial noise models are presented by extending the process and the noise term from functions of time to be functions of both time and space. This allows for incorporating \"priors\" with better spatial dependency models more suitable for specific datasets. ",
            "main_review": "The methods presented are clear improvements to existing literature relaxing the constraints of time-reversal diffusions and moving beyond fully factorial noise models. The flow of the writing is clear, supporting arguments are properly presented, and representative results are shown appropriately. In addition, a code package is provided by the authors in the comments for reproducibility purposes which is clean and well organized. Overall, I did not find specific issues in this paper and hence I do recommend this paper for publication. That said, I'm not an expert in this field and hence I'll leave some room for my lack of knowledge about the state-of-the-art and significance of the work.",
            "summary_of_the_review": "Writing is clear, problem is significant, and proposed methods are novel. I did not find specific issues in this paper (although I did not go through the details of appendix A).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper studies the Brownian bridge formulation for diffusion-based generative models, derives theory for it, and shows connections to earlier works. Some spatial developments are also discussed. This is a theoretical paper that only has rudimentary experiments. The overall idea of Brownian Bridge diffusion is interesting, but the idea’s merits are not shown. The paper suffers from high technical complexity making it difficult to digest. \n",
            "main_review": "Post-response update. The authors' addressed my technical comments, and revised the manuscript \nwith clarifying visualisations and algorithm boxes. The three main major issues with the paper nevertheless still stand: the presentation is overly complex; the motivation is not convincing enough; and experiments are anecdotal. \n\n------------\n\nThis is a theory paper with only minimal experiments. The paper analyses Song et al ’21 bi-directional diffusion model, and proposes to replace and/or generalise it with Brownian bridge formulation, which only moves in generative direction. The problem is somewhat poorly motivated and it is not entirely clear how significant are the problems with the bidirectional approach, or what are the advantages of the bridge. One would expect that the bridge formulation now has a major problem with the (x0,xtau) pairing: surely a poor pairing would lead to a poor model. \n\nThe paper derives the bridge formulation at length. The derivation of the method takes almost the whole paper, and I found it to be difficult to follow. The paper does a poor job of describing what is the final model (algorithm box would help). The paper is also filled with tons of math, but has little exposition or motivation on the higher level concepts or intuition. There are also almost no illustrations of the ideas. Ultimately, after several hours on this paper, I only could understand around half of the material. The reader is overburdened by notation. This is going to limit the paper to narrower audience and impact. The scope and contributions are not sufficiently clear.\n\nThe paper shows that Song’21 method can be seen as special cases of covariant Brownian and OU processes. This seems like a side-result and not particularly useful, but is nevertheless interesting. The paper also presents some discussion of spatialization of the generative process with Gaussian processes. This is again interesting, but feels incomplete or more of a sketch. There are only very tentative results regarding this. \n\nThere are no large-scale or exhaustive experiments. THis is somewhat surprising: the proposed method seems like a variant of Song’s method, and I wonder why no comparative experiments were done. Perhaps the authors run out of time. I feel that the theoretical results alone are not sufficient for publication: from SDE perspective changing a reversible SDE to Brownian Bridge is rather incremental, and thus experimental demonstration would have been necessary to make the paper strong enough for publication.\n\n\nTechnical comments\n* It seems that “terminal distribution” is at time 0, while “initial distribution” is at time tau. Yet, in (i) in 1st paragraph the initial time is 0, and this is for observed objects. In (1) in 3rd paragraph the time 0 for latent base object. These are confusing, and needs clarification. A figure would be helpful.\n* Sec 1 “Firstly..”. “Q_tau retains dependency on P_D”. This could be non-true: if forward process mixes, then it will become oblivious of the initial distribution. This is the goal of density destructors: to lose all information of the original distribution. “The exact reverse-time simulation..”. I can’t understand this sentence. Why are we starting from Q_tau, and not Q_0? I also don’t see what approximation does this “firstly” subparagraph talk about, what was the problem again? Try to clarify\n* Secondly: why is it a problem that neural network approximates diffusion process? Is the problem that it introduces bias, or that it has some residual? Don’t we always need to approximate the diffusion if we don’t have access to the true one? Text also claims we have known reverse time dynamics known, surely this can’t be true? If we know the reverse time dynamics exactly, then why are we learning it? Try to clarify the problem and motivation of this point. \n* Thirdly: this is very clear\n* What is x0? This has not been defined, so I don’t understand if this is supposed to be a real object, a random noise object from base distribution, or something in else. It also seems that in 3rd paragraph the forward direction is now generative direction, while previously it was destructive direction. Please clarify to reader. \n* I generally can’t follow the 3rd paragraph. The text talks about DDPM, DBMT, and redefines DDPM as DTRT. There is lots of detail, but not enough background to understand. It seems the main problem is that Q_tau depends on P_D, but it is unclear why is this a problem. \n* In 4th paragraph the “drift adjustment” is unexplained. It is also implied that it is bad that the adjustment depends on data. Surely everything depends on data? I fail to see what is the argument here\n* The f/g are discussed, but not explained\n* Sec 2. “u” is often used in PDEs to denote solutions, but here its time. This can introduce unnecessary confusion for some readers.\n* I can’t follow what transition density q_u|u’(y|x) means. y is undefined (so is actually x as well). What does transition density mean? Is it a conditional marginal distribution or something else? Please define\n* Before eq 2 we first define q_u to be marginal density, while then eq2 redefines the density as a finite mixture. Surely marginal density should refer to \"fully\" continuous density, while eq2 should then be an approximation of the marginal density, or the N-empirical marginal density?\n* In eq 3 we define X_0 ~ P_Z, while in eq 1 we define X_0 ~ P_D. This is confusing\n* Eq 4 is confusing. The first line seems to be a mixture density, but second line Q_0,u is undefined. Does htis equation just state that one can do MC to sample from a mixture density? What is dX_u?\n* Eq 8: how does one do the pairing between x0 and xtau^{(n)}, or is there any pairing?\n* eq 14: \\Gamma looks like a function here, is it?\n* eq 21: weights \\omega are introduced, and it seems that they are central to the method. Yet, they are not part of any of the equations. They need introduction and needs to be connected to the presented theory\n* Sec 4. I got lost here. There is tons of math and theory here, which looks simple enough to follow, but little of this is motivated and the reader gets lost on where are we going with this. The presentation remains on a very technical level, not exposing the underlying concepts and intuition. Not all math needs to be understandable by a reader, but even a casual reader should understand why the math is there or what it achieves. Here I don’t see why the Song’s method is reinterpreted as the two processes 9+10. Does this relate to the rest of the paper or the proposed method? The presentation is also difficult to follow since a lot is left unexplained intentionally (eqs 15..17). The authors need to describe the motivation and role of the math, and also need to conceptualise the theory by helpful descriptions (or figures). \n* Sec 5. I’m again lost here. So eq 14 is applied to 8 to get 20. But 14 applies to Brownian and OU processes, while eq 8 is a bridge process. Why would the result hold? Furthermore, eq 8 gives a quite complex mixture model, which again seems very different from eq 9 or 10. It seems odd that to derive a mixture model 8, we take a detour to a different (and very simple) model family. Can’t we derive the eq 14 directly from eq 8? \n* Furthermore, the full method proposed in the paper is difficult to follow, since its description is scattered in many places. It’s difficult to go back-and-forth between eq 8 and 20. Later one needs to go back-and-forth between eq 23, 20 and 8 again. The description is also not complete yet, since \\Pi, v(), a(),etc are not defined. Because of these challenges, I then couldn’t follow the paragraphs after eq 21. \n* Where is the \\omega coming from? What is Y? What is E[Y]?}) It seems that method becomes some kind of weight-tuning algorithm, but weights are completely absent from any equations. Including an algorithm block would be helpful. \n* It seems that E[Y_t|x,t] becomes a central term later in the paper, but they are not properly defined. In which way they refer to the expectation terms in 20+21? Please define. \n* It would be very helpful to give the \\sum_n=1^N version of eq 8, since it seems that the proposed theory in the paper is based on the mixture view (or is it?)\n* Figure1: Generally I don’t understand what this experiment is doing. I can’t understand what happens on top. What is “all samples x^{(n)} in CIFAR”? This should be 50k points I assume, but it seems we only visualise around 10 curves. What does “true score” mean? How does one get the “true” score, and what does the score mean? What does Euler(1000) mean? What are the colors? Also at bottom I can’t understand what is \"true score”, and how come the score is an image, it was said to be a “single weight” above, so one expects it to be a scalar? The caption says that we track scores, but these look like images instead. Second and third rows are trained models, but what is then first row (is it untrained?). If we follow here E[Y], then should these images be some kind of average trajectories? The text describes three stages, but I can’t see where these come from (or what they are). I’m again having lots of trouble understanding the fig1 explanations in sec 5 since it’s difficult to follow what the figure shows.\n* Eq 25: why would matching s≈X result in s≈E[Y]? These seem like very different targets. \n",
            "summary_of_the_review": "While the theoretical derivations and the idea of Brownian bridge are interesting, and potentially a breakthrough, I feel that the theoretical results alone are not sufficient for publication. Changing the reversible SDE to BB is still somewhat incremental proposal, and thus the experimental demonstration would have been necessary to make the paper strong enough for publication. Furthermore, the presentation of the paper not good enough for a publication.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper the authors introduce a new model for diffusion-based generative\nmodelling.  Instead of relying on time-reversed processes as in existing works,\nthey propose an approach based on the mixing of diffusion bridges. They show\nthat a mixing of diffusion bridges remains a diffusion with closed-form drift\nand volatility. Similarly to existing score-matching based generative modelling\nworks the drift in this new formulation can be expressed as a conditional\nexpectation and therefore can be seen as the minimizer of some loss function,\nallowing the use of neural-network based approximations. In addition, the\nauthors propose some unification of the SDE classes used in [1] and to use\nnon-identity volatility matrices.\n\n[1] Song, Sohl-Dickstein, Kingma, Kumar, Ermon, Poole -- Score-based Generative Modeling through Stochastic Differential Equations",
            "main_review": "STRENGTHS:\n\n-The paper is well-written. The mixture of diffusion bridges is clearly\nmotivated and the theoretical results seem to be correct.\n\n-The idea of using diffusion bridges (and mixture of diffusion\nbridges) to build a diffusion-based generative modelling is original and\ninteresting. One of the aspect of this work that I find promising is that in\nthis setting is that both the terminal and initial distributions are pinned down\ncontrary to existing works.\n\n-Related to this idea, remarking that a mixture of diffusion remains a diffusion\nwhose coefficients can be computed is an interesting point to raise and might be\nuseful in other settings. The fact that the resulting drift can be approximated\nusing score-matching techniques is also interesting.\n\nWEAKNESSES:\n\n-There is not enough experimental evidence. In particular, even though comparing\nthe approach of the authors with classical time-reversed methods might be\ncompute consuming on image synthesis tasks the authors should have provided\ncomparison on toy examples. As of now it is not clear at all that diffusion\nbridge based generative modelling is superior or comparable to time-reversed\ndiffusion based generative modelling.\n\n-Also, the authors should compared their methods with other works which also pin\ndown the initial and terminal distributions such as [2,3]\n\n-There is no theoretical justification why diffusion bridge based generative\nmodelling might be better than the classical time-reversed one. I understand\nthat the fact that the initial and final distributions are pinned down might help but it\nis not clear to me what kind of theoretical result could be obtained.\n\n-Overall I think that the presentation of the paper could be improved if the\nauthors focused on their core contribution which is the use of mixture of\ndiffusion bridges for generative modelling. I found Section 4 \"SDE Class\" and\nSection 6 \"Transports approximation\" to be a bit disconnected from the rest of\nthe paper. I would have preferred to see more experiments/comparisons on\ndiffusion bridge models or a more detailed theoretical analysis of these models.\n\nCOMMENTS:\n\n-The authors claim that every approach for score-based generative modeling is\n based on time-reversal. I slightly disagree in the sense that [5] never\n explicitly writes down the time-reversal of the forward process but rather find a parametric expression of some backward kernel which is only loosely connected to time-reversal.\n\n-I understand that the authors don't want to clutter the paper with too many\ntechnical results on the existence/uniqueness of SDE and the\nexistence/positivity of their densities but I think that it would be worth to\ncheck that these assumptions are satisfied in some generative modelling setting.\n\n-I found the experiment presented in Figure 1 to be really interesting but I am\nnot sure I fully understand what is done here. How do the authors compute the\nconditional expectation in Equation (20) and (21)? Is the term in (20) and (21)\ncomputed using score-matching as in [1]? If so, I'm a bit confused at the\nfinding obtained at the authors. It would seems that the process concentrates on\na single point and therefore that there is no innovation in the generative\nmodelling process? However it has been observed that score-based generative\nmodelling is innovative, see Figure 7 in [4] for an ImageNet experiment. Also,\ncould the authors precise how they compute the weights displayed in Figure 1?\n\n-I think Equation 8 could be slightly simplified to really illustrate the fact that $A(x_t, t)$ is a conditional expectation and therefore can be approximated using a neural network similarly to score-based generative modeling approaches. As of now, the only time this conditional expectation is written is in A.3.2.\n\n[1] Song, Sohl-Dickstein, Kingma, Kumar, Ermon, Poole -- Score-based Generative Modeling through Stochastic Differential Equations\n\n[2] De Bortoli, Thornton, Heng, Doucet -- Diffusion Schrodinger Bridge with Applications to Score-Based Generative Modeling \n\n[3] Vargas, Thodoroff, Lawrence, Lamacraft -- Solving Schrodinger Bridges via Maximum Likelihood\n\n[4] Dhariwal, Nichol -- Diffusion Models Beat GANs on Image Synthesis\n",
            "summary_of_the_review": "The main idea of the paper is sound and original. However, I think that this\nwork might not be mature enough. I would like to see more theoretical and\nexperimental comparisons with existing works to assess the efficiency of the\nproposed method. For these reasons I recommend the rejection of the paper but am\nready to raise my score if the authors provide compelling justifications for\nyour method.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a diffusion bridge approach — based on conditioning on diffusions on both ends. This alleviates the need to use the reverse construction used by the earlier papers in this field.",
            "main_review": "I think this paper is written in an obscure way for the non-specialists of this field (despite I have some limited background in SDEs). In many places, the motivations and constructions are not clear and not written precise enough for the ICLR readers.\n\nMy comments are as follows:\n\n1) The authors need to derive Eq. (7) within the paper for completeness. This result is deferred to several other theorems but it is hard to check whether assumptions made in these works to prove these theorems hold in this setting. What kind  of assumptions underlie the original result? Do they hold?\n\n2) I wonder why authors move from Eq. (7) to mixtures, as it is not made clear. Why is it not enough to use Eq. (7) for the generative modelling?\n\n3) In Eq. (8) where authors introduce the SDE, $g(X_t, t)$ is not specified.\n\n4) I found Section 4 quite confusing. The authors seem to define in this section an SDE class which are realised through a time-change of simpler SDEs (as they put it), I really couldn’t see how this relates to the rest of the paper. The authors should clarify what they mean by “the following SDEs represent the class of dynamics for (1) and (6)”. If this means that these SDEs, via a change of time argument, can represent (6), this notion has to be made clearer.\n\nAlso, these results need to be proved clearly instead of referring to the Oksendal’s book as done in the paper.",
            "summary_of_the_review": "I found the paper hard to read - but found the ideas interesting.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}