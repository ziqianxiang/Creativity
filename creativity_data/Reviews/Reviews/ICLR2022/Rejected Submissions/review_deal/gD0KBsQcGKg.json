{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper presents a method for producing a mixture of (disjoint) predictive distributions for deep learning models rather than a single predictive distribution.  The reviewers in general found that the idea had strong potential, was well motivated and addresses an important and under-appreciated problem in deep learning.  They seemed to find the proposed approach of using mixture density networks to be sensible.  However, the reviewers seemed to find that the paper was unclear in presentation and grammatically, as if hastily written.  One reviewer noted that they would not be able to reproduce the method given the confusing presentation.  The reviewers also found that the experiments didn't adequately evaluate their method empirically.  Unfortunately, the reviewers all agreed that the paper is not quite ready for publication (5, 3, 5).  Careful rewriting of the paper and the technical contributions and strengthening the experiments would go a long way towards improving this paper for a future submission."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper addresses the problem of determining prediction intervals (PI) in regression task. The prediction interval problem can be summarized as predicting a lower and upper bound between which the potential observation falls with a certain probability. The paper proposes a method to report the prediction interval as the union of disjoint intervals, in contrast with the previous methods which report a unified continuous interval. The motivation is that if the conditional density function has multiple modes, a single prediction interval may not be well descriptive of the uncertainty of the predictive model. To achieve this goal, they propose a differentiable objective function together with a Neural Network architecture that produces the union of disjoint prediction intervals. Through experiments, they show that multimodality often exists in real-world datasets and that their method manages to produce prediction intervals of higher quality (in terms of the commonly used metrics to assess the quality of the prediction intervals such as coverage probability and interval width) compared to the previous work.",
            "main_review": "\nStrengths:\n- Studies an interesting and well-motivated problem.\n- Contains thorough quantitative and qualitative experiments to both motivate the problem and show improvement over the prior work.\n\nWeaknesses:\n[Presentation]\n- The writing needs a lot of improvements. Many sentences do not follow the scientific writing style. Here I only mention the typos but there are many other cases where the sentences need to be re-written in a more elegant way.\n- Fourth line of Abstract: it -> they\n- Third line of introduction: something missing in the sentence probably you meant: functions \"+of\" the NN\n- Last paragraph of page 1: a single continuous PI provides low-quality PIs -> a single continuous PI \"+method?\" provides low-quality PIs. In many other cases, PI methods and PIs are used interchangeably which should not be the case.\n- Last sentence in section 2: The dot before (so-called MVE_ens) should be omitted.\n- First paragraph of page 4: which reflects HQ principle and \"+is\" differentiable.\n- After eq.6: Then, the coverage probability... -> why \"then\"?\n- After eq.6: L_PICP is defined as the mean square error with the target proportion γ with the max(0, x) operation. -> some words missing?\n- Before eq.7: number of the mixture -> number of the \"components\"\n- Last paragraph of 4.2: λ is a hyperparameter that determines how much trade-off between L_PICP and L_MPIW. -> some words missing?\n- Last paragraph of 4.2: If PICP obtained through training is less than γ, increase λ -> writing should be improved.\n- First sentence of 4.3 needs to be re-written\n- In the Hyperparameters section: All neural networks... -> all neural networks\n- last two paragraphs of 5.2 need to be re-written\n- title of 4.2: typo in architecture\n\n[Questions]\n- In Figure 2, why is the blue line sometimes showing a negative value while it should correspond to the width of the prediction interval?\n- In Figure 2, how do you justify your choice of training and testing samples? You keep the points with x>4 or x<-4 but should it not be the case that you shuffle your points and randomly pick a percentage as test data points? The regressor is never trained on points with x>4 or x<-4 while asked to predict them.\n- Figure2 is missing legend.\n- Figures 3,4 are missing axes labels and ticks.\n- Why are the experiments for the Protein dataset repeated 5 times and the other datasets repeated 20 times?\n\n\n[Other Concerns]\n- You mention you normalize the target distribution based on the entire data but you have to rather normalize based on only the training data.\n- There is not enough details to fully understand your method. I do not find the exact mathematical definition of F_i which is an important part of your approach. \n- A discussion regarding the time complexity of the algorithm in terms of the number of components in the mixture model (K) is missing.",
            "summary_of_the_review": "While the paper studies an interesting problem, it suffers from presentation issues which makes it hard to grasp the main technical contribution of the paper. The paper seems to be written in a rushed way and consists of many sentences which have missing words or are grammatically incorrect. This gets bolder as we move on to the later parts of the paper. I tried to mention some of the typos and the incomplete sentences as I went through the paper but they are too many to mention in this review. Many parts of the paper need to be re-written in order for it to get ready for publication. \n\nThe novelty is in re-writing the objective function, which tries to minimize prediction interval length while maintaining a certain coverage probability, in a differentiable manner and then optimizing it.  Some technical details are also missing (such as discussion on the complexity of the approach). Overall I think the paper is not ready for publication and needs improvements both in technical content and presentation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper provides the algorithm for the construction of prediction intervals composed of disjoint intervals. The authors proposed the motivation of disjoint intervals well, and the motivating example is impressive. The algorithms are also well accommodated with the statistical or learning-based prediction intervals, which contribute to the assessment of the uncertainty of prediction in general.\n\n",
            "main_review": "The strength of this paper is to construct better prediction intervals addressing the multimodality of predictive distribution and show better performance compared to the previous. It can be helpful to assess the uncertainty in a more rigid manner. \n\nHowever, I have a doubt that the title ‘Distribution-driven disjoint uncertainty estimation for deep learning” is adequate. The paper uses deep architectures to generate the distribution-driven-disjoint PIs after the preprocessing of mixture density networks. The mixture of density networks is an architecture, and there is no solution or explanation when we use the other deep architectures for other tasks. This implies that the algorithm is not general to address the PIs.\n\nIn detail, I have some issues.  \n\nAt first, I  feel that the figures in the paper are not kind to readers. In Figure 2, the prediction intervals are not shown well exception of the length of intervals, and in Figure 4, there is no explanation about solid curves after 0 on the x-axis.      \n\nAlso, I have some issues with the proposed algorithms. \n1) The use of $F_i$ seems reasonable. If you can provide more concrete evidence for the use of $F_i$, then the paper can be improved. More general criteria can be required for the generalization. \n2) The $K$ in the mixture density network is a hyperparameter, and the empirical evidence for using $K=5$ is validated. Can you use the CV or other learning algorithm to choose the $K$?\n3) I am interested in the computation time for Alg. 1. Is it efficient? \n4) There is no detail concerning $M$ of ensembles.            \n\nAdditionally, I cannot find the more significant use of disjoint PIs in various tasks. I was hoping you could provide more practical tasks related to PIs, such as active learning or calibration problems.   \n",
            "summary_of_the_review": "The paper success in the construction of disjoint PIs. However, there is a lack of thorough insightful motivation, remain of some algorithmic issues. Presentation is not better.     ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper notes that existing approaches to predictive interval (PI) generation and evaluation assume unimodal predictive distributions. This results in unnecessarily loose PIs in the presence of multimodality in the predictive distribution or multimodality in the targets. The authors first propose to extend the definition of PI to be a union of disjoint intervals, allowing for more fidelity in evaluation. They then propose a method to explicitly generate multimodal PIs. This method is based on feeding the output of a mixture density network (conditional GMM) through a secondary NN that outputs a set of lower and upper bounds for PIs. The authors compare their method against alternative heteroscedastic noise models in terms of PICP and MPIW on 11 UCI datasets.",
            "main_review": "This paper makes a strong and somewhat overlooked point that existing methods for the evaluation of PIs provide low fidelity in the presence of multimodality. The recent introduction of flexible methods for heteroscedastic noise modelling, such as SQR, creates a clear gap in the literature regarding their evaluation. The paper under consideration here attempts to fill this gap. However, from my point of view, it has 2 critical shortcomings that prevent it from representing a significant contribution.\n\n1. The two key contributions of the paper are difficult to disentangle. As a result, I do not think the experiments validate the main claims of the paper.\n\t* According to the authors, existing methods provide single predictive intervals and thus can not be evaluated using the tighter multi-interval method provided in equation 1. I do not agree with this point. It is not clear to me why a potentially multimodal predictive distribution, like SQR, can not generate disjoint PIs. A simple procedure to do this would be to place the centre of these PIs at the top-K modes of the predictive distribution. This is not very different from what the authors already do with their GMM, which also has full support in $\\mathcal{R}$.\n\t* As a result of the above issue, the authors evaluate their method using a disjoint set of PIs while evaluating baselines using a single PI. It seems clear that the PICP / MPIW tradeoff obtained by the multimodal generation+evaluation approach produces better results. However, there is no evidence that the proposed PI generation method is any better than competing approaches, which could potentially also produce multimodal distributions. For instance, in figure 3 and figure 5, SQR seems to always cover the right range in output space but its PI is always spread too thin due to multimodality. \n\n\n2. The motivation for the proposed PI generation method is unclear to me.\n\t* The authors propose to use a mixture density network combined with an auxiliary NN that outputs PI lower and upper bounds from the GMM parameters. It is not clear to me why they need the auxiliary network considering that disjoint $\\gamma$ % PIs can be obtained in closed form from the mixture density network directly. Is this last step intended to relax the Gaussianity assumption of the mixture density network?\n\nI am willing to raise my score to an accept if the authors clarify the motivation for their PI generation approach and evaluate competing multimodal noise modelling approaches using disjoint PIs.\n\n**Other comments:**\n\nThe paper has many grammar errors. I would recommend the use of an online grammar checker. \n",
            "summary_of_the_review": "This paper proposes a higher fidelity method for the evaluation of predictive intervals under multimodal predictions. The authors also propose a method to make multimodal predictions. Unfortunately, the author’s evaluation setup assumes that their proposed method is the only one to produce multimodal predictions (which is not the case). This creates a drastically unfair comparison.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}