{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The authors describe an approach to modeling data via an implicit representation that lives on a union of linear subspaces.  While the reviewers consider the authors' approach as novel and having potential, they (and myself) consider that the exposition and notation could be improved, and that the paper as it is is hard to understand and contextualize."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The manuscript considers learning a closed-loop auto-encoder between multi-class multi-dimension data and LDR. It also provides various experiments to verify the proposed method. ",
            "main_review": "The idea of viewing the encoder and the decoder in a close-loop auto-eoncoder as generator and discriminator is quite interesting. \n\nOne of the critical parts of the proposed method is viewing the encoder as a discriminator and measuring the distance between X and \\hat{X} as the maximization of the rate reduction.  The authors explain that the maximization can avoid that g\\circ f is not an auto-encoding map. However, it is still not super clear that why such maximization is reasonable to make g(f(x)) = x. In other words, why not use eqn (7) together with conventional reconstruction loss. This seems like also provide desired maps between multi-class data and LDR. It is a bit hard for me to see the advantages of using maximization there. \n\nSome minor issues:\nIn eqns (2), Z maps to \\hat{X}, it is a bit strange to write g maps Z to (\\hat(X),X). Similar issue for eqn (5). \n\nIn Section 1.2, X has been amused for different sets. \n\n",
            "summary_of_the_review": "The explanation of the maximization part or two-player game part is not super clear to me. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to learn auto-encoder using an adversarial objective based on rate reduction. The experiments show that the proposed method achieves competitive performance on reconstruction, generation, and discrimination. ",
            "main_review": "Strengths: \n\n(1) The proposed adversarial objective function is novel and interesting. \n\n(2) The experimental results are solid. \n\nWeakness: \n\n(1) No background is given on rate reduction and MCR2. As a result, the paper is not self-contained, and is not easy to understand. \n\n(2) The objective function is less simple and clean than VAE and GAN loss functions, which are closely related to log-likelihood of a generative model or a discriminative model. The objective function in this paper is quite complex. \n\n(3) No theoretical analysis of the proposed method is given. \n\n(4) When generating a new x, is z sampled from N(0,I)? The probabilisitic generative model is not very explicit in this paper. A probabilistic generative/decoder model is a natural representation of the observed data, with encoder being interpreted as approximated posterior inference. The auto-encoding point of view appears rather limited in comparison. \n",
            "summary_of_the_review": "The paper is an extension of MCR. It appears a bit complex. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper \"Closed-Loop Data Transcription To An LDR via Minimaxing Rate Reduction\" introduces a two-player minimax game between an encoder and a decoder to yield a linear discriminative representation (LDR). It achieves this by building on top of the recently proposed MCR2 rate reduction principle, and then motivates a contractive and contrastive measure to yield a minimax game. The paper shows generative empirical results on MNIST, CIFAR-10 and ImageNet, as well as discriminative classification accuracies on MNIST.",
            "main_review": "Strengths.\n- the core idea of the paper is novel as far as I can tell. I'd argue it has some resemblance to CycleGan (Zhu et al. 2017), but the setting on top of rate reduction and motivation is significantly different.\n- the experiments demonstrate that the method can work well as a generative method, even though it formulates its losses only in latent space.\n\nWeaknesses. \n- The paper is not as self-contained as it could be, i.e. it is near impossible to understand without reading the core referred literature first. It would help readability a lot if core intuitions and concepts would be briefly discussed when introduced. E.g. when introducing MCR2 in equation 4, it would help the reader a lot what the target of each of the two terms is. Another example, after equation 6 it is stated that this measures the volume - why? (if this is not easy to explain, then there should be a reference to where this is explained). The math also needs more clarification, for example what is the union in equation 6 (Z and \\hat Z are both \\in Rˆ{dxn}, or)? Is the \\Delta R in equation 4 and equation 6 really the same (or is it semantically overloaded)?  writing (and logic of the writing) needs to be worked on.\n- Experiments. The comparisons on generative models in Table 1 are done with rather old baselines (all being at least 4 years old). E.g. DCGAN (Radford 2015) could be replaced by StyleGAN2/3 etc (similar for the VAE methods). It should also be reported what the training time for the method on the datasets is, and how it scales with dataset size and resolution. Also, it should be discussed that the model only roughly encodes the semantics, and sometimes disregards even color (e.g. as seen in Figure 5 where the red car turns yellow). In Figure 14 in the abstract it can be seen that the the model seems to sometimes collapse inputs into the same output? If so this should be discussed.\n- Already in the Abstract it is claimed that the model learns a discriminative representation, and it is shown in Table 3 to perform in the ballpark of VAE methods on MNIST data. As the model is also trained on ImageNet (see Fig5), why is no discrimnative comparison performed on ImageNet? It'd be interesting to see if the model can scale up to more classes and more complex datasets (what is the scaling behaviour theoretically of the model in the latent space? Does the dimensionality need to be proportional to the number of classes to ensure the possibility of orthogonality?) . \n\nMinor Issues.\n- The Introduction should start with a section contextualizing the work, i.e. review of context and background, what has been done by others broadly in the field that led to the current work. \n- Equation 2,3 etc. - the result of function g is not a tuple (X, \\hat X). The figure should be changed.\n- After eq. 3 it is written \"later studies [...] surrogate to earth mover's distance\". I think this is wrong. The original GAN paper showed that GAN's minimize Jensen-Shannon distance, and Arjovsky et al showed how to build the WGAN that operates on earth mover's distance.\n- Also on pg 2. - Combination of AE and GAN: \".. started with somewhat different motivation, they have evolved...\". While true that both methods are quite different, as both are generative models, it was this motivation (generative modeling) that they become successful in modeling of real-world data. So, the logic behind this sentence is confusing. \n",
            "summary_of_the_review": "While the paper proposes an interesting and promising method to learn an encoder-decoder with a novel two-player minimax game, the paper is not yet well enough written and lacks some experimental results that allow for comparison with state of the art methods.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}