{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper focuses on unsupervised image denoising and proposes a method to do so. It shows that using a designed operator based on domain knowledge can help improve unsupervised image denoising. The authors also provide experimental results demonstrating that the proposed methods outperform existing unsupervised denoising and behave similar in performance to supervised methods. The reviewers liked the improvements but (1) limited novelty/simple extension of noise2self, (2) example not convincing, (3) lack of clarity in 2.3, (4) a variety of other technical concerns. The authors partially addressed these concerns. However, I concur with the reviewers that the paper still requires more work and is not ready for publication in its current form."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a method for unsupervised image denoising. It shows that a better designed operator based on domain knowledge can benefit unsupervised image denoising task. The provided experimental results show the proposed method outperforms existing unsupervised denoising ones and achieves similar performance to supervised methods.",
            "main_review": "I think it is a good paper to improve the performance of existing unsupervised denoising methods. However, I have several concerns:\n1. One statement of this paper is the domain knowledge is important which can further improve the performance of unsupervised denoising. This is based on that Eq. 3 is closer to Eq. 1 if the 2nd term of Eq. 5 is close to zero. However, the second term of Eq. 5 is not relevant to f(x). In this way, is the back propagation of Eq. 3 and Eq. 1 the same when optimizing f?\n2. A g(x) closer to the ground truth can lead to a better result for f(x) according to my understanding. In this way, g(x) can also be treated as a denoiser. Can the authors explain why f(x) has a better denoising ability than g(x)?\n3. The visual improvement is too marginal even though the PSNR is higher e.g. in Fig. 4.\n4. It seems spatially correlated noises may violate the assumptions in this paper. The authors provide some experiments to show the effectiveness of the proposed method in real image denoising dataset (Abdelhamed et al., 2018). Can the authors provide more analysis about why the proposed method still works well for spatially correlated noises?\n5. The motivation is strange to me in Sec. 2.1. It is strange to use norm(y) as supervision in Fig.1 (right).\n6. In sec. 3.1 and 3.2, Fig. A should be revised to Fig. 3?",
            "summary_of_the_review": "This paper proposes a novel method to improve the performance of unsupervised denoising methods. But I still hope the authors address my concerns above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed a self-supervised denoing method using domain knowledge. The proposed method seems somewhat like knowledge distillation, where a pre-trained denoiser or handcrafted designed noise models g can provide better initial results than original noisy images, and then a better denoiser f can be trained based on the results of g instead of noisy images. By forcing the complement of f and g, f will not be same with g, and the denoising performance is bossted.  ",
            "main_review": "1. Examples in Motivation are not convincing. There is no doubt that ground-truth clean image can provide better denoising performance, and any other modifications will yield denoising performance decreases. \n\n2. If I understand right, when training denoiser f, the supervision is from complementary denoising results by pre-designed g. The masks on J and Jc have no intersection. From Fig. 2, it is confused to understand the training procedure. Predicted output in above should be masked on J? Why pseduo-target is entire image while the one in bottom is masked? Also in experiments, the results by unbalanced masks are not given, so why give this setting in Fig. 2? \n\n3. The proposed method is evaluated on their designed settings, which is not consistent with most exisitng self-supervised denoising methods.",
            "summary_of_the_review": "This work proposed an interesing self-supervised denoising method and its improvements are significant. But it has several issues to be addressed listed above. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper present an self-supervised deep learning method for image denoising, which is a generalization of the concept proposed in noise2self. The basic idea is to approximate the unbiased estimation of the supervised loss function, which utilized the noise independence of the input and the output of the network. Some experiments are conducted on 2 image denoising tasks.\n",
            "main_review": "*Strong points*\n\n1. The technical writing of the paper is fairly well. The derivation can be easily  followed.\n\n2. The paper presented a generalization of the noise2self, which conceptually can handle more tasks than noise2self.\n\n*Weak points*\n\n1. The novelty is limited, it is essentially a very minor generalization of noise2self, the un-bias estimator of loss function under J-invariance. There is no convincing example to show such a generalization can be realized with a constructive scheme.\n2. The contribution is over-claimed. While the paper is written in the way that it seems to be able to  go beyond image denoising, it is not the case. The paper does not show how  operator $g$ can be defined for other image recovery problems with non-identity measurement matrix.\n3.  Experiments are limited with very specific synthesized noise. \n4.  Its practical value for self-supervised image denoising is not well justified in the experiments.  While the proposed method is a minor incremental work built on noise2self. its performance is rather well below recent existing self-denoising methods,, in terms of the performance gap to the supervised counterpart.\n\n*Comments and Questions*\n\n1. The paper claim a regression network which can be self-supervised. Can the author provide some examples on $g$ for other tasks that goes beyond image denoising?\n2. The examples used in the experiments are removing mixture of noise. The paper did not clearly how the proposed SSRL loss function has its advantage over noise2self, in terms of its approximation to the loss function of noise2truth.\n3. The proposed methods showed better performance over noise2self in the case of Gaussian+poisson+pepper-and-salt. The first 2 noises are independent, thus it fits the assumption of noise2self. The pepper-and-salt noise can be trivial treated in noise2self by excluding all pixels with value $0$ or $1$. The experiments should be conducted with such modifications on noise2self to justify its value. \n4. Since noise2self, there has been a rapid progress on self-supervised image denoising which much better performance than noise2self. Indeed, the performance of recent many methods, e.g.  noiser2noise[CVPR'20], self2self [CVPR'20], Noise-as-clean [TIP'21], \nPartially-linear denoiser [PAMI'21], R2R [CVPR'21], are close to the supervised counterparts. These all are based on the same idea, designing an approximate unbiased estimator to loss function.  To list some, However, From the experiments shown in this paper, the gap between the proposed one and the supervised one is not small. More experiments should be included.",
            "summary_of_the_review": "The case for this paper is quite clear to me. The paper present a minor generalization of the noise2self, which is conceptually more general, but not constructive scheme is developed to make it more applicable that noise2self. There is no concrete example to show its advantage over noise2self in terms of generalization to different problems or different noise. This paper is not a theoretical paper and its value largely lies in its empirical performance.  There have been many recent works on the unbiased estimator of the supervised loss function, the same as the proposed one. e.g.  noiser2noise[CVPR'20], self2self [CVPR'20], Noise-as-clean [TIP'21], partially-linear denoiser [PAMI'21], R2R [CVPR'21]. These methods provide competitive performance to the representative supervised denoising network, such as DnCNN, for various noise types including Gaussian, Poisson and real-world noise. The experiments conducted in this paper does not provide any comparison to these methods on standard benchmarking dataset, and the available experiments seems that the gap between the proposed one and the supervised one is quite big. In other words, the practical benefit of the proposed DDSL is not impressive.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}