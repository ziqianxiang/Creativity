{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper has been reviewed by four experts. Their independent evaluations were all below the acceptance threshold citing various issues ranging from disconnection between stated goals of the presented work and the means in which the approach was evaluated, to doubts about the scalability of the proposed approach, to the lack of clarity regarding the actual novelty of the approach given some key missed references, to name a few items of criticism. Most reviewers were impressed with the empirical performance achieved in the conducted experiments, and one of the reviewers raised their mark in response to the author's rebuttal. Yet, the overall evaluation places this work as it stands now below the threshold for ICLR acceptance. I would like to encourage the authors to continue pushing their promising endeavor and systematically incorporating the feedback received here to improve the overall quality of this work."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper was fundamentally about out of distribution detection. The authors tried to use the changing confidence around prediction values to predict changing distributions. They did so specifically for high dimensional, unstructured data, where distributions are often difficult to understand or quantify and compared their results to a number of current methods in the area on a wide variety of high dimensional data.",
            "main_review": "The strengths of this paper are two-fold. First, they have a fairly intuitive setup with a number of very desirable features (never before seen classes, high dimensional input data). Second, they have strong performance on key tasks, using a useful measure (AUROC) on a diverse set of benchmark datasets compared to a wide variety of other recent methods.\n\nThere were, however, some significant weakness in this paper. \n\nFirst, is I was confused about what problem this was trying to solve - OOD, yes,  but I couldn't really tell if this was focused on detecting concept drift, data drift, anomaly detection, or something else (I think they are focused on data drift, but this is only mentioned in passing when discussing evaluation at the bottom of page 6). I believe there were technical issues in their problem setup: There should be a distinction drawn between ABSTAIN and and FLAG, which is not present in their equation. More bothersome, the temporal aspect of the problem setup was unclear. By my read of the problem setup, $x_t$ is used only as a data point, and neither the output, nor the score function have any dependence on time. The place the temporal information was encoded was in the network structure. That only makes sense if you have certain strong assumptions on your input data (e.g. regular spacing between time samples). That assumption was not stated and not checked anywhere in the paper. \n\nSecond, I have concerns about the experimental setup. The precise problem they are addressing impacts the experimental setup. My previous comment on this spacing between time points applies here. Figure 1 makes me concerned that their temporal scorer is subject to data leakage during evaluation because it is being updated based on test sets. More concerning is the setup, they are inserting change points, so in spite of the excellent data chosen, they are fundamentally using a synthetic dataset for evaluation. Because the details of that synthesis don't necessarily match the intended setup of the baseline measurements, it shouldn't be a surprise that their methods perform better. When you pick evaluation, baseline, and data, it is easy to show good performance. \n",
            "summary_of_the_review": "I think this paper has a lot of potential, but but has technical issues that should be addressed before acceptance. The experimental setup and problem statement are the primary concerns for me. What they measure seems strong, but it is the connection between those measurements and the stated goals of the paper that are the limiting factor. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a spatio-temporal approach for unsupervised out-of-distribution detection. In particular, it suggests a 'hierarchical' evaluation method based on computation of existing distance/similarity scores on hidden embeddings of high-dimensional data succeeded by a temporal component, that is also based on existing work, that treats chunks of  scores, computed by the first step, of input data as a stream. The suggested pipeline achieves state-of-the-art results on datasets coming from different domains ( image, audio, clinical data). More importantly, it demonstrates the importance of semantic context when deciding whether a sample is ood exhibiting better performance than existing baselines when semantic overlap should be taken into consideration. ",
            "main_review": "The paper achieves state-of-the-art results on diverse established datasets and performance metrics. More importantly, the suggested framework appears to be more semantics-preserving than existing scores that rely on data-statistics. \nHowever, I feel that some hypothesis are not very well corroborated (experimentally and/or theoretically) while some details are missing. \nIn particular,\n\n(1) could the authors please provide more details on the embeddings used? Are they deterministic or stochastic? If they are deterministic, how are they formed exactly ( it is the concatenation of the output of the hidden layers of the networks?)\n\n(2) the hypothesis stated in 3.2.1 (that in the latent space ood samples will lie in distance from  iid samples), especially for high-dimensional inputs. may not always hold. A visualization on cifar10 data will better support this claim. Do the authors think that modifying the objective function to explicitly support this claim could further improve performance? In particular, I am pointing the authors to the following work (for stochastic features), that may want to consider for further improving the performance:\n\n[1] Sinha S, Dieng AB. Consistency Regularization for Variational Auto-Encoders. arXiv preprint arXiv:2105.14859. 2021 May 31.\n\n(3) Moreover, what if the embeddings of iid form clusters (also appart from each other)? How many training samples would be needed in this case to sufficiently populate the coreset??  The computation of the similarity score also seems prohibitive for large-scale datasets that may need large coreset. I think it would be good if the authors could report an ablation study on the size of the coreset. \n\n(4) I also think the authors should extent the ablation such that they report results i) based on the spatial scores only (only distance, only similarity, both) and/or ii) the temporal score (computed directly on the embedding h) to better demonstrate the importance of each score used \n\n(5) results on large-scale datasets (imagenet) could strengthen the impact of the paper.",
            "summary_of_the_review": "The paper shows competitive results compared to strong baselines and a promising direction for semantic preserving out-of-distribution detection. However, I think some suggestions on the ablation study and some comments on the scalability on large datasets should be addressed to strengthen the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a spatio-temporal training method to quantify uncertainty and detect out-of-distribution samples. The authors apply their method to audio speech classification, seizure detection using EEGs, and image classification. ",
            "main_review": "*Strengths*:\n* The idea of using temporal structure in data and coresets for OOD detection are both interesting ideas.\n* The paper conducted experiments across several domains, involving a number of different datasets and tasks.\n\n*Weaknesses*:\n* This work is not the first spatio-temporal model for OOD detection as claimed. See [1] for a review. See [2, 3, 4] for examples of how other works deal with problems similar as ‘an obstacle detector deployed in a self-driving car will see images correlated over time.’ (which is an example used in the introduction) See [5] for an example of how the spatio-temporal structure is used when applied on EEG datasets. In light of these works and lack of discussion and comparisons, the novelty and contributions of the work are unclear.\n\n* The method makes sense overall but lacks justification of certain choices. For instance, the choice of the Mahalanobis metric and Gaussian assumptions in OOD detection is unclear. What does the combined spatial uncertainty score imply? How is the coreset extracted and how do the number of points in the coreset impact detection? Some analysis of these different parameters and aspects of the method should be explored further. Because there are no strong theoretical justifications, ablations of some of the different components of the proposed model are necessary. \n\n* How is the model used for image datasets (CIFAR)? The beginning of the results discusses how OvR classification can be used for examining OOD but it is unclear how these datasets are used in this case where the temporal structure of inliers is a core assumption. If different classes are presented as certain temporal rates, this information should be discussed and analyzed. How does the temporal rate of different classes impact learning or detection?\n\n* How is the reference window and the sliding window selected? Does setting this parameter involve an estimation of how temporally distributed OOD examples will be? Details on this and other choices / hyperparameters should be provided.\n\nMinor comments:\n- The authors claim that their proposed method is more robust to dataset statistics and focuses on the semantic meaning of the data. However, the same was not discussed for the EEG experiments, where the InD/OOD is decided based on either patient population or different patient demographics. It would be interesting to see this discussion.\n- Fix the equation in 3.1: ‘th’ as threshold should be explained. \n\n[1] Abdar, M., Pourpanah, F., Hussain, S., Rezazadegan, D., Liu, L., Ghavamzadeh, M., ... & Nahavandi, S. (2021). A review of uncertainty quantification in deep learning: Techniques, applications and challenges. Information Fusion.\n[2] Zhao, Y., Deng, B., Shen, C., Liu, Y., Lu, H., & Hua, X. S. (2017, October). Spatio-temporal autoencoder for video anomaly detection. In Proceedings of the 25th ACM international conference on Multimedia (pp. 1933-1941).\n[3] Nguyen, T. N., & Meunier, J. (2019). Anomaly detection in video sequence with appearance-motion correspondence. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1273-1283).\n[4] Zhou, J. T., Du, J., Zhu, H., Peng, X., Liu, Y., & Goh, R. S. M. (2019). Anomalynet: An anomaly detection network for video surveillance. IEEE Transactions on Information Forensics and Security, 14(10), 2537-2550.\n[5] Fernando, T., Denman, S., Ahmedt-Aristizabal, D., Sridharan, S., Laurens, K. R., Johnston, P., & Fookes, C. (2020). Neural memory plasticity for medical anomaly detection. Neural Networks, 127, 67-81.\n[6] Hendrycks, D., Mazeika, M., & Dietterich, T. (2018). Deep anomaly detection with outlier exposure. arXiv preprint arXiv:1812.04606.\n",
            "summary_of_the_review": "The idea of using temporal structure to perform out-of-distribution detection is an interesting problem with many important applications. While the authors provide some experimental results that suggest their approach outperforms other methods, there was little justification for the different choices made and no analysis of how these decisions impact their model's performance. In addition, the authors did not discuss or compare other approaches for estimating spatiotemporal uncertainty from sequential data and thus it is difficulty to assess the novelty and contributions of the work. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a method for out-of-distribution (OOD) detection taking temporal consistency into account - both the position of a new data-point in the latent space in relation to the other observed data, as well as the relation to temporally close (recently observed) data are taken into account. Experiments are performed on both audio, video and EEG data, and comparisons are made against six baselines.",
            "main_review": "The strengths of this paper are:\n+ It addresses a highly relevant and important problem, OOD.\n+ It provides thorough experiments with baseline comparisons.\n+ It is well organized and easy to read.\n\nUnfortunately, the paper suffers from a number of major weaknesses:\n- Firstly, the title and beginning of intro is misleading - the paper does not address uncertainty quantification or calibration, it just provides a method for OOD. Please narrow down the scope of the first intro paragraphs, and select a more focused title.\n- Secondly, the terminology \"spatial\" and \"temporal\" is not correctly used. The spatial dimension would refer to dimensions within a data-point in time, such as the vertical and horizontal dimension in a video frame. You could instead talk about time-independent and time-dependent distance measure.\n- Third, your temporal uncertainty bears large resemblance to filtering and temporal smoothness methods - Kalman and particle filters pre-deep learning, and deep learning methods such as LSTM and CNN over time. The similarity to these methods should be discussed. Moreover, you should refer to some work from the by now large literature on uncertainty estimation and calibration of classification models.\n- Last, although rigorous, the experimental results lack standard deviations. Without this information, it is not possible the significance of the results presented in tables 1-3. \n\nGiven this, I recommend to not accept this paper, as it is not yet in a state to be published in ICLR. However, given the encouraging results and the importance of the problem, I encourage the authors to address the weaknesses above, revise the paper, and submit it to a later venue.\n\nEDIT AFTER REBUTTAL: The authors have addressed my comments to a large extent, and the paper now looks much nicer - although, I am still not convinced about the technical novelty compared to the vast literature on filtering, outlier detection, uncertainty estimation and calibration. Hence, I raise my grade to 5.",
            "summary_of_the_review": "The strengths of this paper are that is addresses an important problem, it provides thorough experiments and it is well organized and easy to read.\n\nUnfortunately, the paper suffers from a number of major weaknesses, in that the title and intro paragraphs are misleading, there are issues with terminology and there are missing references to the literature on filtering and uncertainty estimation. Moreover, the results do not show if the differences to baselines are significant.\n\nGiven this, I recommend to not accept this paper, but I encourage the authors to address the weaknesses and resubmit to a later venue.\n\nEDIT AFTER REBUTTAL: The authors have addressed my comments to a large extent, and the paper now looks much nicer - although, I am still not convinced about the technical novelty compared to the vast literature on filtering, outlier detection, uncertainty estimation and calibration. Hence, I raise my grade to 5.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No comments",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}