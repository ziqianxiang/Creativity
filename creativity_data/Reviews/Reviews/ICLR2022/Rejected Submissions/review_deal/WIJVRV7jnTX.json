{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This is a borderline case and it's quite difficult to decide the recommendation. The paper works on a critically important problem, namely removing or reducing the in-distribution accuracy drop when we need to also take the out-of-distribution accuracy into account. The proposed method is simple and it works, which is great. However, as the reviewers discussed, the demonstrated applications are not very representative, and the authors should consider more popular setups of few-short learning and even other forms of domain generalization. Furthermore, adversarial examples are also OOD (in most cases, since the ID manifolds are thin films and the attacks can easily go out of the ID manifolds), it would be great if adversarial accuracy can be incorporated as a case of OOD accuracy. Since there is still room for improvement, we hope the paper would benefit from a cycle of revisions for a re-submission."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The submission considers a very topical issue given the active ongoing interest in OOD performance — models developed with OOD robustness in mind can come with a significant loss of in-distribution performance, and since we expect to mostly encounter in-distribution settings in deployment (assuming deployment is being done responsibly), we need to find ways to trade-off performances for in-distribution and unexpected situations.\n\nThe submission illustrates that a fairly simple approach of Platt-scaling done in-distribution for a “standard” and a “robust” model followed by averaging their predictive distributions can sometimes work pretty well: significant performance improvements appear to be achieved both in- as well as out-of-distribution.",
            "main_review": "The strengths of the submission are that \n\n — the topic is highly relevant and timely\n\n — the range of datasets is broad, and the empirical results are fairly promising\n\nSome points that could strengthen the submission are as follows:\n\n — It would be interesting to investigate qualitatively the OOD cases where the (calibrated) standard model has a higher confidence, as well as a quantitative analysis of how often such events occur. I was hoping to find something along these lines in the Appendix, but it doesn’t look like there’s a supplement. It might also be a good sanity check to look at expected calibration errors for both models.\n\n — While I believe the range of experiments (datasets + models) is sufficiently broad, methods aimed at distributional robustness such as groupDRO [1] or domain adversarial learning [2] (to name some) might be interesting to study under the proposed method (on whatever datasets they’ve been showcased to have been successful upon).\n\n[1] Distributionally robust neural networks for group shifts, ICLR 2020\n\n[2] Domain generalization with adversarial feature learning, CVPR 2018",
            "summary_of_the_review": "The submission demonstrates that a fairly straightforward calibration-based approach for an ensemble model can lead to good performance in both in- as well as out-of-distribution settings. While the empirical illustrations could certainly be made more comprehensive, and the mechanism of improvements analyzed a bit more closely, I believe this is an interesting enough illustration to justify drawing attention to it.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "UPDATE:\n\nI acknowledge that I've read the author responses as well as the other reviews.\n\nI now understand the empirical merit of the paper. Furthermore, the authors clarified the baselines' performance and add further analysis. Therefore, I raise my score to 5 weak reject. However, I'm still not convinced about the novelty of the paper. In ensemble learning, diversity between ensembles is a major factor of success. Therefore, while it is straightforward that the proposed method outperforms robust+robust and standard+standard, temperature scaling is not a new contribution and I'm still skeptical about how 'surprising' the results are. I hope the authors can clarify further issues about the point.\n\n================================================================================================\n\nThe paper studies ensemble methods to achieve improved in-distribution (ID) and out-of-distribution accuracy. The paper proposes a simple fix on concatenating the robust model and the standard model via adopting the idea of classifier-specific temperature parameters, which is optimized on the in-distribution validation dataset. Authors test their methods on various distribution shifts. The proposed method seems to improve the ID-OOD accuracy tradeoff.",
            "main_review": "Strengths\n1) A simple algorithm that works in a wide range of datasets.\n\nWeaknesses\n1) I'm not convinced why the proposed method should work. I think the authors should provide more explanations why the newly introduced temperature parameter, which is tuned on the validation dataset, should show improved OOD accuracy.\n2) I'm not sure about the self-training results in Table 3. compared to the original results in the In-N-Out paper [1]. Maybe releasing the author's implementation can demystify the concerns.\n3) The paper lacks any ablation study or analysis to support their claims.\n\n#References \n[1] IN-N-OUT: PRE-TRAINING AND SELF-TRAINING\nUSING AUXILIARY INFORMATION FOR OUT-OF DISTRIBUTION ROBUSTNESS\n",
            "summary_of_the_review": "While I think the results in the paper are competitive against the baselines, the authors do not give any explanations for why the proposed method should work. Therefore, I'm leaning towards rejecting the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper discusses the trade-off between in-distribution and out-of-distribution accuracy, where ERM-based learners have good ID but poor OOD performance, and robust learners the opposite. The authors demonstrate that by ensembling both types of learners, and calibrating their ID accuracy, they can outperform the standard and robust models both on ID and OOD data.\n\nThe main contribution of the paper is the strong empirical results provided by the simple yet effective approach.\n",
            "main_review": "This is an interesting paper that discusses an important and timely topic, the trade-off between in-distribution and out-of-distribution performance. The paper empirically analyzes this trade-off and shows that standard ERM-based learners perform better ID at the expense of OOD, while robust learners are the opposite. The authors then approach this problem head-on, and propose a simple yet effective solution to this perceived trade-off. The proposed solution relies on resembling both types of models. Then, to further improve performance, the authors calibrate the models and demonstrate substantial improvements.\n\nWhile the strong empirical results and simple approach are definitely encouraging, I find significant problems with the paper at its current state. \n\nFirst, the lack of any theoretical contribution or even a substantive intuition into why this approach should work, and when it shouldn’t is highly problematic. This is even more problematic considering existing papers that discuss just that [1]. There seems to be substantial agreement between [1] and this paper, along with some disagreement regarding the connection between ID calibration and OOD performance. Discussing this in the paper and showing how the two might agree will surely improve the paper.\n\nSecond, the methods used in the paper can probably be improved. While good performance with a simple method is always a good thing, it would be beneficial to try and further improve performance with a more advanced re-calibration approach or an end-to-end calibrated model. The same critique also stands for the ensemble approaches, which at the moment only consider a combination of two models.\n\nLastly, the paper is quite thin in analysis and discussion. While I don’t have a particular problem with the lack of novel approaches, I do find it redundant adding an equation for softmax and accuracy. If deleted, the authors should use the 1+ pages remaining space to add a detailed analysis of when and why this approach works in reality. Other potential uses of this space include a representative theoretical analysis, more baselines and experiments on widely used OOD benchmarks such as WILDS [2].\n\n\n[1] On Calibration and Out-of-domain Generalization\n[2] WILDS: A Benchmark of in-the-Wild Distribution Shifts\n",
            "summary_of_the_review": "While I do think that this paper discusses an important problem and presents methods that perform reasonably well on real data, I think that its current form the paper is a bit too thin and requires some more work before it is published. My main concerns are the lack of theoretical analysis, as well as additional experiments and analysis with more advanced methods. I also point to a previous publication that might help the authors relate to in their analysis. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "A robust model attains high accuracy OOD while a standard model attains high accuracy ID. This paper proposes ensembling a robust and standard model to attain high accuracy ID and OOD. In particular, this paper recommends calibrating both the standard and robust models on ID data before ensembling. ",
            "main_review": "This paper has many strengths:\n- The idea is impactful, easy to implement, and explained clearly.\n\nHowever, there are some areas where the paper could be improved:\n1) More complete baselines. \n\nA number of baselines including vanilla ensembling, tuned ensembling, and alternative combination methods are mentioned in Section 3. However, their relative ID and OOD performances are not sufficiently explored. Table 4 contains the OOD accuracies for some of these methods for all datasets, but in terms of ID performance only the averaged version is available (Figure 1, right), and the Tuned ID baseline remains absent. There is room to include this as the paper is currently just over seven pages, and it would improve the paper. In general, it would be nice to create Figure 1 (left) using real data for each dataset, as currently it is only shown with fake data I believe.\n\nThere are also some baselines that are discussed but absent, e.g., in Section 3 the authors mention that they found combining the softmax probabilities to work better than combining the logits. However, this alternative is absent in all tables / plots. This baseline in particular is interesting for the following reason: I believe that without the softmax, there is a mixing coefficient alpha (e.g., in Eq. 3.5) which exactly matches the proposed method (Eq. 3.3). Going back to the idea from the previous paragraph of creating a scatter plot with real data, it would even be possible to plot the curve from varying alpha and see where the proposed method lies on the curve -- how close is it to the optimal alpha? Calibration could then be interpreted as a way to choose this mixing coefficient, which would be very interesting.\n\nWhen comparing with vanilla ensembling, this paper use a default mixing coefficient of 0.5. However, this paper mentions that [1] only shows there exists a way to ensemble a standard and robust model, and requires OOD data to learn how to do so. As 0.5 is assumed to be the default mixing coefficient for vanilla ensembling, it could be nice to also compare with [1] using, e.g., mixing coefficient 0.5. This seems relatively straightforward for ImageNet-R as the experimental set-up is very similar. Going back to the previous paragraph, it would be a very interesting comparison if you could find the mixing coefficient alpha via calibration then apply it to the method of [1]. As I understand, the concurrent method of [1] is very similar to this work as both propose to ensemble a robust model with a model that performs good ID and find the benefits of both in the ensemble.\n\n2) Clarity in experimental details.\n\nClarity in experimental details could strengthen the paper, for instance when fine-tuning on ImageNet for the ImageNet-R experiments which optimizer and batch size are used. Lack of these details will make reproduction difficult. One drawback of this method is that it is more expensive than a single model, but this experimental detail is absent. Moreover, why is the CLIP zero-shot performance on ImageNet-R extremely low? This is substantially lower than what is reported in [1,2] for the same model type (ViT-B/16).\n\n[1] https://arxiv.org/abs/2109.01903\n[2] https://arxiv.org/abs/2109.01134",
            "summary_of_the_review": "This paper explores an interesting idea of ensembling a robust and standard model to attain high accuracy ID and OOD. However, the paper would substantially benefit from a more complete exploration of baselines and thorough explanation of experimental details. For instance, even some of the baselines mentioned are partially included or absent, and accuracy of ImageNet-R is substantially lower than usually reported. I recommend that the paper could substantially benefit from additional exploration, and may have been rushed in current form.\n\nEdit: many of my concerns were addressed and I have changed my score to 6.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}