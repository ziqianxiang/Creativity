{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "There was a consensus among the reviewers to reject the paper. While they noted that the paper proposed a new interesting stochastic algorithm for deep learning, they think the paper needs to be substantially improved in both theory and empirical study. The paper was judged quite incremental in comparison to the work of Öztoprak et al 2018 (where most of the theory was developed), while not showing improved empirical performance on the benchmarks."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a model building approach that replaces the one-step backtracking in stochastic line search methods. The authors provided some convergence guarantees for their proposed method and evaluated their approach on some image classification tasks.",
            "main_review": "1- The majority of the manuscript is well-written and easy to understand. However, some parts require further explanation and clarification that the reviewer explains in the following comments.\n\n2- The literature part needs to be strengthened. \n\n3- The point that is mentioned in Page 2 \"It also incorporates the most recent curvature information from the current point. This is in contrast with the stochastic quasi-Newton methods which use the information from the previous steps\" is $\\textbf{not}$ novel! The studies [1] and [2] (to name a few) introduced this concept earlier.\n\n4- Does the inequality (3) guarantee a decrease in the main objective function or just the stochastic objective function? If just the stochastic objective function, how can we trust the step length?\n\n5- In Page 3 (line 3 in the second paragraph) the authors mentioned group $p$ which is not introduced before. The text needs to be more clear and readable.\n\n6- One of the main goals of this paper is related to reducing the computational costs of tuning. However, in the proposed algorithms 1 and 2, there are many hyper-parameters that still requires tuning. The authors need to exactly mention how they claim the idea of reducing tuning attempts.\n\n7- The theoretical results are quite basic and more stronger results are expected. Theorem 1 does not guarantee any convergence, it just shows that the norm of gradient (in expectation) is bounded above by some terms that are NOT diminishing as the algorithm progresses. If $\\alpha_k = \\mathcal{O}(L^{-2})$ (which is super tiny), the proposed method converges to the neighborhood of stationary point. Even with diminishing step sizes, the proved convergence result does not guarantee any convergence to a stationary point (just the neighborhood around a stationary point). All in all, the theoretical results definitely require modification.\n\n8- Do the authors consider the extra cost per each iteration in the plots with $x$-axis as $\\textbf{Epochs}$? Also, instead of averaging over 5 runs, it is better to show the error band. The mini-batch size is not reported for the experiments, it is very important hyper-parameter as well.\n\n9- That is good that the authors provided some results regarding the robustness of their method.\n\n10- In the abstract, it is mentioned that $\\textbf{This novel\ndiagonalization approach makes the selected step lengths adaptive.}$ The proposed method is not adaptive in terms of step size! In both algorithms 1 and 2, there is not any adaptive rule for the step size.\n\n\n[1] Gao, W., & Goldfarb, D. (2018). Block BFGS methods. SIAM Journal on Optimization, 28(2), 1205-1231.\n\n[2] Berahas, A. S., Jahani, M., Richtárik, P., & Takáč, M. (2020). Quasi-newton methods for deep learning: Forget the past, just sample.",
            "summary_of_the_review": "The theoretical results needs to be more strengthened. The contribution is ok but not good enough.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper presents a stochastic version of the model building algorithm (SMB)  by taking the subsequent iterate to be the minimizer of a quadratic model build from two iterates (the current and the one based on conventional SGD). Empirical results suggest that the proposed SMB has some great generalization performance.",
            "main_review": "Strengths  \nThere is some novelty in the proposed algorithm, and the paper has made some efforts to extend the model building approach to the stochastic setting. \n\nWeakness\n\n1. From theoretical side, by enforcing a relatively strong independence batch assumption, the convergence analysis is not a surprise given the reference Wang 2017. However, the experiments shows that SMB  consistently outperforms SMBi, it seems to me that dependence is the key for getting better performance.  Hence, it is unsatisfactory if the convergence property of SMB is not demystified and completely ignored.\n\n2. Since the paper has targeted its proposed algorithm a special case of quasi-newton method (e.g. (6)), it would be unsatisfactory if comparison with SQN is not made. \n\n3. The experiment conclusion is based on a single run, it is not convicing to me that the reported advantage of SMB is as reliable as described. I would recommend spending more time effort in the experiments before submission.\n\n4 Figure 3 shows that SLS outperforms SGD and ADAM while SGD and ADAM outperform SMB and SMBi. The last figure shows that SMB with Auto-scheduled stepsize performs slightly better than SLS. What is the most important factor in getting good performance? Would the benefits mainly come from the Auto-scheduled stepsize? Can we equip SGD or ADAM with similar scheduled stepsize? This appears to be very promising. \n\n\nSome minor issues.\n1. \\mathbb{E}[F(x; \\xi )] is deﬁned at the beginning but never used.\n\n2. It is confusing that $n$ is used to deﬁne both the number of parameter blocks and the dimension of X.\n\n3. What is  $f_{k,p}$ defined in page 3?\n\n",
            "summary_of_the_review": "While this paper makes some interesting contribution of a new stochastic algorithm for deep learning, it appears to be far from a complete work. I think the paper needs to be substantially improved in both theory and empirical study.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "\nThis paper proposes an alternative to stochastic line search which is based on forwarding step model building which corrects the direction of move and its magnitude at the same time. In its proposed algorithm it first checks if the given step size satisfies the stochastic line search. If yes then just use the step size and do the SGD update. Otherwise, it builds linear models around two points and combines these two models and minimizes this new model and this becomes the new iterate value. \n\n",
            "main_review": "1- The paper is an incremental work and is mainly a stochastic variant of Öztoprak et al 2018. This paper doesn't offer anything technical since all the theoretical results are just slight modifications of already existing results. On the other hand, the empirical results also don't show a big advancement. \n\n2-  Generally tuning both direction and step size is not a new approach. For example, the diagonal variant of AdaGrad has a similar approach. Also in the proposed algorithm in each step, it can require a matrix-vector multiplication and it cannot be relaxed. Parameter partitioning is a way to make the matrix block-diagonal and to reduce the matrix-vector multiplication. However, in the paper, it is not clarified why partitioning is useful. \n\n3- The paper claims that unlike line search there is no backtracking step. However, to set \\eta their proposed algorithm needs backtracking. \n\n4- It hasn’t been mentioned in the paper how many times each experiment ran? It would be more reliable if each experiment runs several times and the average of them is plotted. \n",
            "summary_of_the_review": "The paper is an incremental work and is mainly a stochastic variant of Öztoprak et al 2018. This paper doesn't offer anything technical since all the theoretical results are just slight modifications of already existing results. On the other hand, the empirical results also don't show a big advancement. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors proposed a method called stochastic model building (SMB) that uses a combination of existing techniques to get faster convergence in stochastic non-convex optimization. In particular, they use a stochastic adaptation of the model-building globalization strategy from Oztoprak and Birbil (2018), in which the deterministic Armijo condition check is also computed using stochastic gradients. By re-writing their update as a preconditioned SGD step, they are able to bound the spectrum of the preconditioner. This allows them to obtain convergence results for smooth and non-convex objectives by directly invoking the analysis of Wang et al. (2017). Experiments demonstrate that with additional heuristics, their proposed method can outperform state-of-the-art optimizers on common deep learning benchmarks.",
            "main_review": "**Strengths**\nThe proposed method SMB/SMBi has several advantages. First of all, unlike general preconditioned stochastic gradient methods, the preconditioner here is a blockwise diagonal matrix where each block corresponds to a parameter group in the deep neural network. In addition, because the preconditioned update is only expressed so to facilitate theoretical analysis, the actual implementation itself does not involve computing or storing matrices at all --- only vector inner products are involved in each parameter group update. This is particularly suitable for implementation in popular deep learning libraries like PyTorch where parameters are stored in groups. \n\nSMB also does not use backtracking as is typical in line search with the Armijo condition. When the condition is failed for the stochastic gradient direction, SMB simply computes a new direction based on a stochastic \"quasi-Newton\" update, and take a step along this direction instead. This lets SMB avoid potentially doing many additional forward passes which can be expensive. \n\nFinally, the empirical results show that by using additional simple heuristics to adjust the step size based on how often the Armijo condition has failed in the previous epoch, SMB can achieve state-of-the-art performance on CIFAR10 with a ResNet34 architecture, both in terms of the training loss and test accuracy. \n\n\n**Weaknesses**\nThe main concerns I have with this paper are:\n1. Although convergence results are given in the non-convex setting, the analysis is rather trivial. The crux of the proofs for the theorems is to first re-write the SMB/SMBi update as a preconditioned stochastic gradient step $x_{k+1} = x_k - \\alpha_k H_k g_k$, and this part comes directly from the work of Oztoprak and Birbil (2018) except here the deterministic gradients are replaced with their stochastic counterparts. Then the proof boils down to bounding the extreme eigenvalues of each of the diagonal blocks in $H_k$. This part of the analysis is also nearly identical to what's in Oztoprak and Birbil (2018). Finally, with the extreme eigenvalues at hand, the non-convex results given by Wang et al. (2017) can be directly invoked to obtain Theorem 1 and 2 in this paper. The theoretical analysis therefore is simple as previous works have done all the heavy-lifting,  but at the same time I consider the contribution to be neither novel nor significant.\n2. Because the algorithm is directly adapted from that of Oztoprak and Birbil (2018), the preconditioner $H_k$ would be correlated with the stochastic gradient. This would not fit into the analysis of Wang et al. (2017) as their theory requires $H_k$ to be independent of $\\nabla f(x_k,\\xi_k)$ conditioned on the past. The algorithm in Wang et al. (2017) constructs $H_k$  based on a truncated history of the past stochastic gradients and therefore is independent of the randomness in iteration $k$. To circumvent this issue, the authors here suggest to analyze a different version of SMB, which uses an independent minibatch to compute $H_k$, and the variant is named SMBi. Although this lets the analysis go through, it inevitably introduces a *gap between theory and practice*, since the experiments demonstrate that SMB almost always perform better than SMBi. This observation makes sense as although we are using stochastic quantities to construct the model, the model can be seen as a locally approximation to the minibatch objective, while using independent batches to compute the model does not seem to make a lot of sense. \n3. Figure 1: Although it is true that in this particular case (seed, rather), \"$s_k$  has a smaller length compared to the trial step $s_k^t$, the step we would have taken with $s_k^t$ is usually scaled with a step size instead of the unit step size taken by $s_k$ in SMB. More importantly, the direction obtained in $s_k^t$, the stochastic gradient step depends on which feature vector $\\xi_k$ is sampled, and so it shouldn't always be the case that $s_k^t$ \"lies along a direction decreasing the function value\" while $s_k$ does not, unless this can be proven otherwise. For these reasons I find this illustration slightly misleading. \n4. Robustness to step size: assuming the experiments in Figure 6 are using constant step sizes, it is unclear to me what the point of robustness to step size means given that SMB/SMBi only works well in general when step size auto-scheduling is required. If the authors indeed recommend the step size heuristic in Section 4 to be adopted in practice, then I see little value in this robustness experiment, but I could be interpreting the experiment incorrectly. Instead, I would recommend a robustness experiment on the other hyperparameters such as the percentage threshold mentioned in Section 4 and the step size multiplication factor. The last issue I have with Figure 6 is it seems like for MNIST-MLP, different step size can lead to an order of magnitude difference in the final training loss. Does that imply SMB isn't very robust to the step size on this dataset/model combination?\n5. It would be great to see a replication of the auto-scheduling experiment on CIFAR100-ResNet34 as well, especially to see whether with the auto-scheduling, the \"converge fast but generalize worse\" observation in Figure 3 (c,d) can be mitigated.\n\n**Minor concerns / typos**\n- In the objective (Equations (1) and (2)), the dimensionality of the problem is $n$; however, $n$ is later also used to denote the number of parameter groups at the top of page 3. The latter implies the overall dimensionality of the problem should be $\\sum_{j=1}^n p_j$ where $p_j$ is the number of parameters in group $j$. \n- It's unclear to me why $f_k=f(x_k,\\xi_k)$ is not expressed as the sum of individual function values of a mini-batch, as you do for the stochastic gradients $g_k$. I suppose it should be and this is just to avoid cluttering, or are the $f_k$'s computed in some other way?\n- Top of page 5: when concluding that $\\mathbb{E}[H_kg_k] = H_k\\nabla f$, one should be more explicit that the expectation is taken over $\\xi_k$, the first minibatch sampled at that iteration, excluding the randomness in $H_k$, since $H_k$ is also a random quantity it's just that its randomness comes from a separate, independent minibatch. \n- Top of page 6: \"For given $T$, $R$ be a random variable\" -> \"For *a* given $T$, *let* $R$ be a random variable\"\n- Proof of Theorem 1 (page 11, section A.1):\n\t- First line: \"can be expressed a special\" -> \"can be expressed *as* a special\"\n\t- Please use a different notation for $\\sigma=(y_k^\\top s_k^t + 2\\delta)^2 - \\|s_k^t\\|^2 \\|y_k\\|^2$ as $\\sigma$ is already used in Assumption $\\mathbb{E}_{\\xi_k}[\\|g(x_k,\\xi_k)-\\nabla f(x_k)\\|^2] \\leq \\sigma^2$.\n- Proof of Theorem 1 (page 12, section A.1):\n\t- Second equality in upper bounding $\\lambda_{\\max}$: the last $+y_{k,p}^\\top g_{k,p}$ should be a minus instead. (typo)\n- It would be helpful to further explain the motivation behind the *stochastic model building* part of the algorithm, rather than just citing away Oztoprak and Birbil (2018). In particular, a justification of why their globalization strategy should also work well in the stochastic setting would significantly strengthen the paper.\n\n---\n**References mentioned**:\nOztoprak and Birbil (2018): An alternative globalization strategy for unconstrained optimization\nWang et al. (2017): Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization \nNocedal and Wright (2006): Numerical Optimization\nVaswani et al. (2019) : Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates\n",
            "summary_of_the_review": "Although the paper proposes an interesting, easy-to-implement algorithm for non-convex stochastic optimization, I am hesitant to recommend for acceptance at this point for its lack of significantly novel contribution and some confusing aspects of the experiments. I am willing to increase my score if my concerns are well addressed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}