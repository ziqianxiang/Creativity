{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes a novel ensemble method, CDA^2,  in which base models collaborate to defend against adversarial attacks. To do so the base models have two heads: the label head for predicting the label and the posterior probability density (PPD) head that is trained by minimizing binary cross entropy between it and the true-label logit given by the label head. During inference the base model with the highest PPD value is chosen to make the prediction. During training base models learn from the adversarial examples produced by other base models. \n\nThe evaluation of the manuscript of different reviewers was very diverse, resulting in final scores ranging between 3 and 8 after the discussion period. While the rebuttal clearly addressed the concerns of one reviewer and several additional experimental results were added for different adversarial attacks, it did not fully addressed the concerns of another reviewer, who rated his confidence higher. He was also not convinced by the update in the revised version of the manuscript, in which crucial changes in the pseudocode describing the proposed algorithm were made, which contradicted some statements in the first version. Therefore, the paper can unfortunately not be accepted in its current version. In a future version of the manuscript, the description of the algorithm and of he role of the PPD head should be improved and experiments on another dataset next to CIFAR-10 could be added."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a new paradigm for defending against adversarial attacks with multiple sub-models.\nDifferent from ensemble, the proposed collaboration paradigm, a representative sub-model is chosen to make the decision, instead of letting all sub-models vote.\nThe proposed method has been validated on CIFAR-10 dataset, against both white-box and transferrability-based black-box attacks.",
            "main_review": "Strengths\n1. The proposed method is clearly motivated and defined. And it has been demonstrated effective by quantitative experimental results.\n2. A comprehensive overview of related works is provided.\n\nWeaknesses\n1. Training cost is also a notable issue for adversarial training methods. It would be better if the training cost of the proposed method is compared with that of the previous methods.",
            "summary_of_the_review": "This paper presents a new paradigm involving multiple sub-models which has several advantages compared to ensemble.\nIt has been validated effective by the quantiative results in terms of robustness, but comparison in other aspects such as training cost could be also helpful.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper firstly analyzes prior adversarial defense methods using ensemble strategy and claims that this method could cause a waste of model capacity. To improve the utilization of (multiple ) model capacity, the author proposes an interesting collaboration strategy $CDA^2$ to defend against adversarial attacks. Specifically, the author develop a dual-head model structure: one is for making a prediction and the other is for predicting the posterior probability of the input. During training, each model can address the adversarial attacks of other sub-models so that it improves the robustness of the collaboration. The experimental results partially verify the superiority of the proposed methods.",
            "main_review": "# Strength:\n\n1. This work considers the adversarial defense problem. It focuses on the insufficient model capacity of adversarial training and presents a completely fresh perspective on learning multiple models to improve the robustness. The proposed framework is technically solid. The idea of collaboration to minimize the vulnerable area is innovative compared with baselines. In summary, I assess a high novelty of this work;\n\n2. The proposed framework CDA^{2} makes sense for the goal of minimizing the vulnerable area of the overlap. A dual-head model is used for predicting and providing information about how to assign the adversarial sample. Optimizing the best-performing sub-model is to minimize the vulnerability overlap of all sub-models. The framework has contributed to the research problem and may enlighten other research problems. It could be a good paper for the ICLR community.\n\n3. This work is well-written. Figs. 1 and 2 show the motivation of the proposed method clearly, which helps understand it. \n\n# Weakness:\n\n1. the author states that the insufficient model capacity can hurt its performance in adversarial training. What if the model has sufficient model capacity? Are there other scenarios that have insufficient model capacity? Whether CDA^{2} can be useful for such scenarios if any? Compared to a big single model, can the author discuss more pros and cons of collaboration?\n\n2. To achieve collaboration, the author proposes to assign the samples to the sub-models that perform best. Does it obtain a trivial case? For example, only one sub-model has been trained. Can the author discuss more? \n\n3. For the framework CDA^{2}, ppd head is for evaluating the performance of the other head. Can the author discuss the impact on the quality of this head more?\n\n4. The experiment on the XOR problem is a little bit weak. Can the author provide more experiments to verify the effectiveness of CDA^{2}?\n\n5. For the experimental results on the white-box, the author gives the results in Table 1. The experimental setting is a little vague. I’m wondering about the robustness performance of PGD-50 as in [1];\n\n6. For the black-box experiments, the author uses M-FGSM and PGD to generate transferable adversarial samples. Can the author provide more experimental results to validate its claims?\n\nOverall, there are still some issues stated above. I will increase my score if they are well addressed.\n\n[1]. Dverge: Diversifying vulnerabilities for enhanced robust generation of ensembles. In NeurIPS, 2020a.\n\n========\n\n# Post rebuttal responses:\n\nThanks for the authors' responses. They address my concerns on the concept of collaboration, and the newly added experiments are convincing. Especially, I like the idea of collaboration in adversarial training, which is a new paradigm to defend against adversarial attacks. \n\nBesides robustness, the new paradigm may be helpful to other domains.\n\nAfter reading the review from other reviewers and the corresponding responses, I vote for acceptance and increase my scores further.",
            "summary_of_the_review": "A novel and interesting collaboration method for advancing the robustness of multiple sub-models.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Nothing.",
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes an ensemble or mixture-of-experts method to defend against adversarial examples. Though the authors prefer to use the term collaboration method to highlight its difference from vanilla ensemble.\nThe main idea is that, during adversarial training, the sub-models are trained on each other's adversarial examples.\n\nSpecifically:\n* for each training image, one adversarial example is generated per sub-model by carrying out an attack on each sub-model.\n* each adversarial example is (softly) assigned to the sub-model that has the lowest loss on it as a training image.\n* each sub-model has a second output called PPD that quantifies its confidence.\n* at inference time, for each input, the sub-model with the highest PPD produces the output.\n\nThe rationale is that, because each sub-model only needs to cover part of the adversarial example space, they can do a better job.\nExperiments on CIFAR-10 with L_inf attacks are reported.",
            "main_review": "The idea of adversarial training on each other's adversarial examples is new. However, it is flawed, at least in its current form.\nThe issue is that such adversarial training may not provide enough coverage and, after the training converges, there may exist adversarial examples that can attack all sub-models.\n\nConsider a hypothetical situation with two sub-models. Both sub-models classify clean images well. However, sub-model A is easily attacked by adding a faint cross pattern near the upper left corner, and sub-model B is easily attacked by adding a faint cross pattern near the lower right corner.\n\nDuring the proposed training procedure, we'll only encounter these two types of adversarial examples because they are respectively the best attack with lowest Lp epsilon on the two sub-models. Then the two sub-models learn to solve each other's adversarial examples: sub-model A will become robust against faint cross patterns near the lower right corner and sub-model B will become robust against faint cross patterns near the upper left corner. The training procedure therefore converges quickly.\n\nHowever, after the adversarial training has converged, a vast space of adversarial examples has not been explored at all. There may exist adversarial examples that, although their epsilon is slightly higher than the faint cross patterns, they can attack both models.\nThe above discussion can be easily extended to the general case of M sub-models. The point is that the proposed adv training procedure can be self-limiting and may not provide enough coverage. \n\n \nThe experimental results are not sufficient to support the claims.\n* Standard accuracies on clean images are not reported in Table 1. They are critical missing information.\n* The CIFAR-10 model in (Madry et al. 2018) has robust accuracy of 45.8% against L_inf epsilon of 8/255=0.031 with PGD-20. Table 1 shows that the proposed method has robust accuracy of 44.5% against L_inf epsilon of 0.03 with PGD-10. It's unclear that the proposed method has an advantage.\n* This paper seems to confuse black-box attacks and transfer attacks. Tables 2 and 3 are transfer attacks. There are no black-box results in this paper.\n\nThe role of the PPD head is not explained well.\nAccording to Figure 3(b) and Algorithm 1 line 6, the PPD head is trained by minimizing binary cross entropy between it and the true-label logit from the normal output.\nIt seems that because the PPD head is trained to also track wrong predictions of a sub-model, it can serve as a confidence score at inference time.\nEquations (11)(12)(13) and the surrounding text do not help and they seem irrelevant to the actual implementation.\n\nThe philosophical idea claimed in the conclusion section is not new. What's new here is the adversarial training scheme where sub-models train on each other's adversarial examples.\n",
            "summary_of_the_review": "1) Unfortunately the main idea is flawed and the proposed adversarial training may converge without providing enough coverage.\n2) The experimental results do not show advantage over previous non-ensemble method.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}