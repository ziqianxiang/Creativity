{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "Three out of the four reviews rated this paper well below the acceptance threshold. Although the review scores show a relatively large spread, I think that the review contents are more or less coherent across the four reviewers.\nThe equivalence of the state equations (SEs; a set of equations that macroscopically characterizes optimal solutions of certain high-dimensional regression problems) derived from three different approaches (AMP, CGMT, and LOO) is well expected to hold, as the optimal solutions should be independent of how their macroscopic characterization in the form of an SE is derived, and this paper concretely showed such equivalence to hold for three problems. More concretely, Theorem 1 states the equivalence of the SEs for M-estimator derived from the three approaches, Theorem 2 states the equivalence of the SEs for LASSO derived from AMP and CGMT, and Theorem 4 states the equivalence of the SEs for logistic regression derived from LOO and CGMT. The main concern raised by all the reviewers is that this paper does not provide novel and significant insights as to why and how the equivalence arises.\nSome reviewers also pointed out that this paper lacks citation to the relevant statistical-mechanics literature, as well as that this paper contains so many typos, grammatical errors, and inappropriate typesetting styles. The authors responses were not instrumental in persuading the reviewers with negative evaluation. On the basis of these I would not be able to recommend acceptance of this paper for presentation at ICLR 2021."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper establishes equivalence of state evolution for approximate message passing (AMP), convex min-max Gaussian theorem (CGMT), and Leaving-one-out (LOO) for three different problems: 1. a robust linear model estimator called the M-estimator throughout the paper; 2. Lasso; and 3. logistic regression.",
            "main_review": "-- Strengths:\n- Establishing the connection between these different state evolutions is interesting. Of course we expected them to all have the same fixed points for MSE but showing that even the recursions are the same is novel and interesting.\n\n-- Weaknesses:\n- The main issue I have with this paper is significance and novelty of the results. As I said, it is interesting to show that the recursion can be turned into each other for all these methods, but the paper does not give any intuition what the implications of such results are? The authors just say that they hope this leads to finding a deeper structure in such high-dimensional problems. I find it interesting to see that the SEs can be turned into each other, but I believe it is not surprising and unless showing this equivalence leads to more interesting results, I do not see this result to be significant enough to be accepted for ICLR.\n\n- Theorem 3 is not new. It is well known that fixed points of AMP correspond to critical points of Lasso objective function and the min-max problem in CGMT is of course equivalent to Lasso which is essentially the result of the theorem. In fact, AMP and its variants such as VAMP can be used to solve many problems and showing that their fixed points if they converge satisfies the first order optimality conditions is rather easy.\n\n-- Minor comments/typos:\n- The paper has some grammatical and typographical errors. It needs a quick revision to correct for such errors. For example:\n1. spacing between a word and parenthesis after it is often missing.\n2.  beginning of page 8: self-content\n3. remark 3.1: equivalent -> equivalence\nand many more.",
            "summary_of_the_review": "I believe the contributions of this paper are not significant enough.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper provides derivations of equivalences between what they refer to as \"state evolution\" (SE) equations derived from different perspectives: convex Gaussian min-max (CGMT), approximate message-passing (AMP) and leave-one-out (LOO) approaches. They focus on M-estimators for high-dimensional linear regression. The SE equations describe the mean-square error reconstruction of the M-estimator.",
            "main_review": "The only strength for me is the a-priori good intention of the paper; as a specialist of the field I was a-priori interested by the paper's goal, but unfortunately it did not convince me at all. \n\nThe weaknesses are:\n\n0) it is not written (nor very interesting) for the ICLR audience; it is a paper for specialists, on a a-priori interesting field to ICLR readers, but the way it is written makes it completely inaccessible to non-specialists.\n\n1) It is badly written and full of typos. The authors should make a important effort to make it readable.\n\n2) It clearly is oriented towards specialists of the field but most of the text is only recalling known results by others. \"Pedagogical parts\" like \"Derived SEs from AMP\", \"Derived SEs from CGMT\", \"Derived SEs from LOO\" are completely useless: a specialist does not need it (and anyway it is badly written I believe), and a non-specialist will get nothing out of it.\n\n3) The main results are very restricted: a) they only considered M-estimators for linear regression (and to a lesser extend logistic regression) with random design; b) the results, being (sometimes quite straighforward) consequences of change of variables which link the non-linear equations derived from the 3 different perspectives, bring absolutely no insight to the reader, other than a formal equivalence.\n\n4) The authors seem to miss important knowledge of the field: not a single paper from the community of physics of spin glasses is cited. But the leave-one-out approach is actually the cavity method from spin glasses with just another name. Having some knowledge on the cavity method, it is completely obvious that the SE equation derived from leave-one-out = cavity method yields he same as AMP, because AMP is just an approximation to Belief Propagation, which is derived from the cavity method. I mean: AMP or BP are just the algorithmic counterpart of the cavity method, and the cavity method is precisely designed to study the statistical properties of BP/AMP, and there are thus of course equivalent up ti simple transformations that simply depend of the details on how the computation have been performed. So to me, this part of the \"equivalence results\" was trivial and know for a very long time. Not everyone is familiar with these concepts and literature, so a-priori it may be interesting to show a relation between AMP and LOO, but it is not possible to do so by completely skipping all that is known.. \nAgain, what is really not re-assuring, is that the authors did not even mention these know facts and related literature.\nThe part on the equivalence between CGMT and the rest seems more interesting to me. But in the appendix the authors repeat in their own way computations mostly known already in the literature (that they cite: Thrampoulidis et al, etc). And because their computations bring no additional insights, I don't see the added value (as such) of the contribution.\n\n4) Whole chunk of related literature are completely skipped and not discussed or even cited; it misses too much here to provide a list",
            "summary_of_the_review": "To summarize, I believe that despite the potential A-PRIORI interest by some specialised readers, the paper will not convince these specialist readers either. And it will noot bring much to non-specialists. It is badly written, and no real insights are provided. Many computations are just adaptations of things present elsewhere. Many known facts and equivalences are not cited nor even alluded to (again the equivalence between the AMP state evolution and LOO = cavity method is known to any physicist with some experience in spin glasses). The relevant literature is hugely missing. \n\nI suggest the authors to completely re-write the paper, and consider instead a direct journal submission, I really do not see the point too submit this work to ICLR. And, importantly, to cite much more carefully and expmain more thoroughly what is new or not in the paper (as such, I do not believe that much is actually new).",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper considers the state evolution equations obtained via different methods (AMP, LOO and CGMT), and it shows that their fixed points are the same for some high-dimensional inference problems (M-estimation, LASSO, and logistic regression).\n\nMost of these state evolutions were obtained in recent work (which is properly cited here). An exception is Proposition 3.2, which gives the state evolution for CGMT in the case of LASSO. However, the proof of this result closely follows the existing literature. Consequently, the main novelty of this contribution is in showing that the fixed points (when unique) are the same for different methods. ",
            "main_review": "STRENGHTS\n\nAMP, LOO and CGMT have been widely studied in the recent literature, and it has been already pointed out that, in some cases, results concerning the performance of a certain high-dimensional inference task can be obtained using different methods (with very different proof techniques). Thus, understanding the deep connections underlying this fact is certainly a worthwhile goal. \n\n------------\n\nWEAKNESSES\n\nThis paper falls short from this goal and does not offer much insight into *why* the fixed points are the same. The proofs of the equivalence of the fixed point are rather simple and they do not shed much light into the deep connections between e.g. AMP and CGMT. Proposition 3.2 appears to be novel, but its proof can be derived from existing work.\n\nFrom a stylistic viewpoint, the paper contains several typos (especially in the appendix; e.g. Appendix D.2 is repeated twice). Thus, I would recommend the authors to perform a thorough check during the revision.",
            "summary_of_the_review": "While the paper focuses on a very interesting subject, unfortunately it does not seem to offer much insight into why the different state evolutions lead to the same fixed point. As a result, the contribution of this work appears to be clearly below the acceptance bar. ",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper compares the so-called \"State Evolution\" equations from 3 different derivations for the MSE of \nhigh dimensional M-estimators in the proportional asymptotic regime (number of samples and features go to infinity at a fixed rate).\nThere are currently 3 well-known frameworks for analysis in this regime:\n1. Approximate Message Passing (AMP)\n2. Convex Gaussian Min-Max Theorem (CGMT)\n3. Leave One Out (LOO)\nThe paper shows that there lies an equivalence between the results derived by the 3 methods via a parameter transformation.\nTwo important special cases of LASSO and Binary Logistic Regression are considered and the results are clearly stated.",
            "main_review": "While the main results of the paper were believed to be true by many researchers, to my knowledge this is the first paper that lays out clearly what the equivalence is between the outcomes of these 3 techniques. Hence the contribution of the paper is undeniable.\n\nHowever, on closer inspection, the paper does not provide any insight into why this equivalence holds, most of the explanations are cursory at best.\n\nTo my knowledge, the derivation from the CGMT technique is based solely on the fixed points of an optimization problem. This provides a somewhat geometric method since KKT conditions are essentially geometric in nature.\nOn the other hand, the AMP derivation is essentially carefully constructing a dynamical system that has an equivalent scalar dynamical system. That this dynamical system reaches the fixed point of an optimization problem is an added benefit. Thus AMP-based derivations are in principle based on some iterative decorrelation given by the Onsager correction. This ends up being some sort of adaptive curvature adjustment in an energy landscape.\n\nThere is little discussion about the LOO technique or insights related to it. The bit on page 8 is more confusing than clear.\n",
            "summary_of_the_review": "The claims made in the paper are correct to the best of my knowledge. \n\nThe paper deserves to be published even though it lacks in several other ways, mainly because it is the first of its kind in establishing an equivalence between 3 different derivations for the asymptotic analysis of estimators.\n\nThis body of work can serve as a way for researchers using one method to check their predictions with another method using a parameter transformation suggested in this paper.\n\nHowever, there is serious room for improvement in the presentation of the material discussed. The lack of insight provided is also a concern but perhaps can be clarified in future works.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}