{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a scheme named MID to improve node drop pooling methods. MID extends the original one-dimensional score space to a multidimensional score space to capture the significance of nodes from different views. It further utilizes the filpscore and dropscore operations to highlight dissimilar features and to maintain the structure diversity. Experimental results show that the proposed method can promote the performance of 4 node drop pooling methods on 17 graph classification datasets.",
            "main_review": "Strengths:\n1. The proposed scheme is easy to plug-and-play on node drop pooling methods to improve their performance. The sacrifice in training time is acceptable.\n2. Experimental results are good.\n\nWeaknesses: \n1. The authors pointed out that current node drop pooling methods ignore the diversities in graphs and thus they propose MID that captures/promotes node-feature diversity and graph-structure diversity. However, it is unclear why such diversities will be beneficial to graph-level representation learning and subsequently to downstream tasks. The motivation is lacking here. \n2. Every time when calculating the multidimensional score, the dimension of node feature X will be increased by h times. The increase in computational complexity will be a big problem for hierarchical pooling methods. Is the improvement mainly caused by the increase of parameters? What would happen if we reduce the dimension back after calculating the score? \n3. It is doubtful that the multidimensional score can capture nodes with dissimilar features. If S_multi^a=[-5, 5] and S_multi^b=[5,5], they will get the same S_{L_1} after flip.\n4. In the case study of dropscore, which node was chosen to be dropped? It seems that the node selection result of the visualized case would be greatly influenced by the dropped nodes.\n5. The architecture shown in Fig. 2 implies that MID applies flipsocre and dropscore operations independently to get two output graphs. In this case, how to combine the two output graphs?\n",
            "summary_of_the_review": "The motivation of capturing graph diversity needs to be elaborated. Although the proposed method performs well in experimental study, some important details of its architecture need further explanation. The analysis of the multidimensional score space is also insufficient.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Graph pooling has become one of the essential components for graph classification. One particular approach for graph pooling is through generating scores for each node and retaining only the highest scoring nodes for the next layer, which results in coarser graphs in the later layers. This paper argues that the existing pooling methods only retain nodes that are similar to each other which leads to sacrificing diversity and losing information. Three strategies have been proposed to increase the diversity of the retained nodes and empirical results show the benefit of diversifying the retained nodes through the proposed strategies.",
            "main_review": "[Problem Motivation] The motivation for selecting a diverse set of nodes from a graph seems reasonable and I believe a proper solution to this problem could even have applications beyond graph pooling. It would make the motivation even clearer if an example is provided in the intro (e.g., for one of the graphs in the datasets of Table 8) where selecting similar nodes results in losing critical information and consequently misclassification.\n\n[Solution Motivation] I found the motivations behind the proposed approaches for diversifying the retained nodes rather brittle. This is especially the case for the flipscore and dropscore operations. It feels like these two operations are introduced only for the sake of adding some kind of diversity, but it is not clear what kind of diversity is added, or whether such a diversity is reasonable. I provide more details in the next two paragraphs.\n\n[Flipscore] It is claimed that scoring the nodes based on L1 norm of the scores may result in selecting nodes that are dissimilar/diverse. To me, it seems like it still favours similar nodes; it’s just that instead of favouring nodes that are similar in terms of, e.g., Euclidean distance, it favours nodes that have a high resemblance in some other notion of similarity (e.g., nodes whose initial node features have a higher magnitude). So why should one notion of similarity be better than the other one? Also this makes the results in Fig 4 less exciting because the formulation of information gain is a function of Euclidean distance, and it only shows that the selected nodes are more diverse with respect to this specific notion.\n\n[Dropscore] Let’s consider a graph that has a large subgraph with M similar nodes and several other small subgraphs. The base models may end up selecting the K most similar nodes from these M nodes. When adding dropscore, all that changes seem to be that K random nodes from the most similar \\alpha K nodes are selected which may not lead to actually selecting from the other modes/subgraphs of the graph. \nOne other thing that is not entirely clear to me is how dropscore helps during testing. It is mentioned that dropscore is only used during training which makes it unclear how it can help for test graphs.\n\n[Results] The results in Table 2 are quite nice and comprehensive. I appreciate that the authors have tested their model with multiple base models and on multiple datasets, including large datasets from OGB. I only have one critism here: from the average ranks in the table, it seems like almost all the baselines in the top part of the table are outperforming the four base models in the bottom part of the table, so it would have been more convincing if MID could be added to those models to see if their combination with MID provides SOTA results.\n\n[Writing clarity] I find the writing mostly clear. But one thing I did not like was the fact that I constantly had to go to the supplementary to read about the experimental setup and then come back to main text. In fact, without reading the experimental setup, the results in Fig. (3, 4, and to some extent 5) are meaningless. I suggest that the authors move the description of the experimental setup to the main text and instead maybe move some of the ablation studies to the supplementary.\n\n[Minor point] Table 1 provides a nice summary of the existing work, but the description for some of the models is too general (e.g., f_att(X)). The table could become a really good reference point if some more details are provided for a few of the models.",
            "summary_of_the_review": "A well-motivated problem, but not entirely well-motivated solutions.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper focuses on the improvement of node-drop pooling methods for graph representation learning, in the stage of node selection.\n\nSpecifically,\n* The authors point out that existing node-drop pooling methods ignore the diversity of node features and graph structures, while it should be considered to effectively represent an entire graph.\n* The authors propose three components, which are simultaneously used with existing node-drop methods, to improve the diversity of node selections in graph pooling.\n* The authors extensively evaluate the proposed method with existing node-drop methods on graph classification tasks, in which they show the performance improvements from the existing drop methods.",
            "main_review": "### Strengths\n* The focus of this work, considering the diversity of selecting nodes for node-drop pooling, is important, but relatively undiscovered.\n* The authors provide extensive experimental comparisons against the existing graph pooling methods.\n* The proposed ingredients, namely multidimensional score space, flip score and drops score, are intuitive and simple, yet effective.\n* This paper is highly well-written and well-structured, especially where the intuitions, and also analytical or case studies of every method are well described.\n\n### Weaknesses\n1. Node-drop pooling methods coupled with the proposed MID do not outperform the state-of-the-art node-clustering pooling methods, but also the authors do not describe the clear merit of using the proposed method against such powerful node-clustering methods.\n> * Since the proposed node selection scheme for node-drop pooling methods has clear merits that it can improve the performance of existing node-drop pooling, it is okay for me to not achieve the best performances among many existing pooling methods.\n> * However, except for the performances, it is required to show other strengths of node-drop pooling methods against other graph pooling methods. This is because, since the proposed method is not the best in terms of performance, there is no need to use it, if there are already available outperforming methods (e.g., DiffPool, EdgePool, and GMT). Thus, in my opinion, the authors need to justify why the readers should use the proposed method, while enduring the performance degeneration of it aginst best baselines.\n> * I think one possible solution is to show the efficiency of the node-drop methods against the node-clustering methods, especially on large-scale graphs.\n> * Also, describing why the node-drop methods cannot reach the performance of node-clustering methods (i.e., explaining the limitation of node-drop pooling schemes) would be great to the community, to further develop ideas in the direction of graph pooling.\n\n2. Corollary 1 should be tone-downed. We can easily find the counterexample.\n> * To be more specific, according to the two example graphs in Figure 13, the modern pooling methods can keep nodes B, C, D, E, and F for graph 1, and keep nodes A, B, C, D, and E for graph 2, then the two given graphs are distinguishable. However, when the proposed random drop scheme drops node F for graph 1, and drops node A for graph 2, then the two given graphs are not distinguishable anymore. \n> * I understand the authors' claim that, thanks to the randomness, the randomly dropped different graphs could be more distinguishable, however, this might be more proper to be stated as a Remark rather than a Corollary as we can find several counterexamples. Also, increasing the probability of different outputs across different graphs does not naturally mean that this can be approximated to the WL test. The supporting facts to connect the proposed MID to the WL test are mathematically weak.\n\n### Suggestions\n* In Figure 2, it is more beneficial to explain which scores are flipped from Filpscore or which nodes are dropped from Dropscore, either in the caption of the Figure or in the text of the main paper.\n\n### Questions\n* In Figure 6 (b), why the performance of the proposed MID goes up, when we increase the number of layers? Is there any reason for this?\n* In Figure 7, the Biochemical datasets achieve higher performance on low drop ratio, while the Social datasets are at the opposite ends. Is this because the Social datasets generally have more edges, compared to the Biochemical datasets? Or is there any other reason for this?\n\n ",
            "summary_of_the_review": "The focus of this work is important and the paper is well-written. However, there is no clear merit to using the proposed MID for graph representation learning, as there are already available powerful methods in terms of graph classification performances, such as DiffPool, EdgePool, and GMT. The authors should discover the strengths of the proposed method against powerful graph representation learning methods, except for the performance which is already lower than baselines.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "Most of the existing node drop based graph pooling approaches do not preserve diversity in the pooled graphs. This paper aims to address this gap and proposes a new module MID which can be plugged-in to existing drop-node based approaches and ensure diversity in the pooled graphs. It introduces two operations - FlipScore and DropScore - to select nodes with dissimilar features and nodes from different local structures in the pooled graph. Experimentally, MID improves the graph classification performance on multiple publicly available graph datasets.",
            "main_review": "Strengths: \n\nDiversity is an important topic which has wide applications in different topics of machine learning and graphs. This paper has rightly pointed out that most of the existing node drop based pooling approaches do not preserve diversity in the pooled graph because they tend to give similar scores to nodes which are similar in attributes and close in the graph structure. The paper has also nicely summarise the existing node drop based pooling approaches in Table 1. The graph classification results are also shown on multiple real-world datasets and compared with a number of state-of-the-art pooling algorithms. \n\n\nWeaknesses / Concerns:\n\nThough the paper has addressed an important topic in graph representation learning and GNN, there are several concerns:\n\n1. The paper explains why existing node drop pooling algorithms fail to preserve diversity in the pooled graph. But it is not clear why diversity is need in the pooled graphs for the downstream graph-level tasks such as graph classification? The paper does not give any theoretical or empirical justification about the need of diversity. This somewhat makes the work less significant. I can suggest authors to come up with synthetic experiments / tasks to motivate it well.\n\n2. The main content of paper has too much of dependency on the Appendix. For example, some experimental setup is described in the Appendix while results are given in the main paper. This makes the paper less self-contained. The authors could have probably moved the Related Work section to Appendix as it mostly repeats things which are already discussed in the Introduction. This space can be used to bring more relevant content from Appendix to main paper.\n\n3. The primary technical contribution of the paper is to propose two operations - FlipScore and DropScore. However, none of these two equations (Eq. 3 and 4) are very clear to me. Can authors please clarify how exactly are they computing Eq. 3? What is the dimension of S_{L_1}? A norm by definition gives a scalar. But authors mentioned that S_{L_1} is a matrix. From Figure 2, it seems that Equation 3 is just taking the element-wise absolute values of S_{multi}. If that is the case, then diversity is not really ensured. Rather, it is just taking extreme values from the rage [-1,+1]. Similarly, DropScore seems to be randomly dropping nodes and selected nodes from the remaining set in the pooled graph. Again, this is also not guaranteeing any diversity. In any case, mathematical equations need to be presented / explained correctly.\n\n4. Proposition 1 shows the result for existing node drop pooling methods. But it is not clear how this result will look like for MID. I also request the authors to make the statement of the proposition more clear by mathematically defining terms such as \"local structure\" in the main paper itself.\n\n5. Table 2 shows interesting improvements. However, the paper does not explain the results well. It just describes the reported numbers in the table and improvements in text in Section 5 (\"Overall Results\") without giving any insight.\n",
            "summary_of_the_review": "The paper worked on an interesting idea. But it did not motivate the problem well. There are also multiple concerns in the proposed approach, mathematical correctness and overall presentation. I don't think that the paper is ready to be accepted in the present format.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "There is no ethical concern for this publication.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}