{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Focus is on 1. Simultaneously optimizing patch for content and position, and 2. Improve transferability of patch, so that if it performs well on reference model, it also performs well on target (black box) model. Both objectives are not related. \n",
            "main_review": "Strengths:\n+ Great writing quality.\n\n+ Innovative use of RL.\n\nWeaknesses:\n- Incremental work. Not sufficient for ICLR.\n\n- Low technical contribution.\n\nDetailed Comments.\n- This work is incremental in nature. It combines finding the best patch (i.e. content), and finding the best location for the patch (i.e. position), using Reinforcement Learning. Could you mention what are the challenges to combining these two?\n\n- Although I think the framework has shown good engineering effort, the technical contribution of this paper is low. Could the authors mention the technical contributions of this work?\n\n- I would like to see an evaluation of this approach Vs. existing patch + existing location approach naively applied together. It would be interesting and worthwhile to see if the approach performs better than this baseline. I think the baseline considered currently are too weak, and don’t make a fair comparison.\n\n- I would also like to see a comparison of your approach with the alternate iterative optimization method that you have mentioned in your paper.\n\n- The figure 3 samples are not very typical for a stealthy patch. I think that when patches are placed directly on the face, the stealthiness of the attack is lost. I am also curious about the point mentioned in Section 3.1 that if you are in-fact limiting the pasting position to the area that does not cover the facial features, then this shouldn’t actually happen. \n\n- As an offshoot of the previous point, I think that choosing the mask is very important. It seems like the approach is greedily assigning un-stealthy positions to the patch (e.g. on the face), where it is sure to have maximum impact, but also sure to be least stealthy. Could your approach accommodate position-stealth? \n\n- A related point is that an evaluation of your attack on another domain (i.e. other than face recognition) could be worthwhile. For example, traffic signs. \n\n- I am also curious to know, what is the trade-off between joint optimization of patch content and position Vs. patch detection? Could you evaluate this? It would be interesting to learn whether the best content + position combination, although very effective, maybe equally easy to detect.\n\n- Figures could be improved (vector images, bigger fonts).\n",
            "summary_of_the_review": "Since this paper lacks enough technical contributions, I don't think the current version can be accepted to ICLR. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a method to optimize the position and content of the perturbation at the same time to create more transferable adversarial patches. By utilizing deep reinforcement learning framework, it can predict the position, model weights, and attack step size simultaneously, which significantly improves attack performance, especially in targeted attacks, and still has good adaptability in the physical environment.",
            "main_review": "Pros:\n\n* The simultaneous optimization of position and content is a novel idea because traditional methods usually use fixed positions chosen by randomness or experience. It takes advantage of the coupling between position and content, rather than treating them as independent factors like alternate optimization. Interestingly, the authors also provide some insights into this coupling through some visualizations in Appendix C.\n\n* For solving these two different types of parameters, the use of deep reinforcement learning is ingenious. The author maps the query-based attack process to the reward feedback mechanism in RL. The Unet-based design in the agent is a good way to connect the relationship between position and attack intensity. The policies of various continuous and discrete parameters and the derivation of the solution process are also relatively rigorous.\n\n* Experiments show that the method can achieve good attack effect with a few queries. For practical scenarios like face recognition, this paper provides a lot of performance verification in physical environment, which is also worthy of praise.\n\n* For model ensemble attack, the statistical results of weight change of each surrogate model in the solving process also provide some inspiration for enhancing the attack transferability.\n\nMeanwhile, I still have some concerns as follows:\n\n* The process of face recognition includes in liveness detection, face detection, face recognition and other links. In the attack setup, the author ensures that the face before the attack can be correctly recognized. But after pasting the adversarial patch, does the success of the attack include patches interfering with the detection link? Or does it only include the interference with the final identification process?\n\n* In the experimental section, the choice of surrogate models is a bit confusing. If my understanding is correct, the experiment in Table 1 uses the remaining models excluding the target model as surrogate models, while the choice in Figure 4 contains the target model. Although the regular trend of the two results is consistent, it is somewhat unintuitive to understand.\n\n* In Page 5 \"We use the policy gradient Sutton et al. (1999) method\", \\citep{} should be used instead of \\cite{}.\n",
            "summary_of_the_review": "This paper proposes a method for transferable adversarial patches, which optimizes the position and content of the perturbation simultaneously. This is novel and interesting to me, and might also be useful to this community. The experimental results are also promising. Though there still remain some minor concerns, this paper is clearly above the acceptance bar.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a method to simultaneously optimize the position and perturbation to generate transferable adversarial patches. The content was generated by ensemble model attack, and the position, model weights, and attack step size are set as parameters. These parameters are dynamically adjusted through a small number of queries in a reinforcement learning framework. Their method not only outperforms the previous methods in attack success rate but also significantly improves the query efficiency. And experiments in the physical environment confirmed the practical and effective application value of the method. ",
            "main_review": "Strengths:\nThis paper generates transferable adversarial patches by optimizing the position and perturbation simultaneously, which exhibits a good novelty compared with previous methods that optimize perturbation values while fixing the position or optimize the position while fixing the content of the patch. They use a reinforcement learning framework to solve this problem, and significantly improve the attack success with a small number of queries.\nThe author provides the code to ensure its reproducibility.\nWeaknesses:\nAlthough this paper shows the results of the adversarial patch in the physical environment, there are no experiments to verify the effect of the proposed attacks on the model with defense measures, such as adversarial training.\nIn addition, optimizing the location and content simultaneously on the model without defensive measures results in a better attack effect, so is it consistent on the model with defensive measures?",
            "summary_of_the_review": "This paper proposes to simultaneously optimize the position and the content, which is the main novelty, and its performance outperforms the previous method. Although the experiments may need to be further improved, it is still a good work worthy of recognition and can be accepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a RL-based optimization of patch position and perturbation for attacking face recognition models. With a RL strategy, they can attack with improved attack success rate and have better transferability on black-box models. ",
            "main_review": "Pros: \n+ The idea of optimizing location is somewhat new. \n+ As seen from the results, the attack success rates of the proposed algorithm is better, as well as the transferability on black-box models.\n\nCons: \n+ The written is not clear. As for the mask generation part, the description of learning location is blurred. As said, \"In each channel Mi(i = 1, ..., n) of the feature map M, the relative value of each pixel point represents the importance of each position\", literally it is not learning to \"locate\", it is rather generation a mask region. Besides, patch region should be a squre-shape region, and how does this algorithm restrain its shape? I'd suggest taking a look at STN [1], the real learning location paper. \n+ Some patch-based attack on face recognition algorithms are missing [2], which make me hard to evaluate its contribution.\n+ The contributions are limited. What's the difference of directly generating the adversarial patch and optimization a mask as well as its adversarial perturbation? Putting the RL-module aside, I do not see any novel module or loss constraints to boost the performance. \n+ Patch adversarial attacks are mostly for attacking real-world systems. I'd suggest that more experiments should be executed including the comparions with SOTA methods of real-world real-time face detection models. \n+ It is not correct to claim that \"They belong to white-box attack against image classification task, while our method is a\nblack-box attack versus face recognition task.\" The algorithm is not a black-box attack as it is required to compute gradient using MI-FGSM. \n\n[1] Spatial Transformer Networks. https://arxiv.org/pdf/1506.02025.pdf\n\n[2] Improving Transferability of Adversarial Patches on Face Recognition with Generative Models. https://openaccess.thecvf.com/content/CVPR2021/papers/Xiao_Improving_Transferability_of_Adversarial_Patches_on_Face_Recognition_With_Generative_CVPR_2021_paper.pdf\n",
            "summary_of_the_review": "As detailed in the Main Review, the contributions of the paper are limited. The mothodology is not clearly described, and some of the experimental comparisons are missing. More importantly, experiments of comparions with SOTA methods of real-world attacks are not conducted. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "Attacking face recognition models using adversarial patches may raise security-critical issues. The authors should talk about the potential issues and further propose some remedy for ameliorating the system flaws. ",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}