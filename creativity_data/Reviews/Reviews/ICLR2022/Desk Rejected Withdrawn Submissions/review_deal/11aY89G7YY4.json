{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors proposed an unsupervised data selection strategy where a small set (under a given labeling budget) of unlabeled data are selected for labeling. Specifically, the unlabeled data are first encoded by a pre-trained self-supervised model to an embedding space. The labeled data are selected by K-means clustering and the proposed utility function. Finally, the selected labeled data along with the rest of the unlabeled data are trained with some existing semi-supervised methods (e.g. FixMatch).\n\nGiven the selected labeled data from the proposed method, the proposed method achieves very competitive performance compared to other SoTA semi-supervised learning and active learning methods. Qualitatively, the selected labeled data demonstrate good class coverage, data balance, and informativeness (Fig2 and Fig5)",
            "main_review": "## Strength\n\n- Substantial performance improvement over the random selection and stratified selection, on both CIFAR-10 and ImageNet. The results on the larger ImageNet show that the proposed method is computationally tractable.\n- Well-motivated in (1) why random selection does a worse job and why stratified selection may not be a fair comparison (shown in Fig2); and (2) the proposed utility function and diversity regularization.\n\n## Weakness\n- The biggest concern the reviewer has for this paper is that the proposed method starts from a self-supervised pre-trained model for data selection. It is unclear in the paper that the self-supervised pre-trained model is trained on ImageNet or on the same target dataset. For example, if the self-supervised pre-trained model is trained on ImageNet and evaluated on CIFAR-10, the proposed method is exposed to more data and information. It would be a fair comparison only if the self-supervised pre-trained model is pre-trained on CIFAR-10 when the final model is evaluated on CIFAR-10, or pre-trained on ImageNet and evaluated on ImageNet.\n- Based on the argument above, the reviewer would expect the authors to design the experiments so that the proposed method and the compared semi-supervised or active learning methods are exposed to the same amount of data and information.\n\n## Clarification\n- How are the labeled data selected after K-means? Simply find the data in each cluster with the highest utility score? Please be more specific for \"We then query one instance from each cluster according to their utility score evaluated according to Eqn. 4.\"\n- How sensitive is the method to the hyperparameter h, t, and $\\lambda$ in the regularization part? Do they need to be tuned for different datasets and for different labeling budgets? How long does it take to find the labeled set on the larger ImageNet dataset? \n",
            "summary_of_the_review": "The technical novelty of this work is limited and the reviewer is not particularly comfortable with the extra information introduced by the self-supervised pre-trained model for data selection. However, the reviewer is fine viewing this as a new problem setting. In this case, the proposed method demonstrates its effectiveness with a very low amount of labeling budgets. The authors could make this paper stronger by designing other baselines that are exposed to the same amount of data and information.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposed a new method for active learning. The method utilizes unsupervised feature learning and selects samples for active learning. Then, a semi-supervised learning method is used for semi-supervised data. The method shows improvements on the existing semi-supervised learning method on Cifar-10 and ImageNet benchmarks. \n",
            "main_review": "+ The paper proposes a new unsupervised data selection which is different from previous works in that the method selects m instances in advance using diversity and information utility.\n+ The method is simple and the paper is easy to read.\n+ The proposed method shows an improvement in the existing methods. \n\n- While the paper renames the research problem as data-centric semi-supervised learning, it is essentially the same as active learning. The author claims that the method requests annotation only once but the reviewer did not find any benefit or difference on this claim over the existing active learning.\n- As the paper is mainly about active learning, the paper should focus more on comparison with SOTA active learning baselines (e.g., [1, 2]). In addition, there is no discussion of why the proposed method obtains better accuracy over existing active learning baselines.\n- In the experiments, there are results on very small fractions of labeled data. It would be better to show a graph that shows accuracy over different fractions of labeled data. From Table 2, it seems like the gain of the proposed method significantly reduces even when we have 250 labels (less than 1%).\n- The method heavily depends on the performance of unsupervised feature learning. Is it still useful for other benchmarks where self-supervised learning does not help much?\n- The proposed selection methods assume that the dataset is well balanced across different classes. How does the method work on long-tailed distribution cases?\n\n[1] Kim, Kwanyoung, et al. \"Task-aware variational adversarial active learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n[2] Guo, Jiannan, et al. \"Semi-Supervised Active Learning for Semi-Supervised Models: Exploit Adversarial Examples With Graph-Based Virtual Labels.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n",
            "summary_of_the_review": "Overall, the paper should present and analyze more on comparisons with the recent active learning baselines. And the method may be limited to benchmarks where unsupervised learning is effective. As there is no theoretical analysis, the paper should explore more benchmarks.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Existing works in SSL focus on improving the learner and pseudo-label quality while the initial set of labeled data to start with is often randomly sampled. In this work, the authors argue that how the initial set is created has a significant impact on the final SSL performance. Specifically, the proposed pipeline consists of 3 steps: (a) unsupervised feature learning, (b) unsupervised sample selection for annotation, and (c) SSL learning using the selected samples. Experiments on CIFAR10 and ImageNet-1K show consistent improvements over the random sampling baseline in SSL.\n",
            "main_review": "Strengths:\n* The idea of improving the quality of the initial set of labeled data in SSL is new. The effort of exploring the recent popular concept of data-centric learning in the SSL setting is appreciated.\n* The advantages of performing unsupervised feature learning to guide the sample selection are well motivated and convincing. Figure 2 also demonstrates the advantages well regarding class coverage, data balance, informativeness, etc.\n* The immediate comparison with the active learning literature is also helpful in comprehending the proposed concept.\n\nWeaknesses:\n* The authors compare the proposed DC-SSL with random sampling as the only baseline. DC-SSL has the advantage of using the well-trained feature representations learned from MoCo/CLIP using additional data, and therefore providing a strong guidance in selecting the samples to annotate. Beside the usage of large pre-trained models, which the baseline random sampling does not have, the novelty of the actual sample selection mechanism is somewhat limited to highly-turned K-NN.\n* Furthermore, the proposed modifications to K-NN require many additional hyper-parameters and they seem to also require careful tuning as shown in Figure 8. It is hard to justify how generalizable the proposed modified K-NN given a new dataset in practice.\n* What would happen if the proposed method needed to be deployed in other tasks that are not in image format so it is hard to obtain a well-trained CLIP model to provide guidance in the manifold during sample selection. Would the method here still work in practice?\n* Figure 1 can be largely improved. The arrows between different components are not easy to follow.\n",
            "summary_of_the_review": "There are several weaknesses in the manuscript as mentioned above. However, in light of the effort on pursuing a new perspective in solving an existing problem, I recommend WA rating for this work. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose an unsupervised data selection process for semi-supervised learning. The key idea is that annotating the right training examples given a labeling budget will improve the performance of a semi-supervised model compared to training with initial random data or stratified examples which the authors claim to be impossible to obtain in practice.",
            "main_review": "The idea of a data-centric semi-supervised model that focuses on selecting the most informative examples is interesting and important however these ideas are not novel. The unsupervised data selection process the authors propose combines several unsupervised data selection and ranking into a framework that is utilized by a semi-supervised model. The idea is similar to active learning albeit under a different setting (no initial seed labeled data and data selection only happens once). \n\nThe major drawback in the paper is lack of extensive testing to justify their approach. The authors need to show that their method adds value to semi-supervised learning by showing experiments for several semi-supervised approaches and showing that using their method for initial data selection improves results of these methods. Experiments are only shown for Fix-Match and on a handful of datasets/tasks. \n\nAdditionally, the authors should show more comparison to active learning. The authors explained in the paper that active learning is not applicable to low shot settings and thus would not perform well in their experiments. I find it hard believing this claim if the authors do not show results to back this up. One idea could be to apply the data-centric approach to an active learning pipeline and show that their method gets better performance when used to obtain labels compared to active learning procedures.\n\nMinor notes:\nSome of the writing is not clear. Sometimes it was hard to parse the message the authors were trying to communicate especially is section 3. Table 1 for the experiments is really confusing. What is the improvement of DC-SSL-Random over? Is it over random sampling or stratified sampling? This can be better presented.\n\nI think the claim that stratified sampling is impossible to obtain in practice should be tendered. One example is a case where you have a balanced dataset with dissimilar classes, it is possible to obtain a balanced data under this setting. ",
            "summary_of_the_review": "The experimental validation of the paper is limited and the paper writing needs to be more clear. Additionally, some of the claims by the author may not hold true in every setting. See above explanation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}