{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper provides a smooth approximation of LeakyReLU based on Gaussian function. Experiments on different dataset and machine learning tasks are provided.",
            "main_review": "1. This paper provides a ton of empirical evidence shows that the new activation function performs well. However, I found no intuition or explanation why this happens.\n2. In CIFAR result, SAU is only compared with ReLU but not with other newer activation function. More detailed comparison is required.\n3. Figure 2 and Figure 3 are very vague. Please make it clear. Moreover, it seems that SAU trains much slower than leaky ReLU.",
            "summary_of_the_review": "Overall, the idea is not new and the performance of new activation function is not well supported and explained. The figures should be edited to be clearer.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a new activation function, named SAU, which is a smoothed approximation to Leaky/PReLU. The activation is derived by convolving a given activation function with a smooth approximation to the delta function. The paper only investigates using LeakyReLU as the target activation and the standard concentrated Gaussian approximation to the delta function. The degree of approximation is controlled via a hyperparameter denoted by $n$, controlling the variance of the Gaussian. The new activation is tested in various settings and with various architectures and is shown to outperform other alternatives to the common ReLU activation function.",
            "main_review": "Main strengths:\n* The new activation function is extensively tested under various settings and using several architectures, suggesting the new activation to be a \"plug-and-play\" replacement.\n* SAU is shown to outperform other activation functions in almost all cases, and some by a significant margin.\n* The approach for deriving a smooth approximation to LeakyReLU, while not novel, is somewhat interesting as it connects GELU and LeakyReLU together. However, this is not explored in the paper.\n\nMain weaknesses:\n* No source code is supplied for reproducing results. This is a crucial ommison in an emperical paper such as this one, making it harder to assess the validity of the reported results.\n* Experiments suggest the new activation function to be less expressive than other methods. This is apparent from the loss curves that show SAU to have a higher train loss throughout training. This is in contrast to what is written in the article, where it is said SAU converge faster but the curve clearly show a slower convergence. This could also be the reason why it outperforms the other activation functions, as it might be a simple case of overfitting -- but further investigation is required.\n* Despite the fact the reported experiments report SAU outperforms other activation functions, it is very difficult to say if SAU is the reason why the results are slightly better.\n* Using n = 20000, as used in the reported experiments, would seem to suggest that the activation function will act exactly like LeakyReLU (PReLU), and yet it seems to still have an effect in experiments. In fact, substituting this value in the formulas in the paper would result in almost no contribution from the additional terms. If indeed this value was used and any difference in results was observed, I would highly suspect some mistake in the implementation. Access to the source code would be beneficial in this respect.\n* The paper offers no argument, other than empirical results, to why this specific activation function should be preferred to other similar choices. If there's no theoretical argument, then some empirical investigation should be done to try and differentiate between this activation function and others.\n\n\nNot strictly a disadvantage, but I think it's important to note that when n is small, the proposed activation is very close to GELU. In fact, one could say that this activation is an interpolation between GELU and LeakyReLU, where n controls which activation it is more similar to. The paper makes not mention of this fact, and this might be the most interesting part of this derivation and a missed opportunity. Moreover, the formula for GELU in the appendix is incorrect. The formula as written is for the approximation of computing GELU, not GELU itself. GELU is equal to $\\frac{x}{2}\\left(1 + \\mathrm{erf}(x/\\sqrt{2})\\right)$, where is SAU is equal (when n=1) to GELU(x) + N(x), where N(x) is the standard normal distribution.",
            "summary_of_the_review": "Despite the large empirical evaluation of the proposed activation function, the overall empirical significance of the results is only marginal, and it is hard to say if the better performance should be attributed to the activation function rather than the specific choice of hyperparameters. Specifically, I'm suspecting the benefits are due to the lower expressiveness of the proposed method leading to better generalization, which would suggest smaller models should have been used for the other activation functions. Moreover, the reported value of n is suspicious and could suggest an issue with the implementation. Finally, for such an empirical paper having access to the source code is critical for proper evaluation, and it would also be helpful for looking into the previously raised issue.\n\nOverall, I think the paper is not yet ready for publication in its current state. Nevertheless, the derivation does have an interesting property of connecting GELU and LeakyReLU, and perhaps that should be further explored in the next revision of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new activation function for deep neural networks. Through the lens of *approximate identities*, the authors come up with the new activation function which is a ``smoothed'' version of ReLU/Leaky ReLU, i.e., the Smooth Activation Unit (SAU). The authors conduct experiments on both vision (including classification, object detection, and semantic segmentation) and natural language processing tasks (machine translation) and compare the performance of the proposed SAU with previous widely used activation functions.",
            "main_review": "Strengths:\n\n1. The 'convolution with approximate identities' method is simple and intuitive, which could serve as a good smooth approximation for ReLU/Leaky ReLU activation functions.\n\nWeaknesses:\n\n1. My main concern is the empirical performance of the new activation function proposed in this paper. Specifically, the SAU is marginally better (<1.0%) than other activation functions on most of the tasks considered in this paper. Meanwhile, Table 2 and Table 3 only consider ReLU as the baseline and ignore other widely used activation functions. Outperforming ReLU on CIFAR datasets is not very convincing, since the datasets are not very challenging and SAU also has more parameters (could be further optimized) than ReLU. On the other hand, there are several widely used activation functions are smooth, for example GELU, and the SAU performs similar as GELU.\n\nSuggestions and questions:\n\n1. The optimizers used on CIFAR and TinyImageNet are different. It would be better to apply the same optimizer for the same task. \n\n2. The hyperparameters are not well tuned, for example, the initial learning rate=0.1 is widely used for vision classification tasks. It would be more convincing to consider more learning rates than just one learning rate.\n\n3. I would like to suggest the authors explore other metrics (beyond in-distribution accuracy/mAP/mIOU/BLUE). For example, does SAU perform better in terms of model robustness? Since the smoothness may have some advantages on model robustness. \n\n4. What is the Lipschitz smoothness for SAU (with different $\\alpha$-parameter)? This might be useful for analyzing some properties of the DNNs.\n\n",
            "summary_of_the_review": "The proposed new activation function is simple and intuitive, but the empirical performance is not very convincing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N.A.",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors introduce an activation function (SAU) based on smoothing of the leaky relu. More precisely, the authors use convolutions and approximate identities to achieve that goal. \nFurthermore, they present the smooth version as a learnable activation function, whose parameters can be optimized via back-propagation and hyper-parameter searches. \nThe authors present the smoothing from the ground up and conclude their theoretical development with the gradients needed for back-propagation as well as a proof that guarantees the activation function allows the network to be an universal approximator.\nThe empirical section describes the behavior of SAU on different datasets and architectures.",
            "main_review": "The paper is technically sound and in general well written. I very much like the clever approach to smoothing and the empirical results seem very beneficial. There are however some concerns regarding the contribution and its development.\n\nThe paper presents a smoothing of the leaky relu based on convolutions and approximate identities. However, there are other smooth approximations to the leaky relu [1], consider for instance:\n\n$SLR(\\alpha, x) =  \\alpha x + (1-\\alpha)\\log(1+e^x)$\n\ngiven this pre-existing smoothing, what is this paper providing as contribution?\nIs it that SAU is closer to the original leaky relu than SLR is? This could be shown by computing the integral of the squared difference between SAU and leaky relu, and then comparing to the difference between SLR and leaky relu. \nI computed that for SLR and leaky relu with an alpha=0.25 and the integral of that difference from -1000 to 1000 is 0.33, which is pretty tight. Could you show the same for SAU? is it maybe tighter? \n\nIs it that SAU is learnable? so is the parametric leaky relu and other learnable activation functions that can approximate the leaky relu. But is SAU special in its parametric form somehow?\nWe can see empirically that SAU has benefits, but are those benefits coming from the smoothing or from being learnable? an ablation experiment could help with that. You should also do a comparison to other learnable activation functions, to see how SAU stacks against modern activations.\nAny ideas of what causes the improvements? could it be that the gradients provided by SAU are higher/better than the gradients of leaky relu?\n\nAll those questions point to the same concern. I believe the idea is good, but the paper needs a bit more development and it is not ready for publication as it is.\n\nThere are other minor suggestions:\nIn Fig. 1, you are wasting lots of useful space. I suggest you have two plots instead, one on the left with a range of -25 to 25 or larger, and another on the right zooming in around 0, to properly show the difference between the kink and the smoothing. This way you show that SAU behaves like leaky relu on the large range and show also your contribution on the smoothing.\n\nIn the introduction you have the sentence “Activation functions are usually hand-crafted”, but you also mention that earlier. Maybe you want to mention that only once?\n\nthere is a typo on the experiments section “nest sections” -> next\n\n[1] https://stats.stackexchange.com/questions/329776/approximating-leaky-relu-with-a-differentiable-function",
            "summary_of_the_review": "I believe the paper has a lot of potential, but there are some concerns about the characterization and benefits of SAU especially given the existing smoothing versions of the leaky relu.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}