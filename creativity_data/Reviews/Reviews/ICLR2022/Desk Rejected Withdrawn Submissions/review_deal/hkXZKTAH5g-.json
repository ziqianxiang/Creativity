{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work proposes matrix product state (MPS) tensor networks for compression of image data. The authors propose to approximate input image matrices (order-2 tensors) with a tensor train comprising order-2 and order-3 tensors and claim possible task agnostic compression of the datasets. Experiments are reported on CIFAR, FashionMNIST for two different compression levels and the MPS compression is shown to fare better than other (simple) baselines. Several important details are unclear/missing in the manuscript making it difficult to fully understand/review this work.\n",
            "main_review": "Strengths: \n+ The idea of using MPS for data compression is an interesting one. Prior works such as [1] (not discussed in this work) have studied this in more formal settings. \n\nWeaknesses: \n- MPS tensor networks are primarily used to obtain efficient approximations of higher order tensors using network of lower order tensors. The problem formulation in this work is unclear. Eq. 1 presents the decomposition of a matrix (which is an order 2 tensor) into a product of $n$ local tensors of order 2 and 3. There seems to be some misunderstanding here or some critical details are missing. How do the authors envision compression in this setting when the local tensors are also of the same dimension $I\\times J$ as described in Sec. 3.1? \n\n- Technical details are not sufficiently clear throughout the paper. For instance, the Algorithm 1 where the MPS decomposition is discussed has several notations and steps that are not discussed or elaborated anywhere in the text. \n\n- The authors seem to equate MPS with SVD both in Algorithm 1 and in the text when describing the baseline methods. \n\n- This work misses several important related work, such as [1]. \n\nOther comments: \n\n- The language in the paper in some places is very difficult to follow. It even obfuscates some of the technical details. \n\n\n[1] Bengua, Johann A., Ho N. Phien, and Hoang D. Tuan. \"Optimal feature extraction and classification of tensors via matrix product state decomposition.\" 2015 IEEE International Congress on Big Data. IEEE, 2015.",
            "summary_of_the_review": "The problem formulation of using MPS to approximate image data is not clearly described. Several of the technical details pertaining to the use of MPS seem incorrect or are at least not clearly described. The discussion on how the compression itself is achieved and what the trade-offs are are missing. The paper is not clearly written and some portions of the text are hard to parse. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "While Large-scale data is beneficial for machine learning, it causes high storage and training consumption. Stressing this problem, this paper proposes to use the tensor decomposition method for compressing datasets. Filtering long-range correlation information in task-agnostic scenarios via MPS structure, this method achieves a good compression ratio while preserving the model performance.",
            "main_review": "Pros:\n- The paper adopts the concept of the quantum entanglement entropy of MPS in classical image compression, which is somewhat novel. \n- The compression performance is effective while preserving the model performance. And many experiments are conducted on several datasets.\n\t\nCons:\n- This paper does not provide the training time cost which is important for dataset compression.\n- Although the insight of this paper is interesting, some details are not well analyzed in theory and experiments. For example, the analysis of rank and tensorization shape is needed.\n- The motivation for adopting MPS needs to be elaborated more. What is the advantage of using MPS?\n- Theorem 1 should be a lemma and cited from Equation (2.4) of [1].\n- This paper may not provide enough theoretical contribution for ICLR. As for the empirical results, the cost for the compression is too high, e.g., on ImageNet, loss 12.11% accuracy.\n\nAdditional Questions:\n- Compared with data selection methods, the tensor decomposition method sometimes would cost more time on model training, since there would be more tensor operations. How about the theoretical time cost for MPSD? Adding time analysis could make the proposed method more competitive.\n\n[1] Ivan V. Oseledets. Tensor-Train Decomposition. SIAM J. Sci. Comput. 33(5): 2295-2317 (2011)",
            "summary_of_the_review": "For the unremarkable empirical results, insufficient exploration and contribution, I regret to reject it weakly.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presented an image compression technique based on matrix product states (MPS) decomposition. After compressing each image, knowledge distillation is applied to compensate the information loss for some specific task. The authors verify the effectiveness of the proposed method on several datasets and neural networks.",
            "main_review": "Strengths:\n1.\tThe idea of applying matrix product states (MPS) to image compression is interesting.\n2.\tFigures and tables are clear and well-described.\n\n\nWeaknesses:\n1.\tThe main claim mismatches the research problem. This paper claims to propose dataset compression method and motivates their method from related dataset compression (distillation) methods. However, the work they actually do is (single) image compression, which is another research topic. The proposed method is not designed for reducing the training set size (i.e. dataset compression). Instead, the authors compress the single images individually. Unfortunately, they are not clear about what they are doing.\n2.\tChaotic compression measurement. We know that images are saved with specific compression algorithms/standards (like JPEG) in computers. It is neither suitable nor fair to compare the matrix/tensor element numbers of the original image and that “compressed” by this method. A fair comparison can be the storage size in a hard disk. \n3.\tUnclear short/long-range correlation information. The paper claims that they “compress datasets by filtering long-range correlation information” and “retain short-range correlation information”. However, no correspondence between short/long-range correlation and image pixels/textures is given. It is unknown whether they really removed the “long-range correlation information”.\n4.\tUnconvincing experiments. No repeated results, no standard deviation. As stated in the paper, “Finally, we report results on testing datasets with the best model on evaluation datasets.” In addition, the results in Sec 5.2 are unconvincing. It is unacceptable to use a pre-trained resnet and then fine-tune it on 75% size of training data of ImageNet and get only 52.10% testing accuracy. \n5.\tNo ablation study. No comparison to methods in the past decade.\n",
            "summary_of_the_review": "The claims, research problem, method and experiments are mis-matching and not well-supported. The experiments are unconvincing.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "\nThe paper is insightful. It introduces both task-agnostic and task-specific approaches to compress datasets but sacrifice the performance.\n\n\n",
            "main_review": "\nMPSTotal(Si) = MPS(Si) + MPS(R(Si)).  Is Adding a  MPS on the residual term (R(Si)) similar to use a bigger $d_k'$ ? Can you discuss more here? \n\n\nIt seems that $\\widetilde{S}$ is a quite big parameter. Is it also specific to each image, because it is associated with a subscript $i$ ?\n\nDid you try multiple (>2) residual blocks with MPS, in Eq.5 ?\n\nHow do you decide $d_k'$?\n\n\nMake figure 3 bigger if possible.\n\ntypos:\nlast line in Page 1 :  \"The dataset distillation can Such a property motivates us\"\n \"ResNet18 (He et al., 2016), , for different dataset\"\n",
            "summary_of_the_review": "The paper is interesting. But I am not sure I did fully understand the technical details. The drop in performance (e.g., see table 3) also limits its possible applications",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}