{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a mechanism to dynamically inject noise and clip gradients to resolve the instability of DP-SGD. The paper proposes an adaptive composition noise levels to a desirable privacy level. The dynamics of noise clipping and noise variance is chosen to be controlled according to this composition rule. The paper shows experimental evaluation showing that the dynamic DP-SGD leads to better accuracy than existing composition and DP-SGD methods.",
            "main_review": "Strength:\n1. The paper aims to alleviate the instability of DP-SGD algorithm using dynamic noise and ranges of gradients. This is a pertinent issue in private deep Learning.\n2. The dynamic CLT proposed for composing Gaussian mechanism is interesting and useful result.\n3. The empirical evidence of increase in accuracy due to dynamic-DP-SGD and tightness of the adaptive composition is promising.\n\nWeakness:\n1. What is the value of T after which the presented CLT-type composition of Gaussian mechanism is useful and tight?\n2. Why the exponential mechanism to increase the injected gaussian mechanism is chosen? Is it the tightest or more stable? A theoretical or empirical justification of this choice is imperative. Because this is the choice on which the empirical utility and stability will depend on.\n3. What will be the generalization error bound of dynamic-DP-SGD in comparison with the SGD-type methods?",
            "summary_of_the_review": "The paper has proposed an algorithm dynamic-DP-SGD to achieve better accuracy-privacy trade-off than existing DP-SGD methods. Though the CLT theorem is useful but the method developed on that is a bit ad-hoc. Thus,in my opinion, the paper is below acceptance level.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The paper is on privacy preserving SGD and tightening the corresponding utility-privacy trade-off. But I am not sure whether there's any reason to flag it.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a dynamic DP-SGD to improve the accuracy by lowering the clipping value and noise power. Some experiments are conducted to show the effectiveness of the proposed method.",
            "main_review": "Strengths: This paper is well organized and easy to follow. The targeted problem is interesting. \n\nWeakness:\n1. Could you provide accuracy and privacy trade-off for some commonly used models to illustrate how the accuracy improves with the dynamic DP-SGD?\n2. There are many works considering the Laplace mechanism. Could you provide a similar analysis for dynamic DP-SGD with the Laplace mechanism?\n3. The novelty of this paper is incremental. ",
            "summary_of_the_review": "Overall, the contributions of this paper are marginally significant. It lacks theoretical guarantees for the improvement of accuracy, and the privacy mechanism considered is relatively narrow.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presented a novel method for addressing the issue of training data privacy, while maintaining the model's accuracy. Specifically, the authors proposed a dynamic differential-privacy SGD by extending the Gaussian DP central limit theorem to calibrate the clipping value and the noise power for each individual step. The proposed dynamic DP-SGD improved the model accuracy without sacrificing privacy by gradually lowering both clipping value and noise power. The authors developed two methods, i.e., growing-$\\mu_t$ and sensitivity decay in this work. To validate the dynamic DP-SGD, a few benchmark datasets and baseline methods were utilized correspondingly to show the superiority. Results showed the model accuracy improvement in the strong privacy protection. Overall, the investigated topic is quite interesting and critical in the modern machine learning. The paper is easy to follow. While the experimental results look promising, the novelty of the proposed algorithm is marginal. The authors need to pay attention to the following aspects.",
            "main_review": "1.\tWeak novelty. Though two new techniques were proposed for controlling privacy budget allocation, the novelty of this work is marginal. This work is developed upon the paper by (Bu et al., 2020) as the theoretical proof mostly follows from their techniques. Though Theorem 1 depicts the evolution of the trade-off function, the privacy guarantee, model accuracy guarantee, and convergence are missing in this work. I think the authors should give these theoretical results for Algorithm 2. As these results can explicitly show the relationship between the privacy budget allocation and the performance. Without these, the existing outcomes seem to be incremental, which are not solid contributions.\n2.\tThe computational complexity is missing in this work. Typically, for SGD, with a $\\epsilon$-optimality, it requires $\\mathcal{O}(1/\\epsilon^2)$. While I wonder for dynamic DP-SGD, whether it still maintains the similar complexity.\n3.\tIn the paper, the authors mentioned that the budget is set [0.4, 9]. Does this range apply to all problems with different datasets? Why choosing these range for testing? Not [0.1, 9]? Similarly, as $\\rho_c$ and $\\rho_\\mu$ are hyperparameter, how good are those values selected in the paper?\n4.\tThe empirical results look okay, but it is better to see more promising results with large datasets and model architectures. Only with such solid results, it is affirmative to tell if the proposed dynamic DP-SGD has superiority in difficult machine learning tasks. Additionally, the authors need to show the standard deviation for all experiments.\n",
            "summary_of_the_review": "Overall, I think the current form of the paper requires substantial work to make it technically solid and sound. In particular, the authors need to pay attention to the methodology novelty and some detail of validating the proposed methods. I hope my comments can help out.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Differentially private stochastic gradient descent (DP-SGD) is a well-known method for running SGD while ensuring DP. To ensure that the added noise required by DP is not too large, DP-SGD clips the sampled gradient at each time step. However, this clipping results in bias in the resulting estimator resulting in poor convergence guarantees. This paper proposes to improve the convergence rate of DP-SGD by allocating differing amounts of the privacy budget at different times throughout the SGD iterations.",
            "main_review": "The paper proposes a hyperparameter $\\rho_\\mu$ for decreasing the noise throughout the SGD iterations. The hyperparameter is motivated by the trade-off function on T steps of SGD between neighboring datasets, across all functions that attempt to distinguish between the mechanisms, quantified through the rates of type I and type II errors. The paper notes that the convergence rate of SGD can be achieved through proper hyperparameter tuning, avoiding the usual ramp-up time. Although I think the idea of adjusting different levels of the privacy budget across the SGD iterations is nice, the idea has been previously suggested by multiple previous work referenced through the paper and the specific approach proposed in this paper lacks theoretical analysis of the convergence rate and thus can only serve as a heuristic. \n\nThe paper evaluates their proposed approach on 5 large-scale datasets, comparing to SGD without DP, as well as several variants of SGD with DP. Surprisingly, the vanilla DP-SGD does not seem to be a baseline (the original gradient clipping approach). I do not understand the numbers in Tables 1-4 and I believe there is no discussion of these results outside of the tables. ",
            "summary_of_the_review": "In summary, I believe that while the idea of dynamically demanding privacy cost throughout a number of SGD iterations to meet the privacy budget is nice, it has also been previously studied and the paper does not provide provable guarantees on the resulting convergence rate of their proposed approach of dynamically adding noise, so the theoretical contributions of the paper are underwhelming. \n\nMoreover, even though the experiments considered various baselines over multiple datasets, the presentation of the experimental results can be significantly improved. The experimental comparisons to previous benchmarks, in particular CLT for GDP by (Bu et al, 2020), also do not seem strong enough to merit acceptance given the lack of theoretical guarantees for the proposed algorithm. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}