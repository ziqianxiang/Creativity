{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents a neural network representation that is invariant under permutations of the hidden layer neurons (or feature maps in CNNs). The authors achieve this permutation invariance by factorizing the weight connecting two neurons by the dot product of the two neurons' learned embeddings. They show empirically on MNIST and CIFAR-10 that this representation is equally expressive while requiring fewer parameters and performs competitively with standard MLPs and CNNs. They show that, in MLPs constructed this way, neurons can be exchanged randomly between two networks trained on the same dataset without affecting performance too drastically (unlike in standard networks, where this is not possible). However, an empirical demonstration of the same result for CNNs is lacking.\n",
            "main_review": "## Strengths\n+ Clear motivation\n+ Simple yet effective and novel solution\n+ Well-written paper\n\n## Weaknesses\n- Unclear if neuron transplant can work on CNNs\n\n\n## Detailed comments on weaknesses\n\nExperiments are carried out only on MNIST and CIFAR-10, and the neuron transplant is shown only on MNIST. I think that's fine for a proof of concept paper introducing a novel approach. \n\nHowever, I do have a concern about the lack of empirical results demonstrating the neuron transplant on CNNs. It seems like it's an important feature given the authors' motivation of the problem. Unfortunately, I am not sure if it's even possible given how the authors design the neuron embedding convolution: Each neuron defines its own depthwise convolution filter, which is independent of the embedding that determines its incoming and outgoing pointwise convolution weights. Therefore, the neuron embedding does not determine the semantics of its feature map, since two neurons with the same embedding can have vastly different depthwise (spatial) filters.\n\nUnfortunately, this issue dampens my excitement quite substantially, since it's not clear whether the authors actually achieved cross-model compatibility for CNNs. \n",
            "summary_of_the_review": "I am generally quite supportive of the paper, since it presents a simple and elegant solution to a well-defined problem. It is a very well-written paper that leaves little to be desired. My enthusiasm is only dampened (substantially) by the fact that it's not clear whether the approach presented for CNNs actually achieves what is being promised, since empirical results are presented only for MLPs and theoretical arguments are missing. If the authors can address this issue in their rebuttal, I am likely to increase my score.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose to learn permutation invariant representations of neural networks.\nThey suggest to do this by assuming latent variables z per unit, and deriving weights based on the adjacent unit codes via a mapping.\nThey demonstrate that this representation can be used to train neural networks in unit space.",
            "main_review": "Pros:\n- The unit representation makes sense, is compact, promising, and appears to be a fun idea.\n- Connecting these models to permutation-invariance is also interesting.\n- In addition, the authors show that it works and can be used to train NNs from scratch.\n\nCons:\nThe main problem with this paper is on novelty and attribution.\nThis representation has been proposed before in the ML literature and is unfortunately not attributed correctly here.\n\nA very related construction for neuron embeddings was introduced in \"Probabilistic Meta-Representations Of Neural Networks\" by Karaletsos, Dayan, and Gharahmani in 2018 (https://arxiv.org/abs/1810.00555) together with a variational inference algorithm as a form of Unit-Prior with a per-weight hypernetwork.\nThat paper already introduced this representation of weights and clarified it as a probabilistic model, introduced a variational inference algorithm for explicit and implicit weight models, and visualized and explained Unit embeddings.\n\nThis representation was later also used with a Gaussian Process Hypernetwork (and reviewed again in Sec. 2) of \"Hierarchical Gaussian Process Priors for Bayesian Neural Network Weights\" by Karaletsos & Bui at Neurips 2020 (https://proceedings.neurips.cc/paper/2020/hash/c70341de2c112a6b3496aec1f631dddd-Abstract.html) , where they show how to extend the work with richer mappings between Z and weights which may also depend on inputs and can make structural assumptions on the types of mappings between Unit latent variables Z and weights using kernels k(z,z'), akin to attention (the relationship is well established). \nIt is quite baffling to see this work ignored here, as the core contributions do not even match the original paper, which also introduced a clean modeling framework and a Bayesian Inference algorithm in addition to the maximum likelihood setting.\nThe original work also had already established the links to neuroevolution.\n\nApart from that, the paper does not manage to introduce a clear loss function or modeling framework to follow, which is why the reference to the previous papers is also relevant since it clarifies the underlying probabilistic model and learning problem, even if just used as a regularizer in a maximum likelihood setting.\n\nIt would be appropriate for the dissemination of this paper if the authors could discuss their work in the context of that literature and clarify their intended contribution. Potentially the motivations for their work differ, I would like to have more clarity on that.\n\nAnother issue is that the authors claim the method to be lossless compared to direct weight models, while it is pretty obvious that the embedded models do suffer some performance loss.\n\nLastly, the paper is too light on empirical evaluation beyond even the scholarship and novelty issues, which is particularly severe given that the core content is known to the community.\n\nEdit:\nUpdated the review after considering the paper further.",
            "summary_of_the_review": "The authors propose a technique to embed neurons. While interesting, their work unfortunately does not attribute related previous work or extend it in a significant fashion according to my current understanding. The main differences appear to lie in motivation, though I do not quite see how this is concretely expressed in the technical content of the paper. This work omits the connection to a probabilistic model (unnecessarily) and attempts to present the work under a lens of neuroevolution (which the related work section of previous work also noted).\nAs such I cannot recommend publication in the current fashion but would be interested to understand the motivation of the work better to see if one can cast a different light on this technique.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This submission proposes a parameterization of neural network layers that expresses neurons as embeddings rather than as linear combinations of activations. It is shown that it is possible to learn neural networks with this parameterization on MNIST and CIFAR-10, with some loss in accuracy, and that the embedding dimension need be large to achieve this accuracy on MNIST. Compared to the standard parameterization, the proposed parameterization gives a much smaller (but still quite large) drop in accuracy when neurons are transplanted from one network to another.",
            "main_review": "The method seems relatively simple, but the description is hard to follow. I get that each neuron will be represented by an embedding, rather than as a linear combination of the weights of the previous layer. After that, I am confused. I can see how an _inner_ product between two embeddings could produce a weight, but Eq. 1 involves an _outer_ product between embeddings, which produces a matrix of size embedding_dim $\\times$ embedding_dim. Maybe that’s a mistake and it’s supposed to be an inner product? Eq. 2 is similarly confusing to me. If each row of $\\mathbf{Z}$ is a neuron, it seems like the formula ought to be should be $\\sigma(\\mathbf{Z}^{(l+1)} {\\mathbf{Z}^{(l)}}^\\top) \\mathbf{h}^{(l)}$, and if each column is a neuron, it seems like it ought to be $\\sigma({\\mathbf{Z}^{(l+1)}}^\\top \\mathbf{Z}^{(l)}) \\mathbf{h}^{(l)}$. It would help to explicitly define the number of rows and columns that each matrix has and confirm that the dimensions match for matrix multiplication.\n\nBeyond this issue with clarity, I’m not convinced that the technique has practical value. Although it is now possible to permute _neurons_ in this parameterization without affecting the neural net’s behavior, it is not possible to permute the embedding dimensions unless it is done for all weight matrices simultaneously. This seems like a problem for any notion of “transplanting,” since different initializations will still generally learn different meanings for the embedding dimensions. Although Figure 4 shows that transplanting works better with the proposed parameterization than with a naive parameterization, the accuracy drop is still very large, and this is for the simplest possible scenario (one hidden layer on MNIST). Since the other experiments performed indicate that the method results in substantially lower accuracy than standard parameterizations (albeit with lower parameter counts), it is not clear what other advantages the method would have.\n\nMinor:\n- Using a kernel would further generalize Eq. 1.\n- The related work section is extensive, but some of it appears unrelated. There’s an entire section of related work on transfer learning, but usually transfer learning involves adapting a network for a new task. It's not clear how it is relevant to the work described here.",
            "summary_of_the_review": "This paper has major issues involving both significance and clarity. The idea of a parameterization that is invariant to permutation seems interesting, but the proposed parameterization still appears to be sensitive to permutation of the embedding dimensions, even though it is not sensitive to permutation of neurons. The description of the method is hard for me to follow, and I think it may contain some errors.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper describes a representation of neurons that does not depend on the presence of any particular neurons in the previous layer, or their ordering. Specifically, each neuron is encoded as a vector embedding. The weight between a neuron in one layer and neuron in the next layer is calculated as the product of the neurons' embedding vectors. Networks with such neurons work almost as well as usual on MNIST and CIFAR-10. With this representation, neurons from one network can be replaced with neurons from another network that has a different random initialization, and performance is only moderately impaired.  ",
            "main_review": "Strengths\n1. Interesting idea\n2. The paper is clearly written \n3. The method is basically successful, as shown by reasonably good task performance and close approximation of weights in a normal network\n4. The interpolation and crossover demonstrations are helpful\n\nWeaknesses\n1. I am not sure the proposed neuron representation is compared to the right thing. The authors point out that the method is closely related to low-rank tensor decompositions. The main difference is that this new method uses the same vectors in the products that determine both the inbound and outbound weights. However, it seems to me that a low-rank tensor decomposition would also have the benefits that the authors claim for their method. For example, if a neuron were represented with input and output vectors instead of one input/output vector, it would still be self-contained and permutation-invariant. The paper doesn't explain the motivation for constraining the representation to a single vector or show practical advantages of doing so. \n2. The method seems to impair performance slightly (Table 1). \n3. Some practical benefits of the approach are hypothesized but not shown (this would be fine if the first point above was addressed). \n\nFurther comments on point #1: There is some older literature that discusses using the same vector as part of both input and output weights. Specifically, the Neural Engineering Framework (e.g. Eliasmith, 2005, Neural Computation) uses separate \"encoder\" and \"decoder\" vectors, equivalent to tensor decomposition. Some of their work shows how this relates to the \"population vector\" model from neuroscience (e.g. Georgopoulos et al., 1986, Science). Modifying the Neural Engineering Framework to use population-vector decoding would make the encoding and decoding vectors the same, and fully transplantable, but such a network wouldn't be able to compute anything. In contrast, in the standard Neural Engineering Framework, encoders would be fully transplantable, but decoders would not, because they are optimized in a way that depends on the presence of other neurons (although the decoders should also be transplantable in the limit of many neurons, based on Nicola et al, 2016, Frontiers in Computational Neuroscience). The approached proposed here may be intermediate to these two. I don't know whether to expect the representation proposed here to be more or less transplantable than that of tensor decomposition (which I think would have transplantable encoders and non-transplantable decoders, except in the limit). It would be interesting to know. \n\nMinor: \n- Pg. 6 mentions the dimensions of the embeddings but not the layers themselves. \n- In Table 2, it would be easier to interpret the MSE in the context of the mean-squared weight amplitude.\n \n\n ",
            "summary_of_the_review": "The paper offers a fresh and interesting perspective, and nice analysis, but it seems to me that the approach should be more clearly contrasted with tensor decomposition, which seems likely to offer some of the same benefits.   ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}