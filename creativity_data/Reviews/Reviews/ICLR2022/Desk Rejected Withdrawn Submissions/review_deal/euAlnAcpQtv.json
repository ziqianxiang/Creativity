{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies how to train unsupervised image-to-image (I2I) translation models without the need to put the source and target data at the same place. The authors investigated a recently proposed model, CUT, and showed that the objective can be decomposed into a source-only and a target-only term, which enabled learning CUT with data isolated at two places. The authors further investigated using a pre-trained VGG net as the backbone for the discriminator to save the communication cost. The experimental results look good. However, the paper did not discuss how often the two sides (clients and servers) exchange the parameters. From Algorithm 1, it seems that the parameters are exchanged for every mini-batch (or image), which is very inefficient.",
            "main_review": "==== Strength ====\n1. The decomposition of the CUT objective into a source-only and a target-only term looks interesting.\n\n2. The qualitative and quantitative results look good.\n\n==== Weakness ====\n1. Lack of novelty: The paper seems like a naive extension of FedCycleGAN (Song & Ye, 2021), which already shows that the objective function of an I2I model can be decomposed into a source-only and a target-only term. The authors simply investigated another different I2I model/algorithm. The use of a fixed VGG backbone is practical but does not add novelty to the paper. \n\n2. The paper did not have any discussion on the frequency and rounds of the communication between the server and the client, which is the major factor in federated learning. From Algorithm 1, it seems that the communication happens after every minibatch (1 image), which is extremely inefficient. Please note that if the server and clients can communicate after every minibatch, then federated learning becomes exactly like stochastic gradient descent (i.e., federated and centralized learning become nearly the same), and there is really no challenge in federated learning. \n\n3. Following 2, if the clients and server did communicate after every or multiple epochs, then I do have some doubts about the performance --- in existing federated learning it is very hard to see that, under non-IID situations (server and client data are of different distributions), a federated trained deep model can be competitive to a centralized model. The authors must discuss more why the federated learning procedure can overcome non-IID situations. \n\n4. The discussion in 5.1 is a bit orthogonal to the main purpose of the paper. The authors use the VGG net mainly to save the communication cost. Showing that FedCUT qualitatively outperforms other models may not be a fair comparison --- the other algorithms can also apply a pre-trained VGG backbone in the discriminator. To be clear, if a VGG backbone can improve I2I, then it is a good contribution, but the contribution is orthogonal to the problem the paper tries to address.\n\n==== Other comments ====\n1. In Figure 1, there is no description on what each module is.",
            "summary_of_the_review": "The paper works on an interesting problem, federated learning for unsupervised image-to-image (I2I) translation. However, the paper lacks novelty. Also, the paper did not discuss the communication frequency and its effect on the model performance, which is the main bottleneck in recent federated learning. Thus, I give a score of 3.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes the FedCUT algorithm for federated contrastive unsupervised image to image translation. The main idea of the proposed approach is to employ the FastCUT algorithm in the federated learning setting for image translation. The authors' approach exploits the observation that the losses in the FastCUT algorithm can be decomposed into isolated server-side and client side losses, which can be implemented by sharing only the gradients and the parameters of the discriminator between the server and the client. This is in contrast to the FedCycleGAN algorithm, where both the generator and discriminator parameters must be shared. The authors further propose to replace the PatchGAN architecture of the original FastCUT algorithm with a modified VGG-based discriminator, which uses a pre-trained VGG network as the backbone with the addition or a few more convolutional layers. The authors only share the weights of the additional layers, which further reduces the BW requirement in the federated learning setting and also improves image translation quality. The authors show comparisons to CUT, FastCUT, FedCycleGAN for several natural and medical image translation tasks and show significant improvements in accuracy and reduction in BW over the SOTA FedCycleGAN algorithm.",
            "main_review": "Strengths:\n1. Novelty: The paper addresses the fairly unexplored and new problem of federated image to image translation, for which only one other prior work (FedCycleGAN) exists. It also proposes a new approach using existing SOTA ideas in the image2image translation (I2I) literation to solve the problem of federated I2I and hence is overall novel.\n\n2. Clarity: The paper is well-written and structured.\n\n3. Technical soundness: The technical aspects of the methodology and experiments are most correct (an exception noted in the weaknesses section below). The authors also provide experiments on many different types of natural and medical images.\n\n4. Performance: The proposed approach performs much better or equivalently to the existing SOTA approach of FedCycleGAN in terms of image quality for all translation tasks, while significantly reducing the BW requirement (by nearly 2 orders of magnitude). Hence overall, it represents a significant advancement of this field of research.\n\nWeaknesses:\n1. Narrow Scope: The paper is fairly narrow in its scope and limited in applicability to the niche problem of federated image to image translation. Hence it is not likely to be broadly cited or widely impactful to the broader AI/ML community. However, within its niche area the paper represents a significant advancement of the SOTA.\n\n2. From tables 2 and 4, why does FedCUT_PathGAN perform better than FastCUT? Theoretically they should be identical. Can the authors explain if these differences in Table 2 and 4 or statistically significant or simply due to random differences in training runs? If the authors feel that these differences are significant, what the theoretical justification/explanation for observing them?\n",
            "summary_of_the_review": "The paper is novel in that it addresses a less explored, but important problem of federated image to image translation and proposes a new approach for solving it, which, overall (considering image quality and BW requirements) significantly advances the SOTA research in this problem domain. However, the paper addresses a niche topic and is not likely to be widely impactful in the AI community.\n\nI would like to see the authors answer my question about why they observed differences in image translation accuracy for FedCUT_PathGAN and FastCUT.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces a federated contrastive image-to-image algorithm. The main idea is to split the existing contrastive CUT [1] loss into two parts that each depend only on the data from the source and target domain, respectively. Additionally, the method uses pre-trained classifier weights to initialize the discriminator. Authors compared their method against CUT [1] and Federated CycleGAN [2] in terms of visual translation quality (FID and segmentation quality).\n\n\n[1] Park, Taesung, et al. \"Contrastive learning for unpaired image-to-image translation.\" European Conference on Computer Vision. Springer, Cham, 2020.\n\n[2] Song, Joonyoung, and Jong Chul Ye. \"Federated CycleGAN for Privacy-Preserving Image-to-Image Translation.\" arXiv preprint arXiv:2106.09246 (2021).\n",
            "main_review": "\nStrengths:\nThe paper is easy to read and understand. The I2I benchmarks and metrics are appropriate for the evaluation of the proposed method in terms of translation quality. \n\nWeaknesses:\n1) The **motivation** for the federated image translation remains unclear. Authors refer to [1] for the motivation of federated I2I and provide an example of the medical domain, but there are significant issues with both arguments. First, [1] is not accepted to a peer-reviewed venue, which makes the use of any arguments from that paper questionable. Second, it appears that for the majority of medical applications, federated I2I is rather impractical, because a) I2I, in general, requires a large amount of data which is usually not achievable for most medical applications; b) I2I is unreliable as it may not preserve the important information of the input image in the translation result, which is very undesirable for the medical domains. For most of the other applications generally discussed in the I2I literature, the privacy issue is usually not a concern. \n\n2) The main idea behind this method, which is to trivially split the CUT loss into one that only uses the samples from the source domain (Eq.  12) and the one that uses target domain examples (Eq. 11) provides **very limited novelty**. In fact, [1] uses a very similar and rather trivial loss decomposition strategy. The use of classifier weights for discriminator initialization is also a very well-known technique in the literature.  \n\n3) Even if we disregard the triviality of the proposed approach, the proposed training strategy is **impractical**. Even though the proposed method implies roughly 500 times less data transfer than Federated CycleGAN [1] that required transmitting the weights of two generator-discriminator pairs (Table 1), it still requires the transfer of roughly 4.2 MB of data **per iteration**, which would make training of a single I2I model last for months.  \n\n4) In the experiments section, the authors compare the proposed method with the Federated CycleGAN, CUT and FastCUT, and report the FID scores and segmentation quality on the popular I2I datasets. It appears that the reported improvement in the translation quality comes from either random weight initialization or due to the use of the pretrained classifier weights (because everything else is essentially the same as in FastCUT). Ideally, an average score over several randomly initialized runs must be reported to exclude the first factor, and an ablation study with the randomly initialized discriminator should be held to illustrate the second factor. Also, since the main claim of the paper is the federation component, it is crucial to report the amount of time spent on training of all methods discussed above vs the proposed method.\n\n5) The related work section misses an overview of the existing non-federated I2I methods.\n\n\n[1] Park, Taesung, et al. \"Contrastive learning for unpaired image-to-image translation.\" European Conference on Computer Vision. Springer, Cham, 2020.\n\n[2] Song, Joonyoung, and Jong Chul Ye. \"Federated CycleGAN for Privacy-Preserving Image-to-Image Translation.\" arXiv preprint arXiv:2106.09246 (2021).\n",
            "summary_of_the_review": "The motivation behind the task is unclear and needs further discussion. The method is impractical, as it implies the transfer of a large amount of data at each iteration, which makes I2I training too long. The experimental setup needs further improvement to clearly demonstrate the benefits of the proposed method.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}