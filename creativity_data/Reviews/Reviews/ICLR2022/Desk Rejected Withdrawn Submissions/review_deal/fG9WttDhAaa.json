{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "For modeling structured objects, usage of coordinates as inputs for MLP was shown to be not so effective as projecting coordinates to high-dimensional space before applying MLP. This projection or encoding is done via sine and cosine functions (Fourier frequency mapping). Authors discuss the drawbacks of this Fourier feature mapping and 1) propose non-Fourier embedding functions based on basis functions of form $f(t-x)$ where $x$ - coordinate and $t$ is sampled with deterministic equidistant values $t=0,s,…(d-1)s$ 2) analyze dependence between sampling density and embedding properties. The latter is shown to be related to the matrix rank of proposed embedding and distance preservation between embedded coordinates. Theoretical analysis of proposed embedding depending on the rank and distance preservation is empirically verified with single-layer linear MLP for 1d and 2d signal reconstruction. Additional experiments are conducted with 4-layer non-linear MLP.\n",
            "main_review": "**Strong points**\n- Considering positional encoding in terms of other functions and basis functions not related to Fourier features.\n- Analysis on the trade-off between memorization and generalization.\n- Analysis on random Fourier features in the context of matrix rank and distance preservation.\n\n**Weak points**\n- Theory analysis has flaws and some formulations are based on non-proved statements or without reference to prior work if it was proved before (e.g. distance preservation -> generalization; analysis of approximate distance, no rank analysis in sec. 2.2 for Eq. (4) representation). Entire sec. 2 looks like intuition description rather than strong mathematical theory, which should be strong as analysis done for simple linear model. Moreover, the core of the section is band-limited embedders and even it is stated that \"band-limited embedders are equivalent to bounded rank”, however later Gaussian embedder is introduced which is non band-limited but has the upper bound on the rank.\n- There is no evidence why embedders are chosen to have form of shifted basis functions $f(x-t)$. For example widely used Fourier features are not of this form, they are $f(t*x)$. It should be clear stated that main analysis is about rank and distance. Currently it is mixed with band-limited functions which do not help understanding the paper at all (even there are ambiguous statements which lately contradicts, like band-limited=bounded rank, lately it is not true for Gaussian embedders). Seems the paper should be restructured to better reflect all of this.\nAuthors state that \"for better generalization we need band-limited embedders” however later they introduce Gaussian embedder which is approximately limited (what does it mean approximate?)\n- Experiments show that Gaussian embedder is a bit better than random Fourier features, however i) there should be also the baseline with fixed Fourier features as in NeRF and with recently proposed learnable frequencies instead of sampling them randomly https://arxiv.org/abs/2106.02795; ii) it is worth to have real applications results, for example compare proposed embeddings in NeRF training.\n\nOther comments throughout the text:\n- \"Hence, the output of the penultimate layer can be considered as a positional embedding of a linear model. Thus, we intend to study the preferred characteristics of the penultimate layer and then inject those properties into the positional embedding layer with the hope of achieving better results.” Yep, we can consider that the whole MLP except the last layer is doing positional embedding mapping, however in the whole paper and proposed representation of positional embedding the usage is different from this statement. I would suggest reformulate this statement or remove it entirely. I think it is ok that authors first study properties of positional embedding used in linear model (with some proofs as we can do all things analytically) and later empirically show that found properties also hold for non-linear models.\n- \"Intuitively, the embedded coordinates should preserve the distance between”: why it is intuitive? I prefer to formulate in the sense that this property is desired as having inductive bias and space structure preserving. There is no guarantee that some crazy mapping with no distance preservation will not work better in the end.\n- Could authors elaborate and give more context of their footnote 1?\n- Positional encoding is widely used in image and speech recognition too, possibly it is worth to mention some works in these domains too in the introduction. \n- \"However, the major drawback of training coordinate-MLPs with raw input coordinates is their sub-optimal performance in learning high-frequency content [15].” I would refer to [7, 16] besides [15].\n- Eq. (2) and Proposition 1 we should mention that $d >> N$ otherwise max rank will be $d$ and moreover proposition 2 is not so interesting for us in this case.\n- Eq. (4) is not clear, as the right hand side doesn’t depend on $x$. Also why should $\\beta_b$ be shifted functions, do they depend on $x$ (my current understanding that $\\psi(t,x)\\sim \\sum \\alpha_b(x)\\beta_b(t)$, please correct me).\n- Why is $D(x_1, x_2)$ definition given with integral on $[0, 1]$ before eq. (5)? Could author formulate this more carefully (as before it was given that $x_i\\in[0, C]$ and the same interval for $t$)?\n- Why does generalization relate to the distance preservation? I think some either theoretical or empirical study is needed here (or links). Results from figures 5 and 7 do not strongly prove this statement as test performance is just very bad and far-far away from train.\n- \"which is possible if, and only if, ψ is band-limited. In that case, d = B is sufficient where B is the bandwidth of ψ (by Nyquist sampling theory).” - Nyquist theorem only holds in case of Fourier series, not the general basis functions. Also on which theory do authors rely that integral can be approximated only if function is band-limited?\n- In what extent do authors mean “approximate inner product” (what is strict definition of it)?\n- It is worth to introduce PSNR abbreviation at the first time usage\n- What is displacement in figures, e.g. in figure 6? How is it computed?\n- What exactly do authors mean under “The Gaussian embedder is also approximately band-limited like the square embedder.”? \n- Proposition 2 proof. “Then, Ψ is approximately a circulant matrix.” What does it mean here \"approximately”? Why are this approximation and all following constructions on cirlulant matrix valid? The proof seems to be not strong (any references to theorems can help here or stronger proof is needed because in general Gaussian function is not band-limited in terms of Fourier basis).\n- I think formulas how $D(x_1,x_2)$ are obtained for square embedder should be provided (or at least integration interval), otherwise it is ambiguous formulation.\n- From the text \"Further, we pick ten random rows from the Pepper image as our targets and measure the average performance.”  it is not clear what are train data and what are test. Do we train MLP for each row separately? What does MLP predict?\n- Embedding dimension 10000 seems to be artificially large, not sure that in practice this will be really used.\n- typo “by by [16].” -> \"by [16].”\n- Fig 1 is not mentioned anywhere in the text\n- Proposition 3 type “numnber” -> “number\"\n- Typo “Similarly, When” -> “Similarly, when”\n- Link [23] is published at the conference (not only on arxiv)\n\n\n",
            "summary_of_the_review": "Overall, the work looks interesting as it is trying to have general analysis of the positional encoding and provide alternatives to Fourier features; Also it proposes an important analysis that embeddings work due to trade-off between matrix rank and distance preservation. However, theoretical analysis needs strong formulations and better presentation/explanation writing as well as fixing a lot of ambiguities as I discussed above in details. Furthermore, it is not clear from experiments why one should use Gaussian embedder rather than random Fourier features (only that one deterministic and another not). I think the paper needs additional work and clarifications that is why evaluate the paper to be on borderline at the current stage.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the positional encoding for MLP whose input signal has physical coordinates. The paper discusses the different tradeoffs between memorization and generalization by choosing encoding functions that hold different bandwidths. The encoding function's bandwidth should be large enough to keep full rank to get enough capacity while the bandwidth should not be so large to keep smooth and generalizable. Experiments focus on 1D/2D signal recovery. ",
            "main_review": "Strength: \n* Touch the theoretical aspects of positional encoding, which is crucial and important. \n* The high-level concept is intuitively correct: Choose encoding functions with proper bandwidth to balance capacity and generalization. \n\nWeakness:\n1. The biggest weakness is that I cannot see much significancy of the analysis and generalized positional encoding. \n\n* Theoretically, the setting is over-simplified. The analysis focuses on the linear case, the large enough d and the input is 1-D signal. For the linear case, the conclusion is very interesting and very standard in signal process. Specifically, the capacity analysis targets at exact recovery which results in the full-rank requirement, which reads trivial. To obtain this, one needs the d>>N assumption which I doubt in a practical setting (Prop. 1 misses claiming the condition that d>>N). The typically interesting  regime of capacity analysis should target at the universal approximation capability of the class of functions f(x) defined over all possible position x, i.e., $\\sup_{x\\in [0,C]} |w^T encoding(x) - f(x)|$. This corresponds to the case when $N$ is infinite. The generalization analysis simply targets at distance approximation, which should focus on a more general form. That is, given a class of function f and some weight k(t), how about $|f(x_1 - x_2) - \\int_{t}k(t)\\phi(t,x_1)\\phi(t,x_2) |$. The work simplifies the problem by setting $f(x_1 - x_2) = |x_1-x_2| $ and $k(t)=1$, which is not interesting and pretty standard in signal process. \n\nI strongly suggest the authors to read and learn from the fundamental paper on RFF [1] to check the analysis and the targets there. \n\n*  Empirically, I do not know much use of MLP + positional encoding. I know that positional encoding is widely used in attention models. So I doubt the practical use as well. Moreover, the authors only adopt signal recovery experiments, which is a common problem in signal process by using linear or sparse regression. Why do we need to use neural networks? Moreover, the experiments focus discussing the bandwidth of gaussian encoding. However, no empirical results show the advantage of gaussian encoding over RFF, which however is claimed in the introduction. \n\n2. From the writing, overall, the idea that the authors try to convey is clear. However, the notations are un-organized and used arbitrarily. For example, if x in 1-D, which is the norm in $||x_1 - x_2||$, why not $|x_1 - x_2|$. What happens if some x's are equal in (2). What are the errors of the approximations in Sec.2.2 and Sec.2.3? The paper claims a lot on the theoretical contributions. I see a lot of not rigorous claims. \n\n[1] Random Features for Large-Scale Kernel Machines. Rahimi & Recht. \n",
            "summary_of_the_review": "The technical and empirical contributions of this work are marginal and the quality of this work is lower than the bar of ICLR. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This article focused on characterization and empirical comparison of various positional encoding functions. Meanwhile, it introduced recent related works in detail related to the positional encoding mechanisms for the computer vision tasks.",
            "main_review": "Strengths:\n\n1. The introduction to related work is detailed and well organized.\n\n2. There were experimental comparisons based on results on the small data, thus giving some empirical conclusions initially.\n\nWeakness:\n\n1. The main contribution, more fundamental intuition for viewing positional encoding, may be unclear.\n\n2. Experiments on small-scale data were not enough to support the general conclusions.\n\nComments:\n\n1. The authors claimed that this paper aims to build the necessary theoretical formula for the positional encoding and to verify\nthe claimed theoretical on a toy dataset. If this is the author’s contribution, it may be far from the requirements of a research paper. I think it’s more like some preliminary experiments or an interesting report to study positional coding.\n\n2. Overall, the authors refined the existing positional encoding methods into a general formula. My questions are that 1) why did you do this? 2) What are the problems for the definition of existing position encoding?\n\n3.  Authors showed the performance of some classical position functions on the small dataset. Based on the conclusions drawn from the experiments on the toy data, how much credibility is there for the conclusions drawn from the preliminary experiments on the toy data?\n\n4. According to Sections 4.3 and 4.4, Position coding functions are sensitive to specific tasks. For example, the Gaussian embedder demonstrates slightly inferior performance to RFF in 2-D SIGNALS. What is your conclusion？\n\n5. In Section 5, the authors claimed that this study aims to make a more interpretable and less restrictive way to encode positions. Suppose this study is interesting, what are the benefits of more interpretable and less restrictive?\n\nAlthough positional encoding is very important in the deep learning models, this paper lacks further thinking and in-depth exploration. Overall, this paper lacks the necessary innovation.",
            "summary_of_the_review": "Suggesting that there should be more deep rethinking, exploration, and experimental grounding, and I tend to reject it.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work aims to add mathematical intuition on the recent trend of MLPs used for vision tasks. Specifically, this work provides an analysis of various positional encoding functions. The main contributions are characterization \nand empirical comparison of different encodings. This work takes a different direction from recent related works, such as [15: Rahaman et al, 2019, 16:Tancik et al, 2020], in that it provides a simple and generic scheme \nfor characterizing when different encodings will perform. ",
            "main_review": "\nIn the Introduction, the authors give an overview of recent work using position encodings for regression tasks in computer vision. This section is easily read and understood, and positions itself as an alternative to RFF-based \nencoding schemes. However, the authors should be careful about claims relating to the applicability of their work to language Transformers or BERT, as (as pointed out in [15]) those positional encodings are used in a different way. \nThe authors also claim to provide some more fundamental intuition than the NTK-based analysis. The presentation of the core ideas is not entirely clear (more details below) although overall it seems sound.\nBased on the analysis of the Rank of the features in a linear reconstruction task, the authors propose embedding functions that implement positional encodings with desirable (tunable generalization) properties. \nThe authors provide a comparison with some commonly used encoding functions. However, most of the experimental comparisons seem to be on very limited data and also provide inconclusive results about the empirical benefits of the proposed functions.\n\nI like the simplicity and the core ideas in that paper. However, I think that this work needs more experimental grounding and a clearer theoretical exposition to appear in ICLR, so I think that it should be rejected.\n\nThe experimental work done by the authors centers around reconstruction of simple (1-d and 2-d) functions given partial observation (splitting some pixels of the same images into train/test sets). However, these examples are too toy-like \nto draw significant conclusions about the actual performance of the proposed encodings. For instance, in 4.3 the authors use \"three random rows from an image to test the reconstruction ability of the Gaussian embedder for a 1-D signal. \nThe training data are sampled with an interval of one.\" First off, this does not seem like enough functions to verify the performance even on 1-d signals. Second, please provide a more clear and complete explanation of the datasets involved\nin train and test here. From this description it seems there are no samples reserved for test? Then in section 4.4 the biggest issue with the work as a whole arises, as the authors themselves point out that \"a vital insight to note here is \nthat the advantage of using deeper and non-linear networks to encode signals becomes more significant as the dimensionality of the input signal increases.\" However, much of the guidance seems to be conditioned on the idea that the Rank itself \nis the useful tool in analyzing a problem and tuning the positional encodings. As in almost all serioues application cases for predicting high-frequency functions an MLP (2 layers or more) will be used, this acknowledgement seems \nto admit the limited scope of the impact of the paper. If I am mistaken in this please explain why clearly. Furthermore, the conclusions from the experiments in this section at best modestly favor the Gaussian encodings. Finally, the scope of \nthese experiments are so limited (only doing pixel prediction, while [15] does view prediction [much higher dimensional]). While it is not necessary to reproduce or improve on the hardest scenario to justify the value of an idea, it does \nseem a bit too oversimplified.\n\nA few more points: In Proposition 1 can you explain what \"under perfect convergence\" is meant to signify? Remark 1: the bias-variance tradeoff is a well known issue in ML and thoroughly characterized for a range of functional mechanisms (for instance \nkernel functions give rise to SVM with varying VC dimension). Furthermore, the bias concept is generally applicable as a regularizer. However, in your work you focus on tuning (for example in 4.4 you \"conducted an exhaustive hyper-parameter sweep \nfor RFF and Gaussian embedder to obtain optimal performance\"). Do you think this is scalable? Is there a way to employ your rank-control (via the Gaussians variance parameter) more smoothly? Or is there a more natural way to select it? This seems \nimportant to me since it is the main contribution of your work and it is not clear if parameter sweeping is always a viable solution (the grid step may need to be small).",
            "summary_of_the_review": "I like the simplicity and the core ideas in this paper. However, I think that this work needs more experimental grounding and a clearer theoretical exposition to appear in ICLR, so I think that it should be rejected.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}