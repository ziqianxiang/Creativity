{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies adversarial examples in the deep reinforcement learning setting. The authors note that previous work focus on learning adversarial examples, either through white-box or black-box attacks. Instead, the authors develop a set of 'natural perturbations' that correspond to semantically meaningful alterations to the agent's observation. In particular, the authors study the effect of perturbing agent inputs by (a) altering brightness and contrast (b) blurring (c) rotation (d) shifting (e) compression (f) perspective shift. \n\nUsing the ALE benchmark for their study, a key element of this paper is to demonstrate that semantically meaningful perturbations, as defined by the authors, can have a smaller impact on the agent's representation (shift in hidden features) while having a larger impact on the agent's performance. As the authors point out, this is quite remarkable because such attacks do not require any prior knowledge of the agent, and only some rather generic information about the environment.   \n\nThey further show that a recently proposed algorithm for adversarial training of a Q-learning agent is, somewhat surprisingly, more sensitive to these natural perturbation compared standard training of a Q-learning agent. \n\nFinally, the authors demonstrate that a typical type of adversarial attack in RL tend to manipulate the higher-band frequencies of the Fourier spectrum. In contrast, the author's proposed set of attacks collectively cover a broader range of the spectrum. This suggest that, while the method is quite simple, it exhibit greater flexibility in its ability to attack agents.\n\nOverall, the paper is well-written and quite enjoyable to read. The authors clearly present the literature and their contribution to it. The logic and motivation for their work is impeccable and the experiments are carefully thought out. The main limitation of the current submission is that their results are only comparing against one type of adversarial attack and only one type of adversarial training. Comparing against a broader set of adversarial attacks and adversarial training regimes would strengthen the paper considerably.",
            "main_review": "The premise of the paper is to suggest that very simple attack methods; natural transformations of images, are effective means by which to adversarial perturb deep RL agents. This premise is investigated on the ALE Atari benchmark, and using this benchmark the authors make three contributions.\n\nFirst, to quantify the impact a perturbation has on the agent, the authors make use a similarity measure previously proposed in the literature, which essentially measures the L2 norm change in the agent's hidden features. The authors compare their proposed attack to a specific adversarial attack and show empirically that their proposed 'natural perturbations' have less impact on the agent's internal representation, yet are able to achieve greater degradation of agent performance. This is somewhat alarming due to the simplicity of the attack–there is no need to take gradients of the agent or any other form of privileged access to the agent's internal state. However, I would ask the authors to tone down their claims a little, as their findings are not general enough to conclude that their results are true in general for any form of L-p norm based attack.\n\nSecond, the authors show that a recently proposed method for adversarially training of Q-learning agents is actually more sensitive to these natural perturbations than a default training protocol of the same agent. As this result involves only a single instance of an adversarial training protocol, again this result is not sufficient to conclude that adversarial training is *in general* not robust to natural perturbations, but it certainly calls into question the value of using L-p norm attacks for adversarial training and is a quite interesting insight.\n\nThird, the authors study how the L-p attack considered in this paper affects frequencies in the Fourier spectrum. They find that this attack almost exclusively acts on the higher frequencies. In contrast, the proposed set of natural perturbation differs in which frequencies they are acting on, and as such, as a set of simple attacks, cover a larger range of the spectrum. These results only presented on a subset of the games in the Atari suite, against a single L-p based attack, and must hence be interpreted with a similar caution in terms of how general these finding is. \n\nThe main weakness of the paper is the limited range of its empirical investigation; it is hard to say something general about L-p norms based on comparisons with a specific adversarial attack method, and similarly for the robustness of adversarial training. If possible, I would recommend the authors to considered expanding the scope of their study, the provide more general insights into the phenomena they are studying. If stronger empirical evidence is forthcoming in the rebuttal stage, I will increase my score accordingly.\n\nI do have one question to the authors on Table 1: In BankHeist, it appears that your proposed attacks have a larger relative impact on the agent's performance than Carlini & Wagner, yet your raw scores are higher than that baseline? Are you not comparing against the same baseline agent?\n\nOther feedback\n- A clear presentation of the experimental setting is required before presenting results. In other words, the 'experimental details' paragraph should be presented before the results in Table 1.\n- Page 2 second paragraph, I would dispute that natural perturbations are more general that L-p norms. They are very specific means of perturbing an image.\n- An MDP is usually defined as having an initial state distribution (as opposed to a fixed initial state)\n- Both the policy and the transition kernel maps into [0, 1]\n- a*(s_adv) is never used.\n- Is Equation 3 your own proposal or taken from the literature?\n- Section 3 should either start with a clear description of the experimental protocol, or there should be a section prior to section 3 that outlines it.\n- For the results on adversarial robustness, a more quantitative approach would be to use area under the reward curve as a metric.\n- Last paragraph on page 7, you hypothesise that increasing the L-p norm weight in adversarial training would not help against natural perturbations - would suggest to actually run this experiment to verify this claim.\n- Figure 5 on total energy, to reiterate, I would strongly urge the authors to include data from more environments and if possible, compare against other attacks that use L-p norms.",
            "summary_of_the_review": "This paper proposes a set of adversarial attacks based on 'natural perturbations'. The authors show on the ALE Atari benchmark that, compared to a well-known L-p norm based adversarial attack, natural perturbations can have smaller impact on the agent's representations yet have a larger impact on its performance. They further demonstrate that a recently proposed protocol for adversarial training can actually be less robust to natural perturbations than standard training. The results are on the weaker side in terms of their generality, but highlight an interesting finding that would be of general interest and merits further study.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Motivated from the observation that deep neural policies trained via reinforcement learning have demonstrated significant progress on several tasks, the authors intend to assess the robustness of such policies to natural corruptions that make imperceptible changes to visual observations. Unlike recent work in this space, where the intent is to assess the robustness of learned policies to crafted adversarial perturbations or adversarially train robust policies, the focus here is to measure and understand performance degradation when natural corruptions satisfying the following properties are applied — (1) corruptions render a perturbed observation closer the original one when compared to an “adversarially” perturbed observation,  and (2) applying corruptions is a black-box operation with controlled hyper-parameters which don’t require any knowledge of the downstream objective. Along these lines, the authors make the following contributions. Firstly, they show that while “said” corrupted observations are perceptually closer to the original one when compared to crafted adversarial perturbations, they lead to increased (or competitive) performance degradation, thereby motivating the need to study generalization to perturbations which lie at this “edge of imperceptibility”. Secondly, they show that existing methods to train adversarially robust policies render observations result in policies that are more susceptible to natural corruptions compared to ones trained in a vanilla manner. Finally, to gain some understanding of this phenomenon, the authors investigate the kinds of spatial frequencies affected by natural corruptions vs adversarial perturbations in the fourier domain. If I understood correctly, the goal of the paper is to propose a framework to assess the robustness of learned policies to such natural corruptions with supporting evidence justifying the need to so.",
            "main_review": "I will now highlight the strengths and weaknesses of the paper. The strengths and weaknesses covered here refer to the quality, clarity, and novelty of the submission (whether major or minor).\n\n**Strengths**\n\n1. The paper is generally well-written and easy to follow. With the exception of the points covered in the weakness (see points regarding motivation and situated related work), the authors do a good job of motivating the problem, providing experimental evidence to support the same, followed by analysis supporting obtained experimental conclusions. Modulo the points about related work in this space, I believe the paper is one among a few others in this space trying to explore an aspect of the “robustness in RL” problem that is often overlooked — for instance, generalization to OOD settings in RL is often considered for scenarios that either include crafted perturbations (as stated in the paper), or distractors in observations [A] — unlike the settings considered here. The bit about explicitly establishing the relative severity of corruptions by comparing the impact on perceptual distance and eventual performance seems novel. \n\n2. With the exception of the points covered under weaknesses, the experimental evaluation in the paper is systematic and thorough and seems to support the claims made in the paper. The observation that adversarial training renders policies more susceptible to natural corruptions, although somewhat expected given the proposed performance degradation trends suggested in the paper — i.e., the natural corruptions framework is competitive in degrading the performance of deep RL agents with lower perceptual similarity, is good to verify quantitatively. Further analysis in the fourier space seems to provide some reasoning for the observed performance degradation trends. \n\n**Weaknesses**\n\nPerhaps one of the biggest weaknesses of the paper is the lack of discussion surrounding relevant prior work and situating itself with respect to the same — which in turn diminishes the novelty of the paper to some extent. I’ll cover these (and other) issues in the following points.\n\n1. Assessing sensitivity to natural corruptions has also been studied in [B] — where the goal is to measure robustness to visual corruptions at varying levels of severity for visual navigation agents (with deep neural policies) trained via RL from scratch. This affects the novelty of the paper to some extent. Critical bits of differences here (some covered later) include — (1) differences in experimental settings and (2) lack of characterization of adversarial vs natural perturbations w.r.t. perceptual distance in [B]. While I am in favor of the unique perspective (1) provides in this submission, modulo these differences, the paper would benefit if the authors — (1) considered the visual corruptions presented in [B], at least the ones which fall at the “imperceptibility edge” for experiments or included a discussion situating their adopted corruptions w.r.t. the vocabulary in [B], and (2) include a general discussion characterizing where the proposed framework is situated given the one proposed in [B].\n\n2. I am not entirely certain if the proposed fourier space analysis to investigate how natural vs adversarial perturbations impact observations is novel. Although not exactly similar, [D] includes detailed analysis supporting (1) how natural corruptions — brightness, contrast, blur, and additive noise — affect the fourier spectrum of natural images and (2) how augmentation strategies and adversarial training bias models towards low-frequency information (and other inferences). The paper, in its current form, does not include any discussion surrounding how the proposed analysis in Section 5 is situated w.r.t. [D]. In addition to minor differences in the adopted methodology, the major bit of difference is in the kinds of images considered (natural vs ALE game states). In my opinion, the proposed analysis, given the observations in [D], is not sufficiently novel — for instance, a subset of the inferences in the current submission and [D] seem similar, compression (and JPEG) and blur (and gaussian / glass blur) seem to impact high-frequencies when compared across figure 5 in the submission (and figure 2 in [D]). The paper would benefit if the authors considered explaining and providing + including a discussion about how their proposed analysis is novel (is offering novel insights) beyond what has been observed in [D]. I’m open to hearing the authors’ thoughts on the same.\n\n3. While the paper proposes a framework to assess the robustness of deep neural policies to natural corruptions and shows how adversarial training is insufficient, there is a lack of discussion in terms of actionable insights — as in, what techniques or what general strategies are likely to provide robustness to such imperceptible natural corruptions. While a complete solution is not expected, it is reasonable to expect some actionable insights — for instance, [D, B] seem to suggest that varied data augmentation and self-supervised adaptation [B, C] lead to improved robustness to certain visual corruptions (the former for recognition and the latter for visual navigation). Given that efficient augmentation strategies such as RAD [E] exist, is there a particular reason why the authors did not consider experimenting with aggressive/varied augmentation as a mitigation strategy? I think including results in terms of such mitigation strategies is important for two reasons — (1) it provides an understanding of how difficult the proposed framework is (which in turn would scope out the adoption of the same) and (2) since the results indicate the natural corruptions lead to competitive or more performance degradation comparing adversarial perturbations, experimenting only with adversarial training seems insufficient. I’m open to hearing the authors’ thoughts on the same.\n\n4. [Minor Points] — I was curious how the hyper-parameters for the natural corruptions were chosen? Is it guided based on LPIPS observations? Table 1 is somewhat hard to parse. I would suggest the authors break down the same into two adjacent tables (or some other amenable format) to make it easier for the reader to parse. I’m confused by the behavior for “contrast” in Figure 2 comparing vanilla and adversarially trained policies — have the authors looked into why different sweet spots for the contrast hyper-parameter exist?\n\n[A] - Learning Invariant Representations for Reinforcement Learning without Reconstruction\n\n[B] - RobustNav: Towards Benchmarking Robustness in Embodied Navigation\n\n[C] - Self-Supervised Policy Adaptation During Deployment\n\n[D] - A Fourier Perspective on Model Robustness in Computer Vision\n\n[E] - Reinforcement Learning with Augmented Data\n",
            "summary_of_the_review": "The points highlighted under weaknesses influence my rating of the paper the most. I generally like the direction adopted by the paper and believe it’s addressing a timely issue (generalization in RL to novel corruptions) and offers empirical evidence that adversarial training does not help. Furthermore, the paper is well-written and has no major clarity issues (point 4 under weaknesses seems addressable). However, the points surrounding novelty — lack of discussion or comparison to settings in [B], lack of novelty compared to [D] — and lack of any actionable insights in terms of where to go next limit the novelty and significance of the paper, thereby influencing my rating. To be clear, I think the majority of the novelty of the paper lies in showing how perceptual distance may not be (implicitly) a good predictor of the severity of a perturbation and that adversarial training is insufficient to generalize to natural corruptions (and in fact hurts). The most relevant bits to respond to, in my opinion, would be 1 (comparisons or discussion surrounding [B]), 2, and 3 (via experimental comparisons). Addressing those will greatly help in reconsidering my current rating of the paper. Since the paper is attempting to pose itself as a framework, I would value novelty and thoroughness in terms of the framework more than a technical algorithmic contribution.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors assess the generalisation performance of DRL agents under \"natural\" perturbations. Agents are not robust to these, and such perturbations may even be perceptually more similar than adversarial perturbations. Other proposed contributions were shown in a different domain [4], and hence are not so novel.",
            "main_review": "The authors state that work has gone into assessing the performance of DRL agents against adversarial attacks, but not \"natural\" perturbations. This ignores work that is directly framed as assessing generalisation of DRL agents [1-3]; for example, [3] investigates the generalisation of DRL agents against shifts and brightness. In addition, domain randomisation is another field within DRL where agents are trained and tested against changes in the visual inputs, including brightness, sensor noise, etc. While researchers working on adversarial perturbations might argue that it is better to train against worst-case distributional shift, these areas of research have already shown that agents fail against natural perturbations. To the best of my knowledge the use of perceptual similarity distance to assess corruptions within the context of DRL is novel.\n\nSome of the findings in this work are only novel in the context of Atari video games. [4] investigates adversarial training and natural perturbations, finding that adversarial training can make CNNs less robust to some natural perturbations. In addition, [4] assesses their findings using the Fourier spectrum. Given how much is shared with [4], the authors should really cite this work.\n\n[1] Zhang, A., Wu, Y., & Pineau, J. (2018). Natural environment benchmarks for reinforcement learning. arXiv preprint arXiv:1811.06032.  \n[2] Zhao, C., Sigaud, O., Stulp, F., & Hospedales, T. M. (2019). Investigating generalisation in continuous deep reinforcement learning. arXiv preprint arXiv:1902.07015.  \n[3] Dai, T., Arulkumaran, K., Gerbert, T., Tukra, S., Behbahani, F., & Bharath, A. A. (2019). Analysing deep reinforcement learning agents trained with domain randomisation. arXiv preprint arXiv:1912.08324.  \n[4] Yin, D., Gontijo Lopes, R., Shlens, J., Cubuk, E. D., & Gilmer, J. (2019). A Fourier Perspective on Model Robustness in Computer Vision. Advances in Neural Information Processing Systems, 32, 13276-13286.",
            "summary_of_the_review": "While the goal of the paper is interesting, it is of limited novelty, and an omission of at least 1 highly relevant work means that I would recommend this for rejection.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper investigates semantically meaningful observation corruptions in reinforcement learning. It is demonstrated that deep RL policies lose performance with these natural corruptions. Policies trained to be robust to $\\ell_p$-ball adversarial examples are affected more than the ordinarily-trained models.",
            "main_review": "I'm glad to see that the area of robustness of deep RL policies is starting to become quite popular. \nAs far as I can tell, this paper has two contributions. First, an empirical study on several ALE environments of the effect of natural corruptions to images such as compression, tilting, brightening, etc. It is demonstrated that these natural corruptions can degrade the performance of an A3C-trained agent. Furthermore, the corruptions are evaluated as having a lower perceptual difference than a typical $\\ell_p$-ball attack. The second contribution is the observation that an agent which is adversarially-trained against typical adversarial attacks is actually less robust against these natural perturbations. \n\nThe paper is nicely written and explained very clearly. The experiments (while somewhat limited in being on only four ALE environments) do a good job in demonstrating that policies suffer when these natural corruptions are introduced. However, I'm not convinced of the novelty and significance of the paper overall. It has been demonstrated thoroughly before that supervised learning suffers from these natural corruptions (aka 'natural adversarial examples') in a variety of recent papers which are cited in this paper. Given that we know that this is a problem for supervised learning, showing that this is also a problem for deep RL policies on a few ALE examples doesn't seem like a significant contribution. If there were additional contributions such as methods for training against these corruptions, that would increase the significance in my view. \nIf the authors or other discussion participants could demonstrate that the paper makes a sufficiently separate contribution apart from these previous work, I would consider raising my score. ",
            "summary_of_the_review": "The paper is well-written and clear. However, the significance/novelty is limited; it is already known from supervised learning that neural networks are vulnerable to unrestricted adversarial examples and the paper's contribution is limited to showing that this is also the case for a few ALE environments. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper assesses a variety of \"natural\" perturbations (e.g. shifting, rotation) to image observations of deep RL policies, finding that they often produce comparable (and sometimes greater) reductions in policy performance than targeted, $\\ell_p$-norm based attack (Carlini and Wagner). Additionally the work makes use of an existing perceptual similarity metric LPIPS, finding that these natural perturbations typically have a higher perceptual similarity (lower LPICS distance) than $\\ell_p$-norm attacks. Further experiments demonstrate that (a) adversarial training against $\\ell_p$-norm attacks can make a policy *more* not less vulnerable to these natural attacks; (b) that these natural perturbations change the Fourier transform of the observations in a different way to $\\ell_p$-norm attacks (specifically more focus on low over high frequencies).",
            "main_review": "Strengths:\n  - The natural perturbations are realistic, and likely to occur even in the absence of an adversary in real-world deployment. An empirical evaluation of these perturbations in the RL domain is therefore valuable, and overdue.\n  - The result in section 4 that adversarial training against $\\ell_p$ norm perturbations can decrease robustness in other domains is striking, and was one of my favourite parts of the paper. Although it is not so surprising (similar results have been reported in supervised learning) but it's valuable to have confirmation in an additional domain.\n\nWeaknesses:\n  - While to the best of my knowledge this work is the first to apply these natural perturbations to RL, they've been quite widely used in supervised learning adversarial examples. This limits the novelty of the paper. I would also consider adding a citation to mention some of the prior work on these natural pertubations.\n  - The purpose of the perceptual similarity metric is unclear. Presumably it's intended as a proxy for human perceptual similarity? If that's the case, it seems like a mediocre proxy at best: e.g. rotation and brightness&contrast both have low distance (Table 1) but I find them visually very obviously different, much more so than blurring (which has a higher distance).\n  - The manuscript is generally lacking in polish; clarity of the tables and figures in particular could be approved. For detailed list, see below.\n\nMinor points:\n  - The abstract is very long (over half a page), please shorten it.\n  - Intro: \"increase in the capabilities of RL agents Schulman\" -- \\citep not \\citet?\n  - Section 2.4: similar issue, \"network architectures Iandola et al. (2016) ...\"\n  - Section 3: another case where citations should be parenthetical, \"agent's neural network Huang et al \" ... and in the following two lines.\n  - Section 4: and again, \"self-driving automobiles Dosovitsky et al ...\"\n  - Table 1: very information dense. I'm glad the table exists, but can you find some way to present the data in a more easily parseable manner? Some ideas: 1) have background of cells vary like a heatmap depending on the value of the cell; 2) present some subset of these results in a figure (e.g. barchart of impact).\n  - Table 2 caption: \"brightness &contrast\" -- add space after &.\n  - Figure 4: consider adding a title to the columns (and perhaps rows), hard to manually match up with the figure caption. Also, include a colorbar.\n  - Figure 5: these figures have clearly been generated at a larger scale and then resized, causing the fonts to be tiny. Please generate at the size you will include in the paper, with font sizes that match paper body text. The legend is incomprehensible without zooming in. I'm also a bit confused why these are separate figures -- wouldn't it be easier to compare them if it was a single plot? Might be hard to read if lines are overlapping, but that could perhaps be resolved by use of linestyles as well as different colors.\n",
            "summary_of_the_review": "The paper presents a convincing demonstration that natural perturbations can cause similar or greater performance degradation to the classic $\\ell_p$-norm style attack against RL policies. While similar results have been found in supervised learning, testing in new domains is valuable, and the paper includes some other nice results: e.g. the adversarial training. However, there is nothing in the work that is extremely novel or surprising, some choices (e.g. perceptual similarity metric) are not clearly justified, and it is in places hard to read. Overall I am borderline but lean accept.\n\n**Update**: After reading the other reviews, I find the omission of related work raised by reviewer ZEDG to be significant. In particular, I now find the paper less novel, although it still presents some interesting results. Moreover, I do not agree with the reasons the authors give for dismissing the relevance of this prior work -- I believe the paper would be stronger were it to clearly explain the prior work and make explicit the novel contribution. Consequently, I am downgrading my vote from 6 to 5.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}