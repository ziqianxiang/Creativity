{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a special abduction problem in propositional logic, named combinatorial causal optimization problem, and presents an evolutionary algorithm to solve it. Experimental results on four datasets demonstrate that the proposed algorithm outperforms several algorithms based on causal structure discovery in generating closer-to-real instances.",
            "main_review": "Strengths:\nThe proposed combinatorial causal optimization problem is interesting and may be valuable in practice.\n\nWeaknesses:\n\n(1) The topic of the paper is irrelevant to machine learning and may not fit this conference. It cannot be seen from the paper what should be learnt from the data to underpin the proposed problem and its corresponding algorithm.\n\n(2) The representation of the paper lacks of rigor. Firstly, the term of KB and the term of a solution in it are directly used in Definition 5 and Definition 6 without being defined. Secondly, all operators given in Section 4.3 are incompletely defined. The authors only refer all these operators to algorithms in Appendix and do not provide some declarative definitions for them. Finally, all definitions except the first two simple ones (i.e. Def 1 and Def 2) are given without any examples. It makes it very hard to check the correctness of these definitions.\n\n(3) There is no complexity analysis on the proposed problem. As a special abduction problem in propositional logic, the combinatorial causal optimization problem should be reformulated to a logic-based abduction problem in propositional logic, thus its complexity can be analyzed in the logic-based framework such as whether in PTime or NP-complete, etc.\n\n(4) The empirical comparison with logic-based abduction algorithms is missing. Some modern abduction systems such as A-system [1] and CIFF [2] can exactly solve the abduction problem in propositional logic. This comparison is needed to clarify why the authors do not apply classical abduction algorithms to solve an abduction problem but develop an inexact evolutionary algorithm to solve it.\n\n[1] Kakas, A. C., Nuffelen, B. V., & Denecker, M. (2001). A-System: Problem solving through abduction. In Proceedings of the 17th International Joint Confer¬ence on Artificial Intelligence (pp. 591-596).\n\n[2] Mancarella, P., Terreni, G., Sadri, F., Toni, F., & Endriss, U. (2009). The CIFF proof procedure for abductive logic programming with constraints: Theory, implementation and experiments. Theory and Practice of Logic Programming, 9, 691–750.\n\n",
            "summary_of_the_review": "The topic of the paper is irrelevant to machine learning and may not fit this conference. The representation of the paper lacks of rigor and is hard to digest. No complexity analysis is presented for the proposed problem and the empirical comparison with logic-based abduction algorithms is missing.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors introduce a new method for automated causal hypothesis generation called Evolutionary Abduction (EVA). It is based on framing the problem as a combinatorial optimisation problem where the target solution is characterised by a plausibility score relative to a knowledge base of known cause-effect combinations. The method is evaluated on several real-world data sets against graph based alternatives and found to score higher according to a Jaccard distance metric.",
            "main_review": "The problem is very interesting, relevant and challenging. Having a method to generate likely causal hypotheses / explanations would be very useful in many real-world experimental settings. The approach taking is novel (to the best of my knowledge) and differs from many existing causal inference methodologies. \n\nUnfortunately there are several serious shortcomings \n\n1.Clarity of purpose: having an automated algorithm to generate a set of causal hypotheses is very interesting, but without a clear purpose just having ‘some’ set of hypotheses remains rather pointless, with no means of deciding between good and bad sets. \nAt the moment it is not clear what this set of causal hypotheses is intended to support: if it is to aid in experimental design for follow up experiments to confirm or disprove certain hypotheses, then we may be interested in making sure to generate hypotheses wherein a specific causal link is present or absent … or we may be interested in generating sets that contain all relevant direct causes of a specific target … or sets that would make valid adjustment sets in computing the causal strength of specific cause-effect relations … or we may be looking for recommendations on specific variables to randomise in subsequent experiments in order to decide on the actual causes .. etc. etc.\nThe purpose determines the relevance/quality of a given set of hypotheses, and then a method used to generate these should be evaluated on how well that purpose is supported. The latter should involve restrictions on the size of the set (e.g. do we evaluate succes against the most likely explanation? .. or against the top 5 most likely explanations? etc.)\nAt the moment we just have a collection of statements that says ‘it could be this’, and all we know is that it optimises similarity against some set of background knowledge statements, without any guarantee or indication of how relevant this may be to tackle any real-world experimental design situation.\n\n2. Theoretical rigour / principles: ultimately the approach developed in the paper is based on similarity based predictions rather than principled causal inference. There is no justification why framing the problem as a combinatorial optimisation under given constraints would lead to a principled conclusion on the best possible causal candidates. Worse: there is no objective ground truth in terms of the existence or absence of specific cause-effect relations in given problem, as all generated hypotheses are measured against some reference set Z according to some distance metric rather than to the actual underlying causal structure of the target problem. It is well-known that a strongly associated side-effect will be a very good predictor of some target (so small distance), but that does not make it an actual cause. \nLikewise ‘plausibility’ implies that at least some degree of likelihood is involved, but it is defined in terms of ‘similarity to already known relations’ rather than relative to statistical evidence provided by the available data. \nIt employs several different knowledge bases (KB) as references to construct different sets of hypotheses based on different types of abduction, but it is not clear how we got these in the first place or why the existence of a cause-effect relation in one problem should be considered proof it is the same in a different setting. E.g. mosquito abundance is highly relevant in explaining/predicting fevers in regions with high incidence of malaria, but probably completely irrelevant when explaining fevers from bronchitis. \n\n3. Experimental evaluation is biased: the EVA algorithm is compared against several causal structure discovery algorithms on some (very interesting) data sets based on a self-defined similarity metric. However, this means that there is no objective ground truth, and the graph-based alternatives are not designed to generate the target ‘lists of hypotheses’ let alone to exploit the rather curiously available  knowledge bases. Currently the evaluation metric is tailored to the objective function used in the optimisation strategy of the EVA algorithm. By definition that implies EVA will outperform other methods on that particular metric, but that is not good scientific practice. It is possible the graph based routines find the absolute correct causal structure and still score worse on the given similarity metric. Furthermore, the prior knowledge given to graph based methods could be in direct contradiction with the actual causal structure they try to infer: in App.B, bottom of p15, it says the graph based routines were given that arcs between the potential causes should be forbidden ‘because we are only interested in arcs between causes and effects’. That is fundamentally wrong and could force the algorithms into suboptimal and inconsistent conclusions.\n\nI do like the problem considered in the paper, as well as the ideas behind the approach developed in the EVA algorithm. However, at the moment there are just too many question marks and caveats to consider the resulting end product a justified and meaningful contribution to address this problem, and therefore I have to recommend reject.\n",
            "summary_of_the_review": "Interesting problem, original and potentially promising approach, but lack of theoretical rigour and flawed experimental evaluation make it not ready for publication in the current form.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes an evolutionary algorithm to solve abduction tasks. The proposed EVA algorithm will generate a series of ground clauses and evaluate their effectiveness given the training examples.",
            "main_review": "Abduction is indeed a very interesting process in both human and artificial\nintelligence, its indeterminism and high complexity makes the problem difficult\nto solve (NP-hard). Using evolutionary algorithms to solve it is a natural idea,\nand this work presents two fitness functions to evaluate the effectiveness of\nthe abduced groundings to be searched.\n\nHowever, the authors have clearly missed many past progresses in the field of\nsymbolic AI. For example,  abducing \"hypothetical causes\" can be regarded as\nabducing logic rules such as definitive clauses, which has been studied for\na long time and has gained tremendous improvement over the past years. For\nexample, the abduction-based program synthesizers can now abduce first-order\nlogic rules which allow recursion and invent new relations:\n- S.H. Muggleton, D. Lin, N. Pahlavi, and A. Tamaddoni-Nezhad. Meta-interpretive\n  learning: application to grammatical inference. Machine Learning,\n  94:25-49, 2014.\n\nOn using evolutionary algorithms or genetic algorithms for solving abductive\nreasoning, there are some missing references worth pointing out, too. For example:\n- L.B. Romdhane & B. Ayeb (2011) An evolutionary algorithm for abductive\n  reasoning, Journal of Experimental & Theoretical Artificial Intelligence,\n  23:4, 529-544, DOI: 10.1080/0952813X.2010.505824\n- Philip G. K. Reiser and Patricia J. Riddle. 2001. Scaling Up Inductive Logic\n  Programming: An Evolutionary Wrapper Approach. Applied Intelligence 15(3),\n  181–197, 2001.\n\nThe main difference of the proposed method with those previous works seems to\nbe the introduction of  _novelty_ as a fitness function. However, I think this\ncontribution could be not enough for publishing on this venue.\n\nAnother minor problem is that, abduction itself has nothing to do with\ncausality. Although it can express causality when the background knowledge is\ndefined with causal rules, the \"abduced rules\" does not equal to \"causal\nrelation\" when there is no further constraint on the abductive reasoning\nprocess. For example, one observing that the sun always rises after rooster\ncrowing can easily abduce a \"rule\": $\\text{rooster crow}\\rightarrow\\text{sun rise}$\".\nHowever, these two facts are just correlated. I think the authors should revise\ntheir claims carefully to explain how causality is mined from data using the\nproposed method.",
            "summary_of_the_review": "This paper has clearly missed many important related work in this field, the\nnovelty of the proposed algorithm is limited considering those missed prior\nwork. So I would recommend for rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper considers the problem of abduction: given an observation derive possible reasons that explain that observation. It then proposes and evaluates an algorithm for solving this task by following an evolutionary approach, where a population of abductive solutions is created and mutated across generations to improve their quality. \n",
            "main_review": "\nStrengths:\n\n- The paper takes a broad view of abduction, not just abduction of facts, but also abduction of part of the theory through which observations are going to be explained.\n- The empirical results seem to show that the method improves existing abductive algorithms.\n\nWeaknesses:\n\n- The paper is considerably dense, making it hard to understand the notions and notation that are introduced. There are no examples.\n- The size of the appendix seems to suggest that a lot of information has been omitted from the presentation (which would explain the point above).\n- The discussions on the measures of plausibility and novelty seem to suggest that they are inspired by humans, but there does not seem to be a concrete connection to any relevant literature to support these claims.\n\nGeneral remarks:\n\nConstructing better and human-inspired algorithms for abduction is a problem worth-looking into, especially given its connection to explainable ML; see, e.g., Kakas et al., \"Abduction and Argumentation for Explainable Machine Learning: A Position Survey\", https://arxiv.org/abs/2010.12896.\n\nAn evolutionary approach might be a reasonable take on this problem. But I had a hard time following the paper's contribution. It is very densely-written, and it introduces a lot of notation and concepts without motivating examples. Relatedly, the empirical section is also rather dense, which makes it hard to appreciate the results obtained. I believe that rethinking the presentation of the paper would help to make its contribution more clear. For example:\n\n- Taking a broad scope of what abduction is, is very welcome. Consider using a running example that demonstrates the various forms of abductions, how they relate to each other, and how they relate to learning (especially the most general type of abduction where one abduces new knowledge or rules).\n\n- Since at its heart the proposed algorithm is an evolutionary one, I would suggest that a vanilla evolutionary algorithm is used, and then the paper could state how its various procedures (mutation, fitness, selection, etc.) are implemented in the context of this paper.\n\n- If not all forms of abduction could be fully presented in this paper, consider introducing them all, and then focusing on one of them to further elaborate and evaluate. The other forms of abduction could then be part of the appendix. This would seem preferable than trying to juggle all forms of abduction throughout the paper, and not doing justice to any of them because of the lack of space. \n\n- The paper argues that certain characteristics of the proposed algorithm are inspired by humans. The paper would benefit by grounding these arguments on any relevant literature.\n\n- I understand that you may wish to impose certain hyperparameter choices for EVA (e.g., eta and gamma), but why are you imposing those same choices on pre-existing algorithms? It is a different thing to define certain metrics against which you will evaluate all algorithms, and it is a different thing to parameterize the algorithms (including prior-art). I would suggest that you clearly distinguish between the problem to be solved and its evaluation metrics, from your proposed algorithm and its hyperparameters, and from the empirical evaluation and the competing algorithms. \n\nAdditional points:\n\n- The definitions of k-degree and m-degree are confusing. Are k and m supposed to be names/types of degrees, or are they variables that take values. So, if I say the 3-degree what do I mean: the k-degree or the m-degree? It seems that k is a variable and m is a name. So, perhaps use a different notation to distinguish between the two concepts. Also, m-degree does not seem to be invoked somewhere (but, perhaps, I might have missed it).\n\n- Just after definition 5 you talk about a denominator. What denominator is that?\n\n- Why do you have six plots per problem in Figure 1? I can only guess that they represent {Low, Medium, High} x {Best, Worst}. Please make sure you explain what each plot refers to.\n\n- What is eta. and gamma. (I mean, what is the meaning of the dot after each letter)?\n",
            "summary_of_the_review": "The broad take on abduction is important, and an evolutionary approach might be reasonable. However, the paper does not ground its point that a human-inspired approach is taken on the relevant literature, and the overall presentation and structure of the paper obscures the main message of the paper.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}