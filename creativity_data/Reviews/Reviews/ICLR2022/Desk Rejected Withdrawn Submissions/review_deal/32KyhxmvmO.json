{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work explores how to learn partial subgraph representation.",
            "main_review": "Learning representations with subgraph has gained more and more attention due because of its multiple benefits to deal with large scale graphs. In this work, a new concept namely partial subgraph has been proposed for some special applications like fake news detection. Then authors give a solution based on MI optimization loss for learning the subgraph classification task. However, there are still many concepts that are not explained properly.\n\nConcerns:\n1. The definition of partial graph is not properly presented. As a matter of fact, it's very difficult to have a completed view of a given graph. Because of this, the observed graph is already a partial graph. Why do we need to define the concept \"partial subgraph\"? What is the relation between partial subgraph and observed subgraph? Is the partial subgraph already an observed one or sampled from the completed graph?\n\n2. The motivation behind the solution is not clear. It's difficult to understand why maximizing the MI loss works for learning partial subgraph? Can recent proposed GNN methods not address the challenges?\n\nMinor issue:\n1. The symbols to represent the nodes and edges $\\mathbb{V}$ or $\\mathbb{A}$ can be easily to be confused with the symbol $\\mathbb{R}$.\n\n2. There are some grammar errors. For example, the sentences like \"But the dynamics and temporiality xxx, there may be xxx.\" and \"As we introduce a new class of tasks, these should be mainly xxx \" in Section 2.2.",
            "summary_of_the_review": "The presentation needs to improve. Current version is difficult to follow.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a contrastive learning objective of nodes within (intra) and across (inter) subgraphs for subgraph neural network training, and sees improvement when label data is scarce.",
            "main_review": "Though this paper finally observes advantage over supervised subgnn baseline, the novelty of it compared with existing self-supervised learning method is weak, and there is no any comparison with them.\n\nThe authors should at least discuss and compare with some existing GNN self-supervised learning baselines, including but not limiting to: 1) graph infomax, 2) graphCL, 3) GPT-GNN, 4) MVGRL, etc.\n\nAlso, the datasets used is very limited, even comparing to the original subGNN paper. I think the authors should conduct experiments on more datasets",
            "summary_of_the_review": "The evaluation is weak regarding to lack of self-supervised baselines and limiting datasets.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigates the problem of subgraph classification on a graph, and proposes a subgraph infomax based method to address the representation learning of partial subgraphs. Experiments on three datasets show that the proposed model can outperform the baselines.\n",
            "main_review": "Several main concerns should be well-addressed by the authors.\n\n1. In the Introduction, the authors claim that \"Similar to these previous models that maximize MI between the global summary (e.g., graph) and local parts (e.g., nodes)\". Actually, some of them also consider the MI maximization between nodes and subgraphs. From this point, the novelty of this paper is a bit incremental.\n\n\n2. \n- (1) Another problem, multi-graph classification, which aims to learn graph-level representations for graph classification, is quite similar to this one, if we change the subgraphs (in this paper) to graphs (in the multi-graph setting). However, no comparison as well as the survey of related studies are given.\n\n- (2) Besides, some recent studies such as [1,2] investigate the MI maximization in the multi-graph setting (though [1] is cited). The proposed model in this paper has a similar setting with these approaches, and they also share similar designs. The authors should comprehensively compare these approaches with the proposed model.\n\n\n3. The motivation that why the usage of MI maximization can enhance the subgraph classification performance is not clearly presented. The authors should give more details here. Why the two-levels MI maximization can boost the representations of the partial subgraphs?\n\n\n4. The authors claim that \"With only a few observed nodes, a naive summary based on those few do not hold much information of the entire subgraph\". So why the proposed model can refer to more information than those?\n\n\n5. Some parts of the proposed model are confusing.\n\n- (1) In the section of \"Intra-Subgraph InfoMax\", the authors claim that \"we cannot know the exact boundary of the subgraph at this stage\". Why cannot know? In the following part of this paragraph, V^{sub} is utilized to decide the set of V^{sub_k} and V^{glob_k}.\n\n- (2) Why use the k-hop neighbors to select the positive nodes, and is this method definitely correct so that they can serve as the positive nodes? Besides, why not directly utilize the employed V^{sub}? And why V^{glob_k} can serve as the negative nodes?\n\n- (3) At the end of section \"Intra-Subgraph InfoMax\", the authors employ the hop distance to replace the embedding distance of nodes. However, though efficient, there is an important issue here. GNNs usually resort to neighborhood aggregation to calculate the representations of nodes with a few layers (corresponding to a few hops). Thus the similarity of node embeddings actually reflects the similarity between their context information. Two distant nodes with quite similar context, would also have similar representations. Thus, directly using the hop cannot reflect the true distance between nodes.\n\n- (4) In Eq.(7), the top-k indexes are employed for aggregation. Does the selection of top-k indexes cut off the backpropagation?\n\n\n6. In Experiments, the baselines are not sufficient. Actually, each subgraph can also be regarded as a graph in the setting of multi-graph classification. This kind of method should be compared. Besides, how about the other MI maximization based approaches, can they perform well on this task?\n\n\n[1] Graph Cross Networks with Vertex Infomax Pooling. NeurIPS 2020\n\n[2] Graph Pooling via Coarsened Graph Infomax. SIGIR 2021",
            "summary_of_the_review": "1. Some parts of this paper are confusing and not well-motivated.\n2. The experiments are not sufficient.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}