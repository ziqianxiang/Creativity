{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper compares several blackbox optimization tools on a large number of benchmarks including synthetic and real black-box optimization problems. The compared methods are popular BBO libraries such as CMA, Cobyla, NGOpt, Turbo, SMAC, and others. The methods are compared on synthetic benchmarks from the BBOB library while varying the optimization budget, and the dimensionality of the input domain. Another set of comparisons is performed on direct policy search for reinforcement learning, using gradient free techniques. Experiments demonstrate that classical BBO methods often outperform Bayesian optimization in both the synthetic benchmarks and direct policy search.",
            "main_review": "**Strengths**\n- This paper presents a useful set of benchmarks for several gradient free optimization methods. Traditionally black-box optimization algorithms such as Bayesian Opt have been compared with baselines which are not necessarily well implemented (or tuned). This benchmark is a significant contribution to the black-box optimization literature, and can help guide further design of black-box optimization algorithms through more consistent evaluation.\n- The experiments are quite extensive and also claimed to be reproducible. The details for reproducibility have been provided in the appendix.\n- The evaluations are also repeated using different random seeds or different functions resulting in a more robust evaluation.\n\n**Weaknesses**\n- While the contributions in this paper are significant to the community, the paper is not ready for publication as is. Certain parts of the paper are not easily understandable. For instance, Section 2, para 1: (1+1)-type sampling metamodels doesn't have a definition or citation; Page 2, middle: precision has not been defined (the definition is clear only after reading the rest of the paper); Page 5, middle: 'neural factor' is not properly defined; Conclusion, para 2: 'uniform translation ...' is not elaborated clearly.\n- Some design choices have not been explained well. For example, why is the precision set to 1e-5? The precision to be used can vary from problem to problem, for instance when optimizing for accuracy (out of 100% percent) of a ML classifier a precision of 1e-1 suffices. Similarly, the scaling factor of Nevergrad is set to 3 for higher dimensional problems without a clear explanation. This raises the question of whether all runs involving Nevergrad were out of the box or mildly tuned for some settings.\n- A common application of BBO is hyper-parameter optimization, which is missing from this paper. This is the most significant weakness of this paper. HPOLib is a popular HPO test bench that could be used.\n\n**Minor Issues**\n- Figure 3 is almost illegible.\n- Figure 4 caption: Is it meant to be 'method B outperformed method A'?",
            "summary_of_the_review": "To summarize, the paper is a significant step towards reproducible, consistent, and robust evaluation of BBO algorithms. However the popular application of hyper-parameter optimization is missing. Lastly, the paper also needs more work in terms of elaborating certain sections for a better understanding.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work provides empirical results for blackbox optimizers comparisons on two benchmarking suites: BBOB and policy search on OpenAI Gym. The most important message I learnt from the work is that some classical blackbox optimization methods such as Cobyla or PSO can perform very well.",
            "main_review": "\nThe strengths lie in the inclusion of the large number of benchmarks and the optimizers. Benchmarks range from classical BBOB benchmarks and optimizers include model based optimizers such as BO and other BBOB optimizers such as CMA, Cobyla, etc. \n\nHowever, the contributions of the work are all on the empirical results of existing methods and benchmarking tools. I would expect a much stronger experiments section. I list several directions for improvement in the following.\n\nCobyla performs the best on the BBOB benchmarks (as in Figure 1) but very bad on OpenAI Gym (as in Figure 3). While PSO performs the overall best on OpenAI Gym, it is not included in the BBOB benchmarks. This gives me the impression that the results are very much problem type dependent. Other measures for robustness such as the ranking of the methods over different types of problems can also be used. It would be interesting to know for example how frequent SMAC or other promising methods achieve top 3 on all the benchmarks.\n \nTo make the results more convincing, besides the reinforcement learning benchmarks, it is worthwhile to consider the classifical benchmarks to tune hyperparameters of some supervised machine learning algorithms (for example, I find a git repo listing common ones at https://github.com/automl/HPOBench). As one conclusion of this work is that BO underperforms other classical methods, the results on these benchmarks will be more recognizable by the BO community.       \n\nThe authors mentioned in the introduction that previous comparisons may favor some family of methods. But it is not only about the number of benchmarks used for comparison, but also the configurations or hyperparameters to run each method. Have the authors spent some effort to make sure the methods are not mis-used? This is important as the contributions of the work rely on the soundness and generalizability of the empirical results.\n\nIt might be hard to explain and describe all the optimizers in this paper. But for the top performing ones such as Cobyla and PSO, for self-inclusive reasons, it would be nice to describe the methods in the main text. Even better, when possible, give some insights on why these methods are good for these types of problems. ",
            "summary_of_the_review": "The contributions of the work are all based on the empirical results of existing methods and benchmarking tools, thus lacking enough novelty. The provided results and findings are interesting, but one can only say that other classical methods such as Cobyla or PSO can perform very well on some problems, which is not too surprising in my opinion. Thus the significance of the work is limited from my perspective.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents the findings of two experiments that compare a number of black-box optimization software. The first experiment uses the BBOB benchmark problem set. The second experiment uses direct policy search for OpenAI Gym. Comparing to previous work, this study tested more black-box tools and specifically for low-moderate evaluation budget. \n",
            "main_review": "The experiments include quite a few black-box tools including SMAC, HyperOpt, Cobyla and even random search. The testing environments are only traditional black-box optimization, but also newer reinforcement learning. Some interesting findings are reported. For instance, the paper claims Cobyla is very competitive when solving  classic black-box optimization problems and SMAC seems to be the \nbest among the tested BO methods for both BBOB and OpenAI Gym. \n\nIn my opinion, the weaknesses of this paper are \n1) the purpose of comparing different black-box tools is not clear. Is it to find which tools perform better for which types of problems? or to recommend which tools to use under which conditions? It will be good to explicitly state the purpose and then use experiment results to provide suggestions on the weakness or strength of different tools or different types of tools in general.\n2) the paper doesn't give too many details of experiment setting. What are the hyperparameter settings? Default settings? Version of tools? etc. In page 5,search domain is [5; 5]D, why choose this size of domain?\n3) The writing can be better. Fig 3 is very busy. In Fig 4, should mention that Fig 5 and 6 are in Appendix. In Fig 2, \"Turbo’s speed makes its for this higher budget\" is not clear what it means. ",
            "summary_of_the_review": "A few interesting findings are presented by the paper, which studied various black-box tools for solving classic black-box optimization and OpenAI Gym. However, the innovation and contributions of this paper is not too significant. It is an extension of Hutter's study, however the extra insight arising from this research is not too much and deep enough. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper aims to improve on the lack of robust benchmarking in the context of hyperparameter optimization by performing an empirical comparison of competing techniques which extends the prior work from Hutter et al., 2013. Specifically, new optimizers and tasks are introduced in the comparison, such as direct policy search for OpenAI Gym problems.",
            "main_review": "**Clarity**\n\nThe writing could be improved. Some minor suggestions include removing overly vague statements such as \"people with many different backgrounds are drawn into BBO\" or at least supporting them with specific examples, and avoiding overly long paragraphs such as the third one in the introduction. Figures are too dense and hard to read. Further, the key takeaways from each experiment should be summarize more clearly, such as through bullet points. Currently, it is not easy to tell what are the main empirical conclusions drawn from the benchmarking (e.g., SMAC comparing favorably to competitors).\n\n**Reproducibility**\n\nThe paper in fact aims to improve on reproducibility issues in the context of HPO comparisons. The experiments from the paper are based on code which is given in the appendix, and instructions to reproduce the experiments are also detailed. That said, it would have been helpful if the authors linked to an (anonymous) repository where the experiments can be reproduced simply by executing a single script. While this can be done by following the provided instructions, it adds some unnecessary friction for a paper focused on benchmarking and reproducibility.\n\n**Technical**\n\nI found novelty to be the key weakness of the paper. While it is in the nature of this type of work not introducing any new methodology but rather benchmarking existing ones, a careful comparison study was already available in Hutter et al., 2013. The authors do extend it through new optimizers and problems, and even just refreshing the results after several years is valuable, but this still makes the contribution incremental.\n\nI found the figures (e.g., Figure 3) try to cram too much information into a single plot, which makes it very hard to read. I suggest the authors split these up and use the appendix if lack of space is a problem. Space could also be gained by making the introductory sections more concise and straight to the point.\n\nExperiments such as in Figure 3 do not report error bars. What's the level of noise in the experiments? Without properly defined error bars one cannot tell if the drawn conclusions are significant. Again, if space is an issue, it would be useful to at least report standard errors in the appendix.\n\n\n**Minor**\n1. The claims in the introduction that BBO has a critical importance in many application areas should be supported by more references than only Bajaj et al., 2021.\n2. The reference style is not always correct. You should use in-line citations when appropriate  (e.g., in \"we extend the comparison made in (Hutter et al., 2013)\" the reference have no parenthesis).\n",
            "summary_of_the_review": "Overall, I am inclined towards rejection. While the empirical study performed by the authors is relevant considering the several-year gap from the previous benchmarking, novelty is limited. I am unsure how broad of an impact the optimizers and problems added to the comparison from Hutter et al. comparison will have. Further, the key messages of the paper are hidden under a sometimes verbose writing style and a cluttered presentation of the results in the plots, without clearly defined error bars.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}