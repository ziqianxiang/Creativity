{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper presents a study on identifying word-level polysemy in Korean based on a language-specific BERT model.  It is always very good to see language-specific studies especially in non-English scenarios.  However, as the reviewers point out, there could be significant improvements to the experimental design of the paper (see comments from reviewer eqSx) and also the HCI aspects of the visualization setup (see comments from reviewer MV2W) to transform it to an acceptable ICLR paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper builds a BERT classifier for word-level polysemy of Korean language. Adverbial positions such as -ey, eyes, and –(u)lo are involved in the investigation. There are three findings during the experiments, if there are more functions then the classification accuracy is lower, corpus size is sensitive and more epochs are preferred.",
            "main_review": "It is currently difficult for me to evaluate the technical novelty or methodology novelty of this paper. I could find less baseline systems of this target problem. The target question is more alike a specific NLP field of Korean laugange.\n\nDetailed questions and comments:\n1.\tFigure 1, possibly adding English annotations to these Korean sentences is a better choice?\n2.\tAre there any other methods investigated for comparison with BERT based models?\n3.\tAny details of BERT’s detailed configurations, parameter sizes, hardware used?\n4.\tAny downstream applications of using the trained bert models?\n",
            "summary_of_the_review": "Strong:\n1.\tDetailed bert classification construction and analysis for Korean word-level polysemy;\n2.\tRich examples with figures.\nWeak:\n1.\tThis is more alike an experiment report of reusing existing BERT network for a specific language related classification task;\n2.\tThe three findings are kind of general in deep learning domain and currently it is difficult to learn novel discoveries from them.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a study of training a BERT-style model on the task of function identification for 3 different Korean adverbial postpositions(ey, eyse and (u)lo). The paper presents a new dataset for the task, show that training a BERT model performs well and also introduce a method to visualise BERT embeddings as the training proceeds. The authors show that the model performs better for postpositions which have fewer associated functions (classes for classification), performs better for classes that occur more frequently in data and that model performance improves with additional epochs.\n",
            "main_review": "Strengths of the paper:\n* The paper studies the identification of functions of 3 different Korean postpositions. The task is novel and it is good to see an analysis on a lesser studied language. \n* The paper reports good performance on the task by leveraging BERT\n\nWeaknesses of the paper:\n1. I think there is a misalignment between the title of the paper and the paper itself. The title \"How does BERT address polysemy of Korean Adverbial Postpositions\", in my opinion, indicates that the study is about if polysemy is well characterised by (multilingual)BERT embeddings and if it is a consequent of the MLM style training. However, the paper presents the analysis on the embeddings of a classification model trained to identify function classes. Any well trained classifier would have separation of feature embeddings for different classes, so it is not surprising that a fine-tuned BERT model also does. But the analysis fails to answer the question if this is also inherently present in the non finetuned BERT embeddings (i.e if BERT addresses polysemy of Korean Adverbial Postpositions and if yes, then how)\n2. While the authors have presented examples associated with different functions, it would be good to actually define what the functions denote. I am still not sure about what some of the functions (eg: criterion / final state etc.) mean. If space is a concern, then the definitions as well as examples can be shifted to an Appendix section.\n3. The dataset created in the paper assumes presence of only one postposition and predicate. This also limits the proposed approach: there is no clear way of handling sentences where the same postposition can occur multiple times, associated with different functions. In my opinion, this limitation is artificial: there is no language specific reason for this constraint.\n4. The frequency of some of the classes is quite low (eg: for the AGENT function for ey, the number of samples is 47). If the testing set is 10% of this, then the number of samples in the test set with this class is ~5. This may not be a good indicator of performance.\n5. The authors indicate that they used fixed values for hyperparameters (lr 2e-5, batch size 32 etc.). However, hyperparameter tuning is very dataset specific: some tuning (instead of using fixed values) might help improve performance \n6. Since a new dataset is introduced, the methodology presented in previous work should be trained on the new dataset to provide a fair comparison against the proposed method. In the current version, this crucial detail is not present.\n7. The issues around under-represented classes not achieving good performance is a classic case of class imbalance, and is well studied in literature. The authors can look into the relevant literature to address the issue (eg: a quick search results in [1,2]. Might be good place to start)\n8. The posited findings that having multiple functions (classes) for classification make the problem harder, increased training helps improve performance and under-represented classes have worse accuracies are trends that would occur commonly for any learning task, and in their current form don't provide any new insight about the specific task.\n9. The T-SNE visualizations of embeddings as the training progresses does not provide any insight: a well trained classifier should map feature representations of different classes into separable clusters as training progresses. In my opinion, an in depth analysis of the embedding space of an unsupervised BERT model (i.e not trained on the classification task) vis a vis the task would be more insightful and instrumental in answering how BERT handles polysemy for postpositions.\n\n[1] Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class\nimbalance problem in convolutional neural networks. Neural Networks, 106:249–259, 2018\n[2] Dong, Qi, Shaogang Gong, and Xiatian Zhu. \"Imbalanced deep learning by minority class incremental rectification.\" IEEE transactions on pattern analysis and machine intelligence 41.6 (2018): 1367-1381.",
            "summary_of_the_review": "While the task is novel and it is good to see analysis on less well studied languages, this paper in its current form needs additional iterations in terms of technical contributions for the paper to be accepted. Additionally, there are concerns related to the research question, experimental setup and baselines (please see the Main Review above) that should be addressed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The technical essence of the paper is to use BERT to perform classification and provide basic visualization of the change of contextual word embeddings during training. The classification problem concerns polysemy in Korean: for each adverbial postposition (e.g. -eyse), classify the use of the postposition in a given sentence into one of the available functions (e.g. LOC or SRC), where the number of classes range from two to eight.\n\nThe contributions of this paper are as follows: the paper studies a relatively understudied language, Korean; the authors made progress on a very specific instance of polysemy classification problem by using contextual word embeddings compared to the previous approach that uses traditional static word embeddings (albeit this claim is not supported by the experiments); and the authors created a visualization tool for the word embeddings and model performance.",
            "main_review": "The contribution of the paper is very marginal from both NLP and HCI perspectives.\n\nFrom a NLP perspective, the proposed method is to use BERT in replacement of static word embeddings. Apart from the main technical contribution being the mere adaptation of the existing technique (which is not necessarily a bad thing), the paper fails to provide any meaningful comparison between the proposed method and previous baselines or other alternatives (although they imply that their method is better than previous methods, saying “the model performance reported in the previous studies is unsatisfactory”). Furthermore, the three findings from case studies (Section 4) basically restate the common knowledge about training any ML model and do not provide any new insights from the study. Most importantly, it appears that their data creation and experiments do not follow standard practices in NLP: the annotated data do not have a validation set; the model was fine-tuned for 50 epochs despite the small size of the dataset and the convergence around 20th epoch, etc. These are taken as signals that the experiments are not performed in a correct manner. \n\nFrom a HCI perspective, the visualization system presented in the paper does not include any supporting evidence for its usefulness and effectiveness (either using standalone metrics through user study or in comparison to other existing visualization tools). The system contains t-SNE visualization as well as a few model performance metrics such as accuracy, which closely resembles TensorBoard. More explanations and justifications regarding the validity and contribution of the proposed visualization system are required. \n\nLastly, the specific instance of Korean polysemy classification (-ey, -eyse, and -(u)lo) is an interesting subject of study, but is likely to be too narrow to fully demonstrate and justify the effectiveness of the proposed method. In addition, as of now, the classification problem only considers from two to eight classes, and  the method heavily depends on the manual annotation of data, which are both not ideal.\n\nOverall, I would recommend improving upon the depth and breadth of the subject (studying 100+ instances of such polysemy and integrating insights/intuitions into the model training) and connecting the findings about the model (when it performs well vs. poorly) to observed human behavior/perception/understanding for such phenomenon from linguistics -- maybe NLP conferences are a more suitable venue for this. Also, more discussion about related areas such as word sense disambiguation and lexical substitution is expected from this work.",
            "summary_of_the_review": "The contribution of the paper is very marginal from both NLP and HCI perspectives, and is not well-supported by the experiments.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a BERT-based classification to predict a function of an adverbial postposition. Furthermore, this paper presents a visualization system to interpret the predicted results.",
            "main_review": "Pros:\n+ This paper introduces three Korean adverbial postpositions (-ey, -eyse, and –(u)lo) with examples.\n\n+ This paper constructs a human-annotated dataset for this research.\n\nConcerns:\n- The key concern about the paper is the lack of description about the usefulness and the setting of this study. Despite the paper mentioning that there is a gap between English and Korean, there is no description about why we need to solve polysemy of the Korean adverbial postpositions. Furthermore, since this paper does not specify the reason why only three adverbial postpositions -ey, -eyse, and –(u)lo are chosen, this paper should clarify why those are selected.\n\n- Another key concern is the lack of model novelty used in this paper. This paper adopts BERT. \nThat is, this paper lacks rigorous experimentation about the proposed method. \n\n- This paper uses the [CLS] token to predict adverbial postposition’s function. How about using the adverbial postposition tokens compared to [CLS] token? In other words, an ablation study on network would have been very useful. \n\n- This paper extracts sentences that have only one postposition and predict. This seems that this paper makes the task simple, but there is no justification for this. \n\n- According to Table 1, the annotated data is quite imbalanced. However, this paper uses accuracy as the evaluation metric. At least, this paper should have used the F1-measure as the evaluation metric.\n\n- All experiments are done without the validation dataset. Showing the performances per epoch is meaningless since we do not know test data in advance. Furthermore, there is no point in showing the average performance of epoch 1, 10, 20, 30, 40, 50.\n\n- The claim that the average classification accuracy is at a satisfactory level considering previous reports has not been clearly addressed because the dataset used in this paper and the ones used in previous studies are not the same.\n\n- On the same note, the findings of this paper is not compared with other methods. Without experimental or mathematical comparison with the other published methods, it can't to prove the superiority of the proposed method. \n\n- Explaining visualization system textually is unintuitive. It would be better to convert this paper to a system demonstration paper.",
            "summary_of_the_review": "Predicting a function of adverbial postposition is interesting topic, however the method used in this paper is too naïve and the evaluation settings are incorrectly set.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}