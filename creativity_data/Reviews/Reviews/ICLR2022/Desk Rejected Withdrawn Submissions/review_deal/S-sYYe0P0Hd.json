{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper suggests improving the generalization capabilities for unseen domains of contrastive learning methods for speech data. The presented method, SynCLR, is based on a speech synthesis model with a multi-view setup to replace current fixed data augmentation techniques. The authors demonstrated empirically that the proposed method is superior to the baseline methods when considering speaker verification and speech-to-image translation. Finally, the authors conducted an ablation study evaluating different neural vocoders, views. \n",
            "main_review": "Overall, the idea of the proposed method is improving representation learning using samples obtained from a generative model. Although is an interesting research direction, this paper has major flaws. \n\nEvaluation: \n(1) the authors did not evaluate any speech representation learning method, such as [1-5]. The authors only evaluated vision-based models. As the claimed contribution of this paper is justifying using generative models for speech representation learning, I would expect to see improvement over speech-based models. \n\n(2) the authors suggested using multi-view speech synthesis. If I understand it correctly, the views are content, speaker ID, and prosodic features (I assume it is only F0), which is the standard speech synthesis pipeline. In that case, I'm not sure what is the benefit of doing speaker / F0 / content augmentations in the task of speech-to-image translation. Can the authors provide some motivation for that? Specifically, when considering Figure 4, what is the reason SynCLR helped the model generate fine-grained details in the image? it seems invariant to the F0 / speaker ID. \n\n(3) In the ablation study, Table 3. The authors experimented using subsets of views. However, if I understand it correctly, the ablation was done over SimCLR and not SynCLR, can the authors clarify?\n\n(4) Since the authors claimed to improve speaker verification performance for speech representation learning, I would expect the authors to compare to some of the models in [8], or at least pay attention to it. \n\nNovelty & Contribution: \nOverall, the novelty and contribution seem limited, as the authors used already known components, (for some reason only vision models) and did not evaluate any speech-based models.\n\nAdditionally, the authors suggested using SynGrad as the speech synthesizer. However, it is unclear what is the difference between SynGrad, to previous diffusion-based vocoders. Can the authors clarify what is the difference? Otherwise, this is a little bit misleading.\n\nRelated work:\nUsing speech synthesis models to improve downstream tasks as well as representation learning is not new, for example, see [6-7]. The authors should cite this line of work as well. \n\nCorrectness: \nIn section 3.1 the authors wrote: \"Existing methods are dominated by the generative adversarial networks (GANs), which, however, has been criticized for mode collapse and limited sample diversity (Dhariwal & Nichol, 2021; Creswell et al., 2018).\" \nFirst, GANs are not the dominant method for neural vocoders, recently they show great progress, however, auto-regressive models such as WaveNet, WaveRNN, or Flow-based models such as WaveGLOW are still popular and highly competitive to GAN based models.\nSecond, the citations are referring to vision-based models. \nGenerally speaking, when concerning citations, I would expect the authors to cite more speech-related papers other than vision ones, especially when the claimed contribution is in the speech domain. \n\nI think there is a mistake in equation (1). In the denominator shouldn't the authors use \\tilde{c}_0  everywhere? \n\nWriting: \nThe whole presentation of the method is a bit messy. It is not clear what is new and what the authors used from previous studies. This makes it hard to understand the contribution and novelty of the paper.\n- Speeches -> A better term would be speech recordings / utterances / samples.\n\n\n[1] wav2vec: Unsupervised Pre-training for Speech Recognition\n[2] wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations\n[3] HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\n[4] AUDIO ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF AUDIO REPRESENTATION\n[5] Data augmenting contrastive learning of speech representations in the time domain\n[6] INJECTING TEXT IN SELF-SUPERVISED SPEECH PRETRAINING\n[7] Improving Speech Recognition using GAN-based Speech Synthesis and Contrastive Unspoken Text Selection \n[8] SUPERB: Speech processing Universal PERformance Benchmark",
            "summary_of_the_review": "Due to all of the above, the current manuscript is not ready for publication at ICLR conference. I encourage the authors to improve the paper based on previous comments and resubmit. \nI'm willing to change my score in case I missed something or misunderstand the paper.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes to train self-supervised models on synthesized speech. The key idea is to diversify the views for contrastive learning through a controllable TTS system. The experiments are done on speaker verification and speech-to-image translation task.",
            "main_review": "The idea is interesting, but it is also limiting given the current state of text-to-speech synthesis. The community attempts to do disentanglement of speech, but fundamentally this is an ill-posed problem and there is no clear definition of disentanglement. There is a significant advance in recent years, but TTS systems are far from producing the subtle variability of speech.\n\nWe could limit the discussion to available data augmentation techniques and discuss whether this approach is better. The two techniques are complementary in my opinion -- TTS systems are good at modifying the prosody and style, while data augmentation typically consists of changing the speed and adding noise.\n\nThe experimental results are a little worrying. A basic i-vector baseline should be within 10% EER on VoxCeleb2. I'm not sure why the results in Table 1 are this bad. I also don't think speech-to-image translation is a good task to evaluate the quality of speech representation. We don't know why certain speech end up leading to images with more details. The FID score (modulo that it itself is a problematic metric) is intended for measuring the realisticness of an image. I don't see how the quality of speech representation is related to the realisticness of an image.\n\nThe presentation has a lot of room for improvement. For example, the paper claims that SynGrad is a main contribution, but the description is sparse in the main text and how SynGrad helps contrastive learning is largely missing. The paper is also not self-contained. The paradigm of self-supervised learning is missing. There is too much emphasis on the TTS system. There is even MOS evaluation in section 4, which is not really relevant to contrastive learning. I have a feeling that this could have been written as a TTS paper.\n",
            "summary_of_the_review": "The idea is interesting, but the limitation is also clear. The experimental design is a little odd, and the results are not convincing. The presentation would benefit from a large amount of rewrite.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this work, the authors build upon the recently proposed SimCLR framework by studying techniques to learn improved representations for speech data -- particularly out of domain speech. The paper proposes two ideas: first, to use a “view-controllable” TTS system which can create input speech which varies from the reference along one or more dimensions (e.g., same text and prosody, but different speaker). The actual TTS is performed using new generative model SynGrad which is aimed at improving GAN based synthesis by avoiding mode collapse. In experimental results on a speaker verification task, and a speech-to-image translation task, the proposed methods appear to outperform baselines, particularly for “out-of-domain” speech.\n\n",
            "main_review": "\nThe problem of learning improved representations for speech -- particularly out-of-domain speech -- is a very interesting and challenging problem that is becoming increasingly well-studied in the community. While the idea of using a TTS system to create “fine-grained” differences when creating augmented samples is an interesting idea, I have a number of concerns with the present work, most of which are related to the presentation of this work.\n\n1. Overall, the presentation of the work makes it hard to understand a number of details. For example:\nThe notation is inconsistent and not always introduced when it’s explained. For example, in Section 2, the projected output is denoted as c; in Section 3.2 it’s denoted as v.\nSome of the equations appear to have errors. For example, unless I’m missing something. For example, in equation 1, the info NCE uses the similarity between the encoded representation and the projected representation of the augmented version as the positive pair, but then uses the same quantity computed on other examples as the negative pair. Shouldn’t the negative pair be z_0 and \\tilde{c_i^v}, not z_i and \\tilde{c_i^v} as is done in SimCLR or equation 2? \nSince speech samples have different lengths, it wasn’t clear to me how the authors were accounting for the variable lengths in the input speech, since the encoded representation always appears to be in \\R^d. Could the authors clarify?\nIt was confusing to understand exactly what views (prosody, speaker, etc.) were used in the study. Also, I wasn’t clear on how these are represented when they are fed in to the model. Please clarify in the text.\nEquation 5: \\tilde{z}_i appear in the equation, but should these just be z_i? If not, please specify what \\tilde{z} is in this context. Also, a clarification question: the “negative pairs” in the denominator involve z_i and z_j^(v) -- i.e., the projection of the i-th condition and the j-th condition with v replaced with something else (potentially c_i,v) Is this intentional or did the authors mean to use z_j?\nI think the description in Figure 1, would be much easier to understand if the authors clearly indicated what the views used in this work are, and also provided examples of positive and negative utterances. This figure should also be explained and discussed in more details in the work.\n\n2. I found it very hard to understand the experimental setup for this work. Appendix E which describes the speaker verification task is particularly unclear. For example, the authors mention “The hidden sizes of the self-attention … which will grow if needed.” What does it mean to grown if needed? The section also mentions LVC/LVCNet and NP without describing or providing citations. Also, in Appendix E Page 15: “SynCLR” --> “SynGrad”. \nSimilarly, in Section 4.2, it was not clear what the baselines being compared are since there are no citations/explanations. I think it would be helpful to clearly indicate what the differences between SimCLR and SynCLR (and the other baselines) are.\n\n3. There are a number of typographical and grammatical errors in this work which affect readability. For example:\nIntroduction: “... in the dominated GAN-based generative models ...”\nThe paper mentions improvements in FID in the abstract/introduction without defining or citing these. Similarly, figure 2 mentions a LR (length regulator) but doesn’t define this.\nSection 3: “... we propose a multi-view data synthesis strategy for speech representation learning unseen domain.”\nPage 4: “... in FastSpeech 2, we employ duration, pitch, and predictors, and embed … ”. Energy predictors?\nEquation 5: \\tilde{z}_i appear in the equation, but should these just be z_i? If not, please specify what \\tilde{z} is in this context.\nMinor Comment: In Section 3.2: h_v : H -> Z_v\nTable 3: What are “Con.” “Pro.” and “Spe.”? \n",
            "summary_of_the_review": "Overall, the presentation of the paper and the missing details make it hard to understand the main ideas. While I think the paper has some interesting ideas, it would need significant revisions in order to be accepted.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a speech representation technique relying on contrastive learning and on view-controllable synthetic speech samples. To do that, a new view-sensitive contrastive loss is introduced, and a fast high-quality speech vocoder is derived from denoising networks research. The work is relevant in the contrastive learning and speech representation learning fields. These representations are then tested on two downstream tasks: Speaker Verification and Speech-to-Image translation.",
            "main_review": "**Strengths:**\n- Relying on controllable synthesized samples for contrastive learning is an interesting idea, even though it is not completely unsupervised. It is amazing to see good results on the downstream tasks, especially for generalization, as the encoders have only learned on synthetic data.\n- The vocoder introduced “SynGrad” succeeds in generating high-quality samples in a very short time, which is one of the main strong points of the paper presented. \n- The authors succeeded in transposing what was first a denoising module to speech sample generation. The results on speech-to-image translation confirm the quality of the speech representations, especially concerning their textual content.\n\n**Weaknesses:**\nFirst, and contrary to what is claimed in the paper, we cannot consider this technique as self-supervised. As the speech synthesis model relies on speaker and textual information to learn, this method belongs to the supervised ( on LJ and LibriTTS labeled datasets ) speech\nrepresentation learning field. However, the main baseline compared to this work (SimCLR) is fully unsupervised. For instance, one of the main contributions which is the multi-view-based contrastive loss cannot be computed without information about the speaker in the synthesized speech samples. This piece of information comes directly from the supervision training of the synthesizer. This does not invalidate the gains obtained with the technique, but it should be stated clearly, and the technique should not be labeled as self-supervised or unsupervised.\n\nSecond, while introducing a lot of interesting ideas, the paper is poorly structured and spends a lot of space explaining other works instead of developing crucial materials. Examples: \n\n- The whole first paragraph of the sixth page defines a “self-supervised speaker verification task”. \n- Instead of explaining a bit more \"A majority of representation learning [...] task at hand.” Here, we understand that the representations used for Speaker Verification are the ones directly output in the speaker space ( the “task-dependent view” ) but the contrary is said afterward in the paper “In SynCLR, we generated multiple views and projected samples to content and prosody embedding spaces”. \n- Finally, what does “training SynCLR from scratch” mean in the first case, does it mean using Voxceleb2 data directly into the training?\n\nThird, it is hard to understand the evaluation process of the Speaker Verification task. The EER results shown are very low compared to the literature, and the evaluation pipeline do not seem to follow any SV benchmark ( you can see for that the SUPERB benchmark or\nthe third track of the The VoxCeleb Speaker Recognition Challenge 2021 (VoxSRC-21) which details the self-supervised methods evaluation.) As the code is not provided for replication or for further understanding, which is by itself another weakness,  we can not get into the details. “In SynCLR, we generated multiple views and projected samples to content and prosody embedding spaces.” It also seems unclear reading this sentence why the speaker embedding space has not been used for the speaker verification. Is this just the result of\nempirical testing or just a typo ?\n\nFourth, the literature review lacks a lot of relevant work.  From other attempts to build on SimCLR for speech and audio data ( https://arxiv.org/abs/2010.13991 ) (https://arxiv.org/abs/2010.10915), to the main used models in self-supervised speech\nrepresentations which rely on CPC learning, and thus have a contrastive part. It is also unclear why did the authors only compare themselves to other contrastive learning methods and not expand it to at least one non contrastive technique. This is particularly useful as the second downstream task does not appear in the self-supervised representations benchmarks, while the evaluation process for the first one is unclear and obviously not corresponding to common benchmarks.\n\nFinally, motivations behind the downstream tasks selection are not given in the paper. For instance, speaker and prosody information seem completely irrelevant for the second downstream task, which seems to rely solely on the textual content of the query. While the speech-to-task is a challenging and interesting task, it seems more natural to test the phonetic and textual content of speech representations on speech recognition.",
            "summary_of_the_review": "This paper presents novels and interesting ideas, but the experimental protocol lacks robustness and the paper does a bad job at presenting correctly the novelties and results. The major issues may be fixable during the discussion period. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}