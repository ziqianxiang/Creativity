{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper considers learning simultaneously from multiple tasks and aims at proposing an effective scheduler to adaptively select the tasks to draw samples from for the next training stage thus  maximizing the benefits of multitask learning. The proposed scheduler is based on RL, where a Q-learning scheduler tracks, measures the uncertainty of tasks, generates the task uncertainty histogram, and then uses Q learning to sample/schedule tasks. The overall task uncertainty and validation loss are utilized as the signal from the scheduler. The authors conduct experiments in different settings to examine the effectiveness of the proposed scheduler compared to other schedulers such as uniform, hand-crafted, random, greedy etc. \n",
            "main_review": "While the paper studies an interesting and important question, the proposal suffers from a few issues that prevent me from giving it a high-score. First,  the proposed solution only takes into account the uncertainty of different tasks as a supervision signal to learn the scheduler and ignores the task-relatedness to characterize task difficulty from the perspective of the outcome and the process of learning,, e.g., via the similarity between gradients of the model with respect to the validation set of each task. This is key in multi-task learning and it is not clear how it could perform when the task difficulty varies or the distribution of samples or labels is widely uneven. Moreover,  the proposed idea looks incremental. I understanding utilizing RL and generating uncertainty histograms might be considered novel, but putting the proposal in context of existing methods (e.g., via bi-level optimization etc) it lacks novelty. \n",
            "summary_of_the_review": "This paper proposed a Q learning based scheduler for multi-task learning to adaptively select tasks in the training process based on task uncertainties. The proposed method looks incremental and does not incorporate tasks un-relatedness in training; thus I am leaning toward rejection.\n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a scheduler for multi-task learning. The scheduler dynamically adjusts the learning of the multiple tasks based on a measure of the task uncertainty. The scheduler uses a Q-learning Reinforcement Learning approach. There is demonstration of the solution, called QLS, for three different scenarios, multi-domain setting, simple multi-task setting, and complex multi-task setting. The comparison against a large number of baselines brings out the superiority of QLS.  \n",
            "main_review": "(+) The paper presents a novel formulation and solution.\n\n(+) The design of QLS is well executed. The experimental strategy is sound. \n\n(+) There is extensive evaluation for a comprehensive set of scenarios and against a comprehensive number of baselines. The experiments are mostly at a macro level but they are well complemented by some detailed experiments (like quality of features learned). \n\n(+) There are generalizable insights from the evaluation --- QLS extracts useful and generalizable features, QLS can adapt the learning based on the performance in a few of the current epochs.\n\nThere are no deal-breaking flaws that I found in the paper. Here are some suggestions for improvement:\n(*) What is the implication of making only a single layer (the output layer) to be task specific? What happens if a few more layer close to the final layer are also made task specific?\n\n(*) At the beginning of 4.2, a slew of methods is mentioned for how to balance the various tasks in an MTL (different ways of model uncertainty). Then a specific way is chosen. Why? A rationale here would help the reader.\n\n(*) A supplementary result on the sensitivity to the n_b parameter, the number of uncertainty histogram bins, would be useful.\n\n(*) It is mentioned \"In the experiments, we run our system three times and report the average.\". Is this done for the stochastic baselines too? They may also have an unlucky, or lucky, initialization. \n",
            "summary_of_the_review": "The paper makes a distinct contribution to multi-task learning. It is well designed, well implemented, and well evaluated. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a Q-learning based task scheduler for multi-task learning to achieve effective learning. For this, it uses the histogram of task uncertainty.",
            "main_review": "1. The claim in the paper is ambitious but the proposed approach is still heuristic as in the existing approaches.\n\n2. It seems that Q-learning is simply applied to consider the criteria from the existing approaches together. Accordingly, it seems that the contribution of the paper is marginal.\n\n3. The use of the histogram of task uncertainty should be theoretically justified.\n\n4. In the experimental results, the improvement of the proposed approach compared with the existing approaches is too marginal.",
            "summary_of_the_review": "The proposed approach is still heuristic as in the existing approaches and has no theoretical justification. It seems that the proposed approach simply considers the concepts from the existing approaches together via Q-learning without any significant idea. Besides, the improvement of the proposed approach is too marginal.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes QLS to schedule tasks for multi-task learning. QLS monitors the state of the tasks to measure the task uncertainty, generates the task uncertainty histogram, and then uses Q learning to schedule tasks. The paper evaluates the proposed QLS scheduler across many text classification tasks and datasets. QLS improves accuracy across many datasets and tasks.",
            "main_review": "This paper makes marginal contribution and improvement over the current state-of-the-art. It does provide a rigorous evaluation, as claimed, across many datasets/tasks and compares against many benchmarks. Overall, the paper is clearly written and uses the standard techniques to solve an existing problem. \n\n* I am not sure how the work advances the field of scheduling MTL tasks or what are the learning lessons for others in the field.\n\n* What are the new contributions of the work, and why is the proposed method does only marginally better or close to other techniques? Are the improvements significant?\n\n* What are the tradeoffs? Does the proposed scheduling lead to more or less resource overhead?\n\n* The results only marginally improve over the previous state-of-the-art. I am not sure if the results are going to be consistent across different seeds/hyper-parameters. ",
            "summary_of_the_review": "I appreciate the fact that authors have compared their work against several baselines. However, the results only marginally improve and there is no way to assess the quality of the result without confidence intervals. \n\nI would appreciate if the authors address the comments from previous submissions before submitting it to the next conference. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}