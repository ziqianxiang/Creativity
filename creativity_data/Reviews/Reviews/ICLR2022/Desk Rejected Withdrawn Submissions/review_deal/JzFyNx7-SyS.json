{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes an explanation method for graph neural networks on node and graph classification tasks. The method takes inspiration from information-theoretic measures of causal influence to split the input features into two parts that reconstruct the adjacency matrix from the node features. One of the parts is trained to model that aspect of the adjacency matrix which has great influence on the output prediction given it is intervened on. The results in terms of faithfulness, sparsity and interpretability suggest that the proposed method works favourably in comparison to related methods.",
            "main_review": "### Strengths\n\nThe paper introduces the problem clearly and sets up the foundations for the method. A benefit of the method is that it does not require any changes to the target network, and only relies on taking gradients. The method has been extensively evaluated and shows favourable performance compared to previous works. \n\n### Weaknesses\nThe main weakness of the paper is that there are several issues with Theorem 2.1 and its derivation of an estimator. Specifically:\n- Appendix A.1 does not show a proof for the theorem, but instead just plugs in the estimators that are discussed after that. It remains unclear how this information flow equation based on $Z_c$ has been derived.\n- To estimate $P(y|do(Z_c))$, a Monte-Carlo sampler is used. However, this does not lead to an unbiased estimate because $P(y|do(Z_c))$ is used within logarithms, divisions, and multiplications. For instance, the Jensen inequality states that $\\log \\mathbb{E}[...]\\geq \\mathbb{E} [\\log ...]$. Thus, plugging in the estimator for the terms within the logarithm does not lead to an unbiased estimator, making the equation at the end of page 3 invalid. Furthermore, for the multiplication and division cases, it needs to be taken into account that $\\mathbb{E}[X]\\mathbb{E}[X] \\neq \\mathbb{E}[X^2]$ (simple example: a Gaussian with zero mean), which means that different, independent samples need to be used to estimate all $P(y|do(Z_c))$. This is not the case in the last Equation on page 4 where the same indices are used everywhere.\n- The step in Equation 2 is unclear of how the sampling process is derived. It is clear that $P(y|do(Z\\_c))=\\sum_x P(X)P(y|Z\\_c,X)=\\mathbb{E}\\_{X}[P(y|Z\\_c,X)]$, and that this expectation can be approximated with samples from $X$ (if it is not in a log etc. as mentioned above). But sampling $A$ and $Z_s$, as described in the text, becomes a circular sampling step as $A$ depends on $Z_s$, and $Z_s$ on $A$ according to the text. $q(Z\\_s|A,X^{(k)})$ is likely meant to be $q(Z\\_s|X^{(k)})$ which would lead to a valid sampling step, but an explanation is missing.\nIn summary, in the current form, the described objective in the last Equation of page 4 is not a valid equation since $I(Z\\_c\\to y)\\neq...$ due to the biased estimators. \n\nFurther, the proposed method has the following drawbacks:\n- A hyperparameter search/evaluation over $N\\_x, N\\_z, N\\_c, N\\_s$ seems to be missing in the appendix. This would have been important, also in light with the previous doubts on the equation. Further, these numbers sound very large since for each $X$, $N\\_s$ samples of $Z\\_s$ are drawn, and for each of that, $N\\_z$ samples for $A$ are taken. This multiplies up to 1000 samples. How are the samples for $Z\\_c$ combined into that? This sampling process seems to run out of hands very quickly.\n- The method has 3 additional hyperparameters, $\\lambda\\_1, \\lambda\\_2, \\lambda\\_3$, which have to balance the four loss terms. This introduces considerable hyperparameter dependencies making the method less straight-forward to apply, especially when we don't have any ground truth of model explanations.\n- The naming of the two factors, 'causal' and 'spurious', is not really fitting considering that both parts 'cause' the adjacency matrix in the diagram of Figure 2. The difference between the two parts is that the 'causal' factors are trained to have a higher impact on $y$ and consequently $A$, while the 'spurious' features can model additional parts of $A$. Looking at Figure 9a, all the spurious factors together have a higher information measurement than the highest 'causal' factor. Thus, the 'causal' factors have the highest information on average, but are not causal alone.\n- The last paragraph of section 2 does not clarify how the gradients are backpropagated through the adjacency matrix for the KL term. Is the adjacency matrix sampled, or are soft values used? If the adjacency matrix is sampled, which estimator is used? Many GNN implementations use edge lists for efficiency, which would not allow to differentiate with respect to the adjacency matrix.\n\n### Additional comments\n- Page 4, paragraph 2: the three equations regarding distributions in the text are missing sums, e.g. $P(y|Z_c)=\\sum_X P(y|Z_c,X)P(X,Z_c)$ since $P(y|Z_c)\\neq P(y|Z_c,X)P(X,Z_c)=P(y,X|Z_c)$\n- Page 4, above Eqn 2: \"The term $\\sum_X P(y|Z_c,X)$ ...\" is missing $P(X)$ - \"The term $\\sum_X P(y|Z_c,X)P(X)$ ...\"\n- Equation 4 and 5: make clear over what the objective is minimized - probably the parameters of the VAE\n- The citation style (numbers) is not in line with the ICLR format (author + year)\n- Figure 1 - It is not clear why the causal explanation removes nodes as well (the yellow and light blue) since the full reconstructed graph is conditioned on the same node features as the input.",
            "summary_of_the_review": "The main issue of the paper lies in Theorem 2.1 and its derivation of the estimator. Thus, my current recommendation has to be \"Reject\". However, the good results indicate that there is some truth behind this theorem and benefit of the method, and hence I encourage the authors to take another look at the estimator.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper propose an explanation framework for GNNs which aims to find the explanatory sub-graph structures. It employs a GVAE architecture and learns to isolate the latent variables into the causal & spurious variables via maximizing the information flow with backdoor adjustment.",
            "main_review": "Strengths:\n+ Learn to isolate the causal factor in the latent space is quite interesting\n+ Obvious improvement when compared with the previous works\n+ Clear presentation and comprehensive experiments\n\nWeaknesses:\n-\tThe authors assumes that only the node attributes X would affect the causal factors and no path in the inverse direction. However, for some GCN variants, say Geniepath[1] or Gated GNN[2], the message (i.e., node features) and the passing routes (sub-graph structure) are learnt in a coupled way, say LSTM, would this break the causal graph assumption in the proposed framework?\n-\tThe authors claims the proposed framework is model-agnostic, but it is only evaluated under different GCN architectures (figure 5, the node and graph classification models employ similar GCN blocks). I’m curious how would it performs on different GNN blocks, say GAT, GraphSAGE, GIN, Geniepath, and etc (Especially the ones employ LSTM-like aggregation).\n-\tThe isolation of the causal & spurious factors only relies on maximization the information flow, maybe disentangled representation in the latent space[3] can also help.\n\n[1] Liu, Ziqi, et al. \"Geniepath: Graph neural networks with adaptive receptive paths.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. No. 01. 2019.\n[2] Li, Yujia, et al. \"Gated graph sequence neural networks.\" arXiv preprint arXiv:1511.05493 (2015).\n[3] Mathieu, Emile, et al. \"Disentangling disentanglement in variational autoencoders.\" International Conference on Machine Learning. PMLR, 2019.\n",
            "summary_of_the_review": "In general I like the idea of isolating the causal factor in the latent space, it is reasonable novel and the experiment results suggests its performance is quite good. My main concern is just its application ability to various GCN variants.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposed to an auto-encoder that encodes a graph and its node features $X$ to produce latent embedding $Z$, and split $Z$ to so called casual components $Z_c$ and spurious components $Z_s$. $Z_c$ feeds to an decoder that produces a so called causal graph, and $Z$ (containing both $Z_c$ and $Z_s$) feeds to the same decoder that produces the 2nd graph. Both graphs with the original node features $X$ feed to the original trained GCN to produce potentially two different $Y$. \n\nThe parameters of the auto-encoder would effect the $Z$, decoded graphs, and consequently the predicted $Y$. If the casual component $Z_c$ is learnt well, the predicted $Y$ should agree with the original GCN's prediction. The parameters of the auto-encoder are learnt by minimising the objective function in eq(5), which has the following components:\n1) maximising the (causal) mutual information flow from $Z_c$ to $Y$;\n2) minimising the ELBO;\n3) promoting the sparsity of $A_c$ (i.e. the decoded graph from $Z_c$ is encouraged to be sparse);\n4) minimising the KL divergence of the distribution over decoded graph from $Z_c$ and the distribution over the original graph.  \n    \n\nuse a generative graph neural network to discover the causal relations in the decision progress of a graph neural network. The main technique in the paper is to isolate the causal factors in the latent space of graphs by maximizing the information flow measurements. The experiments show that the proposed method outperforms other methods.\n",
            "main_review": "The mutual information flow from $Z_c$ to $y$, involves calculating or estimating $P(y|do(Z_c)) = \\sum_{X} P(y|Z,X)P(X)$ if $X$ follows backdoor criterion. \n \nThe authors propose to fix $Z_c$, and sample graphs and $X$ from the training set, and for per training graph and $X$, sample many $Z_s$ and the decoded graphs, to approximate $\\sum_{X} P(y|Z,X)P(X)$ empirically.\n\nThough how to get $N_c$ and learn which dimensions of $Z$ should be $Z_c$ is not unclearly explained in the paper, I think one can simply specify a desirable $N_c$ and dictate that the first $N_c$ dimensions of $Z$ to be $Z_c$, and then let the auto-encoder to learn its parameters according to Eq(5), which can generate $Z_c$ that serves the purpose. \n\nThe idea is interesting, and technique is sound. \n\nMinor issues: in the paragraph just before theorem 2.1 $sum_X$s are missing twice. They should be  $P(y|Z_c) = \\sum_{X} P(y|Z,X)P(X|Z_c)$ and  $P(y|do(Z_c)) = \\sum_{X} P(y|Z,X)P(X)$. \n\nIssues:\n1) The proposed method doesn't discover causality from the data, but from a trained model. The trained model itself may not capture causality well (GCN is not designed to capture causality), which questions the usefulness of the proposed method. \n2) In Figure 5, it has been shown that in the experiment, only GCN is considered as target network architectures, which is far from enough. GCN is only a specific graph neural network, and its performance is limited by its fixed graph-laplacian structure. Besides GCN, there are a more general class named message passing neural networks [1], which can be viewed as a generalization of GCN. Furthermore, in the node classification task, one common trick to improve the performance is to do a max/mean-pooling, and append the global feature to each node. Maybe the authors should also consider to explain a network with such tricks applied.\n[1] Gilmer, Justin, et al. \"Neural message passing for quantum chemistry.\" International conference on machine learning. PMLR, 2017.\n1) The proposed method can be computational expense, which is perhaps why the experiments are done with very small graphs. \n\n",
            "summary_of_the_review": "The paper's idea is good, and technique is sound. The experiment is weak. But overall, it can still add value to the community.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a graph explanation framework to predict subgraph masks by treating outputs of Graph Neural Networks (GNN) as causal effects of unknown latent variables. A generative model based on Graph VAE is defined to infer over these latent variables. This generative model infers both the causal latent variables as well as the remaining \"spurious\" variables. The underlying motivation is that only the causal latent variables are responsible for the outputs and hence a faithful explanation could be derived from causal factors. The authors propose a causal graph for the data generative process and according to this graph, the data $X$ is confounder. In order to adjust for this confounder, backdoor adjustment is used and a causal counterpart of mutual information between the causal latent variables and output is used to train the explanation model. Experiments are done on synthetic and real datasets illustrating the effectiveness of the proposed causal framework for generating explanations.",
            "main_review": "Providing a causal explanation to graph neural networks predictions is a very interesting approach and I feel the motivation for having a causal approach to explanation framework is well founded. The proposed method is based on this motivation and can work alongside any GNN model with access to its gradients. Since it requires access to GNN gradients, it cannot be claimed completely as a “black-box” approach and this claim needs to be discussed with greater care in the paper (for example, the paper claims “we consider the black-box setting...we specifically do not require access to, nor knowledge of, the process by which the target GNN produces its output” in Page 3 problem setting).\n\nOne of the fundamental tasks in DAG/ Structural Causal Models (SCM) based causal modeling is the appropriate justification of the causal graph used for modelling as it encodes all the assumptions about the data-generative process. In this regard, the proposed causal graph seems to be vague at best and there is no justification of why this graph is a reasonable one. Why is it reasonable to assume there are no (latent) instrumental variables, (latent) adjustment variables or latent confounders apart from $\\mathbf{X}$? \n\nTangential to the above point is the fact that the causal interpretation of the proposed causal graph seems inconsistent. From the proposed causal graph, should causal factors and spurious factors be regarded as causal effects  of node attributes $\\mathbf{X}$? Technically, should it not be in the opposite direction? In that sense, $\\mathbf{X}$ would not necessarily be a confounder in this setting and the provided analysis would be incompatible. \n\nThe exposition is severely lacking in quality and needs to be significantly improved. Equation 2, 3 and the one below 3 have multiple indices and are not represented with clarity. For example, in the equation $\\mathbf{A}^{(n)}\\sim p(\\mathbf{A}|\\mathbf{Z_c},\\mathbf{Z_s}^{(j)})$, are different $\\mathbf{A}^{(n)}$’s generated for each $j$? If so, then its better to indicate it as $\\mathbf{A}^{(n)}_j$. Also, why is $\\mathbf{Z_c}$ fixed and $\\mathbf{Z_s}$ over multiple indices? Whats the motivation to combine a single $\\mathbf{Z_c}$ with multiple $\\mathbf{Z_s}$? \nA nuanced way to present this is to first present the graphical model corresponding to the data generative process and then discuss the equations before proceeding to Monte Carlo approximations.  There are many such indexing issues in Equation 2, 3 and the one below 3 that it is hard for me to understand the technical details of the algorithm. \nIn addition, I would like to clarify, for a single input, you have a distribution over $\\mathbf{Z_s}$ and $\\mathbf{Z_c}$, from which you sample and then feed to the decoder to give the adjacency matrix $\\mathbf{A}$. So why does this $\\mathbf{A}$ contain a different index as compared to the input instance $X^{(k)}$? Also, in the paragraph below equation 1, the backdoor criterion is written incorrect at multiple locations. The variable $\\mathbf{x}$ needs to be marginalised out.\n\nThe generative network is an important component of the proposed approach as it generates the explanations and needs to be discussed. It is not clear whether there are two MLPs in the generative network, one for reconstruction and other for submask generation, or if it is a single MLP.  \n\nThe authors introduce a causal version of mutual information taking into account the confounders as per the proposed causal graph. I would like to clarify how this would in general be superior to just maximising the mutual information between the “causal” factors and $y$, i.e. how much does the notion of intervention and causal modeling perform as compared to a non-causal treatment in the exact same setup? \n\nThe empirical results on both synthetic and real data seem to look impressive by just comparisons, but it's hard to come to a conclusion given the lack of clarity in the description of the proposed algorithm. \n\n",
            "summary_of_the_review": "Generating graph explanations in a causal framework is an interesting and potentially promising direction. However, the current paper significantly lacks clarity both in terms of exposition and justification of the proposed causal graph. In this regard, I feel its not ready for publication yet and would benefit from another round of review.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}