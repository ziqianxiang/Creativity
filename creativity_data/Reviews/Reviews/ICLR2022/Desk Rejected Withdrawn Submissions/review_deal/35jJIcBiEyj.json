{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a method to mitigate bias by using meta-learning; The main focus of the work is to try to divide the dataset samples into a disjoint set consisting of \"biased\" and \"unbiased\" samples using a pseudo-labeling algorithm (The paper uses the hypothesis that biased samples are learned faster than unbiased samples). Then, a two-fold approach is proposed where, in the inner loop, the algorithm tries to find the best parameters for training both biased and unbiased sets and the outer loop tries to synthesize new samples by using mixup to regularize the model by \"breaking the shortcuts present in the dataset\". The experiments show moderate gains over comparable models, i.e., a class of models that do not assume knowledge of the factors behind the spurious correlations. ",
            "main_review": "The paper proposes a fairly reasonable combination of techniques, written very clearly, and shows good results. However, the paper suffers from some weak assumptions, lacks an original central idea or hypothesis, and misses some key references. While the experiments are adequate, I find there are several ways they could be further improved. Details below:\n\nStrengths:\n\nS1. Good results: The results are good on all the datasets tested. While I have qualms about the color-MNIST and toy datasets (waterbirds/landbirds) dataset's ability to really test bias mitigation (more on W2 below), this is not something unique to the paper and shared by the whole community. The comparisons are fairly done and the results show good promise. \n\nS2. Reasonable proposed method: The proposed model is reasonable. Both of the central ideas are demonstrated to be independently helpful (ie., biased samples are learned earlier and mixup). The proposed model combines them in a neat meta-learning framework. However, I find the whole algorithm as a whole not really coherent enough to have a central hypothesis + experiment setup that I find desirable. (More on W1 below) \nS3. Well written and technically correct: The paper is well-written, the steps are clearly explained and I have little doubts about its technical correctness. The paper should also be easily reproducible by an expert in the field given the details in the paper.\n\n\nWeaknesses:\n\nW1. Some weak assumptions and unclear motivations: The paper states that \"as long as the biased training samples can be learnt [sic] faster than the unbiased ones.\", the proposed model should work well. Why not directly study this? This is obviously not possible with \"real\" datasets but all of the datasets used in the experiments are fully labeled with corresponding spurious factors. I understand that the algorithm works without access to those, but they could still be used to validate the assumptions of the model. How reliant is this assumption? How much does the final result depend on the bias classifier being accurate? Next, the components used are individually shown to be useful (by others) but it is unclear what role they play in the meta-learning framework. It is hard to state concisely what exactly the paper is proposing as a debiasing mechanism when taken as a whole rather than as a combination of techniques that are shown to work well. For example, the paper states that the mixup is used to provide samples for meta-validation \"with data that can be as much “neutral” as possible by mixing samples of the biased split with unbiased split\", but also \"produce synthetic samples which are unusual, and therefore represents cases that are under-represented in the original training data\". These two seem mutually incompatible if not exclusive. \n\nW2. Experimental validation is adequate but could be improved: \n\n\nThis is related to W1; the gamma ablation is not satisfactory. Why only 0.8-0.95? How do these gamma values relate to its ability to \"truly\" divide the dataset into biased and unbiased? What if they were not as accurate. In the same vein, a \"random\" split baseline is missing (randomly assign gamma percentage to \"biased group\". Next, I am increasingly wary of using and reusing the same toy datasets for bias mitigation works. While this is shared with the whole community rather than just this paper, I would encourage the authors to take a more critical view of bias-mitigation and attempt to mitigate bias in harder settings (https://arxiv.org/abs/2104.00170) and in light of possible pitfalls of using simpler assumptions about bias (https://arxiv.org/abs/2006.07710). Overall, this makes for an almost adequate but not compelling experimental section, which is especially necessary for a paper in such a crowded area doing experiments mostly on smaller scale toy datasets.\n\nW3. Some key references missing: The whole discussion around IRM (https://arxiv.org/abs/1907.02893) is missing which spans multiple papers and is very related to the idea of optimizing different environments. This would fall under what the authors call \"supervised\" bias mitigation but a grounded discussion with respect to IRM is essential in my opinion. Second, the paper also fails to compare against the Spectral decoupling algorithm which also does not assume access to spurious factors (https://arxiv.org/abs/2011.09468)\n\n",
            "summary_of_the_review": "Overall, I think this is an interesting approach but, to this reviewer, falls short of a high bar for ICLR. It is unclear what new question the paper tries to answer (W1), and the experiments do not fully satisfy this reviewer (W2). It is also missing some key references, in my opinion. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a two-stage algorithm to tackle spurious correlations learned by machine learning models. Training samples are separated into a biased subset and an unbiased subset in an unsupervised way. And then, update the model by data with pseudo-labels assigned by the separation. The main novelty is using a meta-learning approach to update the model with three parts of data: biased data, unbiased data, and mixed up data. In the inner-loop optimization, the authors train the model on biased data and unbiased data with different weights. In the outer-loop optimization, the authors update the model with the same objective of the inner-loop step and the performance of mixed-up data as a validation evaluation.  The authors conduct experiments on Colored MNIST and Corrupted CIFAR as well as on real-world datasets and outperform several state-of-the-art models.",
            "main_review": "Strengths:\n1. This paper proposes a simple way to train a debiased neural network.\n2. MixUp is an effective tool to denoise pseudo-labels and to fuse biased data and unbiased data.\n3. The authors conduct well-controlled experiments and analyze the results detailedly.\n\nWeaknesses:\n1. The motivation for using bi-level optimization is unclear. Why not update the model with biased data, unbiased data, and mixed-up data with different weights? Differences between direct optimization and bi-level optimization are missing.\n2. Mixing samples are not \"neutral\" but unbiased data are more \"neutral\".\n3. It is confusing that the rebalanced weight of the biased split and the unbiased split has the same value as the hyperparameter gamma, ie, target training accuracy. \n\n\nMinor issues:\n1. The citation of Mixup in the introduction is misdirected.\n2. The decimal point of Sagawa et al's result in Table 1 is wrong.",
            "summary_of_the_review": "This paper proposed a two-stage algorithm to tackle spurious correlations learned by machine learning models. The motivation for using bi-level optimization is unclear and it is confusing on the rebalanced weight of the biased split and the unbiased split.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies training deep models that do not suffer from distribution shifts. This paper tackles this problem in an 'unsupervised' way (e.g. without having attributes, for each dataset example, that describe whether said example has exploitable shortcuts or not).\n\nThis paper proposes the following approach: first, split the training set into two subsets (where one is 'easier' and the other is 'harder'; presumably these map to a 'biased' and 'unbiased' set respectively). This is done through a 'pseudo-labeling' approach. The network is trained on a subset of data, and early stopping is applied on the training accuracy -- once the model reaches a given training accuracy, they stop there and use that to split held-out data. Second, combine examples from the two sets (using mixup in part) and minimize loss.\n\nThe paper evaluates on synthetic tasks (colored mnist/corrupted cifar-10) and both Waterbirds (Sagawa et al 2019) and Bias Action Recognition (Nam et al 2020). The model seems to do okay on all of these benchmarks, e.g. on par with JTT (Liu et al 2021). The paper presents a few ablations also.",
            "main_review": "Strengths:\n* To the best of this reviewer's knowledge, this particular way of splitting the data into 'easy' and 'hard' sets, for the purpose of robust training, seems novel. (It might be good to compare though with other work that proposes such a thing, and uses it to construct more robust datasets for training, e.g. [1]/[2] before).\n* The approach of aggregating both 'easy' and 'hard' sets seems new to this reviewer. The overall approach seems to do all right on benchmarks.\n\nWeaknesses:\n* This reviewer is not fully convinced that this approach works for truly \"unsupervised debiasing.\" The main issue is that, the Waterbirds and BAR datasets do contain 'real images' to some extent, they also are a bit artificial due to the photoshopped backgrounds. Thus, while it might be possible to do \"unsupervised debiasing\" with this context, where there is a clear (artificially added) bias, it is not clear whether this method would work on other datasets. Thus:\n    * It would be great to see how this method does on a dataset like ImageNet, which is known to have a bias towards texture (e.g. [3]). Likewise, this could allow this approach to be compared to other approaches (which I suppose are 'supervised'), like [3].\n    * It would also be good to see how this method does on a dataset outside of image classification, e.g. on VQA where there is a strong language-only bias [4]. This could allow also for comparison with other methods.\n* To this reviewer, the paper could benefit from some more analysis. One important question is -- how good is the 'pseudo labeling' approach at splitting data up into biased or unbiased sub-datasets? Since there exists supervised labels for some of the empirical datasets, it might be helpful to see how successful this approach is at identifying subsets of data that are truly \"biased.\"\n\n\nPapers:\n* [1] Swayamdipta et al, EMNLP 2020, Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics\n* [2] Le Bras et al, ICLR 2020, Adversarial Filters of Dataset Biases\n* [3] Geirhos et al, ICLR 2019, \"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness\"\n* [4] Agrawal et al. CVPR 2018, \"Don't just assume; look and answer: Overcoming priors for visual question answering.\"",
            "summary_of_the_review": "To this reviewer, though there are some novel ideas here, the empirical results are not (yet) convincing. This is somewhat important to this reviewer, since at this point in time, the central claim of the paper seems to be about a general kind of 'unsupervised debiasing,' and it is not yet clear whether it really works outside of artificially-contructed datasets. I might be willing to raise my score if more results were presented during the author response.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors proposed a framework for learning unbiased models from biased data. The framework consists of two steps: 1) bias identification; 2) bias-invariant representation learning. In the first step, a regular ERM model is trained on the entire training data until the training accuracy reaches a certain threshold. Then the authors use the trained model to split the training data based on the prediction correctness (correctly predicted training examples are grouped into $D_{bias}$ while those that are incorrectly predicted are grouped into $D_{unbias}$). In the second step, the authors propose to use meta-learning for finding an invariant representation. Specifically, in the inner-loop, they seek the best parameters across both $D_{bias}$ and $D_{unbias}$ via weighted-sampling. In the outer-loop, they evaluate the model performance on a set of synthetic examples generated by Mixup. The authors evaluated the proposed method on Colored MNIST, Corrupted CIFAR-10, Waterbirds, Bias Action Recognition.",
            "main_review": "Strength:\n+ The direction of unsupervised de-biasing is interesting.\n+ The authors conducted extensive experiments on multiple datasets.\n\nWeakness:\n+ The paper is largely based on [1]. The only difference is the bias-invariant representation learning part where the authors adds meta-learning and mixup to the algorithm. However, the intuition for adding these additional complexities is not clear.\n+ Compared to [1], the proposed model shows no performance improvement on Waterbirds (86.7 for [1] and 86.5 for the proposed model).\n+ The writing of section 3.2 is very confusing.\n\nQuestions and suggestions:\n+ Why do you have $f_{\\theta}$ (instead of $f_{\\theta^*}$) in eq5 when you are updating in the outer loop?\n+ Since there is only one task during training, how is the proposed meta-learning approach different from directly optimizing the weighted ERM loss and the mixup regularizer?\n\n[1] Liu, Evan Z., et al. \"Just train twice: Improving group robustness without training group information.\" International Conference on Machine Learning. PMLR, 2021.",
            "summary_of_the_review": "This paper presented an approach for unsupervised de-biasing. While the results are promising, the idea of the paper is quite incremental (compared to [1]) and the design of the algorithm is less intuitive.\n\n[1] Liu, Evan Z., et al. \"Just train twice: Improving group robustness without training group information.\" International Conference on Machine Learning. PMLR, 2021.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}