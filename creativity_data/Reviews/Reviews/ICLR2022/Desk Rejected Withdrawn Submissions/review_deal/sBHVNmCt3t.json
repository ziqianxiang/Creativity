{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper first introduces how previous MIA(Shokri et al.) could generalize to seq2seq models and rl methods. The authors then demonstrate RNN models are more vulnerable to MIA compared to feed-forward neural networks. Finally, the authors show using dirichlet mechanism could effectively weaken MIA while not drastically sacrifice model performance.",
            "main_review": "This work provides a novel analysis on the privacy risk of training seq2seq model and training a sequence of models in general. It is important to conduct thorough study on empirical privacy risk of language models. However, I have a few concerns w.r.t to the method and empirical results.\n\nI'm a bit concerned with the novelty of this work. The paper basically applies previous MIA to RNN models. In Section 3.1, 3.2, the description of the methods is not clear. The authors should clearly explain in details how the proposed MIA attack is different from Song & Shmatikov, 2019 paper. What is the input and label for the proposed RNN binary classifier for MIA and what's the objective being optimized. \n\nI'm also confused with the experiments represented by figure 1. The authors describe the experiments as splitting the cifar into 10 batches and train each batch sequentially. While performing MIA, does the author train a set of global shadow models and respectively and apply them on 10 different models respectively? Please explain in details for this part.\n\nFor figure 2, I'm not entirely convinced by the claim the rnn is more vulnerable to MIA then feed-forward nn. The training reward of rnn is substantially higher than that of feed-forward nn while the gap between test reward for rnn and nn is significantly smaller. Hence, the higher MIA accuracy of rnn could entirely due to the larger generalization gap of rnn. Consider a feed-forward nn with the same generalization gap, would it induce the same MIA accuracy than rnn? If yes, then the vulnerability only comes from generalizability of model, rather than the type of model.\n\nIn Section 5.3, the authors evaluate the MIA under model with Dirichlet Mechanism(DM). The results look impressive. It would be great if the authors could show the $\\epsilon$ in the DP guarantee provided by DM since one cannot prove the MIA being used is the strongest MIA one could conduct. Moreover, it would also strengthen the paper if the authors could explain why not use DPSGD or compare the privacy utility tradeoff of DPSGD and Dirichlet Mechanism under the proposed MIA.\n\nMinor: \n- figure 3: 'Tesing Score' -> 'Testing score'\n- figure 3: y-axis should be 'Accuracy' rather than 'Attack Accuracy'\n",
            "summary_of_the_review": "The paper identifies and empirically studies MIA on sequence of data. However, improvements are needed to clarify methods and empirical results.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper develops a pair of membership inference attacks (MIAs) for two primary applications of recurrent networks, namely, deep reinforcement learning and sequence-to-sequence tasks. The first attack provides empirical evidence that recurrent networks are indeed more vulnerable to MIAs than feed-forward networks with the same performance level. The second attack shows the differences between the effects of overtraining recurrent and feed-forward networks on the accuracy of their respective MIAs. Finally, this paper proposes a differential privacy mechanism to resolve the privacy vulnerability that the MIAs exploit.",
            "main_review": "Strength:\n1. This paper provides empirical evidence on the vulnerability of the recurrent networks.\n2. Differentially private mechanism is deployed to relieve the privacy issue.\n\nWeakness:\n1. The paper is far from being well-written, where there is a lot of technical detail missing. For example, how MIA proceeds, which is the most important part of the paper, is only sketched in section 3.0. Many important technical details, such as how to train a shadow model, how the shadow dataset have been omitted. They do not show up in the experiment section, either.\n\n2. The contribution of this paper is marginal. For example, the vulnerability of RNNs is well-known in the literature of differential privacy [1]. Additionally, as compared with (Song & Shmatikov, 2019), I do not think allowing for processing arbitrary-length sequences, and changing the distance metric is a huge contribution.\n\n3. Some related work is missing. Membership attack in RNN is well-studied in the literature of DP [1], [2]. They also propose some other membership attack methods. The authors do not mention them, either provide a fair comparison with their methods.\n\n4. DP method is used without justification. Why do we want to use LDP instead of GDP? By the way, I do not understand why the definition  of GDP appears in the paper, since it has not been used anywhere else. Why do we want to use Drichlet mechanism, instead of Gaussian or Laplace mechanism, which are more common in DP? What is the performance of the other DP RNN methods? These questions are not answered in the paper. \n\n[1]: https://arxiv.org/abs/2009.10031\n[2]: https://arxiv.org/pdf/1710.06963.pdf",
            "summary_of_the_review": "The paper has made marginal contributions. However, it is not enough according to the criterion of ICLR. In general, I recommend rejection of this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the privacy implications of deploying RNNs in machine learning.\n\nSpecifically, the authors conduct experiments by employing membership inference attack (MIA) to attack the RNNs and feed-forward neural networks. The empirical results show that: (1) For RNNs, data records used earlier in the history of training are more vulnerable than those used closer to the attack execution. (2) RNNs are more vulnerable than their feed-forward counterparts under MIA. (3) Overtraining RNNs may have a marginal effect on the MIA accuracy, as opposed to the case for the feed-forward model. Therefore, existing defense methods for feed-forward networks that prevent overfitting may not be suitable for RNNs.\n\nBased on these observations, the authors further propose to employ the Dirichlet mechanism to resolve the privacy vulnerability of RNNs.",
            "main_review": "I think the contribution of this paper is limited.\nSpecifically, the attack method (membership inference attack) and the defense method (Dirichlet mechanism) used in the experiments are all from existing works.\nThe only non-trivial contribution is the experiment results that reveal the vulnerability of RNNs to MIAs and the marginal effect of overtraining RNNs as opposed to the feed-forward model.\nThe authors may need to put more effort into explaining the significance of their contribution.\n\nBesides, the writing of this paper requires further improvement.",
            "summary_of_the_review": "Based on the comments, I think this paper does not meet the requirement of ICLR, and thus recommend rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies membership inference attacks (MIA) on recurrent neural networks and mechanisms to defend against them. Two attack settings are considered: 1) reinforcement learning task where MIA attempt to infer regions of space the agent has visited and 2) sequence-to-sequence tasks where MIA attempt to decide if a particular input sequence has been used in training.\n\nThe paper reports several results. First, it is shown via experiments that RNN is more prone to MIA than feedforward networks, and the authors attribute this to a hypothesis that RNN are “designed to better remember their past experience”. Second, a defense mechanism against MIA is proposed based on obfuscating probabilities vectors via the Dirichlet mechanism, which allows for trade off in (differential) privacy and model accuracy. The authors demonstrate that this mechanism can successfully defend against the aforementioned attacks.",
            "main_review": "The paper is written clearly and is generally easy to follow, and this is a key strength. Moreover, the problem it studies is potentially very interesting - as far as I am aware, privacy-accuracy tradeoffs are not as well studied in the RNN scenario.\n\nHowever, the paper currently contains a number of issues due to which I am hesitant to recommend it positively. These are detailed below and the authors may wish to address them.\n\n1. It is claimed that RNN remembers the “past history”, and thus is prone to MIA. While it is true that RNN interacts with “time” differently from feed-forward/convolutional structures in time-series modelling, as far as I understand this has to do with the temporal structures in the *data* (sequences) itself. That is, RNN is designed to deal with memory in the input/output time series. This is a different concept from the “past history” in MIA, which concerns the order at which a training data is presented to the model for training. These are completely different concepts and I do not agree that this is a plausible explanation, at least as presented in this paper’s current form, for the observed disparity in response to MIA attacks.\n2. Following the previous conceptual issue, the experiments aimed at demonstrating this phenomena can be improved. Specifically, in 5.1, more controlled experiments should be considered. \n   1. Figure 3 and Figure 1 are compared, but they are based on completely different models on different datasets, and it is not appropriate to draw conclusions on response to MIA attacks based on settings that differ on a variety of factors. \n      For example, to directly compare the effect of MIA on sequence problems using feedforward and recurrent architectures, the same dataset should be used, e.g. for CIFAR10, one can compare the difference between a feedforward network’s vulnerability to MIA vs. a RNN’s (which treats CIFAR10 classification as a sequential classification problem by flattening pixels. This is sometimes used as benchmarks for RNNs). \n   2. Figure 2 on the RL case is a more controlled comparison, but the model capacity is not controlled for. As presented, the LSTM agent contains additional trainable parameters and direct comparison is again not convincing. For example, a control can be used where a feedforward network contains the same (or larger) number of parameters than LSTM networks. Would the same behavior be observed?\n3. Sec 5.2 and Fig 3: are there any direct evidence that regularization effects has “marginal” effect on RNNs, whereas having more effect on feedforward networks, when it comes to MIA? \n   The evidence presented here is indirect, and a simple test with $\\ell_2$ regularization seems to be straightforward.\n4. The defense mechanism is based on a previously introduced differential privacy mechanism (Dirichlet mechanism). While the authors apply it to a potentially novel setting of RNNs, the method appears to be exactly the same as it only requires a probability vector. Can the authors clarify the novelty of the proposed defense algorithm?\n\nThere are some minor issues that the authors may wish to clarify\n\n1. The MIA definition given here for RL is to infer some region of space that the agent has visited. Is this a standard definition of privacy for RL or is one introduced in this paper? Can the authors give some examples as to why this is a good definition of privacy in the RL setting? \n2. Small technical issue with Definition 1: measurability is defined with respect to $\\sigma$-algebra, so not sure what it means to say “… so that $(\\mathcal{R},\\mathcal{M})$ is measurable”, since it is by definition $\\mathcal{M}$ measurable? Shouldn’t it also require $\\mathcal{A}$ to be measurable? Same with definition 2.\n3. Definition 2: Does $d$ need to be a metric, and in particular, symmetric? It appears that in applications this should be the case, but is not required in this definition.\n4. Sec 3.2 paragraph 3 first sentence: any set can be written as a disjoint union of subsets, so I am not sure what the authors are trying to express. Does these regions need to be well separated in some space? How does this affect your algorithm/experiments?",
            "summary_of_the_review": "The paper studies a relevant topic and gives some potentially interesting findings on the vulnerability of RNNs to MIA compared to feedforward counterparts. However, the conceptual understanding of this difference is unclear, and the numerical evidence can be improved. Lastly, the novelty of the proposed defense mechanism appears limited. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}