{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose an initialization scheme that is able to train normalization-free architectures by modifying the summation operation of a block output. Through variance analysis for both forward and backward propagation, they derive an initialization that ensures the means of activations and gradients are zero, and their variances are preserved throughout the network. They also empirically demonstrate that the proposed method achieves competitive results on CIFAR-10.",
            "main_review": "1. Novelty issue. The proposed modification that downscales the residual block by sqrt(0.5) has been well discussed in [1], where [1] derive how a factor of sqrt(0.5) ensures table forward propagation and show that sqrt(0.5) achieves less accuracy compared to adding a scalar multiplier over residual branches. I do note that the paper also demonstrates that the sqrt(0.5) stables the initial gradients during backward propagation, which is a good add-on but less significant. Therefore, I believe this work lacks novelty, and I suggest the authors clearly differentiate their work from [1]. \n2. Comparison to other normalization-free techniques. In section 3.3, the authors argue that their proposed method is preserving the total variance, unlike other recent normalization-free techniques that bias the network towards the identity function. However, there is neither theoretical or empirical evidence to support why the proposed method is better than others. As mentioned in point 1, [1] even show a contrary conclusion that sqrt(0.5) is worse than adding scalar multipliers over residual branches. More justifications and comparisons are needed. \n3. Weak empirical evaluation. First, the authors only evaluate the method on CIFAR-10 without large-scale datasets such as ImageNet. Second, it is not an apple-to-apple comparison as they only compare two different initialization methods under the same modified structure, which favors their setting. A better setting should be comparing these methods under their own designed architectures. Again, more baselines such as Fixup initialization should be included.\n4. Learning rates for different setups should be tuned instead of fixing the same one. The authors choose the same small learning rate of 0.01 for all setups. BN-based resnet or even some normalization-free techniques can tolerate a larger learning rate and may achieve better convergence and final accuracy.\n5. No definition for \\gamma_{g}^{2} on page 4 and SPP on page 7.\n\n\n[1] De, Soham, and Samuel L. Smith. \"Batch normalization biases residual blocks towards the identity function in deep networks.\" arXiv preprint arXiv:2002.10444 (2020).",
            "summary_of_the_review": "Overall, the paper is mostly well-written, and the proofs and explanations are well-organized. However, I think the paper lacks novelty and requires more empirical evaluations and comparisons. Therefore, I believe the paper needs another round of submission to resolve these problems. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigates the signal propagation dynamics of residual networks at initialization, extending previous analyses which solely looked at forward propagation and relied on the result of Hanin and Rolnick to justify this as sufficient to also cover backward propagation. In this paper the authors argue that this is not necessarily the case and demonstrate analytically and empirically that models can maintain constant signal variance on the forward pass while having gradients that grow on the backward pass. Several modifications to the residual connection are proposed with the intent of rectifying this, and an experiment is conducted using ResNets on CIFAR-10.",
            "main_review": "My Take: \n\nThis paper is well-presented, investigates an important problem in the area of fundamental understanding of neural network training dynamics, and presents an insight which is potentially impactful in this area. However, while the initial analysis appears to be solid, the empirical evaluation of the proposed modifications is insufficient, as the experiments have not been conducted in a regime where the issues under consideration become sharply salient. My score is accordingly set at 5: to me the paper, as-is, is clearly not ready for publication, but I think that it has the potential to be a solid contribution at another venue if the authors can rectify the flaws in the empirical analysis.\n\nSpecifically, the only empirical evaluation is conducted on CIFAR-10 using a ResNet-50. This is a problem for several reasons. First, the specific issue which is targeted here–that of exploding gradients in models whose forward signal propagation is otherwise well constrained–is not really present in this regime. NF-ResNets train just fine up to a fairly large batch size and depth, and by the authors' own analysis, they suffer from the gradient propagation issue which the authors seek to rectify. In order to actually demonstrate the value of the approach, the authors need to show that their approach delivers an improvement or allows for a simplification of techniques relative to a meaningful baseline.\n\nOne example of this would be that NF-ResNets and other nets (FixUp Nets, nets with zero-init’d scalars) are unstable at high batch sizes (around 4096 for ImageNet) and with strong augmentations unless specialized gradient clipping is employed. If the authors could demonstrate that their approach makes it possible to remove this clipping and still retain stable training without a loss in performance, that would be an appropriate demonstration of the validity of the method. Extremely deep models (>300 layers) can have difficulties training without normalization even when their shallower counterparts are stable; demonstrating that this method enables training of such models would also be worthwhile empirical evidence.\n\nThis reviewer recognizes that the authors may not have access to the large scale resources necessary to train on imagenet, or perhaps even to train with a large model on a small dataset like CIFAR. While it is unreasonable to expect the authors to be able to train on ImageNet, it is unfortunately important to note that the cases where gradient clipping become necessary only emerge in these more resource-intensive settings, and any valid empirical investigation must necessarily take place in these settings, or the authors must be able to make an extremely compelling case that the results will transfer to these settings. It is generally the case that things which work at small scale simply *do not work* when tested at larger scales (see things like shake-shake regularization, which attained a SOTA result on CIFAR but provides no improvement on ImageNet), and so the onus of proof is generally on the authors. Please note that this is not intended to be a lazy reviewer response of “oh the authors did not test on ImageNet, reject”:  I think this line of work is very promising, and I do not wish to see the authors abandon it, but I feel that this particular point is one of scientific rigor.\n\nDetailed notes:\n\n-The slight improvement in performance on CIFAR-10 is likely explicable by the increase in model capacity, rather than an improvement in signal propagation. \n\n-Empirical results are exclusively presented in the form of a line plot of training accuracy over time. The authors should absolutely at least include a table with readouts of final performance (and variance, thank you for running multiple random seeds), even if only in the appendix.\n\n-I have tested the proposed modifications using an NFNet-F0 (appropriately modified to attain the desired forward and backward signal propagation in the case where constant signal variance is desired, and validated by visualizing the SPP) and found that they do not enable the removal of adaptive gradient clipping when training on ImageNet at batch size 4096 or with strong augmentations. Even with gradient clipping, I find that these modifications tend to reduce performance or hamper stability. It is worth noting that in the negative results section of the ICLR2021 NF-ResNets paper, the authors mention exploring an approach where the skip path is downscaled with the goal of maintaining constant signal variance, but similarly found that it reduced performance.\n\nNote that I don’t hold this against the paper, and am wholly aware of the commonality of an unskilled reviewer incorrectly implementing a proposed method and rejecting a paper because of their own lack of ability rather than a flaw of the paper itself. My point is more that the authors need to run experiments which appropriately stress-test and demonstrate the value of their method rather than leaving such a gap through which a reviewer might kick a stone. It is entirely possible that I made a mistake in implementation, or that there is a further change necessary, but it is on the authors to appropriately prove things out.\n\n-The correction factor \\gamma_g^{2} in equation (3) \nThere is no such term in equation (3) as written in this paper.\n",
            "summary_of_the_review": "My Take: This paper is well-presented, investigates an important problem in the area of fundamental understanding of neural network training dynamics, and presents an insight which is potentially impactful in this area. However, while the initial analysis appears to be solid, the empirical evaluation of the proposed modifications is insufficient, as the experiments have not been conducted in a regime where the issues under consideration become sharply salient. My score is accordingly set at 5: to me the paper, as-is, is clearly not ready for publication, but I think that it has the potential to be a solid contribution at another venue if the authors can rectify the flaws in the empirical analysis.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a modification of ResNet such that the variance of both forward and backward signal can be preserved, which in return produces well-behaved gradients (neither vanishing nor exploding). The authors first observe that He's initialization can achieve the bidirectional variance preservation only for fully connected networks, but not for ResNet. Then they observe Brock's initialization only achieves forward variance preservation, resulting in gradient explosion. Based on the two observations, the authors propose to scale the shortcut and/or the residual branch for the backward variance preservation (as the forward one is easy to achieve). Small scale experiments are conducted to support the proposed modification.",
            "main_review": "Strengths:\n1. The motivation is clear and well-intended.\n2. The paper is easy to follow. \n3. The literature review is up-to-date.\n\nWeaknesses:\n1. The mathematical derivation is non-rigorous. For example, it is assumed in Section 3.2 that dL/dx(l-1) and dx(l)/dx(l-1) are uncorrelated. Can the author justify this assumption theoretically and empirically? For another example, it is assumed that E[df(x(l-1))/dx(l-1)] = 0. Please justify this assumption as well.\n2. The overall approach is not novel. As the authors stated, the scaling technique has been employed in Brock (2020,2021).\n3. The empirical results are not strong enough to confirm the effectiveness of the proposed method. In the right figure of Figure 2, it seems that the proposed method leads to gradient vanishing, in contrast to the gradient explosion of others. The expected backward variance conservation is not observed. Besides, the experiments are only conducted on a single small dataset CIFAR-10.\n",
            "summary_of_the_review": "While the paper has clear and well-intended motivation to conserve both the forward and the backward variance in ResNet, this mathematical analysis is not rigorous and the empirical evidence is not strong enough to show effectiveness of the proposed method. Therefore, I recommend rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the important problem of initializing residual networks without the use of normalization layers. It proposes a novel method of initialization that jointly controls the variance of the main branch and the residual branch, and matches the performance of batch normalized residual networks. This is unlike prior work that only controls the variance of the residual branch [Zhang et al. (2019); De & Smith (2020); Bachlechner et al. (2020)],  which has failed to match the performance of residual networks with normalization. It is also unlike prior work that does match the performance of residual networks [Brock et al. (2020; 2021)], but has to use heuristics to control the exploding gradients. To achieve performance matching that of a batch normalized network, the authors had to introduce extra convolutions (ConvShort), while the variant without them (IdShort) did significantly worse.",
            "main_review": "Strengths:\n- The paper confronts the important research problem of initializing residual networks without the use of normalization. Like the authors mention, \"batch normalization adds a significant memory overhead, introduces a discrepancy between training and inference time, has a\ntricky implementation in distributed training, performs poorly with small batch sizes\". Recently, Brock et al. 2021 has showed that training a residual network without any normalization is sufficient to achieve a new SOTA top-1 imagenet record. This paper aims to improve upon Brock et al, by avoiding the gradient clipping heuristics used by them.\n- The idea is simple and the paper is very easy to follow.\n\nWeaknesses:\n- The main weakness of the paper lies in the empirical section.\n- In Figure 3, we observe that IdShort does significantly worse than ConvShort, showing that the extra convolutions are essential to matching the performance of a batch normalized network. The authors should quantify why this is a fair comparison, by showing the memory and compute usage of the batch normalized network vs ConvShort. In addition, the authors should also compare against prior work that controls only the variance of the residual branch, with the h(x) = convolution modification as well. If this modification also makes these prior work competitive with the batch normalized network, the authors need to explain why their proposed method that jointly controls both the variance of the main and residual branch is superior.\n- Also in Figure 3, the authors failed to replicate Brock et al. 2021, which is concerning. I suggest that the authors reach out to Brock et al. 2021 to figure out the reason why they are not able to replicate the findings on a ResNet50.\n- While I understand that the computing resources needed to train a SOTA imagenet model (like Brock et al.) using their proposed method of initialization might be prohibitive to the authors, I think there needs to be more substantive experiments in the paper for the authors to build a convincing case. One possible direction is to test their ideas out on Transformers, which are known to be difficult to initialize, in large part because of the residual block. See \"On Layer Normalization in the Transformer Architecture\" for example on the discussion between the difference in a Pre-LN and a Post-LN transformer.\n- Nit: I'd like to see the analysis be a bit more rigorous. E.g. the induction in 3.2 should include the base case as well.",
            "summary_of_the_review": "I vote to reject the paper at this point. While the paper tackles an important research goal and the technical contribution is novel, it is not clear from the experiments how or why the contribution is superior to prior work. There is only one experiment in the paper, done on ResNet50, which is lacking in its comparisons with a batch normalized network. In addition, the experiment also failed to successfully replicate prior work that it compares with like Brock et al., and does not compare with prior work like Fixup (with and without the convolution modifications). Significantly more experimental work has to be done on the existing ResNet50 setup as well as other setups for me to be able to recommend the paper for acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}