{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors explore an exciting topic, source-free few-shot adaptation. Compared to the most recent source-free/test-time adaptation, the proposed setting has a few labeled target samples. Furthermore, by introducing a constrained optimization of batch norm layers, the proposed method could boost the target-domain performance of the source model without suffering from large batch-size or random sampling of test data. The empirical evaluations on style shift (PACS), sim-to-real shift (VisDA17), and natural shift (Camelyon17, iWildCam) indicate the effectiveness of the proposed approach.\n",
            "main_review": "### Strength\n- LCCS looks natural and intuitive to me. I agree with the authors that optimizing a large number of parameters with only a few samples should be an ill-posed problem. Therefore, the effort to simplify the parameter space and propose the re-parameterization looks reasonable.\n- The proposed method is easy to follow. The overall writing looks well-organized to me. Both big picture and implementation details are discussed intensively. I believe the readers could catch up with the most critical insights in an easy way. \n- Both tables and illustrations are helpful for the readers to realize the main motivations and contributions of the proposed method. One possible suggestion is to polish the caption for figures. It is a little bit hard for the reader to follow the main delivery message, especially for Figures 1&2.\n\n### Weakness\n#### The lack of technical novelty\n- I agree with the authors that the proposed setup of source-free few-shot adaptation should be interesting. Such a problem statement would guide the community to develop a more practical system for real-world scenarios. However, the lack of technical novelty could be a fatal drawback of this submission. The effectiveness of BN stats/parameters is well explored in the existing related work. \n- The authors additionally propose a low-dimensional approximation to simplify and constrain the optimization procedure. Unfortunately, the authors failed to echo their statement in the experiments section. I am suggesting that the authors could try to propose several reasonable baselines to highlight the necessity of the proposed parameterization.\n\n#### The missing solid baseline\n- I think the authors should propose some baselines with the SAME setup of LCCS. For example, the authors could freeze/unfreeze $\\mu, \\sigma$ of TENT and replace the entropy with cross-entropy loss with the few-shot labeled samples. Compared to the proposed LCCS, the baselines I just mentioned choose to optimize the affine parameters instead. Such a comparison would verify the necessity of low-dimensional approximation. \n- In addition, I am a little bit worried about the unexpected behavior of TENT. I tried TENT for VisDA and WILDS from my side. My numbers are always better than the vanilla model, with a little bit of carefulness in choosing the proper learning rate. Since TENT is the only learning-based baseline to compare with, I hope the authors could provide more details on applying TENT for these experiments in this paper.",
            "summary_of_the_review": "From my perspective, I think the proposed setting is well-motivated and practical. Moreover, it is not that hard to label a few test samples in real-world applications. The proposed setup continues the story of test-time adaptation and then enlarges the scope of the existing source-free/test-time adaptation methods. However, the lack of technical novelty and solid baselines weakened the significance of this submission, which should be improved in the next version of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper focuses on a novel and practical problem, source-free few-shot domain adaptation, to address challenges in test-time adaptation. They propose a constrained optimization of source-model-batch-normalization layers by finetuning linear combination coefficients between training and support statistics. They also compare their method with baseline methods on some multi-domain datasets.",
            "main_review": "Pros:\n\n+ The problem studied by this paper is novel and practical enough to simultaneously address data missing and privacy leakage. Especially for some special fields, the two issues are very common, while both have still not been solved well.\n\n+ The proposed method solves source-free few-shot DA by optimizing LCCS and greatly reduces the number of optimized parameters, making this method available for large-scale models.\n\nCons:\n\n- As far as I can see, the main technical contribution in this paper is to solve domain shift by adapting BN layers of the source model, and this method seems very novel for the domain adaptation field. However, I can not find any discussion about your motivation for this method in the Introduction. What is the connection between domain adaptation and BN layer optimization? Here is very abrupt for readers.\n\n- This paper seems to miss some important baselines for source-free few-shot domain adaptation. E.g., the novel source-free UDA method [1] (you can add supervised loss into SHOT), naively finetune source model with target data, cross-domain few-shot learning method [2], and hypothesis transfer learning methods. In addition, baselines (e.g, Tent) are not introduced in the experiment part, which confuses readers.\n\n- From Table 3 and Table 4, it is clear that finetuning LCCS almost does not outperform baseline methods, especially for Tent (Adam). The size of the support set is 10, and this may beyond the setting of few-shot learning (usually less than 5). These experimental results are not convincing. \n\n- In the second line from the bottom of Page 1, they said “In this paper, we address a challenging but practical unsupervised DA setting”. But I find that their problem setting and method are about supervised DA setting (i.e., the target data are labeled), which is very misleading.\n\n- There exist many grammar and spelling mistakes. For example, “Then, we introduce our proposed method that optimizes using BN statistics” (what will you optimize?), and “we see that this is equivalent to setting BN statistics in the source BN layer...” (the first letter is lowercase, setting->set), and so on.\n\n- In the third section, an important step about how to determine the spanning vectors using few target data is ambiguous, and I think they need to give a clear and solid solution. Then, they claim that the spanning vectors are suitable for both source domain and target domain. However, they simply achieve this by combining the parameters of two domains. I think they need to design some experiments to verify this claim.\n\nReferences\n\n[1] Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. ICML, 2020.\n\n[2] Domain-adaptive few-shot learning. WACV, 2021.",
            "summary_of_the_review": "The considered setting is interesting. However, some relevant baselines are missing, such as source-free UDA methods (need to slightly change to fit your problem), hypothesis transfer learning methods (can be directly used to address your problem). These methods should be discussed and compared in detail.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new setting dubbed source data-free few-shot domain adaptation, where only a source-trained model and few-shot labeled target data are provided for adaptation to target data. To avoid overfitting during the optimization with few-shot target data, the authors propose to only optimize the Batch Normalization (BN) statistics for target adaptation and approximate the optimal target BN statistics by a linear combination of source statistics and target spanning vectors. Then the authors use supervised cross-entropy loss to only optimize the low-dimensional coefficient parameters. Experimental results on different image classification benchmarks verify the effectiveness of the method.",
            "main_review": "### Pros\n- The few-shot source data-free domain adaptation setting is new.\n- The proposed method is well-motivated, simple yet effective.\n- Writing is well-organized and easy to follow.\n\n### Cons\n- It is confusing and misleading to call the few-shot source-free setting test-time adaptation. Because it is not a streaming or online setting. It does need supervised training with multiple epochs in the target domain, which is the same as the source data-free domain adaptation setting [1]. From the aspect of domain adaptation, this setting is the incremental source data-free version of the few-shot domain adaptation [2]. \n- Important experimental results are missing. As for baselines, experimental comparisons are only two test-time BN methods. First, it is necessary to compare with methods of source data-free domain adaptation [1]. Besides, fine-tuning baselines are required, including vanilla fine-tuning and regularized fine-tuning [3,4]. Results on a challenging and popular multi-class domain adaptation benchmark Office-Home are required.\n\n### Other comments\n- Does the setting reserve validation set for grid-search of the LCCS parameters in the initialization stage? Or the initial parameters are determined by the results on the same training data? \n- Do the reported results of test-time BN and TENT involve the supervised CE loss for training?\n- Compared with TENT (Adam) on PACS, it is hard to say LCCS performs better. But TENT (Adam) has an extremely bad performance on VisDA. Why?\n\n### References\n[1] Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation, ICML'20\n\n[2] Few-Shot Adversarial Domain Adaptation, NeurIPS'17\n\n[3] DELTA: DEep Learning Transfer using Feature Map with Attention for Convolutional Networks, ICLR'19\n\n[4] Explicit Inductive Bias for Transfer Learning with Convolutional Networks, ICML'18\n\n=============================================================================================\n\nAfter rebuttal\n\nThanks for adding the required experiments. However, I have the following two unresolved concerns. \n\nThe proposed setting is not practical and is incremental, compared to the source data-free domain adaptation. From the results in Table 6 in the revision, SHOT with all unlabeled data significantly outperforms the proposed methods trained with few-shot labeled target data only. On the other hand, only given few-shot labeled target data, how do we conduct hyper-parameter tuning like the LCCS parameters? It is not a good idea to validate through the few training data as clarified by the authors.\n\nThe current experimental comparison is in a mess, thus cannot show the advantage of the proposed method. Although the authors provided results of other methods and datasets, all methods are not compared in the same experimental setting. It would be better if the authors compare their method with the state-of-the-art methods in few-shot fine-tuning, source data-free domain adaptation, and test-time training, given few-shot labeled data and many available unlabeled data in the target domain.\n\nAs a result, I decided to decrease my score to \"5\".\n\n",
            "summary_of_the_review": "This paper proposes a simple yet effective method. But there are some concerns about the setting and experiments. Therefore, I decided to give a rating of 6 at the current stage.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new problem setting source-free few-shot domain adaptation, and propose a new method to potentially solve the new problem. Some of the experiments show performance improvement upon previous works. ",
            "main_review": "Strengths:\n1. the paper tries to solve the potential problems of previous source-free test-time adaptation. \n2. the method on parameter reduction seems interesting. \n\nWeaknesses:\n1. Although previous source-free test-time adaptation setting has problems, I don't think the paper focuses on the important one. One problem of Test-time adaptation methods is that it requires large batch size, which is normally not practical. \n2. In the new setting, authors assumes $k$ labeled sample per class. This is not practical in two ways: (1) how can there be exactly $k$ labeled samples per class, isn't this too strict? (2) why assume no unlabeled samples on the target domain?\n3. The motivation of the paper is to adapt quickly on light-weight devices with storage and computational capacity. But the style transfer process of the framework greatly improves the computation cost, which is not practical. \n4. The experiment results have very minor improvements or even degradations on multiple datasets, which makes the paper not strong. \n5. I hope the author can write the method part, especially 3.2 more clearly, the current version is hard to understand. ",
            "summary_of_the_review": "As discussed above, I think the weaknesses clearly outweigh the strengths, so I hope the authors can improve the paper further. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper introduces a new task source-free few shot domain adaptation and proposes an idea of training low-dimensional approximation for batch normalization layer adaptation. The authors demonstrate source-free few shot domain adaptation on four benchmarks for typical domain adaptation. The proposed idea is working well on the proposed settings and some cases it outperforms naive domain adaptation case.",
            "main_review": "**Strengths**\n\nThe combination of source-free domain adaptation and few-shot learning is more appealing to a real-world scenario compared to naive unsupervised domain adaptation.\n\nThe proposed method is simple and effective. It also seems to be plugged into the other neural architecture equipped with batch normalization.\n\n**Weaknesses**\n\nThe proposed method has weak novelty, lacks related works, and has experimental flaws.\n\n*Why do we need source-free few shot domain adaptation?*\n\nThe needs for the proposed task, source-free few shot domain adaptation, are not explained well in the paper while the core of this paper is on the task. This paper did not explain those things, which is crucial to the task-defining paper.\n\nThe datasets of the benchmarks are borrowed from the standard domain adaptation benchmarks. I think that there can be many good sources of data that fit the scenario of source-free few shot domain adaptation.\nFor example, the author of source-free domain adaptation [Liang et. al., 2020] explains the necessity of the proposed task is privacy concerns, although those privacy concerns are not analyzed in the paper.\n\nI hope that raised concerns should be tackled in the experiment section especially for privacy and light-weight device. This paper also did not solve the raised concerns like [Liang et. al., 2020]. In section 1, the author insists that “Our setting is practical in real-world applications where privacy rules prohibit access to source data, and where models need to be adapted quickly on light-weight devices with storage and computation capacity too limited to retrain jointly on source and target data”. But, the datasets and the baseline architecture are borrowed from the typical domain adaptation.\n\nFor example, the paper targets ‘light-weight devices’, experiements might deal with lightweight neural networks, such as a family of MobileNets. Then, the energy consumption can be reported in the experiment section like [Kim et. al., 2016].\n\n*Related works on test-time domain adaptation*\n\nTo my best knowledge, a test-time adaptation of batch normalization was first dealt in Adaptive Batch Normalization for practical domain adaptation (AdaBN), which is not even cited in this paper. AdaBN adapts all whitening parameters, mean and standard deviation, of BN.\n\n*Lack of benchmarks and baselines*\nFor example, the paper [Liang et. al., 2020] demonstrates the idea of source-free domain adaptation on several tasks and baselines. However, this paper did not.\n\n**Question**\n\n*The optimality of the batch normalization.*\n\nIn section 3.1 under the equation (2), the author mentioned that “This observation implies that we can obtain the optimal $(Z; \\mu_t ,\\sigma_t , \\gamma_t \\beta_t )$ by optimizing only the BN adaptation statistics. $\\{ \\mu , \\sigma \\}”? Could you prove your statement in a formal manner?\n\n\n**Improvement**\n\n*Overfitting behavior*\n\nThe overfitting behavior can be visualized by plotting the test/validation accuracy over the training split.\n\nBy the way, AdaBN is hard to be overfitted onto the dataset because it simply estimates the statistics over the dataset for my experiences.\n\n**References**\n\n[AdaBN, journal version] Yanghao Li, Naiyan Wang, Jianping Shi, Xiaodi Hou, Jiaying Liu, Adaptive Batch Normalization for practical domain adaptation, Pattern Recognition Volume 80, August 2018, Pages 109-117\n\n[Liang et. al., 2020] Jian Liang, Dapeng Hu, Jiashi Feng, Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation, ICML 2020\n\n[Kim et. al., 2016] Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, Dongjun Shin, Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications, ICLR 2016\n",
            "summary_of_the_review": "This paper introduces a new task, source-free few shot domain adaptation, and baselines to tackle the proposed task. The proposed task seems to be more general to be applied to the real-world problem, but there were no explanations and designated settings for tackling those. The proposed method seems to be simple and effective, but it was not tested on several baselines.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper propose a novel approach for few shot supervised domain adaptation. Rather than finetuning the shift and bias parameters, the proposed approach rewrites the source statistics as a linear combination of spanning vectors. In this way, it's possible to reduce the number of trainable parameters. The method is validated on several datasets and with different backbones.",
            "main_review": "Strengths:\n- the proposed method is mathematically sound. BN adaptation is known to be efficient but robust estimation of the target statistics can be problematic in the low data regime.\n-When only 1 sample is available, the proposed methods perform well.\n\nWeaknesses:\n- The comparison is not very fair: In Table 3  test-time BN and Tent are unsupervised methods. Nevertheless, they obtain performances similar to the proposed method. In Eq 5, if the cross-entropy is replaced by an entropy loss, the proposed LCCS would be fairly comparable to Tent. \n-In table 3 a simple baseline should be added: Finetuning of the BN parameters.\n- The most important experimental conclusion of the paper is hidden in section 5.2: \"From Figure 5, we see that when the support set is extremely small at k = 1, fine-tuning LCCS is always better than finetuning BN parameters. \" This should be moved to table 1 and not placed in a section named \"suggestion for practical use\"\n-Figure 5 should be detailed. More values for different k values should be added. I am not sure to understand the meaning of Finetuning LCCS +BN\n-Experimentally, we see the superiority of the proposed approach only in the case k=1. In the case of VisDA at k=5, the proposed method already underperform the other baselines. These results are somehow weak.  \n-Table 7:\" Note that n is at most the support set size, i.e. n ≤k ×12\". In resnet, the BN layers are located in the convolutional part. So the BN statistics are also averaged over different feature map locations.\n- In tabs 3-6,  I recommend adding \"(source)\"  for the first line.\n- In Sec. 3.2 \"the target domain is represented by n spanning vectors in Mspt and Σspt\nand can be extracted from the support set by aggregation functions or dimensionality reduction\nmethods such as singular value decomposition (SVD).\" It can be, but how? More details are required here.",
            "summary_of_the_review": "The method is sound and performs well with very limited support. However, some parts of the paper are not clear and the organisation could be improved. Evaluation is not completely fair and lacks in several aspects.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}