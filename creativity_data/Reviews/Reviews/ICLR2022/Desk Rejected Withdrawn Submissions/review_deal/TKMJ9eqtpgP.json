{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a new method that combines fusion models and CLIP to achieve image manipulation using couple of words, and then explores its potential applications, like image translation from different domains. ",
            "main_review": "Strengths:\n1. An introduction of a new method that combines diffusion models and CLIP on the text-guided image manipulation task. Similar visual quality as GAN-based methods has achieved by the proposed method.\n2. The paper also discusses different potential applications that the proposed method can be applied to.\n\nWeaknesses:\n1. There are no quantitative comparisons between the proposed method and baselines, and only limited number of visual comparisons are presented. Thus, it is hard to say the proposed method can achieve a good performance, similar as other GAN-based methods. As there is no quantitative comparison, one alternative choice is to conduct human evaluation.\n2. There is no discussion about the limitation of the proposed method. It might be better to have some discussion about why the adoption of diffusion method is good, instead of using GAN-based methods. \n3. For me, the good image manipulation performance may depend on the pretrained CLIP model, and the image translation between different unseen domain also thanks to the CLIP, which is pretrained on a large dataset. Therefore, I am not sure how much contribution the diffusion method can provide on text-guided image manipulation. \n4. To produce a modified image, the proposed method needs to use the feedback from CLIP to optimize the network, which means each time, when users want to achieve a new modification, they need to optimize the network again, which is less practical (if I am not right, please correct me). ",
            "summary_of_the_review": "The paper introduce a new method by combining diffusion methods and CLIP to achieve image manipulation using text. However, the paper lacks sufficient experiments to fully evaluate the proposed methods, including qualitative and quantitative comparison.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a method of text-guided image manipulation using the pretrained diffusion models and CLIP loss. It shows near perfect inversion property and good performance on both in-domain and out-of-domain manipulation tasks. The multiple attribute control applications are also shown in the paper.",
            "main_review": "\nThe proposed method follows the previous text-guided optimization-based image editing method using Clip loss. In the beginning I was excited to see the high quality results presented in the paper. However when I read the method, I was not sure if this paper makes enough novelty and doubt it will bring some new insight for the community.\n\n\nStrength:\n\n1. The paper is well written and easy to follow.\n2. DiffusionClip has reported impressive image reconstruction and editing results\n\nWeaknesses\n1.  Limited novelty:\nThe only difference between this paper and StyleClip is to replace the pre-trained StyleGAN model with a pre-trained Diffusion model. In similar situations, you can also replace Stylegan with a Flow model, a VAE model, or an autoregression model…\n\n2. Lack of quantitative comparison:\nAlthough the reconstruction quality of DiffusionClip is better than StyleClip, it is not obvious whether it has a better editing effect. At least, a user study is needed.\n\n3. Lack of semantic editing:\nFor me, the reconstruction quality is not that important. Using Stylegan latent space has the benefits of semantic editing, (such as changing pose, gender, lighting...), not sure whether the diffusion model can do the same thing. \n",
            "summary_of_the_review": "The biggest issue to me is the novelty of the paper. In addition, the paper only shows better reconstruction quality. It would be good to show the semantic editing results and quantitative comparisons.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a text-driven manipulation method for pre-trained diffusion models. Such a method makes use of a pre-trained CLIP model to supervise and direct the learning process towards the desired outputs. Two use-cases are proposed that are made possible by the diffusion models capabilities of nearly perfect inversion. First, the ability to translate images from an unseen domain to another unseen domain. Second, by combining the noise coming from the sampling of different diffusion models multiple attributes of new sampled data can be controlled.\n \n",
            "main_review": "*Strengths*:\n\n1. The provided visual results look promising, and they show the potential of using a powerful language-model such as CLIP to also guide the synthesis process of diffusion models.\n\n2. The paper showcases applications of the diffusion models, enabling more control of image synthesis.\n\n*Weaknesses:*\n\n1. The proposed approach has limited novelty. \n\nThe presented methodology is almost entirely based on the related work. The main novelty of the paper consists in the application of the CLIP guidance to diffusion models, fully relying on the way GAN-based models adopt CLIP (Gal et al. 2021, Patashnik et al. 2021) and not bringing any innovation or insights. For achieving a good inversion the proposed approach also adopts (Song et al., 2020).  Even the idea of translating images from an unseen domain to another unseen domain, is simply related to the works of (Choi et al. 2021, Meng et al. 2021). The contribution only consists in the generation of out-of-domain images from in-domain ones. \n\n2. Insufficient experimental analysis, lack of ablations: \n\na) Only qualitative results are provided. It would be beneficial to see some quantitative comparison as well, e.g. a user study comparing the quality between different CLIP-based and other manipulation methods, or adopting the pre-trained classifiers to detect the manipulated attributes of the image.\n\nb) I think it would also be beneficial to see how the proposed method would be affected by removing or perturbing some of its components. \n\nc) I would also like to see more experiments about the multiple attribute change by noise combination (e.g. by changing the weights in front of the attribute-specific noise terms, it should be possible to see how much different attributes are expressed. How much is really controllable?). \n\nd) Additionally, other manipulation methods are not considered, apart from the StyleCLIP, and StyleGAN-NADA ones. I would consider expanding on that.\n\ne) How efficient is the proposed approach on high resolution images (>256x256)? The reported fine-tuning time is 1 ∼ 8 minutes for resolution 256x256? What would be the time if the resolution is doubled? How the inference time is compared to GAN-based approaches, such as StyleCLIP, etc.?\n\nf) The method uses 50 real images to finetune the model. How does the synthesis/manipulation quality change when the number of images decreases (<5 image) or increases (>50)?\n\n3. The paper writing and presentation could be improved by better revision and re-organization:\n\na) The paper is not enough self-contained: most methodologies based on previous works are not sufficiently explained, even when they are recent contributions:\n\t- Section 2.1: theory behind diffusion models (needs more detailed explanations)\n\t- Section 2.2: CLIP guidance by means of global CLIP loss and local directional loss\n\t- Section 3.2: works of (Choi et al. 2021, Meng et al. 2021) are not explained\n\nb) Captions of figures are often not explanatory of what they are illustrating (e.g., Figures 3, 4, 7, 8, 9), and one needs to find the description inside the paper.\n\nc) There is no clear and punctual description of the limitations of the proposed method, also with respect to different generative models-based methods (the image resolution considered for the proposed method could be one of these)\n\nd) Many experimental details are not clear. E.g. which identity loss is used when dealing with no-face data, such as LSUN-Church? Is the face identity loss is still used in eq. 11? How many pre-trained models have been used for multiple attribute control in Fig. 7?\n\ne) Missing references: In section 3.2 \"[...]  With existing works, users often need the combination of multiple models, tricky task-specific loss designs or dataset preparation with large manual effort \". \n\nf) Multiple typos. E.g., Figure 4 (d) instead of Figure 4 (c)\n",
            "summary_of_the_review": "The proposed method shows good visual results and seem to be promising for enabling image manipulation in diffusion models. However, limited novelty, insufficient experimental evaluation & analysis, and lack of details make me evaluate the paper as not ready for acceptance. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}