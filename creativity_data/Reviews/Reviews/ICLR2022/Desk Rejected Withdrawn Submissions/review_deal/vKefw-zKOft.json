{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This is a more empirical-oriented paper, which tackles the existing challenging for variational quantum circuit training. \n\nWhen technical definition and background are very based on the existing efforts on the parameter shift, the author needs to highlight their empirical findings on how the gradient pruning helps to reduce the noisy circuit training. \n\nError bars on the prediction error and accuracy and p-value for current results need to be added in the revised version for fully supporting their empirical claim. \n\nSome of the critical references are missing on random circuit training. \n\n\n",
            "main_review": "## Pros\n\n- The authors provide an empirical method to reduce the noisy degradation from the noisy simulation\n\n## Cons\n\n- The simulation should also consider the noise channel method to provide more comprehensive findings. \n\n- The error bars, p-value are missing in such a empirical paper. ",
            "summary_of_the_review": "This is an empirical finding paper. The results need more careful justification and supports. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "details_of_ethics_concerns": "When the most of paper is clearly written, the ethical section is with some bias to need to resolve. \n\nWhen the authors claim \"Quan-tum computing has the potential to lower the energy and time cost for certain computation tasks and thus reducing the burden of the computing industry to the environment in terms of carbon dioxide emission, energy consumption, etc.\"\n\n\nThere is no evidence to support the carbon dioxide emission for quantum devices compared to the classical one. \nThis really depends on what kind of quantum compter, and most quantum devices need very low-temperature control, which is conflicts with the over-claimed benefits. \n\nBesides, making this claim will easily make the general public not familiar with a quantum circuit-based learning associated with bais. ",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this paper the authors present a method for updating weights of a neural network trained on a quantum computer. The paper is well written and clearly explains the concepts. The paper proposes to iteratively run parametrized quantum circuit to determine values of the deviations due to quantum and data noise, these are then used to determine which gradients are to be used for learning. ",
            "main_review": "In this paper the authors introduce an iterative method for determining the correct value of a gradient for updating a quantum neural network during the training period. The method uses on one hand multiple evaluations of a batch to determine \"bounds\" of the error on individual weights represented by single qubit rotations. The method uses the obtained values through iterative evaluation, measurement and accumulation and the obtained results are then used in a classical computer to obtain the jacobian and the gradients to be back propagated. Finally, the error is back-propagated and parameters are updated. \n\nWhile the paper is interesting in the approach several questions appear.\n\nFirst, the main question is about efficiency and the quantum advantage. While one can argue that the learning on chip is advantageous the number of iterations that one has to do is overly large when compared to the classical approach. Following the authors proposed method, it would be better to train on a classical computer, prune, binarize (quantize the weights) and then simply port it to a quantum chip for high speed processing.  IF this approach is to be used on much larger networks it would cost an at least quadratically more operations than training the model in a classical computer. The proposed approach is not using  any advantages of the quantum computer such as superposition, entanglement or any non-classical acceleration resulting from the usage of this new emergent technology. In addition even if the approach would be comparable in operations, the fact of very large number of initialization, measurement in loops cost an additional overhead making this approach not easy scalable with large number of qubits. Of course when comparing the simulation vs quantum chip, it is natural than quantum chip is more efficient but the technology should be more efficient with respect to classical computers in order to have a quantum supremacy like effect.\n\nSecond, the batch normalization and the scalability to more complex systems would require an exponential larger number of steps. Currently the presented algorithm alters the parameters for a system with eigen values $\\pm 1$. This restricts the model severely and the same scalability issue arises with relatively limited benefits. \n\n",
            "summary_of_the_review": "This paper presents a nice application of quantum computing to to neural networks: an introduction of a method for training quantum neural networks on quantum chip. The main concern is the matching venue and the limited advantages with respect to the relatively high effort of using this method,. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors present a study of practical on-chip QNN training. They describe related work and areas which are relevant for their approach, like Quantum noise and the parameter rule for the computation of the gradients. The authors describe a method, called probabilistic gradient pruning, to overcome the problem of quantum noise and compare their methods to a baseline.\n",
            "main_review": "# Major comments\n- Structure / Clarity:\nThe paper is well structured and written in a clear fashion.\nIn section 3 no references are given. This causes a lack of certainty which of the presented parts are novel and which parts were adopted from other papers.\n- Novelty:\nThe presented work is interesting and include novelty:\nProviding a study of on-chip QNN training is novel.\nComparing different QNN training scenarios (noise-free, real qc, usage of gradient pruning, …) is novel.\nAs stated out correctly in the paper, using QNNs in general is not novel.\nThe parameter shift rule for the computation of quantum gradients is not novel [1].\nUsing gradient pruning in general is not novel [2]. However, in the field of QNNs it seems new and promising.\n- Relevance:\nQuantum computing in general and QNNs in particular seem to be very promising fields in the future. As this work contains some novelty and interesting insights, it was relevant, if the authors would present error bars and statistical information in their experiment section. Unfortunately, all performance values are presented without any error bars. So differences could just arise by chance. The authors should include error bars in their tables and figures (across reruns). The authors should follow best practices and include statistical results, ranking their methods.\n- Reproducibility / Code:\nThe authors provide a link to their PyTorch library for on-chip QNN training to make their approach, including probabilistic gradient pruning, available. Scripts to rerun their experiments out-of-the-box do not seem included.\n\n\n# Minor comments\n- References:\nIn section 1 and 2, the authors include a rich selection of related work. This also includes reference [1]. In section 3 though, not a single reference is given, although the presented ideas are related to previous work. For example, the authors should mention [1] in section 3.1 and work out which part of this section was adopted and which part is novel. There is already work related to gradient pruning [2]. The authors should elaborate if and how existing approaches are related to their proposed method.\n- Quantum basics:\nBasic knowledge about quantum mechanics is given. As QNN training might be interesting for a lot of people in the field of machine learning, including people which are not very familiar with quantum mechanics, the authors might think about adding a part about bra-, and ket-notation and qm operators in the appendix or refer to some information about it to increase the impact of this work.\nFor example, following could be important to be able to understand the formulas in the paper:\nAssuming |e_1> and |e_2> are orthonormal: <e_1|e_1> = 1, <e_1|e_2>=0, …\nAssuming c complex: <e_1|c|e_2> = c <e_1|e_2>=0, <e1|c|e_1>= c<e_1|e_1>=c\n|a> = (a_1, …, a_n)^T -> <a| = (a_1*, …, a_n*)\nTypos:\nSection 2 -  Quantum basics: to story -> to store\n\nReferences:\n\n[1] Crooks, Gavin E. \"Gradients of parameterized quantum gates using the parameter-shift rule and gate decomposition.\" arXiv preprint arXiv:1905.13311 (2019).\n\n[2] Granger, Eric, et al. \"Progressive Gradient Pruning for Classification, Detection and Domain Adaptation.\" 2020 25th International Conference on Pattern Recognition (ICPR). IEEE, 2021.\n",
            "summary_of_the_review": "The paper includes some interesting and novel work. Due to flaws in the experiment section (no error bars, ...) the relevance of this work is diminished. Also, a part of the paper, which is the method ranking part, is considered as not-well supported because this ranking could just arise by chance. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "\nThe submission aims to improve the training efficiency of quantum neural networks. In particular, the key technique proposed in the submission is probabilistic gradient pruning, which can identify gradients with potentially large errors and then remove them. The authors claimed that the proposed probabilistic gradient pruning, which can be treated as a quantum extension of classical pruning methods, can accelerate the training of quantum neural networks, where in each stage, some parameters with small gradients are pruned. Experimental results suggest that the proposed method obtains similar on-chip training accuracy compared with noise-free simulation but has much better training scalability. \n",
            "main_review": "Strength.  \nThe paper is well written. The background of quantum computing and quantum neural networks is well introduced. The experimental results are well exhibited and analyzed.  \nWeakness.  \n• The most critical issue of the submission is its novelty. Specifically, most results presented in the submission, such as the parameter shift rule and the derivation of Eqns. (4)-(5), have been well studied in previous literature. The only contribution of the submission is the so-called probabilistic gradient pruning. Intuitively, at each iteration, only a certain fraction of trainable parameters are selected to be updated. In this way, the training efficiency of QNNs can be improved. Noticeably, this idea can be directly borrowed from the pruning strategy developed in deep learning. Moreover, a similar idea has already been discussed. (see [Sim, Sukin, et al. \"Adaptive pruning-based optimization of  parameterized quantum circuits.\" Quantum Science and Technology 6.2 (2021): 025019.]) \n• Another critical issue of the submission is overclaiming its contribution. The authors claimed that their work is the first experimental demonstration of an efficient and scalable QNN on-chip training protocol. This is definitely not the case! Many studies have also implemented QNN and other variational quantum algorithms on the IBM quantum cloud. The paper misses several critical papers related to QNNs, where on-chip training was performed ([Cerezo, Marco, et al. \"Variational  quantum algorithms.\" Nature Reviews Physics (2021): 1-20; Bharti, Kishor, et al. \"Noisy intermediate-scale quantum (NISQ) algorithms.\" arXiv preprint arXiv:2101.08448 (2021)]). \n• Other issues of the submission are as follows. \no Fix the grammar and typo, e.g., number of qubit-> qubits; \no `Classical simulation has unscalable computational and memory costs.’ This is not the case for the tensor-network-based simulation methods.  See [Huang, Cupjin, et al. \"Efficient parallelization of tensor network contraction for simulating quantum computation.\" Nature Computational Science (2021): 1-10.] for details.\n \n\n",
            "summary_of_the_review": "The novelty of the submission is very limited. Moreover, the paper overclaims its contributions. For these reasons, I score the submission with 3, reject, not good enough. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents the first experimental demonstration of on-chip training of quantum neural networks, they propose probabilistic gradient pruning to address the quantum noise and improve the accuracy of image classification tasks.",
            "main_review": "The reviewer is not familiar with the field of quantum computation, but many problems/methods/techniques are similar to those in other fields. For example:\n\n(1) In the field of neuromorphic computing, the quantum noise is similar to the noise in neuromorphic devices [1], \nthe backpropagation derivations are transformed to inference multiply-accumulate operations (MACs), which is similar to the parameter shift.\nThe probabilistic pruning is also proposed [2] to address the gradient variance problem in the in-situ training scenario. \nBesides, it seems that there are still some operations done on the computer, then the analog-digital/ digital-analog (AD/DA) conversions would be huge bottlenecks in the whole system, which obliterates the superiority of quantum computing.\n\n(2) In the field of network quantization, there are also accumulation/rounding techniques both for inference weights [3] and backpropagation gradients [4].\n\n[1] Ambrogio, Stefano, et al. \"Equivalent-accuracy accelerated neural-network training using analogue memory.\" Nature 558.7708 (2018): 60-67.\n\n[2] Wang, Yaoyuan, et al. \"SSM: a high-performance scheme for in situ training of imprecise memristor neural networks.\" Neurocomputing 407 (2020): 270-280.\n\n[3] Hubara, Itay, et al. \"Binarized neural networks.\" Advances in neural information processing systems 29 (2016).\n\n[4] Wu, Shuang, et al. \"Training and Inference with Integers in Deep Neural Networks.\" International Conference on Learning Representations. 2018.",
            "summary_of_the_review": "Overall, the reviewer thinks that this paper is more suitable for journals such as Nature communications/electronics, the proposed methods/algorithm regarding neural network training are not novel in the deep learning community.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}