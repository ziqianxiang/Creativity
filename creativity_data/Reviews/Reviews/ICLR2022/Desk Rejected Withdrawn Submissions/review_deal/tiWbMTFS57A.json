{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "\nThis paper comes up with a novel theoretical framework that trys to explain the optimization and generalization of (overparameterized) deep neural networks. This paper comes up with the concept of well-conditioned (WC) neural networks, and defines the WC coefficients $\\lambda_i$ to be the norm of the inverse of the Jacobian. This paper then connects WC to Polyak-Lojasiewicz (PL) functions. PL is a function class where gradient descent could achieve linear convergence. This paper shows that PL and WC properties are well-preserved under function composition, thus connecting WC to optimization and generalization in deep neural networks. Based on existing theoretical results about PL functions, this paper designs and demonstrates experiments on real-world deep neural networks. \n",
            "main_review": "\n\n#### Strengths\n1. This paper provides a novel theoretical framework for deep learning theory that connects certain data-dependent properties (WC coefficents) to  properties related to optimization and generalization (PL). \n2. This paper conducts experiments to justify their theory. The mixing theoretical-empirical approach is rare in deep learning theory, but is crucial, and should be encouraged. Remarkably, certain experiments match well with the theory.\n\n#### Weakness\n1. The WC coefficients are closely related to the neural tangent kernel (NTK) theory. I guess the difference may be that the WC coefficients are data-dependent and do not assume gaussian initialization? I would suggest the authors draw more connections between them.\n2. Fig 2. looks too good to be true. The loss convergence bound (e.g., Lemma 1) is a very rough estimate based on e.g., assuming that $L_i$ are the same everywhere, so this should leave a gap between predict loss curve and real loss curve, and thus I would only expect that they are equal up to bounded constant, i.e., $\\frac{\\mathrm{predicted\\ loss}}{\\mathrm{real\\ loss}} \\in [c_1, c_2]$ for some $0 < c_1 < c_2 < \\infty$. However, the plot seems to suggest that $c_1 = c_2 = 1$. Can the authors explain why this should be the case?\n3. The generalization bound in Lemma 3 is defined with respect to the sample size $n$, so I do not understand why Fig. 3 uses epoch as the horizontal axis, and I think it should be better using $n$ instead. Besides, instead of plotting the predicted generalization bound, I would suggest the authors to plot the ratio, i.e., $\\frac{\\mathrm{predicted\\ generalization}}{\\mathrm{real\\ generalization}}$, and show if it is bounded. \n4. Though the authors focus on the overparameterized neural networks, I think there is no difficulties to extend, at least the empirical part, of this paper, to finite-width neural networks, so I would suggest the authors to also show experimental results for finite-width neural networks. It would be good for the community to know whether the results extend to the finite-width case. \n",
            "summary_of_the_review": "\nI think this paper has decent theoretical and empirical novelty, but I have some concerns, so I would currently recommend weak accept.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper attempts to use Polyak-Łojasiewicz theory to make some practical implication of neural networks in terms of optimization. The paper is hard to follow due to readability, in particularly, many notations are not clearly defined. In my opinion, the paper needs some major revision to improve its readability. \n",
            "main_review": "The notation is very hard to follow. It is quite challenging to aggregate the main contribution of the paper from the current version. \nFirst, I am not sure what is the main result? Lemma 5/6? Composition of WC is WC? \nSecond, what new insights we can get from the current theory, in additional to the existing works of overparameterized network (NTK) etc. ? \n\nI get confused from the very beginning due to the notations. Here is an incomplete list of confusing points to me: \n\nSec 2. \n\nNear Definition 1: \n(1)What is $Df_{x_i}$ (range and domain?), what is the relation between $x_i$ and $x$ ? How $D$ is defined (in particular, in Banach spaces, Fréchet derivative?)\n\n(2)What is the difference between $Df_{y_i}$ and $Df_{x_i}$? How is the norm $||Df_{x_i}||$ defined? \n$\\prod_i X_i$ what is the range of $i$, finite or infinite? \n\n(3) The letter $i$ is overloaded in many places! E.g., $(\\mu_i)$-PL (without indicating the range of $i$) and $\\sum_i$. I suggest to use $\\mu$-PL or  ${\\bm \\mu}$-PL to avoid confusion. \n\nNear Definition 2: \nThat is $\\vec x, \\vec y$? Are they the same as $x, y$ in Def 1? What are the norms $|| x_i -y_i||$, are all the Banach spaces have the same norm? What does the square bracket \"[ ]\" mean ?\nLemma 1 introduces a new notation $\\nabla_i$ which seems to be the partial derivative. Then what is the relation between $D$ and $\\nabla$ ?   \n\nBeginning of sec 3;\nI get lost by this sentence \"let $w_S := \\mathcal A(S)$ denote the algorithm’s output on S\", what do you mean by outputs? Do you mean the output parameters, etc? Rather than the output *logits*? \n\nDef 4. \nWhat is $D_i f$, what is $f_w$? I need to guess here. This makes me hard to follow the remaining text. \n$\\lambda_i$ seems to be of $\\sup_w\\sigma_{min}(D_if_w)^{-1}$, where $sigma_{min}$ is the minimum non-zero singular value.  \nWhat is $Y$ here? Above the definition, you use $T: X\\to Y$ to mean a linear functional, which indicates $Y$ is a field, e.g. $\\mathbb R$. However, in the Def 4, you use norm $||y||$ for $y\\in Y$, indicating $Y$ is a general norm space. \n\nLemma5. There are too many to unpack in eq (3) while many of the notations are not clearly defined! E.g., what is $D_{g_{f(x)}}$? I am still uncertain about the notations here after going through the paper for a 3rd time. In addition, are $f(\\vec x)$ and $f(x)$ the same? \n\nThe notation of the equation in Lemma 6 $h(,_{ij} x_{ij})$ seems un-standard to me.\n\nLemma 7. Why $f$ is analytic? When $\\sigma$ is the Relu, $f$ is not analytic.  In Agarwal, there are some normalization assumptions on $\\sigma$, do you need them here? $H^{L+1} =\\hat \\sigma(H^L)$ is not correct, unless some assumptions are met! E.g., the network is infinite-wide! \n\nAgain, near equation (4) we have both $\\vec y, \\vec x$ and $x, y$. Are they the same? In Eq (4), $\\alpha$ is a matrix, which norm do you refer to $||\\alpha||_2$, operator norm , Frobenus norm?\n\nIs there a typo in the equation in Def 6, where the upper index $k$ should be change to some letter with range between 1 and k?  \n\nFor pre-image of a function can be simply written as $T^{-1}y$, which is quite standard. Using $S(T, y)$ makes the notation more convoluted. ",
            "summary_of_the_review": "As mentioned above, the notations of the paper are confusing that I don't think the paper is ready for publication. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper provides the convergence and generalization analyses of gradient descent based on a variant of PL-condition. The main contribution of the study is to propose a Well-Conditioned (WC) function which to verify the constant of PL-condition.",
            "main_review": "[Strengths]\n\nThis study aims to develop a general theory based on PL-condition for neural networks. This sort of analysis is important to give theoretical guarantees for various network architectures rather than the limited class of networks.\n\n[Weaknesses]\n\nThis study builds upon well-known results such as convergence analysis under PL-condition and smoothness, and generalization analysis using algorithmic stability. Hence, the contribution is essentially the verification of assumptions of these results and is rather incremental. \n\nThis paper is difficult to understand because of the ambiguity of notations and lack of detailed explanation about definitions and examples.\n\n- Notations of the derivative are unclear. It is better to provide the definition or comment for $D f_{x_i}$, $\\nabla_i f$ on the Banach space. I guess $\\nabla_i f(x)$ and $D f_{x_i}$ represent the same derivative from the proof Lemma 1. Is $\\nabla_i f(x)$ an element in Banach space that corresponds to Frechet derivative $D f_{x_i}$? In that case, the notation $D f_{x_i}$ is ambiguous because the point $x$ where the derivative is taken is not described.\n- Notation for the point $x$ is also confusing. For instance, $x_i$ seems to be $i$-th element of $x$ and $x_k$ in (2) is a $k$-th iterate.\n- The correspondence between the notations for general theory and the examples of practical neural networks is not provided. Hence, I'm not sure how to apply this analysis to specific neural networks. The lack of concrete examples obscures the importance of the paper.",
            "summary_of_the_review": "Basically, I think this line of research is important, but the paper is not ready for publication because of ambiguity.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work studies generalization and optimization aspects of deep neural networks from the prism of convex optimization, more specifically by treating the loss as a PL function on the domain of weights near initialization (and also near optimality for polynomially wide DNNs). The authors first collected various known results, showing that bounds on and training and generalization accuracy can be made, given that the DNN+loss is a PL function and, importantly, given the smoothness parameters associated with the specific PL function. \n\nThe domain around initialization for which a DNN is PL and these smoothness coefficients depend on architecture details and width. The authors proceed in bounding these coefficients using a notion of Well-Conditioned layer which they introduce. This combined with other results on neural tangent kernels in the overparameterized results, allows them to bound the smoothness parameters based on a layer by layer analysis as opposed to one relying on the overall kernel or the Hessian of the DNN. Finally, empirical results are shown, on deep fully-connected DNNs, showing that despite being well away from the polynomially-over-parametrized region, PL type reasoning still partially holds in the sense that \n1. The training loss estimate at epoch n+1 given an estimate of the PL smoothness parameters at n, fits well the experimental curves within a factor of $2$ throughout raining, and fits very well after roughly 10% training was done. \n2. The lowest gram matrix eigenvalues of the top layer remains constant throughout training and the bound derived from it is highly non-tight. \n\n",
            "main_review": "The theoretical part of the work, while interesting, seems somewhat incremental. Its main contribution seems to be the notion WC and the layer-by-layer estimates they derive for the conditioning (referred to above as steepness parameters above) all of which seem to be relevant in the polynomially over-parameterized region, where various competing approaches exist. This is especially true after half a decade of NTK related results and given that it's quite clear that real-world DNNs operate well away from the regime where NTK predictions hold. \n\nTurning to the experimental side of this work, its purpose is to show that the theory holds away from NTK like regimes. However, the most compelling evidence given, as far as I could tell, that the PL approach generalizes well to finite width is the estimation of the difference in loss between two adjacent epochs. Given that the bound is calculated at epoch $n$ and predicts the loss at $n+1$ following a minor change in weights doesn't feel like a very stringent test to me. In addition, it is done only for a single fully-connected DNNs whereas the main phenomenological difference between kernel methods and DNNs appear for CNNs. ",
            "summary_of_the_review": "The theoretical side is not sufficiently distinct. The experiments are not compelling enough to show applicability to real-world DNNs. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}