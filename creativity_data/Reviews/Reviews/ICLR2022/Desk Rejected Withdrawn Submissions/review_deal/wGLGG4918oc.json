{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper tackles the problem of Rubik's cube solving with as few moves as possible. Inspired by recent successes in reinforcement learning with transformers, a transformer architecture is utilized, leading to the proposed approach CubeTR. A vision transformer is applied to an image-based representation of the Rubik's cube and trained to predict the next action and a pseudo reward, where the latter represents the number of moves until a solved state is reached. An additional move regularization loss is used to incentivize the model to produce pseudo rewards closer to optimal. The authors show results for unscrambling a Rubik's cube with 1000 random shuffles.",
            "main_review": "Strengths:\n1. Investigates transformers in reinforcement learning, which is underexplored\n2. Proposed representation scheme maps 3D Rubik's cube to 2D image, enabling use of vision architectures\n\nWeaknesses:\n1. Confusion over results: CubeTR is not shown in the experiments section. I will assume that \"Beginner\" represents CubeTR going forward, although I'm not sure about this. If this is not true, I would recommend to strongly reject the paper due to no results.\n2. Lack of ablations make insights unclear\n3. Although the introduction says \"[This work] is also the first one to consider higher dimensional cubes\", no results are shown for higher dimensional cubes\n\nAlthough Decision Transformer and sequence modeling is discussed frequently, the proposed method is not actually very similar to these works. CubeTR more closely resembles behavior cloning on Markovian observation with a vision transformer (ViT), and an additional auxiliary value function/reward prediction. This is still an interesting problem, given the successes of vision transformers in traditional vision settings. However, in order to gain insights into RL problems more broadly, it would be important to ablate these contributions: ex. CNN or MLP instead of ViT, the importance of the auxiliary move regularization and pseudoreward losses, comparison to pure behavior cloning, etc.\n\nNamely, I think ablations would support the idea that transformers *should* be used in this problem. On Page 4, it is written that in the Rubik's cube problem \"the past actions and states may not affect future actions significantly\", while long-term dependencies and credit assignment are one of the key strengths of transformers. While the introduction discusses sparsity of reward, the transformer is not actually leveraged to improve the sparsity problem, and based on the results it is unclear if the proposed pseudorewards improve on the sparsity problem.\n\nI'm not familiar with results in the Rubik's cube domain, so I am not well suited to interpret the results, however based on Figure (a) on Page 8, it seems that the proposed method \"Beginners\" requires more moves to solve than the number of moves in the shuffle. I'm also unsure of why the number of shuffled moves in the figures is significantly lower than 1000 in the experimental setup.\n\nDiscussing the method itself, the move regularization loss seems very brittle to me, and hard to imagine that this improves performance. In a similar vein to Decision Transformer/upside-down RL [1], I would suggest a better approach might be to condition on the pseudoreward and then condition on an optimal pseudoreward ($\\alpha$) at test time. Alternatively, a planning-style approach leveraging a transformer similar to a dynamics model (for example [2, 3]) and MCTS or beam search might also be effective, leveraging the predicted auxiliary loss of CubeTR.\n\nIn the related work, it is mentioned that Decision Transformer is \"the first work to propose the use of transformers in reinforcement leanrnig.\" However, this is not true, e.g. AlphaStar [4] uses a transformer. There are many methods which have explored transformers for either imitation learning or TD learning.\n\nIf results are added, the above ablations are included, and the claims are rewritten to support the results, I will increase my rating for this paper.\n\n=====\n\n[1] Rupesh Srivastava et al. 2019. \"Training Agents using Upside-Down Reinforcement Learning\"\n\n[2] Sherjil Ozair et al. 2021. \"Vector Quantized Models for Planning\"\n\n[3] Michael Janner et al. 2021. \"Reinforcement Learning as One Big Sequence Modeling Problem\"\n\n[4] Oriol Vinyals et al. 2019. \"Grandmaster level in StarCraft II using multi-agent reinforcement learning\"",
            "summary_of_the_review": "Given the lack of results and ablations in the paper, I recommend rejection. The claims of the paper are not well-supported by experimental results.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes using a transformer to solve the Rubik's cube. The algorithm generates training data by randomly scrambling the cube and using the reverse action as the desired label. The transformer is then used to predict the sequence of actions needed to return the cube to its desired state. The paper does not give any concrete numbers for how their method performs, but says \"CubeTR, at its current version, may not beat these existing solvers in terms of optimality of the solution.\"",
            "main_review": "Strengths: \n\nWeaknesses:\n\nThe motivation behind the idea of formulating reinforcement learning as a sequence modeling problem is unclear. If the Markov property holds, then there is no need to model it as a sequence. The authors mention this: \"Firstly, instead of some previous state-action-reward pairs, only the present state is passed as the input to the causal transformer. This is because the rubik’s cube is memoryless, and the optimal solution from a particular state does not depend on past states or actions.\" Based on this sentence, all the authors are proposing is a different architecture and to predict the action instead of estimating the cost-to-go. This is different from the DeepCubeA algorithm that used a residual neural network and approximate value iteration to estimate the cost-to-go and then used that network as a heuristic function for A* search. However, swapping in a transformer architecture is not worth publication unless there are dramatic improvements (which there are not).\n\nFurthermore, predicting the reverse action of a randomly scrambled cube will lead to many unnecessary inefficiencies, as noted by the authors: \"There are cases where the state generation algorithm would explore redundant moves, e.g. R R R, instead of R’, and the performance could be improved considering these cases.\"\n\nLastly, the authors do not give any concrete results but say that their results are not as good as other methods in terms of optimality of the solutions.\n\nMinor comments:\nThe state representation can lead to ambiguity due to averaging sticker colors on a single cubelet. A cubelet can be in multiple orientations and I do not see how the orientation can be recovered after averaging.\n\nI believe the number of configurations for the 4x4x4 cube is known. This is different than \"God's number\", which is not known for the 4x4x4 cube. \"God's number\" is the longest shortest path possible.\n\n4x4x4 is not \"higher dimensional\" compared to 3x3x3. Instead, one can say it is of greater length.",
            "summary_of_the_review": "The paper does not propose any novel methods and the results are not explicitly shown, but the authors do allude that they are not as good in terms of optimality.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a new method \"CubeTR\", based on Transformers, to solve the sparse-reward Rubik's Cube problem. The paper claims to be able to generate solutions very close to those given by algorithms used by expert human solvers. Unfortunately, there is very little experiment result on the proposed method.",
            "main_review": "There is very little experiment result or meaningful analysis for the proposed CubeTR method. Table 1 contains performance numbers from prior works and a row called \"Beginner\", which seems to refer to CubeTR. The writing is severely confusing. \n\nFurthermore, Rubik's Cube can already be efficiently solved by tailor-made algorithms. It is neither a practically useful problem nor a widely accepted RL benchmark. It is also unclear how this method can generalize to other sparse reward problems without any experimental results on other popular benchmarks. ",
            "summary_of_the_review": "Very little experiment results or analysis of the proposed method is given. The writing is severely lacking. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper propose to solve Rubik's cubes using a decision-transformer like approach. However, instead of using the transformer for memory, since the Rubik's cube is fully-observable and markov, the authors use it to process the input state representations. Specfically, they represent the input state in a flat, image-like representation, which is processed by the transformer. They point out that a similar representation could be used for different-sized cubes. They use pseudo-rewards to avoid the challenge of sparse rewards, and show some performance on an experimental evaluation.",
            "main_review": "\n* Reporting methods, experiments, and results *\n------------------\n\nThe minimum standard for an academic paper should be that it clearly communicates 1) the methods used to obtain the results, including details of the architecture and training, and 2) the methods used for the experiments, including details of training and evaluation settings, baseline comparisons, etc. This paper needs clarification of most of these fact\nors. For example: \n\n1) Architecture details: the paper does not present sufficient detail about the design of the architecture. The authors repeatedly say that the architecture is \"like a vision transformer,\" but this is far from enough specificity to reproduce the results. The vision transformer paper compared a variety of layer sizes, hidden sizes, number of heads, patch\nsizes, etc. Which particular settings did the authors use? How many layers are the MLPs that predict the \"pseudo-rewards\" and the values.\n\n2) Training details: The authors do not thoroughly describe the training process. For example, the training distribution---they say that \"CubeTR starts from the solve state, and makes moves from here to explore the state space.\" In what way does the model make moves? This phrasing suggests that the model is using its learned action/reward estimates to make the moves, but this would result in very poor exploration if the goal is to reverse the task (so as to explore far from the solved state). Or are the reverse action/value estimates used? Or are the moves actually randomly chosen? Furthermore, how long does training proceed? What optimizer is used, with what hyperparameters? Etc.\n\n3) Experiment & baseline details: The paper does not thoroughly describe the experiments or clearly report results. The results in Table 1 do not include CubeTR, however they inc\nlude a row entitled \"Beginner,\" which may be CubeTR, given that references are provided for all other rows. It would be worth providing more detail about these statistics, at least for the present results (e.g. a histogram of all the solve lengths). The authors apparently performed experiments with varied scramble lengths; these are never mentioned in th\ne text but appear in the unlabeled figure. All figures should have a proper caption, should be referenced in the text, and the corresponding experiment should be described.\n\n4) Larger cubes: the authors frequently state that they are the first work to \"consider\" \"explore\" or \"extend to\" higher-dimensional cubes. However, they never actually perform any experiments in such a setting as far as I can tell! The fact that the proposed input representation could in-principle allow higher-dimensional cubes is not sufficient justification to emphasize this point so heavily. Where this is mentioned, it should be explicitly labelled a future direction unless experiments demonstrating it are presented.\n\n\n\n\n*Experimental evaluation*\n----------------------\n\nBecause of the reporting issues noted above, it is difficult to tell what experiments were even performed. However, under any interpretation the experiments are not up to the standards of an academic publication. At a minimum, there should be some further analysis of factors e.g.:\n\n1) ablation studies that remove particular losses (for example, learning just from behavioral cloning of actions without using \"pseudo-reward\" estimate and loss, demonstrations of the value of the move-regularization loss, etc.)\n\n2) Analysis of the performance of the system on higher-dimensional cubes.\n\n3) Analysis of different architectures or sensitivity to hyperparameters.\n\netc.\n\n\n\n*Background literature*\n----------------------------\n\nWhen revising this article, it would be useful to perform a more thorough review of the prior literature. This article frequently includes statements that substantially misattribute the novelty of ideas in prior work. For example:\n\n1) \"The use of transformers in core reinforcement learning was first proposed by Chen et al. (2021).\" and similar claims are repeated in several places. Transformers have been proposed as memory for RL agents by many papers prior to decision transformers, especially on problems with sparse rewards e.g. Parisotto et al., 2020; Parisotto et al., 2021; Lampinen et al., 2021. Furthermore, many prior memory architectures for RL in sparse reward tasks used attention operations that are based upon transformer-style key-query-value processsing, e.g. Wayne et al., 2018; Fortunato et al., 2019; Hill et al., 2020. Finally, transformers-style attention has been used in input encoding (as the current work does) for several years as well, e.g. Mott et al, 2019. The authors should more thoroughly review and incorporate some of the prior literature on Transformers in RL, in order to situate their work better within the broader context.\n\n2) The paper makes a similarly inaccurate claim that \"In GPT [...] the process of generative pretraining was first proposed.\" The idea of pre-training with next word prediction goes back at least as far as Dai (2015); and the more general principle of generative pretraining goes back at least as far as Hinton et al. (2006a;2006b). The authors should be careful about the accuracy of claims of primacy for prior work.             \n\n3) What the authors call a \"pseudo-reward\" is what the rest of RL would call a temporally discounted value estimate. The authors approach would be clearer if stated in standard RL terminology (see e.g. Sutton & Barto, 2018).\n\n4) There are a number of other omissions, for example Akkaya et al., 2019 is certainly relevant as a prior use of RL to solve Rubik's cubes in the real world, and solves it despite the sparse rewards, without introducing psuedo-rewards (if I recall correctly), unlike the present work. It would be useful to include this in the literature review.\n\nIn summary, the literature review should be much more thorough.\n\n\n*References*\n-----------------\n\nAkkaya et al., 2019: Solving Rubik's Cube with a Robot Hand (https://arxiv.org/abs/1910.07113)\nDai et al., 2015: Semi-supervised sequence learning (https://papers.nips.cc/paper/2015/hash/7137debd45ae4d0ab9aa953017286b20-Abstract.html)\nFortunato et al., 2019: Generalization of Reinforcement Learners with Working and Episodic Memory (https://arxiv.org/abs/1910.13406)\nHinton et al., 2006a; A Fast Learning Algorithm for Deep Belief Nets (https://direct.mit.edu/neco/article-abstract/18/7/1527/7065/A-Fast-Learning-Algorithm-for-Deep-Belief-Nets)\nHinton et al., 2006b; Reducing the Dimensionality of Data with Neural Networks (https://www.science.org/doi/abs/10.1126/science.1127647)\nHill et al., 2020; Grounded language learning fast and slow (https://arxiv.org/abs/2009.01719)\nLampinen et al., 2021: Towards mental time travel: A hierarchical memory for RL agents (https://arxiv.org/abs/2105.14039)\nMott et al., 2019: Towards Interpretable Reinforcement Learning Using Attention Augmented Agents (https://arxiv.org/abs/1906.02500)\nParisotto et al., 2020: Stabilizing Transformers for Reinforcement Learning (https://arxiv.org/abs/1906.02500)\nParisotto et al., 2021: Efficient transformers in reinforcement learning using actor-learner distillation (https://arxiv.org/abs/2104.01655)\nSutton & Barto, 2018: Reinforcement Learning: An Introduction (http://incompleteideas.net/book/the-book.html)\nWayne et al., 2018: Upsupervised Predictive Memory in a Goal-Directed Agent (https://arxiv.org/abs/1803.10760)",
            "summary_of_the_review": "The idea of using transformers to solve the Rubik's cube is interesting, and the author's input representation format and architecture approach seems somewhat novel. However, the paper has a number of major issues which limit its value at present; it is well below the minimum standards of a conference publication in many ways. I outline these here, but see the detailed comments for more information.\n\n1) Experimental evaluation: the authors do not provide sufficient experimental evaluation to demonstrate their approach, and do not communicate exactly what the experiments or re\nsults were.\n\n2) Reporting: the authors do not report nearly enough information to even understand how the architecture is working, let alone to reproduce or build upon the work. \n\n3) Literature: the work is not well situated within the prior literature; indeed it often makes highly inaccurate claims about prior work.       \n\nI think this work could be a valuable contribution if these issues were addressed; but this might require running many additional experiments and entirely rewriting the paper.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}