{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper tackles the task of Few-Shot Counting. In particular, it proposes VCN which learns to augment the existing training data along\nwith learning to count. The counting net is the same as FamNet.",
            "main_review": "+ The paperis well organized and the main point is clearly illustrated. \n+ The proposed data augmentation method seems effective for few-shot counting.\n-  From the structure of counting subnet, VCN almost completely refers to FamNet, such as the process of multi-scale feature maps, using the features from the third and the fourth convolutional blocks for obtaining the multi-scale representation. \n- The title \"Vicinal counting network\" implies a novel counting framework,  but the work mainly focuses on data augmentation. \n- The flow chart of the network in Fig.2 should be more informative to let readers quickly understand the architecture, rather than being a black box.",
            "summary_of_the_review": "The improvement of this work is limited and the method is not novel enough.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposed a few shot object counting method built on FAMNet architecture,  that aims to address the issue of scarcity of annotated training data for object counting.  The proposed method learns to augment the existing training data along with learning to count in a cooperative manner. ",
            "main_review": "Strengths:\n\n1. The paper addresses the problem of not having enough training data for object counting. \n2. It proposes a few-shot object counting method, which is less studied in the literatuere. \n3. The proposed method acheives superior results compared to existing methods. \n\nWeakness:\n1. The paper has limited novelty. The paper uses an existing counting network (FAMNet) and introduces an additional generator network that creates transformed versions of the training images.  The idea of using augmentation networks is well studied in the literature and  [1*] and the advantages of using synthetic data for crowd counting are also demonstrated in [2*].\n2. Although the proposed framework has a reconstruction loss term, it does not have an explicit discriminator network to ensure that the generator's output matches the data distribution of the real images. This may deteriorate the visual quality of the generated images and the overall performance.  \n3. The  majority of the performance obtained by VCN over FamNet  can be attributed to the use of superior baseline. For example, VCN baseline (MAE 21.14 from Table 3-no augmentation),  VCN (20.24 from Table 1), and FamNet (23.75). \n4. Table 5: Why the results were not reported on Shanghaitech PART-B?\n5. Why the proposed generator is particularly suitable for counting task? Is it possible to extend the propsoed approach  for other related tasks such as semantic segmentation?\n\n\n[1*]  Learning from simulated and unsupervised images through adversarial training, CVPR 2017. \n\n[2*] Learning from synthetic data for crowd counting in the wild, CVPR 2019\n\n",
            "summary_of_the_review": "Although the paper has some technical contributions, the main limitation of the paper is its limited novelty. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This submission proposed a learned augmentation method to improve counting accuracy. \nSpecifically, the proposed method is an auto-encoder utilizing random noise vector (latent z), mapping net, and AdaIN (similar to StyleGAN) to augment (distort) the original input.\n\nThe effectiveness of the proposed method has been verified on FSC147 (few-shot counting) and three commonly tested crowd counting datasets. Apart from density-based counting methods, few-shot object detectors have been tested on FSC147. \nIn the ablation study, other commonly used augmentation strategies has been compared with on FSC147, and the effectiveness of the loss terms have been verified.",
            "main_review": "### weakness\n\n#### Major:\n- The method is not very novel as stated in the previous section.\n- The proposed method is more related to augmentation techniques and more in-depth analysis (also see my first concern in Minor) could significantly improve this work. So it would be better to test those augmentation strategies on Table 5 too.\n- I do not think this should be classified as a few-shot counting method and should be compared with more thoroughly studied counting methods. \n- As a few-shot counting method, suitable tracking/semi-supervised video object detection methods should be compared due to the existence of \"Feature Correlation Module\" (along with few-shot detectors).\n- losses in Eq. 3 and Eq. 4 look weird to me. Basically, these two losses encourage similarity in image space and dissimilarity in feature space. Namely, A one-to-many mapping is encouraged here.\n\n#### Minor:\n- Fig. 3: The augmented images look like their color/lighting distorted version. Maybe this is why \"PCA lighting variation\" (in Tab. 3) gives very similar accuracy as the proposed VCN. BTW, \"auto augment\" and \"manifold mixup\" also report similar performance. Any insight on why \"PCA lighting variation\" is better than \"auto augment\" and \"manifold mixup\"? Any insight on the relation between the distortions brought by VCN and \"PCA lighting variation\"?\n- Some descriptions/illustrations are not clear enough and harm understanding.\n- Fig.2 could use more information to facilitate understanding. For example, mark Q, w, add Feature Correlation Module, etc.\n- use \\citep{} where appropriate.",
            "summary_of_the_review": "Please refer to Main Review.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents Vicinal Counting Networks (VCN) for Few-Shot Counting. VCN learns to augment the training data, in a cooperative setting. Extensive experiments on few-shot counting, crowd-counting show the effectiveness of the proposed method. \n",
            "main_review": "Pros:\n\n1.\tThe experimental results are good. It achieves the state-of-the-art performance on few-shot counting.\n2.\tThe paper reads smooth and is mostly clear. \n\nCons:\n1.\tIt is unclear why the augmentation Generator can be trained with the Regressor in a “cooperative setting”. The Generator pre-processes the image for the Regressor. If the Generator and the Regressor are trained cooperatively, the Generator tends to make the input image much easier for the Regressor to use. During training, the Regressor only sees easy samples. But during evaluation, the Generator is discarded. So the input to the Regressor becomes difficult. The distribution of the training samples and the testing samples are different. Especially, the testing samples are much more difficult. How can the Regressor generalize?\n2.\tPlease add more analysis/discussion/experiments about the effectiveness of the “cooperative training” versus “adversarial training”. \n3.\tThe novelty is somewhat limited. The idea of learning to augment has been widely explored in literature. Most of them apply adversarial training, which is the main difference. But the effectiveness of the “cooperative training” is not well validated.\n4.\tJust a suggestion: Figure 1 does not convey more information compared to Figure 2. You may consider deleting it. \n\n5.\tThe paper needs careful proof-read. There are some typos, e.g.\n(1)\tIn Page7, “aslo”-> “also”\n",
            "summary_of_the_review": "In summary, the main claims of the paper are not well-validated. Please add more discussion/analysis on the \"cooperative training\".  \n\nIn my opinion, the novelty of the paper is also somewhat limited. Please add more discussion and highlight the main contributions/difference compared to existing approaches.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}