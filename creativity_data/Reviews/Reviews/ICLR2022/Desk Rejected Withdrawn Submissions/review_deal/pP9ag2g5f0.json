{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors argued that the phenomenon of \"interleaved connections\" causes weight sharing NAS methods failed when the search space contains long-distance connections. Then they propose a sampling method, which excludes all but L predefined interleaved-free connection patterns of a super-network and takes turns to train the L sub-super-networks derived from these patterns. This method outperforms the baseline method when all long distance connections are considered during super-network training.",
            "main_review": "Strengths\n\nThe proposed sampling method reduces a complex search space into a smaller (in terms of connections): only L fixed networks to sample. In the reduced search space, the networks obtained at the end performs better than those from the original search space.\n\nMajor Issues\n\nI'm not sure search spaces with long distance (>=4) connections are superior to those which don't. Potentially, super-networks with more candidate connections, with proper optimization strategies, can produce better network architectures. However, this is not supported in the experiment. The authors only compare IF-NAS with DARTS, PC-DARTS and GOLD-NAS under the case of long distance connections in Table 2. PC-DARTS and GOLD-NAS, without long distance connections, both outperform IF-NAS with L=4, 6 and 8, using similar FLOPs.\n\nThe L connection patterns resemble the search space proposed in Dense-NAS which actually allows interleaved connections. Dense-NAS is able to achieve comparable performance on ImageNet with much less FLOPs. It seems contradicts with the authors' argument in this paper.\n\nOther Issues\n\nThe definitions of interleaved connection in this paper are not consistent with each other. There are three places where interleaved connection is defined: one at second paragraph of section 1, two at second to the last paragraph of section 3.3 (\"Mathematically, ...\" and \"Intuitively, ...\"). \n\nI don't think the analysis about interleaved connections in section 3.3 and figure 1 are convincing enough. It is normal that when more connections (interleaved or not) are present, the distribution of the importance weights of some connections (e.g., pre-1,3,6) will change. \n\nFigure 4 is confusing. Exactly which side is w/ IF and which side is w/o IF? It seems that both sides have interleaved connections.",
            "summary_of_the_review": "The paper failed to justify that the proposed sampling method is effective. It beats some baselines under the case of long distance connections but its performance seems not as good as those without long distance connections and some with interleaved connections.\n\nI don't think the paper has enough value to be accepted given its current status. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a new search space for neural architecture search, which extends the cell-based search space by adding the long-range connections in the super-network. To resolve the problem caused by the introduced interleave connections, the search space is constrained by an iterative interleave-free sampling within some partitioned groups sub-super networks of the search space. The proposed search space can be combined with existing gradient-based search algorithms with scheduled interleave-free sampling strategy towards better generalization performance on benchmarked image classification tasks.",
            "main_review": "Positive points:\n1.\tThe paper is well-organized with good motivation on improving an important NAS drawback of the cell-based search space.\n2.\tThe idea of extending the cell-based search space with interleave-free connections is novel and the effects of the interleave connection to NAS especially gradient-based search algorithm analyzed in the paper is interesting and worth exploring.\n\nConcerns:\n1.\tMy main concern is that the time complexity of the search algorithm is not well-analyzed in the paper. How the extension of the search space affects the search time compared to using the small search space to achieve the comparable accuracy? Does the scheduled iterative interleave-free sampling provide extra burden on the search time and memory cost? \n2.\tSince the designed search space has much inductive bias based on the human prior knowledge and understanding, it would be great to add more experiments and ablation studies for the unsearchable part of the search space. For example, the candidate operations selected to create the search space (besides the two mentioned in the paper: 3 by 3 convolution and skip connection), the learning rate, optimizer selection, warm-start strategies as well as many ad-hoc specified ones in the training and search settings. Also, it would be great to extend the method to more diversified tasks beyond the image classification on the cliché benchmarks.\n3.\tMinor comments: though most of the figures make the presentation easier to follow, some parts could be improved such as: in figure 3 the activated node and inactivated node could be better discriminated (the dashed inactivated node is not apparent enough especially when printed out); in figure 1, using the color may not be a good way to discriminate lines in the figure and also the illustration of the interleaved connection is not very straightforward given the candidate connections. (I’m confused at the beginning about why the purple and green lines at not treated as interleaved connections)\n",
            "summary_of_the_review": "Based on the above concern described above, I currently tend to weakly reject the paper but would like to hear more feedback from the author and discussion from other reviewers towards the final decision.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work proposes a complex searching space with many long-range skip connections. Existing weight-sharing search methods are found to not work well on this search space due to the effect of interleaved connections. They tend to generate shallow network structures which shows worse capacity than deeper ones. The authors propose an algorithm to only sample non-interleaved connections at each iteration. The method achieved better performance on DARTS, Gold-NAS, and the proposed new search space compared to existing methods.",
            "main_review": "Strength:\n-\tThe proposed searching method to deal with interleaved search space is effective. It outperforms traditional differentiable searching method on ImageNet dataset under different settings (different precursor numbers, different search spaces etc)\n-\tThe writing of the paper is clear. Authors provide intuitive samples and useful ablation studies to show the effect of the interleaved connections on the search quality.\n\nWeaknesses:\n-\tAlthough the authors show the interleaved connection affect the search performance. Within a complex searching space with long-range skip connections, it is still unclear why it would have this effect. Since training a normal network with interleaved connection such as the DenseNet will not cause any problem, the issue seems to only relate to the differentiable weights associate with searching method. Why this kind of connection pattern will affect the learning of differentiable weights is still not clear. Hopefully more theoretical analysis can be done towards this direction.\n-\tFigure 4 seems to be problematic. The title said the left are with IF and right and without, but the subtitles are the opposite. \n",
            "summary_of_the_review": "This work presents an interesting scenario of a macro search space with long-range skip connections where traditional differentiable method cannot work well. It proposes a subnet training method to only training the model with non-interleaved connections at each iteration and alleviates the problem. Experiments are solid. Lean to accept.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper focuses on the macro space based rather than a cell based neural architecture search. It first showed that existing weight-sharing search algorithms mostly fail due to the existence of interleaved connections, and then proposed an interleaving-free NAS (IF-NAS) to avoid the interleaved connections. ",
            "main_review": "This paper breaks the limit of cell-based structure in the existing NAS method and focuses on the more general macro search space. And the authors finded that interleaved connections is a key issue in the macro search based NAS, and proposed an interleaving-free NAS. Experimental results verified the effectiveness of the proposed method, which outperform both random sampling and previous weight-sharing search algorithms by a significant margin. However, this paper lacks an in-depth analysis of why interleaved connections prevent the performance of NAS. In addition, when search on a macro space, memory requirement is an emerging issue as we can not search on a small backbone and them transfer to a larger one with stacking more cells as DARTS.",
            "summary_of_the_review": "I generally like this idea, while the paper lacks an in-depth analysis and the experimental results can hardly convince me. I believe this paper is marginally below the acceptance threshold of ICLR.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}