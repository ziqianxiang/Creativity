{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents an evolutionary heuristic approach for Pareto front lerning through a hypervolume based objective. The approach is used for multi-task learning and compared against a few related approaches in similar framework. ",
            "main_review": "The paper aims to formulate the Pareto front learning problem as an alternative multi-objective optimization problem and solve the same by maximizing the hypervolume based objective with an evolutionary heuristic (in spirit, similar to the evolution strategies). The problem is well within the scope of this conference and the paper is, more or less, well organized. However, I have the following concerns that currently prompt me to not recommend an acceptance for this paper in a conference like ICLR:\n\n1) Many statements and claims in this paper appear without a solid and insightful theoretical treatment. The same applies for the preference conflicts where the degree of such conflict or the weak and strong Pareto relationships were not explored analytically. \n\n2) Given the previous evolutionary approaches to PFL, I am not convinced about the degree of novelty of this work.\n\n3) The convergence of the proposed SEO has not been established theoretically. For a conference like ICLR, I think some sort of theoretical guarantee should be provided to such an empirical and ad-hoc procedure. \n\n4) What about the theoretical time and sample complexity of the training process when it comes to a practical multi-tasking instance? I did not see any such analysis.\n\n5) Why the results provided in Table 2 were not validated on the basis of a non-parametric statistical hypothesis testing or more betterly through a Bayesian post-hoc analysis? As we all know, evolutionary algorithms, due to involvement of stochastic factors of different degrees, may give good results in aone of the runs as a matter of luck! Also, how we may conclude that the provided results are really significantly better than the existing state-of-the-art?\n\n6) Why no non-PFL based or non-evolutionary state-of-the-art approach to MTL was not included in the comparative study?\n",
            "summary_of_the_review": "Due to lack of theoretical guarantees and inconvincing experimental analyses, I regret that I cannot give a green signal to this paper for a highly reputed conference like ICLR. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a variant of the Pareto Hypernet (PHN) for multi-objective/multi-task learning. The PHN is parameterized by a preference vector, and this work proposes to sample various preference vectors using Evolutionary Strategy (ES) during training in order to improve the hyper volume of the solutions. \n",
            "main_review": "Pros:\n- Multi-objective/multi-task is an important problem that deserves more attention. \n\nCons:\n- The technical contribution appears somewhat limited to me. It is simply a sampling of preference vectors during training. I think the \"proposed algorithm\" of SEO is basically the same thing as the existing Evolutionary Strategy (ES). \n- The paper writing can be improved. Especially equations and symbols can be made more consistent, in order to make it easier to read.\n- It is not surprising the results in hyper volume are better, and I am not sure how significant it is. This is because the proposed method is trying more preference vectors in a concerted fashion, as opposed to randomly. The tradeoff in terms of training time should be more clearly quantified in figures (rather than just noting 10% increase... which I find difficult to believe).\n\n\nDetailed comments:\n\nSec 3: Definition of domination following Navon 2020 is incorrect. Please correct the typo. \n\nSpelling: \n- Existing/Related \"work\" rather than \"works\"\n\nEquation 3 confused me somewhat, when compared to Equation 2. One looks like a point in m-dimensional space while the other is a loss function. Also, are the L(\\theta,r^1) in Equation 3 related to the \\bar{L}_{PFL}(\\theta) in Equation 2? Similarly, in Equation 4, it says that L_r\\prime is similar to L_r but I don't see how exactly L_r relates to previous equations. It will be helpful if the symbols are more consistent. Currently there are many similar symbols and the reader is left to infer their relation.\n\nSec 3.1 says Fig 1(a) shows the PHN results of two preference vectors, but there are three curves. \n\nI don't understand the discussion of \"preference conflict\" in Sec 3.1. The field of multi-objective optimization exists because the objectives may be conflicting. So defining a new term \"preference conflict\" seems redundant. Are you saying something more than that?\n\nIt might be better to define Linear scalarization after Eq 2, so we have a sense what g() is, rather than around Eq 4. \n\nThe proposed SEO in Algo 1 looks like it is basically Evolutionary Strategy (ES), in particular CMA-ES. Why give it a new name? \n\nTe description of sparse sampling in Sec 4.2 can be cleaned up. First it says quantization is applied to x, which is the symbol for the inputs; then it says it is applied to r which is the draw from the Dirichlet. I thought you are quantizing the preference vector, which corresponds to \\phi in Algorithm 1? ",
            "summary_of_the_review": "The paper proposes a variant of PHN where preference vectors are tried in accordance to an hypervolume objective as opposed to randomly. I think the technical contributions are somewhat limited/incremental and the paper writing could be improved. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel technique for Pareto front learning (PFL). One of the main motivation of the proposed approach is summarized in Section 3.1. To mitigate the issue described in Section 3.1, the authors introduced the technique called sparse sampling. Moreover, to optimize the hyper-parameters and to train the network simultaneously, the authors proposed the search algorithm called self-evolutionary optimization (Algorithm 1). The performance of the proposed approach is compared with existing PFL approaches on several multi-task learning problems.",
            "main_review": "Strengths:\n* A novel but simple search strategy\n* SOTA performance, compared to recent approaches.\n* Generally well written and easy to follow\n\nWeaknesses:\n* STD or any other deviation metrics are not shown for the performance comparison. Therefore, it can not be judged whether the performance difference is significant, e.g., on Table 2. \n* Some parts are not clear enough. For example, what is the numbers in Tables 2? It should be mentioned in the caption of the table. The idea of the sparse sampling is understandable, but it was not clear enough for me to rigorously understand what is meant by the last expectation in Section 4.2 (because the expectation is taken for r, but r does not appear inside expectation.)",
            "summary_of_the_review": "Based on the above main review, this paper contains novelty and significance. However, some parts could be improved, mainly presentation aspect, for clarity and for better understanding of the statistical significance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose new method for pareto front learning based on evolutionary algorithms. In addition, a new network architecture named self evolutionary pareto networks (SEPNet) is introduced by the authors.\n\nThe authors claim the following contributions:\n- Demonstrate that PFL can be interpreted as another MOO problem comprised of conflicting objectives determined by the preference vectors with a goal of maximizing HV.\n- Self evolutionary optimization (SEO) -> jointly optimizing hyperparameters along with model parameters.\n- SEPNet unified model that approximates the entire pareto front along with direct HV optimization.",
            "main_review": "**pros**:\n- The paper is well written and organized well.\n- Good insight about the preference conflict in PFL.\n\n\n**cons**:\n - Limited novelty - According to my assessment, the proposed ideas of SEO and SEPNet appear to be a combination of previous works, and the evolution of this paper is not satisfactory. It would be helpful if the authors could emphasize how it differs from previous works. \n\n- Optimization using validation data - In an applicative view what if you don't have any validation data? or it is limited? I think a good experiment would be to partition the training data into 2 shards, one will be used for model parameters optimization and the second for $\\alpha, \\lambda$ optimization without using validation split at all.\n\n- Sparse sampling - It will be a good addition to perform ablation study to compare the proposed sampling method to other discrete sampling methods. \n\n- Selection of hyperparameters (HP) - In Sec. 5.1 it is stated: \"Unless otherwise stated, we use Adam, a batch size of 256 and a learning rate 1e-3\". And at the top of paragraph 2: \"For our SEO, we use σ = 0.1 and population size n = 10 for the ES algorithm, and fix the learning\nrate to 0.01.\". It is not clear how did the authors selected the HPs for the baselines and for SEPNet. Furthermore, for some experiments a fixed number of epochs were used, what was the stopping criteria? \n\n- Lack of STD analysis - The authors should provide an STD analysis for their experiments (i.e. running them on multiple seeds)",
            "summary_of_the_review": "Some questions raised during the reviewing process, I strongly recommend the authors to address those. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}