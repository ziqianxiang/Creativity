{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "\n**Introduction**\n\nThe authors outline the overall problem that this work seeks to address: \"recent studies have observed that data/sample inefficiency severely impedes its performance when learning from high dimensional observations (Lake et al., 2016). \"  And further their statement of purpose: \"Accordingly, elevating data efficiency is of paramount importance for the broader progress of deep RL.\"\n\nA case is made for augmenting RL w/ Self Supervised Learning techniques: \n 1.  Sole use of sparse rewards to learn an encoder is not enough \n 2. Semi Supervised Learning makes good use of data for down-stream tasks\n\nThey state that effective representations for RL i) encode the input features well and ii) are consistent over time.  They claim that a third property is required for good state representation, iii) the capacity to accurately predict future states or observations and that this will form a portion of this work. *\"Though considerable effort has been dedicated to testing hypotheses (i) and (ii), the explorations for hypothesis (iii), to our best knowledge, are still rare.\"*\n\nThe contributions are summarized:\n* Propose PCR for representation learning in RL\n* Demonstrate PCR exceeds state-of-the-art on DeepMind Control benchmark (DMC) performance.\n* Via ablation study show demonstrate necessary components of predictive consistency representations.\n\n**Background**\n\nPCR builds upon Soft Actor Critic (SAC, Haarnoja et al., 2018): *\"In this framework, the actor aims to maximize expected reward while also maximizing entropy—that is, succeed at the task while acting as randomly as possible\"*.   This is further extended with augmented data via RAD-SAC (Laskin et al., 2020).\n\n**Method**\n\nThis consists of training online and target encoders that are shown in figure 1 of the paper.  One step transitions are used to train the encoders: ((ot, o't), a_t, r_t,(o_t+1), where o'_t is defined as an augmentation of o_t. The target encoder is conditioned on the original observations, current and next step, while the online encoder depends on the augmented observation.  Encoder structures are the same but parameters are not shared. Target parameters are an EMA of the online parameters which are optimized by gradient descent.  They also define a dynamics model to predict future states conditional on action and which is optimized over an L2 loss between latent predictions of the online and target models.  Predictive consistency is defined to be when two latent predictions of different next step views agree to within an arbitrary distance of each other given some distance metric.  They propose a an L2 loss here also over the target latent and a prediction of the online latent will enforce predictive consistency across different views of the data.  \n\n**Experiments**\n\nThey use DMC @ 100K and 500K steps of training.  PCR is build on top of RAD. Online and target encoders are 4-Layer CNNs.  BatchNorm applied.  Dynamics model is a two layer MLP.\n\nBaselines: *\"For a fair comparison, we take as baselines the previous state-of-the-art algorithms that\nalso focus on data-efficiency: SLAC (Lee et al., 2020a) learns compact latent representation through\na forward model; CURL(Laskin et al., 2020a) learns a contrastive representation of the observations; SAC+AE (Yarats et al., 2021) introduces an auxiliary task of observation reconstruction; DrQ (Yarats et al., 2020) uses both data augmentation and weighted Q-objectives. All the above four methods are all built upon SAC (Haarnoja et al., 2018), a simple yet efficient algorithm for learning\nwith state-based inputs. \"*\n\nPCR is evaluated against six DMC tasks: Cartpole, Cheetah, Ball in Cup, Reacher, Walker and Finger. PCR reaches the state-of-the-art performance on the majority of (4 out of 6) DMControl100k and DMControl500k.\n\nTwo ablation studies are performed for PCR comparing to settings where 1. The consistency loss is removed and 2. A contrastive loss is used instead of a predictive loss.",
            "main_review": "**Strengths:**\n\nGood grounded example on learning robotic grasping from a dataset in contrast to how humans do it. This makes a good case for the need of data efficiency in RL for control.\n\nThe authors formalize their ideas around predictive consistency with some theory in section 4.3.  This provides a more solid justification for their overall approach and confidence in the loss they've chosen to optimize over.\n\nOverall I like the ideas around breaking the problem up into predictive and consistency components.  I think a lot of the novelty of this works derives from this and the authors do well to justify the choice of the approach and why this is worthy of exploration.\n\nThe ablations in 5.3 demonstrate well the importance of the consistency loss and the choice of a predictive loss over a contrastive loss.  This is useful to know especially in scenarios where it isn't always clear whether a contrastive or reconstructive loss will confer advantage.\n\n\n**Weaknesses:**\n\nIn this work there is primarily a single environment against which these methods are measured , that is DMC (Tassa et al., 2018).  It seems that to in general demonstrate the effectiveness of PCR it would benefit from at least another dataset or environment setup.  The ideas here seem to me to apply to other types of RL tasks involving navigation, planning and behavioural control.  I believe that it would help the claims being made by showing this.\n\nIn the conclusion it is stated: \"we presented Predictive Consistency Representation (PCR), a self-supervised representation learning algorithm that significantly improves data efficiency for RL agents with visual inputs\" - improvement was only observed in some of the tasks for DMC.  I think that we need more evidence to make this claim.  Overall it would have been nice to see more conclusive results against the baselines.\n\nThere is relatively little description of the setup in section 5.1.  In particular, I'm looking for a bit more detail around how the data augmentation is done and the training examples here are handled. \n\nThere might be other representation learning methods to contrast PCR against.  A few that come to mind are Gregor et al. 2019 (https://arxiv.org/pdf/2002.02836.pdf), Rezende et al. 2020 (https://arxiv.org/pdf/2002.02836.pdf) and Guez et al. 2020 (https://arxiv.org/pdf/2002.08329.pdf).  These focus on representation learning in RL but to be fair may not explicitly focus on data efficiency.\n\nIn general, I think that this paper could benefit from some cleanup in the writing and some re-organization. For instance, there is some repetition in the paper and some bits found in background I thought would be more suitable in the intro along with a few other examples. \n Also, the ablations section is a bit confusing to read.  One suggestion to help here could be to first describe the ablations, then present the results, then finally discuss.\n\nIn table 1 it's indicated that on Cheetah-Run, PCR outperforms all previous state-of-the-art algorithms by 47.4% and 13.6% on DMC100 and DMC500 respectively however the curves in fig. 2 (DrQ & PCR) look much closer than that.  Does the figure simply not depict the best run?  Can these be updated to correspond with the table?\n\nThe last paragraph claims that \"the results show\" that the consistency loss elevates the in control tasks however this isn't the case for each dataset and this claim seems to be somewhat unqualified.  Can the authors point to what in particular about this part of the model demonstrates that the extra performance, in cases where it exists, is attributable to this property? Perhaps this belongs in the next section?\n\nWhy does removing the consistency loss aid performance significantly in the case of the reacher task? Why are a number of the differences so small?  Is a consistent model less important for these tasks?",
            "summary_of_the_review": "\nOverall I think the approach here is very promising however I would like to see stronger and more varied results that demonstrate the power of this approach.  Further I think the presentation and writing is detracting from the clarity of the work and there needs to be some clean-up in this respect. I think if the authors addressed these issues and the results improved somewhat this work would be much more convincing.  As it stands it does seem promising and I encourage the authors to develop it further.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigates the data inefficiency of deep reinforcement learning algorithms by focusing on learning better latent representations of observation space with the help of self-supervised learning tasks (auxiliary losses). They do so by learning a representation good for temporal prediction i.e. by learning latent dynamics with the representation. At the same time, they use augmented observations for having a robust representation.  Both of the techniques have been independently studied in the past and the author claims that the combination leads to be better data efficiency as compared to previous SOTA baselines.\n\nMethod: In order to learn the representation, the authors use an online and target encoder. The target encoder is an “exponential moving average” of the online encoder.  The online encoder is given an augmented observation whereas the target encoder is provided with the actual observation.  The representations given by both the encoder are given to a dynamics predictor which predicts the next-state latent representation for each of them.  Also, the next observation is given to the target encoder to get a next-state latent representation. The later, next state representation is used as a target for the former.",
            "main_review": "Strength: \n\n1. Study of a combination of latent dynamics prediction as well as augmentation for better representation\n2. Study of Contrastive Loss vs Predictive Loss ( Section 5.3.2). They show that Predictive loss works better than Contrastive Loss.\n\nWeaknesses:\n1. Author claims that their method outperforms SOTA on a series of tasks. However, as per figure 1, I would suggest that there is only mild improvement in the case of Cheetah and for all other tasks, it appears to be within the variance of baselines.\n2. In Table 1, the authors break down results for 100k and 500k steps and highlight that their method works better. Unfortunately, I am not convinced with highlighted gains as most of the reported gains fall within the standard deviation of DrQ, other than Cheetah.\n3. In Table 2, the authors are missing the standard deviation ( though the caption says that it has been included). Again, I am not convinced with the highlighted gain in the case of “Ball in Cup, catch”(last column). In my experience, 931(Contrastive Loss) and 936(Predictive Loss) most likely have strong variance overlap.\n4. In ( Section 1, para 4, last line), the authors themselves claim that the proposed combination has been studied before, though it is rare. I would also recommend authors to provide references of relevant work over here. \n5. In Section 4.2 and Proposition 1, the authors introduce predictors (q_alpha and q_beta), however, they don’t give any clear definitions of them. Though, they have cited the references for them. I would recommend adding a few lines on their definition.\n",
            "summary_of_the_review": "1. I believe that the  introduced novelty is mild and have been studied before(as suggested by the author)\n2. In the Abstract, the author claims that their methods “outperforms sota baselines on a SERIES of pixel-based tasks”. However, on, (Page 7, paragraph titled “Analysis”, 6th line), the author says that their method reaches(Does not Outperform) SOTA on 4 out of 6 tasks and only outperforms on Cheetah task.\n3. Results reported in Figure 1 and Table 1 show that most of the performance improvement to be minuscule and within the variance of baseline.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper tackles the sample efficiency problem of pixel-based reinforcement learning (RL). It proposes a new method for improving the learned state representations, called PCR, short for Predictive Consistent Representations. As its name suggests, PCR aims to learn representations that combine predictive information (using a learned dynamics model) and are consistent across different views (obtained through augmentations) when given as input to the dynamics model.\n",
            "main_review": "**Strength**\n\n- The proposed method seems interesting, reasonable, and well-motivated\n- To the best of my knowledge, the idea of combining losses for consistency across augmentations and future state predictability is new in vision-based RL.\n- Most claims are supported by empirical or theoretical validations.\n\n**Weakness & Concerns**\n\n- While the idea hasn’t been previously explored, its novelty is rather limited and is solely based on combining multiple prior ideas. A novel combination of existing ideas could be quite useful, but in this case, I don't think this particular combination is especially insightful. Namely, (and as mentioned by the authors), learning representations by enforcing future state predictions is a well-explored idea in prior work [A,B,C]. The same can be said about multi-view consistency for state representation learning [D,E,F,G]. The main idea of this work is to combine losses enforcing the two concepts.\n- In ⅓ of the experiments, the method leads to a lower score than DrQ, in most of these cases it is also lower than CURL, and in one experiment (walker_walk, 500k) lower than dreamer. Since the paper claims that PCR outperforms prior methods, the current experimental results do not fully substantiate that claim. While unanimously outperforming previous methods should not be a requirement for publication, the authors should at least speculate about the reason behind this occasional underperformance. In addition, in approximately another third of the experiments, the improvement in terms of scores and sample efficiency is only minor. Perhaps this might be related to the previous methods already having a pretty good performance in the corresponding environments, but this of course needs to be validated. Hence, it would be beneficial to experiment with additional environments to better highlight the possible improvement when using PCR. If the results in the new environments are better than the baselines, the paper would have a better chance at acceptance. \n- In contrast to the other experiments, ablation studies are only performed in the data limited regime (DMControl100k),  can you perhaps elaborate on this choice? Would the same results be observed in DMControl500k? This concern is mainly raised due to the following statement “With careful ablation studies, we verify the effectiveness of the predictive consistency itself and against other similar approaches like contrastive consistency”. This statement is mentioned in the introduction, and its claim is not limited to data-limited regimes. Unless the authors have a reasonable reason for only performing ablation studies in DMControl100k, additional ablations studies should be performed on DMControl500k as well to support the authors claim.\n- It is surprising that the reported results in the first column in table 2 are different than the ones in the last column in table 1, although they represent the same method (PCR) and the same environments. Are these different experiments using the same method and under the same settings? If so why are the results different? And why aren’t all results averaged together? If that’s not the case, what is different between those experiments? Also, the standard deviation is missing from this table although the caption says differently. \n\n**Minor Issues**\n\n- The hyperparameters are shown in the appendix, however, the way the authors chose them is still missing (that’s very useful for authors but of course optional for acceptance). More importantly, the two proposed hyperparameters \\lambda_{pref} and \\lambda_{cons} in equation (7) are set to 1 (appendix B.1), which means that they are not really used. Is that choice based on an ablation study? If so can you please elaborate on the results? Otherwise, I would recommend removing them from the equation altogether as they might be misleading to potential practitioners. \n- Certain components are not well-introduced and explained such as q_{\\alpha}, q_{\\beta} and g. Of course one could guess what these components are from the context, but they should rather be more clear from the text.\n- The notation in section 4 gets slightly confusing when mixing bars, hats, apostrophes, and subscript numbers. I would recommend introducing the different terms more often in the text to enable faster reading and improve clarity. \n\nIn addition, I would recommend going over the paper a couple more times to fix the many typos, grammar errors, and broken references. Here are some I found:\n\n- Reference to figure 1 is broken. Instead, the paper keeps referring to section 2.1 instead (page 2,3,4)\n- Typo in figure 1: gradinet -> gradient\n- Page 6, right before equation (6): calculate -> calculated\n- Figure 2 and 3 captions: the results is -> the results are \n- Figure 2 and 3 captions: averaged cross -> averaged across \n\n**References:**\n\n[A] Zhaohan Daniel Guo, Bernardo Avila Pires, Bilal Piot, Jean-Bastien Grill, Florent Altche, Remi Munos, and Mohammad Gheshlaghi Azar. *Bootstrap latent-predictive representations for multitask reinforcement learning.* In ICML, 2020.\n\n[B] Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G Bellemare. *Deepmdp: Learning continuous latent space models for representation learning.* In ICML, 2019.\n\n[C] Thomas Kipf, Elise van der Pol, and Max Welling. *Contrastive learning of structured world models.* In ICML, 2019.\n\n[D] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. *Curl: Contrastive unsupervised representations for reinforcement learning.* In ICML, 2020a. \n\n[E] Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. *Reinforcement learning with augmented data.* NeurlIPS, 2020b.\n\n[F] Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. *Decoupling representation learning from reinforcement learning.* In International Conference on Machine Learning. PMLR, 2021.\n\n",
            "summary_of_the_review": "I believe the paper is well written and presents some interesting ideas and experiments. However, due to the limited novelty, the mostly marginal improvement in result, lacking ablations studies, and the many mistakes and unclear statements in the current version, I can give it a score of 5 for now. I am willing to improve the score if the authors integrate the suggested improvements into the next version and perform the suggested additional experiments :\n- Test PCR on additional environments and hopefully show its advantage over the baselines.\n- Perform the ablation studies on DMControl500k as well, unless a good reason exists not to (in that case, please explain this reason in the paper).\n- Perform ablation studies on the presented hyperparameters (optional for acceptance, but would improve the quality of the paper.) \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a multi-task objective to induce \"Predictive Consistent Representations\" from image-based observations on deep reinforcement learning tasks. The paper provides some intuition for the utility of representations which encode temporal information and remain predictive across multiple views (e.g. rotations, cropping, etc.) of the observations. Finally, the paper leans on empirical support for the proposed PCR method by comparing to several high-performance competitor algorithms on 6 subselected DeepMind Control domains, then ablates one component of the proposed methodology.",
            "main_review": "I am recommending to reject this paper because (W1) it provides insufficient support for its proposed contributions, (W2) it misses critical connection to long-standing literature, and (W3) the paper provides negligible novel understanding of predictive representations, their properties, or their utility.\n\n---\n\n**Details for (W1):**\n\nThe paper states its primary contributions are (C1) a novel method, PCR, (C2) it shows that PCR achieves state-of-the-art performance on the pixel-based DeepMind control benchmark, and (C3) that the paper provides empirical insight into the utility of predictive consistency vs. other approaches like contrastive consistency. \n\nFor (C1), there is no theoretical support for the proposed method. So the support for (C1) relies entirely on (C2) and (C3).\n\nFor (C2), the paper compares on a subset of 6 domains. Because there is little evidence that the performance of any algorithm on these 6 domains is representative of the performance of all 28 domains of the DeepMind control benchmark, it is an overclaim to suggest that (C1) has been validated on this suite. Further, in Figure 2, the paper uses only 5 runs to compare algorithm with stddev bars considerably overlapping suggesting very high variance on most domains. No statistical test is provided (of course likely no test is valid with only 5 samples) and so it is impossible to tell if differences between algorithms are meaningful. Despite this, it is clear from the mean of these learning curves that the proposed PCR is no better performing than baseline DrQ. These have nearly identical learning curves for all 6 investigated domains.\n\nFor (C2), the paper additionally provides Table 1 comparing 6 baselines. The claim that the proposed PCR outperforms competitors by 13.6% for asymptotic performance and 47.4% for limited samples on CheetahRun is a wild overclaim. Note first the high variance of both methods, suggesting that these results may not be statistically significant. Second, this claim is misleading by omission; one baseline (DrQ) outperforms the proposed algorithm by ~36% on another domain (using the same flawed methodology as the paper to compute percentage increase in performance). \n\nFor (C3), the paper provides Table 2 comparing 3 versions of PCR to ablate the predictive loss component. The paper uses the score on the 6 investigated domains to conclude that because PCR is better than the two variants (without the PC component, and switching the predictive component for a contrastive component), then the PC component is responsible for the performance increase. Note there is no statistical support for the claims (no measure of variance, no measure of confidence, no discussion of if this result holds on new data). Also note that for 4/6 domains, the degree of difference between PCR and R is negligible (~2% increase in some cases). On the remaining 2 domains, in one case R significantly outperformed PCR (51% increase) and in the other PCR significantly outperformed R (30% increase). These results are in no way convincing. \n\n**Details for (W2):**\n\nThere is a considerable amount of literature studying predictive representations of state that is never mentioned in this paper. Because the paper significantly lacks its own theoretical and empirical support for the proposed algorithm, the paper should seek support from prior theory and literature. I will point out a few key works here, but the relevant literature is vast and a more comprehensive review is required.\n\n* Predictive representations of state, Littman and Sutton, 2002\n    * Develops a theoretical grounding for representations of state which are predictive, showing the minimal set of predictive features necessary to capture underlying dynamics and perform well on any future task.\n* Learning predictive state representations, Singh et al. 2003\n* Planning with predictive state representations, James et al. 2004\n    * Uses PSRs for model-based RL\n* Better generalization with forecasts, Schaul and Ring, 2013\n* Developing a predictive approach to knowledge, White 2015\n    * Discusses in great detail the utility of representations which are predictive and how these support learning many tasks and views of the data.\n* Temporal difference networks, Sutton and Tanner 2005\n* Gradient temporal difference networks, Silver 2013\n    * Develops a method for learning a representation from predictions, which is shown to itself be a predictive representation for many tasks.\n* Multi-timescale nexting in a reinforcement learning robot, Modayil et al. 2014\n    * Proposes a methodology for learning preditive representations which perform well on many \"views\" of the data in the form of multiple tasks.\n* Improving generalization for temporal difference learning: The successor representation, Dayan 1993\n    * First work to show that representations which capture the underlying dynamics (independent of task) are important for learning value functions and performing well on temporal tasks.\n* Reinforcement learning with unsupervised auxiliary tasks, Jaderberg et al. 2016\n    * This work seems particularly relevant. In fact, it proposes learning the next state (temporal) along with multiple views of the images. How is this work in any way different than PCR?\n\n---\n\n**Did not contribute to score:**\n\n* The paper could generally use a proofreading pass, especially starting at the experiments section.\n* There is a duplicated citation in the bibliography\n* There is no discussion of what the numbers in Table 2 represent. Are these area-under-the-curve?\n* PCR = Predictive Consistent Representation. So the sentence \"(i) PCR without Predictive Consistency\" is just a representation? Is there a better name for this ablation that more clearly details what is being manipulated and how? Are you actually directly manipulating the representation (which is what is being implied here), or are you changing/removing terms in the loss function?",
            "summary_of_the_review": "I am recommending to reject this paper because (W1) it provides insufficient support for its proposed contributions, (W2) it misses critical connection to long-standing literature, and (W3) the paper provides negligible novel understanding of predictive representations, their properties, or their utility.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents predictive consistent representations (PCR) for sample-efficient reinforcement learning. The key idea of the PCR agents is that the learned representations should be consistent across view angles / data augmentations. The representation learning goes on along with the policy learning with SAC. Although the idea itself is simple, it has shown a clear performance boost on several continuous control tasks, given the same number of environment interactions. Extensive ablation studies are also given to help explain the key factors for representation learning in model-based RL. ",
            "main_review": "Strengths:\n\n1/ The paper was well written and easy to follow\n2/ PCR has achieved great performance on continuous control tasks and outperformed the strong baselines, e.g., Dreamer and DrQ. \n\nWeaknesses:\n\nThere are several weaknesses of the paper and the proposed method. \n\nThe resulting algorithm of PCR consists of many interacting components: SAC with double Q-learning, EMA-based parameter update for target Q-network and the target encoder,  a large number of hyper-parameters, and etc. \n\nFrom this perspective, I would think the experiments are not sufficient to explain the good results, where PCR outperforms DrQ. In particular, I noticed that in Tab. 2, PCR (w/o consistency loss) and PCR (contrastive) actually achieve comparable performance with the original PCR, e.g., finger Spin, walker walk, and ball-in-cup catch. In some cases, the ablated variants even outperform the original model PCR: on Reacher-Easy, both PCR (contrastive) and PCR (w/o consistency loss) outperformed the PCR. In addition, as mentioned in the caption of Tab. 2, the author intended to provide the standard deviation of the results, but somehow this is missing. Without the standard deviation, the performance difference of the methods can not be properly justified. I’m a bit unsure about what the real underlying reason to the good performance: is it because of the various tricks used in the method, or is it really due to the PCR itself?\n\nIn addition, the idea of PCR is simple but fairly general. Such a method should generally be examined in more algorithms and domains, to justify its benefit to the learning algorithms. For example, how much performance boost can PCR give to other off-policy RL algorithms, e.g., MPO? Will this performance boost different from SAC? Can PCR be applied to more complex tasks, e.g., robot manipulation, navigation, etc?\n",
            "summary_of_the_review": "The paper has investigated an interesting perspective on model-based RL. However, the experiments are not yet sufficient to support the claim. Hence, I think the paper is not ready for publication yet.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}