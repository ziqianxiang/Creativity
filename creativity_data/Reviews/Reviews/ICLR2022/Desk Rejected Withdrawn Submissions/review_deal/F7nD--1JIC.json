{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes the use of Shapely values to score operations in differentiable architecture search. The idea is interesting and applies concepts from game theory (which is new to me as a reviewer!) and neural architecture search. The paper calculates Shapely values by discretizing and edge and evaluating validation/test accuracy of the supernet. However, because this is very expensive to do for all operations, an approximation is proposed based on monte-carlo sampling. The proposed method is empirically evaluated on the DARTS search space for both CIFAR and ImageNet classification tasks.",
            "main_review": "The paper is interesting and merges concepts from game theory and NAS. However, there are a number of issues that I would like to point out:\n\n- There isn't enough empirical or theoretical evidence to support the use of Shapely values for operation scoring. Figure 2 is compelling but more such results or analysis could further support that Shapely values should be used in differentiable NAS in the first place. \n- The approximation to Shapely values is also not well-supported. Where is the evidence that the proposed Monte Carlo sampling method is a representative approximation of Shapely values?\n- Using discretization accuracy as operation scores has been proposed in the previous ICLR conference [1]. The authors do point to this previous work and compare to it -- I am just mentioning that there is a significant overlap in the methodology although the two papers approach the problem from a different motivation.\n- Evaluation on DARTS and NAS-Bench-201 is standard but the paper would be stronger if additional search spaces were studied.\n- Methological details are missing such as running with multiple seeds and averaging (with DARTS search space).\n\n[1] Wang et al. Rethinking Architecture Selection in Differentiable NAS, ICLR 2021",
            "summary_of_the_review": "While I found the application of Shapely values interesting and insightful, the paper did not provide enough compelling evidence to support the presented methodology. I do think that the evaluation on DARTS + NAS-Bench-201 is a good end-to-end test but theclaims and approximations to get to the final methodology/results is not well-examined. While there is merit in this paper, the methodological gaps make it read like _just another DARTS improvement paper that has good empirical results._",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose to utilise Shapley values in supernet-based NAS to identify importance of different operations.\nShapley values are practically approximated using an existing method utilising Monte-Carlo sampling, the proposed algorithm incorporates this approximation into supernet-based search by replacing traditional architectural parameters from DARTS with approximated Shapley values after momentum and scaling is applied.\nEvaluation includes experiments on NAS-Bench-201, DARTS CNN search spaces (both CIFAR-10 and ImageNet) and the usual four subspaces of DARTS. \nAblations study investigating different hyperparameters is also included.",
            "main_review": "Overall, I really liked the idea behind the paper even if it might seem to be a rather straightforward application of existing work to a new context.\nStill, to the best of my knowledge, this is the first work that tries to combine Shapley values with NAS and the results look promising so I would be leaning to accept the paper.\nHowever, despite empirical significance, I find a lot of places in the submitted paper to be a bit unclear when it comes to some tricky details and in many places writing leaves **a lot** of room for improvement (although high-level structure seems ok).\nAll things considered, for now I'm refraining from recommending acceptance in hope that the authors can address my comments during the rebuttal period and, at the same time, improve writing.\nI'm happy to reassess my position after that.\nThe lists of itemized strengths and weaknesses are provided below for more context.\n\n\n**Strengths:**\n1. The proposed method is the first, to the best of my knowledge, that incorporates Shapley values into NAS - an approach that intuitively makes a lot of sense (due to the aligned objectives of NAS and Shapley values) and seems to be delivering promising results.\n\n2. Evaluation seems correct (with one notable exception mentioned in weaknesses), although maybe not very exciting. NB2 and DARTS (including ImageNet and subspaces) are a good and common standard in NAS. What is important experiments have been conducted correctly (i.e., results include both average and best performance), the choice of baseline methods also doesn't raise any red flags.\nNotably, the results in pretty much all considered settings are quite outstanding.\n\n3. The work includes ablation study of the most important hyperparameters.\n\n\n**Weaknesses:**\n1. When presenting results on NB2 (Table 3), the authors show that their method finds the optimal solutions for both validation and test accuracy (mostly relevant for CIFAR-10).\nIf I understand it correctly, that means that the authors run their method twice - once optimising validation accuracy and then optimising test accuracy directly (due to the discrepancy between validation and test accuracies in NB2).\nThis is a slippery slope - by convention test set should never be used except for the final evaluation.\nWhat most of the NAS methods do because of that is that they never use test set for optimisation - **it is crucial that the authors do not compare their method that was optimising for test accuracy directly with published results that were obtained for architectures optimised for validation accuracy**. If the authors really did optimise for test accuracy directly and want to stick to that decision, it would require: 1) to **explicitly and clearly** mention this fact in the paper, 2) explain why they decided to break with the convention, 3) make sure that comparison is fair by either reruning baseline methods that might need it or simply not reporting their performance (in that case however it's a bit unclear why those results would be needed). \nOf course, it is also possible that I misunderstood what the authors did.\nHowever, in that case I don't understand how the proposed method can obtain both solutions from a single optimisation process with no variance - further clarification would be required.\nSpecifically, the architecture that achieves the best average test accuracy (94.37) achieves average validation accuracy of 91.5 rather than the optimal 91.61.\nOn a side note, the authors should have mentioned somewhere that they use average accuracy across all available seeds in NB2 as the criteria of optimality (as opposed to taking the best seed for each model) - it's not incorrect but should have been made clear.\n(At least it seems to be the case after checked it myself.)\n\n2. Although most of the presented results constitute a rather solid proof-point that the proposed method can achieve very good results, there is a notable exception to that.\nSpecifically, performance of the proposed method on the DARTS subspaces S1-4 on CIFAR-10 seems to be visibly behind other baselines.\nThis would demand some commentary from the authors - is it possible that these results suggest that Shapley-values, with all their properties, might not be as great fit as it might see (see the point below)?\n\n3. Following a bit on the previous point, I find motivation presented in section 3.2 to be a bit flawed in the context of NAS.\nSpecifically, the authors say things like: \"It is crucial to propose a fair attribution metric to evaluate operation contribution instead of relying on values of the architecture parameter α.\"\nI agree that relaying on the architectural parameters, at least in the way explained in the original DARTS paper, seems like a bad idea.\nBut I can't find a compelling argument in the submitted work that would advocate for the proposed focus on fairness.\nFIrst of all, the authors never really define what they mean by \"fair metric\" - I somewhat guess that they refer to the properties of Shapley values that are described a bit later in the same section.\nHowever, I do not see a reason why any of these properties would really be desired in the context of NAS (except for symmetry).\nFor example, assigning 0 to operations that do not contribute at all to the performance of a supernet of course makes sense intuitively and has nice interpretation, but it is not exactly desired.\nI would be perfectly happy with a NAS method that does assign non-zero scores to meaningless operations as long as it doesn't stop it from identifying well-performing models.\nIn this sense it seems to me that the authors try to naively apply notion of fairness from the original context of cooperative games (where assigning zero scores to player that do not contribute anything is clearly a fair thing to do) directly to NAS, but personally I remain unconvinced.\n\n4. As highlighted at the beginning of the review, the text contains **a lot** of minor mistakes that, unfortunately, accumulate and leave rather unfavourable impression from reading the paper.\nIn this last point of the main review, I decided to point out some of the example of problems with writing and presentation in the submitted paper, with some suggestions for the authors on how to improve.\nI would encourage the authors to dedicate some time to work on the writing specifically.\n\n\n> Unlike existing methods which leverage the learnable architecture parameters to represent the operation importance in joint optimization of supernet weights and architecture parameters, we directly evaluate operation influence on task performance according to the Shapley value of corresponding operations.\n\nThe first part of this sentence suggest that the proposed method does not use \"learnable parameters that represent the operation importance in joint optimization\" which is not true, as far as I understand. Please rephrase (or explain the current wording).\n\n> The architecture parameters are optimized by gradient descent, which can not reflect the actual importance of operations.\n\nWe do not know if gradient-based optimisation will always fail (i.e. \"it can not reflect\") or if it's simply known to fail in at least some cases (i.e., \"it might not reflect\").\n\n\n> *(regarding caption of Figure 1)*\n\nIn my opinion, the caption of a figure should attempt at explaining elements in the figure. Currently caption of figure 1 (especially 1b, as it concerns proposed methodology, which would need more explanation than 1a) does not really explain what is presented in the Figure. What is the meaning of dashed squares compared to bolded ones etc.? I can guess it after reading the full paper but the purpose of the caption is exactly to explain things like that (if they are not explained in the Figure itself).\n\n> The operation aggregation in the supernet based on performance contribution enables effective optimization of supernet weights, so that architectures with more promising performance are acquired.\n\nAre you referring to the optimisation of weights within individual operations (\"supernet weights\")? If yes, then I find this claim to be unsupported as the paper does not seem to be talking about supernet weights at any point really.\n\n> We conducted extensive experiments on image classification and optimal architecture search across various search space (...)\n\nConsider rephrasing to \"We conducted extensive experiments on common search spaces for image classification\"\n\n> \"We achieve an error rate of 2.43% on CIFAR-10 according to the search space of DARTS and obtain the top-1 accuracy of 23.9% on ImageNet under the mobile setting.\"\n\n\"Error rate\" cannot be \"according to\" a search space.\n\n> \"Since DARTS optimizes the single point on the simplex of continuous search space and discretizes the final architecture after search, (...)\"\n\nI didn't understand this sentence - I even asked some colleagues who are more familiar with mathematical theory behind optimisation and they couldn't fully understand what was meant here as well.\nI suggest some rephrasing to make this part clearer.\n\n> \"In order to mitigate the performance gap between the training set and the validation data, (...)\"\n\nIt has been a while since I read both papers but from the top of my head I would say that's not the original motivation of neither of the papers.\nCould the authors explain what they meant here?\n\n> \"(...) to alleviate the degenerate of search-evaluation correlation, (...)\"\n\nI don't understand what the authors wanted to say here.\nPerhaps something along the line: \"to alleviate problems with poor correlation between search-time and evaluation-time performance\"?\n\n> \"(...) after the architecture parameter discretization.\"\n\n\"after discretization of architectural parameters\"?\n\n> \"(...), where edge normalization degraded the search uncertainty to prevent edge selecting inconsistency.\"\n\n???\n\n> \"However, empirical studies (Wang et al., 2021b; Zhou et al., 2021) have demonstrated the learnable architecture parameter in DARTS framework fails to reveal the operation importance in the supernet, (...)\"\n\nWang et al. provides mathematical proof of that (for at least some cases), therefore it's not exactly correct to say that this is empirical demonstration.\n\n> \"(...), which requires effective metrics that fairly evaluate the operation contribution during architecture search.\"\n\n\"which requires\" does not make much sense in this context, did the authors mean something along the line: \"which motivates the need for a better importance measure that fairly (...)\"?\nI already commented on the fairness more generically so I'm not going to repeat myself, but of course my comment also applies here.\n\n> \"continuously relaxing architecture search space\"\n\n\"continuously\" here means that relaxing is done all the time\n\n> \"With such relaxation, the architecture search can be performed by jointly optimizing the network weight w and architecture parameters α in a differentiable manner with the following bi-level objective:\"\n\n> \"(...) reformulate the bi-level optimization problem in DARTS given in (2) as follows:\"\n\nIt seems to be a common thing to attribute Eq. 2 to DARTS and connect this to the continues relaxation proposed there.\nHowever, this is not exactly correct.\nAlthough DARTS could have been the first paper that formalized NAS using this particular formulation, it is worth noticing that most (all?) NAS methods actually solve the exact same bi-level optimisation problem - the main different, and contribution of DARTS, is the fact that alphas are continues and therefore can be optimised using Eq. 5-8 from the DARTS paper, rather than, e.g., using an evolutionary algorithm to perform discrete optimisation of binary operation mask.\n\nConsequently, the \"reformulation\" proposed in Eq. 5 of this work does not make much sense to me in the context of NAS objective -- the objective is still the same: to find discrete alpha maximizing $L_{v}(w^{*}, \\alpha)$, where $w^*$ are optimal weights for a relevant architecture.\nThe fact that this is done with the help of Shapley values is a different thing.\n\nOn a different note, formally speaking, Eq. 5 does not even define a bi-level problem as the first part is missing argmax. Also, equality in the first part is recursive, I doubt that's what the authors wanted to say there.\n\n> \"However, this assumption has been proved to be untrue in most cases\"\n\nMaybe I'm missing something, but my understanding was that Wang et al. only formally proved that in a well-optimised supernet skip connections will have higher magnitude of alphas than convolutions.\nAlthough significant, this finding does not prove anything about \"most cases\".\n\n> \"(Shapley, 2016)\"\n\nThe work was published in 1953.\nAlso, since it's part of a book it should be cited accordingly.\n",
            "summary_of_the_review": "An interesting approach to NAS utilising Shapley value. The main value seems to be on the practical side since the proposed methods is simply an application of existing approaches to a new domain - the results, however, are quite outstanding (although some extra clarity would be needed regarding correctness, as at least in one place the results raise some concerns in that regard).\nWriting-wise, the submitted paper requires quite a lot of work before it can be accepted but hopefully the authors can revise it in time.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a Shapley value-based operation contribution evaluation method for NAS. As claimed, Shapley values can have stronger performance on evaluating how each operation contributes to the accuracy than architecture parameters proposed in DARTS. To efficient approximation, the Monte-Carlo sampling-based algorithm with early truncation is introduced.",
            "main_review": "Shapley values are reasonable to evaluate operation contributions. However, there are several questions to be answered.\n\n1. One key point claimed in this paper is that Shapley values are stronger on evaluating operation contributions than architecture parameters in DARTS. However, there lacks substantial theoretical and empirical proof in the manuscript. As architecture parameters in many gradient-based methods have shown promising performance, the benefit from Shapley values in NAS seems unclear. Noting that the experimental advantages of Shapley-NAS are too marginal compared with previous methods.\n2. Please comprehensively analyze the differences and advantages over Neuron Shapley [1], as Shapley values and the Monte-Carlo sampling are both used in [1].\n3. Some experimental issues:\n  - FLOPs are important to measure the model budget but missing in comparisons.\n  - What about the MobileNet-like search space?\n  - In Tab. 4, Shapley-NAS shows worse performance than DARTS-Shapley\\*, especially under the C10-S1 setting.\n\n[1] Amirata Ghorbani and James Zou. Neuron Shapley: Discovering the responsible neurons. arXiv preprint arXiv:2002.09815, 2020.",
            "summary_of_the_review": "The main concern lies in the real effectiveness of the Shapley value on evaluating operation contribution. More comprehensive studies and proofs are needed to support the main claim.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work proposes to incorporate Shapley value with differentiable architecture search approach. In essense, this work falls in the category of improving the architecture selection in DARTS. Motivated from the game thoery, the authors formulate DARTS super-net training as N-players collaboration game: each edge in DARTS-supernet is a player, and the goal of collaboration is to optimizing the super-net performance. \nExperiments are conduncted on DARTS space, (CIFAR-10 and (ImageNet) and NASBench-201. \n",
            "main_review": "\n-- Strength -- \n+ Brings Shapley value into DARTS domain is a novel attempt\n+ clear formulation of Shapley value solution to the DARTS approach.\n\n\n-- Weakness -- \n\nI enjoy the idea of bring knowledge outside computer science into our domain and see some promising preliminary results, but I have several doubts that preventing myself voting acceptance. \n\n1. Does Shapley value really generate to DARTS super-net evaluation? \nThe super-net of DARTS approach is composed with the weights of super-net itself (lets denote this as Omega) and the architecture weights alpha. It does not seem clear to me that these alpha value can be directly formulated as N players try to maximize the super-net performance without considering the super-net weights Omega. \n\n2. NAS's goal is **NOT** to maximize the super-net performance, but select the best stand-alone architecture. \nOn the other hand, the purpose of neural architecture search is not to optimize the super-net performance but to select the best stand-alone architecture. So to me, this idea of formulating different edges as player and maximize the super-net performance is not on the right direction. Essentially, if the authors would like to convince the readers that formulating DARTS super-net training is a game theory problem to extend Shapley-value, it's necessary to concretely study the following assumption: maximizing the super-net performance is equivalent to maximize the final stand-alone architecture performance. Yet, this is disproved in a recent work (Yu. et al, An Analysis of Super-net Heuristics in Weight-Sharing NAS, TPAMI'21). In this sense, this paper proposes a nice method to solve a problem that is not critical to the research community. This fundamentally different from the Wang et al. 2021b paper, which found the previous common DARTS architecture evaluation cannot select the best stand-alone architecture. \n\n3. Fig 2 showing the ranking disorder does not help the story of formulate the problem to extend Shapley-value. \n\n4. Empirical results seems contradictory, lacking of enough evidence to show the downside in weakness #2. \n\nIn Table 1, Shapley-NAS on average clearly outperforms the DARTS+PT approach. However, if we see Table 4, the ablation study, it shows a very different picture. In the table 3 of DARTS+PT paper, we can observe that DARTS+PT outperforms Shapley-NAS in almost all configurations. This is unusual and not convincing. \n\n\n- Minot points\n\nThe presentation can be improved. Some examples:\n\nFigure 1.\nFor Fig.1 (a), it's a clear representation of traditional DARTS method, however, (b) does confuse me. In the capture, it says 'Shapley-NAS directly evaluates operation contribution to the task performance,...' However, this is unclear with the given figure. What are these numbers? why there are two column and three arrows? It seems the three numbers on the right hand side is the difference bewteen these two columns, but what's the meaning of showing this? How does this relates to the Shapley value? ",
            "summary_of_the_review": "Although the idea of using Shapley value in DARTS is novel and interesting, the paper still has several fundamental issues. The most critical one to me is the authors formulate the DARTS as N players try to maximize the super-net performance to extend Shapley-value, this is not the fundamental goal of NAS, which is to select the best stand-alone architecture. The contradictory results in ablation study and CIFAR-10 experiments is also very unusual. \n\nI encourage the authors to revisit their motivation and provide some analysis if possible. \n\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}