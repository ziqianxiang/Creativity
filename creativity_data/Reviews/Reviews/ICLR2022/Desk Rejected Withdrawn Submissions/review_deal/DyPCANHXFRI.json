{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper examines the effect of curriculum learning on the model calibration. In particular, the paper examines if different example orderings and pacing functions are able to improve the calibration error of a ResNet-18 train on Cifar-10 & Cifar-100. The authors show that in most cases, the effect of curriculum learning is negligible. However, in the context of limited training or noisy data, it can lead to improved model calibration. In addition, paper shows that the calibration results are sensitive to the choice of pacing function and its parameters. ",
            "main_review": "The paper provides a clean & rigorous experimental setup to understand the interplay of model calibration and curriculum learning. Unfortunately, the paper struggles to identify a significant phenomenon. This may in part be due to the simplistic choice of the experimental setup. Calibration issues commonly tend to appear in highly overparametrized settings. However, the paper instead focuses on a small ResNet models. As such, in most settings, the authors do not observe any significant improvements to the calibration (if anything at all). \n\nThe paper identifies a setting in which curriculum learning is able to significantly improve the calibration error (Section 6). However, the setting seems rather artificial: the data has 40%-80% label noise and models are severely undertrained. It is unclear if the gains observed in this setting continue to be the case in real life scenarios. ",
            "summary_of_the_review": "The paper is nicely written and provides a great overview of the field and the experimental setup. However, it fails to identify a significant phenomenon. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper empirically analyses the effect of a single curriculum learning method on model calibration; analysing the this effect in with clean (or relatively clean) and noisy data, which is an increasingly more common scenario. The paper present results on the effect of a curriculum learning method and anti-curriculum on CIFAR10/100; with results that claim to suggest (anti)curriculum learning might assist model calibration when the dataset is noisy and training time limited.",
            "main_review": "Merits:\n- Subject of broad interest to the community.\n- Good introduction and background section with introduction to basics, although probably not required in conference paper submission.\n- Paper is easily reproducible\n\nWeaknesses/Potential Improvements:\n- Paper is essentially rerun of Wu et al. but with ECE reported.\n- Only metric reported is ECE. Would be useful to see model accuracy, likelihood, Brier score and other ECE variants.\n- Assesses only one curriculum learning approach,- with many curricula introduced in recent years that surpass the performance of the method tested. Making far reaching statements of the effects of curricula on calibration as the paper does, does not stand in the face of experimental results.\n- Figures and results are unpolished: lack axis names, difficult to read and interpret, different methods have different relative scales.\n- Overall poor execution on a study on an interesting topic: results are poor and difficult to analyse, analysis itself is lacklustre and lacks depth.\n- Abstract not supported by experimental results. More specifically:\n    - \"most of the time calibration has negligible effect\": this seems to be supported by choosing a pacing function with high b value. This mean most of the dataset is included at the beginning of training. Why would curricula assist then. For the one experiment when b is low, it is clear that model calibration (as measured by ECE) is effected. See further comments below but I raise this here because it is common across the paper.\n    - \" curriculum learning can substantially reduce calibration error in a manner that cannot be explained by dynamically sampling the dataset.\" really not sure what this means? Is this a comparison to random/uniform sampling? \n\nFurther comments:\n- Section 3 and Figure 1: no axis' names, axises are not aligned between curricula (giving false impression of results), no mention of number of runs or what bar graph indicates (std dev etc.) in figure text (but included in text). How far into training is this? 8800 steps is what percentage of total training? Most of configs shown start with a large amount of data-- only one config with small amount of starting data and this seems to show difference between standard and curricula learning at the periods in training shown. Comments and analysis of results does not seem to reflect actual results. e.g. \"one configuration produces lower ECE than standard training\" this result seems barely statistically significant, while the statistically significant result a=1.6, b=0.1 is not analysed in much detail? what is occurring?\n- Section 4 and Figure 2: same comments as above. \nClaims that the {a=1.6, b=0.1} config results in bad calibration at this time step because poor parameters for pacing function (no analysis provided for that) and low generalisation because of smaller training set. (2) is obvious, but one should be measuring wall-clock time or the like when comparing then. Perhaps at the wall-clock with that pacing function it outperforms other pacing functions/curricula.\n- Figure 4: clearly apparent that is the wrong x-axis to shown (wall-clock time maybe?). I really can't interpret any useful results from this.\n- Sections and figures 5 & 6: same comments as above; clearly wrong x-axis. Results don't back up conclusions.\n\nI can provide further detail and comments, but this is very unpolished and should not have been submitted in this state to ICLR for review.",
            "summary_of_the_review": "Interesting subject; but poor empirically evaluation, limited novelty, lacklustre analysis of results.\n\nRequires significant improvements before submission to any venue for publication.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the effect of curriculum learning on confidence calibration of neural networks.\nThe authors bring three arguments to ground this hypothesis (Quotes from text):\n\na. When considering what theoretical effects curriculum learning can have on calibration, we note that many calibration methods work by modifying how models are trained (Kumar et al., 2018; Kong et al., 2020; Muller et al., 2019). Thus, it is reasonable to assume that curriculum learning, which affects the nature of training by altering the optimization strategy, can have an influence on\ncalibration.\n\nb. Curriculum learning’s purported benefits in generalization and regularizing training towards better regions in parameter space by optimizing a smoother version of the training objective (Wang et al., 2020) can theoretically punish overconfidence.\n\nc. It has been observed that the increase in a neural network’s confidence over the course of training is one of the key causes of miscalibration (Mukhoti et al., 2020), and as a result being exposed to the most difficult samples far into training can mean there is less of a chance of a model becoming overconfident on data that it is the poorest at classifying. \n\nThe paper studies the ResNet-18 architecture trained for image classification, on CIFAR-10 and CIFAR-100, and compares curriculum learning, anti-curriculum learning, and random-curriculum. Where, in curriculum learning samples are ordered from the lowest scoring to the highest scoring, in anti-curriculum learning the ordering is reversed, and in random-curriculum the ordering is random (it does not depend on the score).\n\nThe main findings of this paper are:\n1.  Curriculum learning can in some cases affect models' confidence calibration. The empirical results indicate that in many cases there is no statistically significant benefit over standard training, but there are specific cases where its benefits can be observed.\n\n2. The significance of curriculum learning for calibration is most notable in limited training time and noisy\ndata.\n\n3. Curriculum and anti-curriculum learning appear to have nearly identical trends regarding model calibration.\n\n4. The choice of pacing function in curriculum learning can significantly impact confidence calibration calibration.\n",
            "main_review": "### Strengths:\n- The paper compares different curriculum strategies and shows that in cases where curriculum learning is beneficial, anti-curriculum learning can have a similar effect. \n\n### Weaknesses:\n- The writing of the paper can be improved. In some part, things are explained very extensively (beginning of the Intro and Abstract) and in some parts not enough detail is provided.\n- Some of the arguments, are a bit sloppy, e.g.,  I find argument a (see the summary) somewhat unconvincing and un-scientific, of course some changes in the training regime can impact the calibration of the models but that doesn’t mean any change in the training regime will have a similar effect unless the authors have a specific case in mind that is relevant to curriculum learning.\n- The benchmarks are limited (only CIFAR-10 and CIFAR-100).\n\n### My Take: \nI think the topic of this paper falls within the category of \"What impacts generalization ability of the models?\". And in this paper, confidence calibration can be seen as an indicator of generalization. If we look at it this way, the contribution of the paper would sound a bit trivial since  prior works have already studied the impact of curriculum learning on generalization, also in case of noisy and limited data. That being said, I think the most interesting part of this paper, is studying the effects of different components of the curriculum learning.\n\n### Suggestions:\n- Abstract is too long. If I were you I would lose the first few sentences that generally explains the importance of model calibration in practical settings. You could simply start by stating that the paper studies the impact of curriculum learning on models’ confidence calibration and then explaining your findings as you have already done in the second half of the abstract (this is of course my subjective suggestion).\n\n- In the intro you refer to Wu et al. (2021)  and you mention that you build on top of that, but you don’t actually explain what the referred paper is about. Also, in one before the last paragraph of the intro, you suddenly mention the term pace functions, without explaining it. I think in these cases, it helps to provide the reader the necessary context.\n\n- I think the paper can become more interesting if more ablation studies are done on different components of the curriculum learning.\n\n### Questions for the authors:\n- Can you link back from your experiments, to argument c (see the summary section). Or do you think it is possible to design an experiment to illustrate that this is indeed the case? Also, how would you then explain the benefit of anti-curriculum? \n- Do you think factors like model architecture and model size, or things like characteristics of the dataset, can potentially change the conclusions of this paper? In other words, can you discuss the generalizability of your findings?\n",
            "summary_of_the_review": "The paper provides and interesting analysis on the effect of curriculum learning on confidence calibration. Considering all the prior work on this topic I am leaning toward borderline rejection mainly because I am not convinced about the novelty of the findings.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors investigate the effect of curriculum learning -- namely assigning a difficulty score to the inputs and processing them with a specified criterion -- in calibration.\nThe paper reports (mostly) a negative result: it seems that there is no benefit in using (anti-)curriculum for calibration in standard settings. \nFinally, the authors noticed that, when the data are noisy and the number of steps is limited, curriculum strategies are successful.\n",
            "main_review": "In general, I am happy to read of negative results. We are very biased towards positive results that having authors reporting things that do not work gives a good contribution to the community.\n\n\nIn the current version, the paper has several limitations that make the result shaky and not reliable. I believe that their results are interesting (if confirmed), but at the moment the paper does not meet the acceptance bar.  \n\n\nReproducibility: \n* The code should already be available, so that we can see it in the reviewing process. It is not enough to say that it will be publicly realised upon acceptance, since as reviewers we should check if that is properly commented and runs without issues. \n* Many details are missing and, at the moment, I am not sure I can trust the results. In particular, the histograms seem to be obtained out of few (many?) simulations, but the plots look like from a single simulation. We need to know how many simulations have been made. I could not find this information and should be made evident to the reader. The plots should show the average over many runs. How can we be sure that the result is not just a random fluctuation? We do not even know the size of fluctuations.\n* The authors seem to use some early stopping criterion, but I did not find what this is. \n\n\n* Figures:\n** All the figures use a very (very) small font. It is impossible to read them on paper and I was forced to zoom in on the pdf. In particular figures 3 and 4 are ridiculously small. This is very unpractical for the reader and gives a very unpolished aspect to the paper.\n** They are referenced randomly. Figures 1 and 2 are referenced after figure 3. Figure 4 is referenced only in the appendix. \n** The colours are not matched in figure 6, despite the fact that they are showing the same protocols in different settings. \n** In figures 1 and 2, the reference line for the \"standard\" protocol is missing. It is impossible to make a proper comparison.\n** The vertical scale of the figures with multiple panels is not matched, how can I possibly conclude that one method is better then the other. In particular in figure 1, after rescaling properly the ax, we found that \"random ordering\" is totally comparable (maybe better) than curriculum and anti-curriculum. Which seems to disagree with the conclusions of the authors. (Anyway it was already hard to say something because the standard baseline is missing.) A simple fix that would solve many problems at once is to merge the panels together and make a legend for the colours (for the histograms) and line style (for the plots). This will save space (allowing for larger fonts) and make the comparison straightforward. \n\nImpact of noise:\n* The authors claim that in a setting with a limited number of steps and noisy inputs, curriculum-like strategies have the largest advantage. I do not see that from their plots. It seems that, instead, they are using the wrong stopping criterion and that the standard strategy is still the best one.\n\nLiterature:\n* In page 1 the authors write \"Curriculum learning has been widely used in the literature [...]\". I honestly find it surprising, I would rather say that the machine learning community hardly use curriculum learning. Which (by the way) is consistent with their finding that it does not work particularly well.\n* To the best of my knowledge NLP is the domain of ML where curriculum is used the most [1,2,3,4]. Interestingly [4] also observed that (in same cases) anti-curriculum can lead to benefits. Also theoretically, it has also been shown that in some cases anti-curriculum is the optimal strategy [5]. However, it is strange that the authors of this paper found situations where both are beneficial or detrimental. This is very counter-intuitive. \n\n[1] Khomenko, Viacheslav, et al. \"Accelerating recurrent neural network training using sequence bucketing and multi-gpu data parallelization.\" 2016 IEEE First International Conference on Data Stream Mining & Processing (DSMP). IEEE, 2016.\n\n[2] Doetsch, Patrick, Pavel Golik, and Hermann Ney. \"A comprehensive study of batch construction strategies for recurrent neural networks in mxnet.\" arXiv preprint arXiv:1705.02414 (2017).\n\n[3] Kocmi, Tom, and Ondrej Bojar. \"Curriculum learning and minibatch bucketing in neural machine translation.\" arXiv preprint arXiv:1707.09533 (2017).\n\n[4] Zhang, Xuan, et al. \"Curriculum learning for domain adaptation in neural machine translation.\" arXiv preprint arXiv:1905.05816 (2019).\n\n[5] Saglietti, Luca, Stefano Sarao Mannelli, and Andrew Saxe. \"An Analytical Theory of Curriculum Learning in Teacher-Student Networks.\" arXiv preprint arXiv:2106.08068 (2021).\n\n* \"Thus it is reasonable to assume that curriculum learning [...] can have an influence on calibration.\" Honestly, I do not really see the connection. The strategies used in the cited papers are very different. They are either modifying the loss by adding regularizers, or augmenting the dataset with generated data, or modifying the labels with label smoothing. It is true that they do not modify the architecture, but this does not seem a good reason for expecting good results from curriculum learning. I may have missed some connection here. \n\n\nTypos (not evaluated in the recommendation score):\n* page 2: \"in recent work\" -> \"in recent works\"\n* page 8: \"forgo\" -> \"remove\"",
            "summary_of_the_review": "The paper is not ready for publication. The results are unreliable and the paper appears to be unpolished. Nevertheless, I think that their results may be publishable upon running more simulations and polishing the paper.  ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}