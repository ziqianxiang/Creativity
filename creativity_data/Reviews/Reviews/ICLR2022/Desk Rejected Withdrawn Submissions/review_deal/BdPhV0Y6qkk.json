{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Performing deep network training is expensive. A common practice is to perform MixUp or CutMix. This paper investigates using MixUp and CutMix to interpolate 2 examples such that training time is speed up. The interpolation requires corrections to work, which motivates a corresponding correction strategy for MixUp and CutMix separately. The paper reports up to 1.6x speedups on ImageNet and 1.8x speedups on Cifar10.",
            "main_review": "Strengths:\n* There is a nearly 2x improvement in training time, which is significant.\n* The correction strategy for CutMix seems novel.\n\nWeaknesses:\n* It's bad form in renaming MixUp and CutMix to LinAvg and RandPatch. There isn't a reason why the latter is a better name other than to confuse the reader (unless there is some change, which isn't clear to me).\n* The mathematical notation isn't clear, which detracts from the work. For example, the interpolations are presented for arbitrary N number of inputs, but then the paper only uses N=2, which makes it identical to MixUp and CutMix. Equation 2 states that the interpolated loss, Loss(X, y_i), is equal to the loss of some random input i, Loss(x_i, y_i). Furthermore, equation 3 left hand side has no label, and reformulates the MixUp/CutMix loss to be an affine combination (is this true?).  I would encourage removing any unnecessary deviations from prior literature.\n* The figures do not have a descriptive caption. The captions should provide a minimal description of the figure's intent and observations.\n* I found the text from 3.2 until 4 to be hard to follow because of the above issues. The figures and results that motivate the interpolation corrections are not convincing to me, since I cannot understand them. It needs to be clear how the algorithm is derived (with supporting data).\n* While I can guess what interpolation is doing with respect to epochs and iterations, I don't see a concrete definition in the paper. Is the epoch one pass over the original dataset, where one \"pass\" consists of touching each example once via a single interpolation? For example, with 100 datapoints and batch size 1, is one epoch 100/2=50 iterations, since each interpolation yields 2 examples? Is the maximum speedup 2x because of this?\n* FixRes reports 79% accuracy or more, so the 75.3% number in the paper seems lower than expected. Why is there a discrepancy?\n* No theoretical reason why this approach would work.\n* Nit: A result presented in abstract (therefore a main result) is pushed to appendix (i.e., 7.3). The result should at least be summarized in main text.\n\nAdditional supporting experiments:\n* A comparison with selective-backprop is missing in evaluation (\"Accelerating deep learning by focusing on the biggest losers\" by Jiang et. al. 2019). It seems selective-backprop is similar when r_{E+1} in \"Adaptive Interpolation\" is biased toward one high loss. Is InterTrain recovering selective-backprop?\n* A comparison with SGD where each epoch has the same number of iterations as the proposed method. This would allow ablation for epoch-level effects. Is skipping iterations sufficient to reach target accuracy?\n\nQuestions:\nWhy does Cifar10 benefit from interpolation in 7.3?",
            "summary_of_the_review": "Overall, the paper raises an interesting empirical technique to speed up training. However, I find the presentation to be very hard to follow. A clear and convincing presentation is necessary for an empirical technique like this to be adopted; therefore, my current score is low. My score would be higher if 1) it was clear how the proposed method worked 2) stronger baselines were used.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to accelerate training by reducing the number of training samples in each epoch by selectively combining up to two samples. The model structure is slightly modified, after the feature extractor, the feature map is split into two parts following the combination pattern,  then the two parts are sent to two separate FC layers, and compute two losses separately.",
            "main_review": "1. Although this paper shows a lot of empirical results and findings, there is no solid theory to guarantee good performance in more general cases. No matter how carefully to choose the images to combine from the dataset, some features will unavoidably mix and harm the accuracy. Interestingly, the authors only consider classification tasks in the evaluation, and such simple tasks are more resilient to noise (caused by feature interference). I guess the proposed solution may not work well on semantic segmentation tasks.\n\n2. It is difficult to understand the introduction without knowing what the two interpolation operators do. They are not complicated and the authors can briefly describe them in the beginning. The word \"interpolation\" is misleading, the operators simply add pixels up with some weights, but \"interpolation\" makes readers falsely assume some changes in the image height and width.\n\n3. The performance upper bound is 2X training speed if we combine up to 2 images. Is it possible to combine more images like 3 or 4 without too much accuracy loss?\n\n4. The split propagation network has two FC layers, but we know the FC layer accounts for a lot of computations. Although parallelism helps here so there is not much latency increase. But the split network consumes more FLOPs and the server can only run fewer training tasks concurrently.",
            "summary_of_the_review": "The idea is interesting, but the paper feels not solid. There are only empirical results but no performance guarantee. Also, the authors only test simple classification tasks but did not try tasks that are more sensitive to input mixing noise like semantic segmentation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper attempts to accelerate the training of deep neural networks (DNNs). Unlike existing methods, the proposed methods for DNN training speedup are based on the mixing of training samples, while maintaining the same number of training epochs but with a reduced number of training samples per epoch compared to the vanilla baseline. Experiments on ImageNet and CIFAR10 datasets with the ResNet family and MobileNetV2 are provided.",
            "main_review": "**Strengths:**\n\n- DNN training acceleration is a critical topic in the deep learning research.\n\n- Applying data augmentation for DNN training speedup is interesting. \n\n- Experimental results on ImageNet show that the proposed methods can get speedups in training to some degree.\n\n**Weaknesses:**\n\n- Technical novelty.\n\nThe authors directly extend mixup (ICLR 2018 work) and cutmix (ICCV 2019 work), two popular data augmentation methods to accelerate the training of DNNs, under the context of maintaining the same number of training epochs but with a reduced number of training samples (some are mixed samples) per epoch compared to the vanilla baseline. Specifically, two tricky methods, namely Adaptive Interpolation for mixup and Split Propagation for cutmix are first used to address so called interference issue between two mixing inputs when performing training sample mixing, which empirically leads to poor model accuracy with a reduced number of training samples per epoch compared to the vanilla baseline. Furthermore, a heurisitc method called selective interpolation for how to perform input mixing during model training is presented to improve the training performance. However, to me, the proposed methods lack of technical novelty: (1) note two works (mixup and cutmix) are seperately considered, which breaks the formualtion of the proposed methods; (2) the proposed method based on either mixup or cutmix is rather starightforward, lacking new theoretical methodoloy although some experimental studies are interesting; (3) mathematical annotations/definitions are messy; (4) last but not the least, \"input interpolation\" is somewhat missleading (also to the renaming of mixup/cutmix), as it just refers to the particular mixing (mixup and cutmix) of two input samples, but not common interpolation methods. For the conventional interpolation, the authors should consider more methods but not just mixup and cutmix.  \n\n- Experiments\n\n(1) Two basic baselines are missing. (a) Reducing the number of training samples to x% (to get expected training speedup) while maintaining the same number of training epochs and other hyperparameters as the vanilla training; (b) Reducing the number of training epochs to x% of that for the vanilla training. (a) and (b) may also easily get training speedup while retaining similar accuracy compared to the vanilla training. \n\n(2) For results comparison, how to calculate the speed-up ratios for the proposed methods, and especially for the state of the art methods, is not clear enough. Furthermore, for fair comparisons, the training time cost (e.g., in hours) should be reported for main experiments. Unfortunately, they are totally missing. \n\n(3) How about the extra memory cost for the proposed methods? \n\n(4) Comparisons with the state of the art methods are not fair. It is not clear how did the authors select the results for the state of the art methods. Did the authors use the same training settings? Did the authors report the optimal results? To the best of my knowledge, for FixRes (NeurIPS 2019 work), it can easily achieve impressive speedup performance (2.3x) yet with pretty higher top-1 accuracy (over 77%) for ResNet50 on ImageNet, see page 8 of the original paper.\n\n**Others**\n\n- The writing of the paper needs improvements. Here are some grammar errors or typos: \n\n  on page 1, \"to produce a interpolated input\" \n\n  on page 3, \"WITH INTERPOLATED INPUTSS\" \n\n  on page 8, \"are presented Section 7\" \n\n  on page 8 \"compared to to the baseline\"\n\n- A lot of paper citations are not formal, e.g., mixup, cutmix and etc. are well published papers but not preprints.\n\n- It seems the authors modify the format style in some aspects, for example, the margin to the section title.",
            "summary_of_the_review": "Please see the comments in \"Main Review\".",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concern.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The submission introduces InterTrain: an approach that decreases the size of the training-set by combining samples through interpolation techniques, aiming to reduce training time. Two interpolation approaches from the literature are adopted by this work (LinAvg and RandPatch), whereas analysis is limited into combining pairs of input samples to single images. \n\nIt is first shown that naively incorporating these techniques harms accuracy, because of interference between the interpolated input samples. To address this issue, an adaptive weighting for loss term and a spatial separation for feature maps methodologies are proposed for the case of LinAvg and RandPatch respectively. \n\nFurthermore, a loss-based metric to identify which samples are amendable to such interpolation methodologies adaptively at runtime, is introduced, restoring the accuracy of the proposed approach to a comparable level with baseline SGD approach, while realising training time speed-ups of up to 1.8x. ",
            "main_review": "Strengths:\n- The proposed approach studies the approach of dataset sample mixing (broadly used for reguralisation purposes) from the new perspective of achieving training time reduction.\n-Although the proposed technique comprises many new components, it manages to keep the workload overhead load, realising important speed-up with minor effect on accuracy. \n-The manuscript offers useful insights on training stability and sample amenability issues (and metrics) to interpolation that can be of general interest to the community. \n\nComments:\n- Limiting the analysis to a maximum number of constituent input samples =2, restricts the maximum attainable speed gains to 2x. It would be interesting to experiment with larger values of N, in order to demonstrate the full potential and scalability of the proposed approach, exposing potential obstacles that future work (or the community) can handle.\n- It is not clear if Eq.2 is trying to capture a constraint that needs to be fulfilled by the combined categorical loss for each interpolated sample pair, or what the combined loss would ideally approximate. Since the former is difficult to be met (e.g. for highly-confident predictions of the original model (on samples where y1 != y2), an experimental analysis of how the left and right hand-side loss terms compare in real data would be of value.\n- Fig.1 shows accuracy-over-epoch for the baseline, naive and intermediate result whereas Fig.10 compares accuracy-over-training time for baseline and final proposed approach. It would be interesting to show how the final approach behaves in comparison to baseline and intermediate approaches on the same x-axis (e.g. add final approach on Fig.1).\n- The experimental evaluation has shown that the proposed approach does not compromise accuracy while providing training time speed ups. However, validation accuracy is not the only important metric to claim model equivallence. The reviewer believes that other aspects  such as: generalization capabilities, robustness to adversarial attacks, resilience to quantisation etc. should be studied, or at least discussed, to better quantify the impact of the proposed training scheme to the resulting model.\n- The paper only studies simple interpolation methods from the literature and addresses their interference issues with solutions tailored to the interpolation scheme. It is not clear, however, whether alternative, more advanced, mix-up based augmentation techniques can be employed (e.g. PuzzleMix, ResizeMix, SaliencyMix etc) can be employed, whether they would offer notable advantages (e.g. increased scalability) and whether the proposed solutions to interference would be applicable is such schemes.\n- The paper does not position or compare against relevant works from the filed of dataset condensation (eg [1]):\n\nPresentation:\nOverall the paper is well-written and easy to follow and has a clear structure. Figure legends and captions are not self-explanatory which slightly disturbs the flow). \n\nTypos:\n- (Sec3.1 - Title) : INPUTSS -> INPUTS\n- (Figure 9) : Red arrow does not point towards a specific point in the plot. \n\nReferences:\n[1] Zhao, B., Mopuri, K.R. and Bilen, H., 2021, March. Dataset Condensation with Gradient Matching. In Ninth International Conference on Learning Representations 2021.",
            "summary_of_the_review": "Overall, the proposed approach is interesting, demonstrates important training time speed-ups without a notable effect on accuracy, and offers some useful insights for the community. The novelty is harmed by the fact that the work studies existing approaches for data-augmentation as a means of accelerating speed-up, and the fact that the proposed solutions to sample interference challenges seem tailored to the employed interpolation techniques. The scalability of the proposed approach is not studied, as a strong assumption of up to 2 samples being interpolated into 1 is followed across the manuscript. Positioning/Comparison against relevant work is not complete and there are more limitations of the proposed approach that need to be explicitly discussed. \n\nAs I consider that there is merit in this work and the overall direction that it follows,  I am willing the improve my score, if the issues raised on the \"Comments\" section are satisfactorily addressed. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}