{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper shows the benefit of training a single transformer model jointly on multiple tasks, where each task involves samples of different modalities (e.g., image, audio, and video) coming from different datasets (e.g., ImageNet, Kinetics, AudioSet). The main contribution is to explore various design considerations when co-training ViT on different tasks/modalities/datasets. Specifically, the authors take the standard ViT model as their architecture and explore different ways to schedule the co-training procedure by varying the order in which different tasks are used to form the gradients for the SGD update. They also test whether it's beneficial to share all the parameters across the modalities or to maintain modality-specific parameters. Experiments are conducted on 9 classification tasks defined over different modalities (image, audio, and video). Results show that co-training shines especially on a small-scale data regime, where it acts as a regularizer preventing high-capacity models from overfitting to small datasets (which conforms to previous results such as Caurana 1997). \n",
            "main_review": "\n**Strengths**\n\n* I like the motivation of this paper, i.e., training a single transformer on multiple modalities and making it work well across various tasks. This is well-motivated both _intuitively_ (human perception is inherently multimodal and, obviously, we all have only just one brain!) and _technically_ (the inner-workings of transformers are agnostic to modalities and thus it makes it easy to study computational modeling of multimodal perception from a single unified model).\n\n* Nice empirical results showing the benefit of co-training a single transformer model on multiple tasks with different modalities, especially on the low-data regime.\n\n* Paper is easy to read.\n\n**Weaknesses**\n\n* The paper lacks technical novelty. The paper focuses on exploring different ways to co-train ViT on multiple tasks. But multi-task learning has been studied extensively (the paper cites Rich Caruana's paper dating back to 1997). The paper might be considered as a modern take on multi-task learning in the transformer era. But the authors simply take ViT pretrained on ImageNet as their model and train/test it on different combinations of datasets under different design considerations.\n\n* Most of the analyses are heavily empirical and lack generalizable insights. For example, one of the main questions asked is how to schedule the co-training procedure. The authors try several straightforward strategies, i.e., task-by-task, alternate task sampling, uniform task sampling, weighted task sampling, and accumulating gradients (graphically illustrated in Figure 2). The results show that the weighted approach works overall the best. But the discussion/analysis is heavily empirical (e.g., the last paragraph of Section 4.2), and leaves the question of whether the results are generalizable to scenarios other than the particular combination of datasets/modalities studied in this paper.\n\n* The paper emphasizes that co-training a single transformer across multiple modalities has the benefit of being parameter-efficient. In this paper, this is achieved simply by sharing parameters across modalities. However, there are recent developments in this area that propose more advanced techniques, e.g., sharing parameters across layers [A] and performing low-rank factorization of weight parameters across modalities [B]. Given that this paper tackles the problem of training multimodal transformers in a parameter-efficient manner, the authors should provide more compelling baselines in this area. \n\n[A] Zhenzhong Lan et al. \"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.\" ICLR, 2020.\n\n[B] Sangho Lee et al. \"Parameter Efficient Multimodal Transformers for Video Representation Learning.\" ICLR 2021\n",
            "summary_of_the_review": "Although I like the motivation of this paper, it appears that the paper lacks technical novelty and insights. I am recommending rejection at this time. In the rebuttal, please focus on addressing the two main concerns (novelty and insights) if they haven't come out clearly in the submitted version. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes to train a vision transformer (ViT) simultaneously on multiple tasks from three different modalities: image, video and audio. The linear projection part and task-related head of ViT is modified accordingly to suit the needs of multi-task training. The authors also explore different co-training techniques such as task sampling. The resulting PolyViT is trained on various large-scale datasets from different modalities and has achieved state-of-the-art performance on video/audio classification tasks with significantly reduced model parameters. ",
            "main_review": "Strengths\n1. The idea of training a ViT from seven different tasks of different modalities seems to be new in this field, and the authors have also explored relevant training settings that can work for such a multi-task multi-modality ViT. The modifications of ViT's linear projection part and task-related heads seem to be reasonable.\n2. The achieved results on video and audio classification is promising: the state-of-the-art performance is improved with significantly reduced model parameters. \n3. The paper writing is generally easy to follow.\n\nWeaknesses\n1. I didn't spot sufficient technical novelties in this paper - it simply takes a ViT model and changes the ways of performing linear projection and task-related heads. The core model is still a transformer, while the idea of multi-task and multi-modality is very common. Even the authors have argued they are first to perform on a large number of tasks and modalities, the novelty here is still not significant enough.\n2. I may have missed some details, but it seems to me that for the video classification task, all the three datasets are used to train the PolyViT? How is the baseline model trained? Is it trained on a single dataset? Please clarify/justify this problem, as the comparison can be unfair if your model is trained with a larger amount of data. Similar problems hold for audio classification results. Besides, have the PolyViT already been pre-trained on image datasets when applied to video/audio classification?\n3. The authors have involved three modalities, but only reported the state-of-the-art results on two modalities: video and audio. How is the performance compared with state-of-the-art results in terms of image modality?\n4. There is only one baseline reported in Tables 4 and 5. Please include more baseline methods for a comprehensive analysis.\n5. It can be a natural idea that the proposed PolyViT can be utilized to perform audio-visual tasks. It will be better if authors can discuss the potentials and perform such tasks as future work.\n   ",
            "summary_of_the_review": "Overall, this paper demonstrates a ViT that is trained on multi-task from different modalities. Although the authors have reported promising results on video/audio tasks, the weak novelty is a major concern, while I still have some confusion on the experimental settings. For now, I will vote for a marginally below.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "* The paper proposes a method for training a shared model for nine different classification tasks in different modalities like image, video and audio.\n* The proposed architecture is similar to the Vision Transformer (Dosovitskiy et al., 2021) and its extension to video (ViViT, Arnab et al., 2021) and audio (AST, Gong et al., 2021). These models have the same architecture and the main difference between them is in the tokenization process. Hence this paper proposes to use a common model across the modalities with a co-training method. \n* Each classification task belongs to one of three modalities. The input embedding operators, positional embeddings and modality specific class tokens are shared by all the tasks within a modality. The transformer layers are shared between all the tasks and each task has its own output head.\n* The architecture also includes the option to have modality specific layers before feeding their output to the shared transformer layers.\n* One of the main contributions of the paper is the comparison of different mechanisms for sampling and scheduling minibatches from different tasks. The experiments show that the weighted task sampling performs the best, where the weight is determined based on the number of training steps in the single-task baseline model.\n* Experiments are performed with two model sizes - Base and Large. The models are initialized using ViT which is pretrained on ImageNet-21K. The best performance is obtained when the models are trained in a multitask setting for each modality separately. A jointly trained model across all the modalities leads to some amount of degradation compared to the individual modality models.\n* There are also experiments to demonstrate that the feature representation learned by the joint model can generalize to new tasks from any of the three modalities.\n",
            "main_review": "## Strengths\n* The paper compares a few different interesting ways for scheduling minibatches from different tasks. The findings from this experiment can be useful for research on multitask problems.\n* The experiments are thorough with two different model sizes and a variety of tasks and datasets. The experiments with modality adaptor layers, linear probing are all useful in providing insight into the working of the proposed architecture.\n\n## Weaknesses\t\n* One of the claims in the paper is that the proposed shared model leads to parameter efficiency. This is slightly misleading because the number of parameters as mentioned in Tables 2, 4 and 5 is actually the number of parameters during training. The baseline number of parameters is counted as the sum of parameters from 9 different models. However, the number of parameters during inference is just the number of parameters in one model since only the task specific model would be used. Hence, this doesn’t lead to any parameter efficiency with respect to inference.\n* The weighted task sampling is a bit confusing. One might assume the weight is based on relative dataset sizes. However, it is actually dependent on the number of training steps in the single task baseline. This form of weighting is non-traditional and might not help in reproducibility. There can be multiple hyperparameter combinations with varying number of training steps that might give the same performance as the single task baseline.\n* The results (Table 2) on tasks from audio modality don’t seem to benefit from training on multiple modalities. The best performance on audio tasks is obtained with a multi-task model trained on audio modality only. This result doesn’t help in justifying the benefit of co-training. \n* It will be interesting to see if the co-training can be performed with text based tasks as well. The video tasks cannot be considered a completely different modality since they share a common input with the image modality. It is natural for a model trained with more images to perform well on videos due to this commonality. This is also reflected in the results in Table 3 where a model trained on image modality performs decently well on video tasks and vice-versa. \n* The paper claims that the method does not need hyperparameter tuning but it is not clear what property of the architecture leads to the conclusion. Would the results improve if the hyperparams are actually tuned? In a direction similar to the linear probing experiments, what would happen if you continued finetuning just one of the original tasks after training on all the tasks. Would it improve the results?\n* Section 4.5 compares the results with state of the art. The proposed method is labeled as “single modality co-training” which is in fact just multi-task training. Hence, the novelty of the method is limited.\n",
            "summary_of_the_review": "The paper identifies that some of the state of the art models for image, video and audio tasks are all being built on top of the same architecture of Vision Transformer. Hence, it proposes to combine them all together in a co-training method with the motivation of obtaining better performance through generalization across the different modalities. However, the results are not convincing to show that co-training helps in generalization and provide better performance when compared to training modality specific models. The comparison of models with the number of training parameters is not justified since the number of parameters and the amount of computation during inference is unchanged. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper aims at simultaneously training a vision transformer-like architecture on multiple tasks of different modalities. One can think of the proposed approach as a multi-task learning framework. However, it is worth noting that in most common MTL setups, for a single given input we have multiple output labels e.g given an image, predicting its surface normals, depth and semantic segmentation etc. In contrast, this work deals with a number of distinct single-output tasks but tries to solve them all simultaneously via aggressive parameter sharing. Authors perform extensive set of experiments to explore different choices of weight sharing strategies, co-training procedures and downstream evaluations. ",
            "main_review": "The research objective of this paper is very valuable. Given the continuous increase in the complexity of models, both training and deployment will naturally see non-trivial complications. That is, a move towards unified models is of great importance and I think this paper has been able to share a few valuable insights in this regard. However, there are also major concerns, specifically in two folds, namely novelty and experimental setup, for each I'll provide further context in the following.\n\n**Novelty** \n\nAuthors have identified three earlier works where each pre-trains ViT-like architectures on a separate modality. This paper has brought them (already pre-trained) together into the proposed PolyViT setup. This in my opinion does not pass the bar for technical novelty unless similar efforts have not been conducted before. In addition to what authors discussed in the last paragraph of Section 5, there are other missing references which have explored similar directions. Please refer to those papers below. The lack of sufficient technical novelty could have been less important if the comprehensive experiments (very good job on that front) were conducted in a better setup. More on issues with experiments and consequently insights that were obtained, later.\n\n**Missing references**\n\n- \"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval\", ICCV 2021 \n- \"Parameter Efficient Multimodal Transformers for Video Representation Learning\", ICLR 2021\n\nNote that, one may have appeared too close to ICLR 2022 submission deadline. The notion of sharing transformer parameters between multiple modalities in a joint training setup has been explored in the above works too. \n\n**Experiments**\n1. In Sec 4.2, authors associate the relatively worse performance of the \"Accumulated\" strategy with the need for using a single learning rate (lr). In order to make such a conclusion, experiments where lr of video (audio) tasks are utilized while respective video (audio) tasks have improved are needed. Without those, it is hard to make the aforementioned argument. The reason behind the worse performance of \"Accumulated\" can simply be the competing gradients of different tasks canceling one another.\n\n2. Maintaining task specific lr seems to indicate keeping multiple optimizer instances at the same time. What are the memory consumption consequences here?\n\n3. For \"Weighted\" strategy, it is mentioned that Pets task is only sampled 0.11% however something which is being overlooked is the large coverage of animal categories that exist in ImageNet-1K. Training on those instances directly helps Pets task.\n\n4. In regards to Table 1, the gap between best and worst co-training strategy is much smaller in Image tasks than video or audio ones. Any insights on why do we see such behavior?\n\n5. The major concern with the choice of datasets is the fact that ImageNet categories are encompassing of (or closely related to) most of the label set associated with the rest of image tasks. The same story is true for video and audio tasks. Hence, what is being referred to (bottom of page 6) as the regularization effect of co-training is less likely instead, it seems that PolyViT is enjoying larger effective training data simply by jointly training on tasks with large overlap between their corresponding label sets.\n\n6. In both Tables 2 and 3, performance of $L_{adapt}=L/2$ is very rarely meaningfully better than that of $L_{adapt}=0$ despite having dedicated modality layers which to me sounds counter intuitive. Any thoughts on that?\n\n7. In Table 3, PolyViT is being compared against single-task baselines. The point that authors seem to be making is rather given in my opinion. PolyViT uses the same architecture as single-task baselines, even the same initialization of weights, yet is trained on larger data than single-task baselines e.g PolyViT is trained on K400+MiT while ViViT is trained only on MiT similarly with AST where it is trained on VGGSound while PolyViT uses VGGSound+MiniAS. With all that, it is not clear why should not it be obvious that PolyViT at least be as good as single-task baselines.\n\n8. Focusing only on the number of parameters while PolyViT uses significantly larger amount of training data does not paint a completely fair picture of how single-task baselines should be evaluated.\n\n**Minor** \n\nIn Section 3.2 where \"Task-by-Task\" is being described, there seems to be some notation issues. While $U_j$ is clear, I am not sure what does $U_{j1}$ means. I am guessing authors meant the portion of $U_j$ in first SGD step fo task j. Writing can enjoy some clarifications here. Same goes for \"Alternating\" and Weighted\"\" where it is not clear what $M$ is.",
            "summary_of_the_review": "The research direction is very valuable however there are concerns regarding the technical novelty and more importantly experimental setup. I am looking forward to the rebuttal where I expect authors to comprehensively address multiple detailed feedback given in the main review.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}