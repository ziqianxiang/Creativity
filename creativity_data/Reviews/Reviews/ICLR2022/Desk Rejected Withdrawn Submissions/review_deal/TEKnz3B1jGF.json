{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work contrasts the ability of different ML models to predict fMRI recordings from 2 fMRI datasets. The authors aim to investigate whether ML models which jointly process visual and text information are better able to predict fMRI recordings than ML models which process each modality separately. The paper concludes that indeed models that jointly process visual and text information predict fMRI recordings of people viewing pictures (without any text) better, as well as people viewing pictures and words together. The authors conclude that this improved performance is due the vision-language models being able to incorporate both types of information, but I don't believe there is enough evidence to support this claim (see Weakness below).",
            "main_review": "Strengths:\n- code is made available and the authors use publicly available fMRI recordings\n- investigating an important and timely question about the interaction between language and vision\n\nWeaknesses:\n- not enough evidence to support the authors' main claim \n   - the authors find that the ML models that combine visual and language information better predict fMRI recordings, and their conclusion is that this is due to the fact that these models have access to both types of information, and they speculate that this is evidence that even in the fMRI data where participants did not see any language (e.g. BOLD5000), the vision processing is influenced by language. This is a huge speculation. There are other possible explanations of why the models that combine vision and language better predict fMRI recordings, that are not investigated or even discussed by the authors. For example, it is plausible that the vision+language models have more parameters and are better able to encode brain-relevant properties of the stimulus that are modality-specific, and not necessarily cross-modality. How do the sizes of these models (i.e. number of parameters) compare to the vision-only and language-only models? How does the dimensionality of the embedding used as input to the encoding models compare across models? Can the authors do more to convince the reader that the improvement in prediction performance of vision+language models over vision-only models is due to the added linguistic information? For example, what happens if you randomize the language captions in BOLD5000 and feed the model the correct image with a wrong caption, and train the model to predict the correct image-elicited brain recording? \n- insufficient statistics provided\n   - this work aims to make scientific conclusions so it can be improved by stronger statistics. The authors should include errorbars across subjects in Fig 3, and do hypothesis tests for significant differences between models (e.g. paired t-tests for paired samples across subjects). They should report p-values that are corrected for multiple comparisons across models\n- the authors focus entirely on comparison with vision-related encoding literature, but there is a rich language-related encoding literature that is also relevant. The same argument that the authors make about the plausibility of language influence during visual processing can be made about vision influence during language processing (e.g. mental imagery). For example, the authors say that the vision-related literature has focused on encoding performance in a few parts of the brain, whereas the authors aim to provide improved prediction performance on the whole-brain level. I invite the authors to survey the language-related encoding literature (Jain and Huth 2018 NeurIPS, Toneva and Wehbe 2019 NeurIPS, Schwartz et al. 2019 NeurIPS, ...) and observe that much larger parts of the cortex are predicted significantly.\n- the clarity can be improved:\n   - the result figures are difficult to parse because they contain a lot of information. I would recommend the authors work on highlighting the important results in the main paper. One way Fig 3 can be improved, for example, is to color the models within the same type (i.e. visuo-linguistic, vision-only, language-only, late-fusion) with different shades of the same color.\n   - there are numerous grammatical and stylistic mistakes that are at times distracting (e.g. \"Enabling effective brain-computer interfaces needs understanding how the human brain...\" -> \"requires understanding\" or \"requires us to understand\", \"For both the datasets, we train..\" -> \"For both datasets\", \"three subjects engaged in 5254 natural images\" -> \"engaged in viewing\" or \"viewed\", \"we extract the layer-wise features from different pretrained CNN models such as VGGNet19...use in predicting fMRI brain activity\" -> I'm not sure what the authors meant to say). The paper can benefit from being proof read several times \n- limited methodological novelty\n    - this work uses previously established methods for brain encoding, and does not make advances either in the methodology or in the interpretation of existing methodology. By itself, this is not a fatal flaw (a paper can have value to the community without such novelty).",
            "summary_of_the_review": "While this work investigates an important and timely question about the interaction between language and vision, it provides very weak evidence that the authors interpret very strongly. The main results that vision+language models predict fMRI recordings better than vision-only and language-only models can be due to other reasons than the one claimed by the authors (see under Weaknesses for more details). These should be investigated and discussed. The paper also needs to include hypothesis testing for their claims and improve in clarity.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper compared the encoding performance on two fMRI dataset (BOLD5000 and Pereira) between a set of machine learning models, with a special focus on the visual transformers and multi-modal transformers. Encoding model was fit for each subject separately, and results were reported after averaging across subjects. The authors did cross-validation to assess the encoding model generalizability, \nThe model performance was evaluated by 2v2 accuracy, Pearson correlation, and representation similarity analysis. The general conclusion is that multi-modal transformers perform the best for brain encoding.",
            "main_review": "The paper presented many results. Some of them are interesting and novel, for example, the ones using the latest multi-modal AI models (e.g., VisualBERT, CLIP) for brain encoding. However, I had a hard time to figure out new insights from current results in the paper, other than the general (but a bit vague) statement that 'multi-modal transformers perform the best'. \n\nFirst, how brain response from different regions are characterized by the encoding model? Specifically, the 2v2 accuracy across all models and across all brain regions (Fig. 2, Fig.3) seems to have only very subtle difference. To my understand, all the cosine distance and Pearson correlation are calculated as 'spatial similarity' within a brain region. Did the authors look at the temporal correlation (which I think is more widely-used for evaluating model predictivity)? Is temporal correlation also show similar value across all brain regions?\n\nI don't quite understand the results shown in Fig.5 (left). It seems the MAE map doesn't seem to have an understandable pattern across brain regions. And the colorbar should not have negative part since this is absolute error. And for Fig.5 (right), did the authors do any thresholding? If so, what statistical test was used? Were these results based on single subject or based on group-level analysis?\n\nIn the abstract, the authors claimed \"we observe a better encoding correlation between Transformer model layers and the levels of visual processing in the human brain when compared to CNN architectures\". It's not obvious to me how this conclusion can be drawn from the results. First, it's not clearly stated features from which layers were used to get results in Fig.1 to Fig.4. Second, the similarity values in Fig.5 are very small, were they robust and significant? The VisualBert results didn't show a similar hierarchy as in the human visual system. But prior works have shown CNN captures such hierachy, e.g., with lower layers best predict early visual areas. While it seems the multi-modal transformers have worse performance for early visual areas, which makes sense to me since the input for its first layer is already a vectorized region feature. Such model doesn't have spatial information in the image pixel space that can be correlated to the retinotopic mapping in early visual areas.",
            "summary_of_the_review": "Investigating the representation in visio-linguistic AI models and compare it to cortical representations through encoding models is an interesting and important way to understand multi-modal processing, in both brain and machine. The current paper did a bunch of comparative analyses across different AI models, claiming multi-modal transformers perform the best. However, the clarity of the writing needs improvement and the validity of the statements are questionable given the current results.\n\nOne suggestion is, instead of doing comparative analyses across many machine learning models (the number of available models continues to grow rapidly), like the current focus of this paper, I think it is necessary to breakdown at least one model (e.g. VisualBERT or CLIP) and do an in-depth analysis to understand how representation at different layers in the model are mapped to the different regions in the brain, how multi-modal representations emerge from uni-modal inputs in the model and in the brain, etc. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper tests the similarity of CNNs, text transformers, image transformers, and multi-modal models to neural fMRI recordings in BOLD5000 (image presentation only), and Pereira (word + image presentation). The main claim is that a multi-modal transformer (VisualBERT) outperforms all other models on the two datasets. Additionally transformers are claimed to outperform CNN architectures in their similarity to fMRI recordings.",
            "main_review": "### Strengths\n1. studying the utility of multi-modal representations and their similarity to brain recordings is novel and an intriguing question\n2. the paper tests a range of different models (a total of 19), two different fMRI datasets, and several similarity metrics to compare model and brain representations\n3. based on their findings, the authors make a testable prediction at the end for a future fMRI experiment\n\n### Weaknesses\n1. unfair comparison of models: The multimodal transformers receive additional ground-truth text input such as an object/scene label or image caption even when human subjects were presented only with an image (in the BOLD5000 experiment). This means that the set of models with text input have access to ground-truth information that the image-only models do not have access to, and additionally makes them no longer image-computable. This could be addressed by instead _predicting_ the text, e.g. with the visual component of the multi-modal network.\n2. no statistical tests if one model is significantly better than others and no error bars. To support the claims throughout the paper that one model (e.g. VisualBERT) is better than other models, the scores need to be tested statistically for significant differences. Especially with fMRI recordings, the data are notoriously noisy and it's unclear to me whether VisualBERT is really better or if the differences are within the noise levels. In Figure 8b, there does not appear to be any difference in the abstract vs concrete results (Abs2Conc vs Conc2Abs), even visually. \n3. The RDM results are glossed over even though here VisualBERT is no longer the best model. I suspect that you found the RDM results to be even more noisy than the predictivity measures (2v2accuracy and pearson-correlation), but this should be explicitly mentioned and discussed -- for instance, if everything is within error bars for RDMs but not the other metrics, then RDMs are just a worse metric for differentiating models.\n4. confusing presentation of results: The figures are really difficult to read without color grouping and one giant figure at the top with dozens of unlabeled bars underneath. I suggest you color-group the models into CNN, image transformers, etc. and perhaps put some labels inside the bars as well. To simplify, you could also only show the best model for each group in an overview figure. RDM results in Figure 4 should also go with their respective 2v2 and pearson figures, i.e. merge with figures 2 and 3. I am also not sure why IncV2Res+C2D150 performs on par with VisualBERT in figure 9, but seems to be missing from figure 2. Are only output layer representations used for model-to-data comparisons? If that is so, then this must be changed -- it is well established that internal layer representations often outperform output layers, especially for early visual areas (e.g. Yamins, Hong, et al. 2014; Cadena et al. 2019; Schrimpf et al. 2018). \n\n### Minor\n1. the introduction keeps confusing the fMRI and the macaque electrode-recording literature: Yamins, Hong et al. 2014 is followed by \"this line of work, namely brain encoding, aims at constructing neural fMRI [...]\" -- this work was done with Utah electrode array recordings, not fMRI; the second paragraph on page 2 (\"There exist [...]\") starts with predictions of fMRI activity but then again uses a range of references to studies based on electrode recordings. It's totally fine to reference the electrode studies, but the text should not confuse the two\n2. page 1, \"the two most studied forms of stimuli include vision and language\" -- citation needed\n3. page 2, \"They manually choose particular CNN layers whose activations are used for accurately predicting brain fMRIs specific to the datasets they work with\" -- isn't the earlier mentioned CORnet anatomically mapped to cortical regions in primate cortex?\n4. What is meant by \"removing the need to manually select specific layers as in existing CNN-based fMRI encoding architectures\"?\n5. I'm not sure what you want the reader to take from figure 5?\n6. section 5.4, why not show results for the best CNN model in this study, InceptionV2ResNet, rather than an older baseline?\n7. It's unclear to me how exactly the model results lead to the experimental prediction in section 6? This seems to assume strong top-down task-dependent modulation of visual brain representations, are there supporting references for this kind of modulation in fMRI literature?",
            "summary_of_the_review": "As much as I like the direction of the work to test multi-modal representations of a range of models on image and image+text neural recordings, there are too many issues for me to like this paper. There are serious issues with the main claim of the superiority of multi-modal transformers (unfair comparisons, lack of statistical tests, inconsistent scores with RDM metric, on-par performance of InceptionV2ResNet). While the paper tests 19 different models, I still find that selection too limited to claim state-of-the-art -- model families like DenseNet, CORnet, or adversarially trained networks are not tested at all for instance. The presentation of results also needs to be improved to make this paper better accessible to readers. Addressing these concerns will likely be too much for an iteration at ICLR2022, but I think there is a lot of good content here, that, with a bit more work (see suggestions above), could be a significant contribution to the field.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper investigates the potential of multimodal transformers in predicting fMRI activity. Using two public datasets, BOLD5000 and Pereira,  the authors perform the following experiments to answer different questions:\n\n1. The authors compare the encoding performance of multimodal models with language and vision models using encoding and RSA to demonstrate whether multimodal information leads to better prediction of fMRI responses.\n2. The authors compare the RSA correlation of different layers of a transformer showing a uniform similarity structure from lower to intermediate layers. Thus, they make the claim that it removes the need to manually select specific layers as in ConvNets.\n3. They perform cross-validated encoding where the encoding model is trained using images from one dataset (e.g. MS COCO) and evaluated using other images (e.g. Imagenet) to demonstrate how well the results obtain generalize to this setting.\n\nOverall this paper makes an attempt to show that multimodal transformers are better encoding models than single-mode visual and language models. While overall the results are convincing there are some flaws in the analysis that make the above claim weaker. I discuss those in the main review.",
            "main_review": "### Strengths\nThe key strength of this paper is the experiments designed to investigate the main hypothesis of this paper. Below I describe them in detail: \n1. To show whether multimodal models are a better choice for comparison with fMRI responses, multiple visual and language models are used as controls.\n2. A stronger cross-validation is used. Here instead of simply selecting a subset of data as train and left out as the test, the authors have selected the images from one dataset as training and images from another dataset as test. The results using this setting shows that they are generalizable even across different image domains\n3. The evaluation is performed on two different datasets BOLD 5000 (visual stimuli) and Pereira (Visual + Text)\n\n### Weaknesses\n1. The visualization of encoding results is too naive. The readers are supposed to associate 19 different colors to 19 different bars which is difficult visually. My suggestion is to color code them according to model type (e.g. Multimodal transformers one color shade, visual transformers other color shade and so on). This will make the message very clear that one type of models is consistently better than other types which is difficult to assess in the current plots.\n2. There are no statistical analysis to validate whether one model is better than other. While VisualBERT on BOLD5000 is clearly showing a higher correlation, it is not clear in Pereira whether VisualBERT correlation is significantly better than other models.\n3. A crucial thing to note from all the encoding plots is that only VisualBERT is clearly better than visual models (and only on BOLD5000), while other multimodal transformers are either similar or in some ROIs worse than visual models. Therefore, the claim that multimodal models are better is not correct, instead, it should be VisualBERT is better. I encourage authors to investigate why particularly this is the case.\n4. In RSA analysis (Figure 4), the authors claim on average VisualBERT shows higher correlation with several brain regions but it is important to note that visual models (VGG, Resnet, EfficientNet and DEiT+Patch) also show similar or higher correlation that VisualBERT on BOLD5000 dataset. This fact is not even acknowledged in the paper. It should be investigated further why in encoding VisualBERT is much better but using RSA it is similar or lower than visual DNNs. Thus, the key claim of the paper is weakened further here due to this discrepancy. One possible reason between the discrepancy in RSA and encoding results could be the dimensions of the featuremap. While in RSA difference in dimensions of the features may not play a major role but in encoding higher the dimension of the featuremap more the number of weights for ridge regression this could lead to significant problems if one feature map is very high dimensional and other is low dimensional. Therefore, I encourage authors to investigate this and report the feature map dimensions for each model in the supplementary. Another option to compare models using encoding is to only use top-k PCA components of the feature maps while keeping k constant for all the models.\n5. The claim that since they use the transformers which shows a more uniform correlation across all layers for all ROIs , it avoids manual selection of layers is misleading. There are some differences in the RSA correlation e.g. in Figure 6 VisualBERT in RHLOC differs from 0.079 to 0.07. While on hand authors use these small differences to claim one model is better than others, on a different experiment they claim that they are uniform. \n6. I agree that in the above case, the correlation between layers of transformers is more uniform than CNN models, however, this suggests that CNNs are more hierarchically similar to the visual cortex with early layers showing higher correlations with early visual areas while deeper layers show high correlation with higher visual areas. On the other hand, uniformity in Transformers suggests that they are less similar to the visual cortex with no hierarchy.\n",
            "summary_of_the_review": "The key idea of the paper to show multimodal transformers and visual transformers are a better choice for predicting brain responses is novel and important to investigate. The experiment setting i.e. use of two datasets, several models and controls, comparison on different metrics, and stronger cross-validation are well designed to investigate this question. However, the results presented in the paper do not support the key claims made in the abstract and introduction due to 1. lack of statistical tests, 2. Failure to acknowledge and investigate discrepancy in results using two different metrics for comparison, and 3. Missing deeper investigation and discussion about why one model outperforms other. \n\nOverall I believe this paper would benefit significantly by addressing the weaknesses I mentioned in the main review. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}