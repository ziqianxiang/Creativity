{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents an adversarial imitation learning algorithm that is easier to train than adversarial imitation learning, \nnamely GAIL and WAIL. Unlike GAIL and WAIL the method relies on off-policy learning to train the policy. It operates on additional \ndata that are weighted combinations of expert and the learned policy's trajectories. Its discriminator tracks the \"level of combination\". \nThe method is evaluated on two benchmarks Y-junction, which exhibits a multi-modal agent behavior, and the motion capture dataset. \n\n",
            "main_review": "I have to admit that I had some difficulty in understanding what exactly the paper is trying to do. The starting point is the work \nthat is done in neural relational inference (NRI) and dynamic neural relational inference (DNRI) which the paper casts in a behavioral \ncloning light. Will in some sense this is true, at least NRI (though I do not know for DNRI) strictly speaking is not a BC approach\nsince it does not maintain actions. NRI is a trajectory forecasting method that given a part of the trajectory of some physical system\nforecasts its future. If we want to cast it in a BC setting that means that not only it chooses an \"action\", which we never see, but it\nalso learns the responce/dynamics of the environment; it does that in the single step of predicting the next state given the past states.\n\nThe paper then shifts to an imitation learning context, and proposes its approach SS-MAIL. The standard setting of imitation learning, \nat least GAIL, requires the presence of actions and the use of simulators to train the policy. However I believe that the motion capture\ndataset does not come with actions; having said that there are imitation learning approaches which do not require actions, which come \nunder the heading of learning from observations only, e.g. GailFO. In addition nothing is said of simulation, is there simulation involved\nin training the policy, something which I would expect since the policy is trained using SAC. In any case there is no comment about the\navailability/use of actions as well as of simulation.\n\nThe paper discuss an off-policy variation of GAIL where the basic change comes in the loss of the discriminator which now is a regression \nmodel. There are off-policy variants of GAIL, e.g. Sample-Efficient Imitation Learning via Generative Adversarial Nets, Blonde, Kalousis, \nwhich make use of gradient penalty and enforce smootheness on the discriminator. In fact such a smoothness has been shown to be required\nfor the models to converge, Lipschitzness Is All You Need To Tame Off-policy Generative Adversarial Imitation Learning, Blonde et all, and \nit is enforced over data points that are interpolations between the expert data and policy data, which are generated in a pretty similar \nmanner as the off-policy data are generated in the present paper. \n\ndetails: \nsection 3.1.3 \"As stated previously in our discussion on the discriminator approximating a vector fiell...\" I am not sure I see where in the paper\nthis previous discussion/statement was made.\n\nsection 3.2.1 what is z_{i,j}^t? is it a latent variable? does it correspond to the statistics of the interaction graph?\n\nsection 3.2.2 the sampling is not very clear here, are these really weights or rather binary values that determine the structure of the graph network? \n\nsection 4.1. as I said above there are things I do not get about the method, does it need/operate with actions, does it use a simulator to go to the next\nstate? these are also reflected in the description of the experimental data which is very rudimentary. \n\nsection 4.2 What is the definition of error used here? is it some kind of MSE between the generated trajectories and the true ones? and how does that link \nwith what we see in figure 4 which seems to be a reward?\n\nsection 4.3. \"The gradient of the policy training loss should be in the direction of a specific mode, instead of the weighted average over all the modes.\"\nI am not sure how this can happen, unless there is an explicit mechanism in the modelling, e.g. explicitly accounting for multimodality. In the sense that \nwhen the model is faced with data that come from a mixture of modes it should have a way to assign these data to their respective modes and only update the \nparameters of the modes. There might be something like that going on due to the way the latent variables are sampled to structure the graph that is used in \nthe policy network.",
            "summary_of_the_review": "While the paper makes some interesting comments on the relation of learning dynamics and learning to imitate it falls short of putting the context in a rigorous manner, leaving a lot of space for confusion. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes multiple techniques for imitation learning (IL), including 1) a self-supervised discriminator, 2) a graph-based actor critic, and 3) a teacher forcing curriculum. The proposed techniques aim to address several issues in IL, including 1) training instability, 2) improving learning multi-modal experts, and 3) improved training speed. Two experiments are presented to support the proposed method.",
            "main_review": "Strength:\n1. The paper introduced multiple ideas to address several important challenges in imitation learning.\n\nWeakness:\n1. The initial motivation does not appear well justified. Given that GAIL, and adversarial imitation learning in general, has complex training dynamics, it is difficult to assert from one toy example that GAIL's reward landscape \"fails to provide dense supervision\". In particular, the motivating example seems to be using only state-based reward R(s). It is also unclear if the same phenomenon applies to state-action reward R(s, a). On a related note, In what sense is the reward (Fig 1b) optimal and how is it obtained?\n\n2. The proposed method appears to have two orthogonal components, namely the new discriminator and the graph network-based multi-agent actor-critic. It is unclear how they are related and or have any synergy. If the proposed contributions are orthogonal, it makes more sense to evaluate each component separately (see Point 5 below).\n\n3. It seems that combining expert and agent trajectories could only work in very limited cases (e.g. when state is 2D coordinates). In the current manuscript, it is also unclear whether actions are considered part of the trajectory and whether new actions are also generated via linear combination. More generally, it appears that naive linear combination of action/states could lead to invalid action/states. Moreover, it is difficult to interpret the inclusion of -1< \\alpha < 0 for linearly combining two trajectories. Could the authors motivate this design decision?\n\n4. The proof for Theorem 3.1 is too brief to understand. Could the authors formalize what \\psi_SS and c(s, a) is for the proposed algorithm?\n\n5. The empirical evaluation is too limited and compares to too few existing methods. As discussed above, the new discriminator could be evaluated independently from other contributions, and the authors are encouraged to evaluate the proposed discriminator on standard benchmarks (e.g. Mujoco). For multi-agent experiments, the authors may wish to compared to [1, 2], which are IL methods tailored for multi-agent cases.\n\n6. The details of the two testing environment are very limited and makes it difficult to understand the experiment conditions. This in turn makes interpreting the experiment results difficult. Some specific questions are:\n    \n    a. Why are the two environments multi-agent settings? Are agents cooperative or competing? In particular, why is the Mocap data considered a MDP process and and a test environment?\n\n    b. Why only subject 35 is used in the second environment? \n\n    c. What are the evaluation criteria for successful imitation learning?\n\n    d. What are the state and action spaces respectively for both environments?\n\n[1] Generating Multi-Agent Trajectories using Programmatic Weak Supervision, Zhan et al.\n\n[2] Multi-Agent Generative Adversarial Imitation Learning, Song et al.",
            "summary_of_the_review": "The paper introduces multiple techniques for addressing several challenges of imitation learning. However, the proposed techniques appear disorganized and not well justified. The techniques also appear to be orthogonal to each other with little connection between them. Lastly, the experiments are limited in scope and not well described, which makes interpreting the results difficult. I thus cannot recommend acceptance of this paper in its current form.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors proposed a method called Self-Supervised Multi-Agent Imitation Learning (SS-MAIL), which introduced a self-supervised loss that encourages the discriminator to approximate a richer reward function. The method leverages a graph-based multi-agent actor-critic architecture that learns a centralized policy conditioned on a learned latent interaction graph and introduces a teacher forcing-based curriculum (called Trajectory Forcing) that improves sample efﬁciency by progressively increasing the length of the generated trajectory. In the experiment, the proposed method outperformed a baseline (dNRI (Graber & Schwing, 2020)) on CMU mocap data, and baselines (GAIL (Ho & Ermon, 2016) and WAIL (Xiao et al. 2019)) on custom-built synthetic environment (Y-Junction). The SS-MAIL framework improved multi-agent imitation capabilities by stabilizing the policy training, improving the reward shaping capabilities, as well as providing the ability for modeling multi-modal trajectories.",
            "main_review": "The strength of this paper is described above. Although the concepts of self-supervised learning and trajectory forcing seem to be new and interesting, the presentation of the method and the significance and validity in the experimental results were unclear to me (detailed comments are below). First, there were many unclear descriptions in the method sections. Second, there is another approach to model multi-modal behavior using stochastic latent variables into the model but it is not mentioned. Lastly, the Y-Junction task may be considered less challenging for me and the motion capture task may not be a multi-agent setting. dNRI used as the only baseline is a method to extract the dynamic relationship between elements in the deterministic approach but not to mainly predict the trajectory in the long term. The specific comments are as follows. \n\n1. Introduction: self-supervised learning seems to have several approaches, but at least the authors’ approach was not fully introduced. \n\n2. Eq.(1): no definition of D and \\pi_E\n\n3. Section 2: “These models are trained using multi-step BC and, therefore, cannot model multi-modal behavior. '' This is an important issue, but there is another approach. A variational RNN (VRNN) or a variant using GNN (GVRNN) [1] addressed this issue in the multi-agent setting [2,3] by injecting stochastic latent variables into the model and optimizing using amortized variational inference to learn the latent variables. This approach may also show better performance in the experiments (4.3 and 4.5).\n\n[1] Yeh et al. \"Diverse generation for multi-agent sports games.\" CVPR, 2019.\n[2] Zhan et al. \"Generating multi-agent trajectories using programmatic weak supervision.\" ICLR, 2019. \n[3] Fujii et al. \"Policy learning with partial observation and mechanical constraints for multi-person modeling.\" arXiv, 2020.\n\n4. Eq. (2): “ which optimizes the min-max objective” but Eq. (2) includes only max. And is \\times \\mathcal{A} superscript? \n\n5. 3.1.3: Fig 3 -> Fig 2? \n\n6. Eq. (6): z is not defined.\n\n7. 3.2.2 was totally unclear. Actor-critic, soft actor-critic, head were not explained well. And the relationships between sentences and equations were not organized. \n\n8. 3.3: Where was Figure 3 described in the main text? And the vertical axis (expected length) was unclear. \n\n9. 4.3: First, the Y-Junction task may be considered less challenging for me. And dNRI used as the only baseline is a method to extract the dynamic relationship between elements in the deterministic approach but not to mainly predict the trajectory in the long term. The above approach ([1-3]) may also show better performance in the experiments.\n\n10. 4.5: The motion capture task may not be a multi-agent setting (I confirmed subject #35 in CMU mocap dataset). About the baseline (dNRI), the comment is the same as the above. And it was unclear what “zero-shot generalization” means. In general, for the definition of zero-shot learning, the source and target domains are defined. In my understanding, there is only one domain (subject #35). \n",
            "summary_of_the_review": "Please provide a short summary justifying your recommendation of the paper.\n\nAlthough the concepts of self-supervised learning and trajectory forcing seem to be new and interesting, the presentation of the method and the significance and validity in the experimental results were unclear to me. Therefore, it is difficult for me to provide a higher rating at this stage. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "no",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}