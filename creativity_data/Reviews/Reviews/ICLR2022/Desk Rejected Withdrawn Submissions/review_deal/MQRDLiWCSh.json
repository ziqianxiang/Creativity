{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a new method for verification of (so-called) semantic properties like hue, saturation, brightness and PCA. The authors propose a new policy for exploring the feature neighbourhood and combine it with GPUPoly to verify robustness to semantic perturbations. They demonstrate the effectiveness of the method on deep models like ResNet18 on CIFAR-10 and AlexNet on Imagenet.",
            "main_review": "Strengths\n1. The problem tackled in this paper is quite important. Most of the work in verification has focused on verification against adversarial perturbations. The problem of verification against more common real-life perturbations like hue, saturation, rotation, lighting changes, etc is important, yet unexplored. This paper verifies some of these properties.\n2. The writing is clear. Fig3 is very interesting. You could perhaps put this or a shorter version of this earlier in the paper or page1 to motivate the problem.\n\nWeaknesses\n1. Discussion of related work: The paper builds heavily on GPUPoly and is very related to Semantify-NN. However, a thorough discussion of the differences between these works is missing. I have the following questions in this regard:\n* Can the authors please describe the technical delta over GPUPoly?\n* How does Semantify-NN verify each sub-neighbourhood?\n\n2. Important baselines and ablation missing: It seems like your only contribution over GPUPoly is a policy to explore the feature neighbourhood. So for a fair comparison with Semantify-NN, you should add a baseline with their policy using GPUPoly as a verifier. This would help us understand how well your policy performs in comparison to the one of Semantify-NN.\n\n3. Unsubstantial technical contribution: Overall the method seems to be a bunch of heuristics to explore the space. The method is not very principled. There is no technical justification, but just a recipe for the policy and heuristic reasons for the different choices.\nEssentially, only two decisions are being made at each step, (i) increase or decrease diameter, (ii) choosing a subset of neurons for refining. Both of them are made in a heuristic fashion. This is alright as this is how it is done in branch-and-bound. But this is not enough for a conference paper.\n* And there is no contribution in the bounding method/verifier. Thus the paper seems to be a simple extension of GPUPoly to feature neighbourhoods. \n\n4. Concerns in Experiments:\n* Table-3: Comparing $d_f$ (pixel perturbation of feature perturbation verified by you) to $d_\\epsilon$ (diameter of $\\epsilon$-ball verified by GPUPoly, makes no sense. You are just verifying along the feature dimension which is very different than verifying in the very high dimensional $\\epsilon$-ball. It feels obvious that the diamter would be much higher in your setting, and these two should not be directly compared. So I have objection with the '99.1x bigger' claim in the introduction because no background is provided.\n* Table-3: Why is Semantify-NN missing from Table3?\n* Table-2: Do Semantify-NN and VeeP use the same verifier? Otherwise this is not a fair comparison.\n* Compute: Are Semantify-NN and VeeP using the same amount of compute? Your method is using an unusually large amount of compute. Most verification papers use 1 desktop pc GPU and you are using 8 server GPUs. Also you are using 1TB RAM. How many cores are you using? And what kinds of parallelizations are you using?  What is the memory on the GPU? Most papers use 1 Titan Xp GPU (or alike) and your GPUs seem to be much more powerful. I am just trying to understand the relative speed of your method to those methods.\n* Please put time directly in Table 3 and also write more elaborate captions for tables so that the tables are self-explanatory. At the moment, they can only be understood after reading the text.\n* Is GPUPoly state-of-the-art? Beta-CROWN [1] and ActiveSet [2] is the published state-of-the-art. These works are not even cited.\n\nI have the above concerns/questions and I am willing to change the rating depending on the response from the authors.\n\n\n[1] Beta-CROWN: Efficient Bound Propagation with Per-neuron Split Constraints for Complete and Incomplete Neural Network Robustness Verification Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, J. Zico Kolter\n[2] Scaling the Convex Barrier with Active Sets Alessandro De Palma, Harkirat Behl, Rudy R Bunel, Philip Torr, M. Pawan Kumar\n",
            "summary_of_the_review": "The authors have proposed a new method for the verification of some semantic properties. \n1. However, the technical delta over existing works seems very small. The method is a simple extension of GPUPoly to the semantic setting. \n\n2. The method seems to be performing well. But the comparison to the existing baseline is not fair (pending clarification from the authors).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper makes a contribution to the formal verification problem of\nestablishing the robustness of neural networks against color transformations of\ntheir inputs.  The contribution is summarised in a procedure that dynamically\nsimplifies the verification problem by refining the bounds of the nodes and\npartitioning the input domain whilst relying on existing tools to solve the\nresulting sub-problems.",
            "main_review": "The verification of neural networks against color transformations is a\nsignificantly more constrained problem than the typical problem concerning\nnorm-ball perturbations. Intuitively this enables the better scalability of the\nverification problem. It is therefore nice to see a work targeting the\nverification of big networks against said transformations. I have a number of\nconcerns however regarding the publication of the paper at its current state.\n\n1. The method does not seem to be exploiting individual characteristics of the\ntransformations it targets. That is, I cannot see why the method is not also\napplicable to norm-ball perturbations. This is not necessarily a shortcoming\nbut it indicates in my opinion that  better gains could have been obtained with\na better exploitation of the constrained nature of color transformations.\n\n2. In my understanding the paper claims that  an advantage of the proposed\nmethod is that it can verify bigger ranges for color transformations than the\nsize of the radii that existing tools can verify for norm-ball perturbations.\nAs this should be the case for most of the existing verification methods\n(because of the more constrained nature of color transformations), I think that\nthe comparison of color transformation ranges and norm-ball radii is not\nindicative of any meaningful notion of  performance of the proposed method.\n\n3. Several of existing methods support the verification of neural networks\nagainst color transformations, see the tools that participated in VNNCOMP'21 [1].\nMany of the  have also strategies for refining the representation of them\nverification problem, including input domain splitting, bound refinement and\nbranching heuristics. I therefore think that the paper should have included\ncomparisons with these tools. \n\n4. Instead the paper compares only against Semantify-NN in an experimental\nsetup for which I have the following concerns:\n    - Only two MNIST fully-connected models are used. This is far from the\n      standard experimental setups in neural network verification whereby\n      various datasets and network architectures and sizes are used.\n    - It is not clear that the gains obtained against Semantify-NN result from\n      the method developed in the paper or from the potentially superior\n      performance (over Semantify-NN) of the underlying verification tool Eran.\n\n[1]  Bak et al.The Second International Verification of Neural Networks\nCompetition (VNN-COMP 2021): Summary and Results, arXiv preprint\narXiv:2109.00498.\n",
            "summary_of_the_review": "Whilst the paper develops a method to verify neural networks against color\ntransformations, it is not clear how it exploits the constrained nature of the\ntransformations, it does not adequately evaluate the method, and it does not\ncompare with the state-of-the art methods.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The subject of this paper is verification of robustness to perturbations of semantic features.\nIn order to be able to verify more properties, a strategy is to split the domain to verify into smaller subdomains, as bounds obtained will be tighter. This paper provides a strategy to follow to decide how to split the input domain into smaller parts.\n\nExperiments are performed on MNIST (verifying robustness to change in brightness), Cifar10 and ImageNet (change in brightness and HSL features)",
            "main_review": "# Value of the framework:\nThe authors claim as their first contribution the introduction of a framework, but it's not apparent what are the benefits of introducing this framework. What are the benefits of \"phrasing the verification task as a dynamic system\"? Why is it more advantageous than just framing it as a prediction task and predicting the optimal splits initially?\n\nIn addition, there are references to branch and bound in the related work section, but there is no similarity that is drawn into the fact that what this paper is describing is a branching policy (except for the fact that this is restricted to a 1D verification problem along a line).\nSome useful reference on this subject:\n[1] - Optimization and Abstraction: A Synergistic Approach for Analyzing Neural Network Robustness\n[2] - Neural Network Branching For Neural Network Verification\n\nIn addition, while the author claim the framework as a useful contribution, it is disregarded for the rest of the paper. While a problem definition is given in Equation (1), nothing is ever evaluated against that definition. Nowhere is the concept of \"velocity\" used or evaluated in the experiment section. There is no evaluation of how well VeeP is doing at solving the problem defined. \n\n# VeeP strategy\nThe summary of the strategy proposed by the authors is:\n- increase the size of the subdomain to verify when there is a large margin in the bounds and there was no recent failures.\n- If the margin is low or the subdomains fail to verify, reduce the size of the subdomain. \n- Pick neurons based on the gradient of the verification margin, the amount by which the activation of that domain can be shrunk and execution time.\nThere is a lack of precision, which makes it impossible to reproduce the work:\nHow do you  define \"recent faliures\"? How do you define \"last neighborhoods quality is high\"? How do you define \"increase the diameter\" / \"decrease the diameter\" /\"adapt the diameter based on recent velocity\"?\nHow do you measure the refinement gain? (do you refine every neuron to check which one can be shrunk? or do you assume that if they shrunk when being refined in the past, they will still shrink again) How do you pick \"all neurons except those with the lowest effect ...\" (is it all but one? all but the lowest 10%) ?\nIf these are all hyperparameters of the method, none of them are noted in the paper, there is no clarification into how they are picked, and there is no evaluation as to whether these are good choices or not.\n\n# Comparison of things that are not comparable \nIn the experiment section, the authors compare in the same table distances in the pixel domain (d) and in the feature domain (delta). On page 2, they claime \"diameter of maximum eps-ball which GPU-Poly verifies is 0.0001 while the diameter of the maximal brightness neighborhood which VeeP verifies is 0.18 (diameter verified in brightness space)\". These comparisons are meaningless as is, because there is no direct relation between those numbers. An absurd example would be to say that GPU-Poly verifies 0.0001 in pixel space, but only 0.0001 in 10x pixel space. This corresponds to the same thing but the numbers are different.\n\nIn addition, when the authors perform actual comparison on pixel space measure, these are also problematic. They make repeated claims throughout the paper. \"Robust feature neighborhoods are larger than robust epsilon-balls\" (page 1). In Table 3, they compare df (pixel space difference of the closest feature adversarial example) vs. d_eps (largest verifiable pixel space eps ball). This is an unfair comparison because one is measuring the diameter in a line (a 1D space), while the other is measuring robustness in a 32x32x3 dimensional space. If you are looking at the volume of the space for which robustness is proven, feature robustness proves a volume of space 0.\n\n## Unclear graphic\nI don't understand what Figure 1 is supposed to represent. What do the black arrow looping means? What does the blue arrow represent (they sometime go into another image, sometime into a box called \"Policy\" or \"verifier\")? ",
            "summary_of_the_review": "This paper presents a general framework and a specific instantiation of that framework for the problem of robustness to feature perturbation. The benefits of the framework are not presented, and it is not used to evaluate the specific instantiation proposed. The method proposed is not described in enough details to be reproducible or useful, and is not evaluated properly.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper presents an approach (VeeP) to efficiently verify adversarial robustness to perturbations of semantic features: that is, verifying that the output classification for all inputs in some domain within a 1-dimensional subspace is identical. \n\nThe overarching idea is to adaptively partition the input domain to optimize overall efficiency, attempting to verify larger (c.f. smaller) regions when the previous verification step was \"easy\" (c.f. \"hard\"). In addition, the proposed algorithm selects a subset of neurons to refine (rather than simply refining all or none), balancing time required to refine the neurons with time required to verify robustness. \n\nVeeP is significantly faster than Semantify-NN, and provides average robustness guarantees that are ~97% of the maximum certifiable diameter.",
            "main_review": "# Strengths\n\n## Novelty\n\nThe paper formalizes an intuitive approach to partitioning a 1-dimensional subspace for verification, and provides a principled framework for selecting the optimal subset of neurons to refine in order to reduce overall verification time.\n\n## Significance\n\nThe paper demonstrates strong empirical results (significant speed improvement over Semantify-NN, and average robustness guarantees that are ~97% of the maximum certifiable diameter)\n\n# Areas for Improvement\n\n## Quality\n\n### Details of Policy Function are Unavailable\n\nVery little detail is provided regarding the policy function $F$ mapping the state and analysis result to the action, making the policy impossible to reproduce.\n\n- The form of $F$ is not provided (is it a trained neural network? a function with constants determined analytically?)\n- Changing the to diameter:\n  - What time window of velocity is considered?\n  - How exactly is the diameter changed based on recent observed velocity?\n  - What does \"quality is too low\" mean when decreasing the diameter? Is this a hard threshold?\n  - What is the exact trigger for VeeP not increasing the diameter when the \"verified has higher velocity for smaller diameters\"? How much higher does the velocity need to be?\n- Determining set of neurons to be refined:\n  - \"Our policy picks for $R_k$ all neurons expect those with the lowest effect on the neighborhood's quality, lowest refinement gain, and highest execution time\" - what proportion of neurons are excluded? How do you trade off the three factors (e.g. a neuron with high execution time but a large effect on neighbourhood quality). What is the relative importance of the three factors?\n  - How do we determine a neuron's refinement gain and (refinement) execution time?\n    - If we carry out the actual refinement: hasn't the time required to refine the neuron already been spent?\n    - If not: do we have a predictive model? How does that work?\n\n### Misleading comparison between incomparable verification problem types\n\nI found the comparison between verification of the feature neighborhoods considered and verification of $\\epsilon$-ball neighborhoods misleading. These two problems are simply not comparable; the feature neighborhoods verified by VeeP are 1-dimensional, while an $\\epsilon$-ball for ImageNet has 224×224×3=150528 dimensions.\n\nThis comparison occurs in at least two sections:\n\n- Brightness & HSL neighborhoods (Page 8): the paper states that VeeP certifies neighborhoods with diameters that are larger by 99.1x.\n- VeeP policy (Page 6): the paper states that \"Our key assumption that there is a relation between close verification steps ... is what enables us to certify deep networks trained for high-dimensional datasets, for which verifiers often fail even for very small diameters because of overapproximation errors.\" Are these verifiers which are failing due to approximation errors being applied towards verifying low-dimensional or high-dimensional neighborhoods?\n\n## Significance\n\nI'm not sure I understand the purpose of the section on PCA features. It's not surprising to me that the classifier is more robust to perturbations in a particular direction; how does this suggest that PCA features are linked to global robustness properties? (For example, if you sampled five random vectors and re-ran the same analysis, would you find a similar gap between the most robust feature and the least robust feature?). I'd recommend leaving this section out unless a stronger statement about PCA features can be made.\n\n# Additional Comments\n\n## Quality\n\n- In the section on \"Time analysis\", the paper claims that execution times can be \"linearly reduced by increasing the number of GPUs\". The problem is presumably not perfectly parallelizable; are there experiments demonstrating the claimed linear improvement, and when does the linear relationship no longer hold true?\n\n## Clarity\n\n- Page 2, Paragraph 2, last line: \"the diameter is measured in the brightness space\" - what does the brightness space mean here? Does it correspond to pixel space?\n- The work refers to the size of the verified neighborhoods as the \"diameter\" - is what is meant here \"radius\" instead? The fact that the PCA analysis has to consider two features per PCA dimension suggests to me that this is not a diameter.\n- Spelling:\n  - In Figure 1: \"brigthness\" -> \"brightness\"\n- Figure 3: The bevel effect on the image is distracting, since it did not exist in the original ImageNet images.\n- Figure 4: The three-dimensional nature of the plot, and the fact that the original and perturbed versions of the image are rotated in three dimensions, is very distracting. I'd suggest replacing it with a table of $d_f$ and the images, potentially sorted by $d_f$.\n- Figure 1, Figure 4: The contraction 'org' is never introduced. (I assume it refers to 'original').\n- Table 2, Table 3: I'd suggest introducing $d_\\epsilon, d_f, \\delta_f, \\delta_{adv}$ in the table description; it's challenging to find the definition in the text.",
            "summary_of_the_review": "I recommend rejecting this paper. \n\nOverall, the paper reads like a work in progress, with 1) no details provided on the core component of the technique, 2) imprecise comparisons, and 3) sections that do not communicate a clear message. \n\nI would encourage the authors to iterate on this work - the paper does have a kernel of a good idea, with strong empirical results and an idea that could be applicable to verifiers in other contexts (namely, deciding which neurons to verify via 1) neuron effect on neighborhood quality and 2) refinement gain). ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}