{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper mainly targets deep metric learning. The authors solve the problem in three steps. They first reformulate the problem into a feasibility problem, then theoretical analysis is provided to show how increasing number of proxies can improve the generalization performance. After that a new algorithm including alternating projection and greedy proxy selection is proposed, which is assessed on several benchmark datasets.",
            "main_review": "Pros:\n1.\tThe methodology part is comprehensive. \n2.\tThis paper provides insightful explanation for proxy-based DML.\n\nProblems:\n1.\tWhile the authors claim that the reformulation and the alternating projection are their contributions, these techniques have already been studied in DML [1]. I suggest the authors reclaim their contribution and discuss the relationship with this paper.\n2.\tThe authors follow the protocol of [2] to conduct experiments. However most baseline results in Tab.1 are different from those in [2]. It would be better to explain the reason of difference.\n3.\tIt is not clear why different ablation studies are conducted on different datasets, e.g. Fig.1 and Fig.2 are based on MNIST while Fig.3 and Fig.4 are based on CUB and Cars.\n4.\tIt would be better to compare Fig.1 with a visualization without proxy selection, which according to the method should be the key to making use of multiple proxies.\n\n[1] Can O, Gürbüz Y Z, Alatan A A. Deep Metric Learning With Alternating Projections Onto Feasible Sets[C]//2021 IEEE International Conference on Image Processing (ICIP). IEEE, 2021: 1264-1268.\n[2] Musgrave K, Belongie S, Lim S N. A metric learning reality check[C]//European Conference on Computer Vision. Springer, Cham, 2020: 681-699.\n",
            "summary_of_the_review": "This paper provides an interesting idea and comprehensive theoretical analysis. However the experiments are insufficient and the contribution needs to be reconsidered given similar method proposed in former papers.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a proxy-based approach for deep metric learning by considering multiple proxies per class which reduces the covering radius and thus improving generalization performance. The method is evaluated on 4 bechmark DML datasets and achieves SOTA results. ",
            "main_review": "Strengths: \n\n1) Building on the current theories on the generalization bounds, the paper attempts to characterize and improve generalization bound for proxy-based DML. The reformulation of DML with chance constraint is interesting. \n\n2) The theory was supported with good experimental validation, specifically the ablation study and comparison with the baseline techniques. The proposed method achieves better performance compared to its baseline and also SOTA techniques.\n\nWeaknesses:\n\n1) One of the advantages of the proxy based methods is that they can generally convergence faster compared to pair based techniques. Since the paper also proposes a proxy-based technique, I would like to see the convergence speed of the proposed technique compared to another proxy-based techniques\n\n2) Using more proxies/classes can lead to increase the total computational complexity of the proposed technique. I would like to see a comparison of the computational complexity of the proposed method with other proxy based techniques (not the timing info.). I see a marginal improvement on medium scale datasets like SOP. Increasing the computations to gain such marginal improvement may not be a good option.\n\n3) One of the interesting applications of the metric learning problem is the scenarios where [Hyun Oh Song, 2016] there are huge number of classes and few samples in each class. Under these conditions, using multiple proxies (proposed technique) might not be helpful since one proxy can be close all the samples in the class. How can the proposed method be extended to such scenarios. I also noticed that the proposed technique significantly increases the cost (time required) for medium scale datasets like SOP. This shows the limitation of the technique for large scale datasets. I would like to see a discussion on this.\n\n4) (Minor comment): The writing could be improved. It would be more readable if the authors move some of the equations to Appendix and explain the intuitions in the main text.",
            "summary_of_the_review": "My rating is borderline. The paper proposes an interesting idea but I see that it works well on small datasets like CUB and CARS where there are more samples in a class (~100 samples/class) and hence using more proxies could help. But I would like to see a discussion about applying the proposed technique to datasets with large number of classes and few samples in each class. \nI would also like to see a comparison of the proposed technique with other proxy based methods on 1) convergence, 2) computational complexity. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents an proxy-based approach to Deep Metric Learning which is formulated as a feasibility problem of finite chance constraints representing similarity and dissimilarty relations. The approach boils down to a standard regularized proxy-based objective which re-initializes multi proxies per class repeatedly over the course of training. Experiments show performance improvements using different DML loss functions.",
            "main_review": "#### Strengths:\n+ The paper is well written and presented.\n+ The evaluation protocol considers the important benchmark sets for DML and also presents ablation studies to provide insights into the learning procedure and optimization.\n\n#### Weaknesses:\n- The presented formulation of (proxy-based) DML is very similar to the work [A], which also performs an optimization based on alternating projections onto feasible sets of constraints. While the presented approach embeds proxy-based learning into the optimization framework, [A] embeds sample-based learning. Although the optmization of the presented model is formulated using chance constraints, the core idea remains similar. Hence, theoretical significance is limited.\n\n- The quantative evaluation lacks comparison to the recent state-of-the-art in Deep Metric Learning. Compared to [B,C,D,E] the presented model performs worse on all reported datasets (CUB200, CARS196, SOP, Inhop). Moreover, these models are based on a single model rather than forming an ensemble of 4 independent models, which achieves the best reported results (512D) in this paper. In particular the work of [B] is also proxy-based and therefore also conceptually similar. Hence, the presented model is inferior in performance to multiple previous works while requiring substantially more training time and resources, which reduces the empirical significance of the presented approach.\n\n- Important state-of-the-art works in DML are not cited in the presented manuscript [B,D,E]. Moreover, also the approach [A] is not cited which is based on a very similar optimization formulation using projections onto feasible sets.\n\n#### Questions:\n- The results for dimensionality of 512 are based on an ensemble of 4 independent models. Why did you choose to use an ensemble to report results using 512d, instead of using an 512 dimensional embedding explicitly (single model) as it is the standard of the most recent works in DML? Showing the corresponding experiment (single model with 512d embedding layer) would be quite helpful for fair comparison to the state-of-the-art.\n------------\nReferences:\n\n[A] Can et al. 2019; Deep Metric Learning with Alternating Projections onto Feasible Sets\n\n[B] Gu et al. 2021; Proxy Synthesis: Learning with Synthetic Classes for Deep Metric Learning\n\n[C] Milbich et al. 2020; DiVA: Diverse Visual Feature Aggregation for Deep Metric Learning\n\n[D] Seidenschwarz et al 2021; Learning Intra-Batch Connections for Deep Metric Learning\n\n[E] Roth et al. 2021; S2SD: Simultaneous Similarity-based Self-Distillation for Deep Metric Learning",
            "summary_of_the_review": "The presented work is conceptually similar to previous works and inferior in comparison to less complex and computationally more efficient state-of-the-art models (which are partly not mentioned in the manuscript). Hence, both theoretical and empirical significance justifying an acceptance is lacking.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}