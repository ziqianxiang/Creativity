{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Optical Pulse Stacking refers to using optical time delays and stacking to achieve some desired optical output. For example, figure 1 shows an example of using stacked time delays to stack several pulses into one higher amplitude pulse.\n\nThe contribution of this work is two-fold. Firstly, they introduce a simulator for controlling optical pulse stacking to maximize pulse amplitude as an RL problem. Secondly, they benchmark several (existing) DeepRL algorithms on this environment.",
            "main_review": "Introducing ML to control and discovery of challenging physical processes is an active area of research of relevance to the community. This work is fairly well-communicated and explains the problem of pulse stacking (and provides a simulator).\n\nThe biggest weakness of this work is that it does not justify why this problem should be modeled as an RL problem rather than just an optimization problem. At each timestep the agent sets the time delays and observes the (stochastic) pulse outcome as the next state, but there is (at least as explained in the paper) no inherent sequential structure in this problem that justifies the use of RL. By posing this problem as an RL problem it appears to making the problem harder (particularly since eq 3 defines the reward in a manner that makes it non-stationary) and this choice is not justified.\n\nFor example, some form of sequential Bayesian optimization seems better suited to this problem structure. At each step of optimization a promising time delay configuration would be tried, the noise result recorded, and based on this result the next choice of time delay parameters chosen. This approach removes the unnecessary sequential structure (the previous experiments don't affect the outcome of the next experiment). Alternatively, if this sequential structure is necessary this should be justified and explained in much more detail (potentially with non RL baselines also included).\n\nBesides the issue above, the other concern about this work is that the simulation seems relatively simply (eq. 4 captures the key idea) and the RL methods standard, so it is unclear how much impact this work will have on either the ML community or the optic community (for example, is this discovering any configurations that are surprising to the optics community?).\n\nMinor Issue:\n\nThere quite a few grammatical errors and I would encourage copy editing prior to publication. However, these don't detract from understanding the paper.",
            "summary_of_the_review": "This work is fairly well-written and explains the optics problem for non-experts. The biggest weakness is the use of RL algorithm (making the problem sequential) when it seems to just be an optimization problem. Besides this, it seems like this work may be of limited interest to the community as it does not introduce much novel ML or provide convincing evidence of ML impact on optics.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes an optics simulation environment to make it easy to apply reinforcement learning algorithms in real-world optical systems. The interface of this environment is the same as the gym. The environment obtains three different difficulty levels and the hardest mode introduces the realistic noise. The training curves of PPO, SAC, and TD3 are provided and trained policies on hard mode are proved can work out in easy mode. \n",
            "main_review": "The reviewer concerns about three things.  \nThe discussion on the model of the reinforcement learning algorithms is missing. Is this process a Markov Decision Process (MDP) or a Partially Observable Markov Decision Process (POMDP)? This should be decided based on the environment. The usage of RNN can make more sense if the environment is a POMDP. \nIt is not discussed yet that why reinforcement learning is suitable for this type of task. Performance about traditional algorithms should also be provided to show the strengths of RL, which can also make it more convincing. \nThe authors claim that realistic noise can be a major contribution to this environment. But it is confusing that there are no experiment results that prove this claim since the realistic noise is actually hard to simulate. ",
            "summary_of_the_review": "Leaning to Accept. I tend to vote for accepting this submission, but rejecting it would not be that bad.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "THe paper proposes a new RL environment with potential to be a new benchmark. It consists of a challenging optics control simulation environment that can be solved with RL. The paper presents the new environment, introduces the state/action notions, and shows that RL can be used to solve it.",
            "main_review": "Strengths:\n+ The environment described is interesting and challenging\n+ Source code will be made available (a must for this kind of paper)\n+ The environment is related to a practical challenge and any advancement in the simulation is likely to benefit the real domain.\n\nWeaknesses:\n- Not enough explanation on why RL researchers should use this domain (see summary of the review)\n- The paper would benefit from more insights regarding how big is the gap between solving the simulation and the real environment that inspired this simulation.\n\nMy recommendation is weak reject, mainly due to the 1st weakness.",
            "summary_of_the_review": "THe authors propose an interesting Optics controlling challenge as a new RL Benchmark. The domain has practical importance and a simulator for this task is very useful, since it is very costly to build the experimental infrastructure needed for performing real experiments.\n\nHowever, this paper is being submitted to a general ML conference, not one focused on the optics applications. This means the main purpose of this paper should be \"convincing\" the reader that the domain has interesting qualities to be explored and analyzed in regard to the agent's learning capabilities. Yet, the paper is mainly presented as an \"additional domain\" to be explored, without delving too much on how this domain differs from the hundreds of already-available domains. \n\nI would expect a more detailed discussion regarding, for example, whether if the n-stage stacking could be more efficiently explored by hierarchical RL algorithms. A good example of showing when this domain would be fitting was the attempt of executing \"transfer\" across tasks of different difficulties. However, the discussion was still pretty shallow and it wasn't clear to me how difficult it is to generalize across the different tasks or to perform sim2real transfer.\n\nUnless you plan on redirecting this paper to a application-focused conference (in which case you will have to focus more on how \"realistic\" the simulation is), some additional effort must be put into explaining why and when an RL researcher should use your domain for evaluations instead of any of the already-available gym environments. In general, I would say that very few people in a AI conference will be specifically looking for this application, which means you have to focus on the interesting \"learning properties\" that this domain enables evaluating.\n\nMainly for this reason, my recommendation for this paper is weak reject.\n\nOn a less important not, Figure 11 doesn't make sense to me. You should have a single \"end task-difficulty\" in each image, showing a graph for the easy task, medium task, and hard task, instead of building a graph for each \"origin task\"\n\n---------------------\nAfter Rebuttal\n---------------------\nNo new information was added in the author response that could change my evaluation. I think the proposed domain is promising, but If the authors plan on publishing this paper on a general AI conference, I suggest rewriting the manuscript to focus on the \"learning insights\" enabled by this new environment, and the features that set it apart from other available environments. If the authors don't have much insight regarding the learning possibilities, I suggest sending the paper to a conference/journal focused on the application, rewriting the manuscript to show how close the simulation is from the real environment.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces an environment that simulates optical pulse stacking (OPS), a physical system that aims at stacking optical pulses to maximize their energy. \n\nPulses can be viewed as an energy function of time E(t), with waves having their pics at periodic times t + NT. \nStaking two pulses at times t+T and t+2T with delay d ends up with energy E(T+d) + E(2T). This output can be seen as new pulses and they can be stacked again, and at the end the output will be a sum of N delayed pulses, with an higher energy. \n\nThe delay d depends on the position of a mirrors system that capture a first pulse and stacks it with the next one.\n\nIn the simulated environment, actions are controlling the positions of mirrors systems, hence the delays of the consecutive stacking. The reward to maximize is the energy of the output pulse.\n\nThey evaluate and discuss the performance of continuous control RL algorithms (SAC, PPO and TD3) used on this environment.",
            "main_review": "Even before discussing the content, my first concern with this paper is that they only introduce an environment simulating a physical system, but no new learning approach to solve it. So I don't think this paper should be published in a machine learning conference, but rather on physics research journal or conf.\n\nThen I have several questions, to make sure I understood the content:\n\n1) Since the function is periodic, without noise the optimal delays are 0, and with noise, they are still of the same value $\\mu_t$ (as it is considered constant during an episode). Then, why not having one single action, that controls an equal space between the mirrors systems?\n\n2) Since the states are the outputs, and the input are independent of the output, $S_{t+1}$ is independent of $S_t$, so what's the point of using the states? TD3 SAC and PPO are assuming the states have a markovian dependancy, so the env is an MDP.\nHere, the env is rather a multi-armed bandit problem with continuous actions, and MAB literature will contain much more suited algorithms.\n\n3) What are the length of episodes / rollouts? and how to set them while there are no markov state dynamics? Also, this impacts the temperature noise $\\mu_t$. Maybe $\\mu_t$ should be the states if it is observable.\n\n\n",
            "summary_of_the_review": "No new learning algorithm, but a simulation of a physical system that can be solved by RL controllers.\n\nIf I correctly understood the description of the system, the used algorithms are not even the most suitable for the considered environment, which is an multi-armed bandit with continuous actions rather than an MDP.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper covers a new simulation environment for optical pulse stacking systems where multiple stages need to be aligned to maximize pulse intensity.  The simulator can model different types of noise and 3 canonical settings (easy, medium [some noise] and hard [non-stationary noise]) are explored.  Some standard RL algorithms, including PPO and SAC are tested with this domain and converge to optimal policies on easier settings and degrade gracefully as noise complexity or the number of stages is increased.  Policy transfer between the levels is also explored.",
            "main_review": "UPDATE: I thank the authors for their response.  I agree that this is an interesting problem, but for an ML conference I think we either need some new lessons about the approaches used in this domain and a more rigorous evaluation of alternative approaches.\n\nI appreciate the contribution of the new simulator and think it should be released as an RL benchmark, but I do not currently see enough of a scientific contribution for this work to be published at ICLR.  Below is a list of some scientific advancements that could warrant publication at a future venue.\n\nThe paper does not currently provide any insight on the tested algorithms based on the new domain.  What have we learned about how different RL algorithms behave under non-stationary noise or the complexity of pulse phases?  Are on-policy or off-policy methods more effective?  Do actor-critic methods outperform value-base methods?  Not all of these need to be answered but as written the paper seems to show that all the algorithms perform fairly well on the easy and medium cases and have difficulty in the hard case but no new insight is provided as to why.\n\nThe most interesting aspect of the new domain (to me) is the non-stationary noise term, which will be difficult for an RL system to model since its value is changing over time and needs to be inferred from multiple observations (requiring an LSTM or other memory module or state augmentation).  However, the state-space description on Page 4 indicates the authors did not provide such information in the state (using only the final stacked pulse intensity).  This is disappointing as the domain seems like a good one for testing the ability for RL algorithms to follow drifting state factors and encode complex representations from multiple timesteps in the past.  But none of these state-feature aspects are explored and it seems like the current representation is not sufficient for modeling the hard domain in a Markovian manner.\n\nIn the introduction, the authors reference non-RL methods that they say are unacceptable in the real world because they require initialization near the global optima.  However, the empirical studies do not include any such baseline.  Would these baselines work in the simulator? Would they be successful in the easy mode but not hard?  And how much better is RL than these heuristic search methods?  The paper does not provide such quantifiable rationale for switching to RL.\n\nFinally, the transfer section was disappointing because the authors mention “even transferring from simulating environment to a real-world experiment”. Unfortunately, no evidence is provided to support this claim.  If the main contribution of this paper is a simulated environment that is supposed to support research on such a physic al system, the authors need to show that the simulation is indeed a reasonable proxy for the real-world version and show that policies from the simulator could actually be used in the real-world and what kind of performance degradation is seen there.\n\nWhile not all of these issues need to be addressed, some of them would need to be fulfilled in order to make the paper publishable in a conference like ICLR.\n\nMinor points:\nPage 6: a couple -> several\nPage 7: as table table -> in Table\n",
            "summary_of_the_review": "The new domain is very interesting but the paper does not show how the new domain provides new insight into existing algorithms or challenges that are not part of other domains.  I don't see strong scientific conclusions that can be drawn from this work yet.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}