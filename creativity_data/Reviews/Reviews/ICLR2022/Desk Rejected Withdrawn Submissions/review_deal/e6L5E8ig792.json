{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The main idea of this paper is to generate linear classifier weights for new classes using a transformer-based hypernetwork.   \n\nThe authors mentioned the differences with non-parametric and parametric classifiers.   \n\nExperiments are performed on the common benchmarks.  ",
            "main_review": "### Hypernetwork\nIntroducing hypernetwork for few-shot learning or meta-learning is not a new idea [R4].   \nThe early work of Meta-LSTM [R1] utilized the LSTM architecture to generate the model parameters.   \nRecently, there are also several works to explore the possibility and benefits of generating class weights with a hypernetwork structure.  \n- [R2] utilized GNN denoising autoencoders to generate classification weights. \n- [R3] generated task-specific classifier weights with an adaptation network.   \n\n\n### Transformer\nExcept for the hypernetwork, the main contribution is utilizing Transformer as the main architecture to generate classifier weights.   \nThis is somehow new. However, it seems Transformer is more than powerful to be adopted here. In Table 8, Transformer is marginally better than the simple Conv1D.   \nThe necessity and effectiveness of introducing Transformer are unclear. And Transformer will increase the computation and parameter load.   \nIn recent work, e.g. [R5,R6,R7], Transformer is adopted to model and capture the feature correlation and interactions, which is more reasonable to take advantage of the powerful Transformer. \n\n### Experiments\nThe improvement over existing state-of-the-art methods is marginal. And some recent work is missing, e.g. [R8]. \n\n\n\n### References\n[R1] Optimization as a model for few-shot learning   \n[R2] Generating Classification Weights with GNN Denoising Autoencoders for Few-Shot Learning   \n[R3] Fast and flexible multi-task classification using conditional neural adaptive processes   \n[R4] Meta-Learning via Hypernetworks   \n[R5] CrossTransformers: spatially-aware few-shot transfer   \n[R6] A universal representation transformer layer for few-shot image classification   \n[R7] Few-shot learning via embedding adaptation with set-to-set functions   \n[R8] DeepEMD: Few-Shot Image Classification With Differentiable Earth Mover's Distance and Structured Classifiers\n\n",
            "summary_of_the_review": "The main idea of this work is not novel enough in the context of few-shot learning.   \nThe effectiveness and necessity of utilizing Transformer as the hypernetwork structure is less convincing.   \nExperimental results is on-par-with the state-of-the-art. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this paper, the authors claim that optimization-based methods for few-shot learning require much time and computational resources and often suffer from overfitting problems. Thus they propose to generate a task-specific classifier for few-shot learning using a transformer (Vaswani et al., 2017). They provide extensive experimental results on three datasets, i.e., miniImageNet, tieredImageNet, and CIFAR-FS.",
            "main_review": "### Strengths\n\n&nbsp;\n\n- This paper is well-organized and easy to follow.\n- The authors provide extensive results on multiple datasets.\n\n&nbsp;\n\n### Weaknesses\n\n&nbsp;\n\n- ***The motivation of the paper is not convincing.*** The authors claim their paper addresses two issues in optimization-based few-shot learning methods: (1) much time and resource required for task-specific training and (2) optimization challenges such as overfitting on only a few samples. For (1), most of the optimization-based methods (e.g., Sun et al., 2019) only finetune the last FC classifier on the few-shot training samples of the meta-test tasks. I don’t think fine-tuning one FC layer on five or 25 samples requires many computational costs. For (2), in my previous experience, the overfitting problem won’t significantly decrease the performance on few-shot learning tasks. The authors didn’t provide any experiments in this paper to show the computational cost problem and the overfitting problem. So I don’t think the motivation is convincing.\n\n&nbsp;\n\n- ***The technical novelty of this paper is somewhat weak.*** The main idea of this paper is based on (Vaswani et al., 2017). It seems an application of (Vaswani et al., 2017) in few-shot learning. \n\n&nbsp;\n\n- ***The state-of-the-art few-shot learning method (e.g., [A-C]) is not compared in Table 4.*** For example, on miniImageNet, [C] achieves 67.40% and 83.40% for 1-shot and 5-shot, respectively. Their results are better than all methods in Table 4 on miniImageNet, including the proposed method. \n\n&nbsp;\n\n### References\n*[A] Zhao, Jiabao, et al. \"Looking Wider for Better Adaptive Representation in Few-Shot Learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 12. 2021.*\n\n*[B] Li, Junjie, Zilei Wang, and Xiaoming Hu. \"Learning Intact Features by Erasing-Inpainting for Few-shot Classification.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 9. 2021.*\n\n*[C] Fei, Nanyi, et al. \"MELR: Meta-learning via modeling episode-level relationships for few-shot learning.\" International Conference on Learning Representations. 2020.*",
            "summary_of_the_review": "Overall, I think the motivation of this paper is not convincing and the proposed method is not novel. I tend to reject this paper unless the authors can address my concerns properly in the discussion period. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a method to generate the classifier head weights for few-shot image classification. It uses a transformer-based hypernetwork to predict the classifier weights. The method is evaluated on several benchmarks and the results show that it can achieve better performance than simply fine-tuning the classifier heads.",
            "main_review": "Pros:\n- It is an interesting idea to directly predict the classifier weights using a hypernet for few-shot learning.\n- The effectiveness of the proposed method is evaluated on several benchmarks. The classifier weights predicted by the transformer-based hypernetwork can reach better performance than protonets and fine-tuned linear classifiers on benchmark datasets.\n- The paper is well organized and easy to follow.\n\n=======\n\nCons:\n- The idea of context-aware decision boundary is not very novel, previous works, like [a], [b] have explored similar ideas. Specifically, [b] also used transformer to obtain task-dependent embedding without optimizing during inference. \n- The improvement over ProtoNet and LR-Aug is marginal (or even worse) in 5-shot 5-way setting (Table 1). From Table 8 other hypernet implementations seem to be even worse. This makes the contribution of using the hypernetwork to predict classifier weights for few-shot learning less impressing, as you have to rely on a specific design to reach comparable results against a simple fine-tuned linear classifier.\n\nReference:\n\n[a] B Liu, H Kang, H Li, G Hua, N Vasconcelos. Few-Shot Open-Set Recognition using Meta-Learning. CVPR20\n\n[b] Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha. Few-shot learning via embedding adaptation with set-to-set functions. CVPR20",
            "summary_of_the_review": "This paper presents a new method for few-shot learning. The idea is interesting, but the significance is somewhat weak as it only marginally improve the performance against simple baselines, and it has to rely on the specific model design. I also have some concern about the novelty of the context-aware classifier. Therefore, I am a bit more inclined to reject.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a few shot learning method using transformer hypernetworks. It is prooved to be effective in the experiments using MiniImageNet, TieredImageNet, and CIFAR-FS.\n",
            "main_review": "The proposed method is straightforward but novel. The improvement from the baseline (ProtoNet) is minor. As in the review of x2L2, in the evaluation using miniImageNet, the proposed method is not SOTA.",
            "summary_of_the_review": "The proposed method is theoretically sound. The evaluation experiments and the ablation studies are well-designed. The comparison with the other methods lacks comparison with SOTA methods. This paper is clearly written.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}