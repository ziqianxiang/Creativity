{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper describes a new method for Bayes-adaptive policy learning. It decomposes the value of an optimal belief-augmented policy into two parts. The first is the \"value of current information\", which is the expected value of playing the optimal policy from one environment sampled from the belief state in a second environment sampled independently from the belief state. The second is the additional value that would be captured by adapting the policy to new information as it comes in. This decomposition is then used to define an exploration reward for TD learning.",
            "main_review": "I enjoyed reading this paper, even though it took a bit of work to get through as someone with only a passing familiarity with the Bayes-adaptive RL literature. The ideas are exciting and I will be curious to see whether they survive the approximations necessary to implement them on practical problems.\n\n**Strengths**\n* To my knowledge, the proposed factorization and learning method are novel.\n* The paper proposes an interesting decomposition for the challenging problem of learning the optimal Bayes-adaptive value function.\n* The results shown on toy problems seem good.\n\n**Weaknesses**\n* This method trades one intractible problem for another: it requires the learning of cross-values $v^{e'}(x_t; e)$ for all pairs of possible environments $\\{e, e'\\}$. It is not clear that this will be an improvement when scaling up.\n* At a few points the paper introduces approximations, but the gap to the true value and the implications of these approximations are not made completely clear to me. The authors should be more precise about the tradeoffs and costs of the methods they propose, both in terms of accuracy and computational cost.\n* On page 6, it claims that estimating $v^c$ according to samples will lead to Thompson sampling-like behavior, which might lead to better exploration. This seems a bit facetious given that this paper attempts to find a Bayes-optimal policy and explicitly points out the weaknesses of Thompson sampling in an earlier section.\n* Not scaled to larger domains, but this is understandable.\n\n**Questions and minor comments**\n* Is the belief state conditioning the policy also supposed to change with time $\\tau$? As written it _looks_ like the optimal Bayes-adaptive policy conditions on one sampled belief about the environment and then plays without updating that belief.\n* It is not intuitive to me how it is possible to estimate $v^f$, despite the Bellman equation written in Eq. 12. It would seem that this update would have to integrate over all possible environments in order to be meaningful, assuming that the true environment is not known at update time. Is that correct?\n* I guess this was probably for space reasons, but the bolded sections in page 6 should really be broken out into `\\paragraph`s — it's currently a huge wall of text.",
            "summary_of_the_review": "This paper proposes an interesting approach to Bayes-adaptive RL. The decomposition sheds some light on the structure of the problem. However it is not clear that this technique will make the problem much more computationally tractable.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper considers Bayesian RL where the agent is required to learn a Bayes-adaptive policy while interacting with a distribution of environments. The main idea is to express the optimal belief-augmented value as a sum of the value of current information and the value of future information, which gives credits for acquiring useful information about the environment. To calculate this, this paper introduces a notion of cross-value which is the value in environment A under the optimal policy for environment B. The empirical results on a tabular T-Maze and a \"treasure map\" grid world environment shows that the proposed method performs better than standard RL algorithms. ",
            "main_review": "* Pros\n  * The main idea is new and interesting. \n* Cons\n  * The presentation of the paper is poor.\n  * Empirical results are weak. \n\n---\n* The notion of cross-value and the decomposition of the belief-augmented value are new to my knowledge and quite interesting.\n* The presentation of this paper needs to be improved significantly. I am not entirely sure I fully understood the entire idea of this paper due to this.\n  1. There are too many lengthy paragraphs, which makes it very hard to read. For example, both Section 1, 2, 5, and 8 consists of only a single paragraph each of which takes nearly one page. They need to be divided. \n  2. The sections need to be re-organized. For example, Section 7 does not seem to deserve an entire section. Section 5 is \"Practical Algorithms\", but Section 6 also describes algorithmic details. Either the section names are misleading or they need to be re-organized.  \n  3. There are many unclear phrases and words. Examples are the following.\n      * What does `q-loss` mean in Figure 2 caption?\n      * What does `the median absolute deviation from the median` mean in Figure 2 caption?\n      * 'the action is chosen to maximize the current estimate of the optimal value through on-policy techniques such as policy gradients' Is this talking about action sampling strategy (greedy selection) or policy optimization method (policy gradient)?\n      * 'This is equivalent to training with the actual ground truth for T tending to infinity, as long as the π-controlled process satisfies the convergence conditions of the posterior to the maximum likelihood estimate.' What exactly is the posterior and maximum likelihood estimate?\n  4. There are many typos. Examples are the following.\n      * 'Since **the** are only two possible environment ...'\n      * '.. as it can **it** interpreted ...'\n      * '**In words**, the value of future information is ...'\n      * **PCR reward** is not defined. \n      * 'PRC-TD' -> 'PCR-TD' in Section 8. \n\n* The empirical results need to be improved. \n  * There is no oracle baseline (which has access to the true environment identity). \n  * There is no analysis of the proposed idea. For instance, one could visualize the learned cross-value to provide insights about how the proposed method works why it is helpful.\n  * There is no ablation study. For example, it is not empirically verified whether the idea of learning the weight $w(x, b)$ w.r.t. $\\mathcal{B}(x,b)$ instead of directly approximating $v^f(x,b)$ is useful. ",
            "summary_of_the_review": "Although the paper proposes an interesting idea for Bayesian RL, this paper seems to go through a major revision to improve the presentation and make the empirical results more comprehensive.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "# Summary & Contributions\n* This paper examines an approach for reducing the sample complexity of solving Bayes-Adaptive Markov Decision Processes (BAMDPs)\n* The key idea behind the approach is the notion of cross value functions which quantify the expected value of acting according the agent's current knowledge of the environment in some (potentially) different underlying environment. This concept allows one to assess the value of current information at any BAMDP hyperstate.\n* The authors consider a reinforcement-learning algorithm that optimizes an augmented reward based on cross values, the so-called predictively cashed reward (PCE, Equation 13).\n* Empirical results in small toy environments highlight the efficacy of the approach over naive Q-learning on the BAMDP hyperstate space as well as other baselines for the policy evaluation experiment. ",
            "main_review": "### Quality\n* #### Strengths\n    * This is an interesting paper that introduces a potentially critical concept for approximate Bayesian reinforcement learning in BAMDPs. The rationale and fundamentals which underlie the approach are nicely laid out. \n    * The mathematical components are simple; intuitive; and elegant, leading to a natural algorithm that seems to pass initial empirical sanity checks on toy domains.\n* #### Weaknesses\n    * ##### Major\n        * I'm quite torn about this submission in the sense that, while it is a nice paper, it lacks a main theoretical or empirical contribution. The experiments conducted in smaller environments are illustrative but are, by themselves, not compelling enough to demonstrate how the concept of cross values aids in accelerating Bayes-adaptive RL in more complex environments. Even one compelling experiment that shows how cross values improve upon the approaches of, for instance, (Zintgraf et al., 2020, Zintgraf et al., 2021) would really ground the paper around a culminating empirical result and take it over the edge. Alternatively, a nice set of theoretical results could also accomplish this.\n        * While the definition of the value of current information in Equation 9 looks reasonable, I can't help but wonder if it is uniquely suited to helping accelerate BADMP learning. Certainly, the literature on PAC-BAMDP methods offers other forms of reward bonuses that do yield provably-efficient learning [12,19]. For instance, rather than integrating out randomness in both e and e', why not take e' to be the environment that has highest likelihood under b_t? Or, alternatively, rather than looking at the expected value across all environments e, perhaps it makes sense to look at the worst case value (minimizing over e) which might have implications for safe or risk-sensitive reinforcement learning? The higher level point here is that, without a corroborating theory, these definitions, while intuitive, seem heuristic and it is unclear if these are the \"right\" quantities to be examining and learning. Why is the augmented reward structure proposed here better than the Bayesian exploration bonus of [12] or the variance-based bonus of [19]? Notably, both of those approaches come with PAC-BAMDP/PAC-MDP guarantees, while the same cannot be said (or at least has yet to proved) of learning under the PCR.\n        * The different in value functions shown in Equation 14 seems to align with the definition of the value of information (VOI) in the context of optimal learning [9,10,15,18]. In the context of multi-armed bandits, such knowledge-gradient algorithms based on VOI will only take an action if there is an immediate improvement in posterior reward. There is an alternative to Thompson sampling [17] which is explicitly designed to address the shortcoming mentioned on page 4: \"a posterior sampling agent cannot learn how to perform actions that are not optimal in any known environment.\" The information-directed sampling (IDS) algorithm, while originally limited to bandits[11,13], attempts to strike the appropriate balance between information gain and regret minimization. Section 4.3.3 of [17] provides a few examples of how an exploration criterion based on VOI is still insufficient for addressing exploration; briefly, the issues boils down to the fact that information revealed at a given moment in time need not result in an immediate performance improvement to be useful, so long as it contributes to performance in the long-term. Highlighting the map example mentioned at the top of page 4, consider a nested version of the problem where an agent must navigate to two separate maps in sequence (for context, say one map to get out of some wrong building and then a second to navigate efficiently in the correct building). Acquiring information by navigating to the first map doesn't result in higher expected return, but is clearly a necessary step towards achieving higher returns and ultimately solving the task. Do the authors have any thoughts on this connection and whether or not a similar impediment arises when using PCR rewards? If the same story holds true, it would again call into question whether or not PCR rewards are the \"correct\" quantity for calibrating information gain.\n\t* ##### Minor\n\t\t* On the point of developing supporting theory for the paper, I can't help but notice that Equation 13 has the exact form of a potential-based shaping function [14], except in the context of a BAMDP rather than the traditional MDP. Have the authors contemplated this connection at any depth? To the best of my knowledge, I don't know of any papers that take the years of work on potential-based reward shaping in MDPs and considers how to invoke those ideas to accelerate BAMDP learning. An updated version of this work could build on that connection as part of a more concrete theoretical contribution.\n\n### Clarity\n* #### Strengths\n    * The paper is both well-written and well-organized. The authors do a fantastic job of navigating readers through BAMDPs, cross values, predictive reward cashing, limitations, experiments, and future work. \n* #### Weaknesses\n    * ##### Major\n        * I would have liked to see explicity algorithms with pseudocode for PCR Q-learning and PCR-TD, just to confirm my intuitions about them beyond the exposition of Sections 5-8.\n\t* ##### Minor\n\t\t* There is some inconsistency in the document between the use of PCR vs. PRC. I believe the authors intended to use PCR and instances of the latter are typos.\n\n### Originality\n* #### Strengths\n    * The authors do a good job of contextualizing the potential of their approach in augmenting and improving recent work on Bayes-adaptive deep reinforcement-learning algorithms.\n* #### Weaknesses\n    * ##### Major\n        * The literature review on more classic work on BAMDPs and Bayesian reinforcement learning seems lacking. For a paper that leans so heavily on these topics and offers a foundational contribution, I would expect a more nuanced discussion of the classic literature [2, 3, 8, 20, 5, 4, 6, 7, 1, 12, 16, 19] without deferring readers to the 2015 survey paper on Bayesian reinforcement learning.\n        \n\t* ##### Minor\n\t\t* \n\n### Significance\n* #### Strengths\n    * The results do a good job of highlighting the potential utility of cross values in Bayesian reinforcement learning. \n* #### Weaknesses\n    * ##### Major\n        * While the conceptual idea behind cross values is interesting, I suspect there is a nontrivial amount of work needed to realize its benefits (if any) in the context of deep reinforcement learning. Without any confirmation of that success yet, it is difficult to see this paper as having high impact. As mentioned above, an alternative contribution would provide supporting theory for cross values and PCR rewards, offering the community a new perspective on how to handle approximate Bayesian reinforcement learning.\n\t* ##### Minor\n\t\t* \n\n# References\n1. Asmuth, John, Lihong Li, Michael L. Littman, Ali Nouri, and David Wingate. \"A Bayesian sampling approach to exploration in reinforcement learning.\" In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pp. 19-26. 2009.\n2. Bellman, Richard, and Robert Kalaba. \"On adaptive control processes.\" IRE Transactions on Automatic Control 4, no. 2 (1959): 1-9.\n3. Dayan, Peter, and Terrence J. Sejnowski. \"Exploration bonuses and dual control.\" Machine Learning 25, no. 1 (1996): 5-22.\n4. Dearden, Richard, Nir Friedman, and Stuart Russell. \"Bayesian Q-learning.\" In Proceedings of the fifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence, pp. 761-768. 1998.\n5. Duff, Michael O. \"Monte-Carlo algorithms for the improvement of finite-state stochastic controllers: Application to Bayes-adaptive Markov decision processes.\" In International Workshop on Artificial Intelligence and Statistics, pp. 93-97. PMLR, 2001.\n6. Duff, Michael O. \"Design for an optimal probe.\" In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 131-138. 2003.\n7. Duff, Michael O., and Andrew G. Barto. \"Local bandit approximation for optimal learning problems.\" In Proceedings of the 9th International Conference on Neural Information Processing Systems, pp. 1019-1025. 1996.\n8. Duff, Michael O'Gordon. Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes. University of Massachusetts Amherst, 2002.\n9. Frazier, Peter I., and Warren B. Powell. \"Paradoxes in learning and the marginal value of information.\" Decision Analysis 7, no. 4 (2010): 378-403.\n10. Frazier, Peter I., Warren B. Powell, and Savas Dayanik. \"A knowledge-gradient policy for sequential information collection.\" SIAM Journal on Control and Optimization 47, no. 5 (2008): 2410-2439.\n11. Kirschner, Johannes, and Andreas Krause. \"Information directed sampling and bandits with heteroscedastic noise.\" In Conference On Learning Theory, pp. 358-384. PMLR, 2018.\n12. Kolter, J. Zico, and Andrew Y. Ng. \"Near-Bayesian exploration in polynomial time.\" In Proceedings of the 26th annual international conference on machine learning, pp. 513-520. 2009.\n13. Lu, Xiuyuan, Benjamin Van Roy, Vikranth Dwaracherla, Morteza Ibrahimi, Ian Osband, and Zheng Wen. \"Reinforcement Learning, Bit by Bit.\" arXiv preprint arXiv:2103.04047 (2021).\n14. Ng, Andrew Y., Daishi Harada, and Stuart Russell. \"Policy invariance under reward transformations: Theory and application to reward shaping.\" In Icml, vol. 99, pp. 278-287. 1999.\n15. Powell, Warren B., and Ilya O. Ryzhov. Optimal learning. Vol. 841. John Wiley & Sons, 2012.\n16. Poupart, Pascal, Nikos Vlassis, Jesse Hoey, and Kevin Regan. \"An analytic solution to discrete Bayesian reinforcement learning.\" In Proceedings of the 23rd international conference on Machine learning, pp. 697-704. 2006.\n17. Russo, Daniel, and Benjamin Van Roy. \"Learning to optimize via information-directed sampling.\" Advances in Neural Information Processing Systems 27 (2014): 1583-1591.\n18. Ryzhov, Ilya O., Warren B. Powell, and Peter I. Frazier. \"The knowledge gradient algorithm for a general class of online learning problems.\" Operations Research 60, no. 1 (2012): 180-195.\n19. Sorg, Jonathan, Satinder Singh, and Richard L. Lewis. \"Variance-based rewards for approximate Bayesian reinforcement learning.\" In Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence, pp. 564-571. 2010.\n20. Strens, Malcolm. \"A Bayesian framework for reinforcement learning.\" In ICML, vol. 2000, pp. 943-950. 2000. ",
            "summary_of_the_review": "# Final Remarks\n* On the whole, this paper seems like an excellent workshop submission. As a conference submission, it simply lacks a main result to rally behind. To have one such theoretical contribution would make up for the lack of a large-scale empirical result or vice versa. \n* Based on Section 9, the authors would like to leave the development of such successes to future work, however I don't find the current contributions of the paper to be sufficient without one of them. \n* I would invite the authors to use their rebuttal to elucidate why the paper, with its current lack of theoretical understanding and empirical validation, still represents contributions worthy of publication at this time.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper considers a Bayesian Reinforcement Learning (RL) setup. This setup can formally be reduced to the usual Markov Decision Process (MDP) formulation via augmentation of state-space with belief states. The key idea of the paper is to decompose the value function into two components: one which can be computed without the knowledge of the belief state (called in the paper *value of current information*), and the remaining residual (called in the paper a *value of future information*). The second component satisfies the Bellman equation with a modified reward function (dependent on the first term). \n\nThe idea is interesting, and the aforementioned Bellman equation should, in principle, allow for the application of the method in deep RL, where function approximators are used.",
            "main_review": "The paper deals with important topic of Bayesian RL. The idea of decomposing the value function into two components to obtain Bellman's equation with modified rewards shows some promise. However, the paper fails to formulate the actual algorithms clearly and does not have a convincing theoretical part. The experimental part of the paper is somewhat limited.\n\nMore detailed remarks are as follows:\n​\n1) The paper should contain clearly described algorithms with pseudo code, explicitly stating the underlying assumptions, what objects are\ntrained, pre-trained, what is the loss function, etc. The addition of the source code would help as well.\n​\n2) If the Authors wish to include theoretical proofs of convergence, an appropriate mathematical rigor is expected (a formal theorem statement accompanied by a proof).\n​\n3) The experimental section could benefit from a more diverse set of environments used to test exploration in reinforcement learning literature.\n​\n4) There are quite a lot of known exploration strategies in Deep RL (e.g. exploration bonus-based methods or ensemble-based methods). Comparison against them could allow showcasing the method further.\n​\n​Other remarks:\n​\n1) Section 3 states multiple facts but does not provide citations (e.g. page 3, line -4, page 4, line 1).\n​\n2) The purpose of Appendix A could be more clear, e.g. why do we consider only two $e_1$ and $e_2$. Furthermore, there is no $\\alpha$ in the definition of $a_t$ in equation (1).\n​\n3) The relation of $v^{e'}$ to $v_{t+1}^{e'}$ in equation (18) could me made more clear.",
            "summary_of_the_review": "The paper provides an interesting idea that shows some promise, however, it lacks in theoretical and experimental aspects. If the Authors improve their paper in any (or all) of these areas, the community could benefit from this research. However, in its current form, the paper is below the acceptance threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}