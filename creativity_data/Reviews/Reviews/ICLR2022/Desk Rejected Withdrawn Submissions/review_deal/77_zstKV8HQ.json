{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes additional two simple objectives，namely entropy maximization and linear independence. The first objective pushes away the representation of the most similar(but different) images. For the second objective, different image representations within the same batch are required to be different (by maximizing the rank of the batch representation matrix).\n\nThe results on CIFAR-10 and Mini-ImageNet show the effectiveness.",
            "main_review": "**Strength**:\n\nThe method is simple. \n\n**Weakness**:\n\n1. My first concern is the comparison which is not totally fair in my perspective. The proposed method involves more views for each image, for e.g., using 10 views to yield superior results in Tab. 1. However, the referred competitors are mostly based on two views. I suspect that this work does not perform best under the same settings. Moreover, more views bring huge memory and computation overhead, which is not practicable in many scenarios.\n2. The paper lacks detailed ablations. Such as the gains of the  proposed losses and their combinations. the gains of the multiple-views, the computation and memory cost and the results on the common-used ImageNet dataset as well as transferring.\n3. The paper lacks discussion with the related works.  Without explicitly using negative pairs is not a new story in the literature (such as VICReg and Barlow Twins),  The paper should validate the superiority over these competitors. \n4. I think the entropy maximization(L_H) and linear independence(L_R) losses still involve negative samples. It treats different images as negative samples.",
            "summary_of_the_review": "Due to the lack of the ablations and the significance of the proposed method. I lean to weakly reject the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new self-supervised learning algorithm, SimMER, which imposes cross-view consistency by minimizing total variance across views, avoids model collapse by maximizing entropy of the sampled batch, and reduces feature redundancy by maximizing the rank of the feature matrix. Experiments on CIFAR-10 and Mini-ImageNet show that SimMER outperforms state-of-the-arts.",
            "main_review": "Strengths:\n1. This paper is well written and easy to follow.\n2. The idea of rank loss is well motivated and shows impressive performance.\n\nWeaknesses:\n1. This paper proposes new objectives for self-supervised learning. However, there lacks comparison with previous loss functions. For example, the proposed rank loss aims to reduce feature dependence, so other loss functions with similar purpose (for example, covariance loss from VICReg ) need to be compared.\n2. Experiments on the full ImageNet dataset are encouraged to fully validate the effectiveness of the proposed method.\n3. In Table 5, it’s counter-intuitive that L_{R1,3,4} which better measure feature dependence performs worse than L_{R2}. More analysis is needed on this phenomenon.\n4. The authors claim that the proposed method dose not need negative pairs, while Eq.5 actually takes nearest neighbors as negative samples. Also, there is no comparison with contrastive loss which also relies on negative samples for alleviating model collapse.\n5. According to Table 4, the proposed SimMER relies on a large number of augmented views for good performance, while most previous works only generate 2 views. So, SimMER may need much more computation cost? Analysis on this issue is encouraged.\n",
            "summary_of_the_review": "This paper is generally well written, and the idea is reasonable. I mainly worry about the experiments and evaluations: comparisons with previous methods in a fair setting, evaluations on large datasets, and more analysis on counter-intuitive results. The authors are encouraged to address the above issues.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new method for self-supervised learning which neither uses momentum encoders, dedicated architectural designs, negative samples, and so on. The method relies on three losses: minimizing total variations, minimizing entropy, and maximizing feature independence. It claims that the method achieves state-of-the-art results on CIFAR10 and Mini-ImageNet although being significantly simple.",
            "main_review": "In my opinion, the paper is very similar to the baseline SimCLR without advancing the field for self-supervised representation learning.\n\n1) As the author mentioned, the entropy loss explicitly uses negatives. This violates the claim made throughout the paper. Since the single hard negative is chosen from a large batch, the claim that the proposed method uses a single negative pair is erroneous. \n\n2) The first term of minimizing total variation is akin to encourage consistency between views. As a result, combining the first and second term, essentially makes the method a SimCLR baseline.\n\n3) The third term of encouraging feature independence has already been proposed for self-supervised learning in : Barlow Twins: Self-Supervised Learning via Redundancy Reduction. The paper is never discussed and compared.\n\n4) Experimentally, the method used 4 views for representation learning, which is a lot larger than baselines with 2 views. The comparisons are thus not fair. Also, ImageNet-Mini dataset is not typically used for benchmarking self-supervised learning. I encourage the results on ImageNet under a fair setting. ",
            "summary_of_the_review": "The paper lacks important literature with regard to prior work, and it boils down to SimCLR after a close inspection. No experimental results are conducted on convincing benchmarks.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel optimization target called SimMER for self-supervised learning,  which explicitly avoids model collapse without negative samples. SimMER is optimized through three proposed loss terms: total variance, entropy and rank. The total variance loss optimizes the distance between each of the K views and their mean in the embedding space. It ensures the model learns a consistent representation of the same sample under different transformations. The entropy term maximizes the distance between a sample and its nearest neighbor in the same batch, to prevent degenerate solutions where all inputs are mapped to the same representation. Finally, the rank loss is used within a batch to remove the linear dependence in the feature dimensions and forces the model to have efficient feature representations. It also prevents the model from collapsing.\n\nSimMER is a novel self-supervised learning algorithm that could learn useful visual representation with a fully symmetric architecture. It also explicitly avoids mode collapse without negative samples using the entropy and rank loss.\n",
            "main_review": "Strengths:\n1.\tThis work proposes a completely symmetric K-head architecture which can be viewed as a generalization of the existing two-head Siamese architecture.\n2.\tThis SSL algorithm makes preventing mode collapse interpretable using the entropy and rank loss.\n3.\tThe learned representations share less redundancy in the feature dimension thanks to the rank loss.\n\nWeaknesses:\n1.\tExperiments are not sufficient. It lacks experimental results on some important benchmarks of SSL, like standard ImageNet. It remains unknown whether this method could have good transfer performance to downstream tasks (e.g., detection and segmentation), which is the common goal of SSL.\n2.\tThe three losses require two additional hyperparameters to finetune,  increasing the complexity. \n3.\tWhen using only the two losses of total variance and entropy, the performance on CIFAR-10 is not better than SimSiam which only uses one loss. (Table.1 and Table.4)\n4.\tFrom Table.4, it seems that much of the performance gain should be attributed to applying multiple views (>2). It is not clear how much gain is from the optimization target itself under the same computation overhead (2 views) with previous works’ settings. \n",
            "summary_of_the_review": "The innovation of the paper is limited. Many ideas are not new in SSL. The performance of the method is not competitive considering its computation cost. Experiment results are also not sufficient.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}