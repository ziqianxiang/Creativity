{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper discusses the issue of gradient imbalance in the context of online continual learning. According to the authors, two potential causes are the imbalance of the data used to guide the optimization and the sequential nature of continual learning.",
            "main_review": "The topic discussed in the paper is novel and very relevant. The authors begin by showing experimental evidence of gradient imbalance from both the data imbalance and the sequential learning perspectives. Then, the authors propose their approach called GAD (Gradient-Adjusted Dynamic online continual learning). The comparison against other baselines is very thorough in my opinion.\n\nHowever, I do have some issues with the paper. First, the description of the proposed method and the motivation behind it (i.e., pages 6 and 7) are not presented clearly enough. Having understood the supporting evidence in sections 3 and 4, it took me 4 or 5 careful reads to fully understand section 5. I think the main issue is that the text at that point is too dense, hence more difficult to understand.\n\nThe second main issue I have with the proposed method is that it assumes that the stream will consist of well-defined tasks and each class will only appear in one specific task. This is a major limitation of the proposed method in my view, as there are no such guarantees in the real world. I do admit that such benchmarks are used to evaluate CL performance (because they are simple to run) but we should not be assuming to know their underlying structure.\n\nRegarding the experimental work, I would like to see a more complete ablation study. The authors picked the two easiest datasets and relatively large memory sizes so I am curious to see what happens, for instance, in CIFAR-100 with a 1k memory or in tiny-ImageNet with a 4k memory. \n\n\\\nQuestions\n- Looking at the ablation study, it looks like the most important individual components of GAD are the separated cross-entropy (which is a minor contribution) and the contrastive loss (which is not a contribution) especially when looking at the CIFAR numbers. Can you comment on this?\n- Do you think your method can be applied in arbitrary non-stationary streams? How would it work if the stream did not consist of well-defined tasks (e.g., if it wasn't obvious which class belongs to which task)?\n- Is there any concrete evidence that continual learners in general exhibit a bias towards the classes appearing in the first task? I would be really interested in reading more on this issue.\n\n\\\nMinor Issues\n- You should probably emphasize that by positive (or negative) gradients you mean gradients corresponding to the positive (or negative) logits. I'm saying this because their actual signs are the opposite (i.e., the gradient of the correct class is p-1 and thus non-positive) and it can be confusing to figure out what you mean.\n- Please, add supporting references for the information mentioned in the last 6 lines before \"Tasks and methods\" on page 3 to strengthen the arguments presented there, otherwise it reads as somewhat speculative.\n- I suggest, using absolute (instead of negative) rates in the graphs of Fig. 2 and 3, since that's the quantity that is used in the proposed method.\n- There's probably a typo in the ER accuracy for CIFAR-100 and 5k memory (the accuracy is smaller than the corresponding one for 1k memory).",
            "summary_of_the_review": "The issue discussed in the paper is novel, but I have a number of objections (i.e., clarity of presentation, assumptions about the nature of the non-stationarity, and completeness of the ablation) and thus recommend rejection. (Regarding the score I think 5 is a bit too generous, but, at the same time, 3 is too harsh, so eventually I went with 5.)",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper analyzes the gradient imbalance when training a model based on ER in online class incremental learning. It suggests that a Negative-Positive (NP) rate of gradients from a certain class reflects the model performance on that class. The authors demonstrate that older classes get smaller NP rates due to the imbalance of samples in the training batch and the sequential manner of class incremental learning. According to these findings,  they propose a new sampling strategy to form training batches that separately constructs two groups: 1)  one is formed by seen classes and each class has equal samples, 2) the other is formed only by new classes.  Following this strategy, a new loss  $\\mathcal{L}_{GAD}$ is proposed to suppress the NP rate for old classes in the group 1  and a cross-entropy loss only including new classes in the group 2 to reduce the interference caused by new classes. The final loss also includes the supervised contrastive loss for both groups.  The experiments show that the proposed method has significant improvements over other baselines. ",
            "main_review": "\nThe paper provides a new view for examing the phenomenon of catastrophic forgetting in online class incremental learning, which is interesting and novel.  The experimental results show that the proposed method can outperform the baselines by a large margin. The ablation study is nice and quite informative. \n\nI think the main idea is interesting but there are some key points that need to be clarified.\n\n1. Isn't  $p^{c_i} -1 < 0$ ? Why do you call it a positive gradient? Did I miss something in Eq.2?\n\n2. In Sec.4.2, the authors analyzed accumulated NP rate in a setting with balanced classes in a training batch. However,  this setting cannot rule out the data imbalance because new classes appear in every batch but old classes may not. I think it would be better to do experiments on a benchmark with fewer classes and each batch can contain samples from all seen classes.\n\n3. According to Eq.8, when A-NP and NP are less than -1, the adjusted gradient would become more negative, how could this help with decreasing the negative gradient?\n\n4. The ablation results in Tab.2 suggest that the main idea of this paper  $\\mathcal{L}_{GAD}$  actually contributes least to the performance compared to the other two losses in the method. I'm wondering what's the performance would be if only applying this GAD loss? Or what if add the separate-ce loss to other baselines?  And does the GAD loss really reduce the NP-rate or A-NP rate? As this relates to the main idea of this paper, it should be examed more clearly. \n\nSome minor issues:\n\n1. The notation should be tightened up, e.g., the true class of a sample sometimes is y_k, sometimes is c_i, really annoying.\n2. This method is proposed for online continual learning, it is possible that there are no clear task boundaries in this online setting, how could the group 2 be decided in such a case?\n3. The definition of $\\mathcal{L}_{sup}$ should be provided.\n4. Is the memory size in terms of the number of samples or the number of bytes?\n",
            "summary_of_the_review": "The idea is interesting and experimental results are attractive, but there are some key points that should be clarified to support the motivation and justification of this work. \nI would give a 4 if there is this rank.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new method for experience replay methods for online CL. The paper is motivated by a set of experiments that study the per-class backpropagated gradient in the classifier layer. This analysis shows a clear explanation for the task-recency bias towards the more recent tasks (often observed in continual learning). Based on this analysis the authors propose a new method GAD that replays in such a way that the positive and negative gradients per-class balance, thereby preventing the introduction of biases. The method obtains excellent results on several standard datasets. ",
            "main_review": "The paper proposes a new method for experience replay methods for online CL. The paper is motivated by a set of experiments that study the per-class backpropagated gradient in the classifier layer. This analysis shows a clear explanation for the task-recency bias towards the more recent tasks (often observed in continual learning). Based on this analysis the authors propose a new method GAD that replays in such a way that the positive and negative gradients per-class balance, thereby preventing the introduction of biases. The method obtains excellent results on several standard datasets. \n\nStrengths\n-\tThe analysis of gradients is interesting (I liked especially the fact that the paper shows why task 1 escapes the task-bias)\n-\tThe proposed method is simple and makes sense (preventing the task-recency bias). It is well motivated.\n-\tThe results show that the simple approach can get very good results. \n-       I like the tool of per-class gradient analysis, and I think it can have a wider impact in CL. \n\nWeaknesses\n- I found the discussion of the most relevant related work very weak. Many papers have discussed the bias-compensation (see section III.c in [1]). Especially [2] should get more discussion than the current one-sentence mention, it is one of the first papers to mention the problem and analyze it. Also, separated soft-max has many resemblances with the proposed paper (separate head for current tasks). \n\n[1] Masana, M., Liu, X., Twardowski, B., Menta, M., Bagdanov, A.D. and van de Weijer, J., 2020. Class-incremental learning: survey and performance evaluation on image classification. arXiv preprint arXiv:2010.15277. \n[2] Wu, Y., Chen, Y., Wang, L., Ye, Y., Liu, Z., Guo, Y. and Fu, Y., 2019. Large scale incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 374-382).\n[3] Ahn, H., Kwak, J., Lim, S., Bang, H., Kim, H. and Moon, T., 2020. SS-IL: Separated Softmax for Incremental Learning. arXiv e-prints, pp.arXiv-2003. (ICCV 2021)\n- I found some parts confusing: e.g. the explanation of the accumulated negative and positive gradient rate was not very clear. It seems kind of obvious that earlier tasks suffer more from this than more recent ones (they have a longer time to accumulate negative gradients). Also the ablation study, the Table could be improved and the discussion refers to 1st, 2nd experiment which takes some puzzling.\n- It would have been nice to have seen the analysis extended to distillation. \n\nMinor (do not need to be addressed in rebuttal):\n- 'some worked on gradient imbalance' needs citations\n- why not do (1-p_k) in Eq (3) then the ratio would be positive, and I think everything is more intuitive to read.\n- I found the notation often difficult to understand. I think with somewhat more effort it should be possible to find an more intuitive and accessible notation (because in the end it is not very difficult what happens).\n- text in graphics too small",
            "summary_of_the_review": "I like the proposed method because of its simplicity and clear motivation. However, since quite some of the ideas are already present in other words, it is very important, in the related work section, and also in the method section to clearly acknowledge prior work which already did similar things. I do think that the gradient analysis is done in more depth and that the proposed method is new. I also think that gradient analysis is a good tool for CL and can be used more widely. Overall, I think the presentation could still be significantly improved. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studied experience replay for online continual learning (CL) from a new perspective of gradient imbalance. It first analyzed this phenomenon experimentally from two perspectives: imbalance of data samples introduced by experience replay\nand sequence of classes introduced by incremental learning. The authors claimed that his problem has not been studied before in online CL and it significantly limits the online CL performance. Based on observations from the experiments and theoretical analysis, the authors propose a new method, called GAD, with a novel loss function to deal with the two imbalances, which is entirely different from the methods used in batch CL to address the data imbalance problem.  Empirical evaluation demonstrated that the new approach GAD helped improve the online CL performance substantially.",
            "main_review": "There are several things to like about this paper:\n\n1. The problem studied in this paper is well-motivated. Gradient imbalance issues in online continual learning (online CL) are pervasive. However, gradient imbalance caused by the incremental learning of tasks and their classes has not been well studied from the research community.\n2.This paper is well-organized and clear written. The authors start from stating the gradient imbalance problem from two perspective: gradient imbalance in terms of the number of samples per class; gradient imbalance caused by the incremental learning of tasks and their classes. Then, the authors present their method GAD to mitigate the gradient imbalance problems\n3. The claims are well supported by multiple tasks on four public datasets. \n4. The paper is very well written. I virtually can not find any obvious errors.\n\nHowever, I found that there are several shortcomings:\n\nMinor issues: \n- in general: better to use comma to separate the mathematical formulas if having multiple lines\n- in general: better to use if/otherwise when defining one variable/function under multiple conditions\n- formula (6): for $v_{y_{k} c_{j}}$, better to explain the definition of $\\wedge$ and $\\vee$\n\n",
            "summary_of_the_review": "In this paper, authors study the experience replay-based approach from a new angle, gradient imbalance. I like it that the authors were able to investigate the problem from a new perspective: gradient imbalance caused by the incremental learning of tasks and their classes.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}