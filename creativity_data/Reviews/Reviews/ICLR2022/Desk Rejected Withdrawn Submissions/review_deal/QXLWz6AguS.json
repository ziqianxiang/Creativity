{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a deep-learning framework that exploits the underlying physics governed by Lagrangian dynamics to improve generalization while inferring dynamics of rigid-body systems from observed trajectory data. In particular, it uses Lagrangian-dynamics based formulation to infer dynamics from data and then extrapolate the inferred dynamics to yield predictions for multi-agent systems. The claims have been demonstrated through relevant experiments.",
            "main_review": "This work's main contribution and novelty lie in its capability to accommodate multi-agent systems wherein the individual agents have identical dynamics. However, this aspect has not been adequately explained. Although the paper mentions that it uses the RelationCells (which have recurrent structures) to accept inputs with varying dimensions and extend the results to multi-agent systems with more agents, it does not provide any clear explanation about these RelationCells and how they work. It is not clear how the RelationCells capture/represent the nature of interactions between the agents. I would strongly encourage the authors to clarify this aspect - without much explanations/insight about these RelationCells, it becomes challenging to evaluate the true contribution of this work. In such Hamiltonian/Lagrangian-based formulations of multi-agent dynamics, researchers use the formalism of port-Hamiltonian theory. In port-Hamiltonian formulation, *Bond Graphs* capture the interaction between the individual agents. I would encourage the authors to explore if they can use the port-Hamiltonian formulation to explain the working of the RelationCells. \n\n\nFollowing are some more specific comments:\n\n\n-- Although the related work section has touched upon relevant topics, it *doesn't provide a comprehensive picture of the current state-of-the on these related topics*. For example, recent work by Massaroli et al. have shown how ideas from dynamical systems and control theory can be used to improve neural networks (*Dissecting Neural ODEs*, NeurIPS 2020; *Port–Hamiltonian Approach to Neural Network Training*, IEEE CDC 2020) - however, this line of work has not been mentioned in the related work section. Furthermore, incorporating physics-based inductive bias into neural networks has been the topic of many recent papers. But the related work section does not reflect whether the authors are aware of the full spectrum of this fast-growing area. For example, *Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control* (Zhong et al., ICLR 2020), *Dissipative SymODEN: Encoding Hamiltonian Dynamics with Dissipation and Control into Deep Learning* (Zhong et al., arXiv:2002.08860), and *Modeling System Dynamics with Physics-Informed Neural Networks Based on Lagrangian Mechanics* (Roehrl et al., arXiv:2005.14617) have looked into incorporating external control forces while incorporating physical inductive bias into neural network architectures. On the other hand, *Unsupervised Learning of Lagrangian Dynamics from Images for Prediction and Control* (Zhong et al., NeurIPS 2020), and *LagNetViP: A Lagrangian Neural Network for Video Prediction* (Allen-Blanchette et al., arXiv:2010.12932) have incorporated physical inductive bias while inferring dynamics from high dimensional video data. These approaches have also been extended to accommodate $-$ Cartesian coordinates with explicit constraints (*Simplifying Hamiltonian and Lagrangian Neural Networks via Explicit Constraints*, Finzi et al., NeurIPS 2020), contacts and collisions (*Extending Lagrangian and Hamiltonian Neural Networks with Differentiable Contact Models*, Zhong et al., arXiv:2102.06794), and measure/process noise through Bayesian formulation (*Bayesian Identification of Hamiltonian Dynamics from Symplectic Data*, Galioto et al., IEEE CDC 2020). In addition, Desai et al. (*Variational integrator graph networks for learning energy-conserving dynamical systems*, arXiv:2004.13688), Jin et al. (*SympNets: Intrinsic structure-preserving symplectic networks for identifying Hamiltonian systems*, Neural Networks, 2020), Jin et al. (*Learning Poisson systems and trajectories of autonomous systems via Poisson neural networks*, arXiv:2012.03133), Chen et al. (*Data-driven Prediction of General Hamiltonian Dynamics via Learning Exactly-Symplectic Maps*, arXiv:2103.05632), and Yu et al. (*OnsagerNet: Learning Stable and Interpretable Dynamics using a Generalized Onsager Principle*, arXiv:2009.02327) have also looked into this type of problems. Some of the recent survey papers - *Integrating Physics-Based Modeling with Machine Learning: A Survey* (Willard et al., arXiv:2003.04919), *An overview on recent machine learning techniques for Port Hamiltonian systems* (Cherifi, Physica D, 2020), and *Benchmarking Energy-Conserving Neural Networks for Learning Dynamics from Data* (Zhong et al., L4DC 2021) - provide a thorough overview of this topic. I would strongly urge the authors to consider this body of work and expand the scope of their related work section to reflect the full breadth of this important and fast-growing topic. \n\n\n-- In the introduction (Section 1), the authors mention that in HNN or LNN, the relevant laws of physics (i.e., energy conservation) \"are altogether implicitly encoded in networks\". However, it is not clear what the authors meant by \"implicitly\". Energy conservation is explicitly encoded into the architecture of these networks; it is not enforced through a soft regularization term (e.g., the way PINNs implement the underlying physics). \n\n\n-- In Section 3, while describing the constraints, the authors introduce the constants $C_i$ (3) and $C_j$ (4); however, they are never used in the rest of the papers. In addition, these constants can be assumed to be zero without loss of generality. For these reasons, it would be better to use zero on the right-hand side of equations (3) and (4). Or, do these constants carry any special significance? Also, prior work by *Finzi et al. (NeurIPS 2020)* and *Zhong et al. (L4DC 2021)* have discussed E-L equations with constraints and have shown how they can be used inside a neural network based learning framework. \n\n\n-- While defining the learning objective in equation (6), the authors use a regularization term involving energy output ($E$) of the network. Although it seems $E$ is the same as the energy of the overall system, i.e., $E = T + U$, it has not been specified in an unambiguous term. Also, it is not clear how the regularization weight \"$\\lambda$\" influences prediction accuracy. The paper only mentions that Experiment 1 uses $\\lambda=0.1$ but does not explain why and how this value has been selected. I would like the authors to provide some rationale behind this choice.\n\n\n\n-- Furthermore, the paper has not explained how the acceleration data has been made available to the learning algorithm. Is it given as a part of the dataset, or is it estimated from the position and velocity data? Earlier, *Zhong et al. (ICLR 2020)* have shown that using $\\ddot{q}$ in the learning objective decreases prediction accuracy. If $\\ddot{q}$ has been estimated from $(q,\\dot{q})$-data, the authors should present an ablation study that shows how the approach to acceleration estimation (e.g., forward vs. central difference) impacts the prediction accuracy.\n\n\n-- Also, the references used in Table 1 appear to be imprecise (e.g., Hamiltonian Neural Network was proposed by *Greydanus et al.*). \n\n\n-- The paper has some minor typos (e.g., \"$L2$\" on page 3 should be \"$L_2$\", \"regulation\" on Page 4 should be \"regularization\"). The authors should do another round of proofreading.",
            "summary_of_the_review": "The problem studied in this paper is very topical and relevant. The main contribution of this work lies in its capability to learn from multi-agent systems of small size and then use it to yield predictions for multi-agent systems with more agents. However, this aspect has not been adequately explained, making it difficult to judge the true novelty of this work. However, if the authors can address the issues raised in the Main Review section, I would be open to reevaluating my score.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a method for learning physics through modular Lagrangian neural networks. The proposed architecture has separate arms for modeling kinetic and potential energy, which allows the use of objective functions that enforce that the modeled system satisfies Lagrangian mechanics. Furthermore, the approach uses recurrent networks to allow for modularity in the systems that are modeled, meaning that e.g. same network used to model the 3-body problem can be used to model the n-body problem. Experiments showed the modular Lagrangian network outperforming baselines on modeling and extrapolation in multiple domains.",
            "main_review": "Overall I think this paper presents some interesting ideas, but I believe there are still some improvements that can be made to the presentation and the experiments before this merits acceptance/publication. \n\nFirst, I don't think this paper was quite explicit enough about what its exact contributions are. It spends a lot of space dedicated to deriving the Euler-Lagrange and dynamics equations, which can be a useful reference for the reader, but it somewhat gives the impression that the enforcement of the Lagrangian is a new contribution of this paper when in fact there have been at least three other papers [1-3] that already proposed similar ideas. Compared to the discussion about the Lagrangian, I thought the main contribution of this paper, adding modularity to this Lagrangian-learning framework, was relatively under-discussed. I would recommend reworking this paper to make it more clear what the new contributions of this paper are and explicitly contrast them with existing works.\n\nMy other main concern is that I'm not sure that the experiments as presented are convincing enough. A few notes/questions:\n- Would it not make sense to compare against any one of [1-3] as a baseline?\n- The baselines in general are not really described in detail. Particularly the one labeled as \"Baseline\" -- I think this is just a standard feedforward neural network? It's kind of strange to cite a paper for this baseline and have it just be a generic deep learning paper.\n- I may have missed something, but how does training with different pendulum lengths take advantage of the modularity aspect of the proposed architecture? I can see how it could help with handling multiple numbers of objects, but am confused about how it would be helpful for different lengths.\n- In the experiments you present results for extrapolating from double pendulum to multi-pendulum systems; have you tried extrapolating from single pendulum to multi-pendulum? I'd be curious whether the model would be able to extrapolate to a system with chaotic dynamics if the data it's trained on is not chaotic.\n- There aren't error bars presented on any of these experiments. Have you tried retraining and evaluating the results? Particularly for the chaotic systems, are we sure that if you trained again you would really get the same thing? Would Fig. 7 be reproducible with a second trained model?\n\n[1] \"Deep Lagrangian Networks: Using Physics as Model Prior for Deep Learning\" (2019), Lutter et al.\n[2] \"A General Framework for Structured Learning of Mechanical Systems\" (2019), Gupta et al.\n[3] \"Lagrangian Neural Networks\" (2020), Cranmer et al.",
            "summary_of_the_review": "This paper contains some interesting ideas that are worth exploring further. However, I don't think it's clear enough about its contributions relative to previous work. Furthermore, I don't think its experiments are well tailored or convincing enough to support some of its claims. For this reason I would recommend against acceptance in its current form.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors present a framework known as Modular Lagrangian Neural Networks as an extension to physics informed neural networks such as Lagrangian neural networks, for modeling the dynamics of physics-based systems. Especially for multi-body physics or general multi-node graph problems, the ModLaNN framework suggests modeling the system through its individual blocks as opposed to treating the entire system as a single unit. By modeling the individual subsystems instead of the full system, it is technically possible to scale to new configurations by ‘wiring’ the subsystems differently: such as training on a 3-body system and extrapolating to a 6-body system. The authors apply their technique to the cases of double pendulum to multi pendulum; 3-body system to multi-body and show that the proposed technique results in more accurate predictions than simple MLPs and Hamiltonian neural networks. ",
            "main_review": "**Strengths**\n\n1. The paper proposes an interesting technique that emphasizes modular subsystem modeling, as an alternative to modeling entire systems as most techniques in the literature do. Intuitively, this has the potential to achieve better generalization. \n2. The proposed technique results in better trajectory modeling for dynamical systems like double/multi pendulum, N-body systems etc. compared to Hamiltonian neural networks from the literature.\n\n**Weaknesses**\n\nI think the paper needs significant rework in its presentation. Mainly, it is unclear to me how the proposed algorithm works, and exactly why it results in better performance. Looking at the results, one can see that ModLaNN has lower prediction error and better generalization than the baselines, but it is hard to get a sense of exactly which part of the model is contributing to which factor in the success. For example, there is a textual description of how the model works in Section 4 Case 2 for a multi-pendulum system. I would recommend changing that to something like an algorithm, or a visual depiction instead for better understanding. Or it would be better to consider a toy problem and have a description of how that toy problem is modeled through MoDLaNN. I would also strongly recommend the authors to discuss the shortcomings of their algorithm as well, along with the benefits. \n\nSecondly, I feel that the evaluation and comparison in the paper is insufficient. Is there a reason the authors have not compared their technique to the Lagrangian class of models such as Lagrangian Neural Networks [1] or Deep Lagrangian Networks [2]? This class of techniques does not require knowledge of canonical coordinates as HNNs do, so perhaps they would result in better performance for individual systems, while being closer to the proposed technique as well. Without that comparison, it is hard to gauge the novelty and localize this method in the current literature.\n\nFurthermore, one can perhaps understand that because this is a modular approach, it can allow for better generalization, and this is different from existing methods as the authors state in Section 1. But if we ignore the generalization aspect for a moment and train/test on the same system, the paper needs to explain better why the trajectory predictions are better for the current technique compared to HNN/MLP. If that boost in performance is happening because of the Lagrangian formalism itself (compared to Hamiltonian for instance), then the authors should certainly compare the results with LNN.\n\nSome specific clarification questions and comments follow. A lot of these questions could be answered through a rewrite of the algorithmic details section in a way that it carefully walks through the important steps of the algorithm.\n\n1.\tWhat exactly is a RelationCell? The paper would benefit from a clearer visualization of how this layer works. And what is $I_\\dot{q}$ in Figure 2?\n2.\tIt is unclear why recurrence is needed at the input of the RelationCell/ModLaNN block. The idea of the network is that given an N-body system, one should be able to train and test it with different values of N. Hence, the ‘different dimensions’ should apply to the number of nodes in the system, but it doesn’t seem like the expectation is to be able to scale to different state dimensionalities. Why is then the recurrent structure important to “accept input with different dimensions”? Shouldn’t the recurrence be more about handling trajectories over time?\n3.\tIt is also not clear what the function of the ‘regulation term’ is. If the ModLaNN block is supposed to output acceleration and force, which part outputs the energy required for this regulation term?\n4.\tAs seen in Lagrangian Neural Networks [1], given $q_t$ and $\\dot{q_t}$, system dynamics can be estimated by computing the generalized accelerations $\\ddot{q}$ from a Lagrangian. Why is the force estimation needed in equation 5, and why does the ModLaNN need to output forces?\n5.\tIn Table 1, and the $l \\in [1, 9]$ column, why weren’t the HNN and MLP subject to the same kind of test?\n6.\tThe results in Table 2 seem a bit inconsistent with expectations. Why does the baseline perform better for a double pendulum while its predictions were bad for a single pendulum? Also for the 3-body and multi-body system (which should be harder to model than a single pendulum), both baselines perform better than MoDLaNN at least in terms of train/test losses. \n\nBut at the same time, the low baseline test loss for N-body system does not correlate with the bad trajectory predictions in Fig 8(d).  \n\n7.\tI suggest using actual error metrics (e.g. difference in predicted vs actual energy/coordinates ) in the comparison tables as opposed to train and test losses, as the losses only tell part of the story. It is also easier to quantify prediction error through such numbers than the trajectory plots.   \n8. Why does the paper cite Hamiltonian Generative Networks [3] for HNN as opposed to the actual HNN paper [4]? \n\nThere are minor typographical errors at several places in the paper, I would recommend taking a pass through for proofing:\n\nEuler-Lagrangian equation -> Euler-Lagrange\n\"once obtaining the Lagrangian\" -> \"once the Lagrangian is obtained\"\n\n[1] Cranmer, Miles, et al. \"Lagrangian neural networks.\" arXiv preprint arXiv:2003.04630 (2020).\n[2] Lutter, Michael, Christian Ritter, and Jan Peters. \"Deep lagrangian networks: Using physics as model prior for deep learning.\" arXiv preprint arXiv:1907.04490 (2019).\n[3] Toth, Peter, et al. \"Hamiltonian generative networks.\" arXiv preprint arXiv:1909.13789 (2019).\n[4] Greydanus, Samuel J., Misko Dzumba, and Jason Yosinski. \"Hamiltonian neural networks.\" (2019).",
            "summary_of_the_review": "The paper presents an approach to model dynamical subsystems using the Lagrangian formalism and thereby enforcing energy conservation principle. While the concept is interesting, and the results show good performance in trajectory modeling/generalization, the paper lacks the proper presentation to fully understand the algorithm itself. The proposed technique is also not compared with other Lagrangian methods in the literature, making it hard to evaluate its efficacy. I don't think this paper is ready for an ICLR publication without significant rework in presentation and evaluation. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper claims to propose a variation of physics-inspired deep networks for learning dynamics models. While the current approaches e.g., Deep Lagrangian Networks and Hamiltonian Neural Networks, are constrained to systems with fixed dimensionality, the proposed Modular Lagrangian Neural Networks can generalize to systems with varying dimensionality, different masses and lengths. In the experiments, the authors claim to show that the proposed algorithms can accurately learn the dynamics of n-link pendulums and n-body problems. ",
            "main_review": "The claimed contribution of this paper is extending physics-inspired networks to generalize to systems with different numbers of particles/links, masses and lengths. However, this generalization is mainly due to smart feature engineering. The authors derive specific Lagrangian that includes most of the structure of the system and can be easily extended to systems with multiple links. Therefore, most of the Lagrangian is hard-coded. Even the supposedly 'generic' relation cells as well as the kinetic and potential energy cells include a large amount of structure. If you have a hard-coded parameter for length / mass and then change the hard-coded parameter surely your learned model generalizes to the new mass/length. The same is true for the number of links, if you derive the Lagrangian as a sum over links/particles, and add new links/particles, yes your algorithm will generalize to varying the number of links and particles. However, this generalization is not due to the new algorithm design but more due to your feature engineering. \n\nIf the paper would nicely detail the complete algorithm, describe all the benefits and shortcomings of this approach and discuss the differences to existing work, it would all be fine. In this case, one would need to review the significance of this feature engineering. However, the authors do not explain their algorithm in the paper and leave out all the details. Therefore, one cannot understand how the algorithm works or how generic the algorithm is. I had to reverse-engineer the code to fully understand the implemented algorithm. Due to this large discrepancy between code and described algorithm, the results are not valid and not reproducible. The experiments are also not surprising as when you hard-code the optimal solution within the code, yes your method will work better than the generic Hamiltonian neural networks. \n\nA selection of un-documented structure within the code is: \n* The kinetic energy cell contains the square, i.e., `h = self.linear1(v.pow(2)) + self.linear2(v)`\n* The relation cell of the n-body problem nicely computes the distance between two particles as are feature, `dx = x[:,i*self.dim:(i+1)*self.dim]-x[:,j*self.dim:(j+1)*self.dim]`\n* In contrast the relation cell of the n-link pendulum nicely multiplies everything by the velocity, `y += self.mlp(x[:, i:i + 1]) * v[:, i:i + 1]`\n* For the n-link pendulum the velocities are rescaled by the pendulum lengths as described in the analytic Lagrangian, `v = self.length * v`\n\n\n### Minor Comments:\n* In Eq. 6 you state the optimization loss. This loss optimizes the system energy using an l2-distance. However, this is not possible for real-world systems as the energy cannot be observed. Therefore, most other physics-inspired networks do not do optimize the energy. It would be good to point this out explicitly. \n\n* In the Lagrangian described in case 3: Multi-body systems, the x_i and y_i in the kinetic energy part are missing their dots. Furthermore, the naming of the section is misleading. the naming 'multi-body systems' hints that you include bodies with inertial properties. However, you only use particles with point masses. \n\n* I would recommend that the authors should improve their software engineering as the code contains many duplicates and includes a lot of spaghetti code. I had to manually diff many files to see whether the functions with the same name are duplicates or contain changes.  ",
            "summary_of_the_review": "The paper does not describe the implemented algorithm and misses many important implementation details. Therefore, there is a large discrepancy between the described and implemented algorithms! This discrepancy invalidates all results and hence this paper is a clear rejection. Furthermore, the claimed contribution is mainly due to smart feature engineering rather than proposing a new algorithm. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}