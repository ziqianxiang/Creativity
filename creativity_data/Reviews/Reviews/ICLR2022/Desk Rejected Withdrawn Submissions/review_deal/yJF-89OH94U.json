{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work presents a post-hoc network pruning strategy for improved out-of-distribution (OOD) detection. The technique consists in ranking the units in the second to last layer of the network, according to their average contribution (weight times activation) and keeping the top k. OOD detection is done using the prunned network, in a similar fashion as other works (e.g. establishing an OOD score and thresholding). The paper presents a detailed analysis, both theoretical and experimental, on the properties of the method. ",
            "main_review": "Strengths\n- Clear and well written\n- Thorough experimental setup\n- Simple idea on applying a \"directed\" dropout, which seems to work well\n\nWeaknesses \n- The selection process for the threshold \\lambda could be detailed better\n- Assumption of equal means (lemma 2) is not well justified\n\nDetailed Comments\n- The term DICE has a well-established use to denote the Dice-Sorensen coefficient, typically used to assess the quality of image segmentations. Since this work falls into a field that holds some relationship with segmentation, it would be adviced to change the name of the method to avoid confusions\n- A p=0.9 implies that only 10% of the original weights in the second to last layer are kept. That seems to be quite a lot of weights removed. Please comment ",
            "summary_of_the_review": "This work presents a simple idea on how to prune post-hoc a neural network to improve OOD detection. The method is well explained and its properties are thoroughly studied and validated.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a simple yet effective method for out-of-distribution detection. The paper is clearly written and well organized. The proposed method is well motivated, and the experimental evaluations are comprehensive and convincing.",
            "main_review": "Major concerns: \n\n1. To my best knowledge, the proposed idea is similar (to some extent) to the adaptive dropout [1]. Although [1] focuses on dropping feature activations while this paper aims to sparsen model weights, I think it is still necessary to make a comparison with [1], as compared with the classical dropout in the ablation study. Moreover, I encourage the authors to include a discussion of the adaptive dropout and the essential difference between it and the proposed method. \n\n2. Since the sparsification in this paper is implemented by masking (matrix M), I would be more than happy to see an analysis of a learned M. For example, which entries are 1s and which are 0s? Is there a block phenomenon in M similarly to that in [2] which drops features? \n\n3. In Eq. (3), the extracted feature is from the penultimate layer of a network, I would encourage the authors to discuss whether Eq. (3) can be extended to the intermediate layers. If so, how about the results?\n\n\nMinor concerns:\n\n1. It would be much easier for readers if there is a clear description of how to use Eq. (3)-Eq. (5) in general training and testing procedure. \n\n2. For the remark in section 3.2, the authors may consider giving an example in the appendix to better explain the last sentence in this remark. \n\n3. In section 3.1, it is better to use $h_{\\theta}(x)$ rather than $h(x)$ since $h$ is parameterized by $\\theta$. If omitted for simplicity, the authors may clearly state it. \n\n4. It would be more interesting to see how the feature variance of intermediate layers change when DICE is used. The authors may consider studying this by referring to [3]. Note that this is a very personal suggestion, and it will NOT be considered in my assessment of this paper. \n\n\nReference\n\n[1] Adaptive dropout for training deep neural networks https://papers.nips.cc/paper/2013/file/7b5b23f4aadf9513306bcd59afb6e4c9-Paper.pdf \n\n[2] DropBlock: A regularization method for\nconvolutional networks  https://proceedings.neurips.cc/paper/2018/file/7edcfb2d8f6a659ef4cd1e6c9b6d7079-Paper.pdf \n\n[3] Lightweight Probabilistic Deep Networks https://arxiv.org/pdf/1805.11327.pdf\n\n",
            "summary_of_the_review": "I think the above concerns need to be addressed for this paper to clear the bar of acceptance. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors proposed a new OOD detection algorithm DICE which uses directed sparsification under the supervised setting. The key idea is to use only a subset of important weights to derive the output for the OOD detection. DICE is shown to decrease the variance of output and enlarges the separability between ID an OOD data. The algorithm is compared with state-of-the-art methods using both the FPR95 and AUROC metrics and shows its superiority on different datasets.",
            "main_review": "Strengths:\n\nThe authors proposed a simple OOD detection algorithm by post hoc weight masking on a pre-trained model, which is easy to implement. Extensive simulations have been implemented for comparison between DICE and benchmarks. The experimental results validate the efficiency of DICE for the OOD detection.\n\n\nWeaknesses:\n\n. At the end of section 2, the authors tried to explain noisy signals are harmful for the OOD detection. It's obvious that with more independent units the variance of the output is higher. But this affects both ID and OOD data. The explanation is not clear. \n\n. The analysis in section 6 is kind of superficial. 1) Lemma 2: the conclusion is under the assumption that the mean is approximately the same. However, as DICE is not designed to guarantee this assumption, the conclusion in Lemma 2 may not apply to DICE. 2) mean of output: the scoring function used for OOD detection is max_cf_c(x). The difference of mean is not directly related to the detection scoring, so the associated observation may not be used to explain why the algorithm works. \n\n\n. Overall, it is not well explained why the proposed algorithm would work for some OOD detection. 1) From the observation, although DICE can reduce the variance of both ID and OOD data, the effect on OOD seems more significant. This may due to the large difference between ID and OOD. Therefore, it would be interesting to exam the performance of DICE by varying the likeness between OOD and ID. 2) From Figure 4, the range of ID and OOD seems not to be changed much by sparsification. Similarly, Lemma 2 requires approximately identical mean as the assumption. These conditions are crucial for DICE, but is not well discussed, eg., how to ensure DICE meet these conditions.  \n\n. In the experiment, the OOD samples generally are significantly different from ID samples (thus less challenging). As pointed out in the above comment, it would be interesting to compare the performance of DICE by varying the OODness of test samples. For example, the ID data is 8 from MNIST, OOD datasets can be 1) 3 from MNIST; 2) 1 from MNIST; 3) FMNIST; and 4) CIFAR-10.\n\n\n. The comparison between DICE and generative-based model (Table 3) is unfair as DICE is supervised while the benchmarks are unsupervised. It's not surprising that DICE is better. The authors should add comments on that.\n\n. It is claimed in the experimental part that the in-distribution classification accuracy can be maintained under DICE. Only the result on CIFAR-10 is shown. Please provide more results to support the conclusion if possible.\n\n. Instead of using directed sparsification, one possible solution may be just using a simpler network. Of course this would change the original network architecture. But as one part of the ablation study, it would be interesting to know whether a simpler network would be more beneficial for the OOD detection. ",
            "summary_of_the_review": "Main weakness: It's not well explained why the proposed algorithm would work for some OOD detection, and more analysis is required. \nTo better evaluate DICE, add more challenging OOD data instead of simple ones that are significantly different from ID data.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}