{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes one novel and efficient training method to accelerating the training speed for acquiring the N:M fine-grained \nstructured sparse CNN model. In particular, the authors aim at leveraging the idea of winning tickets by considering the N:M sparsity constraint. To further reduce the training cost, this work propose two additional methods (Proactive local pruning (PLP) and Proactive\nlocal pruning (PLP)) to relieve the training cost while maintaining the high accuracy.\n",
            "main_review": "Strengths:\n1. As shown in Figure 3, the average hamming distance shows that the  N:M  structured sparsity holds a strong restriction or regularization \n   on the winning ticket distribution, which is one novel contribution and may bring some inspirations for other related researchers. \n2. The proposed PLP can be integrated with mIMP to eliminate more weights in the losing ticket pool and the experiments demonstrate that \n    the PLP can further accelerate IMP.\n\nWeaknesses:\n1. In Table 1, besides sparse pattern 2:4, 1:4 and 2:8, one more sparse pattern 4:8 is suggested to be added for exploring the effectiveness \n   of the proposed L-mIMP.",
            "summary_of_the_review": "This paper proposes one simple but effective methods to train the N:M fine-grained structured sparse CNN model. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors study the fine-grained structured sparsity in deep neural networks (DNNs) pruning.\nThe fine-grained structured pruning can be seen as a trade off solution between the structured pruning and unstructured pruning.\nTo this end, the authors proposed a new pruning framework termed L-mIMP to efficiently train DNNs with fine-grained structured sparsity.\nIt adopts the IMP and EB pruning technology to achieve the N: M structured pruning.\nExtensive experiments are conducted to demonstrate the effectiveness of the proposed method.",
            "main_review": "1. Innovation is limited.\nThe method proposed in this paper is not innovative enough. L-mIMP just uses IMP's pruning technology and EB's acceleration methods. \nThe only technical contribution is group-based pruning in the lost tickets pool and reducing the pruning rounds by PLP tricks.\nTo sum up, I think this pruning framework is just a slight improvement on IMP to achieve group-based sparsity for DNNs.\nSo I think the innovation of this paper is insufficient.\nI hope authors can give more reasons to refute my opinions.\n\n2. Practicality is limited.\n(1) The authors argue that N: M structured sparsity can be effectively accelerated in Sparse Tensor Cores. So I hope authors also give more practical experimental results, such as the training or inference time by this technology.\n(2) Since N and M can only take positive numbers, this ratio severely limits the value of sparsity and cannot be flexibly adjusted in practice.\n\n3. More baselines are needed.\nFor better comparison, the authors should also show the experimental results of the following baselines:\n(1) N:M structured sparsity with random pruning\n(2) IMP unstructured pruning\n(3) IMP structured pruning\n\n4. Issues in methodology section.\n(1) This paper introduces a large number of experimental settings in the methodology section. For example, detailed experimental settings are listed in 3.2. Please move these detailed implementations to the experiment section.\n(2) A large number of experimental results also appear in the method section, such as Figures 3, 4, and 6. Please do not list specific experimental results in the method section.\nFor a research paper, the method section should introduce the theory of the proposed method, the rationality of the method, the specific algorithm or the overview of the proposed framework, rather than the detailed experimental settings or results.",
            "summary_of_the_review": "In general, the paper is not well written and the innovation is limited. I hope the authors should first answer the aforementioned questions cautiously and carefully.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors proposed to leverage the lottery tickets from model pruning to reduce the training cost of neural network models with the fine-grained structured sparsity.",
            "main_review": "Pros:\nThe proposed approach for the setting is novel. And can potentially lead to reduced training costs.\n\nCons:\n\nThe authors failed to compare their work with the proper prior art. \nA proper comparison of model performance against [Zhou 2021] and [Yu 2021] (attached below) would put this work in a better position. The authors also took an unusual approach to define training costs as FLOPs instead of epochs, making comparison difficult. \n\nThere are also several significant issues in their manuscript. \nP4: The authors claimed that a weight decay factor of 10 was used in their model training. I believe this is not correct.\nP5: According to the definition given in the first paragraph of Section 3.3 $\\left | V^l_{k,j} \\right |<\\alpha$, $\\alpha$ is compared against a positive value, yet negative $\\alpha$ values were used throughout their experiments. This does not make sense.\n\nZ. Yu, C. Wang, X. Wang, Y. Zhao and X. Wu, \"Intragroup sparsity for efficient inference,\" 2021 International Joint Conference on Neural Networks (IJCNN), 2021, pp. 1-8",
            "summary_of_the_review": "It appears there is a significant performance gap between the proposed approach and the two previous works I suggested for comparison. However, this is my rough estimate based on the data shown in those papers. I would be happy to change my evaluation if proved wrong.\nTwo issues I raised in the main review section suggest the authors do not know their work well. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}