{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a backdoor attack against cooperative multi-agent RL with a focus on games. They provide handcrafted action poisoning and reward hacking schemes to effectively disrupt the performance of the agents.",
            "main_review": "The problem of backdoor attacks against CMARL is an interesting and significant research problem. I have doubts on how much this paper increases our knowledge about these scenarios. The following are my concerns about the paper\n\n1 - The proposed algorithm seem to be extremely specific to particular games. The handcrafted reward hacking in Eq. 7 is not generalizable to other CMARL problems.\n\n2 - The paper does not formally explain their problem definition and threat model. It is left to the reader to interpret it from the rest of the paper and the algorithm. I would strongly preferred if essentials assumptions were formally explained. For example, mentioning if the attacker has a strong (perturbing the model/actions) or weak (perturbing rewards) access to training phase, or how the poisoned agent and non-poisoned agent are defined and determined. The paper lacks these formal definitions which is apparent from the fact that none of the notations defined in the background section are used.\n\n3 - I couldn't understand what is specific about the value decomposing algorithms that they have focused on them as the targets. It is not clear which part of the attack requires this type of agent.\n\n4 - It seems like there is not enough novel contribution to fill 9 pages. Most of the paper is explaining the literature.",
            "summary_of_the_review": "The proposed method is too much focused on specific domains and does not seem generalizable. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a backdoor attack method against value-decomposition cooperative MARL and implements their method in VDN and QMIX, which shows that it is doable to insert a backdoor into multi-agent reinforcement learning models. Specifically, it requires the training of an expert policy model beforehand and then poisons the actions of agents when observing the triggers. Then it hacks the global reward using heuristics to enable the training of the backdoored policy model.",
            "main_review": "Strengths:\n1. The problem of backdoor attacks against cooperative MARL is important.\n2. The writing of this paper is clear. Given I've known the baseline methods, the writing of this paper does not hinder me from understanding it.\n\nWeaknesses:\n1. The reward hacking method is based on heuristics and lack of guarantee. The equation includes the number of poisoned agents as a parameter, while one would be curious to see how about using the percentage of poisoned actions as an indicator?\n\n2. The problem setting is not realistic. Triggers are designed to be the steady objects in the environments, while in most cases the attacker cannot modify the environment, all they can control is the poisoned agents' action. It would be more interesting to see if the triggers could be in more realistic forms, say at a certain time, or the position reached by the poisoned action, or the formation of the agents, etc.",
            "summary_of_the_review": "Though the paper shows a possibility to backdoor attack the CMARL problem, it is clear that it has some weaknesses in the heuristic design on the reward hacking method and the somewhat ideal problem setting. I give a score of \"marginally below the acceptance threshold\", and I'm happy to discuss the changes in the rating.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies backdoor attacks against cooperative MARL. The main contribution of this work is a backdoor attack method against value-decomposition cooperative MARL, but its proposed solution is mostly a combination of existing algorithms.\n",
            "main_review": "Strengths:\n\nThe paper presents a backdoor attack method against value-decomposition cooperative MARL.\n\nWeaknesses:\n\n1. The paper has discussed the challenges of backdoor attacks against Cooperative MARL due to local observation and CTDE. However, it does not clearly explain how and why these factors bring non-trivial difficulties for backdoor attacks. The authors might want to elaborate on them in detail and provide a concrete example to make it clear.\n\n2. The threat model section does not explicitly state the assumptions and information available to the attacker.\n\n3. The Methodology section is too brief and not self-contained. For instance, without reading the literature listed in Section 3.1, it is unlikely to understand what in-distribution and out-of-distribution triggers are and how the authors use them in their proposed work. The same also applies to other subsections. Without formal definitions and accurate explanations, it is unlikely to understand the technical contributions of the paper. The authors might want to introduce the proposed methods formally with substantially more details for each step.\n\n4. Many details of the experiments are not available, making it difficult for the audience to reproduce the results: for instance, the hyperparameters for the chosen RL algorithms.\n\n5. While the paper presents some ablation studies, it does not include a comparison of the convergence rates of different algorithms. Besides, error bars are missing in the reported results.\n\nOthers:\nCan the proposed method be extended to other MARL methods or multiple attackers? How?\n",
            "summary_of_the_review": "While the paper has some ideas about backdoor attacks against cooperative MARL, the current version makes it difficult to understand the main technical contributions. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies backdoor attacks in cooperative multi-agent reinforcement learning (MARL). To mitigate the challenges that one typically faces in cooperative multi-agent environments (locality of agents' observations and global reward structure), the paper proposes a new approach to implementing backdoor attacks, called MARNet, that attacks MARL methods based on value function factorization (e.g., VDN or QMIX). The paper experimentally validates the efficacy of the proposed approach, and argues that it leads to more efficient backdoor attack compared to baselines which are extensions of methods developed for single agent settings to MARL (e.g., TrojDRL). ",
            "main_review": "In my opinion, the main strength of the paper is the novelty of the proposed problem setting. To my knowledge, adversarial attacks in multi-agent environments haven't been extensively studied, especially when it comes to backdoor attacks. In that regard, studying backdoor attacks in cooperative MARL is important, and as the paper argues, brings additional challenges that are specific to multi-agent environments. That said, I have several concerns regarding this submission, listed below:\n\n- **Method**: While the studied problem setting seems novel, the developed method seems to only incrementally contribute to existing approaches, without necessarily resolving the challenges specific to multi-agent RL environments. In particular, it's not clear to me which attack model this paper actually considers. Based on Section 3.2 and Section 3.3, one could conclude that only agents that observe triggers are *directly* attacked. But if this is the case, then it's not clear how  Eq. 7 resolves the problem of backdoor attacks in multi-agent RL. More specifically, this reward function is shared among agents so it seems that not only agents that the attacker intends to poison are poisoned. While the paper specifies that *we should only hack the global rewards in a way that increases the rewards of agents whose actions are poisoned (agents who have observed the triggers) and tries to maintain the rewards of other agents*, it does not specify\nhow to ensure this consistency purely by chaining the global reward. I also feel that the formal treatment of the problem setting is not rigorous enough, e.g., the definition of the hacked reward function could be more formally defined.\n\n- **Experiments**: I find some parts of the experimental section somewhat hard to follow. Baselines based on TrojDRL don't seem to be explained in the text (at least not in great detail). For example, the paper mentions that these baselines extend TrojDRL to multi-agent RL, but it doesn't mentioned how exactly, which possibly affects the reproducibility of the results. It's not clear why TrojDRL actually leads to lower performance when there are no triggers (e.g., results in Table 1 for 0%). These details are quite important for evaluating the proposed method.... The paper also doesn't discuss the implications of the results to a great extent. For example, for the QMIX method, TrojDRL seems to perform better than MARNet, so the results don't seem to fully support the claim in the abstract: *MARNet outperforms baselines extended from single-agent DRL backdoor attacks*. I generally believe that the experimental results could be expanded and include more test cases (e.g., by including more SMAC scenarios). Minor comment: Some abbreviations are not specified (EGA in Table 2?)\n\n-  **Presentation**: Finally, in my opinion, the presentation of the paper could be considerably improved. While I was able to comprehend the main ideas, I feel that important details are missing, some of which are mentioned above.  The structure of the paper could also be improved. For example, I find it strange that there are so many references in the abstract. Moreover, the paper contains quite a few typos (even in the abstract) and there are inconsistencies in the text. E.g. in the sentence *Popular MARL algorithms like VDN (Sunehag et al. (2018)) and QMIX (Rashid et al. (2018)) are designed to deal with local observations and are widely deployed in autonomous driving (Dosovitskiy et al. (2017); Cao et al. (2013))...* , VDN and QMIX are published in 2018, while the citations for the mentioned applications are from earlier years. ",
            "summary_of_the_review": "The paper studies a novel and important problem (backdoor attacks in multi-agent RL), however it can be considerably improved in terms of algorithmic design, experimental evaluation, and clarity. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}