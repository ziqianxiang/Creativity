{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper introduces a continual learning method, based on minimising Cramer-Wold distances. The method alternates between solving a task (loss term has two parts: the loss over new data, and the Cramer-Wold distance between output and a generator's output), and training a generator to output pseudo-data similar to past tasks (trained also using Cramer-Wold distances). The authors show results on Split MNIST, Permuted MNIST, and Split CIFAR-10, and compare with a few baselines.",
            "main_review": "This paper takes a simple idea, applies it, and looks at results for a few benchmarks, comparing with only a few baselines. In short, I think this paper is much more suited as a workshop paper, and significant work needs to be done to make it suitable for a full conference paper. \n\n1. Firstly, I liked Figure 2 and the intuition I got from it. A couple of minor points: The x-axes should be the same for all 3 subfigures, and there should be a legend saying that blue is task 1's data.\n\n2. This seems like a pseudo-rehearsal method to me, and not a regularisation method as the authors say. In fact, it seems very similar to Deep Generative Replay (Shin et al., 2017), with the only differences being in using Cramer-Wold distances both when training on a task and when training the generative model. This is quite a small technical contribution by itself. \n\n3. The experiments have 3 benchmarks. Usually I would suggest more / larger scale benchmarks (as is becoming common in continual learning). But far more importantly, there need to be comparisons to more baselines, especially (pseudo) rehearsal baselines. For example, Deep Generative Replay. In fact, Deep Generative Replay performs very well on all 3 forms of continual learning, achieving very high accuracies >90% (see for example van de Ven & Tolias (2019) for results with DGR; note that the architecture they used might be different).\n\n4. The proposed method performs very poorly in general (when compared with rehearsal baselines). The authors are up-front about this, which I respect. However, for a rehearsal method, the results should be stronger (see my Deep Generative Replay point above).\n\n5. What is the cost of this method? Presumably it takes time to train the generative model, and time to calculate the additional term in the loss function.\n\n6. The three categories of CL scenarios is also introduced in van de Ven & Tolias, 2019. Also, the citation (\"Yen-Chang Hsu\") is in a wrong format. It should be \"Hsu et al., 2019\" or similar.",
            "summary_of_the_review": "Overall, I thought this idea was of limited novelty theoretically, and the experiments are far from comprehensive to make up for it. Comparisons to more relevant baselines are required. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents CW-TaLaR, a method for continual learning that relies on a Cramer-Wold generator for modeling output data representation and a penalty term to minimise the distance between two probability distributions (of two tasks) on a target layer. The authors apply the proposed method to different continual learning regimes such as task-incremental, domain-incremental and class-incremental learning. The experiments are reported in benchmark datasets such as Split MNIST, Split CIFAR-10 and Permuted MNIST, in comparison to other continual learning approaches such as EWC, SI and MAS, in terms of final accuracy and accuracy along the sequence. ",
            "main_review": "Strengths of this paper are: \n- The paper tackles the important problem of learning a sequence of tasks continually in the presence of catastrophic forgetting\n- The paper is sufficiently contextualised in the literature of continual supervised learning\n\nWeaknesses of this paper are:\n- The main weakness that I see in this paper is in the experimental results, for which I do not see any advantage of the proposed approach. Although the method outperforms others in Split-MNIST IDL and ICL, it does not seem to perform well enough on more challenging datasets such as Split CIFAR-10 and Permuted MNIST. Furthermore, in a continual learning study such as this I would expect to see more throrough measurement of other important metrics in the area, such as forgetting level and backward transfer [1]. Although some conclusions in terms of forgetting can be inferred from Figures 3-5, it is important to measure the standard metrics in the area. Moreover, given the results in Figure 3, I would expect to see more analyses/explanations of why the method seems to work well for the ICL scenario specifically, for this dataset, and not in other scenarios? \n- A related second weakness is related to the counterpart methods selected for comparison. Why do you compare only to regularization-based methods? Even though replay-based methods and network expansion methods are different in nature, comparisons in terms of accuracy, forgetting, backward transfer, memory/storage requirements, computational cost, etc., can still be made. \n- I am also missing measurements and further explanations in terms of the computational cost of the method. What is the cost of keeping the CW generator network, in terms of memory/storage? And the cost of using this generator during training? What about inference time?\n- Finally, the writing and format of the paper could be greatly improved. For example, the algorithm presented in page 4 is very unclear since it repeats almost the same information over and over. Is there a way to represent those big stages somehow - for example, in actual pseudocode?\n\n[1] Lopez-Paz, D., & Ranzato, M. A. (2017). Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 6467-6476.",
            "summary_of_the_review": "Based on the weaknesses identified above, I do not think that the paper is ready for publication. There is a substantial amount of experimental and writing improvements to be made. Therefore, my recommendation is to reject. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a continual learning method which is a hybrid of regularization and generative replay.\nThe main novelty is the use of Cramer-Wold (CW) distance.\nWhen learning task $T_{j-1}$, a generator is trained to minimize the CW distance between the generated distribution and the task distribution.\nWhen learning the next task $T_j$, this generator generates a pseudo dataset for $T_{j-1}$, and the main network is regularized to minimize the CW distance between the embedding distributions of $T_j$ and the pseudo dataset $T_{j-1}$.",
            "main_review": "## Strengths\n- The paper is well-written and concise.\n\n## Weaknesses\n\n### Unjustified regularization term\nAs expressed in Eq.(3) and Eq.(4), the authors force the target layer distributions to be similar for two adjacent tasks.\nIt is hard for me to grasp the intuition behind this algorithm.\nSpecifically,\n- In the case of offline training, the target layer distributions for each task are obviously different. Why should we force them to be similar?\n- Why do you consider only the last task? What about other previous tasks?\n\n\n### Lack of motivation for Cramer-Wold distance\nThis paper proposes to use Cramer-Wold distance for training the generative model and computing the distance between two tasks.\nHowever, the authors do not provide an explanation for why that specific distance metric is useful for continual learning.\n\nIn place of the generative model, one might use other generative models such as VAEs, flow-based models, GANs, etc.\nWhy is the CW generator particularly useful than other models?\nWhat about saving a small number of samples instead of the generative model?\nI think it can be more memory-efficient and much easier to implement.\n\n\n### Underwhelming experimental result\nThe proposed method outperforms other baselines in Split MNIST, does not in Split CIFAR10 and Permuted MNIST.\n",
            "summary_of_the_review": "This paper proposes to use the Cramer-Wold distance for continual learning.\nHowever, the authors do not explain why CW distance is chosen nor compare it with other distance metrics.\nAlso, the empirical results are unimpressive.\nOverall, it was hard for me to find valuable insights in this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a regularization-based method for Continual Learning called Cramer-Wold Target Layer Regularization (CW-TaLaR), which contrasts forgetting by requiring network features at the penultimate layer to be close to the ones achieved at previous tasks. The proposed model does not compute these targets by storing and replaying past data, but rather by training a Cramer-Wold Generator to generate samples with the same distribution starting from Gaussian noise.\nThe authors provide some evaluations with a limited set of non-state-of-the-art regularization methods, the performance of CW-TaLaR is mixed.",
            "main_review": "I find that this paper is characterised by the following strenghts:\n\n+ it is clearly written and easy to follow;\n+ it devises a solution for continual learning which is based on interesting and grounded statistical intuitions;\n+ the proposed approach seems to be general enough to reasonably scale beyond simple classification; I am confident that CW-TaLaR could  be possibly combined with other continual learning methods.\n\nHowever, I find that the overall proposal is not entirely convincing due to the following weaknesses:\n\n+ I found the experimental section to be quite underwhelming mainly due to the choice of competitors.\n  + The authors claim that the proposed competitors qualify as state of the art. This is not at all in line with the well-established notion that regularization methods are underperforming in CL benchmarks (and especially ICL) as stated in [1, 2, 3]. Please not that the chosen roster of competitors only includes methods dating back to 2017/2018, while there have been very meaningful advancements in this field in recent years, mostly revolving around rehearsal methods which are entirely absent from this evaluation. The authors might be willing to make a point about their regularization approach being more efficient than rehearsal approaches. If this is the case, I would strongly encourage them to include the latter in the experimental evaluation and also include a memory footprint comparison.\n  + I find that the proposed CW-TaLaR is very similar in principle to LwF, since both methods focus on constraining the model's responses to be coherent from one task to the next. LwF does so by the means of knowledge distillation, while CW-TaLaR uses a generator. For this reason, I believe that LwF should be very prominently featured in the experimental section, letting the readers gain an understanding of its relation to the proposed method. The have claimed in the related section that LwF could be seen as an architectural method, but this is simply not true: LwF has a varying number of classification heads (only in the last layer) because it was designed of the ITL setting; I refer the authors to the LwF.MultiClass implementation of [4] for an up-to-date implementation of LwF which instantiates all classification heads at the beginning of training and is also capable of tackling IDL and ICL.\n  + The authors suggest that CW-TaLaR could be seen as a pseudo-rehearsal approach. I do not entirely agree with this claim, since the proposed method uses the generator to condition the prior distribution of targets in lieu of generating examples to be replayed. At any rate, if the authors feel that this comparison can be made, I would recommend also including pseudo-rehearsal methods in the evaluation section, so as to let the reader also understand how pseudo-rehearsal and CW-TaLaR compare performance-wise.\n  + The proposed ITL/ICL benchmarks (Split MNIST/Split CIFAR-10) are generally regarded as simple w.r.t. to current continual learning literature. For this reason, the authors might consider including harder benchmarks consisting of a longer sequence of tasks such as Split-CIFAR100 or Split-Tiny Imagenet.\n\n+ In addition to the issues I mentioned above regarding the setup of experiments, I also find that the presented results are not particularly strong. I am especially perplexed by:\n    + the ICL performance on Split-CIFAR10, which seemingly suggest that taking no measure against forgetting (Adam) yield better accuracy than CW-TaLaR. The authors tentatively explain it by linking to the use of a CNN as backbone; however, working with CNN is standard in current continual learning classification experiments, so I feel that extra effort should be put in improving this result;\n    + the performance on Permuted-MNIST is not on par with online EWC and MAS. The authors state that the model is actually better at learning new tasks, but suffering from increased forgetting; however, this is not at all desirable, since the main purpose of a regularization method is indeed to limit forgetting (typically at the expense of the accuracy on the current task).\n\n+ I found the related work section to be at times unclear:\n     + what do the authors mean by stating that rehearsal methods are \"rather a theoretical solution\"?\n     + why are the methods in the last paragraph disorderly listed without classifying them according to the chosen collections?\n     + why is there a citation to Hinton et al, 2015, which is a general theoretical work on Machine Learning, not directly linked to continual learning?\n\nI would also like to add the following suggestions to the authors, which - however - did not affect my evaluation:\n+ The authors use Adam as optimizer for their experiments. It should be noted that recent literature discourages its use in a continual learning setting and highlight that standard SGD might be preferable [5];\n+ I believe that the resolution of Figure 1 should be increased, since it is hard to read;\n+ for the same reason, I would suggest increasing the font size in figures 3, 4 and 5;\n+ the original work proposing online EWC [6], which is used as a competitor in the experimental section, does not seem to be cited;\n+ in discussing the chosen categorisation of continual learning benchmarks, the authors consistently refer personally to Yen-Chang Hsu; however, the corresponding work [7] is never cited.\n\n\n[1] Towards Robust Evaluations of Continual Learning, Farquhar et al., ICML 2018 workshop\n[2] Gradient based sample selection for online continual learning, Aljundi et al., NeurIPS 2019\n[3] On Tiny Episodic Memories in Continual Learning, Chaudhry et al., ICML 2019 workshop\n[4] iCaRL: Incremental Classifier and Representation Learning, Rebuffi et al., CVPR 2017\n[5] Understanding the Role of Training Regimes in Continual Learning, Mirzadeh et al., NeurIPS 2020\n[6] Progress & Compress: A scalable framework for continual learning, Schwarz et al., ICML 2018\n[7] Re-evaluating Continual Learning Scenarios: A Categorization and Case for Strong Baselines, Hsu et al., NeurIPS 2018 workshop\n",
            "summary_of_the_review": "The work is overall clear and easy to follow, but suffers from some key weaknesses. Most prominently, the experimental section is problematic, not up-to-date and lacks comparisons with methods very similar to the one proposed; the performance of the proposed method is not on par with competitors, suggesting that it might be failing significantly on more than one benchmark.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}