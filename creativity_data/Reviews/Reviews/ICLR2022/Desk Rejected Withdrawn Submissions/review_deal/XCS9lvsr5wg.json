{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Authors propose a method to learn causal structure in a de-centralized manner. ",
            "main_review": "I believe the setup needs a bit more justification. Could you give real-world examples where federated causal structure learning is needed? \n\nThe shift concept mentioned in passing in the introduction should be explained in a bit more detail to convey the point more clearly.\n\nMechanism approximation is not typically used in non-parametric causal discovery. Please explain in the introduction why this approach is taken. [turns out this is due to ANM assumption but it should be mentioned early on]\n\nThe ANM assumption must be mentioned in the introduction and abstract. \n\nCould you make Figure 2 caption a bit more informative on the methodology?\n\nPlease explain the notation in (4).\n\nIsn't (7) suggesting solving local score maximization or am I missing something? If so, why not say this explicitly in words.\n\nI am having trouble parsing the suggested method. (7) is what each server should do? And then you write an abstract problem which optimizes the sum of each server's scores? How can this be solved without a centralized approach? I am a bit confused.\n\nIn other words, are algorithms 1 and 2 solving this (8) exactly?\n\nHere is my current understanding, please comment on this to verify: Each client solves causal discovery problem locally and learns some U which is a continuous proxy for adjacency matrix. The center unit sees everyone's U's and broadcasts the average out. \n\nBut what does each server do after receiving these new U's? I think this is not explicitly stated.\n\nPlease add the explicit definition of restricted ANM instead of citing (Assumption 2).\n\nThe theorems do not relate to the proposed method in any way. I believe simply citing the existing identifiability results would be sufficient. It might be misleading to have a proposition that has nothing to do with federated learning.\n\nCould you please explain how the proposed method is outperforming, for example, GES, or NOTEARS with all data brought together in the IID setting? I am not sure how to explain this.\n\n\nAfter Rebuttal   \n----------------  \nI would like to thank the authors for the detailed explanations and being receptive to feedback. I still believe the paper needs significant revision to demonstrate that \"federation\" does not cause too much loss in the power of the causal discovery tests. Currently it seems like it outperforms everything including the centralized methods which seems too good to be true and indicates that the message of the paper is quite confusing. Hope this feedback will help the authors in making the paper stronger in the future.  ",
            "summary_of_the_review": "I had trouble understanding the exact method. Would be open to discussion and updating my score after clarifications. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper develops a DAG structure learning method from decentralized data. The learning is formulated as a continuous optimization problem with an equality acyclicity constraint and solved by gradient descent. Numerical experiments on synthetic and real-world datasets have been conducted to demonstrate the performance.",
            "main_review": "Causal discovery from decentralized and private data is certainly an interesting area. However, the overall contribution of this work is quite marginal. Each local machines learns a DAG model and then the central sever averages them and sends the information back. Such distributed and then combine idea has appeared in previous work, such as:\n\n[1] Danks, D.; Glymour, C.; and Tillman, R. E. Integrating locally learned causal structures with overlapping variables. In Advances in Neural Information Processing Systems (NIPS), 2009. \n\n[2] Tillman, R., and Spirtes, P. Learning equivalence classes of acyclic models with latent and selection variables from multiple datasets with overlapping variables. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS), 2011. \n\n[3] Triantafillou, S., and Tsamardinos, I.. Constraintbased causal discovery from multiple interventions over overlapping variable sets. In Journal of Machine Learning Research, volume 16, 2147–2205, 2015. \n\nNone of the above papers are cited. At a conceptual level, the authors’ approach seems to be quite similar, except the use of the equality acyclicity constraint, a straightforward application of a recent method.\n\nThe sever update part (page 5) does not enforce acyclicity constraint. The method takes a random subset of U from the local machines and then take an average. Then the new U may not satisfy the acyclicity constraint in Eq (6). There is no discussion on this issue.\n\nThe numerical results are also quite limited:\n\n(a) The synthetic data were simulated from a very sparse setting, d nodes and 2d expected edges. What happens if the DAG is denser?\n\n(b) When the competing methods were applied on separated data, did you try to combine their estimated graphs into an overall estimate?\n\n(c) Some results are a little confused. E.g. Table 1, PC (and a few other competing methods) showed almost identical (or even worse) performance on all data compared to separated data. \n\nThe real data is too small to be useful, only 6 nodes and 7 edges.\n",
            "summary_of_the_review": "Given the concerns over the significance, the method and results of this work detailed in the main review, I recommend rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors introduce a federated learning method for causal discovery in which each client tries to learn the DAG and neural networks to approximate the SEM. The SCM, and associated DAG, of participating clients are aggregated and processed by the server and then sent to each client for updating their personalized model.",
            "main_review": "# Abstract\n\nYou claim \" However, data owners gradually refuse to share personalized data to avoid privacy leakage, making this task more troublesome by cutting off the first step\" - is this really true? I have not heard this before and it seems data is becoming easier and easier to access as it becomes the norm, certainly within research, to open-source data. Hence, can you point to some sources? I find one in the body of your paper. It seems to be a rather weak case for developing a whole new method.\n\n# Introduction\n\n- Typo: \"randomized controlled trials\" -> randomized control trials\n\n- Isn't CSL simply CD i.e. causal discovery?\n\n- Typo: \"owning to the issue of\" -> owing to the issue of\n\n- \"data owners gradually prefer not to share their personalized data with others\" this is a major claim for which you provide one source. Please dig into this in a lot more detail because it goes against what is commonly perceived. Certainly, privacy has become a much bigger issue but that is dealt with through anonymization not (as far as I know at least) blank refusal to share the data.\n\n- Style: perhaps stick with either underlining text of emphasising it through italics; having both is a bit too much.\n\n- Figure 1: you haven't told us yet what SHDs stands for so you may want to spell that out in the figure legend otherwise we'll have to search for it in the main body.\n\n- How reasonable is your first assumption? That seems like a very big ask of the data and not mild at all. SEMs and the DAG usually go hand in hand, perhaps the noise model may change a bit between datasets but not the mechanisms themselves (but again, examples would be very welcome). \n\n- Isn't the, what you call \"CGL\" (I think you mean the learned graph), part very revealing w.r.t. privacy? You may not have access to each individual but instead you are providing organisations with causal information as to how a group of people causally act. In the wrong hands that could be very dangerous. Please comment.\n\n# Preliminaries\n\n- You ought to reference Pearl's Causality (2000) and specifically page 203 which defines structural causal models and the associated causal graphical model (which you for some reason decide to call a graphical causal model which is not standard).\n- You do not mention non-manipulative variables in $X$ nor do you mention the exogenous set $U$ nor confounders - are these not relevant for you method?\n- You may want to tell us what the assumptions say rather than just referencing them. You may have have a novice audience who study causality for the first time reading your paper.\n\n# Problem definition\n\n- I would recommend a nomenclature for your symbols\n- What is $k$?\n- You are using notation $\\{[m]\\}$ - what does that mean? Is it short for $0:m$ or however you're indexing the first item.\n\n# Methodology\n\n- You'll need to spend a lot more time on figure 2. It is not self-contained at all. You are introducing notation in the figure before you've introduced in the paper. What does each dashed box mean? What are the layers on the top? Why are there arrows? What do the arrows mean? You say you share DAG ok, but then the first box has a matrix which is different from the other two.\n- Section 4.1 is preliminaries no? Why have you placed it under the method section?\n- Sentence construction: \"preventing the efficient of penalty\" what does that mean?\n- Sentence construction: \"identifying up the ground-truth\"\n\n## Privacy and cost discussion\n\n- This section is the most important. It should be at the top of the paper. This it the main motivation for the method. You are saying that you avoid personalised data leakage because you are _only_ sharing the causal graph. You are sharing $\\mathbf{U}^{c_k}$ which you say is the same for all clients (but still index it for each client) but then you claim that because the ANM may be different and the SEM may be different too, you are in fact not _actually_ sharing any personal data. Never mind that those assumptions are questionable the fact that you are sharing the DAG alone should be cause for concern (and even that assumption is also very questionable - that all clients have observations generated from the same DAG).\n",
            "summary_of_the_review": "The authors propose a method for combining federated learning and causal discovery.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a federated causal discovery which aims to learn causal graphs from multiple observational datasets that are stored locally and cannot be shared due to privacy concern. ",
            "main_review": "Strengths:\n\n•\tFirst paper to consider causal discovery under federated learning setting. \n\n•\tThe method allows local datasets to be non-iid, which is likely in practice. \n\nWeaknesses:\n\n•\tEmpirical results can be improved by \n\n      o\tIncluding more competing methods beyond continuous optimization approaches. The PC and GES algorithms are non-identifiable. Many identifiable alternative approaches exist such as ANM. \n\n      o\tConsidering more real data validations. Currently, only one real data result is presented. And that dataset is not in the federated setting. To show the real usage of the proposed method, at least one real distributed dataset is needed in my opinion. \n\n      o\tSimulations show the proposed method is superior when data are generated under the proposed model. It will be more interesting to see how the model performs under model misspecification.\n\n•\tHyperparameter tuning and sensitivity analysis. Since causal structural learning can be seen as an unsupervised learning task, tuning hyperparameter is particularly important. From Section A.5, it looks like some hyperparameters are fixed to certain values and others are tuned by simulations. For the former, I believe a sensitivity analysis is in order. For the latter, how does this strategy generalize to real data or simulation settings that are substantially different from the scenarios used for tuning? In fact, it is not clear to me what the authors meant by “tune these two parameters via the same scale of experiment with seeds 1 ∼ 10”. Can the authors provide more details on that? \n",
            "summary_of_the_review": "In summary, I enjoyed reading the paper overall and find the method to be novel and potentially useful in real data, although the potential usefulness can be strengthened (please refer to the “weaknesses”).  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}