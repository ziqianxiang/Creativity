{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposed an imagination-based automatic evaluation metric for language generation tasks. Extensive experiments on three tasks and seven datasets show that adding this new metric to current NLG metrics could improve correlations with human judgments. ",
            "main_review": "Strengths:\n - the idea of using pre-trained text-to-image generation as a proxy measure of their abstractive representation sounds promising\n - the correlation study with human judgment shows consistent improvement on various tasks.\n\nWeaknesses:\n - no additional semantic analysis on objects or entities in the generated image is missing; this lack of detailed analysis makes me difficult to believe how this mapping between two artificially generated images works in practice.\n - besides the extensive empirical findings, there is no scientific evidence of how the method works: for instance, do generated images contain consistent objects? Do the text-to-image generators produce abstractive context like adjectives (e.g., beautiful)? How does co-reference work in the generated image pairs? Have you analyzed how correctly do the images reflect the semantics of text?\n - when the IMAGINE metric is combined with other NLG metrics, the relative improvement over BERTScore seems quite marginal except for the MT task. ",
            "summary_of_the_review": "An interesting idea inspired from previous multi-modal metrics, but lack of scientific merit and further analysis on underlying semantics across objects and context abstraction in generated images.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes an imagination-based automatic evaluation metric for NLG. \n\nThere are two main contributions:\n* This paper provides detailed empirical results/analysis across different text generation tasks\n* This paper proposed a solution that is more similar to/better correlated with human judgment",
            "main_review": "Strengths:\n* This paper is well written, clear, and easy to understand.\n* This paper presents convincing results/analysis to show that ImaginE is better correlated with human judgment\n* This paper provides an ample amount of comparison and contrast between other metrics and ImaginE\n\nImprovements:\nIt would be great if this paper can provide a deeper analysis about why ImaginE is performing better than the others\n",
            "summary_of_the_review": "This paper is well written and provides detailed results/analysis.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces the generation of images from text, and the comparison of representations for the resulting images as a means of implementing a reference based NLG metric. \nThe paper has some motivations for wanting to introduce images into the comparison of output and reference texts that are based on the human brain, but overall the paper is unlikely to be more than a novelty given the poor performance as an NLG metric, the large cost of this approach, and that lack of directions to extend this work. ",
            "main_review": "Strengths: \n- A lot of experiments are carried out across 3 different types of NLG problems and all on public datasets. \n- The work is likely reproducible if the human scores and NLG system outputs are also released (are they? perhaps I missed this).\n- It is clearly written. \n\nWeaknesses: \n- The authors of the paper themselves I think would admit that the method is expensive, doesn't result in a system that is significantly or consistently better than standard, much cheaper NLG metrics. Given this and the large cost of the method (generating each image to represent the text required 1000 rounds of updates on a loss through two models) I find it very unlikely that anyone is going to use this metric in their work. There are some interesting although limited motivations to want to include images in the evaluation of NLG (cf. Related work, Mental Imagery), however taken together at best this is an initial step towards something, but I'm not really sure it suggests any directions for future research. Perhaps it's an idea that nice to have an empirical comment on, but not sure it's an ICLR paper. \n- In a few places the writing suggests \"great evidence for introducing multi-modal information\", however this is based just on marginally improved correlations when added to existing NLG metrics. Combining multiple existing NLG metrics may well have resulting in such improvements in correlations against human judgments as well, but it's not tested. Hence these statements tend to indicate that the authors are looking for positive evidence of introducing multi-modal (which should just be image, since it's the only mode introduced) info, without rigorously demonstrating the benefits. \n- human ratings are collected for fluency/grammar/factualness -- why not report correlations of IMAGINE against these individually? I personally don't see how images can aid anything other than factualness. Can the text --> image process comment on fluency? Grammar? \n- Table 2 is pretty clear in the limited gain (from a very expensive metric) across the different NLG problems. \n- Comments on future work about how CLIP is limited to reading only a shortish section of text (77 BPE tokens) and wanting to remove this limitation in future to be able to apply this metric to document summarisation seem really poorly motivated IMHO. Isn't it very possible that it's quite hard for a single image to capture what's in a whole document?",
            "summary_of_the_review": "I think I'm being generous in rating this a 5 and not a 3 (reject). There's been a solid amount of work done here, but IMHO I'm not sure that there's any likelihood of adoption of the method or extension of the line of research. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes 3 new metrics for measuring textual similarity which allegedly incorporates \"imagined\" depictions of the compared texts by the model.\n\nIn technical terms, the approach relies on an text-conditioned image generator for producing an image from the input text, and an image caption system for producing representations of the input text and the generated image. The 3 metrics are obtained as follows:\n\n* ImaginE_text, is defined as the cosine similarity of the textual representations obtained by the proposed architecture.\n* ImaginE_image, is defined as the cosine similarity between the image representations obtained by the same architecture.\n* ImaginE_text&image is the average of the last two.\n\nThe authors evaluate their approach on three Natural Language Generation Tasks: Machine Translation, Abstractive Summarization and Data-to-text generation. They compare the performance of commonly-used metrics for these tasks, to the performance of the same metrics \"extended\" by the ImaginE metrics. \"Extended\" here means simply adding the ImaginE metric to the original metric. Performance is measured as the correlation between each metric and human judgements.\n\nThe authors show that in some cases the original metrics extended by the ImaginE metrics see increased correlation with human judgements.\n",
            "main_review": "# Strengths\n\n* The paper is well written, well structured and is overall easy to follow.\n* The introduction and related work sections are robust and provide a good overview of the problem the authors are trying to solve, while also referencing relevant literature.\n* To the best of my knowledge, exploiting image generation for evaluating textual similarity has not yet been explored.\n\n# Weaknesses\n\n* Some claims of the paper are only supported by anecdotal evidence, lacking empirical support.\n* The approach heavily depends on the performances of the image generation and image captioning systems, which are already difficult to evaluate.\n* The shortcomings of the proposed approach seem to significantly outweigh its benefits.\n\n## Questions and comments for the authors\n\nAs mentioned above, I feel like the shortcomings of the suggested approach outweigh its benefits. To be more specific, the ImaginE metrics are supposed to help us automatically and more accurately assess the similarity between snippets of text (or one hypothesis and several references). However, it seems that their performance is heavily influenced by the semantics of the texts they are trying to evaluate, which is undesirable for any metric.\n\nFor example, you mention that ImaginE is not good at visualizing abstract concepts, therefore it does not perform well when evaluating biographical text. In your defence, I would argue that nobody or nothing is good at visualizing abstract concepts which foreshadows ImaginE or any other image-generation-based metric struggling when dealing with abstract concepts.\n\nAnother shortcoming you mention is that ImaginE may be misguided by irrelevant information. Again, this is not directly a shortcoming of your approach, but of the underlying systems on which it relies. Creating models that do not get tripped by irrelevant information is still an unsolved problem; You can see https://arxiv.org/abs/1902.01007 for a good example on this.\n\nSo, in general, the applicability of ImaginE is very constrained (only English text that deals on non-abstract concepts and without irrelevant information) because it relies on systems that still struggle outside these domains. I do not see a way out of this without fundamentally changing how ImaginE is implemented. Until we figure our how to make highly performant and general conditional image generation and image caption generation systems, I don't see how ImaginE or other derived image-generation-based text-similarity evaluation metrics could work.\n\nOn a different topic, you mention that ImaginE cannot replace textual similarity metrics, but I never really understood why this was the case. Is it because they did not correlate with human judgements at all? If the answer to this question is \"yes\", then by adding the original metrics to the ImaginE metrics your results might be affected by the Simpson's paradox (https://en.wikipedia.org/wiki/Simpson%27s_paradox). In general, dealing with correlations is not as intuitive as it seems. This blogpost illustrates the pitfalls commonly found when interpreting correlations https://janhove.github.io/teaching/2016/11/21/what-correlations-look-like.\n\nRegarding the claims only supported by anecdotal evidence, you mention on page 6 that \"ImaginE can capture the keyword difference between reference and hypothesis text, even if they have similar sentence structure and high n-gram overlaps\". However this claim is solely supported by the cherry-picked example shown in Figure 4 (a), which is insufficient. A better experiment to support this claim would have been to create a test set with controlled word modifications and studied how the metrics performed then. This claim is repeated again in the first sentence of the \"Future Work\" paragraph on page 9.\n\nSome other comments:\n\nEquation (4) shows that \"extending\" a metric simply means adding the ImaginE score to it. However original metrics have different scales and are affected differently by the underlying text they are evaluating. I suggest normalizing the scores before adding them together; this might have a significant impact in your results.\n\nIn my opinion, Figure 6 (a) is blatantly wrong. The Reference and Hypothesis have completely different meanings (the hypothesis doesn't even make sense), but the human judgement was 5.0/5.0. I would suggest manually evaluating sub-sample of the human annotations to assess their quality. If you had several annotators per (hypothesis, references) pair, you could also make annotator agreement analyses to detect biased annotators, and assess the difficulty of the annotation task.\n\nThe fact that ImaginE created similar images for these two semantically-different summaries, and therefore produced a high ImaginE_image score is further evidence of the approach's shortcomings.",
            "summary_of_the_review": "The idea is novel, but it has too many shortcomings inherited from the pre-trained models on which it relies. To be useful, this metric can only be used on English text dealing on non-abstract concepts and without irrelevant information. In the end there is no significant proof that it does better than the already-existing metrics.\n\nFurther experiments to support some claims are also needed.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}