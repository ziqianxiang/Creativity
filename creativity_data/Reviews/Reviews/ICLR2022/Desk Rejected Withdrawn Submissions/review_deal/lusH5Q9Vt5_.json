{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose a statistical method for estimating the data distribution for few-shot learning. The method is built on a previous work (DC) with minor fixes but have non-trivial performance improvements. Specifically, the main differences lie in the transformation function, weighted random variables and the covariance shrinkage. ",
            "main_review": "The proposed method is highly based on the previous work (Yang et al. 2021) but has the following strength: 1. consider the similarity strength between the base and novel classes, 2. can be applied to arbitrary off-the-shelf feature extractors (with activation functions different from relu) and 3. large feature dimensions often capable of producing ill-deﬁned covariances. The proposed method addresses these issues well and the fixes are simple and only have several hyperparameters added. Performance shows good improvement. \n\nA minor suggestion is to have a visualization of the sampled features as did by Yang et al. 2021. or other formats of interpretability as claimed in abstract ``they either require the design of sophisticated models and loss functions, thus hampering interpretability''. It would be more complete to have some interpretability.",
            "summary_of_the_review": "Although the novelty of this submission may be limited by the incremental design on DC, the proposed method did solve some issues with the previous work and the performance looks great. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper suggests a data augmentation method for few-shot learning. They generalize the similar method, DC (Yang et al., 2021) introducing a couple of hyperparameters to tune. Like DC, without introducing any learnable parameters, they suggest a way to augment few-shot data by extrapolating its distribution base on weighted empirical distributions of base classes.\nIn addition, in a case when the number of data is smaller, they suggest a covariance shrinkage technique.\nThe suggested method is empirically supported with experiments where the suggested method outperforms other baselines.\n\n(Yang et al., 2021) Shuo Yang, Lu Liu, and Min Xu. Free lunch for few-shot learning: Distribution calibration. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id= JWOiYxMG92s",
            "main_review": "The paper is easy to read due to its method simplicity. they design a generalized version of DC (Yang et al., 2021) and empirically show that the suggested method outperforms baselines. In addition, covariance reduction technique is accompanied to consider the case where the number of base class items are less than the feature dimension. \n\nHowever, there are three weakpoints in the paper.\n1. The backbone of design is based on DC (Yang et al., 2021), which devalues the novelty of the proposed approach; the only difference would be that they applied \"weighted\" random variable sampling and the addition of covariance reduction.\n2. Although the term \"generalized\" is incorporated in the design, it  requires exhaustive hyperparameter tuning because its policy should be varied to datasets.\n3. Lastly,  the proposed method may work well only on datasets where base class are semantically similar to novel classes. If authors can provide any superior empirical result on datasets where all the novel classes are dissimilar to novel classes followed by its theoretical or justification, I am more than willing to enhance the review score.\n \nBased on these pros and cons, I would like to offer weak reject.",
            "summary_of_the_review": "The paper seems to be an incremental approach compared with DC (Yang et al., 2021) in spite of its performance improvement.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a statistical sampling method to augment related extrapolated features to the novel class few-shot samples by constructing extrapolated distribution with sampled related base classes to construct much robust classifier for few-shot classification. The paper is an improved version over a previous sampling method DC for few-shot learning by Yang et al 2021 addressing the issue of strength of semantic similarity and the gaussianization technique of features for activation functions with negative outputs other than relu. The first of the above issues are dealt with a weighting scheme to have weighted average of sampled base class features along with the novel class features and consequently estimating the associated mean and the covariance for each novel class. The second issue of gaussianization for activation for negative values are handled with Yeo-Johnson transformation while keeping the same Tukey transformation for the non-negative features. Also, the covariance shrinkage method is adopted for dealing with non-singular covariance matrix formed from ill conditioned feature space with feature dimension number higher than the number of samples per class. Finally, the additional features are augmented from this extrapolated distribution per class to get rid of low data problem and simple logistic regression is utilized and classification results are reported on query samples.",
            "main_review": "Strengths:\n- The overall idea addresses the issue of weighting the semantic similarity of sampled base classes, the singular covariance issue and the gaussianization for features with negative values for activations other than relu which was missing previously in Yang et al 2021 paper.\n- The paper shows improvements over the state of the art methods with their proposed optimized framework on mini-imagenet, CUB and Stanford dogs.\n\nWeakness:\n-  I miss the results on the standard Tiered-imageNet (which was present in the Yang et al 2021 paper) or realistic **Meta-dataset**. Coming to the motivation of the proposed method, the method is actually assuming to have some semantically similar base classes for the target novel classes. While it is ok for the mentioned experimental datasets, I find it to be a very strong assumption and yet would like to see how it can be utilized for the realistic multitask setting like meta-dataset. So my question is whether the method is based on this assumption of semantic similarity I have mentioned or could be generalized to multitask setting like Meta-dataset or with more number of diverse set of classes with more gap between base and novel classes? \n\n- Is it the same exact feature extractor utilized as in DC (Yang et al 2021) method when reporting the comparison results? For example, the proposed method's feature extractor from base class training is further utilized to smoothen the cosine classifier with validation classes. I did not see this detail present in the Yang et al 2021 paper (not sure if they have utilized the same or not). In this regard, the results of baseline methods in the Table 1 and 2 should also have been compared with the same or similar feature extractor involving self supervised features as well. \n- How many number of features are actually augmented finally i.e. what is the value of $n$?  How this number can influence the performance?\n- There are a number of other hyperparameters ($\\beta,m,k,\\alpha_1,\\alpha_2$) which were optimized using Optuna (Akiba et al 2019). During the optimization it is reported in the Appendix B that top-3 candidates after optimization are picked. I only found the best tuned parameters reported at the end in Appendix C, while to me, it would be better to give the accuracies for the rest of the two candidates to see how the performance is influenced. On top of that, optimization of these range of hyperparameters specifically for each setting of shots (1 or 5) is exhaustive to me.",
            "summary_of_the_review": "To me the proposed method is an incremental novel improvement on the previous Yang et al 2021. Although the motivation of the paper is clear and aligned with the proposed techniques, I still have several concerns which are highlighted in the main review section. I still think the method should have been validated to other dataset like Tiered ImageNet  and/or Meta dataset to really see the impact of the method in general and realistic few-shot setting where the assumption of related semantic similarity between novel and base classes might be missing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "Based upon DC, this paper further (1) proposes a weighted form of DC (equation 5), and (2) introduces two additional hyperparameters for covariance matrix estimation (equation 9).",
            "main_review": "1. One main concern is whether the proposed method is reproducible. From the submission, I can find neither the reproducibility statement, nor any plan to release the code. Below are the results from my implementation of the proposed method on miniImageNet dataset:\n\nDC 5-shot/1-shot: 84.65%/68.73%\nProtoNet 5-shot/1-shot: 85.37%/66.99%\nproposed method 5-shot/1-shot: 85.01%/65.83%\n\nI used exactly the same beta, m, k, alpha1, alpha2 values as reported in the paper, but cannot obtain comparable accuracy as claimed. Can the authors please provide supplementary code to demonstrate the reproducibility of the proposed method?\n\n2. The DC paper uses tieredImageNet dataset, another large scale dataset for the few-shot problem. I am wondering why this paper does not use the same tieredImageNet dataset. The size of the Stanford Dogs dataset is still quite limited. It is unclear how the method works on large scale dataset.\n\n3. In the DC paper, the authors illustrate the generated features by DC via t-SNE and compare them to those ground-truth distributions, showing that the estimated distribution is somewhat similar to the ground-truth distribution. In this paper, there is, unfortunately, no illustration at all. I would suggest the authors of this paper to visually compare the following three distributions: estimated by DC, estimated by the proposed calibration, and the ground-truth one. This will show whether the second one is indeed closer to the ground-truth than the first one.\n\n4. There is, unfortunately, no theoretic analysis at all. It would significantly strengthen the paper if the authors can provide a generalization error bound for empirical risk minimization and show that the proposed method encourages better generalization than the original DC method.\n\n5. In Table 3, it can be seen that alpha1 and alpha2 play a key role in improving the accuracy. However, only the final values of alpha1 and alpha2 are given. It would be interesting to know how changing these two hyperparameters will result in different accuracy by, for example, 2D heatmap, which is important for a better understanding of the behavior of the method.\n\n6. In Algorithm 1, the details about logistic regression are missing, for instance, maximum iterations, regularization, etc.\n\n7. Please perform a normality test on the calibrated data. In Table 3, the accuracy is improved after each step, and it is concluded that this is due to better Gaussianization after each step. So please show the statistics from the normality test after each step to support this claim.",
            "summary_of_the_review": "The paper is technically very similar to the DC paper. The contribution is incremental and minor, the theoretic analysis is absent, the experimental result especially detailed analysis is far from sufficient, and the reproducibility is questionable. Thus, the paper, in its current shape, falls short of the publishable standard for ICLR, both technically and empirically.\n\n\nThe author's feedback addressed a few concerns listed in the original review. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}