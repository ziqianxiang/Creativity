{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors present a methodology for upper-bounding the test error of a neural network on a given sample. The method involves considering the network as a piecewise linear function, observing the training error on the linear pieces (\"subfunctions\"), and extrapolating to the test sample using a pre-specified matrix of similarities between subfunctions.",
            "main_review": "The paper is written in a clear, compelling manner. The method presented is novel as far as I know, though I have limited familiarity with other approaches in the literature for bounding generalization error on individual test points. But I am strongly recommending rejection because it appears the authors' theoretical claims do not support the claims they make about their method.\n\nIn particular, as far as I can tell the authors are confused about a crucial point. They claim that their method allows for test error to be bounded given the training error. But the generalization bound they rely on (a standard Hoeffding-based bound, Equation 2.17 in Mohri et al.'s textbook) holds only when the hypothesis is fixed before the training sample is drawn. Thus, it does not hold when the network is actually trained on the training data, because in this case the learned classifier depends on the training sample. That's why there can be such a thing as overfitting. Thus, the proposed method does not actually work as the authors claim it does, unless the network's accuracy is tested on a held-out validation set rather than the training set.\n\nThe authors' mistake may have stemmed from misunderstanding the assumptions in the generalization bound, or perhaps they mistakenly believe that the subfunctions $h_i$ are fixed ahead of time, which they are not because of the network's trainable weights. This brings me to my second concern, which is that as far as I can tell the authors never use the fact that the subfunctions are linear. They could have partitioned the input space in some arbitrary way, and indeed they do use a coarser partition (only dependent on the top-layer ReLUs) in their experiments. (aside: In my opinion it would be more natural to use some continuous similarity metric rather than the discrete shattering method presented.)\n\nIn their theoretical discussion of how to generate a distance function $k$, the authors only use the Lipschitzness of the network. For most real-world problems for which deep learning is useful, the naive distance between inputs (e.g. in pixel space) is not a particularly useful similarity metric, so this section's relevance to practice is presumably weak.\n\nIn section 2.4 the authors claim that their method might be relevant for understanding phenomena such as double descent, but their argument is unconvincing because of the inapplicability of their theorems as discussed above. And since their method has no need for the partition to be into linear subfunctions, it is unclear why the number of such subfunctions (which they term \"expressivity\") has the explanatory power they hope for.",
            "summary_of_the_review": "I recommend rejection because the paper uses generalization bounds inappropriately. In particular, they use a single-hypothesis bound, which does not hold if the training set affects the learned hypothesis. Also, they do not adequately explain why they use a partition into linear subfunctions rather than another partition.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper gives a probabilistic error bound on a fixed target function (Theorem 2.1.), that can be estimated from additional labeled samples. A key aspect of this bound is that it reflects which \"linear region\" of the neural network a samples falls into. Empirically, authors show that the surrogate quantity used in the bound is actually an informative metric for two practical tasks: Out-of-distribution sample detection and misclassification prediction.",
            "main_review": "On the positive side, I think it is an interesting empirical fact that the proposed metric is quite well-correlated with the out-of-distribution-ness of the sample or the misclassification-likelihood. Yet, I have several concerns about both theoretical and empirical aspects of this paper.\n\n- **Clarifying the scope of the \"Generalization bound\":** If I understood correctly, the main result of this paper is Theorem 2.1. (and the following corollaries), which is supposed to provide an improvement over the single-model bound of Mohri et al., (2018, eq. 2.17). Both bounds consider a setup where there is a \"single fixed hypothesis\" in our hypothesis space that cannot be learned from the training data. This is quite different from the typical results that we denote by the term \"generalization bounds,\" which aim to provide test risk guarantees on the model that has been learned by some learning algorithm (e.g., via empirical risk minimization). I think that the paper should put more effort to clearly separate two categories (e.g., by avoiding the use of the word \"training set\"); in fact, I am quite worried about the discussions in section 2.4., where the authors compare the provided result with works that use the number of parameters as the model complexity metric---such parameter-based model complexity metrics appear in the literature only because they are considering more than one candidate function!\n\n- __On the quantity $\\widehat{R}^*$:__ The main results of this paper are stated in the form of the quantity $\\widehat{R}^*$, instead of the usual empirical error $\\widehat{R}$. While the definition of this quantity is given in equation 10, it is not clear to me what this quantity is, why it is important, or how it can be used to bound the actual empirical error.\n\n- __On the sharpness of Theorem 2.1.__ One thing I noticed about Theorem 2.1., is that it has the slack term $\\frac{1}{\\mathbb{P}(h_i)}\\sqrt{\\frac{\\log(2/\\delta)}{2N}}$, while the slack term of the usual Hoeffding-style bound is $\\sqrt{\\frac{\\log(2/\\delta)}{2N}}$. As the quantity $\\frac{1}{\\mathbb{P}(h_i)}$ is always greater than or equal to $1$, this should imply that the gap of the new bound is never less than the gap of the naïve bound. Looking at Figure 1, I think this is indeed the case. While Fig1b and Fig1d have very different z-axis scaling, it seems like the lower bound in Fig1b is strictly better than Fig1d (if we ignore how Fig1d can be more \"visually appealing\"). Is this indeed the case?\n\n- __Assumptions of Theorem 2.1.__ A big limitation of the main result is that it only applies to the same-size datasets that have the same dataset sizes in each activation region, as the dataset that has been used to approximate the upper bound. In other words, it imposes some assumptions on the well-behavedness of the test data, making the usefulness of the presented bound quite restricted.\n\n- __Experimental results: Are the baselines properly tuned?.__ About these, I am not sure if the authors have paid enough attention in tuning the baseline methods well for the fairness of comparison. For example, look at the \"Class distance\" metric of Lee et al. (2018). In Table 2, the experimental results given are depressingly low, lower than 0.50 AUROC in many cases. However, the actual figures appearing in Lee et al. (2018) is quite higher than those; most exceed 0.97. I wonder where such discrepancies come from. Perhaps one reason could be that the authors are using ResNet-50 (which is an unusual choice for CIFAR-10 scale dataset), instead of the models appearing in Lee et al.; I am not arguing that the authors should follow the model choice of Lee et al., but instead, I think authors should pay more attention to tuning the parameters of the baselines in such case for a fairer comparison. From my understandings on the OOD detection literature, I suspect that the results would be quite different if one does so---recent works on OOD detection tend to achieve extremely high AUROC in many cases. (If you think I am missing a critical difference in the experimental setup that makes a big difference, please let me know! I could have missed it.)\n",
            "summary_of_the_review": "The main theoretical result is quite vague and limited in its message, and is potentially worse than the existing result. The empirical result looks promising at a glance, but I have concerns about the well-validatedness of the results.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper aims at providing input-conditioned generalization bound, which varies according to the given test sample.\nCompared to the existing input-agnostic bounds, the proposed methods work well in detecting out-of-distribution problems.\nAlthough the idea behind the paper is attractive, I am still concerned with the theorem's assumptions.\nEspecially, I am puzzled by how this paper deals with the dependency between training data and the trained models.\nI will raise my score if the authors can clarify the concerns. ",
            "main_review": "Maybe I lost some details. The authors can point it out if I write anything wrong.\nMy major concern falls in the notation h_i. Could the authors give a formal definition of h_i? Especially, is h_i trained during the training process (given p_i, for example), or just predefined? For example, in Equation4, it seems that it requires that h_i is predefined. Otherwise, there is an annoying dependency between h_i and S_i coming from the training procedure.\n\nThe things can be split into two folds:\nIf h_i is predefined, it seems that the model considered in the paper is not trained at all (at least, I have not found the training components).\nHowever, if h_i is trained, I start to doubt the correctness of Equation4 and Equation11 due to the dependency between dataset and h_i.\nCould the authors provide more explanations about that?",
            "summary_of_the_review": "As said before, there are still some points unclear to me. I will update my score after the rebuttal period.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No.",
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper introduces input-aware generalization error bounds that depend on the piecewise linear subregions of a neural network (e.g. with ReLU activation) and the local density of training examples. The generalization bound holds even for linear subregions for which there are no training examples, by weighting the samples in nearby linear regions. The paper also includes promising empirical results for out of distribution detection and in-distribution error prediction, using a practical procedure that relies on only a weaker guarantee than the full bound (which would be impossible to compute with present knowledge). ",
            "main_review": "Overall, I think the paper makes a valuable contribution by providing a generalization error bound that is input-dependent, and showing it to be practically competitive with other (often less theoretically-grounded) uncertainty metrics.\nIn particular, some strengths of the paper:\n- Dependence on the local density of training data\n- Comparison to many other candidate uncertainty metrics\n\nWeaknesses are listed roughly in order of importance; many points are suggestions that might improve the clarity of the presentation:\n- Dependence on the number of distinct piecewise linear regions. The authors acknowledge that this quantity is not computable in practice (at least not at present), which makes me wonder how much benefit it offers compared to capacity-based bounds (which are typically computable but loose). That said, I do appreciate the experimental results which use a loose approximation and are still competitive with other metrics. Perhaps the authors could discuss the comparison between their bound in this worst case scenario (with the full exponential number of linear regions) and other bounds based on the number of network parameters.\n- In equation 17, I don’t quite follow the notation or broader purpose of the corollary. On the left side it appears we are taking an expectation over h_i, but h_i also appears on the right side.\n- Figure 1: What is the difference between subfigures (d) and (e)? Also, comparing figures (b) and (d,e) the axis scaling makes it seem that the model-level error bound produces a tighter error bound (compared to the proposed local error bound) for regions far from the training data; is this correct?\n- A proof sketch for Theorem 2.1 in the main text would help the reader gain intuition and better understand later claims like the relaxation of the bound used in the experiments (in the paragraph split between pages 7 and 8).\n- Figures often appear on the page prior to where they are introduced in the text. This places an unnecessary burden on the reader to flip back and forth frequently.\n- I didn’t completely follow the argument about shifting samples at the bottom of page 4. This seems to mostly rely on a Lipschitz constant of the error, but its purpose seems only to conclude that the weighting (kernel?) function k should be decreasing with distance and not too close to zero, both of which seem fairly intuitive. My suggestion is to replace this argument with a more straightforward discussion of the desirable features of a kernel function for this setting.\n- It isn’t quite the same, but equation 5 is reminiscent of Nadaraya-Watson kernel regression. Perhaps the authors could make the connection to kernel methods more explicit, beyond the naming of the similarity function k.\n- Many of the papers that are introduced in the related work section also appear in the tables in the experiments section, but in a completely different order. It would be easier to follow the experiments if they were presented in the same order as the related work.\n- It would be preferable to include the definitions of the experimental metrics in the main text rather than the appendix. \n- Table 1 seems a bit redundant; if I understand correctly it can be computed directly using the data in Table 2 and Table 3.\n- Figure 6 is uninterpretable when printed in black and white (or for colorblind folks); I’d recommend using colors that are more grayscale-friendly.\n- Why does R sometimes have * (e.g. equation 10, equation 16, equation 17)?\n",
            "summary_of_the_review": "The paper makes a valuable contribution in the main theorem and experiments. However, the corollaries are unclear and various aspects of the presentation could be improved.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}