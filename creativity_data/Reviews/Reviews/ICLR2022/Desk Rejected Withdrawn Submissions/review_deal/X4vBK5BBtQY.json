{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose a method to improve the transferability of adversarial examples using a scale ensemble. They aim to achieve this goal by integrating the gradient resulting from different image sizes. The proposed method can be combined with different adversarial attacks. The authors have also considered a vast number of models.  ",
            "main_review": "Strength:\n\n-- The proposed method is easy to implement.\n \n-- The model can be integrated with different adversarial attacks.\n\n-- The paper is easy to follow. \n\nWeaknesses:\n\n-- The paper seems to include several grammatical and spelling errors.\n\n-- The resizing model need more explanation. Do the authors follow Cihang Xie et al. (ICLR 2018)?\n\n-- The proposed method does not seem to be novel enough for publication in ICLR.\n\nQuestions:\n\n-- Can the authors expand on the limits on l and r? Do these limits vary with the input size? \n\n-- There can be several ablation studies on the Eq. 5. For instance, for different values of $\\alpha_i$, how the gradient values change? Do we see a pattern? What about the direction of the gradients?   \n\n-- Can the gradients be weighted based on the image size?",
            "summary_of_the_review": "The paper provides a method that can be implemented easily. However, it lacks the required novelty and ablation studies. Although, I believe the proposed method has the potential for a good publication, I do not recommend the acceptance of the paper in the current form. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper aims at improving the robustness and transferability of adversarial attacks for image classification. Authors argue that the classification attention varies a lot when the image are resized before being fed to the classifier, thus combining adversarial noises from different scales of the input can increase the attack success rate and transferability. To this end, authors propose a framework called SEM, to conduct adversarial attack by combining adversarial perturbations from multiple scales of the raw input image, at each iteration of I-FGSM or its follow-up works. Author shows that the attack success rate for both white-box and transfer-based attack can be greatly improved, and the proposed method is shown to be able to combine with existing attack enhancements.",
            "main_review": "The attack process is not articulated clearly enough and I am a little bit confused about how are the images of different sizes used. For example, in Eq 5 and Fig 1, how are the gradients of different scales ensembled? I guess they are reshaped to the original size to compute the average? I am also not very sure what will happen if a classifier only allows inputs to be a specific size (e.g. VGG classifier which has a FC layer instead of pooling layer after the stack of convolutions)? Maybe these networks have to be modified to be compatible with SEM attack?\n\nAnother concern I have is that the difference between the proposed SEM and existing DIM is not very clear. DIM also supports to randomly resize the input image to different sizes (say between 299 and 330) during each iteration. I am not quite sure what is the fundamental difference between SEM and DIM, except for that SEM handles multiple scales in a single iteration. Authors claim DIM is scale-invariant in Fig 3 and I don't quite understand that argument.\n\nThe third concern is about the number of forward/backward times, which is already discussed in the appendix. However, in the appendix, authors adjust the number of iterations for other methods to very large numbers (e.g. let DIM do 1000 iterations). This concerns me a lot, since DIM is originally supposed to have very small number of iterations (i.e. $min(\\epsilon+4, 1.25\\epsilon)$ as discussed in the DIM paper). So I believe the experiments in A.2 is not solid enough to justify the efficacy of SEM. Maybe one possibility is to randomly feed an input image with shape $\\alpha_i$ where $i\\in{1,2,\\cdots,N}$, instead of having 50 passes inside one iteration. This can be a fairer comparison with number of forward/backward passes aligned.\n\nThe writing quality can be improved, and there are a few typos. For example, in the caption of Table 1, there is a typo \"evluation\" and in the upper half of Fig 2, the axis titles \"l\" and \"r\" are misplaced (\"l\" is supposed to be smaller than 1.0).",
            "summary_of_the_review": "Considering the concerns on presentation clarity and experiments fairness, I believe this paper is not ready for publication yet.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Paper proposes a new attack called the scale ensemble method which ensembles adversarial examples of different sizes to improve the transferability of adversarial examples. The core idea is primarily inspired by Deepdream. While the technical novelty of the method is limited, the empirical results of the method are good.  \n",
            "main_review": "#### Strengths: \n1. Experimental setting and results: \n    * The experimental setting and the results are quite good. \n    * Proposed approach is tested on four models and compared with three SoTA input transformation adversarial methods and tested on ten adversarial defense methods.\n2. The simplicity of the proposed approach makes it very easy to integrate this approach with existing gradient and input transformation methods to improve their performance \n#### Weaknesses: \n1. The entire work is primarily inspired by the fact that adversarial examples generated using existing methods are sensitive to image interpolation methods.  But in the experimental results, the difference between the resolution used for source and target models is very minimal; for example, the source model has an input resolution of 224 x 224 and the target model has an input resolution of 299x 299 or vice versa, this difference is very minimal.  It would be interesting and useful to understand the performance of the proposed method when the difference between the input image resolution of the source and target models is higher, something like 128 x 128 to 224 x 224 or something in this range. \n2. For resizing the images the paper uses Nearest neighbor, it is unclear why this choice is the best. While this has a low computation cost it’s important to run some ablations using the most recent image interpolation methods (Transposed convolution [1], Sub-pixel layer [2], Meta-upscale module [3]) \n3. Maintaining texture is posed as a major advantage. Why is maintaining texture very important? The only reason I see is that the adversarial sample will be indistinguishable perceptually when the texture is maintained. Is that the only reason or is there something else that maintaining the texture gives?\n    * It might be useful to compare the texture obtained from the existing method with SEM\n\n[1] Long et al. 2014, Fully Convolutional Networks for Semantic Segmentation\n[2] Aitken et al. 2017, Checkerboard artifact free sub-pixel convolution: A note on sub-pixel convolution, resize convolution and convolution resize\t\n[3] Hu et al. 2019, Meta-SR: A Magnification-Arbitrary Network for Super-Resolution\n",
            "summary_of_the_review": "As mentioned above, despite the limited technical novelty, the experimental results are good, but the paper needs more ablations to answer the weaknesses pointed above and conclusively say **“scale of inputs can be an important factor for generating more effective adversarial examples.”**. Moreover, since the entire premise of the work is based on the input scale of the image, it is important to test the method on transferring adversarial examples between models with high differences in input resolution. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors proposed a Scale Ensemble Method to enhance the transferability of black-box attacks. The method first resizes input images with different scales and calculates gradients from these resized images. Finally, the method uses the cumulative gradient to perform the updates. The authors performed a number of experiments, including box-attacks on clean and defense models, combination with other attack methods, and model ensemble attacks. They also compared with other transformation-based attack methods. ",
            "main_review": "Strengths\n1)\tThe method is simple and can be implemented easily like other transformation-based methods.\n2)\tThe experimental results show that the proposed method is more effective than other transformation methods.\n3)\tThe authors evaluated their method on both defense and clean models. \n4)\tThe proposed method can enhance the capability of other methods when they combined together. \n\nWeaknesses\n1)\tOne very serious problem is that this paper is full of grammatical errors. It is too many and many of them can be detected and corrected by grammatical checker. I only list some in here to justify my observations, instead of all because I don’t want to proofread the authors’ paper. \n\tPage 1, learned,,\nPage 2 and Kurakin et al. (2018) starts\nPage 2, MI-FGSM, which integrate\nPage 2, several run-on sentences in the section Gradient Optimization Based Attack\nPage 2, which is divide \nPage 2 can removes \nPage 3 is extend \nPage 4 mollifer \nPage 5 hyper-parameters .. is \nPage 8 with SP-MI-FGSM should be SE-MI-FGSM?\nPage 8 via SP-MI-FGSM should be SE-MI-FGSM?\nPage 9, overcomes these two feedbacks.\nI would like to emphasize once again that this list is far from complete. \n2)\tAlthough this paper has only 6 equations and several of them are copied from previous papers, the authors made a mistake. In Eq. 4, when i=N-1, alpha=l+(N-1) *(r-l), which is not r, different from what is described before (above Eq. 4).\n3)\tThe organization of this paper is also problematic. The authors reviewed other input transformation methods in the method section. They should be putted in Section 2.\n4)\tIn the comparison, the authors limit their baselines on input transformation methods and only compare with VR, DI and SI. However, they intentionally/unintentionally ignore the state-of-the-art LinBP and some other methods. As far as I know, LinBP is the currently the best transfer method. \n5)\tThe authors only evaluate their method on epsilon=16. Although they follow Wang&He 2021, it does not give a complete picture. More different epsilons are expected. \n6)\tAdding a method on the top of other methods to improve transferability is good but cannot be considered a significant contribution. \n7)\tIn addition to untargeted attack, target attack, which is more challenging, should be considered. \n8)\tOne problem in the experiments is that some images are incorrect classified by the models. However, they authors do not say it clearly. Only said that “mostly classified correctly by the evaluation models”. How do they impact the experimental results? They will naturally provide better numbers to all methods. It means that the success attack accuracy should be lower than the one reported. \n",
            "summary_of_the_review": "I recommend rejecting this paper, because three major reasons:\n1)\tThe grammatic errors and other presentation errors such as the error in equation 4 are too many. It is far above the tolerance level of a top conference. \n2)\tThe experiments and comparisons are not enough to demonstrate that the proposed method outperforms the state-of-the-art.\n3)\tSome additional experiments are expected. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N.A",
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}