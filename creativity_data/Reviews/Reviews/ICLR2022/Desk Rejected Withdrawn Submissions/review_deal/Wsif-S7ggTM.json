{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an architecture, called Cross-Stage Transformer, for video classification. The main technical contributions are from the new design of the cross-stage self-attention (both spatial and temporal) and the cross-stage feature aggregation module. Experiments are conducted on Kinetics-400 and Kinetics-600. Although the idea of cross-stage connections (with self-attention and feature aggregation) is well-motivated, the ablation shows only small gains from baseline. The proposed approach is comparable with current methods on Kinetics-400 and Kinetics-600. Written is mostly clear except for a few places which may need clarifications.",
            "main_review": "## Strength\n- the paper conveys the idea of using cross-stage interactions (self-attention and feature aggregation) for improving video classification which is interesting and well-motivated.\n- the paper provides various ablations which are helpful to the readers to understand the contribution of each design choice.\n\n## Weakness\n- The gains compared with baseline are small (see Table 2b). More specific, CSSA brings +0.5 and FAM brings another +0.4 improvement over baseline. In total the proposed architecture makes an improvement of +0.9% over baseline which is not significant and somehow shows that the cross-stage interactions are not super useful for video classification.\n- Although the paper claims to approach \"video learning\" as in its title, in practice it tries to solve video classification, more specific, the experiments are conducted on \"action recognition\". Even more narrow road, the experiments are done only on Kinetics. Kinetics-400 and Kinetics-600 are still two variants of the same dataset. In short, experiments are done in one dataset.\n- Accuracy (by the proposed method) is comparable with current methods (when compared with the similar backbone and same training data).",
            "summary_of_the_review": "In summary, although I think the idea of making cross-stage connections in transformer architecture is interesting, the experimental results are not convincing enough both in term of ablation and comparing with current methods. With these evaluation, I cannot recommend this work for a top-tier conference publication.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents two simple extensions to existing work on Video Transformers that are shown to improve the accuracy. The improvements are: (a) previously computed attention masks are added up to the one from the current layer and renormalised. Features from intermediate layers are aggregated to obtain a final feature for classification. The results show some decent increase in performance over the baseline.",
            "main_review": "On the positive side the paper presents two very simple improvements to transformers that seem to work. On the negative side the novelty of the proposed improvements is not so significant and the accuracy improvement is decent but not impressive. Actually, the paper's main contribution i.e. cross-stage self-attention improves only by 0.5%. Moreover, the authors need to clarify in more detail the differences of their baseline model with ViVit and Timesformer. This would help the readers understand to what extent  the improvements are coming from the baseline  and to what extent from the proposed improvements. I think the authors first need to ablate the components of their baseline and then on top show the impact of their proposed improvements (cross-stage self-attention + feature aggregation). Finally, reporting results on one dataset only is insufficient for a high quality conference like ICLR.  ",
            "summary_of_the_review": "The method is promising to some extent but clarity along with more experiments are missing. This in combination with lack of novelty makes me recommend reject.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the author introduces a new video transformer, which is consisted of STBs (spatial transformer blocks) and TTBs (temporal transformer blocks). During the transition from one STB (TTB) to its subsequent STB (TTB), a cross-stage self-attention is introduced, which is to assign the self-attention from previous stage to the current stage by a learnable ratio. To formulate the final representation, another learnable ratio is introduced to the output of each stage and the final representation is the weighted average of each stage's output. At last, the model is evaluated in Kinetics-400 and 600 datasets.",
            "main_review": "Strengths:\n1. The entire paper is well and clearly written. The core idea is easy to follow and understand.\n\n\nWeaknesses:\n1. The proposed algorithm is lack of novelty. Compared to the previous video transformer (ViVit, TimeSformer), the difference of this work is: 1) stacked a set of video transformer with spatial attention and followed by another set of video transformer with temporal attention, while, the TimesFormer with divided space-time attention alternately adopt spatial and temporal attention. 2) this work introduce additional residual connection (with learnable ratio) between attention block from adjacent transformer blocks. 3) the final output is weighted average of the output from each transformer block while the TimeSformer take the output of last block as the final representation.\n\n2. This paper would need more evidence to support its claim. For example, it's not clear why we should have the current layout of the network. How a set of spatial transformer block followed by a set of temporal transformer block is better than alternating design? considering the cross-stage self-attention could work in both cases. \n\n3. The improvement of performance is minor. Take the ViVit as the baseline. If we are going to compare between the best performance between ViViT and this paper, the improvement is minor (81.3% from ViViT vs. 81.8% from this paper). If we are going to compare the performance using the same input resolution (16x224x224), the performance of this paper is still on par with ViViT (80.6% from ViViT vs. 80.1% from this paper). In terms of the computational cost, it is wrong to compare GFLOPs between two methods with different inference views. As the performance of any method will not be linearly increased with number of inference views, it is not fair to put 4x3 views for ViViT and 1x3 views for this paper.  As a result, the number of 8.6% does not make too much sense. \nAt last, I would suggest to include MViT[1] as one of the reference, which is the SOTA video transformer so far.\n\n4. The proposed work should be evaluated in at least one or two more video benchmarks. The nature of K400 and K600 is the same. I would suggest also include Something-Something-V2 or Epic-Kitchen datasets. \n\n[1] Multiscale vision transformers, ICCV 2021\n",
            "summary_of_the_review": "Overall this paper requires major revision and more evidence to support its claim.\nConsidering the weakness I mentioned above, I decide to reject this paper.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a cross-stage transformer paradigm to fuse self-attentions and features from different blocks more efficiently and effectively. The proposed method is evaluated on several standard video benchmark datasets and shows similar or better performance than previous Transformer methods.",
            "main_review": "Strengths:\n1. This paper is well-motivated since the problem of finding appropriate Transformer architectures for videos is still unsolved. It is worth investigating how to effectively and efficiently model rich contextual information in videos.\n2. Fusing multi-stage information is a new idea for video Transformer.\n3. The paper is well-written and easy to follow.\n4. The experimental results demonstrate the effectiveness of each component of the proposed method. \n\nWeaknesses:\n1. novelty concerns:\n\n(a) One of the main concerns is the novelty issue. One of the main contributions is the connection between different layers, which is similar to DenseNet [1]. The paper claims that “Since video learning needs to capture more complex information from spatial and temporal dimensions, simple shortcuts cannot efficiently work in existing video transformers”. However, there are no further analyses or ablation studies for this claim. It would be much better if the paper can show or prove that the proposed “cross-stage fusion” works better than simple shortcuts.\n\nReference:\n\n[1] Gao Huang, et al. “Densely Connected Convolutional Networks”, CVPR, 2017.\n\n2. technical detail concerns:\n\n(a) Cross-stage transformer structure: Currently the structure is like [(STB)xM-(TTB)xN], where M and N represent the numbers of STB and TTB, respectively. Why not adopting the structure like [STB-TTB-STB-TTB-……] or some structures as shown in TimeSformer? It is more reasonable to modeling spatial and temporal information alternatively like TimeSformer. It would be great to provide more analyses or explanation for the choice of the structure. Moreover, it would be great to explain how to decide the choices of Transformer blocks in the paper.\n\n(b) Figure 2: The caption says, “For simplicity, we only show cross-stage self-attention flow of two consecutive transformer blocks”. Does that mean “cross-stage” is actually across more than two consecutive blocks? This part is a little bit confusing.\n\n3. experiment concerns:\n\n(a) Table 1: There are lots of ablation experiments for the model configuration. However, it is a little bit confusing about the selected configurations in the final comparison.\n\n(b) Figure 4: the difference between bright and dark regions are not very obvious (2nd and 3rd rows). Maybe showing visual attention maps could help.\n\n(c) State-of-the-art comparison: It would be great to compare MViT and VidTr [2] as well since they are both state of the arts for video Transformers. \nThe results of MViT are as follows (which are missing in the paper):\n\nKinetics-400:\n\nTop-1 / Top-5 / Views / TFLOPs\n\n81.2 / 95.1 / 3×3 / 4.1\n\nKinetics-600:\n\nTop-1 / Top-5 / Views / TFLOPs\n\n84.1 / 96.5 / 1×5 / 1.2\n\nThe paper only mentions that MViT is much different from the original ViT structure, but it is not a convincing reason for not comparing it. Moreover, MViT has competitive performance although it is trained from scratch. Therefore, more analyses are needed.\n\n(d) Parameter number should be another metric to compare efficiency.\n\n(e) Table 4: The numbers for TimeSformer-L are not correct. They should be corrected as follows:\n\nTop-1 / Top-5 / Views / TFLOPs\n\n82.4 / 96.0 / 1x3 / 5.1\n\nReference:\n\n[2] Yanyi Zhang, et al. \"VidTr: Video Transformer Without Convolutions\", ICCV, 2021.\n",
            "summary_of_the_review": "The paper proposes a simple yet effective method for video Transformer. Although the main idea is incremental, the paper provides sufficient ablation studies to support the proposed method. Therefore, my rating is slightly beyond the borderline.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}