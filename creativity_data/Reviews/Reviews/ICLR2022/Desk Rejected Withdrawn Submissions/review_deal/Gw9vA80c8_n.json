{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper combines hyper network and interval network with IM-Net to produce a method that can generate 3D objects with higher visual quality.",
            "main_review": "Strengths:\nThis paper combines hyper network and interval network with IM-Net to produce a method that can generate 3D objects with higher visual quality.\n\nWeaknesses:\nThis paper only combines the 3 papers without thorough analysis. The reviewer propose several questions:\n1. Why would there be artifact in the IM-Net and why interval network can eliminate the artifacts.\n2. It seems that using SDF or just improving voxel resolution can also be helpful to remove the artifact. Why do the authors use interval network. Is it the best way?\n3. In figure 1 and figure 6, the term HyperCube has different meanings.",
            "summary_of_the_review": "A rejection is suggested for this paper based on the above mentioned concerns.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents a method to perform an implicit shape representation using voxels. Unlike current methods which sample points on the voxel to learn the shapes, it uses an interval representation of voxel and performs interval learning which allows the network to learn the entire voxel. This overcomes the main limitation of voxel-based learning methods which is that they cannot learn object boundaries very well. Furthermore, this performance is achieved at a significantly faster speed, thanks to the proposed HyperCube. However, the proposed network is not generic like existing methods, such as IM-NET. While IM-NET is learnt on various object categories, the proposed network is trained separately on each object category. ",
            "main_review": "Strengths:\n\n1. Better object representation achieved through better understanding of  object geometries and its boundaries.\n2. The computation is relatively fast, as compared to IM-NET.\n\n\nWeakness:\n1. Loss of generality. The network has to be trained on each object category separately. \n2. The main claim of the paper is that it learns the object boundaries better at a reduced time. The HyperCube only tackles the latter. It is the HyperCube-Interval that allows the better object representation, also seen in figure 6 results. Then, why is there no quantitative comparison between the performance of these two. I think it is important to put HyperCube-Interval in table 1 and 2.\n3. The failure cases/limitations of the proposed approach are never mentioned.\n\n\n\nThere are some typos:\n1. On page 4 before 3.1: It reads: Finally, we introduce interval arithmetic, which allows us to propagate 3D cubes instead\nof point sampled from voxels, and we show \"how we can use incorporate is within our approach\"\nHyperCube-Interval.\n2. section 3.2, second paragraph: returns ab inside/outside category :=> returns an inside/outside category\n3. Table 1 is never cited.",
            "summary_of_the_review": "The technical novelty is highly limited. However, the proposed approach can be useful as it does allow better object representation. The paper reads nicely except for the minor typos mentioned above. My major problem is the lack of quantitative analysis of HyperCube-Interval.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper considers the problem of training an implicit field decoder to decode the xyz position to a binary inside or outside value. The authors argue the baseline, IM-NET, trains the same implicit decoder for all the 3D objects in the dataset, leading to convergence. Moreover, the authors claim the way IM-NET samples 3D positions is “within voxels (and not the entire voxels)”, which results in boundary problems. To address the training problem, the authors propose  HyperCube to generate implicit field neural network weights from a hyper network. Furthermore, to solve the sampling problem, the authors propose HyperCube-interval, which uses interval to replace point sampling. The experiments show HyperCube converges faster.",
            "main_review": "strength\n+ apply hyper network to generate implicit field network weights is an interesting application\n+ faster convergence speed, as shown in Fg. 5\n  \nweakness\n- paper writing. \n\nThis paper has lots of vague claims, is not self-contained, and lacks many details. I believe it needs huge refinement from the current form. For example, in the abstract, the authors directly write the shortcomings of implicit field decoder. However, after I finish reading, I find it is actually the shortcomings of how to train the implicit field decoder, namely, the shortcomings of the training in IM-NET. Its claim is unclear and makes the reader very confusing. In another example, it criticizes the position samples, but I am not sure what does “within voxels (and not the entire voxels)” mean. The same happens in many other places in the paper, like how to apply interval arithmetic. While I appreciate the authors releasing the code in the supplementary, I would recommend the authors to provide a detailed explanation document to add all the missing information in the supplementary.\n\n- innovation\n\nThe authors apply the hypernet to the task of implicit field generation and achieve faster training speed. However, none of them are new. It seems the authors combine several existing technologies together, which makes the innovation of the paper to some extent limited.  \nOne missing related work is “Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations, NeurIPS 2019”, which did similar things. It applied a hyper network to generate implicit decoder used in the rendering field.\n\n- training speed comparison.\n\nThe first contribution of the paper is to claim the hypernetwork brings faster training speed and less memory cost.  However, Fig.5 is very confusing. First, in the left subfigure, I think the authors want to say for the same batch size, the training time of HyperCube is less than IM-NET. However, why is the time marked with second?  Does it mean you train the whole model in just 0.5 second? If not(I guess it is per iteration?), are you training two methods with the same iteration numbers? It is meaningless to show the time of one iteration if one method needs more iteration to converge. I think you can simply report a total time here, which is much more clear.  This is another example of unclear writing. \n\n- sampling claim.\n\nThe authors claimed the way IM-NET samples will result in boundary issues. However, in IM-NET section 3.1 they said they “sample more points near shape surfaces” and “sign a weight wp to each sampled point p, representing the inverse of the sampling density near p.” It is reasonable to me and thus I don’t think the claim of the authors is correct. Specifically, the authors said “the implicit decoder processes only points sampled from within a voxel, instead of the entire voxels”(Sec 1). Please explain in more detail what it means and why it causes problems. It seems the authors sample uniformly in the cube space(Sec 3), which should take more memory as it is o(n^3). Why is it more memory-efficient than IM-NET(sampling with o(n^2))?\n\n- interval samplings.\n\nThe authors show that they can use interval arithmetic to replace point sampling and claim it results in fewer artifacts. However, it is not supported in the paper.  Fig.7 shows it only results in better performance in several classes, not all classes. Table 2 the score of HyperCube-interval is missing. \n\n- other results\n\nWhat does table 1 for? It is not referred to anywhere in the paper.\nIt is said in Sec.4 HyperCube-interval is faster than HyperCube, where is the time comparison?  Table 5 seems to be just HyperCube.\nWhy only 5 classes? More classes would be better, as Im-NET verified on 13 classes. \n",
            "summary_of_the_review": "Overall, I feel this paper is not ready to submit in the current form. The problematic claims, incompleteexperiments and unclear paper writing makes me feel it needs a second review.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a 3D reconstruction scheme that, given an object shape embedding, produces the occupancy map based on a hyper-network structure and via interval bound propagation (IBP). The authors claim that the proposed scheme makes improvement on prior work, referred to as IM-NET, particularly in terms of model capacity and sampling strategy. Experimental results showing that the proposed scheme actually behaves as intended were provided. ",
            "main_review": "The paper is moderately clear and the proposed scheme is moderately reasonable.\n\nUse of Hypernetwork:\nWhile I agree that the hypernetwork structure improves the model capacity, this is not the first work that has shown it (see Spurek et al., 2020;  Mitchell et al., 2020). I notice the difference that this paper is to build an occupancy classifier, while the referenced papers are to explicitly generate the point cloud samples. However, they are all in quite the same field of study and the hypernetwork structure in this paper has nothing special other than IBP, which will be discussed below, in comparison with those in prior studies. Especially, the \"essential\" efficiency of such a hypernetwork structure was already revealed in (Mitchell et al., 2020).\n\nUse of IBP:\nThe authors adopted IBP to train the occupancy classifier so that it should produce the same output to the 3D points in the same voxel. The mechanism implementing the IBP itself remains the same as originally proposed in (Gowal et al., 2018), but the way (or the purpose) of applying it is somewhat novel. The original IBP was applied to the image pixels to make the neural network robust to the perturbations in the pixel domain. Here, the IBP was used to make a volumetric assignment for the 3D grid cells. However, a major concern arises in that the volumetric assignment may discretize the surface representations, undermining the merits of having the implicit occupancy function on continuous 3D coordinate inputs.  \n\nIn my opinion, the mesh transfer from a 3D ball to the reconstructed shape in (Spurek et al., 2020) can also be used to connect all the pieces of an object together, essentially solving the same problem. What is the merit of using IBP over the mesh transfer?\n\nExperimental Results: \nIn Figure 1, the results in the name of HyperCube seem to be actually from HyperCube (+interval), as indicated by the caption. The authors should make every notation be clear and, whenever possible, should provide both results from HyperCube and HyperCube (+interval). This comment applies to all other figures and tables.\n\nThe effects of the proposed scheme are slightly exaggerated. For example, in Figure 5, the complexity of HyperCube is 2-2.5 times lower than that of IM-NET, but the caption says \"HyperCube method offers over an order of magnitude decrease ...\"\n\n\nReferences: \n- Przemyslaw Spurek, Sebastian Winczowski, Jacek Tabor, Maciej Zamorski, Maciej Zieba, and Tomasz Trzcinski, \"Hypernetwork approach to generating point clouds.\" ICML, 2020 (cited).\n- Eric Mitchell, Selim Engin, Volkan Isler, and Daniel D. Lee, \"Higher-order function networks for learning composable 3D object representation,\" ICLR, 2020 (uncited).\n- Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli, \"On the effectiveness of interval bound propagation for training verifiably robust models.\" arXiv preprint arXiv:1810.12715, 2018 (cited).",
            "summary_of_the_review": "The paper is moderately clear and the proposed scheme is moderately reasonable. However, the use of a hypernetwork structure is not novel, and regarding the use of IBP, there is a concern that it may discretize the surface representations, undermining the merits of the existing scheme.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}