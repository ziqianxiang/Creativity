{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a new algorithm named \"linear backpropagation\" to calculate the gradients in neural networks. The theoretic analysis and experimental results indicate that with the same learning rates (small enough), the linear backpropagation can make the model converge faster and achieve better generalization performance compared to the standard backpropagation.",
            "main_review": "The idea of this paper is interesting. It allows the gradient to pass unactivated relu units to help update the parameters. The theoretic analysis proves that when the learning rate is small enough, such updating rules can make the SGD algorithm approach the optimal parameters closer in the same iterations. The above theoretic predictions are observed in the experimental results of applying the proposed method on several deep neural network models.\n\nMy main concerns include:\n\n(1) The assumption of small learning rates limits the proposed method and causes inconsistency between the experiment setting in this paper and the papers of the baselines. For example, in the papers proposing DenseNet [1] and ResNet[2], the learning rates are both initialized as 0.1. And in the paper proposing MobileNetV2[3], the learning rate is initialized as 0.045. Both of the two settings are significantly larger than the learning rates in this paper. And from Figure 6 of this paper, we can see that the final converged test accuracy of DenseNet with standard backpropagation on CIFAR-10 is lower than 90%, while in the original paper of DenseNet, the performance on CIFAR-10 is better. (In the original paper of ResNet50 and MobileNetV2, the performances on CIFAR-10 are not reported)\n\n(2) There are some activation functions that are similar to relu but allow the gradients to pass through, such as leaky relu [4].\n\nTo address the above issues, I suggest that the following experiments could be included:\n\n(1) Try to initialize the learning rates consistently with the baselines, but after the learning rates drop, replace the standard backpropagation with the proposed method.\n\n(2) Compare the proposed method with the models applying other activations and standard backpropagation.\n\nAlso, I have a suggestion about the small-learning-rate assumption. In graph neural network area, researchers use smaller learning rate. For example, in Graph Convolutional Networks [5] and Graph Attention Networks [6], learning rate is set as 0.01. Also, in the unsupervised version of GraphSAGE [7], the learning rate is even smaller. So maybe experiments (of course, it is only a suggestion, not mandatory for recommandation to acceptance) on this area could be more convincing.\n\n\n[1] Huang, Gao, et al. \"Densely connected convolutional networks.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n\n[2] He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n\n[3] Sandler, Mark, et al. \"Mobilenetv2: Inverted residuals and linear bottlenecks.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.\n\n[4] Maas, Andrew L., et al. \"Rectifier nonlinearities improve neural network acoustic models.\" Proc. icml. Vol. 30. No. 1. 2013.\n\n[5] Kipf, Thomas N., and Max Welling. \"Semi-Supervised Classification with Graph Convolutional Networks.\" International Conference on Learning Representations. 2018. \n\n[6] Veličković, Petar, et al. \"Graph Attention Networks.\" International Conference on Learning Representations. 2018.\n\n[7] Hamilton, William L., et al. \"Inductive representation learning on large graphs.\" Proceedings of the 31st International Conference on Neural Information Processing Systems. 2017.\n",
            "summary_of_the_review": "In general, I think that the idea of this paper is interesting and novel. But I also have some concerns about this paper (in the main review section). If the concerns can be addressed (no matter with further experiments or more detailed response), I think the paper is acceptable.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proves that in a \"teacher-student\" adversarial attack (two-layer) or model training (one-layer) setup, a method named linear backpropagation (LinBP) converges faster than standard backpropagation (BP).",
            "main_review": "The proofs seem sound, although I question the need for a \"teacher-student\" framework to illustrate the point, especially in Section 3.2. But my main reservation with this paper are in the empirics and the significance.\n\nEmpirics: I am almost certain that the \"faster convergence\" results from LinBP follow from the fact that the \"gradient\" norms from LinBP are bigger since nothing is clipped. Thus, for the same learning rate, LinBP takes larger updates than BP. So I view any \"same-learning-rate\" comparisons in the empirical section as extremely suspect. Unfortunately, almost all of the \"model training\" results are comparisons across the same learning rate. To alleviate this concern, authors should show the convergence speed of LinBP across a wide range of learning rates (much tighter than multiples of 10) and demonstrate that LinBP's \"fastest convergence\" path is better than that of classical BP. I will note that authors have attempted to mitigate this concern in the adversarial experiments through the use of normalization, though I again would think that showing a sweep across learning rates would be more convincing.\n\nSignificance: I am not certain that the significance of this work is high enough for ICLR. Nothing in this paper is making me want to go out and use LinBP, as this paper does not make any headway into the main practical issue (usability only at very low learning rates). The fact that at said low learning rates, I can possibly get faster convergence from LinBP, doesn't mean all that much if I can also get faster convergence from higher learning rates with classical BP.",
            "summary_of_the_review": "Because of concerns with \"apples-to-apples\" comparisons in the empirics, as well as skepticism as to the overall significance of the work, I recommend against acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper studies the behavior of linear backpropagation, which keeps the forward process unchanged but removes the gradients of the activation function in the backward process. The paper argues that LinBP converges faster than BP by comparing the distance to optimal of two series of iterates generated by LinBP and BP in several constructed example networks. ",
            "main_review": "The paper has several drawbacks.\n\n1. There is no strong motivation to study the linear BP. \n\n i. The setting is restrictive: networks with ReLU activations in the small learning rate regime. Not to know what will happen for other activations. This is critically important as nowadays Transformer architecture is taking over CV and NLP tasks.\n\n ii.  The theoretical result the paper obtain is not satisfactory. Theoretically, there is no convergence guarantee for LinBP in any case but only a relative bound compared with BP. Note that BP has been shown convergence in the NTK regime and two-layer mean field setting. We know little about LinBP theoretically.\n\niii. The empirical result is not convincing. The experiments use very small learning rate compared to common choices. In this regime, BP underperforms and it is unfair to compare BP and LinBP in this setting. One possible reason, not seriously checked, why LinBP has theoretical advantage over BP with the same learning rate might be LinBP has larger norm than BP.\n\n2. The writing is clear and easy to follow. However the paper lacks of explanation why this happens and the high level idea of how the proof goes through. The reader has not much to learn from this paper instead of accepting the results under certain condition.",
            "summary_of_the_review": "Interesting observation but not convincing results. I would not recommend its publication.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}