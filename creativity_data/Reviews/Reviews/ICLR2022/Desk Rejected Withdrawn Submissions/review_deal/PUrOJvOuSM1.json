{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes GAN-based model for COVID-19 and Cardiomegaly detection (i.e. classification) on Chest X-ray images. The model can be trained using a labelled normal set and an unannotated mix set containing normal and abnormal images. Experimental results show that the proposed model achieved better performance for COVID-19 classification but worse performance for Cardiomegaly detection. Five contributions are listed in the paper, but after reading the paper and comparing it with some existing work, it is really hard to agree with them. Therefore, the authors should carefully conclude the contributions.",
            "main_review": "+++ The topic of using unannotated data for anomaly detection is interesting. Although many existing approaches have been proposed in the field of computer vision and medical image analysis, this paper presents a simple GAN-based model for COVID-19 and Cardiomegaly detection on Chest X-ray images.\n\n--- The title of this work is too big since only chest X-ray images are used for evaluation. So it should be specified for chest X-ray images instead of medical image analysis.\n\n--- The topic of improving anomaly detection by using a labeled set and an unannotated mix set is not posed by this work. Many existing works [1,2,3] have focus on this topic.\n\n--- The idea of this work is very similar to previous work [4] which also can be trained using the same data setting. But this paper did not introduce and discuss this related work. So the novelty of this work is very limited and the first three contributions listed in the paper are overclaimed.\n\n--- The statement \"The generator network takes both diseased and healthy images without knowing their labels\" is incorrect. Because the model has to be trained using the data with a large number of labeled normal images.\n\n--- The generator can directly output a mask map M which can activate the diseased region as foreground. Why not use M to compute the score for classification instead of the difference image?\n\n--- How about the performance when only using the normal images for training? To verify the effectiveness of using an extra unannotated mix set for training, this experiment should be performed. A previous one-class learning approach [5] is also trained and tested on the  ChestX-ray8 dataset and achieves an AUC of 0.84 for normal vs abnormal classification, which is better than the normal vs cardiomegaly classification in this work. This work only tests the model for normal and one specific disease classification, but when we use the model in practice, we do not know the disease type. Therefore, practically, it's not very useful. The fourth contribution of this work is also overclaimed.\n\n--- The performance of cardiomegaly classification is worse than some compared approaches.\n\n--- The results shown in Fig. 3 are unreasonable, especially the left part. The quality of the generated normal image for the abnormal inputs is too bad while it's really good for the normal inputs. Based on this result, can we say the model is not well trained or the model is highly overfitted for the labeled normal images?\n\n--- It looks like that the experiment of simulated anomaly detection is meaningless since this task is too simple to provide some convincing evidence.\n\n\n[1] Ruff, L., Vandermeulen, R.A., Görnitz, N., Binder, A., Müller, E., Müller, K.R. and Kloft, M., 2019. Deep semi-supervised anomaly detection. ICLR 2020.\n[2] Pang, G., Shen, C. and van den Hengel, A., 2019, July. Deep anomaly detection with deviation networks. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining (pp. 353-362).\n[3] Zhang, J., Wang, Z., Meng, J., Tan, Y.P. and Yuan, J., 2018. Boosting positive and unlabeled learning for anomaly detection with multi-features. IEEE Transactions on Multimedia, 21(5), pp.1332-1344.\n[4] Tang, Y., Tang, Y., Zhu, Y., Xiao, J. and Summers, R.M., 2021. A disentangled generative model for disease decomposition in chest x-rays via normal image synthesis. Medical Image Analysis, 67, p.101839.\n[5] Tang, Y.X., Tang, Y.B., Han, M., Xiao, J. and Summers, R.M., 2019, April. Abnormal chest x-ray identification with generative adversarial one-class classifier. ISBI 2019, (pp. 1358-1361).",
            "summary_of_the_review": "The novelty of this work is very limited. The tasks used for evaluation are simple. The performance improvement is not significant.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper is tackling the problem of anomaly detection and anomaly localization. The authors introduce A2B-GAN for one-directional unpaired image-to-image translation. A2B-GAN aims to recover healthy images from a mixed set of healthy and anomalous images. The authors claim to detect and localize anomalies by subtracting the generated image from the original input. Finally,  the authors evaluate A2B-GAN’s ability to detect anomalies on two datasets against other methods using the area under the ROC curve metric.",
            "main_review": "Pros:\n+ The manuscript is written clearly. The method is understandable and consistent with prior work. A2B-GAN uses a reconstruction loss for the generator network to enforce cycle consistency.\n\n+ Table I shows clear improvement on the anomaly detection task using COVID-19 dataset. \n\n+ One strong positive point about this paper is the fact that details of all experiments are well-explained. This will definitely help the community to understand this work and easily reproduce the authors’ results.\n\nConcerns:\n- Although Table I shows improvement on COVID-19 dataset, no improvement can be seen when evaluation is done on the Cardiomegaly dataset. Even the PaDiM method trained on less than 50% of A2B-GAN’s training data is outperforming A2B-GAN on this dataset. I encourage the authors to discuss why this inconsistency is happening.\n\n- An issue with Table 1 is that the comparison is not fair. I don’t think it is generally a good idea to average the performance over multiple datasets and use that as a single comparison metric. Different datasets are not extensions of each other. They have different properties, difficulty levels, number of samples, and many other features which make them incomparable. Here in this case, the COVID-19 dataset’s test case contains only 200 images. On the other hand, the Cardiomegaly dataset’s test set contains 7890 images. Classifying a few more samples correctly compared to other algorithms on the COVID-19 dataset can heavily influence the result of this average.\nEven if we accept this as a metric, then the evaluation on all of the datasets has to be completely fair. The amount of data from Cardiomegaly dataset used to train the A2B-GAN, PaDiM, and PatchCore algorithms is not the same, thus making any comparison unreliable. If training PaDiM and PatchCore is not possible with the authors’ computing power, I highly recommend reducing the amount of training data for other methods. Using the same training, validation, and test sets for all the methods will make the results reliable.\n\n\n- Another set of questions which arise regarding the effectiveness are:\n1. What happens if the number of anomalous samples are much less than the number of healthy samples in set A? Does the generator learn to act as an autoencoder only? I believe this is the case for both COVID-19 and Cardiomegaly datasets. The fact that the Cardiomegaly dataset suffers from a more severe data imbalance issue compared to the COVID-19 dataset, might be one of the reasons behind the drop in the performance on the Cardiomegaly dataset. It will be also interesting to investigate this scenario in even more extreme cases theoretically or empirically.\n2. What happens if the number of healthy samples are much less than the number of anomalous samples in set A?\n\nSo based on the questions above, I believe class imbalance may play a huge role on how effective this method is and it is important to investigate how the performance of A2B-GAN changes in different regimes of training data imbalance.\n\n- The authors propose A2B-GAN as an effective solution for anomaly detection and producing localization maps. However, the paper does not quantitatively evaluate how accurate the localization maps are. I’d recommend adding this evaluation to the paper, otherwise the authors’ claim regarding anomaly localization is not supported and has to be removed.\n\n\n- There are some very minor writing issues which need to be addressed:\n1. Page 5: “We denote the mixed dataset containing both diseased and health images”. Please change “health” to “healthy”.\n2. The term “cycle consistency” sometimes is written as “cycle consistency” and sometimes is written as “cycle-consistency”. The inconsistency in how terms are written can make it hard for the readers to search the keywords effectively.\n",
            "summary_of_the_review": "Overall, the paper is well-written,  A2B-GAN seems effective, and has some novelty. However, I feel the evaluation against other methods is not done fairly and I’m not persuaded if A2B-GAN is actually performing better than PaDiM in the field of medical imaging.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors proposed an anomaly detection framework via generating normal versions of images for those with disease patterns. The detection is completed by looking at the difference between the generated normal and original ones. In the GAN model, the generator produces the normal images and saliency masks(indicating the abnormal regions). Several losses (commonly used ones) are employed here for the adversarial training. The proposed methods are evaluated using two chest x-ray datasets with COVID-19 and cardiomegaly. Superior results of the proposed framework are reported in comparison to previous anomaly detection methods for both medical imaging and natural images. However, I have some critical concerns about the presented work. Details are as follows:",
            "main_review": "+ The idea of anomaly detection in medical imaging is not new, but the authors try to deal with it using an extra dataset with a mix of normal and disease cases, which is different from previous works that often have the abnormal instances identified. \n- Some parts of the method are not clearly introduced. A. How the final anomaly detection is conducted is not clear to me. Is the final detection a classification problem? Thresholding the difference maps? Only looking at the body parts, omitting the background regions? The diff maps seem to be quite noisy. B. is the L_id only applied to the normal images? How about the images in the mixed dataset? \n- I am not quite sure why the authors chose COVID-19 and cardiomegaly as the examples in the experiments. So many other thoracic diseases could be examined, e.g., 14 diseases in the NIH-datasets?\n- I am not quite sure why only a subset of the cardiomegaly dataset is utilized in the experiments. The described out-of-memory problem does not make sense to me. \n- I am seriously concerned about the presented results in Fig.3. The output images for the normal cases are mostly identical to the original ones. On the contrary, the output for the diseased ones has much more significant changes in overall body shapes and structures in the COVID-19 dataset. It looks suspicious to me that a generation model could produce such a perfect reconstruction of the original images. I did not think even a dedicated generation model trained only on the normal cases could reach the same accuracy. \n- Additionally, it would be extremely helpful if the generated mask M could be illustrated in Fig.3 as well. Therefore, I can better understand whether the generation model is so good or the mask M generation contributes to the generation??\n- The testing cases are much fewer than the training ones, e.g., 200 vs. 15,000? I also missed the split for the validation set.\n- The author claims that the mix of normal and disease data could help train the anomaly detector. However, I expect more inside about the setting can be revealed via experiments. For example, who the ratio of normal and disease cases in the mixed dataset will be optimal? How the ratio between the number of data in the normal dataset and the mixed ones? Which losses indeed help the learning of diseased cases in comparison to using normal cases alone? ",
            "summary_of_the_review": "The authors try to deal with an interesting problem by training an anomaly detector using a normal dataset and a mixed (normal/diseased, without labels) one. However, some parts of the proposed method are not clearly introduced. Furthermore, the experimental results are questionable, especially those generated output of normal cases (almost identical to the input), while the output for the diseased ones looks much more reasonable. Essential results (generated mask M) are missing. It is hard to see whether the proposed method truly works based on the currently presented results. Additionally, the experiments and discussion are lack depth in justifying why the proposed method works in these data types. Therefore, I am not able to recommend it for publication in its current shape.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper is about anomaly detection using generative adversarial networks. Different from previously popular methods that use only normal images for training, the proposed method (A2B-GAN) uses a set of normal images and another set of images containing both normal images and abnormal images (but the image labels of the second set are assumed unknown).\n\nTo achieve anomaly detection, the authors of this paper propose a one-directional unpaired image-to-image (I2I) translation approach using a mixed set of normal images and unannotated anomalous (abnormal) images. Ideally, an input image to the generator network is decomposed into a potentially normal image and a disease mask after translation and then combined to generate final output. The main motivation behind is that the difference between the generated output of an anomalous image and its input will highlight the anomalous regions, which indicate the diseased regions in the input image, to localize the potential diseases/abnormalities.\n\nThe proposed method is validated on two medical imaging datasets and a simulated natural image (human face) dataset for anomaly detection. Several state-of-the-art methods and their results are compared.",
            "main_review": "Strengths:\n+ Instead of using only normal images for training, this method also utilizes an unannotated dataset that contains both normal and abnormal images. This setting is very practical in medical image triaging, where existing healthy images are publicly available and novel diseased patterns are unknown during the initial outbreak of disease (e.g., COVID-19 at the beginning of the pandemic).\n+ The proposed method is very simple and straightforward.\n+ The paper is easy to follow.\n\nWeaknesses:\n- The main contributions/novelties of this work lie in Eq. (1) and Eq. (2), where the input image is decomposed to a normal image and its potential disease mask, and constructed using these linear combinations. Apart from that, the design of loss functions and training of the framework are very similar to the previous GAN-based methods (e.g., Eq. (3), (4), (5) and (8)). Moreover, normal/disease decomposition using GANs has been studied previously. For example, Tang, Y., Tang, Y., Zhu, Y., Xiao, J. and Summers, R.M., 2021. A disentangled generative model for disease decomposition in chest x-rays via normal image synthesis. Medical Image Analysis, 67, p.101839.\n- The network architecture of the GANs used in the paper is missing. Is it an off-the-shelf GAN model (please provide a citation) used here? If not, what is the detailed architecture design?\n- The experimental designs are too loose: The difference map can indicate the abnormal regions, therefore, can be used to localize abnormal regions/diseases. However, only classification results are shown in the paper. It would be interesting to see how this method performs in terms of disease localization. Moreover, only AUCs are reported. It would be better to have other metrics at an operating point, such as sensitivity, specificity, F1, etc. which are widely used in medical imaging analysis.\n- The medical datasets in this study may not be appropriately used. Firstly, the COVID-19 dataset used here actually contains healthy, COVID-19, and non-COVID-19 pneumonia chest x-rays. The authors only pick healthy and COVID-19 images to validate their method to detect COVID-19 x-rays. However, this setting is not very practical in a clinical setting. If one would like to screen for COVID-19 (may have similar symptoms as pneumonia), a better setting is to classify COVID-19 vs. non-COVID-19 (healthy+pneumonia). Secondly, the use of the NIH Chest X-ray dataset is problematic. Actually, the \"no finding\" chest x-rays are not simply healthy: only part of the images are \"healthy\" chest x-rays, other images may have some abnormalities outside the 14 defined diseases/patterns. Please refer to the following GitHub repo to find a sublist of normal and abnormal chest x-rays: https://github.com/rsummers11/CADLab/tree/master/CXR-Binary-Classifier Another concern is about the cardiomegaly detection task. Cardiomegaly detection is relatively simple in chest x-rays for CAD. Supervised learning can easily achieve an AUC > 0.9. However, only around 0.6 for GAN-based anomaly detection methods, which is far away from good. This makes the reviewer suspect the feasibility of the proposed method. Finally, both of the medical datasets are chest x-rays. More medical image modalities (e.g., CT, ultrasound, MRI) should be evaluated.\n- Though the chest x-ray images in Fig. 3 are very low-resolution images, the reconstruction quality (outputs/generated images) is markedly different for positive and negative inputs. The normal regions of outputs of positives are also blurry, but they look clear for the negative counterpart. Can the author explain the reason? Do the negative samples come from subset A or subset B in the training? Or are they from the test set? Is the proposed method able to generate very realistic 256*256 chest x-ray images? Anyways, high-resolution generated images should be provided in the supplementary materials.\n- There is no validation set in every experiment. Typically, they should be used to tune the hyper-parameters.\n-In the caption of Table 1: \"PatchCore and PaDiM throw out-of-memory error for Cardiomegaly dataset on our 500GB machine\", what do the authors mean by a 500GB machine? RAM memory or GPU memory (probably not the latter)? It is not clear here.\n- The \"Average\" column in Table 1 is meaningless. These two datasets/experiments are not related. It is the fact that the performance on the Cardiomegaly dataset is inferior to comparing methods.",
            "summary_of_the_review": "In summary, the problem investigated in this paper is interesting. But the technical contribution is somewhat limited compared to existing work. Moreover, the paper lacks details about the network architecture designs. The experimental settings have some technical flaws. The experiments and results are not comprehensive. The authors are encouraged to address the above concerns from the reviewer.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}