{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper investigates the impact of the size of pre-training data and the number of classes in the pre-training data on the few-shot image classification performance. From the empirical analysis, the authors draw the following insights: (1) the few-shot performance with respect to the pre-training dataset size can be approximated by power laws for scenarios where the distribution of few-shot test data can be either same or different from the pre-training data distribution and (2) few-shot performance on new classes converge at a faster rate than standard classification performance on previously seen classes. The experiments are conducted 4-layered CNN, VGG11, ResNet18, DenseNet121 and EfficientNetB0. The datasets used are the 10 datasets from Meta-Dataset where either pre-training is done on one dataset while few-shot testing is done on another dataset or same dataset is divided into two splits containing 80% and 20% classes and training is done on the 80% split and testing on the 20% split. The few-shot testing is performed using 5-way 1-shot and 5-way 5-shot protocol with multiple episodes where classes are randomly chosen from the test set.\n\nThe experiments suggest that in general the test performance follows a power law curve with respect to both size of training data and number of training classes. From the figures, it seems that the scaling law fits well when trained on ImageNet or COCO but there are exceptions to the rule when trained on some other dataset. When trained on imagenet, the authors further show that for regular classification where the test set has the same classes as the training set, the change in performance is larger as the size of training data increases. But for few-shot performance, the change is performance is much less as the size of training data grows.",
            "main_review": "Strengths\n\n1. The paper provides analysis on multiple image classification models and multiple datasets. \n2. The paper is also thorough in training and testing on multiple dataset pairs to highlight the impact on the test performance given different training datasets.\n3. There are some interesting insights drawn by the paper on how the nature of training dataset impacts the few-shot performance on the downstream test dataset. For eg, when training dataset is one of natural images, the few-shot performance on other natural or non-natural image datasets seem to improve. But when training dataset has non-natural images such as Omniglot data, the test dataset performance does not seem to improve on average.\n\nWeakness\n\n1. One of my primary concerns with the paper is the size of the models chosen for experiments. The largest model used for experiments is ResNet18 which has less than 12M parameters. Current state-of-the-architectures are much larger in size and usually have more than at least 50M parameters. Any observation in the pattern of the performance with respect to the training data can change significantly if the size of the model changes. Since the work attempts to address scaling laws on the performance of few-shot image classification, I would suggest the authors to perform the analysis on model architectures that vary in size on the log scale. For eg. experimenting on the family of EfficientNet models from B0 to B7 or a more recent model architecture family such as CvT: Introducing Convolutions to Vision Transformers (Wu et al., ICCV 2021). As highlighted by the authors in the abstract, it will be ideal to show analysis on some of the recent transformer-based architectures for its usefulness and relevance to the community that is moving quite swiftly to transformers that are both better performing, much more amenable to scale and allows better unification with other modalities.\n2. My other concern is with the datasets chosen for analysis. Apart from ImageNet and COCO, the other datasets are very small in size and not very practical when it comes to training. Considering the large-scale nature of pretraining that is performed these days, Imagenet-1k is probably the only dataset that falls in that scale. If we consider the models mentioned by the authors in the abstract such as CLIP and DALL-E, these are trained on a dataset with about 400M images (400 times the size of ImageNet). Compared to these, my concern is the practicality and usefulness of the analysis to derive scaling layers by training the model on datasets like Flower and Bird that are order of magnitude smaller than the current dataset used for pre-training. I would suggest the authors to pick additional datasets like ImageNet-22k and TinyImageNet-200 to further improve the analysis.\n3. I find the few-shot protocol of having just 5 classes per episode to be quite restricted and does not convey much about how the scaling laws will behave in a more real-world scenario where the number of classes can be arbitrary. In order to derive more robust and generalizable scaling laws for few-shot scenario, I would suggest authors to show an analysis with varying number of classes where the number of classes can be as few as 1 and as many as the total number of classes in the test dataset. Moreover, it would also help to see how the performance trends behave when multiple datasets are combined together to further increase the number of classes. Another interesting evaluation can be to merge all natural datasets together and similarly, all non-natural datasets and evaluate how the few-shot performance on this varies with size of training data and training classes. In such experiments, again it would be good to have varying number of classes and not just 5.\n4. Although the authors make the observation that few-shot performance converges faster than standard test classification performance, it seems that there is little to no justification on why this might be happening. Having some more insights on this will help improve the work and make it more useful to the community. I believe that the reason for this can be simply because as the network observes a larger part of the data distribution, it’s performance on standard test classification (which belongs to the same distribution) improves significantly. However, with few-shot scenario, given the number of classes is restricted to 5, the number of samples are at most 5 and the classes lie mostly out of distribution, there is nothing significant to learn from observing more data from the same training distribution. However, this changes when scaling is analyzed with increasing number of training classes. Adding more training classes expands the data distribution captured by the network and allows the network to learn concepts. This in turn opens an opportunity for the network to improve on the few-shot episodes because what might have been out-of-distribution with less number of training classes can now be in-distribution when more training classes are part of the training set. This is a common phenomenon that occurs with supervised learning and if this is the only observation that can be drawn from the analysis, then I am also concerned about the overall contribution of the paper. \n5. It seems that the Appendix section A.1 is missing from the paper. Moreover, some other details about the experiment setup are unclear. The paper says that the few-shot testing is done over thousands of trials. It would be more useful to have a precise number of the trials conducted for others to reproduce the setup. Further, there seems to be little information about how the splits for the training data and training classes are performed. Are experiments performed by randomly splitting the training set multiple times and reporting the average of the experiments? For eg, when a ratio of 6.25% of the classes are used, is this randomly sampled? If so how many times is the random sampling performed?\n6. The observations made in the few last paragraphs of Section 3 seems more work both in terms of writing and elaborating on them. Many of the observations seems conjectured and inconclusive to read. ",
            "summary_of_the_review": "I would like to provide a recommendation of ‘Reject’ to the paper. Given the paper is addressing scaling laws, my concern is that the model architectures and datasets chosen for analysis may not be representative of the current state-of-the-art architectures for image classification. Therefore, I am concerned about the generalization and usefulness of the analysis in practical scenarios. Moreover, the few-shot protocol used for testing is also very restricted and is not representative of the real-world few-shot scenarios where the number of classes with few-shots can be arbitrary in number. Next, I feel that the observations and justifications provided in association to the experiments needs more work and description. Some of the observations follow the general trends observed when transfer learning to a new dataset and do not seem to contribute significantly in terms of novelty. Lastly, the overall quality of the paper needs improvement by providing more details of the experiment setup, making the experiments more thorough and improving the readability of some sections (eg. Section 3). I would love to reconsider my recommendation if the authors can address my concerns raised above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Authors investigate the impact of training dataset size on downstream few-shot classification performance. They show that larger training datasets benefit out-of-distribution performance in general, which is intuitive, but also that this relationship holds even when domains are very distinct, and we would not expect good generalization, which is not. This relationship is clearer for data scaling than it is for class scaling, in that class scaling appears to saturate more quickly. ",
            "main_review": "STRENGTHS:\n\n- Good motivation – we don’t actually know whether out-of-distribution performance scales with training data, or in what situations, and this knowledge would be a valuable contribution. \n- Detailed prior work review\n- Well-written and organized\n- Large, exhaustive body of experiments combining different datasets, experiments, evaluation settings, and neural architectures. \n\nWEAKNESSES:\n\n- Training details promised in Appendix A.1 are not provided – please be sure to add this. Because these details are not provided, I do not know whether or not training is truly comparable across differently scaled settings. When the dataset is half as large, does the model train for twice as long? If not, then is the observed scaling behavior simply an issue of underfitting in small-scaled settings?\n- The fine-tuning method uses only five SGD steps at test time for adaptation. Is this enough? Can authors confirm that further steps have no impact on performance?\n- No discussion of episodic vs non-episodic training. In fact it appears that all methods are trained non-episodically, which for protonets and matching nets is highly non-standard, and likely also explains why Conv-4 does not respond to scaling the way that better-fitted architectures do – it is not sufficiently fitted to the few-shot evaluation task in any setting regardless of data availability. For these results to be relevant, a convincing ablation is required showing that the non-episodically trained proto/matching nets behave similarly to their episodically trained (or fine-tuned) equivalents in a variety of settings, up to constant or affine differences in accuracy. Alternatively, if authors wish to stand by their argument in Sec.5 that these results stand as an asymptotic proxy for shot-free scaling behavior, then this motivation should be moved into the problem setup and stated clearly up front. \n- I question the conclusion that few-shot performance saturates more quickly than supervised classification performance with respect to scale. While Fig.2 speaks for itself, the two tasks are not exactly comparable in terms of raw accuracy scores – one is a five-way problem while the other is 20-way, and so chance performance differs greatly. In this sense it is not surprising that the 20-way task requires more data to solve effectively. A more rigorous comparison might be to evaluate 20-way/1-shot accuracy instead of 5-way/1-shot currently, or to at least plot percent error reduction instead of raw accuracies. \n- Text appears to reference results that are not in the paper or appendix – I could not find class scaling results for training on omniglot, and could not verify the claim that traffic signs performance generally doesn’t improve with data scaling (trendlines appear generally weakly positive). Similarly, the claim at the end of section 3 that smaller and more diverse datasets benefit more from class scaling in general has no provided data to support it. More broadly, claims would be greatly bolstered by a set of tabular results showing the calculated (and presumably consistently positive) scaling coefficients across datasets and settings, even if just in the appendix. \n- Paper claims that scaling follows power laws, but does no analysis showing this is the case, instead simply fitting power curves to existing results. How does goodness of fit for these curves compare to, say, a simple best-fit line mapping log(datasize) to scores? \n\nSMALL COMMENTS/TYPOS/GRAMMAR\n\n- Pg 1 pgraph 1 line 5: both the input and the task, class distributions -> both the input and the task class distributions (comma should be removed). \n- In prior work, the paper “Do Better ImageNet Models Transfer Better?” (Kornblith, Shlens, and Le) may be worth a mention, as a well-known investigation of transfer task scaling with respect to model capacity and training performance. ",
            "summary_of_the_review": "Paper presents a well-motivated problem and conducts an exhaustive empirical study with interesting results. However, not all of the relevant results appear to have made it into the paper (or appendix), claims are not fully supported by the data that is provided, and many crucial analyses are missing. This also makes the degree of rigor behind the experiments difficult to determine. As such the paper is not yet ready for publication. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies the effect of scaling data both in terms of the total number of training data and the number of classes. The focus of the paper is on the few-shot transfer performance of the models to out-of-distribution tasks. The paper investigates various convolution based architectures on datasets from the Meta-Datasets benchmark. \n\nThe main findings are:\n1. The scaling behaviour of the few-shot performance of the models with respect to the size of training data can be explained by power-law for both in and out of distribution domains. \n2.Few-shot performance on new classes converges at a faster rate than the standard classification performance on previously seen classes.\n3.The observed behaviour is similar across the three different few-shot learning techniques explored in the paper (fine-tuning (linear eval), matching networks, prototypical networks).",
            "main_review": "### Strengths:\n- Important problem to study: Effect of scaling data not just with respect to the total number of training examples but also in terms of number of classes which can be seen as an aspect of data diversity.\n- It’s nice that when looking into the effect of the total number of training examples, they control for the number of classes.\n- The paper studies various datasets as training sets, rather than studying the scaling behaviour of only one dataset.\n\n### Weaknesses:\n- The writing of the paper can be improved a bit.\n- The novelty and contribution of the paper in the context of the prior work are not clear (at least not well discussed).\n- The authors fit a power-law equation to their data and conclude that power-law can explain the scaling behaviour, but there is no evidence in the paper to show why power law is a good fit (it’s like applying a classifier on a dataset and showing the value of its fitted parameters. That doesn’t mean that it is necessarily a good fit).\n- The power-law equations used in the paper (equations 1 and 2) seem to be inspired by earlier works such as [1] but the prior work is not referenced in this section.\n- I think, considering the experimental setup, the findings of the paper could be limited (not necessarily hold in all cases), but these limitations are not discussed in the paper.\n\n### Suggestions:\n- About the writing of the paper: I think it makes more sense for the introduction of this paper to focus on concepts and prior works relevant to scaling the size of training data. I find the current intro a bit uninformative. Throughout the paper, the authors state that the goal of this paper is to understand the effect of scaling on out-of-distribution generalization. I think it helps to be more specific and mention explicitly that the paper studies the impact of scaling data, and emphasize that scaling data is studied both in terms of the number of training examples and the number of classes. In the related work section, it is nice that you summarize the findings of some of the prior works that study the scaling phenomena, but what is lacking is how all these prior work are related to each other and to this paper, and what the findings in this paper add on top of what has already been studied in the literature. \n- There seems to be a missing discussion to explain the difference between the reducible and irreducible error rate terms in the formula.  \n- It would be nice to report the size of the models in the paper (in terms of the total number of parameters, depth and width).\n- To truly understand the effect of each scaling axis such as dataset size, it is important to run experiments with a diverse set of hyperparameters, as the optimum hyper-parameters for different data sizes might be different, hence your plots might not be capturing the best performance of the models for cases where a smaller portion of training data is used. If authors can argue that changing hyperparameters, here, would not affect the shape of the plots and the scaling behaviour, this is something that is worthy of being discussed in the paper.\n- As you see in your results, with CNN, the scaling behaviour is not independent of the model size. I think this is probably true for all different axes of scaling (e.g., compute, param-size, data size). It would be interesting if you can study or at least discuss how in different regions (based on other scaling variables), the scaling behaviour with respect to dataset size changes. E.g., if the ratio of model size to dataset size could matter. In a parallel work to yours, [2] shows that for different regions (based on the total number of parameters), different scaling strategies (with respect to network architecture) work better. I can imagine that, here, we could see different behaviours with respect to data size too.\n- I’d suggest not referring to ``your fine-tuning’’ setup as fine-tuning. I think often fine-tuning refers to complete or partial updating of the weights of the parameters of the network, whereas you are simply attaching a new head, e.g., training a linear classifier on top of the representations obtained from the model. I think this is often referred to as linear eval (assuming the head is linear). \n\n### Questions for the authors:\n- I wonder how the scaling plots with respect to the total number of training examples changes, for different numbers of classes.\n- Do you tune hyperparameters for each dataset size separately?\n- Can you clarify how your experiments and results suggest that ``while features in the natural domain can be useful for non-natural domains, the inverse is not’’, as you state this on page 6? I can see how increasing training size in different domains can have different implications, but I am not sure there is enough evidence for distinguishing between natural and not-natural datasets (I am also not sure if this terminology makes sense).\n\n### Typos:\nthe first paragraph of page 6: more more → more?\n\n[1] Henighan, Tom, et al. \"Scaling laws for autoregressive generative modeling.\" arXiv preprint arXiv:2010.14701 (2020).\n\n[2] Tay, Yi, et al. \"Scale efficiently: Insights from pre-training and fine-tuning transformers.\" arXiv preprint arXiv:2109.10686 (2021).\n",
            "summary_of_the_review": "I think the paper provides empirical evidence for common intuitions about how scaling dataset size affect the few-shot performance of the models, and even though the experimental setup is limited, it convincingly takes a few important aspects into account. However, the writing of the paper is a bit sloppy and the relation to prior work is not well explained. Additionally, the part where the paper claims to show power-law can explain the scaling behaviour with respect to dataset size, seems methodologically incomplete. Hence, for now, I am leaning toward rejection. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors study the task of few shot adaptation in image classification and experiment with adapting to unseen target datasets and unseen classes. They show results on several datasets and for different model architectures.",
            "main_review": "### General comments:\n„Scaling on real world vision tasks can potentially improve generalization on other real world vision problems.” I do not understand this sentence, please rephrase.\n\n“Because of its widespread use, vision is a very interesting case study. It’s a simple, yet very important problem and a very good example of real world application.” -> How is computer vision simple? It has been studied for over 50 years and we still do not have a system that works reliably, is robust and fair, and generalizes well.\n\n“Neural scaling laws research emerged quite recently, triggered by successes in large-scale pretrained models, as it aims to better understand promises and limitations of scaling.” Citation missing.\n\n“The importance of approaching learning methods from an empirical science perspective and discovering scaling laws with respect to various factors, including, but not limited to, the data and model size, is being more and more widely recognized in the deep learning community.“ Citation missing \n\n“The ten datasets from Meta-Dataset (Triantafillou et al., 2019) are used in our study: Aircraft, Bird,\nCOCO, Describable Texture, Flower, Fungi, ImageNet, Omniglot, Quickdraw and Traffic Sign. “ All datasets should be cited and explained briefly.\n\n“Any additional detail is provided in the Appendix section A.1.” There is nothing in Appendix A.1.\n\n“Common metric choices are euclidean distance and cosine similarity” Citation missing\n\nFigure 1: “The one-shot performance scales with the training set size following simple power laws.” The blue curve is essentially flat which also follows a power law, I suppose, with k=1 and alpha=0, thus showing that the dataset size actually does not matter for the CNN model. This might be due to low capacity. Model capacity plays an important part here and should be part of the equation: My model needs to have sufficient capacity to actually benefit from more data. \n\n“This is important, as it will be much more costly to improve the few-shot performance via scaling the training set size than the standard classification performance.”  -> Why is that? In both cases, one needs to pretrain the model with a large dataset.\n\n“On average, when the training dataset is one of natural images (Aircraft, Bird, COCO, Describable\nTexture, Flower, Fungi and ImageNet), the few-shot performance on other natural image datasets\ndoes seem to improve, see Figure 3. We find this very interesting, as it is not so obvious if training on\nmore fungi for example can make a model better at classifying aircrafts. This suggest that there are\nfeatures even in specific natural domain which generalize well to all sorts of other natural domains.” This is not surprising. The same behavior is shown in Figure 3 of Djolonga et al.\n\n“Finally, the only training dataset where on average scaling does not seem to improve the few-shot\nperformance is the Traffic Sign dataset. […] See section A.2 for results.” In Appendix A.2., Fig.6+9, the results on the Traffic Sign dataset are missing. In Figs.8 and 11, the performance on the Traffic Sign dataset does go up with increasing dataset size.\n\n“On average, see Figure 1 and 2, when scaling the amount of training classes, few-shot performance\nseems to improve at lower percentages, but then plateaus or start to plateau at some point, the only\nexception being the CNN.” -> I am confused. In the previous section, Figure 1 and 2 were referenced and there it was stated that “The amount of training classes remains the same no matter the ratio.” Was the previous statement wrong or is this one wrong now? Does the number of classes change in Figures 1 and 2 or not?\n\n“For ImageNet though, more classes does seem to help, but the gains seem to level off after more than 200 classes. See Figure 5.” I do not see this in Figure 5: There, the maximum number of classes is 100 and not 200.\n\nTable 1 is not referenced in the text.\n\nDo Figures 3,4,5 show fine-tuning/ Matching Network or Prototypical network performance? In general, the differences in the results between these approaches are not discussed in text.\n\n\n",
            "summary_of_the_review": "There are several confusing parts in the paper, making it hard to understand the results. In many parts, claims are not supported by citations. Some parts of the results actually contradict each other which is confusing. Some claims are not supported by the presented evidence.\n\nI did not find the results surprising and I do not see the novelty in this paper. It is well known that higher model capacity and more training data result in better generalization performance, see e.g., Djolonga et al but there are many publications showing this. Having more training data results in better and more general representations that are suitable for all kinds of tasks, provided that the model has enough capacity to fit the additional data. For example, in [1], the authors perform a large scale study looking at many different models and datasets, and show a strong correlation between model performance on ImageNet and OOD performance on various datasets. They also study the effect of dataset size on OOD robustness and find that dataset size directly influences the in-distribution performance and thereby also OOD robustness. The authors should discuss how their work differs from and adds to ref. [1]. Here, the authors study few-shot adaptation instead of directly evaluating on the target dataset, but they freeze all model layers that are responsible for feature extraction, so the setting is in fact very similar.\n\nConsidering the power-law fits: Given that having more training data results in better representations and thus, better OOD generalization, it is further not surprising that one can fit a function to this relationship. Since this function hugely depends on the dataset and the used architecture and is thus not general, it would still need to be tuned to each task separately. Thus, knowing that “such function exists” is not useful in practice. \n\nThe related work section reads like a list of strongly or vaguely related papers. The current work should be put in context much better since it is currently unclear which benefits this paper has over the current state of research in this area.\n\nReferences:\n[1] Taori et al, “Measuring Robustness to Natural Distribution Shifts in Image Classification”\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}