{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper collects a new dataset of the format (passage, critique/review) and trained a RoBERTa based encoding model in the style of CLIP. Authors did some human evaluation to show that CARP does better than other baseline models.",
            "main_review": "This paper, though needs a lot of work, proposes a dataset that could have the potential of offering more diverse feedback/evaluation metrics on story generation. My main problems with this paper are the following:\n\n1. The paper is very poorly structured. It wasn't until the very last few pages, I start to understand what CARP is used for. If the goal is to conduct better story evaluation / feedback on machine generated stories, the authors should mention this earlier instead of leaving this as a mental exercise for the reviewers to find out. And please correct me if I'm wrong, the end goal of CARP is to automatically tag each story with one of these:\n```\nA) This kind of drags on.\nB) This is a bit too short.\nC) This is too cheery.\nD) This is really depressing.\nE) This is really exciting.\nF) This is boring.\nG) This ending leaves things too open.\nH) This ending feels abrupt.\nI) Could use more visual imagery.\n```\n2. If (1) is true, that the goal of CARP is to annotate each story with a listed tag, therefore it provides better story evaluation than a ROGUE score or relying on expensive human evaluation -- then CARP must defend why these tags are particularly interesting or important. There are 8 negative critiques and 1 positive critique -- the design of this feedback space is not well-justified. \n3. If (1) is not true -- that the author just wants to use CARP as an embedding model to better embed/encode stories into vectors, then the title of this paper is not true: \"Fishing for zero-shot story evaluation\" -- I fail to see how better embeddings of stories in themselves can serve as tools for story evaluation.\n4. If (1) is half true, meaning that CARP is used as a retrieval model, but can retrieve reviews/critiques beyond the original 9 tags, then the authors need to show exactly how these retrieve reviews/critiques in their free-text format can provide better evaluations for existing story generation models. Free text is difficult to use as evaluation metrics -- for example, how can we say model A is better than model B using what CARP provides? Or is this a free exercise left for the users of CARP to specify? The paper fails to discuss this properly.\n5. Though authors mentioned some criticism of ROGUE and Fabula-Entropy Index, they were never compared with CARP story annotations. The paper never supports its claim that CARP is a better measure for story coherence than alternative metrics. \n6. The dataset seems to be private -- which is understandable, but if a paper's entire novelty and contribution to the NLP community hinges on whether the dataset will be published (and there's no guarantee of that), it puts reviewers at a very risky position. I do believe this dataset has value, especially the format of passage-level annotation is incredibly valuable for NLP researchers. However, this conference is a poor fit and we cannot judge the quality of the dataset in a double-blind review. I encourage the authors to check out NeurIPS dataset track (https://neurips.cc/Conferences/2021/CallForDatasetsBenchmarks) and submit this paper there.\n7. I like the analysis, especially in contrast to another dataset (Writing Prompts) -- it's hard to know the quality of a dataset in and of itself, but when you compare to another dataset, it becomes much clearer.\n8. A minor problem: the images are not very clear -- a tip for the authors, you can embed images as PDF in LaTex. Please do not use \"screenshot\" images in your paper.\n\nI strongly encourage the authors to participate in programs like https://blog.iclr.cc/2021/08/10/broadening-our-call-for-participation-to-iclr-2022/ -- these research programs pair you up with an experienced mentor that can correct many of the major or minor problems that I pointed out. I believe the dataset you guys collected is a very cool and novel dataset that can potentially benefit the NLP field -- the data cleaning, preprocessing seem to be well-done. However, due to many of the problems, this feels like an engineering report, not a research paper that meets the bar of this conference.",
            "summary_of_the_review": "The paper fails to construct a coherent narrative that demonstrates CARP can provide a significantly better story evaluation method compared to other evaluation methods. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper introduces the Story-Critique dataset, which is a cleaned, filtered, and anonymized story dataset with critiques corresponding to each story.  This dataset could be quite useful in training text evaluation systems as the critiques are more robust and interpretable than the typical NLG evaluation metrics.  They also train a story evaluation system (CARP) on this data using contrastive learning which they use in a quantitative study comparing with human annotators.\n\nContributions:  This paper cleans and releases a new dataset of stories with corresponding critiques of the writing.  They provide baseline performance using a contrastive learning based model similar to the one used recently in CLIP.",
            "main_review": "Strengths:\n- Novel data: This paper contributes a new dataset consisting of stories with labelled critiques of the writing quality.  I can imagine that the Story-Critique dataset could be useful in facilitating a variety of research directions in NLG.\n- They train a model using contrastive learning to achieve benchmark performance on the new dataset\n\nLimitations:\n- Authors should revise their descriptions of the contributions in section 1.3: \n  - In the first bullet point: they describe their work as the “first automated model for story evaluation”.  This is not true -- there are other automatic story evaluation models (Gu et al. 2020; Guan et al. 2020; Sagarkar et al. 2018).\n  - In the second bullet in section 1.3, one of their contributions is that they “​​show that contrastive learning is an effective tool for learning”.  This is not a novel contribution because contrastive learning has already been shown to be useful for a myriad of tasks. Instead the authors should be more specific about the fact that they are showing that contrastive learning is useful in this specific story evaluation task.\n- Missing related work and comparisons: This paper includes a brief discussion of related work in the introduction and Section 1.1 but is still missing related work in learned metrics for story evaluation such as (Gu et al. 2020; Guan et al. 2020; Sagarkar et al. 2018).  They also don’t make any empirical comparisons to any of the standard evaluation metrics used on story generation.  The explanation given for this is that the categorization scheme used by annotators here wouldn’t align with what is used by other metrics.  But I think that there are still ways to perform comparative experiments.  For example, it may be possible to aggregate the nine categories used here into binary “good”/”bad” labels to facilitate comparison.  This would be quite helpful to add to the paper. \n- Missing descriptions of the Story-Critique data:  Who wrote the stories? (were they students for classroom purposes? Internet users for entertainment purposes?) Who wrote the critiques? (were they educators? Students? miscellaneous internet users?)  Can you provide examples of stories and their critiques?\n- More details needed about the set of nine categories introduced in Sec. 4.2:  It’s unclear how these categories were chosen and they don’t seem to be exhaustive to all scenarios.  It’s unclear to me how this system would categorize “positive” writing examples (that is, well-written stories).  It seems the only positive label is “this is really exciting.” But that doesn’t cover all well-written stories — some are not intended to be exciting. I am hoping authors will provide more details on how these categories were chosen.  Was this based on what was the natural clustering of categories in the Story-Critique data?\n- The main results (Fig 4) are computed over very few data points: Authors should please correct me if I’ve misunderstood something about section 4 or 5.  My understanding of those sections is: first, they asked humans to rate 7 stories. Then, they aggregated the human results per story to create a distribution over categories for each story. Finally, they compared the human distribution to the model output for each of the seven stories.  If all of this is correct, then the results (in Figure 4) were computed using only 7 data points per model.  I am skeptical that these results will generalize more broadly because it’s likely that these 7 stories do not cover a very diverse set of writing types.\n- Lack of experiments with machine-written text:  One of the key motivations of the model is to use it to evaluate machine-written text, but all of the experiments use human-written stories. So, it’s unclear how the CARP model would actually behave in a realistic evaluation set-up with actual model output.  Authors should add experiments with using CARP on output from NLG systems.\n- Limited technical novelty: While this paper offers a novel dataset, the machine learning techniques themselves are mostly re-applications of pre-existing contrastive learning methods.  More specifically, they take a lot of inspiration from the CLIP paper and use methods from that paper with only slight modifications so that it uses two text encoders.\n\nMinor fixes and typos:\n- On page 1: “easily Goodhearted” - I’m not sure what this was supposed to mean in this context.  Was it a typo?\n- Typo, page 2: “its critical reviews.This “ - There’s a missing space between sentences.\n- I’m a bit confused about why (Fleischman & Roy, 2005) was cited in the sentence about using contrastive learning.  Did you mean to cite (Chopra et al. 2005) - the original contrastive loss paper - instead?\n- Section 5: please add citations for Storium and Scarecrow where they are mentioned.\n\nOther questions for the authors:\n- My understanding is that each human annotator rated 3 of the 7 stories.  But how many annotations are there per story?\n- As far as I can tell, the only story examples included in the appendix are the stories on page 15, which are not from the Story-Critique data itself, but rather the set of stories collected by the authors’ colleagues in 4.3. Can you also show examples of the Story-Critique stories and gold critiques?  \n- What is the overall distribution of human responses across the 9 categories (Sec. 4.2, 4.3)?  Are there any categories that don’t get selected (or only get selected infrequently)?\n\nReferences:\n- Chopra et al. 2005 - Learning a Similarity Metric Discriminatively, with Application to Face Verification\n- Gu et al. 2020 - Perception Score, A Learned Metric for Open-ended Text Generation Evaluation\n- Guan et al. 2020 - Union: An unreferenced metric for evaluating open-ended story generation\n- Sagarkar et al. 2018 - Quality Signals in Generated Stories",
            "summary_of_the_review": "This paper is about an interesting topic and introduces a potentially impactful dataset but there’s a lot that needs to be revised or added to the paper, such as: adding more details about the data and categorization systems, using a larger and more diverse set of stories in the human annotations, adding experiments using machine-written text, and adding quantitative comparisons to existing evaluation systems.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors describe a dataset, Story-Critique, of stories with critiques annotated at the passage level, together with a CLIP-like model (called CARP) which is trained to match story passages with critiques. The authors then use this model to automatically evaluate novel stories by assigning a predefined set of critiques to passages, and compared the automatically assigned critiques to a variety of baselines, including human-assigned critiques and fine-tuned large seq2seq models.",
            "main_review": "The authors propose two main contributions in this paper: a dataset and a model trained on the dataset.\n\nThe dataset, Story-Critique (SC), consists of stories with critiques annotated at the passage level. For data privacy reasons, the description of the dataset in the paper is desperately thin, with no examples of the data, nor even summary statistics of the data, such as story length or genre. This makes reviewing the dataset contribution very difficult, as it is difficult to judge the quality of the dataset from the lack of information we are given about it. The authors present some summary statistics of the critique portion of the dataset, noting that negative sentiment critiques are longer in SC compared to the publicly available Reddit WritingPrompts (WP) dataset, which the authors take to mean that the average negative sentiment critique in SC is more informative than positive SC critiques, and WP replies of any sentiment. Further, they use an off-the-shelf toxicity classifier to measure what proportion of responses were toxic or insulting, which the authors take as a proxy for non-constructive, and show that the critiques are less non-constructive in SC compared to WP. Again, this claim is difficult to assess, as no examples of toxic vs untoxic comments are given. These could even be from the WP dataset, to help assess what kind of language the off-the-shelf classifier finds toxic/insulting.\n\nThe model contribution the authors propose is a dual encoder model, with one branch encoding the story passage and the other encoding the critique. The representations of both encoders are trained using a contrastive objective similar to CLIP, where the representations of a passage and its corresponding critique are pushed together, while representations of passages and critiques which aren't aligned are pushed apart. The authors use a series pretrained RoBERTA checkpoints of varying sizes as starting points, and fine-tune the model on their SC dataset. The authors show (unsurprisingly) that larger models perform better at the matching task.\n\nThe authors also report the results of an experiment where they give the CARP model 9 pre-written critiques and ask the model to assign them to unseen stories. The authors compare the resulting critique distribution to a range of baselines, including fine-tuned generative language models, and show that CARP is able to replicate human assignments of those critiques to passages better than all competing baselines. In addition, larger versions of CARP are better able to replicate human judgments. Aside: the paper mentions that 'Normalization is performed by, for a given story, subtracting the minimum score from all other eight labels and then softmaxing'. This seems a bit unusual, considering the output of the softmax operator is invariant under linear shifts of the input -- could the authors clarify?\n\nI feel this experiment demonstrates both a strength and a failure of the CARP approach towards narrative evaluation: it is able to assign judgments to local passages well, but the space of possible judgments is limited by the prompts given by the user -- indeed, all of the prompts given are critical in nature, and it would be interesting to see whether CARP can also use positive prompts correctly. Further, the forced-choice nature of this experiment, with a limited set of available judgments, seems counter to the actual human criticism process, which generates open-ended critiques. In addition, the local nature of CARP judgments doesn't seem like it would help one of the known issues of long-text generation with language models, which is maintaining global coherence. As a thought experiment, which passage in a story would CARP assign a judgment about the narrative arc of the main character?",
            "summary_of_the_review": "Overall, I feel that this paper presents an interesting idea towards evaluating narratives, but I feel like I need to be convinced a bit more that the proposed model is useful. This can be done by using the model to answer outstanding questions in evaluation of long-text generation, or at least demonstrating more practical utility of the model beyond some proposed use-cases in the appendix.\n\nFurther, while I appreciate that there may be data privacy issues to do with releasing too many details about the SC dataset, as currently presented in the paper it is almost impossible to understand the content of the dataset, making it really hard for me to review it.\n\nFor these reasons, I recommend a weak reject. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new evaluation metric (CARP) and a new dataset (story-critique) for story generation. Particularly it has the following contributions:\n\n1. The authors proposed a new story generation evaluation metric CARP, that is based on a contrastive learning approach. Concretely, the authors train CARP end-to-end using story-critiques pair. Similarly to CLIP, the authors use a story encoder and a critique encoder, and CARP is trained to maximize the similarity between a story and its true critique (while minimizing the similarity with other critiques).  \n\n2. The authors proposed a dataset of (story, critique) pairs, which could be useful to the community (However, I wish more details could be provided on the construction of the dataset. Please refer to the weakness section)\n\n3. The authors provide further analysis of the proposed dataset, particularly zooming onto the issues such as length and toxicity,\n",
            "main_review": "Strengths:\n1. The use of contrastive learning approach to train CARP as a story evaluation metric is novel and interesting.\n\n2. The dataset is potentially very useful to the story generation community (esp the critiques aspects of the dataset)\n\n3. The authors are very aware of many ethical issues in creating this dataset (copyright, privacy, willingness from the content creators). I commend the authors for their effort in the process.\n\nWeakness:\n1. My biggest concern is that I think there should be more explanation on how the critiques are gathered in the story-critique dataset. Are they from story forum users / authors? What are the steps the authors have taken (besides analysing length/toxicity/sentiment) to make sure that these critiques are valid and relevant to the given stories? And how do the authors determine the context associated with in-line critiques? Maybe the authors could use some examples to make the data creation part clearer.\n\n2. I am rather confused by the evaluation section 4.3 and section 5. Are the human participants rating on story-critique match and whether human judgments correlate with CARP? The validity of these critiques to the given stories is very crucial for the usefulness of the dataset but I fail to see the authors' evaluation on that issue from section 4. That said, I'd be happy to hear back from the authors and increase my score if it could be clarified.",
            "summary_of_the_review": "Interesting evaluation and potentially useful resources but more validation and analysis on the dataset is needed. I recommend weak reject in its present form.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The authors made very conscious effort to protect privacy of the content creators (of the story-critique) dataset.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}