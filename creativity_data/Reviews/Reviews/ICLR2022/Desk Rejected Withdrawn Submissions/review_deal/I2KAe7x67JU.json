{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper tried to conduct a comprehensive study between different methods of GNNs with a focus on Dynamic GNN (DGNN) on existing benchmark datasets. The authors observed that simple link prediction heuristics often perform better than GNNs and DGNNs,  different sliding window sizes greatly affect performance, DGNNs consistently outperform static GNNs",
            "main_review": "I do not find any significant novel contribution in this work. I agree that doing a thorough comparison between models and developing understandings based on those experiments are super important in the field of science. However, I think dynamic GNNs are a specific narrow subdomain, thus the requirement of standardization and a framework for training could be a premature step.\nConcerns:\n1. The word benchmarking in the title gives an impression that authors are curating the datasets that are used for the model evaluation. However, those datasets were introduced in previous related papers. \n2. I am not totally convinced about the objective of creating a training framework. Are you going to publish a framework that will do a hyperparameter search for DGNN training and thus would be a standardized eval platform for future DGNN researchers? \n3.  Another interesting avenue could have been curating interesting and bigger size datasets for this task. (see https://ogb.stanford.edu/docs/home/ for reference).\n",
            "summary_of_the_review": "I recommend rejecting this paper since the contributions are not significant enough to get published.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors aim to evaluate the performance of current graph neural networks (static, discrete dynamic, and continuous dynamic GNNs) and compared them to heuristic baselines. They did several experiments on 6 public datasets, and performed hyper parameters tuning on each of the model. The authors summarized their finding that simple link prediction heuristics often perform better than GNNs, and that hyper parameters including sliding window sizes greatly affect performance. They also concluded that dynamic GNNs consistently outperform static GNNs.",
            "main_review": "The authors has raised an important question that we need a comprehensive assessment of the many different graph neural networks together with the heuristic baselines. It is not straight forward how to do this in a complete, fair and informative way. While the authors of this paper has made many efforts to shed lights upon this questions, I have a few questions here:\n\nThe writing of the current paper needs to be improved. For example, in section 2.1, the author tries to define temporal edges by the nodes they connect and the start and end time, etc. The definition here does not seem to be used later in the paper. In these cases, it is confusing what the authors want to convey through the definitions.\nAlso, after reading, I got the impression that the author discussed too much about the dataset imbalance and why PR-curve is a good metric with dataset imbalance, while this is a fairly well-known concept. The heavy discussion on dataset imbalance distracts the discussion on the purpose of the paper, which is their findings from the comparisons. \n\nSecond, many published work has done extensive comparisons. One of the main contribution of this work is that they proposed a framework to compare discrete DGNNs to continuous DGNNs. Currently the description of how this is done is not clear and a bit confusing in the main text. I suggest moving some of the text of A.4.3 to make it more clear. Comparing continuous DGNNs to discrete DGNNs is not a problem with straight forward solution. It is also not clear to me the current proposal makes a fair comparison, as the continuous DGNNs generate node embeddings that perform well on continuous setting. They can be understandably not performing as well on predict discrete graph snapshots even thought the decoder is retrained on discrete graphs, unlike the discrete DGNN models which are trained to work on discrete graph settings. Therefore, it might not be surprising to see TGN and TGAT not performing well compared to other GNNs under this setting.\n\nIn addition, many kinds of discrete DGNNs have been developed by now. The authors have evaluated EGCN and GC-LSTM and made conclusions based on that. Given the variety of DGNNs been developed, it would be better to include more different types of DGNNs in the comparison to draw solid conclusions. \nThe authors also mentioned that link prediction heuristics are better at ranking the links initially, and suggested incorporating heuristics into GNNs. It would be good to include some suggested designs/trials on this direction and see if the performance increases. One example given in the paper is the model by Sankar et al (2020). However, Sankar et al developed a discrete dynamic model with GAT+attention. I'm not sure which part contributes to \"incorporating heuristics into GNNs\"?\n\nMinor typos:\nSection 2.3, the first sentence, \"...networks. Where there is..\"\n",
            "summary_of_the_review": "The authors performed many comparisons across three different types of GNNs in addition to link prediction heuristics. While the comparisons generate some insights, the way comparisons have been done might need to be more thoughtful. It would also be good to include more representative models in the comparison to draw more solid conclusions. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors proposed a framework to make a fair and comprehensive comparison for discrete and continuous Dynamic Graph Neural networks (DGNN) on the dynamic link prediction task. Results are interesting as the heuristic methods can outperform more complicated and advanced DGNNs.",
            "main_review": "Strength:\n1. The paper is overall clearly written and easy to follow.\n2. The paper provided some very interesting insights into dynamic link prediction and listed out some interesting future directions. \nWeakness/concerns:\n1.\tGNN is known to be capable of aggregating node features to inference. However, for all the datasets, node features are initialized randomly or by node degree. The conclusion is thus limited to graphs without rich node features.  \n2.\tWhy do authors not use negative samples (with an equal number to the positive ones) when they are evaluating on valid and test sets? If negative samples are used for AUC, the problem occurred for false positive rate can be alleviated. Besides, it would be more intuitive to apply the hits@k metric if we wanted to know whether positive samples were ranked higher than negative samples.\n3.\tAll the datasets used in this work are quite small, even the largest one only has 11k nodes.  \n",
            "summary_of_the_review": "The idea of providing a fair and comprehensive comparison for the dynamic link prediction task in this work is interesting and meaningful but the experiments still deserve further completion to support the authors’ conclusion. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper benchmarks the performance of serval models for dynamic link prediction tasks.\nWhile the findings of the paper may be interesting, I don't this paper presents enough technical contributions for being considered as an ICLR paper.",
            "main_review": "While the findings of the paper may be interesting, I don't this paper presents enough technical contributions for being considered as an ICLR paper. The authors should consider proposing technical methods based on the insights learned from this benchmark.\nMy evaluation is especially true due to the existence of PyTorch Geometric temporal library, which makes such a benchmark especially easy to implement: https://github.com/benedekrozemberczki/pytorch_geometric_temporal\nI noticed that the authors use their customized implementation, but I don't think this justifies a technical contribution.",
            "summary_of_the_review": "Few technical contributions have been made.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}