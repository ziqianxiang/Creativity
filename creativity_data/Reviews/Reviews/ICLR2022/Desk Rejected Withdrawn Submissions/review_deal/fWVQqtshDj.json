{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a Multi-teacher model-based RL framework (MOBA) that trains an ensemble of students as follows: The environment is altered to create N instances, whose trajectories are used to train N teachers (state transition function and reward function). The teachers together train each student. Multiple students -- K (as shown in Alg. 2) or N (as shown in Fig.1)? -- are trained by teachers, with different initializations. The student models ensemble is then used to train the policy of the agent that will act in the environment.",
            "main_review": "Positive points of the paper: (i) Use of multiple models to increase generalizability and robustness of learned policy. (ii) Use of ensemble -- ensembles are known to increase the performance and robustness of systems and the authors took advantage of this. (iii) Several experiments with comparative analysis, in various benchmarks.\n\nNegative points of the paper: (i) The contribution seems to me to be incremental and quite costly; (ii) It seems to me a limitation the fact of having to generate different instances of the environment – How discrepant should these instances be? How to define them in more complex and real problems? (iii) In the comparative analyzes performed (with model-free RL and other model-based RL proposals), do the other algorithms use only a single environment? If so, comparisons with MOBA do not seem fair to me, as the latter uses several environments generated and trained in parallel. As there are several models being learned in parallel, the time and space (computation cost) of each MOBA iteration must be much higher than that of other algorithms. Because MOBA learns several models in parallel, perhaps comparing the learning curve as a function of the number of episodes is not very fair. The cost in time and memory should also be compared.\n\nMinor issues: There are some typos and phrases that should be reviewed. Ex. envrionment -> environment; last paragraph on page 4 (We call...); first paragraph of section 4.3 (WE -> we, def. of eta); Alg. 1: N is the number of instances and teachers (it is better to use another letter to designate the number of episodes); K is the number of students (it is N in Fig. 1); Make Alg. 2 clearer (especially its cycles); Review the policy index in step 2 of Alg.1; Section 5.1 (three standard benchmarks -> six); model-base -> model-based; etc.\n\n",
            "summary_of_the_review": "A paper that proposes an incremental contribution to the state-of-the-art model-based RL, but that makes several evaluations in experiments in six different environments of Pybullet, aiming to increase the robustness and generalization of the system.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "not applicable",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents MOBA, a model-based reinforcement learning approach, which exploits the notion of knowledge distillation to address the problem of model bias in model-based RL scenarios. MOBA utilizes multiple instances of an environment to learn teacher dynamic models, and then distills and transfers the teachers’ knowledge to a student, so that the student model can learn a generalized dynamic model that covers the entire space. The authors further employ an ensemble of student models to overcome instability of multi-teacher knowledge transfer. Experiments demonstrate significant improvement in terms of data efficiency as well as performance compared to model-free RL methods.",
            "main_review": "Strengths: \n+ The empirical outcome of the paper is significant as the proposed method outperforms the state-of-the-art model-free algorithms, while being more data efficient compared to those methods.\n+ The authors conduct extensive experimental analysis including comparison with several state-of-the-art model-free and model-based algorithms.\n\nWeaknesses: \n- The contributions of the paper are straightforward and incremental, as they combine previously established ideas without any major modifications.\n- The main ideas of the paper can be better clarified. Some parts of the paper are elaborated in detail (which was not very necessary), while some others are not addressed at all.\n- The writeup needs a revision as there are multiple instances of grammatical and notational errors in the writing. \n- There are not enough details on the implementation of the approach to reproduce the results.\n\nComments: \n- MOBA combines previously established ideas in model-based reinforcement learning and knowledge distillation to improve the performance and data efficiency. Therefore, the technical novelty of the paper is limited. \n- For a paper with empirical contributions, the experiments are particularly limited to the simulations, while it would be interesting to evaluate the performance in real-world applications, and how multiple different instances of the environment can be obtained in such scenarios. Besides, the implementation details are not provided, hence, the results are not reproducible for validation and confirmation. \n- I would suggest to elaborate upon some parts of the main ideas in more detail. For instance, what are the differences between different instances of an environment? Also, is there any other factor that leads to the differences between different teacher datasets other than the differences in environment instances? If the only difference between the datasets comes from the differences between the instances, could you elaborate how this might drastically improve the performance and efficiency of the whole system. \n- In addition, it seems that obtaining different instances of an environment is not straight forward, particularly in real-world application, which  contradicts with the practicality of the approach. The authors could provide alternative ways to acquire different instances of the environment so that the main idea of the paper can serve the main purpose.\n\n",
            "summary_of_the_review": "The paper shows promising empirical results by leveraging knowledge distillation from multiple teacher models in model-based RL settings. However, the paper contains several issues in terms of novelty, clarity, reproducibility of the results, and practicality of the ideas, and I thus vote for a weak reject.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces a new algorithm for MBRL that first learns multiple dynamics models (teacher models) for the environment, and then uses an ensemble of student models to distill these teacher models. Then one can run policy optimization methods inside the student model to obtain a policy. The authors show that the proposed method outperforms the SOTA model-based baselines in the pybullet locomotion environments. ",
            "main_review": "One of the motivations for the work is not accurate: the authors claim that MBRL methods only achieve success in tasks with low-dimensional state space, and tend to \"exploit regions with insufficient data for model training\". However, several works have been proposed to tackle high-dimensional dynamics learning (either learning directly [1] or via learning the latent dynamics [2][3][4]) and exploration in MBRL [5][6], which also works for high-dimensional locomotion such as humanoid and ant-maze. \n\nThe comparison with model-free methods (Sec. 5.2) is unnecessary, especially since this is a comparison in terms of sample efficiency.  \n\nThe learning of the teacher models also seems like learning an ensemble model, so it looks like the method is using an ensemble to distill another ensemble. It is hard to make sense why such a pipeline could bring substantial improvement in terms of performance. Also, the authors motivate the usage of these teacher models as \"different instances of the same environment\", but it is hard to understand what \"instances\" really mean here and there is no further explanation to it, for instance, what defines an instance of an environment?\n\nThe ablation study shows that if we do not use an ensemble for the student model, it proposed method is easily overperformed by the previous baseline. So it seems hard to understand why the model distillation is a good idea. \n\nBesides that the authors neither include a hyperparameter table or reproducibility statement, there are many confusions in the presentation of the algorithm:\n1. In Eq. (2), is the first term inside the summation the same for all $k = 1, \\dots, N$, so that it is repeated added for $N$ times? It also seems surprising that the coefficient for the two terms is fixed and there is no need for any hyperparameter tuning for such coefficient.\n2. Eq. (3) is just a number between 0 to 1, however, in line 13 of Alg. 1 it is treated as the termination condition. \n3. In line 2 of Alg. 1, how many trajectories are sampled? I might overlook this one but I haven't found it mentioned in anywhere in the paper.\n4. Line 3 to 6 of the paper, it seems like $k$ is switching between the number of total teacher models and the iterative indicator, which is confusing.\n\nOther minor issues:\n1. Line 2 of Sec. 3.1, the image of $p$ is missing.\n2. Line 4 of Sec. 4.1, typos in \"environment\".\n3. Sec. 4.1.1, line 4, $N$ is a number. It doesn't make sense to say $i \\in N$ where $N$ is a number. Also $i$ changes to $k$ in Eq. (1) which is inconsistent. \n4. Line 3 of Sec. 4.3, extra bracket.\n5. Line 2 of Alg. 1, $\\pi_1 , \\dots, \\pi_1$ seems like a wired notation.\n6. Fig.2 is unnecessary and it is taking a lot of spaces...\n\n[1] Kaiser, Lukasz, et al. \"Model-based reinforcement learning for atari.\"\n\n[2] Schrittwieser, Julian, et al. \"Mastering atari, go, chess and shogi by planning with a learned model.\"\n\n[3] Hafner, Danijar, et al. \"Dream to control: Learning behaviors by latent imagination.\"\n\n[4] Hafner, Danijar, et al. \"Mastering atari with discrete world models.\"\n\n[5] Song, Yuda, and Wen Sun. \"Pc-mlp: Model-based reinforcement learning with policy cover guided exploration.\"\n\n[6] Lopes, Manuel, et al. \"Exploration in model-based reinforcement learning by empirically estimating learning progress.\" \n",
            "summary_of_the_review": "Overall, the method lacks some intuition of the significance of model distillation. Also, there is space for improvement for the presentation of the method, especially the math part. More details should be added to avoid confusion. Overall I would recommend a rejection. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a model-based reinforcement learning algorithm for solving tasks with domain randomization in the dynamics. The method proposes a multi-teacher multi-student approach based on knowledge distillation for training less biased dynamics models. The results are decent when focusing on the one MBRL algorithm designed for similar experiments, but spend a lot of time on things that does not feel relevant. ",
            "main_review": "## General Comments\n- *Technical content*: This paper's technical content seems to be a contribution for the field and is interesting.\n- *Framing & presentation*: The paper's introduction and placement are confusing. It reads as if the authors are proposing a new algorithm for RL benchmarking on standard continuous control benchmarking paper, but it switches to be a domain adaptation paper. This makes it hard to fight hard for the merits of the approach.\n- *Writing*: The writing is on the unclear side, which again makes it harder to understand the authors point.\n\n## Paper specific comments:\n- *Model bias?*: Specifically, the authors write frequently of \"model-based\" in model-based reinforcement learning (MBRL), though I do not think they do a good job describing it. What do you mean by it? When I think of model bias I think of the bias in performance of the model to a specific part of the dataset. This is the predictions being more accurate in one part of the data. When the authors introduce varied environments, they should make it clear what they mean by model bias. Which spaces & why? Is it because the data is less dense? Is it because that space in the environment is harder to model? Does it matter? It's not clear that an \"accurate\" model is the best in MBRL  https://arxiv.org/abs/2002.04523\n- *comparing to different types of algorithms*: It is not clearly stated in this paper when you compare to things like SAC and MBPO that the MOBA algorithm is designed for a different use case. It should be abundantly clear why and how the environments are tweaked throughout.\n- I think my confusion comes from not being an expert in the knowledge-distillation related works. I tried to read some of it and figure out how it fits in, but I think expanding that material could make the paper more readable.\n\n## Ways authors could get a higher score\n- substantial re-write framing results better.\n- I think introducing the method with and without the ensemble is unclear. If the non-ensemble approach did not work, is it worth including at all?\n- the experiments with baselining MF and MB algorithms should be combined. This will let the authors include key ablations to give more understanding what MOBA is doing for MBRL relative to existing work.\n\n## Comments By Section\n\n### Abstract\n- the end of the abstract is confusing. This ends up mirroring the experimental section (see comments on combining MB and MF baselines).  As the experiments use humanoid, say that is the high-dimensional task used. \n- the abstract should way more heavily inform the reader that the approach is used to accomodate domain randomness. \n- the numbers in the abstract are not meaningful because they're cherrypicking across environments.\n\n\n### Intro\n- this should focus more on the motivation for domain adaptation / handling domain randomness. \n- the phrasing that model-free RL algorithms \"do not need the knowledge of the environment\" is misleading to borderline not true. They act in the environment with the same constraints as model-based? That is a large motivation of RL in general. \n- the sentence saying \"the system model can be given\" is unclear. Can the authors clarify?\n- the citations for model bias are pretty good. It would benefit the authors to have a definition like the one in the PILCO paper: \"they in- herently assume that the learned dynamics model suffi- ciently accurately resembles the real environment\". What are the catastrophic failures from model bias? Seems exaggerated. \n- if the \"couple of system models\" the authors speak of is an ensemble, they should just say that.\n- the last sentence of paragraph two has environment changes for the first time, which ends up being a very important sentence for the paper. Please elaborate on this. Why should I care?\n- The last paragraph of the intro can be removed to save space.\n- My reading and trying to learn knowledge distillation (KD) / student teacher training is that it's used in model compression and related works. Can that be clarified in the start of the paper? Is that correct? Why would RL benefit from this?\n\n## Grammatical comments / nits\n- in abstract, saying \"thus avoiding the frequent environment interaction\" is unclear and maybe not a correct sentence. Tweaking this would help the abstract.\n- intro, pluralization issue: \"successes ... network\"\n- saying low sample efficiency and requires huge amounts of data is very repetitive and done throughout this paper.\n- \"Section 2 is the realted work\", \"leanring framework\", \"experiment results\" -> experimental \n- related works - \"Neural network\" fix capitalization and pluralize\n- 2.2 \"layer of student network\" -> layer of the student network\n- weird citing \"Yuan et al. (Yuan et al., 2021)\"\n- first two sentences of 4.1 read weirdly. \n- missing period 4.2 \"trajectories At each time-step...\" \n- \"we perform K times of shallow trail\" is confusing\n- 5.1 say you use 3 environments, but experiments show 7\n- no period after PPO bullet \n- SAC intro: ... is an off-policy algorithm, rather than saying is optimized in an off-policy way\n- in 4.2 I think s_t+1 should be hat(s_t+1), along with r?\n\n### Related works\n- the authors cite the MBPO paper (Janner et al) paper wrt model bias, but I don't really think that's the point of the model. That paper is varying the horizon based on accuracy across horizon length / what just \"worked\" with SAC. Am I wrong?\n\n\n### Methodology\n- Figure 1: is it always 1-to-1 teacher to student? Adding a notion of env. Variation would help this. \n- in equations 1) and 2) is there a weighting among the loss terms? Is this something done in related works or does it have any effect on the system? Is there a case when one term is more important?\n- I think the \"policy rehearsal\" name is pretty good. Is that coined in the paper or from related work?\n- In policy rehearsal, how long are the imagined trajectories? This was extremely important to MBPO so a comparison would be interesting. \n- shallow trail should be defined the first time the phrase is used.\n- in 4.4 the authors say that initialization is important. Were the parameters from a uniform distribution? Was the data uniform? Please clarify and explain what results you found. \n- in the states and rewards ensemble, what is meant but the end of the last paragraph. What is \"majority vote\" that is confusing? \n\n### Evaluation\n- the figure with the environments can be shrinked / partially moved to appendix to save space.\n- The algorithms don't all need to be described in bullet points. Just the most relevant MBRL algorithm can use a definition. This will save a lot of space for more insight.  \n- the authors claim each student model learns different aspects of the state space. Especially when learning ensembles this would be an awesome claim to back up with an experiment. It would be interesting to see the student tested on different spaces in the state-space (static test set) and report errors.\n- What is the takeaway from figure 5? It's weird that there isn't much of a trend from left to right.\n- table 1 can be removed. \n- 5.4, why did the authors choose to add noise in this way? Noise is one way models could be inaccurate, but the per-environment variation is likely much higher. Showing how the models perform across environments would be interesting. Why was half cheetah chosen? \n\n\n### Conclusion\nThe conclusion really is like the rest of the paper. I think where the authors talk about model bias, it should be talking about environment variation. Some comment on the future work would be helpful, as this seems like a new approach for MBRL.\n## Grammatical comments / nits\n- in abstract, saying \"thus avoiding the frequent environment interaction\" is unclear and maybe not a correct sentence. Tweaking this would help the abstract.\n- intro, pluralization issue: \"successes ... network\"\n- saying low sample efficiency and requires huge amounts of data is very repetitive and done throughout this paper.\n- \"Section 2 is the realted work\", \"leanring framework\", \"experiment results\" -> experimental \n- related works - \"Neural network\" fix capitalization and pluralize\n- 2.2 \"layer of student network\" -> layer of the student network\n- weird citing \"Yuan et al. (Yuan et al., 2021)\"\n- first two sentences of 4.1 read weirdly. \n- missing period 4.2 \"trajectories At each time-step...\" \n- \"we perform K times of shallow trail\" is confusing\n- 5.1 say you use 3 environments, but experiments show 7\n- no period after PPO bullet \n- SAC intro: ... is an off-policy algorithm, rather than saying is optimized in an off-policy way\n- in 4.2 I think s_t+1 should be hat(s_t+1), along with r?\n\n## Other comments / ideas (not included in scoring of paper)\n- The authors say MOBA is algorithm agnostic, it would be interesting to see this expanded on.\n- some of the citations are weirdly formatted. The authors should look into it.\n- seems like these adaption in RL experiments may be covered my meta-learning researchers now. That's not me, but it may be worth checking out.",
            "summary_of_the_review": "This paper has some interesting elements, and some things that could be viewed as contributions in the future, but the paper is written in a way that makes it hard to place in the literature. Combined with needing many clarity edits, it's hard to recommend this paper for publication. Things fall much further on the \"not well supported\" side rather than \"incorrect\", which means the paper could be updated to an accept, but would be a large effort.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}