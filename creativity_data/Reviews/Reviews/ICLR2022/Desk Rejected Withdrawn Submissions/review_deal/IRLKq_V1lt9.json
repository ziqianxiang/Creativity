{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper addresses the problem of rare words in pre-trained language models. The quality of representations depends on the word frequencies of the training corpus and the word frequencies usually have a heavy-tail distribution. To mitigate the problems, the authors aim to use dictionary definitions of rare words to supplement the lack of data during the training and prediction. \n\nFirst, the authors propose that the rare words are the least frequent words amounts to 10% of the total frequency. Each rare word in a sample will have its definition appended to the sample text. Then, they introduce two loss functions to incorporate the definitions: maximizing mutual information (MIM) and definition classification (DD). During the prediction, the authors use the visibility matrix to mask the attention of words in the sample text, so that the definitions will only influence their corresponding rare words in the sample text.\n\nIn the experiment, the authors show that the proposed method (Dict-BERT) outperforms BERT and BERT-TNF in the GLUE benchmark and DAPT in the domain adaptation benchmark. Further analysis shows that the training using the masked language modeling objective along with MIM and DD losses is an important step, and fine-tuning with MIM and DD losses improves the performance of the downstream tasks further. In addition, both MIM and DD contribute to the performance gain, and the lack of visibility matrix (full attention) worsens the performance. Finally, in the WNLaMPro experiment, Dict-BERT is found to perform better than BERT in rare words while slightly better infrequent words.",
            "main_review": "## Strengths\n\n1. The paper proposes a novel approach that addresses a fundamental problem of language processing.\n2. The extensive experiments support that the claim that the proposed method outperforms the previous work.\n3. The ablation analysis justifies each component of the proposed method.\n4. The detail of the training is provided in the appendix, and the experiment results come from an average of 5 randomly initialized models. So they are likely to be reproducible.\n\n## Weaknesses\n\n1. The writing of this paper is confusing at times. For example, in Section 3.4.1, there is no explanation of how to get $h_i$ and $h^{(i)}$ for Eq 3 (though we can infer from the figure). There is Appendix x,x by the end of the paper. More importantly, there are many definitions of words. It is unclear how polysemy is taken care of in Section 3.1.\n2. There are some unsupported statements in the paper. For example, when compared with BERT-TNF, the authors claim that Dict-BERT can dynamically adjust the vocab, it is unclear whether any of the experiments support this. In addition, we cannot draw from the experiments that \"Dict-BERT spends less memory on rare words\" (and so also improves the frequent words as observed). In addition, +1.15 (82.65 to 83.80) is not a great improvement when others reach 90+ in the leaderboard.\n3. Although the experiments show that the proposed method outperforms the others in the overall result, it is not certain that this comes from the rare words as originally intended. Further analysis (similar to WNLaMPro) of the two groups of samples in different tasks could reveal the true effectiveness of the proposed method. \n4. There is a large chunk of previous work on using definitions that the authors should have mentioned. For example, [Bosc and Vincent (2018)](https://aclanthology.org/D18-1181/) use definitions to learn word embeddings, albeit not in the pre-trained language modeling.",
            "summary_of_the_review": "I am inclined toward rejecting this paper mainly because the experiments do not fully support the main claim and a few statements. The experiment results only show the overall performance, not the rare words as motivated. The authors could have divided the datasets into two different groups (containing rare words) and compared the performance. While the authors claim that the definitions help alleviate the burden of rare words, it is still unclear whether this is indeed true (and might not be verifiable). Since the state-of-the-art on GLUE is now in the 90s, it is also important to state why a better method such as RoBERTa or T5 is not compared. This is to show the robustness of the proposed method as well as keep it up-to-date with the literature.\n\n\n### Minor questions\n1. What happens if the definitions are absent during the prediction using the Dict-BERT-PF (i.e. the performance)?\n2. How polysemy words are handled during the training? \n3. Does the model pays attention to the words in the definitions? (Perhaps, some analysis as in [Noraset et al., 2017](https://arxiv.org/pdf/1612.00394.pdf)'s Figure 1.\n4. Why absolute positional embedding?\n\n ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors attempt to address the challenge of optimizing the embeddings of rare words that follows the long-tail distribution. To this end, they leverage the rare word's definition from a dictionary (i.e., Wiktionary) to append to the end of the input sequence. They propose two novel pre-training tasks i.e., word-level and sentence-level alignment between input text sequence and rare word definitions. In addition, they adapt a knowledge attention mechanism to reduce the impact of a rare word's definition on the original text. They evaluate the proposed system against eight natural language understanding tasks in GLUE and eight domain-specific classification tasks. The pre-trained and fine-tuned version of Dict-BERT achieves competitive scores against baselines.",
            "main_review": "Strengths\n1. Addressing rare/unseen words during LM pre-training is a relevant and challenging problem. As reported, the proposed system seems to improve the performance of vanilla BERT and RoBERTa on various tasks. \n    \n2. Author's are proposing two novel pre-training objectives (i.e., word-level mutual information maximization and sentence-level definition discrimination) to learn the representation of rare words.\n    \n3. The technique to select the rare words is effective in the sense that we can dynamically adjust the vocabulary of rare words.\n    \n4. Adapting the visibility matrix to keep a check on the impact of rare words' definitions on the semantics of the original text. As reported, knowledge attention improves the model performance. \n    \n5. Overall, the paper is well written and easy to read. \n\nWeakness\n1. The choice of two pre-training objectives is not well motivated. Did the authors try using the original MLM and NSP pre-training objectives with knowledge distillation along with the definitions of rare words appended to the input text? \n    \n2. Evaluating domain-specific classification tasks needs more rigorous comparison with various SOTA/widely-used models. For instance, how does Dict-BERT fair against BioBERT for the bio-med domain?\n\n3.  Reported BERT-TNF scores in (Wu et al., 2021) have consistently outperformed the proposed Dict-BERT on GLUE benchmark tasks. But the authors have reported less performance of BERT-TNF with their re-implementation. Is there any clarification/feedback taken from the BERT-TNF paper authors' for this mismatch? The reported results in Table 1 are concerning given that the higher performance of Dict-BERT is influenced by the author's re-implementation of the baseline work.\n    \n4. Authors are consistently using weak baselines to compare their system. A lot of effort has been made to fine-tune BERT and other PLMs on domain-specific datasets. Comparing with Vanilla BERT/RoBERTa does not justify the current state of the literature. Also, it doesn't give a clear picture of how does Dict-BERT, in its current state, is useful as compared to the already existing solutions that do not explicitly address the rare word problem.\n    \n5. There is a missing discussion on the distribution of rare words in various datasets that are used to report the results. It further raises several questions such as the choice of downstream tasks to report the performance of Dict-BERT, how does Dict-BERT influences the representation of these rare words in various datasets, how do the two proposed pre-training objectives influence the representation of rare and seen words in a specific domain, etc. ",
            "summary_of_the_review": "The authors attempt to address a novel and relevant challenge with pre-trained language models i.e., the representation of rare words. The proposed technique is not evaluated well enough against the strong baselines to demonstrate its effectiveness. Furthermore, there is a lack of discussion on the choice of datasets for evaluation. No clear mentioning of the distribution of rare words in the selected datasets makes it difficult to understand the impact of the entire experiment.  \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper aims to help PLMs learn better representations for rare words. Following this idea, the researchers propose a novel method to train LMs with two new tasks: mutual information maximization and definition discrimination. Experiments on a wide variety of tasks have shown the supposed Dict-BERT model leads to some improvement on PLMs performance, which is higher than previously proposed BERT-TNF. According to analysis, their extra self-supervised tasks might also improve the model’s understanding of frequent words.",
            "main_review": "Strength:\n1. This paper is the first work towards strengthening PLMs understanding of rare words and proposes two somewhat plausible training tasks based on dictionary definitions. These tasks successfully combine the structure of dictionary definition and masked language modeling to reach a performance improvement.\n2. The implementation of leveraging dictionary definition to strengthen PLMs is somewhat novel and illuminating for future modeling on other corpora with special structures. \n3. Experiment results show that Dict-BERT results in higher improvement on GLUE tasks than previously proposed knowledge-based methods and this conclusion are supported by affluential results. \n\nWeakness:\n1. The baseline for comparison is weak. This is somewhat acceptable as previous work has not opened its source.\n2. The researchers have missed some parts about their experiments in your paper, like the analysis for Figure 3 (b).\n\nQuestion:\n1. Will you release your code in the future?\n2. It is true that your training method can be generally applied to all PLMs, but will this lead to general improvement? Would you like to experiment on some stronger PLMs like RoBERTa? If you do so, my confidence in accepting this paper will be stronger. \n3. Will dictionary-based training lead to degradation for understanding frequent words as frequent words always have multiple meanings in sentences.\n\nSuggestion:\n1. Apply your method to a stronger baseline like RoBERTa to see if the improvement will decay on stronger baselines. \n",
            "summary_of_the_review": "On the whole, the work is interesting, but the experimental part still needs to be improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work proposed to inject the definitions of rare words into previous models. The definitions are fetched from dictionary (e.g., Wiktionary) and are then appended to the end of the input text sequence. The model is trained via the Transformers architecture. Despite the masked language modeling objective, the authors proposed another two novel pre-training tasks: 1) a word-level objective aiming at maximizing the mutual information (MIM) between Transformers representations of a rare word in the input text and in the dictionary definition; 2) a sentence-level objective learning to differentiate between correct and polluted word definitions (DD). The experimental results (good to see that they are average results of run with random seeds) show that Dict-BERT can improve the performance of pre-trained models. The method can be extended to other Transformer-based pre-trained models. ",
            "main_review": "-------------\n\nStrengths:\n\n1. The problem is clearly defined. This work provides a solution for the rare word problem in pre-trained language models. \n\n2. The method of retrieving word definition from external resources is sound and reasonable. \n\n3. Empirical results on language understanding benchmark GLUE and eight specialized domain benchmark datasets verify the effectiveness.\n\n-------------\n\nConcerns:\n\n1. This work is based on the basic hypothesis, “a large proportion of words appear very few times and the embeddings of these rare words are poorly optimized. Such embeddings usually carry inadequate semantic meaning, which complicate the understanding of input text, and even hurt the pre-training of the entire model.”\n\n- This work is based on pre-trained models which are basically trained on the subword units. It is not clear how the rare words would hurt the language model pre-training. Using subword units for language modeling would have already help the model get rid of sparse representation.\n\n- This model fetches definitions for words. However, the tokenization of the language model produces subword sequences. Do you align the dictionary words with the subword units, or the method only deals with words in the subword sequence?\n\n2. Regarding the possibility of application impact, the pre-trained language model is not quite general, due to the fact that the selection of the rare words is strongly related to the corpus. When fine-tuning for downstream tasks, this may bring inconvenience as one should append the definition of rare words to the end of the input text.\n\n3. The method of appending rare word definitions to the end of the input text sequence will result in much longer sequence, increasing computation complexity.\n\n4. As this work shows that using rare word definition may improve the model performance, how about using definition for all the words that exist in the dictionary? Maybe the dictionary would benefit not only the rare words.\n\n5. The experiment results on GLUE datasets quite incremental concerning the large-scale datasets such as MNLI and QQP. For the comparisons in the eight special domain datasets, it seems the most contribution comes from domain-adaptive pre-training. In comparison, the proposed method does not show obvious advance. Besides, ablation study concerning the knowledge-visible attention should be done on large scale dataset such as MNLI and QQP to demonstrate its effect.\n\n6. The innovation of this work is limited, which are highly based on the traditional problem of rare word representation for word-based neural networks (though not previous studies were mentioned).  “We are the first work to integrate word definitions in a dictionary into PLMs.” \n- This idea is closely related to widely-used techniques that retrieve knowledge items/definitions from a knowledge graph. \n- I think the major novelty lies within the pre-training tasks, however, they would be quite computationally expensive which require pre-training – Would it be necessary to pre-train a new language model given the marginal gains?\n\n7. For the sentence-level definition discrimination objective, it is not so reasonable to me. The original rare word sets are replaced with the probability of 50%, which may lose a lot of information regarding to the original context. The objective would be meaningless in its current form. The model learns that the definition is for word A, but not for word B. However, the definition is already given in the dictionary, there might be no need for the model to learn which is the definition to which word. \n\n8. “This indicates the pre-trained language model cannot quickly learn rare word definitions in the dictionary to help improve downstream task performance… Therefore, it is important to integrate dictionary into language model pre-training so the dictionary definitions can be better utilized”\n- It suggests that the method should only work in pre-training, instead of fine-tuning. Again, it might be less efficient. \n\n-------------\n\nMinor presentation issues:\n\n1. Caption of Figure 2 “Colored circle means….can attention information from”: attention -> attend.\n\n2. The notation in Table 1 is a bit redundant. Values in “Dict in” already represent the “-F”, “-P” and “-PF” notations.\n\n3. Figure 3(b) is not even mentioned in the context.\n\n4. The bold results in Table 1 are not the best ones. It seems the authors neglect the results of existing models in the first two lines, why?\n\n5. In Table 2, the result (87.62) from RoBERTa-DAPT/RCT should be bold instead of 87.51 from w/o MIM\n",
            "summary_of_the_review": "Overall, this paper deals with a traditional problem in the new pre-training perspective. However, the motivation is not clear, the method is straightforward and has the flaws of computation complexity. The effectiveness is still in doubt, with marginal improvements. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new pre-trained language model called Dict-BERT. Dict-BERT mainly enhances the representations powers of rare words by introducing dictionary knowledge. Besides the traditional MLM, Dict-BERT also employs two additional pre-training tasks that is related to rare word modeling. The experimental results on GLUE and  datasets for specialized domains show that Dict-BERT could give consistent improvements.\n",
            "main_review": "Strengths:\n\n1. Using definitions of rare words in language pre-training is technically sound.\n2. The proposed approach achieves relatively stable improvements on GLUE and datasets for specialized domains.\n\nWeaknesses:\n\n1. The main concern is that the improvements on GLUE are moderate, considering that the baselines are relatively lower than others. The original BERT uses a batch size of 256 and is trained for 1M steps. Your model uses a batch size of 2K and is trained for 300K steps. It can be seen that your models are trained longer than the original BERT, while your baselines are still lower. Also, it has been widely proven that the original BERT results are quite underestimated. Taking all these into consideration, I cannot say the results of Dict-BERT is significantly better than vanilla BERT.\n2. Training and fine-tuning Dict-BERT is relatively complicated (than vanilla BERT). Considering it outcomes, I am not convinced that Dict-BERT can be generally used and adapted.\n3. As we also need to append the definitions of rare words in fine-tuning stage, it introduces additional computing cost and might not be friendly to long-sequence tasks (such as document classification and machine reading comprehension). Unfortunately, the authors did not discuss these aspects.\n\n\n\nMajor comments:\n\n1. Figure 1: I am not sure if the rare words are always masked. At least in this example, `Covid-19` is masked. Maybe a clear illustration should be given on whether the rare words are always masked. \n2. Section 3.4.2: You mentioned that the negative sample is RANDOMLY chosen from the whole vocabulary. However, in Figure 1, the negative sample `SARS` seems to be a very good negative to `Covid-19`. It has been widely proven that hard negative samples are much useful in discriminative training. Did you try to find a better way rather than randomly choosing a negative sample?\n3. If there are $n$ rare words, does that mean we have to append $n$ negative samples accordingly?\n4. As mentioned in weaknesses, Dict-BERT may not be friendly to long-sequence tasks. Have you tried your model on those tasks? (such as SQuAD)\n\n\n\nMinor comments:\n\n1. Section 4.1: we use BERT -> We use BERT\n2. Section 4.2: 252581 -> 252,581\n\n3. Please try to reorganize the positions of the figures and tables. For example, you first mention Table 1 on page 8, but the actual table 1 is on page 6, which is not friendly to the readers. I have to read back and forth in these sections.\n4. Section 4.6: In `Knowledge Attention v.s. Full Attention.`. What is  `Appendix x.x`?",
            "summary_of_the_review": "The overall design of the model is technically sound, and it provides moderate improvements on several datasets. However, there are also some concerns on the complicated modeling, efficiency, and weak baseline results, etc. Considering all these aspects, I am not sure if Dict-BERT can be widely adopted, and thus leaning slightly towards rejection at the moment. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}