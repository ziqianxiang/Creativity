{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper investigates the performance of an approach to fine-tuning that trains jointly on the target task and a subset of the source task that is close to the target task. Based on some novel theoretical results as well as empirical results from previous work, it claims that fine-tuning provides little or no benefit over training solely on the target task. It then investigates a specific approach to performing this kind of joint fine-tuning, and shows it leads to benefits across two models and 8 transfer tasks.",
            "main_review": "The biggest strength of this work is the validation of the proposed unbiased optimal transport technique for dataset selection for joint fine-tuning and the associated empirical results. Although the overall improvement in Table 1 is not enormous, it is likely large enough to be practically meaningful, and it seems to be consistent across many tasks. I also think that the direction of continuing to train on (a subset of) the source dataset while training on the target is an interesting one that is still underexplored.\n\nWith that said, I am not happy with the way that the submission is framed, the way it is situated within the literature, or the informativeness of the theoretical results. The former two issues seem relatively straightforward to address, but unless the authors can convince me that the theory is relevant to the practical setting investigated, I don’t think it should be part of this paper.\n\n1. The relationship of the proposed approach to prior work is not adequately discussed. Ge & Yu [1] proposes a joint fine-tuning strategy that, like the joint fine-tuning strategy in the experiments here, involves first selecting a subset of data points from the source task that are in some sense similar to the target task and then jointly training on the source and target tasks. In the case of [1], data points are selected from the source task by searching for nearest neighbors of images from the target task in (a normalized version of) the network’s representation space. This work uses a different approach to select examples, but otherwise seems similar. Ideally the authors would reproduce the approach of [1] and compare to it, although the example selection strategy in [1] is somewhat esoteric and it may be nontrivial to adapt to newer architectures. Cui et al. (2018) is discussed only in the context of its example selection algorithm, but is also somewhat related: it pretrains on a selected subset of the source data, and then fine-tunes only on the target task. There is some subsequent work on source model/data selection, but little on joint fine-tuning, so I think there could still be a meaningful contribution here, but it should be better-situated within the literature.\n2. The paper is based around the claim that pretraining does not improve converged accuracy. However, the strength of this claim is overstated relative to evidence from the literature. The introduction says that “there is almost no benefit from training with ImageNet pre-trained models because training from scratch can achieve the same accuracy after a period of additional training” but reality is more complicated. There is almost no benefit from ImageNet pre-training on the COCO detection/segmentation tasks, which have 118,287 images, but He et al. (2019) show that, on the smaller Pascal VOC 2007 detection/segmentation tasks, training from scratch never catches up. Kornblith et al. [2] compare the performance of fine-tuning from ImageNet and trained from scratch on standard image classification benchmark tasks, some of which overlap with those studied here. They find that, on most tasks, fine-tuning outperforms training from scratch regardless of the number of steps the model is trained for, although there are some fine-grained tasks where it doesn’t. There are also some high-profile cases where pretraining seems to give some impressive gains, e.g. for image classification with larger pretraining datasets than ImageNet [3] and for fine-tuning in NLP [4,5].\n3. The theoretical result that the submission claims demonstrates that “the pre-trained model has almost no benefit over training from scratch” comes from the form of an upper bound on excess risk, which I do not find entirely compelling. The argument, as I understand it, is as follows. Lemma 1 provides an upper bound on the excess risk of a pretrained model. Lemma 2 then provides an upper bound on the excess risk of the fine-tuned model, and in this bound, the expression that corresponds to the upper bound from Lemma 1 is buried inside a log. However, since these are both upper bounds, both could be loose, and the true dependency on pretraining could be stronger than the bound. For the theory to demonstrate meaningful things about the efficacy of pretraining, I would either want to see some description of why we should expect that this is the tightest possible bound that holds in a realistic scenario, or some empirical validation that the scaling law suggested by the bound is observed in practice. I would also appreciate some explanation of how the success of larger-scale pretraining can be reconciled with the theoretical claims.\n4. I am having a bit of trouble seeing how the theory works when the classification head used for the downstream task is different from the pretraining head, because the loss is a function not just of the pretraining parameters but also some auxiliary parameters. I see how one could define the loss so that the auxiliary parameters are determined by some kind of nested optimization, e.g. so we are picking the optimal head weights on the downstream training set given the backbone parameters, but if we do SGD on the head and the backbone simultaneously, then the gradient of the cross-entropy loss WRT the backbone will not be an unbiased estimate of the gradient of this objective.\n5. Section 5.4 suggests that some of the gain from the UOT method over the previously proposed greedy OT method comes not from UOT itself but from the use of cosine similarity in place of L2 distance. I am frustrated by this, particularly in the case of the self-supervised model, where the decision of whether to consider L2 normalization as part of the model or as part of the loss function is an implementation detail. The results in Table 2 still indicate that UOT works better than greedy OT, which makes me somewhat less frustrated, but I would’ve been happier if the authors had used the same distance measure for both techniques for the results in Table 1, to demonstrate that the gains really come from doing UOT.\n\nMinor:\n- Self-training as in Noisy Student [6] and Zoph et al. (2020) is another method that could be used to integrate additional information from the source task. Ideally the study would include a self-training benchmark, but if not, it should at least be discussed in the related work section.\n- Many of the tasks evaluated in Table 1 have small test sets. Although the pattern of bold-faced numbers in Table 1 clearly favors the proposed method, it would be useful to have an estimate of the variance of the estimates and/or statistical comparisons between the numbers.\n- The importance of the batch size in $\\delta^2$ of Eq. 2 is weird to me. Do the authors see this dependency in practice?\n- I think Figure 2 is showing the difference between methods as a function of fine-tuning dataset size, but that could be a bit clearer. Also, because accuracy saturates at 100%, the fact that the difference in accuracy between methods decreases as the base accuracy goes up seems somewhat unsurprising. If a method gives a 1% improvement from 50%->51% in one setting, I would not expect a 1% improvement in a setting where the base method gets 99%.\n- Can the authors confirm that the validation set that the hyperparameters selected on is not the set on which accuracy is reported in Tables 3 and 4? Assuming that is the case, is the model retrained on the combined training + validation set with the optimal hyperparameters or is it evaluated on the validation set and then on the test set?\n\n[1] Ge, W., & Yu, Y. (2017). Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1086-1095).\n\n[2] Kornblith, S., Shlens, J., & Le, Q. V. (2019). Do better imagenet models transfer better?. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2661-2671).\n\n[3] Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., & Houlsby, N. (2020). Big transfer (bit): General visual representation learning. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part V 16 (pp. 491-507). Springer International Publishing.\n\n[4] Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding by generative pre-training.\n\n[5] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n\n[6] Xie, Q., Luong, M. T., Hovy, E., & Le, Q. V. (2020). Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10687-10698).",
            "summary_of_the_review": "The empirical results in this paper seem convincing, but the novelty of the methodology is somewhat limited in light of closely related previous work that is not cited or discussed. The theory is less convincing; it supposedly demonstrates that fine-tuning from a pretrained model \"has almost no benefit over training from scratch,\" but this claim is questionable based on empirical results and is based on the mathematical forms of upper bounds on excess risk that could be loose. Overall, I feel that this submission would need significant changes to be acceptable for publication at ICLR.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work studies an important problem in transfer learning, that is, how to improve the generalization of fine-tuning the selected samples used in the pre-training stage. To this end, the problem is formulated as an unbalanced optimal transport problem. So the proposed data selection method is straightforward to use an existing solution. This work provides theoretical analyses on the values of pre-trained data and models, and the proposed data selection method is evaluated on eight classification datasets. The experimental results show that the extra training samples selected by the proposed method improve the performance of the fine-tuned models.",
            "main_review": "Strengths:\n\n+ The core idea of the proposed method, i.e., selecting pre-training samples for fine-tuning, is straightforward yet plausibly effective. It could be beneficial for a variety of applications.\n\n+ This work provides theoretical analyses that help the audience better understand the effects of the pre-trained model and data on the fine-tuning process.\n\n+ This work conducts extensive experiments on eight target datasets for image classification.\n\nWeaknesses:\n\n1. The related work of fine-tuning/transfer learning is not thorough. It misses (but is not limited to) Co-Tuning [r1] and SpotTune [r2]. Moreover, a highly related work of dataset selection for transfer learning and optimal transport based similarity is Optimal Transport Dataset Distance (OTDD) [r3].\n\n2. OTDD [r3] takes into account the importance of labels, whereas the proposed method neglects such information. Also, Co-Tuning [r1] aims to find out the correlations between the labels from different datasets, whereas the proposed method does not capture such big picture information. In this sense, the proposed method may be a bit primitive.\n\n3. The integration (Eq. (5)) of the information learned with training data and the pre-training data may be neglectful. The gradient $\\nabla f (\\theta_{t}; \\zeta_{it} )$ could conflict with the gradient $\\nabla h (\\theta_{t}; \\xi_{it} )$ in directions. For example, the angle between $\\nabla f (\\theta_{t}; \\zeta_{it} )$ and $\\nabla h (\\theta_{t}; \\xi_{it} )$ is greater than 90 degrees. It seems that the proposed method does not have a way to detect such a case for better updating the model. Moreover, it would be clearer if this work can add an analysis of cosine similarity between $\\nabla f (\\theta_{t}; \\zeta_{it} )$ and $\\nabla h (\\theta_{t}; \\xi_{it} )$.\n\n4. It seems that this work does not provide the baseline that uses the vanilla fine-tuning method with both training samples and pre-training samples. It would be helpful to provide two baselines: 1) using the vanilla fine-tuning method with training samples, and 2) using the vanilla fine-tuning method with both training samples and pre-training samples.\n\n5. The integration (Eq. (5)) takes place in the back-propagation. To demonstrate that Eq. (5) is an optimal design, it would be more convincing to show the comparison with Mixup [r4] or its variants (e.g., [r5]). Is it possible that combining the pixels of two images leads to better performance than combining the gradients?\n\n\nReferences:\n\n[r1] You, Kaichao, Zhi Kou, Mingsheng Long, and Jianmin Wang. \"Co-Tuning for Transfer Learning.\" Advances in Neural Information Processing Systems 33 (2020).\n\n[r2] Guo, Yunhui, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing, and Rogerio Feris. \"Spottune: transfer learning through adaptive fine-tuning.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4805-4814. 2019.\n\n[r3] Alvarez Melis, David, and Nicolo Fusi. \"Geometric Dataset Distances via Optimal Transport.\" Advances in Neural Information Processing Systems 33 (2020).\n\n[r4] Zhang, Hongyi, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. \"mixup: Beyond Empirical Risk Minimization.\" In International Conference on Learning Representations. 2018.\n\n[r5] Lee, Kibok, Yian Zhu, Kihyuk Sohn, Chun-Liang Li, Jinwoo Shin, and Honglak Lee. \"$ i $-Mix: A Domain-Agnostic Strategy for Contrastive Representation Learning.\" In International Conference on Learning Representations. 2020.",
            "summary_of_the_review": "I think the idea of this work is practically useful and the empirical evidence shows that the proposed method is effective. On the other hand, due to the lack of a thorough literature review, I am not convinced that the problem (i.e., using pre-training data in the fine-tuning process) has been fully studied and the design is optimal. This holds me back from recommending to accept this work. I will go over the review comments by other reviewers and the responses by the authors and adjust my recommendation accordingly.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper analysis the commonly used fine-tuning techniques by providing theoretical results of the weak dependency of the final results on the pre-trained model. Based on the above observation, this paper leverage pre-training data for fine-tuning, and develop a data selection scheme to enhance the performance of the target learning task. Some experiments demonstrate the effectiveness of the proposed fine-tuning method.",
            "main_review": "Strengths:\n[1]. This paper focuses on the very important problem in deep learning. It gives theoretical results about the fine-tuning scheme on deep neural networks.\n[2]. This paper is very organized, and technical details are presented clearly. \n[3]. The design of the algorithms and the experiments are reasonable, and the results look good.\n\nWeaknesses:\n[1]. The theory is not convincing enough. The effectiveness of fine-tuning is not very explored. The most important thing is to figure out the role of pre-trained parameters in the subsequent learning stage. It has been proved in many papers such as \"CNN Features off-the-shelf: an Astounding Baseline for Recognition\" (CVPR 2014), that the off-the-shelf feature is already very strong in real applications. But in \"Rethinking ImageNet Pre-training\" (ICCV 2019), Kaiming He stated the pre-trained model is not very necessary. What's the role of the pre-trained parameters? Are the pre-trained parameters are a better choice than random initialization for a novel learning task? I haven't found the related discussion about this. \n[2]. The selective joint fine-tuning (\"Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning \", CVPR 2017) seems like an implementation of the proposed algorithm. Can the proposed theory explain the performance?",
            "summary_of_the_review": "Large-scale fine-tuning is very popular in both natural language processing and computer vision now. I think this paper should give a more solid theoretical analysis of the fine-tuning scheme.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}