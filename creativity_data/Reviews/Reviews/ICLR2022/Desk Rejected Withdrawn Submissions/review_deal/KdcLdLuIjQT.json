{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes intrinsic rewards to train agents without environment rewards in text-based games. The key contribution is a goal generation method that samples random goals from a set of valid goals in natural language, which are obtained based on commonsense rules. Reviewers generally agree that the proposed method is intuitive and simple to implement, and appreciates the new results added during discussion. However, there are two main concerns: 1) the goal creation process is largely rule-based and task-specific, therefor it's unclear how well this method would generalize to other tasks; 2) related to 1), the generate goals carry a significant amount of domain knowledge about the task that is not available to the baselines, making the comparison a bit unfair. A future submission would benefit from demonstrating the generalizability of the proposed approach, e.g., by using more generic resources such as game meta data, generic knowledge/commonsense bases."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose a simple intrinsic reward for text-based games, in the absence of environment rewards. The method, GoalRand, is based on uniformly sampling random goals from a set of natural language goals generated using common-sense rules. The authors evaluate their method on both seen and unseen text-based games and find that their method outperforms a random agent and a count-based intrinsically motivated agent.",
            "main_review": "### Strengths\n- The proposed method seems straightforward and intuitive. Natural language is a promising modality for goal specification, and this work can potentially be insightful for goal conditioned RL research beyond text games.\n- Empirical results  seem convincing. The authors suggest the improved performance of GoalRand is due to goal randomization improving exploration and decomposing complex tasks, which is interesting. \n\n### Weaknesses\n- Goal generation sets are based on common sense rules specific to the game — it seems like relying on such curated rules may be hard to scale and/or extrapolate to more complex text games (e.g. where the action and item spaces are large). It would be interesting to see if the goal sets could be automatically generated or learned, rather than hand engineered?   \n- As the goal space grows with game complexity, uniform sampling might be less and less effective. Is there a way to prioritize a curriculum of goals to sample instead? \n\n### Questions / Additional Feedback\n- Would it be possible to combine GoalRand with a method using environment reward for further gains?  \n- More clarity on the specifics of game scoring would help.  \n- Bottom left plot on Figure 2 seems to be missing the Random baseline. ",
            "summary_of_the_review": "The empirical results seem compelling and the method is straightforward, and overall this paper provides insights into a more self-supervised method for text-based games. However, it is hard to say how well the method will extend to games where common sense rules are harder to come by or the goal space is much larger, where uniform sampling may work less well or be less sample efficient. It seems like the strengths come from crafting a goal space and generator tailored the given text games, and it would be interesting to see if the method could be improved by reducing reliance on hand picked rules. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies text-based games using a technique called goal randomization. This method uses random basic goals to train a policy in the absence of the reward of environment. Authors show that agents can learn policies that generalize well across different text-based games. It also seems working better than a state-of-the-art algorithm that uses environment reward.",
            "main_review": "Strength\n\nThe method proposed seems quite effective. It not only beats a random agent but also a SOTA algorithm that uses environment reward.\n\nWeakness\n1. The novelty of this paper seems quite low, not enough to meet ICLR bar. Goal randomization is a very simple idea and similar to intrinsic motivation.\n\n2. I am a bit skeptical on whether the experimental results are solid. For example, the authors suggest that only encouraging exploration is not sufficient when the environment reward is not available, because the BeBold baseline performs even worse than the non-learnable random agent. From Figure 2 it seems BeBold's normalized scores are still increasing and is very likely to surpass \"random\".\n\n3. Same for the comparison in Figure 3, I kept wondering whether GATA is still undertrained. It seems in many settings GATA is still improving even at 60k. I also feel the authors need more explanation on how goal randomization is able to outperform an agent that has access to environment reward. This is still a very surprising finding this is indeed the case.\n\n4. Other than pure random goal, perhaps explore some other types of goal generation, e.g. something that is similar to curriculum learning?",
            "summary_of_the_review": "This paper studies an interesting problem, but lacks novelty and results are not so convincing. Authors need more results / discussions to show why this proposed method works so well.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces GoalRand, an algorithm for playing text-based games in the absence of extrinsic reward. In particular, it uses generated goals to train a goal-conditioned reinforcement learning agent. These goals are generated by extracting factual information from the agent's knowledge graph. Given a goal sampled from the set of possible goals, as defined by the current knowledge graph, the agent is given a intrinsic reward for completing this goal within a specified time limit of 5 steps. Goals are randomly sampled from the set of possible goals. \n\nThe paper contributes this goal generation methodology which allows the agent to be trained without access to extrinsic rewards. Additionally, experiments are conducted on Textworld's Cooking Games, comparing against an intrinsic motivation baseline (BeBold) as well as a previously high-performing agent on these games, GATA. The scores show higher performance from GoalRand compared to the baselines across a set of heldout layouts and levels.",
            "main_review": "This paper presents a compelling narrative about how long-horizon text-based games can be decomposed into short-horizon goals, which can greatly help an agent solve the larger task. I really like the overall idea, and think that it is particularly applicable to text-based games which large tasks can be neatly decomposed into series of smaller tasks.\n\nThe main weaknesses of this paper are the lack of detail regarding GoalRand. \n\nThe following points would benefit from clarification:\n\n0) What RL algorithm was used to train GoalRand?\n1) How is the knowledge graph populated/constructed from step to step? Is it trained to parse the observation into a <subject,relation,object> tuple or does it use hand-crafted rules?\n2) How are goals extracted from the knowledge graph? In Section 4.2 we learn that goals are generated by common-sense rules relating to collecting ingredients that are not currently in inventory and preparing ingredients according to their requirements in the KG. How does the agent know what objects are ingredients versus other items in the environment like the stove? How are the requirements for preparing ingredients understood by the agent?\n3) How is the pseudo-reward constructed for each goal - in particular the agent must know when the goal has terminated and whether the termination was successful or a failure. How does the agent determine successful criterion for each goal?\n\nIn short, while the authors describe a main contribution of the paper as an agent learning without rewards, however it's unclear how much of the learning ability of this agent comes from built-in heuristics surrounding the creation of the knowledge graph, the extraction of goals, and the construction of pseudo rewards. At the minimum I think the authors need to be much more clear on exactly how much heuristic knowledge is involved in steps 1-3 above.\n\nUpdate after author response: I appreciate the author response and updated manuscript to include necessary detail. I think the amount of prior knowledge that is incorporated in the agent is a significant drawback of this approach - and one that would prevent it from being applied beyond the TextWorld Cooking games. Additionally, it makes comparisons to other baselines difficult. As the authors suggest it may be possible to replace some of this domain knowledge with learned components, which might yield a more general agent. Score updated accordingly.",
            "summary_of_the_review": "In the current form, I believe the paper is missing details that are crucial to understanding the agent and evaluating how much prior domain-specific prior knowledge was used to build and utilize the knowledge graph and goal sets. I'm hopeful the authors can provide these details in the review process and I'm open to adjusting the scoring of the paper accordingly.\n\nUpdated",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}