{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work presented imitation learning algorithms with Self-Organizing Generative Model (SOG) that can mimic multi-model expert behaviors  better than prior works' algorithms such as Info-GAIL and VAE-GAIL. The key characteristics of SOG-based imitation learning algorithms is that they are encoder-free and the latent codes are genererated from the latent distribution that is fixed before training, whereas VAE-GAIL is based on the encoder-decoder architecture used in VAE. Instead of training encoders, SOG-based algorithms greedily select the latent code among the multiple samples generated from the fixed latent distribution. Specifically for any given sample (the sample here is the state-action pair in the imitation learning), the proposed algorithms choose the best latent code such that the reconstruction loss is minimized. The contribution seems mostly on the empirical results although the analogy to Expectation-Maximization (EM) algorithm is given. SOG-BC and SOG-GAIL that respectively combines SOG with BC and GAIL are tested for scenarios where either discrete or continuous latent variables are assumed. The empirical results support the authors' argument saying that SOG-BC and SOG-GAIL outperforms their baselines in terms of preciseness on separating expert demonstrations' modes, i.e., diverse behaviors of experts can be more effectively imitated.",
            "main_review": "I think the contribution of this work is clear. I have only a few comments which I believe will improve the submission's quality.\n\n- In Section 2.2 regarding the background on imitation learning, I think the desciption for TRPO and PPO are redundant and needed to be moved to the appendix if it is necessarily required. Instead, putting more detailed explanation on existing encoder-free frameworks will be helpful for readers since I believe the readers interested in this work are not likely to understand those frameworks. \n\n- When I read Section 3.1, whether SOG is a new idea or it already existed was unclear to me. In either case, I think the contribution of this work is sufficient, but I would recommend authors to clarify this.\n\n- I think the readability for Section 3.3 and 3.4 should be improved. We can move Equation. (6) right below the sentence \"Although EM or variational inference methods ... Equation (6) has several implications.\". For the paragagraph starting with \"Points can be made:\", I would rather use \"enumerate\" in LaTeX and separate each items. How we can reduce SOG into VAE by using local latent code improvements is unclear (although it intuitively seems correct). In Section 3.4, I think more detailed and kind explanation for EM algorithms would be helpful for readers. Maybe we can move some contents in Appendix B to the main context.\n\n- In the experiment, Table 2 is not refered in the text (which I believe should be refered at the paragraph below Figure 2). Also, I don't know why higher mutual information means better performance. (Intuitively, higher mutual information means more indepedent latend codes are generated, but why should this represent SoG's performance in this experiment?)\n\n- In Table 1, although I know that considering the episodic rewards is a conventional way to evaluate the imitation learning agents, can we think of using other kind of quantitative measure? For example in the paper \"Primal Wasserstein Imitation Learning\", we can evaluate the imitation learning agents' performance by using sample-based Wasserstein distance.\n\n",
            "summary_of_the_review": "The empirical results in this work are interesting, but some parts should be enhanced for better readability.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a self-organizing generative (SOG) model for imitation learning from expert trajectories with multi-modal behaviors. The multi-modal output data y is assumed to be generated from a conditional distribution following a form of Gaussian distribution with its mean mapped from the input data x and a latent variable z. The main idea is to find a latent code that minimizes the reconstruction error of y; then use the latent code to train the mapping function. SOG generalizes to both discrete and continuous latent variables. Its effectiveness has been evaluated through theoretical analysis and empirical experiments on several robotic locomotion tasks. ",
            "main_review": "Pros:\n\n1. The multi-modal imitation learning setting is practical. The proposed method SOG can be directly adapted to multi-modal behavior cloning and can be generalized to GAIL by combining behavior cloning. \n2. The relations between SOG and classic maximum likelihood estimation methods (EM and VAE) are established. Compared with EM model, SOG shows faster convergence speed. Compared with VAE, SOG is encoder-free, and directly trains decoder by searching for a latent code that maximizes the likelihood. \n3. The proposed method is evaluated on both discrete and continuous locomotion tasks, showing superior performance than baselines. The experiments also shows some robustness of SOG for unseen states. \n\nCons:\n\n1. It is nice to see that the term “self-organization” is justified. However, the visualization results in the appendix are not related to the imitation learning tasks in the paper. It would be better if the authors can show the self-organization results corresponds to the MuJoCo tasks that used in the experiments. Another problem is that the reason why the “self-organization” emerges in Algorithm 1 is not well explained. In other worlds, why can the latent codes corresponding to nearby data points get organized close to each other? \n2. The experiments covers both discrete and continuous latent variable, however, both of them are low-dimensional. It would be better if the proposed method can be evaluated in high-dimensional environments, such as Atari games for discrete variable, and DeepMind Control Suite for continuous variables. Is the proposed method still computationally tractable in high-dimensional experiments? \n\nMinors:\n1. In the last line of section 3.1: ... find a z such that \\mathcal{L}(y_i-f_\\theta(z, x_i)) is minimized...  ==>  ... \\mathcal{L}(f_\\theta(z, x_i), y_i) is minimized ?\n",
            "summary_of_the_review": "Overall, I vote for a weak acceptance. The multi-modal setting is novel and practical in imitation learning. The proposed method is simple and its effectiveness and robustness has been well justified through both theoretical analysis and diverse experiments. I hope to see further improvements by addressing my above concerns in the rebuttal.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The submission proposes a version of VAE-GAIL (Wang’17) that uses hard (i.e. deterministic) posterior distributions. It is argued that the algorithm converges to the optimal solution even though using hard posteriors. Empirically, the method outperforms InfoGAIL and VAE-GAIL on continuous control.",
            "main_review": "The submission proposes a reasonable method that is a small and known variation on existing methods. Empirical results show an improvement in performance, however, do not analyze the mechanism for this improvement. Only a hypothetical explanation is provided, which is that deterministic posteriors encourage better mode separation and more stable training. The paper suffers from the following key issues\n- The SOG model is not novel, despite the claim in Section 3.1. Models of type have been previously used extensively, e.g. see Vondrick’16, Li’18.\n- The theoretical analysis does not show the model converges to maximum likelihood, despite the claim in the introduction. The analysis in Sec 3.4 simply restates the definition of EM while remarking that one can use hard posteriors in practice. There is no proof that the method optimizes likelihood in the paper.\n- Perhaps the most confusing is the appendix section B.2.1. The main paper claims it contains the missing proofs, but the section is titled “TOY EXPERIMENT RESULTS”. While there are some intuitive arguments, I can’t see any proofs in this section. \n\nLi’18, Implicit maximum likelihood estimation\nVondrick’16, Anticipating visual representations from unlabeled video",
            "summary_of_the_review": "As discussed above, it appears that the claimed contributions in point 1 of the final intro paragraph are not realized. The paper does not propose a novel model, and does not show that it optimizes likelihood. The novelty of the paper is very limited, and the experimental results are not extensive enough to compensate for that. For these reasons, I am unable to accept the paper. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}