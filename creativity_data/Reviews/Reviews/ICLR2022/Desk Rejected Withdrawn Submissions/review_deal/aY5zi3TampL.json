{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper considers the problem of estimating causal effects (atomic interventions) under the canonical conditionally ignorable model. The authors use neural network-based outcome modeling, following several papers in the \"causal learning community\". The authors employ a novel loss function that aims to re-cast input covariates into a representation that achieves balance between the treatment and control groups. While this is a novel way to employ balancing and an intuitive use of representation learning, it is not clear what this method adds relative to simpler balancing + outcome model methods (like AIPW) that achieve balance directly/explicitly, have theoretical guarantees, and are generally less data hungry than neural network models.\n\nBeyond this primary concern, aspects of the paper are somewhat poorly written. For instance, the language that attempts to justify combining balancing with outcome modeling is unclear on why this might be a good idea. The paper needs to be cleaned up substantially.",
            "main_review": "Point by point comments numbered according to approximate order of appearance in the paper:\n1) The authors' approach entails fitting separate models for treated and control groups. I am curious what their reason is for doing so? Generally there is some covariate information shared between the two groups and it is more statistically efficient to fit one model (i.e. regress Y on T and X and then plug in values of T when actually estimating) rather than two.\n2) \"Real World Data\" -- I think it's more appropriate to use the expression \"observational\" data. RCT data is real and implying it is not is just going to confuse readers.\n3) Throughout the citation formatting is done poorly. In many places a text citation is used where a parenthetical makes sense from context and it makes the flow of the paper very stilted. I'd advise alternating between \\citep and \\citet where appropriate.\n4) \"Footnote 1: BCAUSS roughly stands for 'Balancing Covariates Automatically using Self-Supervision'\"? It either does or it doesn't. It's your method. There's no need for this footnote and the authors maybe shouldn't try so hard to come up with a buzzword name for their method if they have to say it \"roughly\" stands for some backronym.\n5) Bottom of section 2: \"In our work we don't make any assumptions on the generative process of the underlying covariate and outcome distributions\". This is not true. The authors, in the next section, assume conditional ignorability and a variety of other (standard) conditions. This is not \"assuming nothing\". Perhaps the authors meant to say they don't make any _parametric_ assumptions?\n6) \"Bayesian\" conditioning. I think the authors just mean condtioning?\n7) \"It is easy to see that x is the finest balancing score\" --> what is meant by fine (or coarse) here? How can x be a balancing score if it's is conditionally independent of t given b(x)? The implication is if t and x are unconditionally independent then no re-balancing is needed?\n8) Definition 2: if phi is injective and has an inverse, why not just say phi is bijective? Are the authors verifying that their learned representation is invertible?\n9) Why use ReLU for some activations and sigmoid for others?\n10) \"incentivated\" is not a word\n11) The thrust behind using representation learning was not very clear until around the middle of section 3.2. The authors should make clearer what the goal of using rep learning is in this context. Beyond this point, it is not clear why this method should perform any better than fitting an AIPW estimator. In both cases (this method and AIPW) it is necessary to correctly specify the model. In AIPWs case, there is at least the chance to still be unbiased if one of the two models (outcome or balancing) is incorrect. Here, if the outcome model is incorrect, the balancing model will likely be incorrect too since they are fit jointly and rely on each other.\n12) The authors should add the AIPW estimator to their comparison models since it is 1) well-known in the literature, 2) has some theoretical guarantees that most of the other comparison models don't have, 3) probably will do pretty well and is MUCH simpler than fitting an over-parametrized MLP model (after all, there are only 3-4 covariates in one of the sim studies).\n13) The authors' explanation of the balancing loss is confusing. The interpretation of the g-weight terms isn't immediately obvious. One other way to phrase this might be as \"we're imposing a constraint such that the learned representation achieves balance. In practice this is achieved by minimizing the squared deviation between a reweighed treated cohort and a reweighed untreated cohort. We chose these weights because ...\"\n14) Comparison with Dragonnet: the authors note that dragonnet has some consistency results. This method does not have such results, correct? Also, the authors should argue why the BCE loss does not sufficiently impose balance despite essentially fitting a propensity score.\n15) Physical exercise data set: why do the authors switch to A corresponding to treatment?\n16) What's the distribution of the outcome in the physical exercise data set? Only the mean is given.\n17) \"In our simulation, we also consider a fourth observation variable highly depending on the decision of treatment, the improvement in physical function X4, generated as independent normal variable with mean gamma*A and sd 2.\" <-- the authors claim that a real world study would mistakenly include this as a covariate and thus it helps demonstrate that their method achieves balance. That doesn't make sense to me. X4 is clearly a post treatment (mediator) variable. A practitioner with causal expertise would _know_ not to condition on a post-treatment variable. Owing to that observation, the experiments that rely on this variable would seem not to demonstrate the behavior the authors think they are.\n18) Why us MAE rather than a squared error like PEHE? A squared residual forms the basis of your loss function so it makes sense to use it for evaluating performance.\n19) It's not clear why the authors report a different train and test performance. What are train and test here? Generally in effect estimation contexts the goal is to simply estimate the effect on the data at hand, which means using ALL of the data since that is the most statistically efficient thing to do. If any data needs to be held out (e.g. for tuning hyperparameters), it is key that sample splitting (cross-val or methods like those described by Chernozukhov and Robins) be employed. It should be sufficient to simply fit the model, evaluate the effect and compare to the ground truth that is available due to the nature of the considered data sets.\nTo be clear: the goal of causal effect estimation is generally not out of sample prediction. It is a parameter estimation problem as in classical statistics.\n20) Why are OLS/Logistic Regression only used in the Jobs data? Are the authors simply copying these results from another paper? It shouldn't be too hard to re-run those methods on the other data sets. Also, here is where AIPW should be included. In light of comment #19, it's not immediately clear that the proposed method is still \"state of the art\".\n21) Where are the Table 1-style results for the exercise/depression data set? Based on the described data generating process in section 4.1, I'd expect linear regression and AIPW to do better than the proposed NN method since the outcome is (apparently?) Gaussian and AIPW with logistic regression for the propensity score and linear regression (correctly specified here) should converge faster (i.e. attain variance = fisher info matrix) than the data-hungry NN with many thousands of parameters.\n22) Section 5.1 analysis: what was the observed Wasserstein distance between the treated and control groups (i.e. before applying the representation function g)? What is the distance when the \"representation\" is a simple re-weighing by propensity scores (observed or fit via logistic regression)?\n23) Section 5.2: See the above comment about X4 being post-treatment. It doesn't seem appropriate to claim that the analysis here is measuring/addressing the difference between treatment and control _covariate_ distributions since X4 is only a \"covariate\" under sketchy terms (i.e. ignoring the fact that it is post-treatment which one wouldn't usually do in a properly run analysis).\n24) Conclusion: the authors claim that they show their method's outperformance increases when there is a larger distance between the treatment and control groups. This is certainly not shown in the first set of experiments (i.e. Table 1), as distance between cohorts isn't considered. The second experiment talks about distances but doesn't actually measure performance of effect estimation. It is also flawed, as discussed above. Based on these points, it seems the authors can't claim this increasing or \"widening\" outperformance as a function of cohort distance.\n22) ",
            "summary_of_the_review": "See initial summary above.",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed a new representation learning-based approach to estimate the treatment effects from observational data. The approach is based on modifying the existing Dragonnet developed by Shi et al. The new approach is called BCAUSS.\n\nThere are two terms in the objective of BCAUSS: 1) the first term aims to minimize the regression loss using the factual outcomes, which is the same as those used in Dragonet; and 2) the second term aims to minimize the mean square between the g-weighted covariates within the treatment and control group. In Shi's work, the second term makes use of the binary cross entropy to minimize the distribution distance between the treatment and control group in the representation space. Therefore, the second term of BCAUSS is different from Dragonnet and the new idea of this work.\n\nExperimental results on one synthetic dataset, one semi-synthetic dataset (IHDP) and one real-world dataset (Jobs) demonstrate the proposed BCAUSS can achieve lower error in estimating the ATE and ATT.\n\nFurther analysis showed BCAUSS learns representations leading to more similar distributions between the treatment and control group compared to Dragonnet. The performance gain of BCAUSS compared to Dragonnet becomes greater when the distributions between the treatment and control becomes more dissimilar.",
            "main_review": "The key contribution of this work, i.e., replacing the binary cross entropy term in the Dragonnet with the auto-balancing term in Eq. (2), seems quite incremental. \n\nMy major complaint of this work is the lack of hyper parameter tuning for those baseline models. To fairly compare different approaches, it is essential to tune the hyper-parameters of baselines (even only Dragonnet in this case). Otherwise, it's hard to determine if the performance improvement of the proposed approach is due to a more appropriate hyper-parameter tuning or the objective function itself.\n\nIn Table 1, besides the error in estimating ATE and ATT, did the author compute the error of estimating ITE as well? If so, would the proposed approach still outperform the competing approaches?\n\n",
            "summary_of_the_review": "Novelty: Limited novelty due to incremental modifications of existing work.\nExperiments: Major flaws due to the lack of hyper-parameter tuning for baselines",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper tries to minimize dissimilarity in learning treated and control distributions in the causal treatment effect estimation. To this end, it considers an auto-balancing self-supervised objective in addition to the existing works.",
            "main_review": "1. The paper seems not novel. It reuses the concepts and neural network architecture from the existing works and its only contribution is the auto-balancing self-supervised objective.\n\n2. Even for the objective function, no theoretical analysis or background is provided.",
            "summary_of_the_review": "The paper repeats the concepts and approaches from the existing works. Then, it adds only a marginal contribution (i.e., auto-balancing self-supervised objective) to them. Besides, there are no theoretical results for it.",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed a new method for causal treatment effect estimation through self-supervised covariate balancing. The main contribution of the paper is to adopt the auto-balancing self-supervised loss function to learn the balancing score (like the propensity score).   ",
            "main_review": "1. The whole Section 3.1. just presents existing results, which looks redundant. In particular, it is not clear why Theorem 5 needs to be included here. It is completely disconnected with the proposed method. \n\n2. The major selling point of the proposed method is to use the auto-balancing self-supervised loss function to learn the weight function g. It is not clear why this is better than the binary cross-entropy loss function considered in the literature. Some discussions of the intuition or theoretical justifications should be provided. \n\n3. The asymptotic distribution of the proposed estimators of ATE/ATT and their associated variance estimation methods are not considered in this paper. \n\n4. In experiments, the tuning parameter $\\lambda_{BAL}$ was set as 1. Is this a recommended choice for all applications? Is a data-driven method necessary selecting the optimal $\\lambda_{BAL}$ in practical applications?     ",
            "summary_of_the_review": "My main concern is the novelty of the work. Compared with existing approaches, the only difference of the proposed method is to use the auto-balancing self-supervised loss function to learn the weight function g (similar to the propensity score function). However, its intuition and theoretical justifications are not provided.  ",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}