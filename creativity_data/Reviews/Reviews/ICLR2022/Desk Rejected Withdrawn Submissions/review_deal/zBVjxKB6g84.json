{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the convergence of federated learning approaches that are implemented under differential privacy constraints at the client level. The most common technique to ensure differential privacy in federated learning clips the parameter update from each client and perturbs it with Gaussian noise before sending it back to the server. In this context, the present work aims to analyze the impact that clipping can have on the convergence of the algorithm. The authors claim three main contributions summarized below.\n\n1. ***Analysis of existing clipping methods:*** The authors study two clipping methods from the literature, namely model clipping (where the client directly clips its new parameter vector) and difference clipping (the client clips the difference between the model sent by the server and its new model). The authors compare the behavior of these two methods on some toy learning tasks and conclude that difference clipping outperforms model clipping in terms of final model accuracy.\n2. ***Empirical study on the impact of difference clipping:*** According to the authors' analysis, the impact of clipping on the performance of the learning algorithm depends on the structure of the model, the distribution of the data and the closeness of the clients' updates.\n3. ***Provide theoretical results for convergence and privacy of federated learning algorithms:*** This work claims to be the first to rigorously study theoretical issues concerning clipping methods in federated learning.\n",
            "main_review": "### Overall merits of the paper.\n\nFederated learning is a learning paradigm that has recently gained a lot of visibility. As it is implemented by many practitioners, it seems important to better understand its behavior in terms of privacy and performance. Therefore, the main purpose of this article is interesting for the ML community in general. Moreover, even if I am not familiar with all the literature, the analysis of the impact of clipping on federated learning algorithms seems to be a new contribution to this field. \n\n### Concerns regarding contribution 1. \n\nMy main concern with the first contribution is its significance. The authors compare the notions of difference clipping and model clipping as if they were two standards in the literature (at least this is the feeling I got from reading the last paragraph of page 3). However, my understanding of previous works is that difference clipping is both the primary and the most classic scheme when trying to ensure differential privacy in federated learning (See, for example, [1] and [2]). Model clipping seems younger and less used; therefore, highlighting its limitations (while interesting) seems less important than advertised in the paper. \n\nFurthermore, I feel that Section 2.1 somewhat exaggerates the conclusions that can be drawn from the comparison between the two methods. As I understand them, Claims 2.1 and 2.2 indicate that 1) model clipping can, at worst, hinder convergence and 2) difference clipping can, at best, help convergence. However, the authors conclude that their results \"indicate that difference clipping should outperform model clipping in terms of convergence guarantees.\" I think that to draw such a conclusion, it would have been necessary to directly compare the two methods on a relevant class of learning problems and not on separate toy examples.\n\n### Concerns regarding contributions 2 and 3.\n\nMy main concern regarding the second and third contributions is technical quality. I detail below by separating the experimental and theoretical results. \n\n1. In Section 2.2, the authors study the impact of clipping on the convergence of federated learning algorithms in the non-private setting. While their findings may represent an interesting contribution, I think their experimental protocol is not strong enough to support them. In particular, I do not think that setting the same hyper parameters for both methods makes the comparison fair. Because of their different nature, the two methods may have very different sets of optimal hyper parameters, so it I do not think we can justify the superiority of one over the other or draw general conclusions by comparing them only on one fixed set of parameters. Similarly, the authors consider only one clipping strategy. Since the choice of clipping strategy can also have a significant impact on the final performance of the model (see for example [2]), I do not think that the paper should draw conclusions based on the observation of a single strategy. \n2. My main issue with the theoretical contribution is that the assumptions made to demonstrate Theorem 3.1 seem to be conflicting with those made for Theorem 3.2. In fact, looking at the statement and at the proof  of Theorem 3.1, the author assume that Algorithm 1 is using a sampling without replacement to build a batch $\\mathcal{P}_t$ of fixed size $P$ at every round of the learning procedure. On the other hand, Theorem 3.2 is not provided with any proof but is presented as a corollary of Theorem 1 from [3]. However, this prior result is based on the assumption that the studied algorithm is using Poisson sampling to build the batch (that can have variable sizes across rounds). From the description of the algorithm it was unclear to me whether Algorithm 1 is using sampling without replacement or Poisson sampling, but from the experimental part it seems to be the former. Accordingly, if think that Theorem 3.2 cannot hold for Algorithm 1.\n\n### Additional comments and questions\n\n1. When presenting the main theoretical result, it seems to me that it would be more readable to place the hypotheses outside the theorem and explain them before going to the result. \n2. It might also be useful to explain what the noise injection parameter $z_i^t$ is directly in the description of the algorithm so that it is self-contained. \n3. I had a hard time reading the proof of Theorem 3.1 because it seems that the authors use the same notation for all conditional expectations and for the expectation over the whole learning procedure. I would advise to introduce a specific notation for each conditional expectation to make the proof more readable. \n\n[1] Learning Differentially Private Recurrent Language Models H. Brendan McMahan, Daniel Ramage, Kunal Talwar, Li Zhang\n\n[2] Differentially Private Learning with Adaptive Clipping Galen Andrew, Om Thakkar, H. Brendan McMahan, Swaroop Ramaswamy\n\n[3] Deep Learning with Differential Privacy Martín Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang\n\n\n",
            "summary_of_the_review": "Overall, I think this paper investigates an interesting question but, as it stands, I think that the claims are not sufficiently well supported by theory or experiments. In particular, I have serious concerns about the collision between the assumptions made in Theorems 3.1 and 3.2 and the shortcomings of the experimental setup. Accordingly, I will advocate for rejection.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper discusses differentially private algorithms in federated learning, specifically DP-FedAvg. The paper focuses on client-level DP, where the privacy granularity is defined by client data. There are several interesting findings:\n+ The authors compared two different variants of clipping for FedAvg: clip the client model vs. clip the client model difference, and claim difference clipping is analytically better. \n+ With some experimental results, the author shows the non-IID distribution in FL is more challenging for clipping than IID distribution. \n+ Convergence rates of DP-FedAvg are provided: under common assumptions in FL, and when clip norm is set to be large, the convergence rate recovers the well-known FedAvg rate up to constant with an extra term for privacy noise. \n",
            "main_review": "This paper has a few interesting discussions,  but it is not quite clear to me what the main contributions are. I would appreciate it if the authors could clarify. More specifically, \n- I like the discussion of clip the client model vs. clip the client model difference. However, it is a bit hard to tell the value of Claim 2.1, which provides a smart counterexample by choosing the clip norm to be smaller than the norm of the optimal model. \n- The convergence rate seems to be standard without privacy noise. See more detailed comments on the noise term below. \n- It is hard for me to tell what is new in the empirical results. \n\n\nAn important  related work is missing: [McMahan’17 Learning differentially private recurrent language models]. McMahan’17 is a concurrent  work of Geyer’17, which proposed DP-FedAvg and provided extensive experiments. Another potential related work is [Andrew’21 Differentially Private Learning with Adaptive Clipping], which provides extra experimental results on DP-FedAvg. \n\n\nMore questions on theory:\n- Could you elaborate more on Claim 2.2 “there exists a linear regression problem such that under the same c, \\eta_l and Q, FedAvg with difference clipping converges to a better solution with smaller loss than the original FedAvg.”? I may have missed it but I did not find any support for this claim.  A minor comment, Q==1 is just minibatch SGD, so probably should argue for Q > 1.\n- For Theorem 3.1, I may have missed the definition of P? The (1/\\eta_l) term for privacy noise is very hard to understand. Could you provide an intuitive explanation on where it comes from?\n- In Corollary 3.2.1, I guess the bias will disappear because the clip norm is chosen to be so big, but it also makes the first term of eq (5) not significant. The noise term in eq (5) is O(T), which seems to be pretty big and not really converging?\n\n\nMore questions on empirical results:\n- In Table 1: how are the fixed hyperparamters chosen? And why fixing the hyperparameters is a fair comparison?\n- The authors mention results on Shakespeare NLP task, but I did not find them in the draft. \n- The legend of Figure 1 & 2 are confusing, could you clarify what blues docs and black dots are?\n- In section 4, the authors report privacy budget \\delta, but what is \\epsilon, and the corresponding noise multiplier? How are hyperparameters chosen in this section?\n\nA minor issue:\nI think “user-level DP” is a more popular name than “client-level DP”, for example, see [Kairouz’19 Advances and open problems in federated learning].\n",
            "summary_of_the_review": "\nThis paper has a few interesting discussions,  but it is not quite clear to me what the main contributions are. I would appreciate it if the authors could clarify. More specifically, \n- I like the discussion of clip the client model vs. clip the client model difference. However, it is a bit hard to tell the value of Claim 2.1, which provides a smart counterexample by choosing the clip norm to be smaller than the norm of the optimal model. \n- The convergence rate seems to be standard without privacy noise. See more detailed comments on the noise term below. \n- It is hard for me to tell what is new in the empirical results. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper analyzes Federated Averaging with model update clipping and differentially private perturbation. The paper first conducts experiments to show that the update clipping influences the model performance more when data distributions among devices are Non-IID. Then the paper establishes convergence bounds for DP-FedAvg (FedAvg with clipping and privacy noise). ",
            "main_review": "Strengths:\n\nThe paper is well written with a comprehensive summary of the prior work. \n\nThe first claim is that difference clipping performs better than model clipping. The authors construct a concrete example to show that model clipping fails. The paper conducts experiments to show that clipping degrades the model performance more when data distributions are Non-IID, and this is caused by the difference in the updates of different devices. For the DP-FedAvg algorithm, the authors establish convergence bound for converging to first order stationary points, and also provide differential privacy guarantees.\n\nWeakness:\n\nI have several concerns about the significance of the work:\n\n- About the model clipping and difference clipping: It should be not hard to see that clipping the model update is effectively changing the learning rate of the client update, while shrinking the entire model generally does not work unless you assume some structure in the parameter space. \n\n- About the influence of clipping on the performance: Clipping the model update can be viewed as changing the learning rate of the client update. If the data are IID, it is not hard to see that using different learning rates for different devices does not influence the convergence. When the data are Non-IID, different learning rates would result in \"client drift\" and influence the convergence. e.g., See [1] for details.\n\n- About the convergence guarantee: The two terms caused by clipping in the upper bound should be further investigated since this is the main contribution of the result. In the paper, the two terms depend on some vague quantities (those $\\alpha$'s) and it is hard to see how small are they.\n\n- About the assumption: It is very strong to assume that every stochastic gradient is bounded, though doing so will simplify the analysis. Therefore I would think of $G$ as a very large number, which means the two $O(1)$ terms caused by clipping will be also large. Also, the issue of $G$ is somehow ignored in Equation (5). \n\n[1]. Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization",
            "summary_of_the_review": "I have several concerns about the significance of the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigates the effects of adding Differential Privacy to Federated Learning (FL). The investigation looks at two crucial parts of adding DP somewhat separately: clipping and noising. First the authors empirically examine different methods of clipping, and offer insight into how impactful clipping is and what affects the level of impact. The authors then look at the additional affect of adding privacy providing noise, and derive a convergence bound for FedAvg under clipping and Gaussian noise. ",
            "main_review": "Strengths:\n- Empirical exploration of the relationship between clipping and gradient diversity provides interesting and actionable insight into the impact of adding DP to FL.\n- Theorem 3.1 decomposes the impact of adding DP intuitively, with the authors identifying terms for clipping and noise. The different terms correspond to the clipping and noise respectively, and they provide insight into what can be mitigated (and how) and what is inherent. \n- Valuable experiments showing the relative importance of clipping and noise in practice.\n- Entire paper is easy to read and understand. Proof of Theorem 3.1 is well written and easy to follow. \n\nWeaknesses:\n- Proof techniques for Theorem 3.1 appear relatively standard, using techniques and inequalities which are quite similar to other SGD convergence based proofs. Additionally the theorem makes standard but very strong assumptions (almost surely bounded gradient, bounded variance etc). Proof techniques are being developed which do not require such strong assumptions, and so such strong assumptions may not be needed.",
            "summary_of_the_review": "The paper gives a thorough investigation into DP-FedAvg with useful empirical and theoretical results. A proof with fewer assumptions may be possible, but this can be future work and should not prevent the paper from being published as is.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}