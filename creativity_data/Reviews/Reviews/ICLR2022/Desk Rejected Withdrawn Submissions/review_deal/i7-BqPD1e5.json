{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper describes a new adversarial attack setting called “Generalized Transferrable Attacks” (GTA). The goal of GTA is to create transferrable adversarial examples in the black-box setting without query access and without access to surrogate models trained on the target data distribution. The paper proposes a novel attack called “Image Classification Eraser” (ICE), which allows the creation of transferable examples for any arbitrary dataset. ICE trains a “universal model” that has good adversarial transferability for surrogate datasets using a meta-learning objective and uses this model to create transferable adversarial examples.",
            "main_review": "\n**Strengths**\n1. The problem setting of Generalized Transferrable Attacks is well motivated and novel.\n2. I found the solution of creating a universal surrogate model using a meta-learning objective to be interesting.\n3. The paper is well-written and the figures make it easy to follow.\n\n**Concerns**\n\n1. Evaluation against Defenses\n\nThe proposed attack has not been evaluated against any defenses. Given the novelty of the threat model, I don’t think this is sufficient to disqualify the paper. However, I would have liked to see the proposed attack evaluated against simple defenses such as adversarial training.\n\n2. Choice of loss function for baseline evaluations\n\nThe adv. example for the baseline evaluations is created by maximizing the entropy of f(\\hat{x}). While this does maximize the uncertainty in the model’s predictions, it does not maximally change the model’s prediction (which is the goal of creating adv. examples). Section 4.5.2 tries to address this concern by treating the model’s original argmax prediction: argmax(y) as the pseudo label. However, the authors could have taken this idea a step further by considering the model’s original softmax prediction (y) as the soft pseudo label and maximizing the KL divergence between y (the model’s original softmax predictions) and f(\\hat{x}). Did you consider using this alternative loss function for creating adv examples? I think the effectiveness of the baseline attacks would improve with this alternative loss function. Can you justify this choice of loss function for the baseline evaluations?\n\n\n\n3. Baseline evaluation with multiple source models: \n\nThe authors suggest using the following method for creating adv examples for the baseline case with multiple source models:\nCreate adv examples individually for each of the source models\nAverage the adv examples from step 1 to create a single adv example.\nI’m not convinced that the averaged example (from step 2) remains adversarial for all the source models. This also possibly explains why the results for baseline do not improve with additional source models. A better way to create adv examples for multiple models would be to optimize a single image and make it adversarial across the ensemble of source models. \n\n**Nit**\n\nAbstract: “differente datasets” -> “different datasets”",
            "summary_of_the_review": "I have some concerns about the evaluations in this paper. However, I recommend that the paper be accepted mainly because of the novelty of the threat model.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper considers untargeted blackbox transfer attacks, in particular in the relatively underexplored scenario where the target model’s dataset, classes, input resolution are unknown. This paper introduces ICE, whose universal surrogate model is trained to successfully disrupt an ensemble of whitebox models, which may have various architectures, input resolution, or data distribution. By doing so, the hope is that the universal surrogate model can produce adversarial attacks that generalize well, despite not knowing the aforementioned target model characteristics. The effectiveness of the approach is demonstrated by considering various combinations of datasets (CIFAR-10/100, TieredImageNet) and CNN architectures, outperforming a number of baselines adapted to the setting.",
            "main_review": "Pros:\n-\tConsiders an underexplored, challenging, and realistic setting in adversarial attacks\n-\tFlexible to input/output dimension, labels\n-\tFormulation/training of universal surrogate model is reasonable and fairly clever; I don’t think it’s something I’ve seen before in the attack literature\n-\tProposed method can take advantage of multiple models/datasets to get better performance while the baselines cannot.\n-\tRelatively thorough experiments comparing against several baselines across multiple model architectures and datasets\n-\tClearly written and presented\n\nCons:\n-\tOverstated claims of novelty of the problem setting\n-\tAn ensemble of source models during training increases computation cost\n-\tUniversal surrogate model must be trained, which adds extra prep and complexity versus just using an off-the-shelf pre-trained whitebox model.\n-\tUntargeted attack, which is easier and provides less control to an attacker than targeted attacks.\n-\tEthics statement is overly simplistic\n\nDetailed Comments\n\nNovelty of the setting:\nWhile it is true that the prevailing assumption in previous blackbox transfer attacks, the claim that *all* methods assume that the white-box model is trained on the same dataset as the blackbox model is not true. For example, [1] considers the setting where the blackbox and whitebox models share none of the training samples, and [2] explores settings where there are no overlap in labels. This setting has also been referred to as “no-box” or “data-free” as well [3,4]. While I’m not saying the existence of prior work necessarily render this work completely moot, the claims being made nonetheless need to be adjusted: in particular, the authors should be careful with the use of the words “all” and “none”, and discussions/empirical comparisons (“no existing methods can be directly used as baselines”) of these previous works should be added.\n\n\nExperiments:\n1.\tOverall, the experiments are fairly well-designed, demonstrating that the proposed method can effectively take advantage of both additional datasets and architectures, which the compared baselines cannot. The proposed method outperforms the baselines at untargeted attacks. Ideally, there would also be comparison with other similar methods: e.g. see comments above on novelty of the setting, or knowledge distillation below.\n2.\tWhile they’re not identical, there’s a decent amount of overlap between the labels of CIFAR10 + CIFAR100 and TieredImageNet. I wish the authors had better teased out the relationship between label overlap and overall attack performance, or considered datasets with less overlap (e.g. CUB birds -> cat and dogs, or animals -> cars or planes).\n3.\tHow does the universal surrogate model compare with a model that learns to combine sources through knowledge distillation? \n4.\tI’m not sure if the proposed method of adapting GTA to the baselines necessarily shows them in the best light. In particular, resizing and averaging the adversarial samples may suppress their individual perturbations and mess up their effectiveness. Why not optimize for a single adversarial example that maximizes the entropy for all the networks?\n\nEthics Statement:\nThere’s an extensive literature when it comes to adversarial attacks, and the potential negative societal impacts of adversarial attacks have been discussed quite a bit. This work is not unique in exposing such concerns, but I do think the authors should spend a little more time discussing the potential impact of their work, especially since the proposed attack setting is (relatively) new. Claiming “no potential negative societal impacts” doesn’t demonstrate very much thought.\n\nMiscellaneous:\n1.\tCiting just MAML (Finn 2017) for the introduction of the proposed “bi-level training framework” is an odd choice. I think the authors are trying to draw similarities between the proposed method’s inner and outer loop with meta-learning, but there are other meta-learning methods that are perhaps a closer analogy than MAML, which optimizes model weights in the inner loop (ICE does not).\n2.\tThe numbered step-by-step description in the latter half of Section 4.1 is a little sloppy, with a mix of complete sentences describing what the authors did and imperative sentences. I’d recommend reformatting all to the former. \n3.\tSection 4.4: “sbusection\"\n4.\tTable 4 out of order compared to Section 4.5\n\nOther questions:\n1.\tVisually (Figure 5), the proposed attack appears to generate different looking noise when compared to the other methods. Any thoughts why this might be the case?\n2.\tWhat is ICE’s advantage over the baseline methods when using a single model and dataset (i.e. the first set of rows in Table 2)? The performance seems significantly better, even without taking advantage of multiple models and datasets. How does ICE compare to state-of-the-art in the more standard query-free blackbox setting?\n\n\n\n[1] “Perturbing Across the Feature Hierarchy to Improve Standard and Strict Blackbox Attack Transferability”, NeurIPS 2020\n\n[2] “Can Targeted Adversarial Examples Transfer When the Source and Target Models Have No Label Space Overlap?” ICCVw 2021\n\n[3] “Practical No-box Adversarial Attacks against DNNs”, NeurIPS 2020\n\n[4] “Data-Free Universal Adversarial Perturbation and Black-Box Attack”, ICCV 2021\n\n\n=====Discussion Phase Update=====\n\nI thank the authors for all the effort they put in during the rebuttal phase, especially for all the new experiments that were done. I bump my score up slightly to marginally above the acceptance threshold, though I do think this paper could also still benefit from another round of review. A good deal of my concerns have been addressed, but a lot of work still needs to be done on the paper. Regardless of what the final decision is, please do try to work in the suggestions made here by the various reviewers for the next version.",
            "summary_of_the_review": "Overall, this paper explores a challenging and practical attack setting, proposing an interesting and reasonable method to achieve good untargeted attack rates. While it is indeed underexplored, there are repeated overclaims on the novelty of the proposed setting. Regardless, even though the problem setting isn’t novel, the proposed method suffices as a good contribution. I question how fairly the baselines are portrayed, and some other important baselines are missing, but overall the experiments are well-designed and get good results. I rate this paper just under the threshold, but I’m willing to raise my score if my concerns are addressed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper studies a new setting for adversarial attacks: Generalized Transferable Attack, in which the adversary has no source models trained on the same dataset as the victim model. The new setting is more realistic compared with existing transfer-based attacks. The Image  Classification Eraser (ICE) method is then proposed to generate adversarial examples under this setting. A universal model is trained as the surrogate model, and meta-learning algorithm is developed to train the model. Experiments on several datasets show the effectiveness.",
            "main_review": "Strength:\n\n- A new adversarial attack setting is proposed to reduce the knowledge of the victim classifier. The adversary needs to generate adversarial examples without access to the training datasets. This setting could be more realistic in practical application.\n- To handle this problem, a new attack method is proposed, which is designed to address two problems: 1) the label of testing images cannot be predicted; 2) the shape of testing images is different. A meta-learning algorithm is develop to train the universal surrogate model.\n- Initial experiments on several datasets validate the effectiveness of the proposed method.\n\nWeaknesses:\n\n- The main weakness lies in the experiments. Three datasets (CIFAR-10, CIFAR-100, TIered) are used, but these datasets would be quite similar in terms of visual contents, i.e., they contain images of general objects. However, a more realistic setting would be using the datasets with different/diverse visual contents, including digital image classification (MNIST),  face recognition models, etc. Using the diverse datasets can show whether the adversary can succeed under the proposed setting.\n- As the paper proposes a meta-learning algorithm, it is better to formulate the bi-level optimization problem to make it clearer.\n- It is not clear why the authors use MTA (Eq. (2)) to craft adversarial examples during inner optimization. Can other attacks (e.g., FGSM/PGD) be similarly used?\n- Why do you adopt a universal surrogate model? Can you use some kinds of generative models (which input original image and output adversarial one) to generate adversarial examples? \n\nA related work is missing: \"Enhancing Cross-Task Black-Box Transferability of Adversarial Examples with Dispersion Reduction\", CVPR 2020.",
            "summary_of_the_review": "This paper proposes a new attack setting and a novel adversarial attack method under this setting, but it has some limitations in experimental evaluation and clarity. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes an algorithm, Image Classiﬁcation Eraser (ICE), to perform transfer-based adversarial attack across different datasets. ICE involves a meta learning to learn to model on which performing adversarial attack leads to a generlized adversarial example for different datasets. The algorithm is mainly based on MTA, while additional techniques are also proposed to solve the problem of inconstitent labels and input sizes. ",
            "main_review": "Pros\n1. Performing adversarial attack across datasets is a practical problem. For practical models used for the real application, the training set is actually not available to the attacker. And the attacker needs to attack the models trained with domain shifts.\n\n2. Using the meta learning to perform the attack across datasets is natural and reasonable. After seeing different datasets, the trained model may be able to learn the invariant features across the datasets so that it can transfer to the unseen datasets.\n\n3. The experiments do show that ICE leads a large improvement compared with the baselines.\n\nCons.\n1. The authors miss some important baselines that also focus on the attack across datasets [1, 2]. [1] studies the targeted attack with non-overlapping labels. It uses FDA and the pesudo labels to transfer the adversarial examples to different dataset. [2] shows the self-supervised training can be used to train a model to perform transfer-based attack. In this setting, one only needs to collect a few images of the target domain to train a self-supervised model. The model can be used as a surrogate model as trained with the classification loss on the target domain. These two methods should be compared with ICE.\n\n2. The paper only evaluate the effectiveness of ICE on un-targeted attack. It seems that the entropy the ICE uses is not able to perform targeted attack. Targeted attack has lower transferablity and it is more important than the un-targeted attack.\n\n3. ICE should be evaluted on the larger dataset like ImageNet.\n\n4. In section 4.1, the assumption that the catagory of $x$ is unavailble is unreasonable. For the given image, we can classify the image with human eyes. Therefore, the strongest baselines should use this information to peform attack with real label. For the models that do not have the grouthtruth label of $x$, entropy loss may be a substitute loss.\n\n5. The technical contribution of the paper is limited. ICE is technically similar to MTA. They both use meta learning and customized PGD. The difference is choice of tasks. MTA chooses different networks as different tasks for meta learning while ICE chooses different datasets as as different tasks for meta learning.\n\nMinors\n1. In section 4.4, sbusection should be subsection.\n2. As the paper is discussing an adversarial attack, which may bring safety issues to the current machine learning systems, it has some potential negative impacts.  The authors should discuss more about the ethics issues in the statement.\n\n[1] Nathan Inkawhich et.al. Can Targeted Adversarial Examples Transfer When the Source and Target Models Have No Label Space Overlap?, ICCV 2021\n[2] Qizhang Li et.al. Practical No-box Adversarial Attacks against DNNs, NeurIPS 2020",
            "summary_of_the_review": "The paper works on a interesting problem with a reasonable algorithm. The experiments show that ICE has large improvement compared with some baselines. However, the technical novelty of ICE is limited and the empirical evaluation is incomplete. Therefore, I recommend for rejection at the current phase. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Potentially harmful insights, methodologies and applications"
            ],
            "details_of_ethics_concerns": "The paper proposes a more practical adversarial attack methods. It may bring potential threats to the machine learning systems. While the attack can be useful to evaluate the security of DNNs, it could also be used to attack some undefended models.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work points out one of the limitations of transfer-based attacks. Namely, the assumption therein is usually unrealistic as the attacker may not know the dataset used by the victim model. To overcome the limitation, this work proposes to build a virtual surrogate model with multiple datasets and models. The process to build the surrogate model is formulated as meta-learning process.",
            "main_review": "Strengths:\nThe authors work on an interesting question, which is of importance to the community.\nThe meta-learning based formulation as a solution is novel to me.\n\nWeaknesses:\n1. The biggest concern for me is the efficiency of the proposed approach. To build such a ‘virtual’ surrogate model, the authors have to train K models on N datasets respectively. The building process can be expensive. I guess that that is why there don’t test and report the results on ImageNet-1K dataset. However, almost all other approaches listed in this paper work pretty well on ImageNet-1K dataset.\n\n2. The effectiveness of the proposed ICE can be affected by the following factors: model architecture difference, datasets, label sets, and input sizes. The impact of all these factors should be studied. ICE could be very sensitive to some of them. Especially, the authors report the results on CIFAR10 dataset where the datasets used to build surrogate models is overlapped with the target one. To what extent the overlap will affect the final transferability?\n\n3. The authors have a good summary of different attack approaches in Tab. 1. Regarding this table, I have a question about the input resolution. The author states that other attack approaches require information about the input resolution while the proposed one does not. Why it is so? For other approaches, they can create transferable adversarial examples at a different scale. I assume it is the responsibility of the target model to resize the adversarial examples to a specific size (which is true in real-world applications). If it is not the case, what size of adversarial example the proposed ICE should create to attack the target model?\n\n4. In the experimental section 4.5.4, the study shows that the performances of ICE will be damaged by the increase of the number of gradient ascent steps. What about if we use more attack iterations? Is the transferability of the adversarial examples decreased further?\n\n5. In some cases, the authors also report the results on epsilon= 8/255 (a popular setting). Most experiments are conducted on the setting where the perturbation range is epsilon= 15/255. Is there a particular reason for this choice?\n\n6. In Figure 5 in the appendix, the authors visualize the created noise created by ICE. They are visually different from the ones created by others. Does the difference tell us anything about the effectiveness of the proposed method? Can the author comment on why it is different?",
            "summary_of_the_review": "The paper works on an interesting problem, which is also relevant to ICLR community. The proposed solution is also novel to some degree. However, the limitation of the proposed approach is not addressed. The ablation study on the proposed method is not comprehensive enough. Hence, I rate this paper blew the acceptance threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}