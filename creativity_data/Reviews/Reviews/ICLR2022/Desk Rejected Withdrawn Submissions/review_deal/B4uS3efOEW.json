{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "**Summary of the paper**\n\nThis paper exploits the early learning effects of DNNs to prevent memorization of the wrongly labeled examples when trained on datasets with label noise. Different from previous methods that directly use the confidence of the model's predictions to identify mislabeled samples, this paper introduces a separate branch that is added to the original model to assign \"large values\" to the correctly labeled examples and small values to the wrongly labeled examples.  Moreover, an auxiliary regularization term is introduced to \"address\" the problems occurred when the number of classes is large. The paper also includes details about the estimation of the targets $t$.",
            "main_review": " **Strengths** \n\nThe proposed methodology is simple yet effective, and easy to understand. The experimental performances on various datasets including simulated and real data sets show improved robustness on label noise. \n\n\n**Weaknesses** \n\n- One of the concerns is that the authors argue that the separated branch is needed because the model will eventually memorize the noise label. However, memorizing noise labels doesn't mean the confidence score would be very high for those examples since memorization is in terms of accuracy not in terms of calibration. More evidence needs to be provided to show a separated branch is actually a needed part that results in better performance. \n\n- The author includes the auxiliary regularization term that is very similar to SL loss proposed in [1], the theoretical analysis of the regularization is not novel either as it basically follows a very similar approach as [1]. The authors claim this regularization is used to address the issue that $\\tau$ is similar to both clean and noisy samples when the number of classes is large, however, the reviewer cannot see why this reverse cross-entropy is able to address that. \n\n- The theoretical analysis of the paper is also only on the regularization term which is very similar to [1]. However, the training is done on both the proposed CAL loss and the regularization, it would be nice if the author can do some theoretical analysis on their mainly proposed loss function. Theorem 1 and 2, as well as their proof, should not deviate too much from what has been proved in [1] and related work, thus the reviewer is also concerned about the novelty of the theoretical analysis.\n\n- The ablation study shows that the main improvements come from label correction, it would be interesting to show how much the separated branch and $\\tau$ plays the role here. \n\n\n[1] Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross-entropy for robust learning with noisy labels. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 322–330, 2019.",
            "summary_of_the_review": "This paper is a good follow-up paper in the line of using early learning to address memorization when learning with label noise.  The experimental results show good results under various settings. However, the reviewer has concerns mainly focused on the novelty of the presented method: the CASE loss is essentially linearly combining the target $t$ and the prediction $p$, the regularization term is essentially reverse cross-entropy loss, and the target estimation is an upgrade version of the previously used exponential moving average in ELR. The paper would be stronger if the author can perform more analysis of their main contribution -- a separate branch and the cal loss. Therefore I give an 'OK but not good enough' rating. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposed a method called CAR to train deep models under noisy labels. CAR measures the reliability of each sample’s label using an indicator branch to the original prediction model and then trains a prediction model with a newly proposed loss function and a label correction technique. The authors provided several theoretical results mainly involved with the gradient investigations of the loss function to justify their method. The authors also gave experimental results with both small and large datasets and showed the superiority of CAR. ",
            "main_review": "Pros.\n- The idea to directly assess the confidence of a sample’s label is novel. \n- The paper validates their method both theoretically and empirically. \n- The suggested method gives enhanced performance than other baseline methods. \n\nCons and questions. \nAs I noted, CAR gives competitive or the best performances compared to other existing methods and is computationally efficient, but I have some concerns about this method. \n- First, this method has too many hyper-parameters to be tuned. Their optimal combination highly depends on the data to be analyzed. Besides, as can be seen in Figures 6&7 in Appendix, some of the tuning parameters (e.g., lambda and beta) are very sensitive that just a slight change of them (e.g., increasing lambda from 0.5 to 1.0) leads to dramatically degraded test accuracy, even reaching 0%. Thus, I am worried about its practical usefulness when applied to other data sets. \n- Second, the theoretical investigations in Section 4.1 are not convincing. CAL and CAR are devised to overcome the limitation of the conventional small-loss method by adopting the concept of confidence for each sample’s label into the loss function. So, if I understand correctly, the confidence score, tau, is the most important element in this method. In this sense, the authors should have discovered how the confidence score acts differently depending on label cleanness. However, in the theoretical part, the authors regarded the confidence score as a constant and only concentrated on analyzing the gradient of the pre-softmax nodes. \n\nAnd the followings are minor questions/comments.\n- Is the auxiliary regularization term introduced in Section 3.3 effective indeed? The results in Table 4, Figures 6, and 7 indicate that using this term hardly improves prediction models and sometimes worsens them.\n- The loss function is too complicated to understand intuitively. It would be better to add some explanations of how each element composing the loss function is developed.\n",
            "summary_of_the_review": "The proposed method is too complicated (eg. too many regularization parameters) and thus is hard to be\nevaluated. More intuitive explanations would be helpful for reading. Also, the theoretical results would not be related to many issue of the proposed algorithm.  Thus, at this point, I recommend\n\"not acceptance\", However, if the authors can provide more intuitive explanations and more problem related theoretical results,  I would  be willing to  change my evaluation.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a noisy label learning method, which adds an additional head to the neural network prediction that represents the confidence of the prediction. The method then iteratively perform label correction based on the predicted confidence value to generate the final predictions. The paper proposes a few auxiliary loss terms for the confidence head, under the previous observation that correct labels are learned earlier than noisy labels.",
            "main_review": "Strength:\nThe paper utilizes the previous observation that correct labels are learned before noisy labels and proposes a method to generate the confidence of the model's predictions.\n\nWeakness:\n1. The proposed method consists several auxiliary loss terms. However, I don't find clear intuition on designing the specific form of the losses. For example, why use the log function in L_P to achieve the desired goal? Is it possible to combine L_P and L_{CACE} into a different form so that the correct labeled samples would prefer a confidence of 1? In addition, these losses also introduce some more hyparameters as their relative weights into the model. The overall approach seems to be a combination of engineering efforts without very strong principle.\n\n2. The theoretical results are good to have. However, I find them not very intriguing as they are directly based on the worst loss values you can get in the proposed loss terms.\n\n3. For the experiment part, the results are not compared with [1]. From the reported results, it seems that [1] performs better in some of the cases.\n\n4. The paper lacks discussions/connections about literature of estimating confidence/uncertainty of the neural network predictions, such as some calibration or abstention methods (though they may not get directly applicable to noisy label learning setting, those methods are very closely related).\n\n[1] Robust Curriculum Learning: From clean label detection to noisy label self-correction. T Zhou, S Wang, J Bilmes. 2020",
            "summary_of_the_review": "I think the paper needs the following improvements: 1) more principled design/explanation of the methods, 2) comparisons with more recent baselines such as [1], and 3) connections to the confidence/uncertainty estimation literature.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper tackles the problem of learning with noisy labels. Benefit from the memorization effect of deep networks, the networks will first clean examples. This paper inherits the advantage and proposes to use an indicator branch to improve the identification of mislabeled data.\nExperiments show that the proposed method can outperform a series of advanced baselines such as SELF and DivideMix. ",
            "main_review": "Learning with noisy labels is one of the hottest research problems in weakly supervised learning. This paper contributes to this problem. By exploiting the memorization effects of deep networks, this paper proposes an indicator branch to perform a regularization. Moreover, by using the iterative label correction technique, the side-effects of noisy labels are further reduced. The authors provide some theoretical analyses to verify the effectiveness of their method under some mild conditions. Experimental results are also provided with discussions on results. \n\n**Pros**  \n(1) Convincing experimental results. The proposed method in this paper achieves the best classification performance in most cases.   \n(2) The overall organization of this paper is great. \n\n**Cons/Concerns/Questions**  \n(1) I concern the novelty of the proposed method in this paper. Actually, it is not very new to introduce extra additional criteria (e.g., the indicator branch in this paper) to combat noisy labels. The criterion can be regarded as an uncertainty measurement during the identification of clean/mislabeled data. The authors can compare their core idea with related work to enhance this paper, such as [1].     \n[1] Yazhou Yao et al. Jo-SRC: A Contrastive Approach for Combating Noisy Labels. In CVPR 2021.   \n(2) The proposed method highly relies on the indicator branch. However, like the prediction branch, the indicator branch is trained on noisy labels. It will also be influenced by mislabeled data, which will be not reliable. Hence, why could we trust the indicator branch to handle noisy labels?   \n(3) The proof of Appendix A.1 assumes the target $t$ equals to ground truth distribution. What role does such an assumption play in the proof? Also, if we rely on this assumption, how to justify it?  \n(4) The authors state that an example with a clean label in expectation has a larger conﬁdence value. Are there any experimental results to support this hypothesis?  \n(5) The theoretical analysis needs that the global optimum of risk minimization is achievable. In this paper, does this condition hold?  Also, can we set $A$ to be as close to zero as possible?  \n(6)  It seems that the proposed method is a bit complex. One main reason is that it introduces too many hyper-parameters into training. For the experimental settings in the Appendix, these hyper-parameters need to be tuned very carefully to achieve great classification performance. In addition, it seems that they are somewhat sensitive. I am hence concerned about the application of the proposed method in the real world. It is rather hard to determine these hyper-parameters. How can the authors address the issue?\n\n \n\n\n\n\n",
            "summary_of_the_review": "This paper focuses on an important problem in machine learning. Its advantages mainly lie in the promising experimental results. However, there are some concerns on this paper as mentioned. Until now, it cannot reach the acceptance line of this top conference in my opinion. Before rebuttal and discussions, I keep my score for this paper to be \"5\". ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}