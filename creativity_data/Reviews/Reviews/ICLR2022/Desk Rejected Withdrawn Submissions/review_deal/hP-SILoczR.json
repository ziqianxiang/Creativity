{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposed a new benchmark specifically designed for zero-shot neural architecture search. Unlike several existing cell-based NAS benchmarks, the search space considered is designed to be more realistic (using the same building blocks as ResNets/MobileNets). The authors also proposed a new metric to better evaluate zero-shot NAS under resource constraints, and provided an empirical analysis of several zero-shot methods.",
            "main_review": "Strengths:\n\nThe paper is well-motivated. In general I agree with the authors that architecture search methods, including zero-shot NAS as a particular instantiation, can be better evaluated in a constrained setting and over more practical search spaces. The benchmark collected in this work by itself could be a useful contribution that might benefit future NAS research. The newly collected benchmark also allowed the authors to perform some interesting analysis that revealed several limitations of existing zero-shot NAS methods in terms of performance and/or transferability.\n\nWeaknesses:\n\nOne of my concerns is the overall quality of the newly collected benchmark. The largest search space proposed (SS-R1) has only 14K architectures which is tiny as compared to similar (mobilenet-like) ones examined in, e.g., Cai et al. 2018 or Bender et al., 2020. The quality of individual architectures also remains unclear due to some specific choices in the training settings. E.g., while all models in the paper are trained using SGD, the original MobileNet-v2/EfficientNets were trained using RMSProp with a very different learning rate.\n\nAs a key metric for most of the experiments in this work, the motivation behind mBR could be better motivated. There are many other potential options that satisfy the same property (that better rank correlation in the feasible set will get a lower score), and it remains unclear to me why it has to be formulated in the current way (e.g., would a constraint-aware variant of Kendall’s tau perform better?). Moreover, it seems the magnitude of mBR will be affected by the tightness of the constraints. As the result, tighter resource constraints will naturally lead to lower mBR, adding additional confounding factor swhen comparing algorithms across different resource settings.\n\nnitpick: Most figures are too small to read (Figure 1-10).\n\n[1] Cai et al., Proxylessnas: Direct neural architecture search on target task and hardware. ICLR 2018.\n\n[2] Bender et al., Can weight sharing outperform random architecture search? An investigation with TuNAS. CVPR 2020.\n\n",
            "summary_of_the_review": "The proposed benchmark seems potentially useful for zero-shot NAS research. However, I'm not fully convinced about the quality aspect of this benchmark. I also believe the proposed mBR metric needs to be better justified.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a benchmark for zero-shot NAS. An extensive evaluation is performed on conventional ResNet/MobileNet search spaces (full coverage of 50,894 networks in three regularized RS/MB search spaces) with three popular image classification datasets, i.e., CIFAR-10, CIFAR100, and ImageNet-16-120. An index called MEAN BEST RANK (MBR) is proposed to find top-ranked architectures under budget constraints with high probability. ",
            "main_review": "The idea of building a benchmark for zero-shot NAS is valid as it helps understand the zero-shot NAS. However, regarding this paper, I have several questions and comments:\n\n1. In my point of view, the reason for utilizing the NAS technique is to find some new structures to inspire human network architecture design. However, instead of evaluating \"irregular\" structures, this paper performs extensive analysis of conventional  ResNet/MobileNet search spaces, i.e., the building block from ResNet-18/34/50 and MobileNetV2 with the choices of depth or channels. It seems that this benchmark becomes a collection of conventional human-designed structures with different network hyperparameters. Also, as proved by many new human-designed structures, the original ResNet/MobileNet block can only be a baseline for new structures with more sophisticated designs. In this case, why do we need a benchmark for conventional RS/MB blocks?\n\n2. The design motivation of mBR is unclear and the analysis of this index is missing, i.e., what is the benefit of using this index? What is the limitation of it?\n\n3. The captions of all tables and figures are meaningless. For example, in Tab 1, what is LC KD SR? Why there are 20% 40% ... 100% in the accuracy columns? What are they stand for? Also, the legends of all Figures are incredibly small, and I can barely understand the meaning of each figure. I do not know what is the purpose of each figure and what should I get from these figures.\n\n4. As an experimental paper, the experiment section of this paper needs significant rewriting.  I could hardly follow the description of each experiment and find the conclusions of each experiment. \n\n5. As the core motivation of building this benchmark, what are the takeaway messages for Zero-shot NAS? How should we design a zero-shot NAS? Compared with other few-shot NAS or standard NAS, what is the advantage of zero-shot NAS? Also, as shown in this benchmark, the correlation between commonly-used zero-shot proxies and the actual performance varies between datasets, then how should we select the right proxies in zero-shot NAS?\n\n6. There are some minor comments of the paper:\n   1). I would not use \"novel\" to describe my own work, a \"new\" should be enough.\n   2). \"Just as ImageNet ushered in an era of deep learning, a comprehensive understanding of ZS-NAS is inseparable from large-scale NAS datasets\", this seems to treat this paper on par with ImageNet, I do not like this expression. ",
            "summary_of_the_review": "1. Explain the motivation of this benchmark.\n2. Details of mBR.\n3. Missing explanation of tables and figures.\n4. The writing needs improvement.\n5. Takeaway messages of this benchmark.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a new NAS benchmark called NAS-Bench-Zero for evaluating zero-shot NAS methods that do not require networks’ parameters to be trained.\n",
            "main_review": "Strengths:\nThe targeting problem of developing a NAS-Bench-Zero to evaluate zero-shot NAS methods is of great importance to the NAS community.\n\n\nWeakness:\nThe introduction is too wordy and often distracts the reader. For example, the author mentions mBR but leaves the definition to a later section. A clean logic flow and compact storyline would be appreciated. There are also few sentences that convey zero information, e.g., “Popular zero-shot proxies are extensively analyzed in NBZero from various aspects” -- what aspects? Too many grammar issues exist in the manuscript, the author may want to proofread before submitting.\nThe fonts in Fig. 1, 4, 5, 6, 9, 10 are too small and the figure size seems too large to be normally rendered, preventing readers from understanding this work.\nThe open source database is a critical criteria to evaluate how well the proposed benchmark is and how likely it will promote the development of zero-shot NAS methods. The author may consider attaching an anonymous link to the database.\nFor the hybrid zero-shot proxy, how much the number of parameters and FLOPs metrics contribute?\n",
            "summary_of_the_review": "While the topic is new and promising, this paper clearly does **not** do a good job of presenting the results.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes NBZero, a new benchmark for zero-shot NAS. On this benchmark, the focus is made to search over ResNet and MobileNet architectures, about 35K networks are evaluated (on cifar-10/100 and imagenet subset) and their statistics are collected. A new z-nas score (mean best model rank) is proposed for comparing zs-nas methods over the benchmark. Multiple popular zs-nas methods are tested and evaluated according to the previous and new metrics.",
            "main_review": "Pros:\n* new benchmark for zs-nas with intuitive construction and sensible metric\n* comprehensive study of all the concurrent zs-nas methods\n* good and thorough review of existing zs-nas benchmarks and works\n* useful observation on zs-nas\n\nCons:\n* The choice of experiments and evaluations is somewhat arbitrary\n* Did not find a clear discussion / argumentation, nor experiments showing the new benchmark is superior to earlier ones. Some promises of that are in the text, perhaps it is there but not very obvious? If this is the case I suggest to clarify this.\n* looking on table 1 which show eval on cifar-100 (was not clear but I discerned it from later numbers in a later section talking about datasets for eval) - I see the margins from the Oracle best and the zs-nas methods are in most cases small (2-3%). Does it make sense? I mean it is a new benchmark and already saturated? This seems like a big drawback in my view, perhaps the authors should re-do this table for other datasets to show bigger potential gains or add richer models?\n* also on table 1 - 79% oracle best performance is much lower than cifar-100 sota on PapersWithCode, not sure what good would be conclusions drawn on this benchmark going forward, will they hold for a richer family of models reaching sota? maybe, again, the dataset and/or family of architectures should be changed?\n* Resnet and mobilenet are good arch, but they are both a bit outdated? as a new benchmark - perhaps it would be good to include concurrent archs? like efficient net and transformer based? vit, swin, etc?",
            "summary_of_the_review": "In light of the above thoughts, I feel more work needs to be done to make this benchmark more impactful. I must admit though I am not a NAS or ZS-NAS expert, my impression is purely based on the contents of the paper and some intuition above. I will be looking at other reviews and authors' rebuttals to reach the final recommendation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents three things in the area of Zero-Shot NAS. A new benchmark dataset, a new metric and an analysis of the metric. ",
            "main_review": "The area of Zero-Shot NAS is very popular at the moment and it is nice to see some work in this area. \n\nStrengths:\n- It’s nice to see people releasing new datasets into the community. \n\nWeaknesses:\n- Although it is nice to see new datasets there is very little presents din the paper to explain what is in the dataset. This makes a lot of the subsequent work hard to follow and definitely difficult to reproduce. I appreciate that you may not wish to release the data before the paper is published, but some supplementary text explaining more details of the data would help a lot.\n\n- For your metric mBR, there are a number of points which need clarifying.\n  - What are you ranking r_i against? Is it the ranking of an architecture against all other possible networks - this would be computationally expensive to compute and could not be used as a proxy.\n  - What you seem to be looking at here is the spread of the different architectures over different constraints. Would mean and variance not be a better way of thinking abut this. Thought exercise: a ZS-NAS which is always ranked 5th out of a possible 10 constraints would get the same mBR score as one which was top 5 times out of 10 and worst 5 times out of 10. \n  - Is stability a good thing here? In the last thought experiment you’d prefer to get the one that was top 5 times out of 10 - would you not?\n\n- There are many claims made in the paper which require evidence to back them up - either by research you have done or by citing appropriate work. I can’t list all of them here, but some of them are:\n  - which are rarely used in manually designed networks.\n  - In real-world practice, many SOTA networks are built upon conventional well-established designed spaces, such as ResNet (RS) and MobileNet (MB) search space.\n  - the irregular connection patterns of search spaces defined by existing NAS datasets prevent\na deeper understanding of the ablation results\n  - In this way, NBZero approximates the true performance of ZS-NAS in real-world applications as closely as possible.\n  - NBZero are more interpretable than those in the previous NAS datasets\n  - it is counter-intuitively more challenging than existing NAS datasets.\n  - We find that these conventional indices cannot precisely reflect the true performance of ZS-NAS in constrained NAS\n  - Moreover, the architecture search by zen is neither too wide nor too deep and its depth is progressive, which matches the common experience of networks designing.\n\n- Many concepts are used without explaining what they are or what exactly you uses. Some examples are:\n  - following the best practice of modern training recipes\n  - what is a strong baseline?\n  - What are zero-shot proxies?\n  - What is a chain-structured search space?\n  - For all datasets, we use the official training/testing splitting.\n  - what is budget constrained NAS?\n  - minimum and maximum of FLOPs of the original search space.\n  - What are S1L, S1out etc\n\n- The related work is too short and fails to take into account significant work in the field. In fact there are other works that should be mentioned here which are introduced later in the paper.\n\n- “More details can be found in our source code”. The reader should not be expected to trawl through source code to identify the details they need to know. As the reviewer does not have access to the source code the only assumption that can be made is that these details are not provided.\n\n- “Detail information of these datasets can be found in.” - found in where?\n\n- Why do you use SGD? It’s not the most popular approach.\n\n- There is growing evidence to suggest that in few-shot NAS auto-augmentation is more significant than the NAS - is it therefore sensible to use it? Either way it will affect the performance.\n\n- “We mainly focus on a practical scenario of NAS where we are given inference budget constraints” - you need to motivate why you are doing this.\n\n- I can’t find a definition of ‘domesticated’ in the dictionary which makes sense in your work.\n\n- The bulk of the paper seems to be about mBR. Though there really aren’t enough sample points to conclude much of what you claim,\n\n- Section 4.3 has the same block of text repeated three times for three different metrics. This is a large waste of space. You could amalgamate these.\n\n- In a number of places there is a claim of a proof - but no proofs are presented.\n\n- Conclusions are far too short and say nothing.\n",
            "summary_of_the_review": "The paper lacks a clear focus. The title abstract and start of the introduction seems to suggest a work on a new dataset, but as the paper continues it moves more to a metrics paper. There are too many ideas bounded around and none are fully explored.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}