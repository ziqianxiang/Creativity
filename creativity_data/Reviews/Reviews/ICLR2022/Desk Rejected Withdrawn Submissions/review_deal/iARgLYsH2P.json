{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies learning disentangled mask attention to reduce the redundant features and enhance the semantic information of multi-head attention layers. In their scenarios, the disentangled attention heads are learned by maximizing the independence over two heads. After that, they conduct semantic clustering with GMM probabilities and construct the semantic mask according to the clustering probability of tokens. Extensive experiments on massive machine translation datasets demonstrate the strong performance of the proposed method.",
            "main_review": "I’ll detail the strengths and concerns below:\n\nStrengths:\n\n1. The focused problem is interesting and important. Making the full use of the multi-head attention layers is an effective way to improve the performance of transformer, and intuitively disentangled mask attention seems to be an effective way.\n2. The empirical performance of the proposed approach is very strong on multiple datasets.\n3. The proposed method  that consists of constructing disentangled attention heads and semantic clustering is novel.\n\nConcerns:\n\n1. The proposed method is a complicated combination of several components, each component is not simple including MI and semantic clustering. The total loss consists of sereral items, each one potentially introduces new uncertainties and hyperparameters and impacts the final performance. Such complexities could limit the proposed method in practical usage.\n2. This work needs more baselines. The paper only compare the proposed method with the DeLighT model, but lack the comparison with other related works cited in the introduction section.\n3. Some details are not clear. How do you select the values of all parameters $c_{qq}$, $c_{xq}$ and $c_{kl}$ etc. ? Is the proposed method sensitive to the hyperparameters? Can the constant value of $c_{kl}=0.01$ deal with the KL collapse well?\n4. Lack ablations on the training objective. I would like to see the effect of each item in the total training objective.\n5. I suggest the authors to seperate the introduction into several paragraphs that is more friend to readers.\n",
            "summary_of_the_review": "Overall, this work demonstrates very strong empirical performances but the presentation is confusing. I give a score 6 in its current form but I'm willing to change my score if I missed anything.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work looks into improving the self-attention mechanism of Transformer. The authors extend the attention mechanism with an approach called disentangled mask attention (DMA). The DMA approach includes two mechanisms, semantic mask attention and disentangled attention, to improve the existing attention mechanism. The semantic mask attention introduces latent clusters for clustering attention heads of each layer, and creates an attention mask based on the similarity of cluster assignment with respect to each pair of attention heads. The resulting mask is used to multiply with the attention distribution and renormalized to drive a new attention distribution. The learning of the semantic mask attention is through variational inference.\nIn addition to the semantic mask attention, motivated by the fact that there can be some redundancy in the multi-headed attention, the disentangled attention looks into maximizing the difference between attention heads.\nThe experiments on machine translation show some improvement compared to the baselines.\n",
            "main_review": "The attention mechanism is critical for the performance of Transformer, and this paper looks into improving this mechanism which addresses a critical topic for the field. The semantic mask attention reduces the redundancy within each attention head through first finding representations for each layer, and then uses the derived representations to mask and reduce the redundancy among the attention distribution. The disentanglement mechanism reduces redundancy across attention heads through their mutual information.\n\nThere are a few direction this work can further improve upon:\n\nAblation studies. It is not clear to me how much of the improvement comes from the semantic mask and how much from disentanglement. \n\nDesign of semantic mask. There are other existing mask attention works like [1]. How is semantic mask compared to those existing work? Does the additional representation learning really bring benefits? Can one take a simpler approach and simply use z to calculate similarity for masking?\n\nLimited improvement. The improvement compared to existing approaches seems limited. For example in DeLighT [2] it achieved 35.3 on De-En with 30M parameter size while this work achieve 35.31 with 39.7M parameter size, and in [1] it achieved 36.3 with 37M parameter size on De-En and 29.1 with 63M parameter size on En-De, while this work achieve 28.35 with 66.6M parameter size on En-De.\n\n[1] Mask Attention Networks: Rethinking and Strengthen Transformer. Fan et al. NAACL 2021\n\n[2] DeLighT: Deep and light-weight transformer. Mehta et al. ICLR 2021",
            "summary_of_the_review": "This paper aims to reduce redundancy with each attention head and across attention heads. While the directions of the approach are reasonable, it’s not clear that the proposed approaches outperform existing mechanisms, and the overall improvements are limited compared to previous works.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes an alternative to the attention module in Transformer layer, named DMA. This module solves two problems of the original multi-head attention mechanism: (1) token features are redundant and (2) attention heads are similar. First, the authors propose GMM-based token feature clustering to enhance the attention between tokens of the same cluster. Second, the authors utilize MI-based loss to minimize the similarity between attention heads. For three NMT benchmarks, DMA Transformer has achieved comparable or better performance than the original Transformer.",
            "main_review": "(1)\tThe paper is hard to follow (partially because of the writing and partially because of the insufficient explanations). To name a few, the introduction is only a single paragraph, and contributions/modifications are not clearly presented. Also, it was difficult to connect the dots between the latent variable “z”, the GMM, and information-based losses to the attention mechanism.\n\n(2)\tFor the semantic masking, it seems like the purpose is to make tokens of similar semantic meanings attend to each other more. However, how is it related to ‘reduce the redundancy in representations’ and ‘improvement in NMT performance’? Is it desirable to only attend to semantically similar tokens? Isn’t the redundancy still existing between tokens in the same cluster because they still attend to each other?\n\n(3)\tWhy is the semantic mask disregarded in the test phase? (Section 4.3) Does it show better results?\n",
            "summary_of_the_review": "Motivation is important and experimental results are good. But it was difficult to understand the overall process, and explanations seems to be insufficient.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this work, the authors work to improve multi-headed attention (MHA) in transformer models. Since previous works have shown that multiple heads in the same block might capture redundant information which could effectively reduce the capacity of the model. In order to improve the model, the authors propose to disentangle the attention representations learned by the various attention heads. This is accomplished in two steps: first, the authors define a variational Seq2Seq transformer model where the latent distribution is modeled as a mixture of Gaussians; these are used to define a “semantic mask” which identifies input tokens which are likely to correspond to the same cluster which is multiplied with the attention weights. Second, in order to ensure that different attention heads do not capture redundant information, the authors propose to add a regularization term that minimize the mutual information between attention heads. \nThe proposed techniques are evaluated on machine translation tasks, (IWSLT’14 De-EN; WMT’14 En-DR; and WMT’17 Zh-En) where the authors find that the proposed techniques allow the authors to obtain improvements in BLEU scores over the baselines. The authors also compute measures of head independence, and find that the proposed techniques do, in fact, reduce redundancy.\n",
            "main_review": "The main motivation behind this work -- that there exists some redundancy between the multiple attention heads -- is a reasonable claim. The proposed techniques to increase the independence amongst the attention heads seem reasonable, and the experiments appear to indicate that the proposed techniques improve accuracy (as measure by BLEU scores) relative to recent baselines.\n\nHowever, I had a few criticisms of this work. One of my main criticisms is that the presentation of this paper could be improved by re-phrasing parts of the paper and correcting grammatical errors, but also by updating the presentation in Section 2 and 3 which I found somewhat hard to read.\nIn a number of places, the text is somewhat poorly phrased, or contains grammatical errors. Please fix these lines:\nIntroduction, Page 1: “... or the attention weights lacking of semantic interpretation.”\nIntroduction, Page 1: “... and the resulting transformer received similar outputs even the attention weights were considerably different.”\nIntroduction, Page 1: “... this paper aims to increase the semantic meaning ...”\nSection 2.2: “... an Gaussian mixture model” --> “a Gaussian mixture model”\nSection 4.3: “... rapid the convergence of GMMs ...”\n\nAdditionally, in a number of places in the paper, the authors could strengthen the argument by providing relevant citations. For example:\nIntroduction: “In recent years, transformer (Vaswani et al., 2017) has obtained state-of the-art results on sequential learning in the applications of speech recognition, machine translation, question answering, reading comprehension, to name a few.” I completely agree with this statement, but please cite relevant works in each of these areas.\n\nAnother question I had about this work is to compare the proposed techniques against the baseline in terms of training time. In Section 4.3, the authors mention that they make a number of assumptions to read computation cost (for example, by neglecting certain terms in the optimization). I would like to know how the proposed techniques compare with the baseline in terms of total computational cost/training time.\n\nThe authors also mention that they “disregard the semantic mask” in the test phase. I was somewhat confused about the motivation to do so since it creates a mismatch between training and test. Could the authors comment on why they do this and the impact of using the mask during inference?\n\nIn Section 4.4, the authors analyze the semantic clustering to identify similarly cluster tokens in Figure 4. The authors mention that the cluster shown “captures the semantic meaning of topic words” based on the fact that it contains ‘es’ ‘ing’ ‘ed’ and ‘ies’. I personally did not find this analysis very convincing. For one, it is somewhat anecdotal since we only look at one of the clusters for one decoder layer, but more importantly I don’t think that there’s a clear pattern based on these tokens. Also, the other tokens in the set ‘p’ ‘c’ etc. do not seem to be part of a unified pattern.\n",
            "summary_of_the_review": "In this work, the authors proposed and evaluated a system to disentangle attention heads in a transformer with the goal of reducing redundancy. In experimental evaluations, the proposed techniques do appear to improve performance slightly over a baseline technique. My main concerns with this work are related to the presentation and the fact that some of the claims (e.g., the analysis in Figure 4) do not clearly demonstrate that the model is indeed learning meaningful patterns. \n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}