{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper consider the mutual information estimation problem. The starting point of this paper is the variational formulation, which cast the mutual information as the optimal value of an infinite dimensional supremum problem over all bounded critic function. However, because the objective function has an expectation of an exponential function, this optimal value is sensitive to outlier values.\n\nThis paper proposes an estimator that is less sensitive to outlier. This estimator is constructed based on the following two ingredients:\n\n1. restrict the set of possible critic functions to a smaller family using spectral norm normalization,\n2. use gradient stabilization by ``avoiding gradients generated by the partition function from back-propagating\".\n\nThe numerical experiments benchmark against existing methods (Belghazi et al 2018, Poole et al 2019, and Song & Ermon 2020) in various applications (correlated data, GANs, etc.) and deliver competitive results.",
            "main_review": "The content of this paper is presented in Section 4. The paper provides a thorough discussion of existing methods along with important properties in Section 2 and 3. The paper, however, falls short of the clarity for the proposed methods:\n\n1. There lacks a formal definition of T_\\theta^{SN} in Section 4.\n2. There lacks a clear motivation for spectral normalization. There are also many other ways that we can regularize the weight matrix W. Why is the spectral norm appropriate for this application?\n3. A more details of the gradient stabilization should be included. What are the theoretical justification to omit the gradients of the partition function? How can we guarantee that there is no significant loss of performance when we omit this gradient?\n4. Does Algorithm 1 even converge? If we use gradient stabilization, what is the limit point of theta?\n\nThe current theoretical results (Lemma 6 and the consistency results) follow from minor adaptation of well-known results (such as Belghazi et al. (2018)). \n\n\nMinor comments:\n- Algorithm 1 is poorly written: Why do we need to compute I_{MICE}(theta)? What is the left-hand side of I_{MICE}?",
            "summary_of_the_review": "This paper introduces a new estimator for the mutual information. However, the paper lacks clear theoretical results to support the correctness and robustness of the proposed estimator.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper reviews some existing estimators of mutual information and points out that these estimators have the issues of significant bias and/or high variance. Then a new estimator of mutual information is proposed which is called the Mutual Information Continuity-constrained Estimator (MICE). Some properties of the MICE are shown such as strong consistency and a simple expression for the upper bound of the variance of the critic. An algorithm is presented to calculate the MICE. Experiments suggest that the MICE outperforms most of the existing estimators in terms of bias and variance in the standard benchmark.",
            "main_review": "**STRENGTHS:**\n\n(a) Unlike most of the existing estimators discussed in the paper, the proposed estimator, MICE, is free from the overestimation of mutual information. Some other desirable properties hold for the MICE such as strong consistency and a simple expression for the upper bound of the variance of the critic.\n\n(b) A sufficient number of experiments are carried out to demonstrate that the MICE provides a better performance than most of the existing estimators in terms of bias, variance and MSE.\n\n(c) The paper presents a well-summarized review of existing estimators of mutual information. \n\n&nbsp;\n&nbsp;\n\n**WEAKNESSES:**\n\n(d) Apart from its critic, the MICE has a similar form to the estimator of Nguyen et al. (2010), NWJ estimator, given in Lemma 2. However the theoretical comparison between the MICE and the NWJ estimator is not sufficiently investigated in the paper. In particular, it would be good to clarify whether the theoretical results such as Lemma 6, Inequality (16), and strong consistency also hold for the NWJ estimator or not.\n\n(e) There is not enough discussion to compare the computational cost of the MICE with that of the existing estimators. It could be nice to add this discussion apart from the computational complexity of the critic given in Section 5.1.\n\n&nbsp;\n&nbsp;\n\n**MINOR COMMENTS:**\n\n(f) p.9, l.9 up: It is claimed that MICE is an unbiased estimator, but it is not proved in the paper.\n\n(g) p.9, l.9 up: a unbiased ===> an unbiased",
            "summary_of_the_review": "Through a sufficient number of experiments, it is seen that the proposed estimator outperforms most of the existing ones. Some nice theoretical results hold for the MICE, but I am not sure whether these results are novel ones or straightforward from existing theory. My recommendation is \"6: marginally above the acceptance threshold\".",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I do not find any ethical concerns with this paper.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed a new estimator, named MICE for mutual information estimation. The heuristic is to smooth the partition function by constraining the Lipschitz constant of the log-density ratio estimator. The author(s) show the proposed estimator can be more accurate & reliable than the prior approaches when the i.i.d assumption is unfulfilled.\n\n",
            "main_review": "Strengths:\n(1) Considering the non iid scenarios is a good call.\n(2) Clear writing and adequate summary of existing literature.\n(3) Using Lipschitz constraints to regulate the variance is an interesting idea.\n\nWeakness:\n(1) Section 2 heavily repeat well-known results, extensively quoting known theories does not add value to the paper. There is no need to spend so much space reviewing and please significantly shrunk this section (prune to the Appendix). \n(2) I would consider the comparison to SMILE unfair, as there is no detail in the paper discussing how you tune your Lipschiz constant is tuned (while SMILE use fixed truncation). \n(3) Section 4, why do you only normalize T_{\\theta}^{SN} by the spectral norm of the last layer. The right way should be normalizing T by all layers. It feels a bit of strange only regularizing the output layer with the spectrum norm, which does not adequately control the Lipschiz constant (the neural net can easily rescale preceding layers’ output to cope with a smaller output Lipschiz). \n(4) In section 4.2, if the critic functions have a bounded Lipschitz, then MICE is guaranteed to be a biased estimator if the true log-likelihood ratio violates the constraint. Since it is impossible to prove a biased estimator is consistent, the theory is flawed. \n(5) It appears that the experiments are weak, since MICE is not evaluated on real-world data. Section 5.1 & 5.2 are very basic toy models. Also, section 5.3 is not a challenging problem. It is meaningless to use vanilla GAN as baseline, most more recent GAN variants can recover this multi-modal Gaussian without additional regularization. In addition, other MI estimator might also achieve the same results, you have to show cases where MICE is a clear winner.\n(6) Some of more recent MI estimation literature on MI estimation (and their application) such as [1][2] are missing from the current discussion. Note they all try\nto improve the estimation. Also, I think the discussions should give specific attention to\n[Poole, 2019]’s outlook on the future directions for MI estimation, please clarify how the proposed approach addresses open problems such as high MI estimation and optimization\n(due to the fundamental tension pointed out by [McAllester, 2020)).\n\n[1] Mroueh, Youssef, et al. \"Improved Mutual Information Estimation.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 10. 2021.\n[2] Gupta, Umang, et al. \"Controllable Guarantees for Fair Outcomes via Contrastive Information Estimation.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 9. 2021.\n\nAlso equation 11 strongly connects to idea from [3], which is to achieve variance reduction by exploiting the empirical partition function. Some discussions or comparisons is advised.\n[3] Guo, Qing, et al. \"Tight Mutual Information Estimation With Contrastive Fenchel-Legendre Optimization.\" arXiv:2107.01131.\n\nAnd the author(s) should cite [4,5] which introduced the Lipschiz constraint to Machine Learning context\n[4] Arjovsky, Martin, Soumith Chintala, and Léon Bottou. \"Wasserstein generative adversarial networks.\" International conference on machine learning. PMLR, 2017.\n[5] Arjovsky, Martin, and Léon Bottou. \"Towards principled methods for training generative adversarial networks.\" arXiv:1701.04862 (2017).\n\nMinor: The CPC is better to be re-named to InfoNCE, which is well-known\n",
            "summary_of_the_review": "This work may not be very likely to make significant impacts, although the author(s) proposed an interesting new method for MI estimation. Note that the MICE is motivated by well-known principles (variance-control) and largely built on existing techniques (spectral normalization) so it only bears very limited novelty. The theoretical arguments are either straightforward (bounding variance with Lipschiz constants), or problematic (e.g., the convergency theorem). Thus this work is more toward an empirical investigation rather than theoretical contribution. However, the author(s) failed to present strong experimental results on real-world data to convince me of its practical utility.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes reducing the variance of the MINE mutual information estimator by shrinking the Lipschitz constant of the critics network $T$, in an attempt to reduce the variance of $T(x, y)$.\n",
            "main_review": "\n## Technically Flawed ##\nThis paper is technically flawed.\n\n**1. The proof of consistency provided in Section A.2. is wrong.**\n\n**1.a)** The statement of what the authors claim they need to prove on Page 12 is not specific to MICE. $T_\\theta$ cannot simply be considered any neural network. The authors should have used $T_\\theta^{SN}$, and proved that strong consistency is preserved despite shrinking the Lipschitz constant. Trying to prove strong consistency without restriction on the Lipschitz constant of $T_\\theta$ is equivalent to trying to prove strong consistent of MINE and concluding that MICE is strongly consistent, which is incorrect.\n\n**1.b)** In the proof itself (Page 13), the authors assume a density ratio $T^* := \\frac{dP}{dQ}$ is necessarily upper-bounded (by the bound they denote $T_{\\max}$). This is obviously wrong. The bivariate Gaussian with any non-null correlation has a density ratio that explodes. **Hint:** the density ratio is a Gaussian copula pdf.\n\n**1.c)** The authors need to specify which version of the universal approximation theorem, if any, is consistent with the restriction they placed on the $T_\\theta$.\n\n**1.d)** The authors' understanding/application of the density property of universal approximation theorems is wrong. If $T^* \\in \\mathcal{T}$ and the family $\\\\{T_{\\theta}, \\theta \\in \\Theta\\\\}$ is dense in $\\mathcal{T}$ in both the $L^1(P)$ sense and the $L^1(Q)$ sense, then the authors can indeed conclude that for any $\\epsilon > 0$ we may find a function $T_{\\theta(\\epsilon, P)}$, and a function $T_{\\theta(\\epsilon, Q)}$ such that $E_P|T^*-T_{\\theta(\\epsilon, P)}| < \\epsilon$ and $E_Q|T^*-T_{\\theta(\\epsilon, Q)}| < \\epsilon$. It is very important to note that $\\theta$ depends on both the threshold $\\epsilon$ and the distribution under which the expectation is taken.\n\nIn writting inequalities (30) and (31), the authors assume that with two different thresholds $\\frac{\\epsilon}{2}$ and $\\frac{\\epsilon}{2}e^{-T_\\max+1}$, and two distributions ($P$ and $Q$), the function that satisfies the inequality is the same, which is wrong. \n\nBecause of this, inequality (33) does not hold; the functions $T_\\theta$ in both expectations should be different.\n\n**1.e)** The rest of the proof is even more confusing and has many more mistakes that I'll skip here for brevity.\n\n\n**2. It's not just the proof that is wrong, the consistency result itself does not hold. MICE isn't asymptotically unbiased either.**\n\nFundamentally, if you constraint the critics network $\\\\{ T_\\theta, \\theta \\in \\Theta \\\\}$ to have a bounded Lipschitz constant $L_\\max$, it cannot possibly be dense in the space of all true density ratio $\\frac{dP}{dQ}$. \n\n**Hint:** in the simple bivariate case (i.e. when $x$ and $y$ are both scalars), use Sklar's theorem to express the true density ratio as a function of the copula density and the CDFs of $x$ and $y$: $T(x, y) = c(F(x), F(y))$, where $c$ generically denote the copula density, and $F$ generically denote marginal CDFs. \n\nNote that, the gradient of $T$ reads $\\nabla T = \\left\\(p(x)c^\\prime(F(x), F(y)), p(y)c^\\prime(F(x), F(y))\\right\\)$, and that it is therefore extremely easy to choose marginal pdfs that would make $\\nabla T$ unbounded for many copulas. The approximating family will therefore have a hard time properly approximating $T$ in neighborhoods where $||\\nabla T|| > L_\\max$, and we have no reason to believe these neighborhoods are of probability $0$ under either $P$ or $Q$.\n\nIt is very important to note that the extent to which the approximating family can approximate the density ratio depends on both the true joint $P$ (its marginals and/or its copula) and the bound $L_\\max$. Or, equivalently, that how good/bad a bound $L_\\max$ is in practice will depend on the data generating distribution, unlike what the authors claim.\n\nBecause of the non-universality of the shrunk approximating family, taking the $\\sup$ over it in the DV characterization will result in a lower-bound of the KL divergence that is not tight (i.e. MICE will not be consistent nor asymptotically unbiased).\n\n\n**3. (Asymptotics) The tradeoff between the cap on the Lipschitz constant of the critics' network and the asymptotic bias of MICE should have been studied theoretically and/or empirically.**\n\nGiven that MICE is in fact **not** consistent, the author should assess the irreducible (w.r.t the amount of data available) price their model is paying for constraining the Lipshitz constant of the critics' network.\n\n\n**4. (Finite samples) The tradeoff between the bias introduced by shrinking the Lipschitz constant of the critics' network and the reduction of variance (compared to MINE) should have been discussed.**\n\nFor a given sample size, it is important for the authors to compare MINE and MICE in order to properly assess the tradeoff variance reduction (which they claim) vs. bias increase due to their idea of shrinking the Lipschitz constant. \n\n## Practical Value-Added Unclear ##\n\n**5.** At the very least, I would have expected the authors to provide a fair comparison MINE vs MICE where the two implementations are identical but for the shrinkage of the Lipshitz constant; they didn't.\n\n## Does Not Beat The SOTA ##\n**6.** The SOTA when it comes to data-efficient mutual information estimation is [1]. [1] showed that, by working in the (copula-uniform) dual space, the two fundamental limitations identified in [2], namely the excessive variance of variational estimators and the $O(\\log n)$ upper-bound on mutual information estimates, no longer apply, and consistency can still be preserved.\n\nSee also [3] (Appendix A) where the link between MIND ([1]) and applying MINE in the (copula-uniform) dual space is established. In particular, [3] showed that applying the DV characterization in the dual space (MINE-style) to estimate a copula entropy can be regarded as a mini-max copula entropy problem. You can use this result to justify why you would want to shrink the Lipschitz constant of the statistics function $\\phi$ in MIND (which plays the same role as the critics' network in MINE). \n\nThe higher the Lipschitz constant the more data you'd need to reliably estimate the max-ent constraint. Thus, shrinking the Lipschitz constant of $\\phi$ can be viewed as applying Occam's razor to account for the limited amount of data available, and the upper bound $L_\\max$ would depend on the amount of data available. Once again, by shrinking the Lipshitz constant of $\\phi$ you would lose consistency, but you can use Corollary 3.1. in [1] to conclude that, although you can no longer estimate individual copula entropies with perfect accuracy, you may still estimate the mutual information with perfect accuracy. \n\nI encourage the authors to consider applying their shrinkage idea in the (copula-uniform) dual space (assuming they can demonstrate practical benefits in doing so).\n\nFinally, in their NeurIPS submission (withdrawn after reviews were released), it was pointed out to the authors that MIND, which the authors are still not comparing their model to, achieved far better accuracies than MICE with orders of magnitude less i.i.d. data in the standard experiments considered in this paper.\n\n[1] Samo, Y.L.K., 2021, March. Inductive Mutual Information Estimation: A Convex Maximum-Entropy Copula Approach. In International Conference on Artificial Intelligence and Statistics (pp. 2242-2250). PMLR.\n\n[2] McAllester, D. and Stratos, K., 2020, June. Formal limitations on the measurement of mutual information. In International Conference on Artificial Intelligence and Statistics (pp. 875-884). PMLR.\n\n[3] Samo, Y.L.K., 2021. LeanML: A Design Pattern To Slash Avoidable Wastes in Machine Learning Projects. arXiv preprint arXiv:2107.08066.\n",
            "summary_of_the_review": "This paper is technically flawed, the merit of the idea proposed hasn't been fairly empirically established, and the authors made no effort to compare their work to the SOTA as far as data-efficient mutual information estimation goes [1].\n\n[1] Samo, Y.L.K., 2021, March. Inductive Mutual Information Estimation: A Convex Maximum-Entropy Copula Approach. In International Conference on Artificial Intelligence and Statistics (pp. 2242-2250). PMLR.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}