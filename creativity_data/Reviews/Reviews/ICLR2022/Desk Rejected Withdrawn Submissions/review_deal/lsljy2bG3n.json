{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes m-mix for mining hard negative examples in the contrastive learning setting. They achieve this by mixing multiple samples and dynamically learning to assign different mixing weights. Also, different versions, i.e. w or w/o priors, are introduced so that they can be applied to both graph and vision domains. The method shows relative improvements w.r.t to contrastive learning baselines. ",
            "main_review": "The paper is well-written and easy to follow. It is addressing an important part of the contrastive learning pipeline, i.e negative sampling, which is now very popular amongst the machine learning research community. I have a few suggestions and concerns that I would like to share below:\n\nThe method and experiments are mostly on graphs and augmentations in the graph domain aren’t as clear as they are in the vision domain for general machine learning readers. So it would be better to briefly define graph augmentations in section 3, instead of section 4.\n\nI think pseudocode might be more helpful than figure 1 in understanding the steps of your approach.\n\nOn page 4, the authors introduced the diversity loss to reduce self-mixing weights and in the experiments, they showed that this auxiliary loss improves the performance. However, have you checked the distribution of \\lambda w and w/o this loss? It would be good to see the effect for a couple of datasets.\n\nAlthough m-mix shows performance gains in a lot of experiments, the margins are not huge and that might be concerning for the utilization of m-mix in future studies.\n\nIn section 2 the authors discussed some baselines for mining hard negatives. However, the vision domain evaluations are limited to binary-mix and their method (Table 3).\n\nHow sensitive m-mix-op is to the \\theta parameter?\n\nThe backbone of your method is mostly MVGRL and it is shown that the proposed hard mining results in relative improvements. On the other hand, GRACE w and w/o mp-mix is only reported on two datasets in Table 4. Can the authors show that mp-mix boosts the GRACE + (mp-mix) results in other reported datasets?\n\nRobustness of your method w.r.t to negative samples is discussed in Section 4.3 Have you compared the robustness of vanilla MVGRL or GRACE with m-mix?\n\nTypo: Table 4, right column, GARCE should change to GRACE. \n\n\n\n\n",
            "summary_of_the_review": "The paper is well-written and has studied an important matter in contrastive learning. Extensive experimentation has been conducted however, there are a few concerns regarding the contributions and evaluations that I mentioned.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes m-mix which  generates hard negatives via multiple samples mixing with different weights. They provide good theoretical analysis on the effect of combination of sample size and contrastive size embedding. They also propose diversity loss which mixes weight between similar samples. They also show results on a lot of datasets; however I’m concerned about significant improvements are over the state-of-the-art methods. The results in vision domain especially seem incomplete and doesn’t compare to other methods.",
            "main_review": "\nStrengths: \n1) The idea of mining hard negatives in graph based methods is relatively new and novel. They also provide good theoretical analysis on why assigning mixing weights can help in generating more difficult hard negatives rather than random samples.\n\n2) They claim that this method can be applied to vision datasets which was not evident in  previous methods.\n\n3) They conduct a large number of experiments across different graph datasets to show effectiveness of the method.\n\n\nWeakness:\n\n\n1) Results on graph datasets are very incremental. For example, inTable1 it seems to improve on  Citeseer and Pubmed but doesn’t seem to improve on Cora. Can the authors comment on why there are these discrepancies in the results?\n2) Results on vision datasets are weak. The results reported by the MVGRL method are weaker than SImclr on CIFAR datasets. Can the authors comment more on that?  Also the results onimagenet-100 will be more convincing.\n3) On graph datasets, usually the random baseline performs quite well. Can you also show  results with random initialization? \n4) I’m not sure what is the advantage of showing Rademacher Complexity of m-mix ? It doesn't seem to be adding much to the paper.\n5) Grammatical/Writing mistakes:\n\n     a) Why is simclr called graph-level contrastive methods (Chen et al., 2020b).\n\n     b) Typo: may should on page4.\n\n     c) Grammatical issue: Utilize on page 4.\n\n",
            "summary_of_the_review": "Overall I like the idea of the paper but it still have some concerns specially the improvements over the state-of-art-methods. There are results on vision datasets like cifar10 and Cifar100 but these results seem incomplete. Probably adding more results on imagenet-100 will make the paper even stronger.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed a new m-mix hard negatives generating method for contrastive loss. Such method can mixes multiple samples simultaneously and assigns different mixing weights dynamically during the training process. Theoretical analysis for the properties of the proposed method is also provided. Furthermore, extensive experiment results demonstrate that the m-mix method can achieve state-of-the-art performance on most image/node/graph classification benchmarks. ",
            "main_review": "Strengths:\n\n1. The proposed m-mix algorithm is new,  reasonable and easy to be implemented in practice. Most importantly, such method obtains SOTA performance on most benchmarks when compared with existing ones.\n\n2. The paper, especially the introduction and related work section, is well written and easy to follow.\n\n3. The conducted experiments are truly extensive and demonstrate the advantages of the proposed method.\n\nWeaknesses:\n\n1. The proposed method seems not so novel, when compared with the latest i-mix method (Lee et.al., 2020) and the similar DACL (Verma et al. 2021). I need to admit that I am not an expert in this field, but according to the content in Related Work, its seems like that the proposed m-mix method is just a direct extension of the binary-mix version (Lee et.al., 2020, Verma et al. 2021).  In my opinion, more explanations about the differences between the m-mix method and the similar i-mix one should be given, to make the advantages of m-mix clearer. Besides, the total objective of loss function in Eq. 9 is almost the combination of the node-level loss and graph-level loss, which is common in existing works.\n\n2. There exists some mistakes in the proof, which i think should be absolutely avoided in the paper submitted to a top conference. Actually, some of them will hurt the correctness of the theoretical claims in the main paper,  and this is where  my major concern lies in. I will list them as follow in detail.\\\n(1) In the proof of Prop. 1, when comparing the value of H(\\hat{z}_{i},z_{j}) and  H(\\hat{z}_{i}^{‘},z_{j}) (i.e., the first line under Eq.13), since it is unclear whether the term <z_{k},z_{j}> is positive or not, we can not determine the relationship of the size between \\lambda_{k}^{‘}<z_{k}, z_{j}> and \\lambda_{k}<z_{k}, z_{j}>. Therefore, according to the proof strategy, we can not say that H(\\hat{z}_{i},z_{j}) is greater than  H(\\hat{z}_{i}^{‘},z_{j}). Similar mistake can also be found in the proof of “H(\\hat{z}_{i},z_{j})>H(\\hat{z}_{i},z_{k})”.\\\n(2) In the proof of Theorem 1, in the bottom of Page 18, when combining Ineqs. 14, 15 and 16, how can we get the Inequality (17)? Just consider the counterexample: 2<3, and 2<4, but by combining these two inequalities, can we obtain (0=) 2-2<= 3-4 (=-1)? Inequality 17 is absolutely wrong. Generally speaking, it is non-trivial to directly compare the values of two different Rademacher complexities, and the classical technique to handle this problem is the so-called contraction inequality (see examples in the paper “A vector-contraction inequality for Rademacher complexities”(Maurer2016ALT) ).\\\nBesides that, there exist other flaws in the proof of Theorem 1: (2.1) the fourth equality in Eq.14 does not hold, and it should be “<=” due to Cauchy-Schwarz inequality. (2.2) the lack of superscript “2” of the norm of z_{i}  in the last term of Eq.14. (2.3) In the Eq.15, first line,  lack of superscript “1/2” outside the brace. (2.4) Eq. 15, the second inequality, lack of the multiplier constant 2, due to the basic inequality (a+b)^{2} <= 2(a^2+b^2), instead of the wrong version (a+b)^{2} <= a^2+b^2. (2.4) Eq. 16,   lack of superscript “1/2” outside the brace in the rightmost term. (2.5)  Eq. 16, lack of subscript “j \\ineq i”   in the rightmost term.\n\n3. The flaws in the proof of Theorem2: Eq. 21, the position of the right parenthesis is wrong.\nTo be honest, the proof of Theorem 1 and Theorem 2 is chaotic, and needs detailed explanations for some crucial steps (e.g., whether use some basic inequality).\n\n4. Minor typos:\\\n(4.1) Page 3, Eq.2,  The description “\\sum_{i}^{n}=1” lacks of \\lambda_{i}.\\\n(4.2) Page 4, the first line of Proposition 1, it should be ”which satisfy” instead of “which satisfies”.\\\n(4.3) Page 4, the second line of Proposition 1, it should be “assigning larger weights” instead of “assign larger weight”.\\\n(4.4) Page 4, Eq. 3, the numerator of \\lambda_{j} should be “H(z_{i}, z_{j})” instead of  “H(z_{i}, z_{k})”.\\\n(4.5) Page 5, Eq. 8, the subscript should be “i” instead of “u”.\\\n(4.6) Page 5, theorem 1, line 3, it should be “difference between two” instead of “different between two”; “Rademacher” instead of “Redemacher”.\\\n(4.7) Page 5,  in  the part before Theorem 2,  the theoretical claim “R^{m}_{Z}(H_{f}) <= R_{Z}(H_{f})” needs more explanations.\\\n(4.8) Page 6, in the part before Experiment section, the word “and ” in the sentence “and the sample size...” should be removed.\\\n(4.9) Page 19, Eq.20,  it should be “obtain a union bound” instead of “obtain a union bounds”.\n\n5. Additional suggestions:\\\n(5.1) Even though the proof is absolutely correct, authors should give more explanations for what Theorem 1 implies.  In my opinion, to be honest, Theorem 1 makes no sense, as this result dose not reveal any theoretical property (e.g., asymptotic  behavior) of the proposed m-mix method. In contrast, it will be more interesting to give a Rademacher complexity based generalization bound for the proposed m-mix method (cf. Theorem 3.3. in Chapter 3 of Mohri2013 Foundations of Machine Learning),  and show that the Rademacher complexity of m-mix data is strictly small than the Rademacher complexity of original data.\\\n(5.2) It will be much better to give more explanations for the  motivation of your theoretical analysis, before presenting your theoretical results. Such explanations will shed more lights on how to derive meaningful theoretical analysis, and are truly significant in the writing of a theoretical paper.\\\n(5.3) If  authors do want to add theoretical analysis, i believe that the rigorous definition of the Rademacher complexity should be given in the main paper for reader’s better understanding. Besides,  the detailed definition of other important notations  should be given, such as the “infinite norm of one matrix”.\n",
            "summary_of_the_review": "Given that there exist many mistakes in the proof of the theoretical results, I believe the current version of this paper should not be accepted by ICLR2022. Even though the mistakes can be corrected after revision, more explanations for the difference between the proposed m-mix method and the similar i-mix method should be included. Besides, more meaningful theoretical analysis for the properties of the m-mix method should be given to increase the paper’s novelty.  At the current stage, I vote to reject this paper.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper extends mix-up, a simple data augmentation algorithm from binary case to multiple cases and apply it in multi-view graph classification problems. \n\nThe key idea is to identify importance weight from multiple samples and generate new sample data via a linear combination of the importance weight. \n\nThe result looks additive on top of multi-view graph classification problem.\n",
            "main_review": "Pros:\n1.\tSimple idea and easy to replicate. The motivation is given a positive data, negative samples with similar feature representation are hard negative. A larger weight should be assigned in the new generated data. The intuition makes sense and implementation seems easy. \n2.\tGreat presentation and clear results. Table 1 and 2 clearly compares the different set up across multiple benchmark results. Both claimed benchmarks and replicate results are included. Results looks valid and solid. \n3.\tTheoretical analysis is included. This could be interesting for those audience with graph theory background\n\nCons:\n1.\tImportant comparison is missing. Original mix-up is a simple and effective data augmentation algorithm for classification tasks. I assume m-mix as well. However, results are only performed on multi-viewed graphs, which seems very narrow, raising my concern for its generalization. \n2.\tCorrectness concern. It seems to me m-mix is a data augmentation method to generate hard negative samples, in the experiment, X/D/A are available, without Y, so how to sample the negative samples? Are samples without a link treated as negative samples? The set-up seems not that clear? \n3.\tThe significance and novelty seem kind of marginal. The basic idea is extending mix-up from binary case to multiple samples scenario and the solution is kind of straight-forward. One limitation of m-mix comparing to mix-up is efficiency. The computation of weight involves a softmax function over multiple samples, it could be a bottleneck when extend to large-scale graphs. \n",
            "summary_of_the_review": "This m-mix idea is simple and reasonable and the quality of presentation is high. \n\nMy major concern are two folds \n1. The set-up is in a narrow domain - multi-view graph classification, which conflicts my understanding as a general data augmentation approach. Important comparison with standard data augmentation approaches, e.g., mix-up is missing. \n2. The significance and novelty seem OK and marginal.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}