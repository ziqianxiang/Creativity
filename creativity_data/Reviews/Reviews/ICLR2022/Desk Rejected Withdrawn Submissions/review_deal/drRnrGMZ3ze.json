{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes Interior Policy Differentiation, a variant of Interior Point Method, for learning diverse policies in RL. The paper uses the empirical Wasserstein distance  to measure the difference between policies, then formulates the novel policy generation problem as a constrained optimization problem. However, the novelty of the paper is limited and the empirical evaluation is not convincing to make it pass the bar of ICLR 2022. ",
            "main_review": "Originality & Novelty: The main innovation of the paper is to rewrite the traditional reward function for training with intrinsic reward (Eq. 5) into  a constrained optimization problem. However, most of the techniques in this paper is not new, including measuring the distance between policies with Wasserstein distance, solving the constrained optimization with interior point method, etc.. \n\nStrength: The paper is written clearly and enjoyable to read. The proposed algorithm is reasonable for the setting considered in the paper.\n\nWeakness: \n1.  The authors fail to convince the readers the necessity of re-formulating the problem into a constrained optimization problem. It is known that when the Pareto front is convex, linear combination and constrained optimization can find the same solution set. In this complicated deep RL problem, we do not know the shape of the Pareto front, so I was expecting the authors to compare their algorithm with linear combination methods with different $\\alpha$.\n2. It is still tricky to determine $r_0$. In the paper, the authors suggest to set $r_0$ to be the average distance between PPO policies. If this is the case, it means that we need a pre-trained PPO. Now that we already have a RL policy that works in the environment, why would we need diversity then?\n3. The empirical evaluation is problematic. (1) The proposed method directly use Wasserstein distance as an optimization objective, so obviously it is going to have the best diversity under this metric. Will the ranking of the algorithms change with different diversity metric? (2) If the proposed method improves the exploration efficiency, the authors should provide comparison with advanced exploration techniques. (3) I doubt the scalability of the method because the authors did not provide enough experiment results on different environments. The authors should provide more results on the mujoco locomotion tasks. (4) Ablation study is needed since the algorithm seems very sensitive to the hyper-parameters. What if $r_0$ is chosen by a suboptimal strategy?  ",
            "summary_of_the_review": "The paper still requires a significant improvement to be able to get published on top-tier venues, since the proposed method is not so novel and the empirical evaluation are not enough to convince the readers. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper explores a gradient-free constrained optimization framework for generating diverse policies. Its main contributions are: (i) a computationally light metric to measure the diversity of a policy with respect to an existing set of policies, based on the Wasserstein metric W2; (ii) a practical algorithm with instant feedback at every timestep, inspired by constrained optimization to generate diverse policies. Experiments on continuous benchmarks show that the proposed algorithm, IPD, can generate diverse as well as well-performing policies, and outperform competitors relying on multi-objective optimization.",
            "main_review": "Strengths:\n- The authors explain in detail the limitations of prior works on diverse policy seeking, in order to motivate their work.\n- The authors do their best to position their work and make all necessary connections to prior work on constrained optimization and interior point methods. I liked the fact that the authors often provide the intuition behind their design choices.\n- Prior work is covered quite thoroughly, with several citations.\n- To ensure a fair comparison, the authors provide the constrained optimization variant of their competitor TNB (CTNB). Furthermore, the authors try various continuous control benchmarks.\n- Results on various benchmarks appear to be promising, compared to other competitors.\n\nWeaknesses or Questions:\n- The novelty is rather limited. For instance, defining the diversity in terms of the Wasserstein metric is a natural choice, given that the Wasserstein distance between probability distributions is a well-known distance metric. Also, even though the ideas behind using constrained optimization (instead of multi-objective optimization) are worthy of attention, I feel that he proposed algorithm is not particularly novel or interesting,\n- With IPD, all the collected samples are inside the feasible region, which means these samples are less likely to appear in previously trained policies (and hence more likely to be novel). It is not clear though how efficient this process is. Does IPD need to discard a lot of samples before collecting a sufficient number of samples? Is it possible that algorithms that make use of the gradient information can make much faster progress (at the expense of being much more complex)?\n- The authors explain that they use the Wasserstein metric (instead of, say, the KL-divergence), because the  former is a distance metric (unlike the latter). It was not clear to me why this would necessarily be a problem in the context of diverse policy seeking. I understand that the Wasserstein metric has some nice properties (symmetry and proper distance metric), but other metrics such as the KL-divergence may also be meaningful.\n- The paragraph containing Equation (4) could benefit from some clarifications, in the following sense. PPO is a method that uses an evolving policy; after each update, we get the updated parameter \\theta_i. Talking about the state distribution of \\theta _{PPO} may be a bit confusing; does for instance \\theta_{PPO} refer to the latest policy parameter \\theta_i (after last update)? Also, in the final calculation, do the authors set importance weights \\frac{q(s)}{\\rho_{\\theta_i(s)}} equal to 1 as an approximation?\n- The results in Figure 3 suggest that IPD leads not only to more diverse but also better-performing policies, compared to PPO. Why would that be the case? PPO directly optimizes over the cumulative discounted reward, whereas IPD additionally cares about the diversity of the new policy. Given IPD uses the same primary reward function as PPO, why would diverse policy seeking also improve the rewards?\n- A more extensive empirical evaluation could strengthen the results. Given the theory is not very strong on the theory front, I believe an extensive and through empirical analysis would be necessary to demonstrate the effectiveness of the propose ideas under a variety of settings.",
            "summary_of_the_review": "The paper deals with an interesting problem, and presents a practical algorithm to deal with the shortcomings of prior works. The experimental results look promising. At the same time, I feel that the novelty is rather limited, while the paper could benefit from a more through experimental study and various clarifications.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors describe an approach to regularise policy iteration methods towards regions of novel policies. They propose that constraint optimisation is superior to regularised objective functions. They attempt to to derive the algorithm form theoretical principles and demonstrate its superiority on a couple of artificial RL experiments. ",
            "main_review": "The authors propose to use the Wasserstein distance (WD) between policies (taken as state conditional distributions over actions) as measure for policy novelty. The approach of leveraging WD, as far as I understand it, is simple and may be indeed effective. I cannot tell much about the novelty, but at least I did not come across a similar idea. The authors try hard to derive their method from first principles and fail, in my opinion, because what they ultimately do is something different than what was derived before. It is nowhere shown that breaking up  an episode based upon distance computation with a growing sample set $\\tau$ (see Alg 1, l6-8 and page 7, paragraph 1) actually solves or approximates their objective (6).\nAfter all, the method is a heuristic and it would have helped and added to the clarity of the paper if they had treated it as that.\n\nThe exposition is particularly cluttered, full of ambiguities and imprecisions and, thus, hard to follow. A few examples (with different severity):\n\n* The paper has a section \"2. Related Work\" followed by \"3. Experiments\". Where does the introduction of the own work start? Somewhere between the CMDP paragraph and 2.1?\n\n* Throughout the paper it is ambiguous whether we deal with discrete or continuous state and action spaces. That's important because it's nowhere considered whether or under what conditions expectations actually exist. \n\n* The MDP assumed for this approach is not described, but CMDP is. Why? Is CMDP used for this work? What is $c$ then?\n\n* What's the point of the importance sampling business (4)? Later it is ignored as in Alg. 1 $U(\\theta,\\\\{\\theta_j\\\\}|\\tau)$ is used, which I guess is meant to be (3) with $\\Theta_{ref}=\\left\\\\{\\theta_j: j=1..M\\right\\\\}$ and $q=\\frac 3T \\sum_t \\delta_{s_t} \"=\" \\tau$ (a sum of dirac distributions), and $\\tau=(s_1,a_1,r_2,...,s_T,a_T,r_T)$ (If that's the case, it would be super helpful to write it as explicit as this). Computing $U$ subject to the distribution $\\tau$ does work according to (2). So the whole clutter around (4) is superfluous.\n\n* line 6 in Alg 1 is constant as $\\tau$ doesn't change anywhere. I guess the authors mean it's appended to in line 5, but why don't they write in like that\n\n* $r_{\\textrm{int},t}$ is nowhere defined between (5) and (6). If we use $r_{\\textrm{int},t}=r_{\\textrm{int}}$ as in page 5, line 6 then the $t$ indexing and the moving average at the end of Sec. 2.3  and elsewhere don’t make sense.  \n\n* After all this theory, on page 7 one can read that actually something completely different is done: *\"We can simply bound the collected transitions in the feasible region by permitting previously trained M policies $\\theta_i\\in\\Theta_{ref}, i=1,2,...,M$ sending termination signals during the training process of new agents\"*. That's it, no further elaboration, no indication how this relates to what was written above.\n\nThat was the for me the show stopper, and a clear indication to reject the paper. \n\nI did not further make a detailed assessment of the validity and significance of the experimental results, because the precondition for it, i.e. a correct description of the approach is missing. ",
            "summary_of_the_review": "The idea may be novel and significant, but the quality of exposition does not meet ICLR standards. Repeated inaccuracies, omissions, and implicit introductions make it impossible to validate the correctness and significance of the presented experiments. In the end, the actual heuristic being used is not detailed out at all. Therefore I vote to reject the paper.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper aims at enabling the learning algorithms with the capacity of solving the task with multiple solutions. The paper introduces a new metric to evaluate the difference between policies. It also proposes a practical novel policy-seeking algorithm, derived from the interior point method in the constrained optimization literature. The algorithm is evaluated on multiple mujoco tasks.",
            "main_review": "Strength: \nThe strengths are summarized below:\n\n1. This paper brings a new view to optimize for diverse policies via constraint optimization.\n2. The paper provides several nice plots to help understand the algorithm.\n\nWeakness:\nI would like to raise several questions to the paper: \n\n1. In Sec.2.2, the choice of $q(s)$ seems a little arbitrary. Can the author provide some more intuition why the averaged stationary state occupancy is a good choice? Better than uniform distribution seems to be not very convincing.\n\n2. I don't follow the logic of Proposition 2. It is trivial a single trajectory is an unbiased estimation, but it doesn't mean the algorithm can estimate the state occupancy well. Especially the sentences \"The error introduced by approximating the importance weight as 1 will get larger when $\\theta_i$ becomes more distinct from normal policies, at least in terms of the state visitation frequency. We may just regard increasing of the approximation error as the discovery of novel policies.\" What error are you referring to here? Is that the error in estimating state occupancy? \n\n3. In Sec.2.3, I don't think the argument on less hyperparameter tuning is very convincing. Eq.6 also introduces a new threshold $r_0$, unless the author shows the choice of the threshold leads to very little difference in terms of the performance, I think this is still a new hyperparameter.\n\n4. In Fig.3, how is the distribution $\\bar{\\rho}$ chosen? Are they chosen differently for each method? \n\n5. I think some more experiments will make the paper more convincing. Right now the paper only evaluated 3 mujoco tasks, which is hard to convince me.\n\nClarity: The paper needs some improvement on the organization and clarification. However, the figure is pretty straightforward to illustrate the point.\n\nFeedbacks & Questions: Please see details in the weakness.",
            "summary_of_the_review": "Overall, I think the paper is interesting. However, I am still having some concerns aforementioned. I encourage the author to engage in the discussion period and clarify these if there is any misunderstanding. I am happy to re-evaluate if the author convinces me.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}