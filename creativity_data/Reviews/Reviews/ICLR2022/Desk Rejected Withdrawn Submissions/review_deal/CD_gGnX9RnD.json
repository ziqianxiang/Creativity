{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper propose a learnable early-stopping method for preventing meta-level overfitting. This is especially useful when there exists severe discrepancy between meta-train / meta-val / meta-test set. Specifically, they focus on empirical observations such that some of the simple statistics such as expected inner-product between pairs of samples is highly correlated to the meta-test performance. Based on some of such statistics, they propose learning to predict the optimal stopping time by minimizing a proxy objective with meta-validation set. The experimental results demonstrate the effectiveness of their method especially for meta transfer learning setting which reveals severe discrepancy between meta-val and meta-test set.",
            "main_review": "Pros:\n1. The methodology is well motivated by empirical observations. It would have been better if more intuitions were provided on why such correlations take place, but anyway, it is always good to support one's claim based on such meaningful observations.\n2. The performance improvement agains the \"validation baseline\" is large, although not sure if it is statistically significant.\n\nCons:\n[Major] The main weakness of this paper is on the experimental section. First of all, there is no meaningful baseline. While I'm not super familiar with this topic, I believe there exists tones of meta-level regularizations readily comparable to this baseline. I understand the authors may argue that it is too much to compare with all the existing meta-level regularizers, but should be compared with a few of them, at minimum. If the proposed learnable early stopping method cannot outperform them, why do we have to use this method? This concern is very significant in my view, and I cannot give acceptance before it is resolved.\n\n[Minor] It would be interesting to see if the proposed method is effective in solving few-task meta-learning problem, where we are given only a few meta-training tasks such that it is easy to meta-overfit. (See https://arxiv.org/abs/2106.02695)\n",
            "summary_of_the_review": "Overall, while I think the observations found by this paper and the proposed method is very interesting, the aforementioned limitation seems very critical to raise my score. I strongly suggest the authors to add as many relevant baselines as possible. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The submission investigates a few-shot classification setting in which access to a few unlabelled meta-test set examples is allowed; the submission proposes to use these examples to determine the optimal early stopping time for meta-test set performance, which depends on the relationship between the meta-training and meta-test tasks. Several methods to make use of the examples are proposed, including some are simple functions of the statistics of the activations of the main model over time, and some that learn a parameterized function of the representations of the test set examples.",
            "main_review": "### Main Review\n\n#### Strengths\n\n1. Early stopping may be particularly relevant in the few-shot transfer learning setting, in which test tasks differ greatly from training tasks.\n\n1. The details of the submission are easy to understand and the paper is for the most part clear, although some sections are unnecessarily dense.\n\n#### Weaknesses\n\n1. It is not clear that the proposed techniques are particularly relevant for meta-learning. The proposed techniques for optimal early stopping could be straightforwardly employed in standard machine learning setups (in which there is no grouping of data points into tasks). \n\n1. Related to the above point, the submission misses numerous references that propose techniques for automating early stopping in the context of standard machine learning tasks, including some that do not require access to test set examples:\n    - Mahsereci, Maren, Lukas Balles, Christoph Lassner, and Philipp Hennig. \"Early stopping without a validation set.\" arXiv preprint arXiv:1703.09580 (2017).\n    - Liu, Yinyin, Janusz A. Starzyk, and Zhen Zhu. \"Optimized approximation algorithm in neural networks without overfitting.\" IEEE transactions on neural networks 19, no. 6 (2008): 983-995.\n    - Zhang, Xiao, Dongrui Wu, Haoyi Xiong, and Bo Dai. \"Optimization Variance: Exploring Generalization Properties of DNNs.\" arXiv preprint arXiv:2106.01714 (2021).\n\n1. The submission could be greatly improved in clarity by separating the prescriptions / normative claims (\"X should\" / \"we would like Y\") from what is objective reality. In its current form, the text often confuses what is observed or known, with what is proposed or assumed for the narrative of the paper. As two examples:\n    1. The motivation of changing the problem setting from the standard few-shot transfer learning setting to the setting of having access to some target task examples is justified on the basis of \"a need to early-stop\", which is circuitous. There should be an independent motivation provided for why this modified setting is a reasonable assumption.\n    1. In Section 3.2 it is written: \"Assume that for a given target problem, optimal generalization doesn’t happen at the same time as for the source domain, i.e., $t^∗ \\neq t^∗_\\text{valid}$, and more precisely, assume $t^∗ =  t^∗_\\text{valid}$. Typically, a generalization curve is generally increasing between $t_0$ and its maximum, whereas it is generally decreasing after the maximum.\" However, this unimodal-peak phenomenon is not proven or thoroughly demonstrated anywhere in the paper.\n\n1. The broad reliability of the metrics is not clear. Figure 6 and Tables 1 & 2 depict a subset of the metrics in a subset of settings. However, it is unclear how representative this selection is of all possible few-shot transfer settings (datasets × meta-learning methods). Is there any metric that is useful in the majority of cases or on average? Relatedly, many metrics are evaluated directly on the test set, which does not give confidence that the metrics are robust. To avoid this aspect of overfitting, the experiments should have held out a second test set on which the best-performing metrics were then evaluated.\n\n### Minor comments\n\n#### Minor, specific sections:\n1. Names for techniques like \"deep learning\" and \"meta-learning\" do not need to be capitalized.\n2. The first sentence (\"Deep Learning research has been successful at producing algorithms and models that, when optimized on a distribution of training examples, generalize well to previously unseen examples drawn from that same distribution\") could equally describe machine learning in general. Can you make it specific to deep learning, or write \"machine learning?\"\n3. \"Important practical progress has been made in this direction over the past few years.\" But no citations are provided.\n4. I felt that the Goodfellow et al. (2016) quotation in the introduction was unnecessary and could have been paraphrased in the text.\n5. Figure 1: The diagram on the right depicts ptrain, pvalid, ptest as disjoint supports (since densities / colored regions are non-overlapping). The assumption in practice is instead that for the true underlying distributions, ptrain = pvalid= ptest . The requirement here of ptest being disjoint in the few-shot transfer case is too strong---there can merely be distribution shift.\n6. \"This also implies that any algorithm estimating the optimal early-stopping time t∗ should have a very low sample-wise (and task-wise) variance for its estimate of t∗.\" This is not clear to me. Is this a prescriptive claim, or a description of reality?\n7. Figure 2, left: Is this real or imagined data? Are all loss curves in all settings considered monotonic in training iterations?\n8. Is Eq. 5 equivalent to a regret formulation?\n9. \"All the metrics $\\psi_1$ to $\\psi_4$ and their negatives can actually be expressed by a linear combination of the following moments (assuming ReLU activation functions)\" Is a derivation provided?\n\n#### Minor, general:\n1. Having the figures embedded in the middle of the text was confusing. I recommend placing figures only at the top of pages.\n2. In several cases, Appendix sections are not identified (\"App.***.\")\n3. title: The methods are shown on few-shot classification only, so I think a more appropriate title would replace meta-learning with few-shot classification.",
            "summary_of_the_review": "I do not believe the submission is significant in its current form, mainly because of a lack of evidence of the reliability of the proposed metric (W4), and a lack of discussion of and comparison to prior work (W2).",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors address an important but under-explored area of meta-learning training regime which is to identify when to early-stop during meta-training such that it generalizes better to few-shot test tasks. The authors propose multiple such metrics to do this (justified empirically) and show that their method can outperform the current practice of early-stopping based on the average performance on a set of validation tasks. ",
            "main_review": "Strengths:\n--------------\nThe authors address an important practical problem of a meta-learning training setup which so far has not garnered enough attention -- how to decide what is the best model to save during meta-training such that it generalizes well to few-shot test tasks or in other words, how to do early stopping effectively. This is not a problem in a standard supervised training setup because both the class and the underlying data distribution does not change between train and test, which is not the case for a meta-learning training. The authors design multiple such metrics to determine the time $t$ during training which produces the best model (most likely). Authors also show that these metric(s) can outperform the current practice of saving the best model using average performance of the validation tasks, at times significantly if there is a large domain-gap between meta-train and test. I like the presentation of the paper -- how the authors have started from defining a simple metric (inner-product of last-layer activations on a test task) and progressively improved it to handle different scenarios and finally handling the case when the data distribution is entirely different between train and test. I also liked the diversity of datasets chosen for the experiments which could show why the simple metric was not sufficient and why modifying that metric was important. Overall, this paper provides a useful tool for meta-learning training and I find it has the potential to be widely adopted if some concerns around it can be mitigated (see my points below). \n\nAreas of Concern:\n------------------------\n* The whole paper is based on conjectures and solutions coming out from running a set of experiments. There is not a lot of theoretical or intuitive justification as to why a particular metric is working for a method/dataset and why it is not working for others. For example, the authors mention that for some datasets, the information coming out from the last layer features are the deciding factor while for some, it is not the last layer features but some intermediate features. The paper does not explain why that is happening - is it because the domain-gap between the train and test set such that for one, the features are common until the last layer and for some, the features diverge at some early layers? Similarly, at Fig 6, it shows a particular metric is good for MAML/CNN/Aircraft whereas another metric is good for Proto-Nets/CNN/VGG. The fact that the proposed method is predominantly empirical creates a doubt in my mind as to how do we know that these set of metrics are enough/exhaustive? I'd have to liked to see a more thorough analysis between domain-gap (quantifiable if possible) between meta-train/test and then using that to decide which metric would be best rather than trying out all possible metrics.\n* Following up on the previous point, it also creates a question regarding the computational cost of this method to pick the best early-stoping time $t$. If I understand the implementation correctly, to pick the best layer for which the product of activations correlate best with the test accuracy, we need to run it for all the layers of a given network and then pick the best. It means for a deep network like ResNet-101, we need to run it for all the 101 layers? And these are not the only set of metrics that the papers consider. Therefore, I am not sure if this method is practically feasible given its high computational demand during meta-training. To reiterate, using some other way of analyzing the domain-gap between train and test might help to reduce the search-space significantly.\n* Finally, there is a simple baseline that I think the authors have missed. As the authors are using a randomly sampled task from the test distribution, how about using the performance of the model (during meta-training) on this task (rather than the average on the validation tasks) as the proxy to save the best model? How would this compare to the method(s) proposed in the paper? I might be wrong but I believe it will work non-trivially better than the validation baseline especially when there is a large domain-gap.",
            "summary_of_the_review": "Positives:\n-------------\n* Tackles a problem which has not received enough attention and proposes a method that has novelty and also empirically verified.\n* The narrative of the paper is nice and clear; dataset and baselines in experiments have sufficient diversity to back the claims.\n\nNegatives\n--------------\n* Method is not theoretically or intuitively justified, all based on empirical results on the datasets chosen for the experiments -- questions the universality of the method.\n* Computation cost increase to incorporate this during training seems non-trivial, especially with deeper networks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns. ",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Meta-learning methods typically apply early stopping by performing adaptation until validation accuracy starts to decrease. This paper claims that this practice is suboptimal because of the distribution shift inherent in testing on previously unseen tasks. They thus propose learning a different criterion: a weighted sum of three statistics of neural network activations.",
            "main_review": "Strengths\n- The problem of predicting an optimal stopping time from activation dynamics is interesting and is likely to improve meta-learning.\n\nWeaknesses\n- Section 2.2 is very dense for the core information it attempts to convey. Furthermore, the observations are presented in a way that makes it seem like they contradict each other: observation 1 claims that $\\psi_1$ is predictive of target accuracy, but observation two immediately says that $\\psi_1$ is only predictive in some layers. Observation 3 says that even the statistic $\\psi_1$ is not always predictive and one needs to consider other statistics. These \"observations\" are presented confusingly. I think most of section 2.2 can be summarized into \"Together, the moments $m_1, m_2, m_3$ can form a good predictor of target accuracy, potentially because their span includes the metrics $\\psi_1, \\psi_2, \\psi_3, \\psi_4$\".\n- The proposed method is entirely based on empirical observations, and the evidence for the efficacy of the method is also experimental. Yet, the only experiments presented are Tables 1 and 2, which only compare against the baseline of using a validation set and the optimal choice. To provide convincing evidence, I think standard deviations for reported results and ablations for the design choices ($\\psi_i$ only, etc.) are necessary.\n- Figure 1 is hard to understand for many reasons. p(x_target) appears twice, and it is not immediately clear that the two orange areas correspond to few-shot and few-shot transfer learning. At first glance, the \"training classes\" images seem to only explain the scatterplot under it, etc.\n\nMinor comments\n- \"App.***\" appears several times in the text. Is this a typo?\n- Large empty space on page 11.\n- Section B.2.2 empty",
            "summary_of_the_review": "While I find the problem setting interesting, the writing is confusing, and the experiments do not sufficiently demonstrate that this method works reliably.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}