{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposed an input-denoising and feature-restoring (IDFR) method to defense adversarial examples/samples, where input-denoising is designed to transform an adversarial image to its clean version, and the feature-restoring is used to restore those partially damaged clean samples so that they can be correct. Experiments conducted on benchmark datasets show that the proposed method outperforms the selected comparison methods. ",
            "main_review": "strengths: \nThis paper is well written and organized. \nThe proposed FR learning/training algorithm was clearly introduced. \n\nweaknesses: \n1. will the proposed methods update the weights of the classifier? How are the ID and FR trained? Are they trained one by one or jointly end-to-end trained? Have the authors evaluated the defense cost of the proposed method compared with other methods (training and inference)? \n2. How do the authors generate the input, i.e., AE, training dataset? Do the authors assume the defender knows the attack methods? Will the proposed method works on agnostic attacks? What strength of the attack methods (e=4, 8, or 16?) are used for training? \n3. some SOTA papers are not compared, like: \"Liu, Zihao, et al. \"Feature distillation: DNN-oriented JPEG compression against adversarial examples.\" 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2019.\" \n4. From my understanding, the whit-box means the attacker knows your defense method, instead of the target model.\n5. the experiments only show the defense results on infinite-based attack methods, i.e., FGSM, CW(infinite) and BIM. what about the defense efficiency on the other two types? \n6. why the results from table 1 and 2 and table 5 not consistent? From table 5 the increase tends to the model’s defense performance with the increase of the adversarial attack powers, i.e. when e increases, but table 1 and 2 show the opposite trend. \n7. some references are incorrect, like DAE (Denoising AutoEncoder) in section 3.2. \n\n\n",
            "summary_of_the_review": "Overall, this paper is well written, with claims that have minor issues. \nsome SOTA papers are not cited and compared. \nsome parts of the paper are not clear. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new input purification based adversarial defense named IDFR, including an input denoiser (ID) and a hidden lossy feature restorer (FR). The authors claims that previous methods ignored the purification impact on clean examples, thus they propose the additional FR module optimized by convex hull optimization.",
            "main_review": "Pros:\n+ The motivation of this work is quite clear for me.\n+ Compared with the listed baselines, the quantitative results showcased in experiments seem satisfying.\n\nCons:\n+ Some definitions and formulations are confusing and even wrong, e.g., \n1) $f_{cls}$ in Eq. (1) needs to be changed as $f$ according to the problem statement; \n2) $f=f_{enc}\\circ f_{cls}$ in Figure. 1 needs to be changed as $f=f_{cls}\\circ f_{enc}$; \n3) $f_{cls}$ in Eq. (2) needs to be changed as $f$;\n4) the definition of $I_w (\\cdot)$ is not clear since the authors also use format $I_w (\\cdot, \\cdot)$ for $I_w (x, x')$ in Sec. 3.2;\n5) more crucially, the formulation of convex hull $Co (\\hat{\\textbf{x}} \\cup \\hat{\\textbf{x}}')$ seems wrong, since $\\beta h_{x_1} + (1-\\beta) h_{x_2}$ should also be included in $Co (\\hat{\\textbf{x}} \\cup \\hat{\\textbf{x}}')$ for $\\forall h_{x_1}, h_{x_2} \\in \\hat{\\textbf{x}}$ and $\\beta \\in [0, 1]$ according to the definition of the convex hull, which is the same for convex combination in $\\hat{\\textbf{x}}'$. Thus this confusing formulation makes the claimed optimization for FR become not quite convincing for me.\n+ The reason \"As while-box attacks are less likely to happen in practical systems, defenses against\nblack-box attacks are more desirable\" stated in Sec. 4.2.1 is not convincing for me, unless one of the following can be achieved:\n1) the authors would better to provide a reasonable defending scenario to explain this reason.\n2) the authors could provide more white-box attack results, including strong attack like Auto-Attack [1].\n2) the authors could also consider if the attacker knows about defense, i.e., the attacker trains a local defense similar to IDFR and then utilizes BPDA [2] to bypass the defense.\n\n[1] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse \nparameter-free attacks. ICML, 2020.\n[2] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), 2017.\n",
            "summary_of_the_review": "The current version is below the acceptance bar of ICLR, but I'm willing to raise the overall rating if authors can well address the above concerns.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose a new defense against adversarial examples for image classification, based on *Input Denoising* and *Feature Restoring*.\n",
            "main_review": "The idea of preprocessing inputs to counter the effects of adversarial perturbation was already proposed in other works (many of them are cited, others, e.g., [3] are missing from related works). The authors should better discuss how their work differs from related approaches and it poses an advancement in the state-of-the-art. Nevertheless, the approach and its formulation are interesting.\n\nThe main issue when proposing a new defense against adversarial examples resides in its evaluation: as it can be done only empirically, often it can overestimate the effectiveness of the defense when not thoroughly conducted. Many researchers discussed this and proposed several best practices [1]. For instance, it’s remarked that adversarial robustness evaluations must be conducted in worst-case scenarios, considering the stronger adaptive attacker.\n\nUnfortunately, several pitfalls are present in the experimental evaluation conducted in this work:\n- The authors evaluate their proposed approach against three attacks; FGSM is a very weak attack that does not provide optimal solutions; BIM and CW are stronger but they are run with only 20 iterations, which is a very small value: usually, this kind of attack needs up to 1000 iterations to converge (a usual value is 100, but often it is not enough). Furthermore, other kinds of attacks should be applied, especially parameter-free attacks [2] that overcome the issue related to the tuning of attack parameters (such as the number of iterations and the step size) and provide state-of-the-art performance.\n- The white-box setting should be considered as the main evaluation scenario, whereas the authors privilege the black-box scenario (which is not very relevant when proposing a new defense against adversarial examples).\n- The other considered defenses do not provide state-of-the-art performances, and many of them were broken in successive works. To have a comparison, you can refer for instance to published leaderboards (e.g. https://robustbench.github.io/).\n\nIn addition, the authors state that other defense methods “ignore the information loss/the partial knowledge forgetting learned by the CNN model for clean samples during the adversarial training/purification process, which leads to a decrease in the target classifier accuracy”, but the provided results do not strongly support this claim.\n\nOther comments:\n- The threat model is not properly discussed, \n- Figure 3 is not very clear, the meaning of each bar should be better explained.\n- The writing quality of the paper can be improved\n- I tried to reproduce experiments from the provided source code, but I was unable to do it due to missing pre-trained models and libraries requirements (for instance, I could not determine which version of ART library I had to use, as interfaces changed)\n\nTo sum up, I can not recommend the publication of the paper in its current form, but I encourage the authors to improve their work, in particular by performing a more reliable robustness evaluation, as the proposed approach seems interesting.\n\n\n- [1] Carlini, N., Athalye, A., Papernot, N., Brendel, W., Rauber, J., Tsipras, D., Goodfellow, I.J., Madry, A., & Kurakin, A. (2019). On Evaluating Adversarial Robustness. ArXiv, abs/1902.06705.\n- [2] Croce, F., & Hein, M. (2020). Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. ArXiv, abs/2003.01690.\n- [3] Meng, D., & Chen, H. (2017). MagNet: A Two-Pronged Defense against Adversarial Examples. Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security.",
            "summary_of_the_review": "### Strengths:\n- interesting approach, though the idea is not totally novel\n\n### Weaknesses:\n- evaluation is not fair\n- not clear the advancement with respect to the state of the art",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors proposed a method named Input Denoising (ID) and Feature Restoring (FD) to defend adversarial attack. The authors apply ID on the input images and apply FD on the high-level representations to respectively weaken adversarial signals and restoring original signals. The authors mention a property of convex hull when designing FD. The authors also compare the proposed method with a good number of other methods on several datasets. \n\n",
            "main_review": "Strengths\n1)\tUsing FR to restore original feature is a good idea because most of the current works concentrate on defense.\n2)\tUsing linear augmentation method [1] to train FD is another good point.\n3)\tThe proposed method outperforms other methods in the comparison.\n\n[1] Hongyi Zhang, Moustapha Ciss´e, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In 6th International Conference on Learning Representations, ICLR 2018 \n\nWeaknesses. \n\n1)  The mathematical presentation, equations and related claims are problematic. Some mathematical errors make the paper difficult to read, but some others would mislead readers. \n\na. Theorem 1 is well known. The authors have no point to prove it. For example, in Boyd and Vandenberghe [2], it has mentioned that “The convex hull of a set C, denoted conv C, is the set of all convex combinations of points in C” The proof gives an impression to readers that it is your contribution but in fact, it is not. \n\nb. Eq. 4 is wrong. The set on the right-hand side is a subset of the set on the left-hand side. More clearly, the set on the left-hand side includes the points, e.g., $\\alpha h_x+(1-\\alpha) h_x$ but they are not included in the set on the right-hand side, according to definition and property of convex hull. \n\nc. Page 2, Eq. 1, how can $f_{cls}$ takes both $x^*$ and $h_{\\tilde{x} }$ as input? They can have different dimensions. \n\nd. Page 2, The authors said that “$x^*=\\pi_1(x’)$ that is as close to the clean/original example $x$ as possible”. It means that $x^* \\neq x$, but in Eq. 1 authors constraint $x’$ by $||x^*-x’||\\leqslant \\varepsilon $, which implies that  $||x-x’||>\\varepsilon $ can happen. In other words, attack budget can be larger than $\\varepsilon$. The actual budget is in fact not clear.\n\ne. In page 4, $I_\\omega (\\cdot) $ has one input but in equation 2 and 3, it becomes $I_\\omega (x, x’)$. \n\nf. Algorithm 1: $A(\\cdot\\)$ is not defined, although I think that it should be attack model/ \n\n\n2) White-box attack results (Table 4): The experiment results indicate that Fast AT is not as good as preprocessing methods. Are the adversarial examples derived from the target model without considering the preprocessing defense schemes? If yes, it is an unfair comparison for Fast AT etc. \n\n\n\n3) Table 3: the preprocessing methods, e.g., JPEG are designed for large images. Why don’t include them in Table 3? \n\n\n4) Database problems: \n\na) page 7, “ImageNet …… consists of 30,000 ….. We use 25,000/5000/10,000 images for training/validation/testing respectively” Do you mean some images contain in more than one dataset? Or a typo? \n\nb)Page 7, the authors list 5 databases but I cannot find out the MNIST results and SVNH results.\n\n\nOther presentation errors\n\nAbstract, against adversarial attacks, which damages\n\nPage 4, with the input an AE\n\nPage 4, DAE. the U-Net\n\nPage 7, results, Table 3\n\n\n[2]\tStephen Boyd and Lieven Vandenberghe, Convex Optimization, page 24\n\n",
            "summary_of_the_review": "Due to the mathematical errors and other issues, including databases and comparison, significant efforts are needed to improve the quality. In my opinion, this version is not enough for ICLR.\n\n\n\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N.A.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}