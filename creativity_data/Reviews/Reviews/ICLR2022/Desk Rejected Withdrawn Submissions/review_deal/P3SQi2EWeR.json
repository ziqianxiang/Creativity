{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a new convolutional kernel, a circular kernel, for image classification. To get a receptive field corresponding to the circular kernel, this paper applies bilinear interpolation inside the original square receptive field. Unlike the previous deformable convolutions, this method does not require bilinear interpolation depending on an input feature map, thus it is more efficient. This paper further adds this circular convolution into the search space of NAS and verifies its effectiveness. The experimental results show that standard CNNs (e.g. ResNet and DenseNet) with the circular kernel can improve their original performance for image classification tasks.",
            "main_review": "+ The idea of a circular convolution is a simple yet reasonable choice for a new convolutional operation.\n+ Results show that CNNs with standard convolution and the proposed circular convolution can achieve better performance than that of the original model.\n\nConcerns:\n- It does not seem that the effectiveness of the circular kernel is supported well by the experimental results. Firstly, the standard kernel shows better performance than the circular kernel but the circular kernel requires additional computational costs. Secondly, there have been many attempts so far that replace the standard convolution with a new convolutional operation, but there is no comparison with these methods in this paper. Such a deeper analysis of the proposed method would be nice.\n- Although this paper mentions that the new search space (i.e. original DARTS + circular convolutions) shows better, it seems natural because the search space is expanded.\n- This paper claims that a large kernel is an important ingredient, but a CNN with smaller kernels achieves better performance according to Table2. Thus, the claim is somewhat unclear to me.\n- It would be better to describe the detail of circular separable/dilated convolutions. ",
            "summary_of_the_review": "The idea of circular convolution is simple yet promising. My major concern is about the effectiveness of the proposed idea (see concerns above). Hopefully, the authors can address my concern in the rebuttal period. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper suggests using a circular kernel with a concentric and isotropic receptive field to replace the standard squire kernel. The authors show that this simple modification can improve the generalization performance.  Then the author proposes to mix square kernel and circular kernel, the optimal mixture is found by a neural architecture search (NAS) method. \n\n",
            "main_review": "**Strengths**\n- The idea, that is inspired by the biological visual system, is interesting.\n- The proposed method is easy and clean. Under the NAS setting, it shows improved performance. \n\n**Weaknesses**\n- The motivation of using a circular-shape kernel is not mathematically grounded. Being just similar to the biological system would not guaranty success in deep learning. \n\n- The experimental results indicate rather subtle improvements. For example in Table 1, the performance difference between squre and the circular kernel is negligible. \n\n-  The NAS experiments are not very related to the key method of the proposed method i.e. circular-shaped kernels. \n",
            "summary_of_the_review": "The biological motivation is interesting however not enough. The improvements gained by the main idea proposed in the paper (circular vs square kernel) are rather marginal. Including more theoretical analysis on circular kernel over square kernel would make the method more well-motivated and sounded. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work replaces the square kxk convolutional kernel with a circular-shape kernel of radius $k$. The authors first show that this simple modification can improve the generalization performance. A NAS method is then used to find the optimal mixture of square kernel and circular kernel.",
            "main_review": "[Strength]\n1. The idea of using isometric, circular kernel is more or less rarely discussed in previous literatures.\n\n[Weakness]\n\n1. The motivation of using circular-shape kernel is not convincing. Being more similar to biological receptive field means little in deep learning. Actually, the design of modern deep neural networks is fundamentally different to biological neural networks. I would expect more theoretical insight of the motivation.\n\n2. The improvement of the circular kernel is very marginal. In Table 1, the difference between Square and Circle is only 0.07%. No need to mention that the training epoch is only 90, which is insufficient for stable convergence. The modern training recipe usually use 360 training epochs. In Table 3, the same marginal improvement after NAS.\n\n3. The training setting is not SOTA. For example, ResNet-50 can easily achieve 79.0% (https://paperswithcode.com/lib/timm/resnet). The one reported in this paper is only 76.9%. Please consider using the SOTA training setting in comparison.\n\n4. The NAS part is not very interesting. It is a simple application of DARTS with different kernel size (and shape) and has nothing to do with the key argument of this work. It would be better to remove this part and focus on the circular kernel design only, in order to strengthen the latter one.\n\n5. The numbers reported in Table 4 are not under fair training setting (Please correct me if I am wrong here). As mentioned above, the accuracy is heavily affected by the training setting so it is hard to tell whether the difference is due to the different training setting or not.",
            "summary_of_the_review": "The proposed circular-shape convolutional kernel is not well-motivated. There is little theoretical insight of why circular-shape kernel is better. The empirical improvements of the proposed method is very marginal.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper experiments with circular kernels in convolutional networks. Applying the circular kernels to pixel images involves bilinear interpolation, which is done efficiently. The kernels slightly impair image recognition performance when they replace 3x3 kernels in common architectures. However, the paper also incorporates them into neural architecture search, and here they result in better performance. Circular kernels also make performance more robust to severe image shearing and rotation. ",
            "main_review": "Strengths: \n- The idea is interesting\n- The writing is clear\n- The method is elegant and efficient \n- In the context of architecture search, the method seems to result in improvements\n \nWeaknesses: \n- The performance differences are subtle and standard deviations are not given in tables 1 and 4\n- I am not sure I understood the point of Appendix A and the related section in the main body. Is the point simply that the bilinear interpolation affects the gradients? Does the math make this any clearer? \n\nMinor: \n- The motivation in terms of retinal ganglion cells seems narrow, since this is one of many groups of cells in the visual system. \n- In Figure 4, it looks like the shear plot extends to 90 degrees. In that case is the image not a line? \n- Rephrase, \"It indicates that the weights at the four corners of large square kernels and stacked 3x3 square kernels are sparse.\" I don't think you really mean \"weights\" or \"sparse\" here. \n- In the caption of Figure 1, please say explicitly what the dots and dashed lines indicate. \n- On page 5, the phrase \"circular convolution\" is used to indicate convolution with a circular kernel, but it has another established meaning.\n- Can any more be said about why this approach is useful, particularly with larger kernels? \n- Can any more be said about why large circular kernels are mainly incorporated into reduction cells?  \n ",
            "summary_of_the_review": "This is an interesting architectural change that leads to small performance improvements, although more runs would be helpful in tables 1 and 4.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}