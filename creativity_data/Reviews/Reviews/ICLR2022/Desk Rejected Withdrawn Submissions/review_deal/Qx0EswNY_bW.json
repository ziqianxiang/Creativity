{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "For multivariate time series forecasting, this paper proposes to use a tensor network to model the variable space and improve the quality of the variable space by designing the series-variable encoder. Under the variable space, this paper also proposes an N-order residual connection approach and the skip-connection layer for processing the long-term data. The proposed model MVSRTN achieves good results on some datasets. However, this paper has not well explored MVSRTN, and the results are not very competitive. In addition, there are many typos in the paper.",
            "main_review": "Pros:\n- The proposed model is feasible in practice.\n- Empirical results are not bad.\n\t\nCons:\n- The main contribution (i.e., forming a variable space and decomposing it through tensor train) is not really novel. There have been plenty of works solving dimensional curses through tensor decomposition such as [1,2,3].\n- In Table 1, there is no clear advantages compared to previous methods. MVSRTN  slightly outperforms MTGNN-sampling only on Solar-Energy. On Exchange-Rate, MVSRTN is slightly better. However, On Traffic, MTGNN-sampling is better at 24, and on Electricity, MTGNN-sampling exceeds MVSRTN. Therefore, this causes concerns of the performance of MVSRTN. And if the N-order residual connection approach is indeed efficient at long-term sequences, why will MVSRTN fail on Traffic at 24, while MVSRTN is better at 6?\n- There are lots of related works about Multivariate Time Series (https://github.com/Alro10/deep-learning-time-series). And more works may need to be considered such as informer[4].\n- There lacks of investigation on Series-variable Encoder, for example, which self-attention component is more important? The series-level one or the variable-level one?\n- Punctuation needs to be written at the end of equations.\n- There lacks an arrow above $b_2$ in FIgure 3(c).\n- I find too many typos in the paper:\n  - \"In literature, there have been some approaches to capture interactions in series history from different perspectives. research utilizing\" -> \"Research utilizing\";\n  - \"Tril refer to lower triangular\" -> \"Tril refers to lower triangular\"\n  - \"we leverage Mean Absolute Error\" -> \"We leverage Mean Absolute Error\"\n  - etc.\n\nI sincerely hope that the authors can write the paper carefully!!\n\nAdditional Questions:\n- Why use the 1-D CNN at first?\n- Are scale parameters $\\alpha$ and $\\beta$ trainable? How to set them in experiments?\n- How to split train, validation, test set?\n\n[1] Yu Liu, Quanming Yao, Yong Li. Generalizing Tensor Decomposition for N-ary Relational Knowledge Bases. WWW 2020.\n\n[2] Yinchong Yang, Denis Krompass, Volker Tresp. Tensor-Train Recurrent Neural Networks for Video Classification. ICML 2017.\n\n[3] Yu Pan, Jing Xu, Maolin Wang, Jinmian Ye, Fei Wang, Kun Bai, Zenglin Xu. Compressing Recurrent Neural Networks with Tensor Ring for Action Recognition. AAAI 2019.\n\n[4] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang.\nInformer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting. AAAI 2021.",
            "summary_of_the_review": "In terms of the comparable empirical results, insufficient exploration, and poor writing, the contribution is quite weak and there is a lot of room for improvement.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a deep learning architecture for time series forecasting called MVSRTN. The model is composed of 3 blocks with skip-connections in-between them: a 1) \"Series-Variable Encoder\", 2) an \"N-Order Residual Tensor Network\", as the authors call it, and 3) the output layer. Out of these, 1) is essentially a 1D CNN combined with (causal and non-causal) self-attention, and 2) seems to be a tensor network built from taking the tensor product of the sequence entries, and then contracting that by a TT-rank-constrained weight tensor. The computation of the tensor network output is formulated as a recursion across time-steps, and motivated by residual networks, an identity mapping with respect to the hidden state at the previous time-stamp is added to the recursive formulation. Then, by the analogy with higher-order solvers for ODEs, a \"higher-order residual connection\" is introduced. The model is applied to four time series forecasting datasets, where some improvements are achieved, and an ablation study is included to show how the various model blocks contribute to the performance.",
            "main_review": "Overall, the paper is quite hard to interpret due to its writing style, such as sentence structure, confusing terminology, and convoluted explanations. Most of the model seems to be a straightforward combination of existing building blocks, such as CNN, self-attention, and tensor networks. On the other hand, the proposed novel component, the residual connection in the tensor network, does not seem to be theoretically understood.\n\nFurthermore, the explanation given in Section 4.2 seems to be incorrect. First of all, the tensor in eq.(6) is already a CP rank-1 tensor, and hence, I'm not sure why they would want to approximate this by a TT low-rank tensor, as they say in the explanation between equations (6) and (8). In particular, CP rank-1 implies TT rank-1, so there is nothing to approximate here really. It seems to me what they are actually doing is they are taking a TT-rank-constrained weight tensor, and using that to contract the rank-1 feature tensor from eq.(6), which is a completely different thing than what they claim they do. I was only able to decode that this is what they are actually doing from eq.(10), since the given explanation suggests something else, which makes me question whether the authors properly understand what they are doing. By the line above eq.(10), it seems that this weight tensor is also constrained to be symmetric in the sense that that all of its TT-cores are equal, under which there exist no approximation guarantees for TT-decompositions anyway as far as I know. With this in mind, Claim 1 is completely meaningless regarding the proposed methodology, and its contents seem to be tautological either way.\n\nThe residual approach in eq.(11) is given by a straightforward modification of eq.(10), but there is no discussion given about how this translates back to the tensor network structure, or what this means from a theoretical point of view. The higher-order residual connection in eq.(12) is not properly explained, and what the coefficients $\\alpha_j$ and $\\beta_j$ are supposed to stand for (or the details of how they were acquired). Overall, only little discussion is given about the novel idea in the paper, which is combining the recursive formulation of the tensor network with (higher-order) residual connections. \n\nThe strength of the paper for me is that the experiments look fairly promising, where it is shown that some improvements can be achieved over some seemingly popular baselines (such as TPA-LSTM and MTGNN) on the task of time series forecasting, which is arguably an important problem.  The ablation study is also relatively interesting, which shows how the various model components contribute to performance. However, especially with highly empirical papers such as this, in my opinion the evaluation would have to be more detailed and it could include more than this one task, or at least more datasets should be included for this one.",
            "summary_of_the_review": "To summarize, I believe there are several issues with the paper as of now, such as clarity of writing, convoluted or misleading explanations, and the lack of proper understanding and theoretical analysis of the proposed idea. I think it would be beneficial to better understand the proposed methodology before the paper is considered for publication. So overall, the following improvements would help: improving the clarity of writing, understanding and explaining what the proposed residual tensor network is doing, and either providing a sufficient theoretical analysis, or at least a sensible interpretation for it. The experiments could also use more datasets and/or a larger range of tasks, and a detailed investigation of the proposed residual connections in the tensor network on some simple \"demo problems\", which could illustrate where the improvements come from.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed the MVSRTN architecture for multivariate time series modeling. The MVSRTN consists of an encoder to extract latent variables and residual tensor network (TN) blocks to capture the interactions in the latent space.\n\nThe main contribution of the model is the TN block part. In particular, the authors used tensor-products to fully represent the latent variable space (Eq 2). Then, they proposed an N-order residual TN block to alleviate potential gradient problems of high-order TNs in long-term time series. They conducted experiments on four multivariate time series data for prediction. Moreover, an ablation study shows the effectiveness of the proposed residual TN blocks.\n\nCombining ResNet and TN is an interesting direction. However, I think this paper did not present this problem sufficiently and some notations are confusing.\n",
            "main_review": "Strength\n\n-The proposed framework combines a self-attention encoder and TNs to fully capture the complex correlations. The expressive power of TNs has potential in modeling complex data.\n-The proposed N-order residual connections in TN improve the performance of traditional TNs.\n-The ablation study shows the effectiveness of using TN blocks and skip connections in TNs (Section 5.4 and 5.5).\n\nWeakness\n\n-There are some works concerning TNs for time series and residual TNs. For example, [1] constructed variable spaces using tensor-products. [2] used tensor-products to capture long-term dependencies. [3] discussed residual MPS. The authors did not mention them in the related works.\n\n[1]. Toth, C., Bonnier, P., & Oberhauser, H. (2020). Seq2tens: An efficient representation of sequences by low-rank tensor projections. arXiv preprint arXiv:2006.07027.\n[2]. Yu, R., Zheng, S., Anandkumar, A., & Yue, Y. (2017). Long-term forecasting using tensor-train rnns. Arxiv.\n[3]. Meng, Y. M., Zhang, J., Zhang, P., Gao, C., & Ran, S. J. (2020). Residual Matrix Product State for Machine Learning. arXiv preprint arXiv:2012.11841.\n\n-In page 6, the authors claimed that TN suffers from gradient problems, so they proposed the N-order residual TN to address this issue. However, ResNet was proposed to solve the degradation problem. Moreover, the architecture of DNN is quite different from TN. So I think the authors should provide more discussions about why skip connection works for TN and experiments about how the depth of TN affects the performance.\n\n-Claim 1 seems to be trivial. Most tensor decompositions without constraints satisfy Eq 8. Moreover, in Eq 10, the authors assume the core tensors \\mathcal{W} are equal, which is a uniform MPS. Does Claim 1 still work for this case? If not, Claim 1 seems redundant for this paper.\nThe notation is a little bit confusing, especially in Section 4.3. Besides, \\mathcal{T} in Eq 6 represents the variable space of $x$, and \\mathcal{A} should be the coefficients according to Eq 10. Why does \\mathcal{A} equal to \\mathcal{T}?\n",
            "summary_of_the_review": "The idea has something interesting, but some related works are not well discussed and compared. Some technical insight and experiment evaluation is not well explained or supported.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper considers the problem of forecasting of multivariate time series. The authors propose an architecture for such forecasting that incorporates several layers, including a residual tensor network layer. The idea is to tensorize (via outer products/Kronecker products) the features passed on from the encoder layer and then handle these very large tensorized features with a tensor network. The point of the tensorization is to better incorporate the effect of combinations of variables from different time steps. Experiments are done on four benchmark datasets.",
            "main_review": "# Strengths\n\nS1. The problem itself is interesting and it's easy to see why this is a relevant problem.\n\nS2. Tensorizing features and then using tensor networks to work with those features is an interesting approach (although it's unclear how this is done; see below).\n\n# Weaknesses\n\nW1. It is unclear how the tensor network is used. Is it used to approximate the tensor $\\mathcal{T}$ in Eq. (6)? Throughout the paper up until about half way through Section 4.2, this is what I thought, but then after reading through the second half of Section 4.2, it seems like this is not the case, and that the only thing you do is to use a single tensor $\\mathcal{W}$ in Eq. (10). If it’s the latter, then it’s not clear why you e.g. include Claim 1.\n\nW2. It is not clear what the point of Claim 1 is. It is well known that tensor trains (and in fact, any tensor networks with tree shaped structure) can exactly decompose arbitrary tensors. See Theorem 8.8 in Ye and Lim [2019, https://arxiv.org/abs/1801.02662]. This also means that the first contribution in the list of contributions at the end of Section 1 really isn’t a new contribution. \n\nW3. This also relates to the points above. If you do use tensor train to decompose $\\mathcal{T}$ in Eq. (6) (which is what e.g. Claim 1 suggests), then what is the point of doing that? $\\mathcal{T}$ is already in decomposed format since it’s a rank-1 CP tensor. Similarly, in Definition 1, the set of all tensors of the form in Eq. (2) does not make up the whole space $\\mathbb{R}^{D^T}$ (which the definition seems to suggest); the former is the set of all rank-1 tensors which is a strict subset of $\\mathbb{R}^{D^T}$. \n\n# Minor other comments\n\nC1. In Sec. 1, 2nd paragraph, you say that “In 2011, the ‘Switzerland’ variable changed significantly earlier than the other,” but it looks to me like it changed later?\n\nC2. Minor things throughout the paper, like letters at the start of sentences not being capitalized, periods in places where there shouldn’t be, incorrectly spelled words, and other similar issues.\n\nC3. In Sec. 1, 4th paragraph, you say that “this is still incomplete for the variable spaces shown in Figure 1 (c).” It's not clear what you mean here. Are you saying that the NN shown in the figure is insufficient for a multivariate time series? Or is the figure meant to illustrate what a multivariate time series looks like?\n\nC4. Text in figures is way too small in many places.\n\nC5. Below Eq. (3), you say that $W_C$ is of size $kD \\times d$. Should this be $k \\times d$?\n\nC6. What exactly does the Tril function in Eq. (4) do?\n\nC7. In Eq. (7), you first write the core tensors $W(1), \\ldots, W(T)$ as if they were separate tensors, but then in the next line it seems like you assume that all the core tensors are the same? This is confusing.\n\nC8. Also, in Eq. (7), are the tensor modes corresponding to the labels $h_0$ and $h_T$ singleton dimensions? If not, the decomposition is not a tensor train decomposition. In the tensor train decomposition, the cores at the end are in fact matrices. \n\nC9. Below Eq. (7): Should it be $x_t \\in$ {1,…,$d$} and $y_t \\in$ {1,…,$r_t$} rather than what you have now? Also, it’s not accurate to call the summation index $h_t$ rank---it is just a variable which goes from 1 to $r_t$, where $r_t$ is the rank. \n\nC10. Above Claim 1: You say that “it is worth noting that $\\mathcal{A}$ and $\\mathcal{T}$ are equal when the error is allowed.” What does that mean?\n\nC11. In Claim 1: What do you mean by “their complexity are equal to $O(d^T)$”. Do you mean storage complexity?\n\nC12. Above Eq. (9): What do you mean by L2 regularization of a vector? Do you mean that you normalize the vector to have unit length? \n\nC13. In Eq. (10), $h_t$ appears on both sides of the equation. Should it be $h_{t-1}$ in the bottom line?\n\nC14. In Eq. (15), $M W_M$ is of size $d \\times 1$, but $W_O$ is of size $d \\times D$, so it doesn’t match. $M W_M$ needs to be transposed for it to match with the size of $W_O$.\n\nC15. In Eq. (16): The left hand side is $Y_{s}’ = Y_{T+h}’$. But where does the h appear in the right hand side?\n\nC16. How were the coefficients in Table 2 chosen?\n",
            "summary_of_the_review": "The topic tackled by this paper is interesting. However, there are too many things in the paper that leave the reader guessing what the authors actually mean. In particular, as I raise in points W1-W3 above, there are several things that are unclear about the residual tensor network component. There are also many spelling mistakes and other similar typos that would have been detected in a careful proof reading. With these points in mind, the paper is below the acceptance threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}