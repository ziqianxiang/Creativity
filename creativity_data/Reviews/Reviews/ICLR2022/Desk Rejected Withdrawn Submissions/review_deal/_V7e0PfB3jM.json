{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper extends ell_0 SSC of Yang et al. 2016 in two aspects: 1) handling data noise by penalizing the self-expression residual (method is not new, but analysis is), and 2) handling high-dimensionality by preprocessing with a random projection (method is mostly the same as [a] which is not cited in this submission). The paper presents a solid theoretical analysis of the dimensionality-reduced noisy SSC under the deterministic as well as random data models. \n\n[a] Dimensionality Reduced \\ell_0 Sparse Subspace Clustering, AISTATS 2018",
            "main_review": "Novelty:\n\nAs mentioned above, the dimensionality reduction for ell_0 SSC has already been studied in [a], which is not cited / discussed. As far as I see the DR-ell0-SSC in [a] is precisely the same as the Noisy-DR-ell0-SSC in this submission. Hence there does not seem to have any novelty here.\n\nWriting:\n\n- The paper puts the theoretical results for Noisy-DR-ell0-SSC in the main paper, and the results for Noisy-ell0-SSC in the supplementary material. Personally I find the former to be too complicated to parse directly, and it is not clear how noise and dimension reduction each affects the conditions in the theorem. I would find it more beneficial if the results for Noisy-ell0-SSC is presented first in the main paper, which has a much cleaner form and is much easier to interpret. \n\n- Going to the appendix, I'd also suggest that Theorem A.4 is presented before Theorem A.2 as the latter is mostly a technical lemma that is very difficult to parse but does not offer any insights. \n\n- The paper mentions randomized model in the intro, but randomized model is only presented in the appendix. Given that the results for randomized model is not that complicated, I don't quite understand this choice. In fact in some of the previous works (e.g. Soltanolkotabi and Candes '12) the randomized is presented upfront as it is simple and offers a lot of interesting insights. \n\nTechnical comments:\n\n- The conditions in Theorem 3.1 involve both the quantities sigma_X, r and sigma_Y, r, which seems redundant. Presumably if the noise is small enough then the difference between sigma_X, r and sigma_Y, r should also be bounded, hence using only one of them should be enough. \n\n- Theorem 3.1 is characterized by sigma_X, r and sigma_Y, r which are the smallest singular values. However in the subspace clustering literature it is more common to use inradius to capture the distribution of data points, which is also arguably more geometrically interpretable. These two quantities are related as shown in [b], so it will be interesting, as a means of comparing to other results in subspace clustering, to rewrite the Theorem in terms of inradius.\n\n- The randomized analysis presented in Theorem A.7 of the appendix does not specify the modeling of the subspaces. Looking at Eq. (34) it seems that it is assuming a semi-randomized model where subspaces are fixed and only points are randomly sampled. But, the paper also says that their randomized model encompasses both semi-random and fully-random model at the beginning of Appendix A.1. So I am confused as to what modeling THeorem A.7 is using.\n\n[b] Wang et. al, Provable subspace clustering: When lrr meets ssc.\n\n",
            "summary_of_the_review": "The paper presents solid results for noisy ell_0 sparse subspace clustering on dimensionality reduced data. However, dimensionality reduction part has already been studied in previous work, so seems that the only novel part is on the noisy data. Also, the presentation makes the paper hard to follow. There are also a few points with the technical results that may worth looking into further. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper focuses on the subspace clustering problem, and in particular sparse subspace clustering. Two algorithms for noisy sparse subspace clustering with dimensionality reduced data, that utilize the $\\ell^0$ norm to induce sparsity, are presented, each based on slightly different forms of dimensionality reduction, alongside theoretical results.",
            "main_review": "The claimed contributions of this paper are the theoretical analyses for noisy $\\ell^0$ subspace clustering and noisy $\\ell^0$ subspace clustering for dimensionality reduced data. Exposition is lacking, as there are notational inconsistencies, and the paper is not organized properly, forcing the reader to go back and forth between the appendix and the main text. Finally, parts of this manuscript are very similar to another recently published paper, see [2].\n\nDetailed comments are included below: \n\n1. Some references include full names whereas others do not.\n\n1. Some related works, namely [1] [2] are not mentioned. This paper has a lot of the same notation and structure as the one in [2]: similar projections for dimensionality reduction are considered; similar notation; Fig. 2 is almost the same as Fig. 1 in [2].\n\n1. There are no comparisons with other competing alternatives, such as [1], on dimensionality reduced data.\n\n1. The notation $\\bf H_T$  used for a subspace spanned by the columns of a matrix ${\\bf T}$ can be easily confused with the notation ${\\bf A}_{\\bf I}$ that is used for a submatrix of ${\\bf A}$.\n\n1. In Sec. 2.1 noise is initially denoted using $\\bf N$ and afterwards using $\\bf Z$. This $\\bf Z$, which does NOT correspond to noise, is then used in (1) and subsequent  equations and theorems.\n\n1. $\\beta$ is suddenly introduced in place of $\\bf Z$ in (3). Does $\\beta$ correspond to a column of $\\bf Z$? If so why is not the notation introduced earlier used? \n\n1. Definition 2.1 repeats the same thing three times. Also $\\bf Z^i$'s should probably be $\\bf Z^{*i}$'s\n\n1. After Definition 2.2 - \"The assumption of general condition\" should be \"The assumption of general position\".\n\n1. In Sec 3.2 $\\bf R$ is an upper \"triangular\" matrix, not \"triangle\".\n\n1. $M_i$, \\bar{\\sigma}_{{\\bf Y},r}, \\sigma_{{\\bf X},r} and $\\mu_r$ are not defined in the main text, yet they are referenced in Sec. 3.2. The entire  analysis for noisy $\\ell^0$ subspace clustering is in the appendix. \n\n1. Why is the second algorithm focused on the OSNAP method for dimensionality reduction? Other projections that have similar properties, such as Johnson-Lindenstrauss Transforms, exist, and the analysis would probably extend to these as well. \n\n1. Should $M_{i,\\delta}$ in Thm. 3.2 be $\\tilde{M}_{i,\\delta}$ ? \n\nReferences:\n[1] R. Heckel, M. Tschannen and H. Bölcskei, \"Subspace clustering of dimensionality-reduced data,\" 2014 IEEE International Symposium on Information Theory, 2014, pp. 2997-3001, doi: 10.1109/ISIT.2014.6875384.\n\n[2] Yang, Yingzhen (2018). Dimensionality Reduced $\\ell^{0}$-Sparse Subspace Clustering. Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, in Proceedings of Machine Learning Research 84:2065-2074 Available from https://proceedings.mlr.press/v84/yang18c.html.",
            "summary_of_the_review": "Overall, while the claimed theoretical results justify using $\\ell^0$ subspace clustering for noisy and dimensionality reduced data, the organization and exposition of the paper could be significantly improved. Parts of the manuscript could also benefit from clarifications. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
            ],
            "details_of_ethics_concerns": "See comment 2 in the main review. ",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes two efficient randomized algorithms for L0-Sparse Subspace Clustering (L0-SSC) with noisy data. The two randomized algorithms are based on the random projection idea. In particular, Noisy-DR-L0-SSC is based on randomized low-rank decomposition, and Noisy-DR-L0-SSC-OSNAP is based on the efficient sparse projection, Oblivious Sparse Norm Approximating Projection (OSNAP). The theoretical guarantee of noisy L0-SSC with both randomized algorithms is provided. Experimental results demonstrate that both randomized algorithms show comparable performance with the original noisy L0-SSS, and they are more efficient than noisy L0-SSC thanks to the projection of the original high-dimensional data onto a low dimensional space. This paper also provides a theoretical analysis of noisy L0-SSC for the first time.",
            "main_review": "Strengths:\nIt is interesting to use random projection to accelerate subspace clustering methods, and this paper provides a comprehensive theoretical analysis for the two new random projection-based algorithms, Noisy-DR-L0-SSC and Noisy-DR-L0-SSC-OSNAP, for noisy L0-SSC. The analysis of the two randomized algorithms is based on the theoretical analysis of noisy-L0-SSC. The theoretical analysis of the proposed randomized algorithms seems correct and interesting, which involves tools from random matrix theory. The experimental results show that the randomized algorithms, while only performing subspace clustering on dimensionality reduced data with a considerable factor (5-15), deliver clustering results comparable to the original noisy L0-SSC. The empirical study about the clustering performance on data with various noisy levels confirms the theoretical finding that a large \\lambda is in favor of the correctness of subspace clustering.\n\nWeakness:\nThe correctness of noisy-L0-SSC is only presented in the appendix. I suggest the authors summarize important results from the analysis of noisy-L0-SSC, and explain in detail how they are used in the analysis of the two randomized algorithms.\n\nMinor notes: the references to equations (63), (66), (65) at the end of Theorem 3.1 should be changed to references to equations (5), (8) and (7). Similarly, references to equations (66) and (65) at the end of Theorem 3.2 should be changed to equations (8) and (7). Please move the definition of C_{p,p_0} from the appendix (equation (61)) to Theorem 3.1.\n",
            "summary_of_the_review": "The theoretical analysis of this paper seems solid and interesting, with the theoretical findings justified by experimental results. I suggest the authors revise this paper so that the important results of the analysis of noisy L0-SSC are summarized in the main paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper studies the L0-Sparse Subspace Clustering (L0-SSC) problem. In such a problem, there are $n$ points in $\\mathbb{R}^d$, each from a low-dimensional subspace and there are only a few possible subspaces. The task is to cluster the $n$ points according to the subspaces they belong to. The method is to first obtain a coefficient matrix $Z$ in $\\mathbb{R}^{n\\times n}$, as sparse as possible, such that $X = XZ$ for the data matrix $X \\in \\mathbb{R}^{d \\times n}$ and then perform clustering based on $Z$ (such as spectral clustering on $(|Z|+|Z^T|)/2$).\n\nThis paper considers the variant of L0-SSC with noisy data, i.e., the data matrix $X$ is noisy as opposed to the noiseless case in an earlier work. This paper focuses on obtaining a good coefficient matrix $Z$ by sketching the data matrix $X$ using two methods of dimensionality reduction - random projection and OSNAP. The experiments support the correctness and time efficiency of the proposed algorithms.\n",
            "main_review": "The paper is interesting though I have a few comments/questions.\n- Definition 2.1 is not very clear. What if $y_i$ belongs to two subspaces? Does it require the related columns to belong to the same subspace? It said the subspaces are distinct. Do you mean disjoint (i.e., they intersect at $0$ only)?\n- Conditions (9)-(14) seem rather technical and almost impossible to be understood well (it seems that the authors just adapt some previous proof to the noisy case and make technical assumptions to overcome the difficulties in the proof). Are there any intuitive explanations of these conditions? If so, please elaborate on the intuitive explanations.\n- How does one verify conditions (9)-(14)? The smallest restricted eigenvalue, defined in (6), is already difficult to calculate for a given matrix $Y$. Since $Y$ comes from the data, this diminishes the value of the theoretical results as a user cannot verify the conditions in order to apply the theorem. A relief might be to have a corollary with a list of weaker but more verifiable conditions. It would also be much better to give some concrete instances that satisfy those conditions. \n- It also does not say how large $p$ should be so that Conditions (9)-(14) hold. This can be made a corollary. In addition to experimenting with different values of $p$, it is important to have a theoretical result on the choice of $p$.\n- $C_{p,p_0}$ is only defined in the appendix and there is even no mentioning of what it is. This is not acceptable.\n- The PGD algorithm is shown to converge to a critical point only. It would be better to say something about how far it is from the optimal solution. Despite Section C in the Appendix, the expression is rather obscure with many terms that are difficult to bound. I am also curious how critical the assumption that $\\hat\\beta$ is a critical point of (3) is in this theorem. I tried to look up the proof of Theorem 5 in (Yang & Yu, 2019) but could not find it, not even in the supplementary material of that paper. The authors should supply the proof for completeness if they want to use this result.\n- The runtime of OSNAP can be better: multiplying a matrix $A$ by a extremely sparse $S$ (each column contains only one non-zero entry) takes time only $O(nnz(A))$, where $nnz(A)$ denotes the number of nonzero entries of $A$. If $A$ is $d \\times n$, this is at most $O(dn)$, saving a factor of $p$ from $O(pdn)$.\n\nSome typo-level comments:\n- At the end of the statement of Theorem 3.1, (63), (66) and (65) should be (5), (8), (7). A similar issue occurs in the statement of Theorem 3.2.\n- In Eq. (8) and several other occurrences, the subscript $r$ in $\\bar{\\sigma}_{\\bar Y,r}$ seems to be $r’$.\n- In Section 4.1, how are AC and NMI defined?\nThe paper contains a lot of typographical/grammatical errors. The author should proofread the paper thoroughly. The following are a few random observations.\n- Page 2, paragraph 2, they almost surely -> they are almost surely\n- Definition 2.3, line 2, linear independent -> linearly independent\n- Page 5, paragraph 4, present probabilistic -> present a probabilistic\n- Section 3.6, line 2: on optimal -> on the optimal\n- Section 4.2, the first line: demonstrates -> demonstrate, Analysis -> analysis\n- Section 4.2, line 13: constains -> contains\nPlease note that a sentence should not begin with mathematical symbols - this appears a lot in the paper.\n",
            "summary_of_the_review": "Overall the paper studies an important and interesting problem, but the theoretical results are not very meaningful, as it is rather difficult to verify the technical conditions and to deduce guidance on the magnitude of p for the sketch size in the random projection method. I hope the authors can provide simpler (despite being weaker) conditions. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}