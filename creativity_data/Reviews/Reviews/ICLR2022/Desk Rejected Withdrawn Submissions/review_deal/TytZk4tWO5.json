{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a new compositional learning setup where the reference for the compositions is subject to low supervision. While humans seem to be able to solve the problem, it can be a challenging task for the existing learning algorithms. The paper looks into this problem and proposes new benchmarks and baseline results.",
            "main_review": "**Strengths**\n\n- I agree it is an interesting problem to consider the case of limited reference.\n- The authors put several existing datasets together to create new datasets for RLCL.\n\n**Weaknesses**\n\n- Many recent works in Compositional Zero-Shot Learning and Few-Shot Learning are not considered in the related works and experiments. Some examples of missing related works:\n\nMemory-Augmented Relation Network for Few-Shot Learning, ACM Multimedia 2020\nDynamic Few-Shot Visual Learning without Forgetting, CVPR 2018\nEmbedding Propagation: Smoother Manifold for Few-Shot Classification, ECCV 2020\n\nGiven missing references to those recent works, it is hard to claim that ProtoNet is indeed a competitive method in the literature and the proposed refined-ProtoNet is a strong baseline, because some recent models may prove to be better. We don't have a clear understanding of the state of the art from the current experiments.\n\n- It is strange to see the claim \"Such semantic descriptions of classes are difficult to collect in the real-world environment\" although those descriptions are available in the other CZSL datasets and they have been broadly used in the literature. At the same time, the discussion of baseline methods for CZSL cases is very limited.\n\n- refined-ProtoNet uses a beefed-up network without proposing new ideas. As we can improve any model by adding new components borrowed from the representation learning literature, it is not clear what we can learn from this model. ",
            "summary_of_the_review": "Although the proposed question is interesting, I found the discussion of the related works and claim about the new base model is problematic. It is not clear what is our current progress on the proposed problem, therefore I am leaning towards rejection.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work introduces a new task, reference-limited compositional learning, that extends the compositional zero-shot learning set-up by introducing a constraint on the number of  primitive categories and on the number of observed combinations of primitives. The authors also provide two benchmarks and extend current few-shot methods to the proposed set up, showing all methods struggle with the task.",
            "main_review": "Strengths:\n* This work identifies an important problem and proposes a step in the direction of defining more and more realistic tasks for AI systems.\n\nWeaknesses:\n* Why were the benchmarks splits defined so that the train and test samples are very different datasets? Could this be enough to justify the bad performance of all methods on the task? And how many classes were considered? Reporting accuracy without the number of classes can be misleading. \n* The explanation in figure 1 is not quite clear, adding an explanation of what is happening in the \"Realistic scenario\" side could help. More examples of what are primitives and of how the RLCL task differs from established tasks would also be helpful.",
            "summary_of_the_review": "While the authors highlight an important limitation of work in the area of compositional learning, namely that humans can learn not only novel compositions of previously learned elements but also novel compositions, the described benchmarks present some issues. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The article describes the problem of Reference Limited Compositional Learning (RLCL), where the goal is to recognize compositions of primitives (e.g. attributes-objects, actions-objects) given a few examples depicting them.  The query images may contain both seen and unseen compositions of the support primitives. To address this task, a refined version of ProtoNet (Snell et al. 2017) is proposed, where the input features are filtered through an attention mechanism. Experiments on two proposed benchmarks show that refined-ProtoNet surpasses the performance of various few-shot baselines and a simple compositional zero-shot learning one (CZSL).",
            "main_review": "**Strengths**:\n1. The paper presents an interesting problem setup, where a model should recognize possibly unseen compositions of primitives at test time, given a few examples. This problem formulation merges the challenges of few-shot learning, CZSL, and generalized zero-shot learning, since the model could be inherently biased toward support compositions. The low performance achieved by all models (e.g. below 5% on unseen composition recognition) demonstrates that the problem is very challenging.\n2. The proposed RLCL-attr and RLCL-act benchmarks encompass various compositional datasets and may serve as a future, interesting benchmark for the community, also in CZSL.\n3. Related works and the introduction are comprehensive and well written, perfectly explaining the contribution of the work.\n\n**Weaknesses**:\n1. The first weakness is the technical contribution. The proposed refined-ProtoNet is not a method tailored for the RLCL task, but rather an additional baseline built on the combination of existing techniques (the term baseline is also used to define the approach at the end of Section 4.2). As acknowledged in Section 4.2, refined-ProtoNet combines the original ProtoNet with the CBAM attention module of (Woo et al. 2018). This is also similar to what other few-shot baselines have done in the past (e.g. [a] with CrossTransformers) and there are no justifications on why the attention module promotes the higher accuracy on unseen compositions, shown in Table 1. A more solid method, addressing all the challenges of RLCL would have strengthened the contribution of the manuscript, showing the need for a tailored approach to tackle this task. \n2. While it is true that most recent CZSL techniques assume the presence of side information describing the category of interest, it is also true that Table 1 tests on RLCL only a single CZSL method, VisProd, the very first baseline for this task (Nagarajan &\nGrauman, 2018). A thorough benchmark would have included more recent CZSL methods (e.g. Atzmon et al. 2020, Naeem et al. 2021, [b]) and, possibly, coupling them with existing few-shot learning techniques. For instance, an easy comparison would have been instantiating the side information used by these recent methods through ProtoNet-based class weights. Including more recent baselines and naive combinations of few-shot and CZSL ones would have contributed, as for the previous point, to stress the need for specific methods addressing the RLCL problem.\n\nReferences:  \n[a] Doersch, Carl, Ankush Gupta, and Andrew Zisserman. \"Crosstransformers: spatially-aware few-shot transfer.\" NeuriPS 2020.  \n[b] Li, Yong-Lu, et al. \"Symmetry and group in attribute-object compositions.\" CVPR 2020.  ",
            "summary_of_the_review": "I find the problem presented in the manuscript very interesting and worth studying. However, the proposed method is not tailored to the proposed task and the experimental comparisons do not include recent CZSL baselines (eventually combined with few-shot learning approaches naively). For these reasons, I lean toward a negative rating.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a new task - Reference-Limited Compositional Learning, a corresponding benchmark and a baseline method. The task is a combination of compositional object + attribute recognition [Misra et al., CVPR'17] and few-shot image classification. In particular, they posit that it is unrealistic to expect that a learning system would have access to a large dataset of all possible object and attribute combinations at training time, and propose a more realistic scenario in which a mode is given only a few object-attribute pairs for training and has to learn to recognize not only these exact combinations, but also any other combinations of the same objects and attributes (i.e. training set might include red wine and green tomato, but at test time red tomato needs to be recognized). \n\nThe benchmark consists of two parts, one focused on objects + attributes and another on objects + actions. Both are constructed by combining existing datasets.\n\nThe method is an extension of prototypical networks with a channel attention block that presumably allows to better disentangle object and attribute features during training. ",
            "main_review": "The paper is not very well written, though readable. In particular, there are virtually no illustrations making it hard to understand the details of the benchmark and the approach. \n\nWhile the proposed task and benchmark are valid and novel to the best of my knowledge, I find that they will be of little interest for the community. The problem of compositional recognition is challenging and under explored even in the non-few-shot regime, so I doubt that any real progress will be made in the more challenging few-shot setting in the near future. This is evident from the fact that all the evaluated methods show an extremely low performance on novel attribute-object combinations on the proposed benchmarks.\n\nThe baseline approach is also a combination of existing methods and the improvements over vanilla Prototypical Networks are not statistically significant or barely statistically significant, depending on the setting.",
            "summary_of_the_review": "The proposed task and benchmark are novel to the best of my knowledge, however, they are simply a combination of two existing tasks and several existing datasets respectively. I don't think this constitutes a valuable contribution for the reasons stated above. That said, strictly speaking, all the boxes are checked (novel task + benchmark, a baseline that provides some minimal improvements over existing methods), so there are no formal reasons for rejection aside from the sub-par presentation. I will leave the decision to AC's judgement.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}