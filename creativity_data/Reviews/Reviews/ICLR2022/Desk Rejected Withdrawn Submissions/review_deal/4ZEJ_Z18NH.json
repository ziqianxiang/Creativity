{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces a new learning based approach for compressing facial images / videos. The main idea is to exploit off-the-shelf image generation models like StyleGAN as the image prior and learn to compress the latent code generated by the corresponding image inversion model (e.g. StyleGAN encoder). Compared with existing works in learned image compression, the proposed method re-uses existing encoder and decoder instead of learning them jointly with the entropy coding model. The authors further extend the approach to video coding by compressing the residual of the latent code between adjacent frames. Empirical results on both static images and videos show that the proposed method outperforms state-of-the-art codecs in perceptual metrics such as LPIPS and PIM.",
            "main_review": "This paper exploits the idea of using StyleGAN as the image prior to improve image and video compression. While the empirical results seem promising, some important information are not sufficiently covered in the paper:\n\n1. It is unclear why the proposed method performs better than prior learning based compression methods. The main claim of this paper is that off-the-shelf encoder and decoder perform better than those that are learned end-to-end in learned image compression. This claim is somehow counterintuitive and should be examined and justified more carefully. For example, does the results imply that there's not enough data to learn the encoder, decoder, and entropy coding end-to-end, 2) existing models do not have sufficient capacity for optimal performance, 3) existing works use sub-optimal loss for compression, etc.\\\nAlso, there are some factors that may favor the proposed method in the evaluation and should be excluded: 1) the proposed method is trained on more training data, i.e. FFHQ + Celeba-HQ than the baselines, 2) the quantitative results are computed on images generated by StyleGAN, which have a different distribution compared with real images on which the baselines are optimized.\\\nFinally, the authors should provide sufficient details about the baselines' implementation, e.g. the parameters for traditional codecs, how the models were optimized for learning based methods.\n\n2. There is no discussion about the limitation of the StyleGAN based encoder / decoder. The approach is based on two hypotheses: 1) every image can be generated by StyleGAN, and 2) the inversion model can perfectly invert the image to a latent code for StyleGAN. These are very strong hypotheses, as mentioned in the paper, and there's little information about how likely they are true. Therefore, they should be taken into consideration when discussing the pros and cons of the proposed method. For example, it is not fair to evaluate the results using the output of StyleGAN, as StyleGAN might alter the data distribution while the goal should be compressing the real data. Also, the authors should try to discuss and / or quantify how well existing StyleGAN model and StyleGAN encoder cover real data and how they affect perceptual quality. For example, how significant the difference between \"Original\" and \"Projected\" in Figure 12 is? This information is necessary for evaluating the effectiveness of the proposed method.\n\nBesides the questions above, there are also some minor suggestions for the paper:\n1. The qualitative results are hard to read even after zooming-in. A higher resolution version in the appendix might help.\n2. A short description for the references in the implementation details may help the completeness for the paper.\n",
            "summary_of_the_review": "This paper introduces an interesting approach for improving learning based image / video compression, and the empirical results are promising. However, further analysis is necessary to understand and justify the benefit of the proposed method.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a deep learning-based method for compressing talking-head videos to very low bit rates, while maintaining perceptual quality. The main idea of the paper is to use a pre-trained StyleGAN network along with a pre-trained encoder to encode facial images into StyleGAN's w+ space. The authors then propose to learn an optimal transformation of W+ space into a diffeomorphism compression space, which enables entropy and distortion optimization coding via normalizing flows. The authors also propose a new distortion preserving loss in the latent space, which is easier to implement than the image-based pixel-wise reconstruction or perceptual losses that are used as the de-facto standard. The authors further propose methods for both intra and inter-frame coding. They evaluate their proposed method on two datasets and show SOTA performance in comparison to the recent classical and deep-learning-based video compression algorithm, especially for very low bit rate encoding.",
            "main_review": "Strengths:\n\nNovelty: The paper addresses a very relevant problem, which is pertinent to our times. There has been a huge rise in the demand for video conferencing since the pandemic. However, this isn't the first work to address this problem. It was was previously addressed in Wang et al, One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing, CVPR 2021. Nevertheless, there is little work in this general research area and this work proposes a novel approach to solving the problem in comparison to the existing one and hence is sufficiently novel, overall.\n\nTechnical Soundness: The method is mostly technically sound and the authors have provided many experiments. A few exceptions are noted below in the weaknesses section.\n\nClarity: The paper is most clear and well written. Minor language-related typos exist, which can be easily fixed with a thorough editorial review.\n\nWeaknesses:\n1. Missing citations: The authors failed to cite the work: Wang et al, One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing, CVPR 2021, which addresses the same problem as the authors'. Nevertheless, the authors' approach to solving the problem is significantly different from that of Wang et al.'s also simpler and easier to implement using off-the-shelf pre-trained networks. Also the authors of this work consider both intra and inter coding explicitly, which is novel. The authors should cite Wang et al's work along with any other pertinent previous citations from it as well, and thoroughly explain the differences of their approach from Wang et al.'s (and the proceeding lines of work).\n\n2. Technical Correctness: The authors propose to employ normalizing flows to ensure diffeomorphic mappings between the W+ and W_c spaces, but to get around the issue of making the quantization operation differentiable they approximate it with adding uniform noise while training. However, adding this noise breaks the diffeomorphic assumption and according to the distortion loss defined in equation 4, different values from the W_c are enforced to map to the same value in the W+ space. How to the authors explain this technical contradiction of their proposed solution?\n\n3. Perfect Reconstruction Assumption: The authors, by their own admission reply on the pre-trained encoder to be able to produce perfect reconstructions of input image. This is a drawback of current approach. To further quantify the effect of this assumption on image quality, the authors should provide quantitative results of comparing to the input image as well, besides comparisons against the reconstructed image for both their and the baseline video encoding method. It is not fair on the author's part to only report the results of comparisons against the reconstructed image only.\n\n4. Lack of Comparisons to SOTA: The authors lack quantitative and quantitative comparisons to the SOTA approach of Wang et al., CVPR 2021. They should provide qualitative and quantitative comparisons against this approach. Furthermore, note that Wang et al, compare against the  input frame using LPIPS and hence the authors of this work should similarly compare their encoded frames against the input frames and not just the reconstructed frames, while comparing against Wang et al's approach. Furthermore, I would also like to see how well their proposed approach preserves the identity of the subject in the encoded video versus Wang et al.\n",
            "summary_of_the_review": "Overall the paper investigates an important problem, for which little research currently exists. It further proposed a new method for solving the problem, which is technically sound and potentially simpler than the existing SOTA one. However, the proposed method has not been thoroughly compared against the current SOTA, and not all aspects of the algorithm -- specifically those related to how well does it preserves the input frame and person identity across the vide have not been quantified. Without these comparisons it is unclear whether the proposed approach overall advances the field or not. I would like to see these latter results to be able to make my final decision.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a video coding method based on pre-trained GANs. StyleGAN is used as a backbone for the approach in order to encode streams of faces. A normalizing flow is used to obtain a bijective transformation from the latent space obtained by the backbone generator and the compressed codes latent space. Distortion/rate trade-off is controlled with a loss computed in the latent space which is convenient since it does not require to generate high resolution images to be computed. The compression rate vs quality is controlled at training time via an hyperparameter.\nExperiments show good results on video and image compression.\n\n",
            "main_review": "The main issue with this work regards novelty. A similar approach was presented in CVPR 2021:\nhttps://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Deep_Learning_in_Latent_Space_for_Video_Prediction_and_Compression_CVPR_2021_paper.pdf\n\nIn this work a very similar idea is proposed with two main differences: \n- the approach is not targeted only to encode face streams\n- the network training is performed in a way that the R-D tradeoff can be achieved at inference time.\n\n",
            "summary_of_the_review": "The approach is sound but lacks novelty. It should cite recent work performing the encoding with a very similar approach discussing the differences and highlighting the advantages.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "details_of_ethics_concerns": "One main ethical concern with face processing is the bias induced by dataset. There is no mention on the effect of data bias introduced by the GAN encoder. Apparently results are ok and do not suffer, but it has been shown recently that face reconstruction via pre-trained network may yield catastrophic results (https://twitter.com/bradpwyble/status/1274380641644294150)\n",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}