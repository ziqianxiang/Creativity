{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Overall, this paper reviews the monotonicity constraints and implementation tricks adopted by previous MARL algorithms, and proposes a new MARL algorithm RMC which generalizes the monotonicity conditions and implementation tricks adopted by prior MARL algorithms. Experiments on SMAC and PP benchmarks suggest that monotonic representation of the team-level objective (with respect to agent-level objectives) used by mixing networks is indeed effective in boosting MARL performances.  ",
            "main_review": "Strengths:\nThe paper is reasonably presented, with no obvious structural flaws.\nExperiments have been conducted on various benchmarks and the state-of-the-art-algorithms, and the results do seem to corroborate with the claims made in the paper.\n\nWeaknesses:\nThe level of novelty is limited, as monotonic networks and implementation tricks are already standard and well-explained in prior literature and models(e.g QMIX, QTRAN). It is unclear which aspects of RMC differ from prior work. \nThe steps of RMC should be better explained. What is meant by a 2-stage approach? How is action selected if the critic is no longer used for greedy action selection as the paper claims? Many crucial details seem to be missing.  \nThere are many notable typos and grammatical errors throughout the paper(e.g. Pp.4 As the monotonicity constraint on the critic….mixing network). Moreover, some algorithmic flowcharts referred to in the paper(e.g. Algorithm 1) are not included. \n",
            "summary_of_the_review": "The RMC algorithm paper has limited novelty. Aside from some new training tricks on the engineering level, all the components/insights in the RMC algorithms are already standard practices from value/policy-based multi-agent reinforcement learning.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper analyses important of monotonicity constraint of QMIX for MARL. The paper shows that QMIX with monotonicity constraints, when fine-tuned works better than some other algorithms. In addition, the paper gives examples of tasks which may look as competitive but are in fact cooperative, out of the box or with small modifications (reward reshaping).  To explore the influence of monotonicity constraint, the paper proposed a scheme/method/model/algorithm they call RMC. ",
            "main_review": "The paper shows a fresh view at some aspects of QMIX and comparison with other algorithms tackling CTDE. A major limitation of QMIX is that it is applicable only to settings in which monotonicity assumption holds. Follow-up works tried various ways to lift the monotonicity constraint to broaden applicability and improve performance.  The paper argues that comparative evaluation in some works are inconsistent and not sufficiently rigorous. In addition, fine-tuning parameters of QMIX may bring it to the same or better level of performance as other algorithms, undermining the need to lift the monotonicity constraints. In addition, the paper draws attention to the fact that some tasks are in fact cooperative (that is comply to the monotonicity constraint) although they may look otherwise.\n\nThese are interesting observations, but it is my feeling that they are either trivial (monotonicity is not always obvious and must be checked/proved in some cases, tuning improves parameters) or not convincing --- were the competing algorithms tuned with the same amount of effort as QMIX for the comparison? Also, a minor consideration but adding up to my impression from the paper, the paper needs proof-reading, both English and content --- Algo. 1, for example, is only in the appendix, without any mention of that.\n\nThis may become an interesting analytic paper with useful insights, potentially leading to further development of CTDE MARL schemes, but requires a significant effort in theoretical discussion, analysis, evaluation, and presentation before it can be published, in my opinion.",
            "summary_of_the_review": "Maybe contains important insights but requires much more work before can be published.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper combines some modules of existing methods to form the proposed RMC method and carries out ablation studies on the monotonicity constraint. The authors conclude that the monotonicity constraint can improve sample efficiency in some purely cooperative tasks based on the ablations. Besides, this paper optimizes QMIX, and the Finetuned-QMIX achieves high win rates on SMAC. Furthermore, to perform \"fair\" comparisons, the authors find a general set of hyper-parameters for some existing methods, although there may be some concerns. Finally, this submission provides some analysis on the monotonicity constraint. ",
            "main_review": "**Strengths**:\n1. This paper conducts sufficient experiments on several modules in QMIX and proposes Finetuned-QMIX, a value-based off-policy method that achieves high win rates in SMAC and will be a good baseline for all value-based off-policy methods.\n2. The authors realize that some existing methods utilize implementation tricks to achieve excellent but unfair results and try to normalize these tricks. It is crucial for a fair comparison between MARL methods. I highly appreciate and agree with the authors' motivation, although there are some potential controversies during their re-evaluation.\n\n**Weaknesses**:\n1. This submission has many mistakes and typos, including grammar and expression problems, some of which affect my understanding. I only list SOME OF THE MISTAKES here. \n\n    (1) Many grammar and misspelling mistakes. For example, in the paragraph above Equation 6, the word \"practical\" is misspelled, and \"an novel\" should be revised into \"a novel\"; Words \"switch\" and \"Besides\" are misspelled in the last paragraph on page 4; \n\n    (2) Compilation mistakes. For example, the paragraph below Definition 2 has a latex compilation error.\n\n    (3) Typos in equations. For example, $T$ in Equation 1, $z_t^i$ in the definition of Adaptive Entropy, etc.\n\n    (4) ...\n\n    These writing and compilation mistakes make me feel that this paper has not been adequately and carefully prepared. Please re-read and re-check your submission to improve the credibility of your paper and help readers understand your work more clearly. \n\n2. The writing logic is not clear, and some subsections seem to be irrelevant to the paper. Section 6 is the central part of this submission, but the content of the four subsections does not seem to be related, and subsections 6.2-6.4 seem to be irrelevant to the main idea of this paper. I think these subsections respectively discussed monotonicity constraint (subsection 6.1), implementation tricks on existing methods (subsections 6.2 and 6.3), and QMIX's shortcoming (subsection 6.4. This is my understanding of subsection 6.4, please see my following doubt below) with no clear relevance.\n\n3. The conclusion in subsection 6.4 is weak and unreliable for two reasons. (1) I do not dispute the conclusion that reward shaping may solve this problem, but the authors do not provide any insights into reward shaping. Why did you change -12 to -0.5, and how was this value determined? Why did you modify some of the values instead of others? I think this is a meaningless conclusion that does not make any sense if it is only illustrated by a simple example without any intuition and explanation. (2) Most importantly, I can also interpret this experiment as testing QMIX in a totally new environment. This experiment can only illustrate that QMIX works well in some environments but not all environments. However, this conclusion is already known to everyone. In summary, I think that the authors do not provide any new insights about the content of subsection 6.4. The conclusion they want to illustrate is not clear, and the explanation of the experiment is far-fetched. Therefore, this subsection does not contribute anything vital and only makes the paper logically confusing.\n\n4. It is essential to note that, even ignoring the above issues, I think the authors are only experimentally showing that monotonicity constraints facilitate higher win rates/scores in two settings with relatively simple policies. Regardless of whether this conclusion is correct, its contribution to the MARL community is limited for some reasons.\n\n    (1) First, this idea does not have any generalizability. Units' strategies in SMAC/PP are homogeneous and straightforward (hit/approach the enemies). You may verify your conclusion in some environments with more complex policy spaces;\n    \n    (2) Second, although RMC and VMIX achieve better results with the monotonicity constraint, a method in the future may improve the performance by reducing this constraint. The conclusion about the monotonicity constraint proposed by the authors is too strong without a rigorous and convincing explanation.\n\n    (3) Third, the authors do not provide any solid theoretical explanation for why the monotonicity constraint can help the methods perform better in these two environments. Theoretically, the monotonicity constraint limits the expressiveness of the mixing network. Instead of discussing this well-recognized issue, the authors only mentioned in subsection 7.2 that this practice reduces the search space of the network parameters. This explanation is far-fetched. According to this explanation, I can even classify the monotonicity constraint as an \"implementation trick\" that even the authors denounce. I do not oppose the authors' understanding of taking absolute values, but this conclusion does not provide any new and worthy insights into the monotonicity constraint for the community. \n\n    However, if the authors can provide more convincing explanations during the discussion period, I may change my opinion on this problem.\n\n5. The proposed method RMC itself does not provide any new intuition or motivation. Besides, RMC does not present any new contributions at the theoretical or methodological level and is only a simple combination of existing methods or modules. \n\n6. Last but not least, I have some concerns and doubts about the results in Table 3. Although I understand that the authors intent to compare methods fairly, I cannot accept some experimental settings and results.\n\n    (1) Firstly, regarding reducing neural network size, we generally believe that a larger network has greater expression ability, thus bringing unfairness. However, I think this view is valid only when the network inputs and outputs are the same kinds. In fact, because of their own needs, some methods have a much larger input size than others. For example, the inputs of LICA's mixing network are not Q values but policy vectors. Such methods will have a much larger network size after middle layers to implement their basic ideas, which I think is acceptable. As long as these methods ensure that the network layers number is the same and the neuron numbers in the middle layers are roughly the same as other methods, then I don't think this is an implementation trick. Forcibly reducing the network size of these methods introduces new unfairness instead. If there is a need to ensure the network size to be the same order of magnitude, consider expanding other methods' networks or different ways that will not influence the methods' essential efficacy.\n\n    (2) Secondly, to the best of my knowledge, the RL community has not proposed a reasonable way to compare on/off-policy algorithms. Off-policy methods have better sample efficiency because of the replay buffer. Therefore, on-policy methods generally utilize more parallel actors for sampling. Based on these facts, I wonder how did the authors \"fairly\" compare the on/off-policy methods at the same time with the same experimental settings in one table? As one of the few on-policy MARL methods, LICA uses 32 rollout processes to collect samples to make up for the lack of replay buffer and sample reuse, which inevitably makes such methods cost more than an order of magnitude more samples than off-policy methods. Therefore, the forced reduction of rollout processes for the on-policy methods also introduces new unfairness. \n\n    My questions only take LICA as an example because LICA happens to be limited by both of these two changes, and some other methods also face at least one of the problems. \n    \n    I agree with most optimizations in the paper, such as Adam and Learning rate annealing, Orthogonal initialization, Observation normalization, N-step returns, etc. I also appreciate the authors' hard work on the performance improvement of QMIX. However, the baseline that Table 3 tries to build is still unfair due to the above and other potential reasons. It is very unfavorable to develop new on-policy methods or methods with unique input and output. This will limit the community development considerably. \n\n    I hope the authors can continue their work, constantly revise these results and build a more fair way for comparison. Before that, I don't think this work could be published because it will aggravate the unfair comparison between methods in some aspects.",
            "summary_of_the_review": "I think this paper deserves credit for revisiting the existing methods and trying to establish a fair comparison between these methods. Besides, their implementation improvement on QMIX will be a baseline for value-based off-policy methods in the future. However, this paper has too many flaws. (1) There are numerous writing errors and logical confusion. (2) The analysis and conclusions in section 6.4 are weak. (3) The analysis on monotonicity constraint is superficial and lacks theoretical contributions. (4) The proposed RMC method is only a combination of numerous existing works without new insights. (5) Most importantly, the \"fair\" comparison that the authors try to establish introduces new unfairness.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}