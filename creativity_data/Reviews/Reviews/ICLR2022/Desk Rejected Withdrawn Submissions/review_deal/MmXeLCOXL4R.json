{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an enhanced message-passing method to address the over-smoothing problem in deep graph neural networks. The authors propose to add connections between nodes that are in close proximity in the embedding space, such that the labelling information can better propagate to unlabeled nodes at long distances. An edge dropout technique has also been applied to mitigate the effect of noisy connections.",
            "main_review": "Strengths:\n1. This paper addresses an important problem of how to improve message passing mechanisms in GNNs. The problem is tackled from two aspects: over-smoothing and overfitting.\n\nWeakness:\n1. The technical novelty of this work is quite limited. Its main idea is simply based on the existing techniques like adding connections between nodes and edge dropout, that are already widely used in graph-related literatures.\n2. In my view, adding connections between nodes would change the distribution of node degree, potentially leading to nodes of large degree. Why do the authors claim otherwise? It is unclear to me why the two thresholds (start and end) would guarantee the number of edges added to each node is nearly the same.\n3. The proposed method is evaluated only on two datasets (Cora and Citeseer) with marginal improvements. This is insufficient to validate the effectiveness of the proposed method. More datasets should be added to strengthen empirical evaluation.\n4. The structure and clarity of this paper need to be largely improved. For example, Sections 1,2,4 should be restructured to better position the proposed method in the literature, as well as reinforce its motivation and technical contributions.\n5. This paper is not well-written. There are a number of grammatical errors that should be fixed. All the figures are too small, making them difficult to read.",
            "summary_of_the_review": "The technical novelty of the paper is quite limited. The proposed method is based on the existing techniques that have been broadly applied in graph-related literature. The writing of the paper can be largely improved in terms of its clarity, presentation and structure.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper builds GCN on an Enhanced Message-Passing Graph, which directly connect two nodes if they have similar features in the latent space. This paper also introduce DropEdge on the Enhanced Message-Passing Graph to alleviate over-fitting.",
            "main_review": "Strength: this paper targets on two critical problems of graph representation learning: over-smoothing and over-fitting.\n\nWeakness: \n1) I do not think the motivation is very novel since the similar idea of extending the graph neighborhoods in the latent space has been proposed, such as [1]. At least, authors could emphasize the advantage of the proposed methods to [1].\n2) The experimental datasets are quite simple. The results obtained on such simple datasets are not much higher than some baselines, which cannot explain the significant superiority of this method.\n3) Another problem is the computational costs. If the method is run on a very large scale graph, the similarity computation would be heavy, limiting the generalization on the large-scale graph.\n4) DropEdge is a common operation to stablize training and improve generalization.\n\n[1] Geom-GCN: Geometric Graph Convolutional Networks, ICLR 2020.",
            "summary_of_the_review": "The idea is not novel enough.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper focuses on improving the performance of node classification on graphs. Their key ideas are to 1) augment the original graph topology with a similarity graph, where the node similarity is defined by unsupervised node embeddings; 2) randomly drop a certain percentage of connections to stabilize training. The proposed method is compared against some baselines on two small datasets, where there is some minor improvements.",
            "main_review": "The paper does not reach the standard of an ICLR paper in a number of ways.\n\n**[Novelty]**\n\nThe idea of using node embedding during aggregation has been explored in Pei et al. (2020). The only novelty of this paper is that it uses a rank based proximity threshold for generating augmentation edges, which is not a good ideal due to the scalability. Edge dropout is a standard technique that has been implemented in packages such as torch_goemetric.\n\n**[Scalability]**\n\nThe authors propose to generate augmentation edges with the following, quote:\n> Instead, we sort r from small to large and include the nodes in a certain range of proximity into Nz (v).\n\nHowever, this is on its self a O(n^2 log(n)) operation if implemented naively, which is not suitable for large graphs. In theory, the authors could use a KD tree type space partition algorithm to reduce the complexity, but the effectiveness need to be verified carefully on real-world graphs, especially when the node embedding dimension is high.\n\n**[Experiments & Baselines]**\n\nThe proposed algorithm is only tested on two small datasets, and the gains are not impressive. In addition, given the similarity of this work to Pei et al. (2020). Geom-GCN needs to be included as a baseline. If the scalability can be addressed, I suggest trying on datasets such as OGB.\n\n**[Grammars & Typos]**\n\nThere are many grammar issues and typos in this manuscript. E.g.\n> The added edges do not change the distribution of node degree, since the number of edges added to each\nnode is nearly same, which is about (start − end).\n",
            "summary_of_the_review": "not good enough",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}