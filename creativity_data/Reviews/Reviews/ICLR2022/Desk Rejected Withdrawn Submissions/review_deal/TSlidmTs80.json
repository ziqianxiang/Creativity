{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper tries to solve the Semantic Audio-Visual Navigation (S-AVN) task by using knowledge-driven scene priors. They incorporate scene priors in knowledge graph form by encoding object-region relations and extract relational features for visual and audio modalities using Graph Convolutional Networks. They use Scene Memory Transformer to capture long-term dependencies by recording visual features in memory and locating the goal by attending to acoustic features. They also extend the SoundSpaces task by evaluating agents on novel sounding objects which have not been seen during training as opposed to unheard clips of known objects and report metrics on this new split and show improvements over strong baselines.\n\n### Contributions:\n-\tIntroduce the use of knowledge-driven scene priors in the Semantic Embodied Navigation task, by combining semantic information from knowledge graph that encodes object-region relations, spatial knowledge from Graph Convolution Networks and background knowledge from pretraining tasks â€“ all within a reinforcement learning framework for audio-visual navigation\n-\tDefine a knowledge graph that encodes object-object, object-region and region-region interactions in 3D indoor environments; this knowledge graph gets used for pretraining of Vision and Audio Networks for goal specification and progress monitoring.\n-\tThey curate a multimodal dataset for pretraining of visual encoder.\n-\tExtend the S-AVN task by evaluating on novel sounding objects not heard or seen in training environments.\n",
            "main_review": "### Strengths:\n-\tThis paper is the first to apply knowledge graph to Semantic Audio-Visual Embodies Navigation task and show improvements over previous methods (SAVi and AudioGoal).\n-\tThey extend the S-AVN task by adding novel sounding objects not seen during training. This helps in evaluating generalization performance to unheard sounds.\n\n### Weaknesses:\t\n\n-\tThe authors have not reported any ablations of their approach. They have reported how their full model performs on the S-AVN task with metrics on Seen Houses - Unheard Sounds and Unseen Houses - Unheard Sounds splits, but how much do individual component of their approach help is unclear. I think it would be helpful to know how much Modular Pretraining (section 4.1.1) helps for achieving the final performance.  How much do Location Predictor and pretraining of Visual Encoder help?\n\n-\tTheir model does not perform well on SPL and SNA metric and the reason provided by the authors is that the agent is encouraged to explore the environment and that accounts for higher number of steps (175 on average) per episode compared to other baselines (50 steps). I wonder if the proposed method can outperform baselines on SPL and SNA as well if the agent is forced to explore less during the pretraining stages (by changing the reward structure to penalize more for longer trajectories).\n\n-\tThe paper does not report any qualitative analysis of how their method is better than baselines and does not showcase where the improvements are coming from.\n",
            "summary_of_the_review": " I am borderline on this paper. I think this is an interesting approach in Audio Visual Navigation and it should be published but after being polished further. More rigorous empirical evaluation of the proposed approach would improve the quality of the paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new approach for audio-visual navigation that exploits prior-knowledge about object-object, object-region, and region-region relationships. This prior knowledge is encoded as a knowledge-graph which is combined with visual and audio encodings and processed using graph convolutional neural networks (GCNs). This is then integrated end-to-end with a state-of-the-art audio-visual navigation policy for the task of semantic audio-visual navigation (SAVi). Additionally, the paper proposes a more difficult variant of SAVi where the agent needs to navigate to objects for which no audio is available during training (unheard objects). This is different from existing work that only test on new audio recordings of objects for which some training audio is already available. Results are demonstrated on the Matterport3D dataset with the SoundSpaces simulator.",
            "main_review": "# Strengths\n\n* The proposed problem setup is new and interesting. At a high-level, this seems analogous to older works on novel-object captioning and zero-shot transfer in the vision-and-language community. \n* The proposed method is the first application of prior knowledge to the audio-visual navigation task (while this is not technically novel, it is a novel application nevertheless).\n* The selection of baselines is reasonable and includes recent state-of-the-art. \n\n# Weaknesses\n\n## The paper significantly lacks clarity\nSeveral parts of the paper are unclear for both the method and the task description.\n\n### Task clarity\nWhat does unheard sounding object mean? There are two extremes to this definition.\n1.  Does it mean that the object is completely new and no knowledge is available apriori about what objects the agent will be evaluated on? In this case, none of the training techniques should rely on knowledge of what the objects will be (eg. getting image/audio classification training data specific to these objects, getting knowledge graphs including only the novel objects + known objects --- this is different from using an external database that contains knowledge about thousands of objects, etc), and not even rely on how many unknown objects will be tested on.\n2. Or does it simply mean that associated audio recordings are unavailable for the object training, but other information such as object annotations, object relations, etc are available?   \n\n* The task could be either one of these two options, or even somewhere in between in the spectrum. Could the authors define this clearly? The definition of the task would drive what baselines are sensible, and what knowledge the proposed method can use.\n* (Only) If the definition 1 (above) is used, is it reasonable to expect a method that has never seen or heard any objects from a certain category to find this object during evaluation? Note that this difficulty itself could be the point of the task. I'm a little surprised that the results in Table 1 are better than random for the baselines under such a scenario.\n\n\n### Approach clarity\n\n* From pages 1 to Page 4 (Sec. 4.1), the terms \"prior knowledge / knowledge-enhanced prior\", \"dual graph convolutional network\", \"pre-training tasks\", and \"background knowledge\" are quite vague, making it harder for a reader to get an intuition on what the paper is proposing to do. These are clarified only after Sec. 4.1.1 and can be a significant source of confusion.\n* In Figure 1, the GCN^v takes as input the visual classification probabilities over objects and regions. However, in Figure 2, the GCN input appears to be the image embeddings (not classification probabilities). Could the authors clarify this?\n* How is GCN^a from Figure 1 defined? This has not been discussed in the paper. Does it use some object-audio relations as priors? If so, how are these obtained?\n* In Section 4.2, the dataset for pre-training is collected from all 85 Matterport3D houses (includes train / val / test splits). Are the unseen houses and unheard objects excluded from this set? It looks like the method relies on some knowledge about the unheard objects and scenes during this pre-training phase (see below):\n  * 45,223 images from 85 scenes are used to train the vision classification model. The vision classification model predicts probabilities for all 21 objects (includes unheard objects).\n  * 1.5M spectrograms are generated for each sounding object from all the 85 scenes. Audio classification model is trained to predict for all 21 objects (this isn't outright stated, but this is a reasonable take-away from Sec. 4.2).\n* For navigation episode generation (from \"Episode specification and success criteria\"), is it ensured that the sounding object is visible from the \"set of viewpoints within 1 meter of the object's boundary\"?\n* For the Random walk baseline, how is Stop executed automatically? \n\n# Experimental results are not convincing\n\n* As intended by the authors, the task is significantly more difficulty (best success rate is 15% on unheard sounds case for proposed task vs. best success rate of 24% on unheard sounds case from SAVi). However, the proposed method is significantly worse than the AudioGoal and AudioObjectGoal baselines for navigation efficiency (SPL, SNA metrics), despite using prior knowledge about unheard objects for visual/audio classification and object-object / object-region relationships. The authors suggest that this is due to the agent being trained to explore, but I don't see why the agents are not trained to avoid exploring and just navigating. Note that it is standard practice to rely on efficiency metrics over only success rate for navigation methods (eg. see embodied AI challenges from prior CVPR workshops: https://embodied-ai.org/). \n* The proposed K-SAVEN approach has access to privileged information about the unheard objects for image classification, audio classification, and knowledge-graph building. This gives the method an unfair advantage over the baselines. \n* There are no ablation studies to verify that the model is working as intended, and that the different components are benefiting the final performance. \n* The paper is missing robustness studies for the presence of noise in the audio, and other distractor sounds during navigation (similar to SAVi).\n* The authors also speculate that \"Once we complete the entire training process, we expect to see a significant improvement in performance for our model\". This makes the results sound incomplete and the overall experiments unconvincing. For example, will the baselines also not benefit from completing the entire training process? \n\n\n\n",
            "summary_of_the_review": "I feel that the task is interesting, and that the proposed method is sensible. However, the task and approach are significantly lacking clarity. The experiments are also unconvincing due to the aforementioned weaknesses. ",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper focuses on the problem of semantic audio-visual navigation. This paper introduced a GCN-based mechanism for audio-visual navigation that incorporates prior knowledge about the relations among objects and regions. The empirical results are promising, outperforming recently published baselines by Chen et al. ",
            "main_review": "Strength:\n1. The paper is well-motivated. It is promising to improve audio-visual navigation by connecting prior knowledge and region semantics. \n2. The paper is well-written and easy to follow.\n\nWeakness:\n1. Lack of experiments. What I do want to hear from the authors is whether combining prior knowledge and region semantics can make the model more generalizable since it is a long-standing unresolved issue in visual-audio navigation. The authors provided the latest baselines, but I think they can also provide the results in seen/unseen scenes with heard sounds to show whether the model generalizes to unheard sounds bad like in SoundSpaces (Chen et al., 2020) and Chen et al., 2021a. It would also be better to provide some failure results of the proposed mechanism and give an analysis in the main paper. Failure cases can help readers to better understand the drawbacks of the proposed model.\n2. I do also concern about the generalization of the model in dynamic scenarios, especially finding small movable objects.  Letâ€™s imagine a scene: \n*One man in the room left his phone in the kitchen or restroom. It rings now. Apparently, It is not easy to recognize the associations between the acoustic-driven goal prediction (phone) and the (future) visual observations (kitchen or restroom). Can the agent find the phone?*\n Can the authors explain how the agents work in such a situation?\n",
            "summary_of_the_review": "The main idea of this paper is intuitive and straightforward. The main concern about this paper is the generalization of the proposed method in unseen scenes, unheard sounds, and moving objects.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper tackles the audio-visual embodied navigation task of navigating to an unseen indoor visual scene as well as generalizing to unheard sounding objects in a 3D virtual environment such as Matterport3D. The paper introduces the use of knowledge-driven scene priors in the semantic audio-visual embodied navigation task by combining semantic information from the novel knowledge graph that encodes object-region relations, spatial knowledge from dual Graph Convolutional Networks, and background knowledge from a series of pre-training tasks within a reinforcement learning. In addition, the authors define a new audio-visual navigation sub-task, where agents are evaluated on novel sounding objects, as opposed to unheard clips of known objects. The paper performs multiple semantic audio-visual navigation benchmark tasks using the Habitat-Matterport3D simulator and compares it with recent state-of-the-art and strong baselines. The paper claims to show improvements over earlier baselines on various performance metrics in unseen contexts.\n",
            "main_review": "The paper is well written and clear. I appreciate the authors' experiment protocol of avoiding simpler and easy to reach episodes (e.g., straight paths or short distances) during the training. The literature review is thorough. The strength of the paper lies in extending the use of a prior enriched with general experiences, in the domain of audio navigation for better generalization of agentâ€™s performance to novel sound sources. The paper also evaluates the agent on unheard (or novel) sounding objects which was not evaluated by previous methods. Authors have extended the concept of knowledge graph and Graph Convolutional Networks which was earlier applied to only vision-based navigation tasks to audio-visual navigation tasks. This contribution is worthwhile but not technically novel. \n\n\nI recommend authors to provide further clarification on -\nConstruction of knowledge graph - How exactly did they connect an object with another object if they frequently exist in different regions? Similarly, the method/logic used to connect a region with another region has similar objects placed in them.\n\nWeakness\nThe paper produces results using only Matterport3D dataset. Authors should also perform experiments on Replica database as performed by the baselines with which the author makes comparisons. \nThe paper lacks figures which showcase navigation trajectories on top-down maps using multiple approaches. It will be helpful to see the trajectories generated by the K-SAVEN to appreciate and understand the quantitative improvement reported in Table 1. \nThe complete lack of any ablation study makes it hard to understand the source of improvement in navigation. Strongly recommend to perform ablation studies such as  K-SAVEN w/o v_t and K-SAVEN w/o a_t. It would be appreciated if the authors could provide human results as the gold standard for comparison. This can help put the current performance in the context. \nAuthors should also provide failure cases as images or videos at least in the supplementary along with the reasoning. \n\nMinor comments:\naudio-visual is sometimes written as audiovisual. Please use one consistent (suggestion: audio-visual) version all throughout the paper. \nAlthough this is not critical, I suggest improving Figure 1 to look visually more pleasing. The font size for the title is smaller than the content inside the modules, multiple color codes have been used without mentioning about the relationship in the caption. \nThe references are sometimes inside brackets (Chen et al., 2020) and sometimes outside like this Chen et al. (2021a). Revise all of them to be consistent with the former (XX et al., YYYY). \nThe authors should provide metric definition (formulae) for the evaluation metrics (1) distance to goal (DTG) and (2) success when silent (SWS) to make it absolutely clear for other researchers during comparison. \nTypo in Figure 2 caption - visoion networks should be vision networks.\n",
            "summary_of_the_review": "The concept of using scene-priors in the form of Knowledge graph and using Graph Convolutional Network to compute relational features on the graph is not new and has been applied in earlier methods as stated by the authors (literature review section and  https://arxiv.org/pdf/1810.06543.pdf). Given the limited novelty of the method and the absence of ablation studies, visualization of failure conditions, and top-down map view of agents trajectories, I suggest a weak reject (marginally below acceptance threshold). \n\nTo strengthen the current proposed method, I suggest authors perform ablation studies using some combinations of the below two important modules.\n1> Knowledge representation (e.g., logical formalism, knowledge graphs, probabilistic graphical models) \n2> Knowledge type (e.g., spatial commonsense, declarative facts). \nI appreciate authors briefly discussing the above fact in their literature review section and what motivated them to elect knowledge graphs that included object-object, object-region semantics. But, it would be more convincing if the authors could back it with results. \n\nI am ready to change my decision after the author reports positive results from the above evaluation. \n",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper tackles the semantic audio-visual navigation task, and the main novelty is the scene priors in the form of object and region semantics.  A knowledge graph is constructed for training the model to leverage knowledge-driven scene priors. Experiments on Matterport3D dataset shows the proposed method compares favorably with the baselines. ",
            "main_review": "*Strengths*\n\n- The task is interesting and leveraging scene priors to improve the performance of semantic audio-visual embodied navigation makes sense.\n\n- It's a nice attempt to train audio-visual embodied navigation agents to generalize not only to unheard sound clips as in prior work, but also to truly novel sounding objects.\n\n- The design and use of graph convolutional network to compute visual-semantic and audio-semantic audio embeddings to analyze object-region relation is nice.\n\n- Clearly written in general with nice illustrations and problem formulations.\n\n\n*Weaknesses*\n\n- The overall novelty of the paper is somewhat limited. The paper over-claims the contribution of curating a new dataset for training the visual encoder and defining a new task for semantic audio-visual navigation. However, the main difference compared to SAVi is the new evaluation protocols of transferring across objects, and explicitly model room regions for navigation.\n\n- For modular pre-training, what is the classification accuracy for object and region prediction? It would be important to do some ablation study to analyze how important the region prediction accuracy is to the navigation performance since it's one of the main ideas in the paper. \n\n- The results in Table 1 is not very convincing. Compared to AudioGoal and AudioObjectGoal, the proposed method is worse in terms of SPL or SNA. More discussions are needed on why the proposed method performs worse on these two important metrics.\n\n*Some minor typos:*\n\nE.g. \n- Figure 2 caption, in visoion network -> in vision networks\n\n- in conclusion, we provid -> we provide\n",
            "summary_of_the_review": "The paper proposes some model improvements to the semantic audio-visual navigation task. However, the novelty of the paper is rather limited and the experimental results are not convincing enough to show the proposed knowledge graph scene priors are useful. Therefore, I am leaning towards rejection of the paper.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}