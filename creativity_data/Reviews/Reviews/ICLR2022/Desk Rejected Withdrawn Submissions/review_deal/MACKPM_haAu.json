{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper addresses the problem of adversarial untargeted attack to 3D point cloud networks by proposing a method for creating adversarial point clouds by taking into account some constraints on the number of added points and deformation degree. The proposed method was applied in several point cloud networks, e.g., PointNet, PointNet++, DGCNN, and evaluated on two benchmark datasets: ModelNet40 and ScanObjectNN.",
            "main_review": "* Strengths: The paper addresses an important research problem\n\n* Weaknesses:\n1) Novelty: The paper addresses a similar problem with the method in [A] and also proposes a similar approach with [A]. However, compared with [A], the proposed method has limitations in terms of the generality of the problem formulation and its applicability. In particular, first, the method in [A] can be applied for both manipulating existing points and adding new points, while the proposed method is limited to adding new points only. Second, in the proposed method, n and epsilon are upper bounds, while in [A] all the factors including F, dist, delta_X are jointly optimized and thus, as shown in [A]'s results, optimal values (determined by their algorithm) are well below the upper bounds. \n2) Presentation: The description of the proposed method is not clear enough. For example, what does \"projection\" (in Figure 1) mean? I also cannot see how and why nearest neighboring point is used in the proposed method.\n3) Experimental results: \n- As the proposed method addresses a similar problem with [A]. It is necessary to conduct a comparison between the proposed method and [A].\n- I have double checked the results in [A] which seems to outperform the proposed method. For example, for PointNet and on ModelNet40, [A] achieved a success rate of 89.01% while using only 36 points. \n- I also note that [A] was validated with more recent backbones, e.g., PointASNL, which are missed in the paper.\n- I wonder how the proposed method works against the Dup-net by Zhou et al. \n4) Missing references\n- [A] Minimal Adversarial Examples for Deep Learning on 3D Point Clouds, ICCV2021\n- I also suggest the authors include references for ModelNet40 and ScanObjectNN dataset.",
            "summary_of_the_review": "Given the limitations in the novelty, generality and applicability of the proposed method compared with an existing but miss-referenced method, as well as issues in experiments, I would recommend reject for the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies adversarial attacks on 3D classification of point clouds. To escape from local maxima, this paper proposes to gradually decrease the step size during adversarial attacks.",
            "main_review": "Although this paper is clearly written, it has some weaknesses as detailed below.\n\n- The novelty of the proposed method is limited. The motivation of this paper is to avoid local maxima during generating adversarial 3D point clouds, and an adaptive step size scheduler is proposed. However, to overcome the problem of local maxima, different techniques could be adopted (e.g., momentum optimizer), but why the authors specifically choose to use adaptive step size is unclear. This method itself lacks novelty.\n- The experiments lack some important baselines, including \"Robust Adversarial Objects against Deep Learning Models\" (AAAI 2020), \"Geometry-Aware Generation of Adversarial Point Clouds\" (T-PAMI 2020), etc.\n",
            "summary_of_the_review": "The paper lacks technical novelty and some important baselines in experiments.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work proposed an adversarial attac against point cloud classification. First, it modified existing attacking methods by adding hard constraints on the number of the modified points and on the point perturbation norms. Second, to realize effective optimization, it proposed to   use a high step-size at the beginning of the optimization to avoid the local optimization.  The results are conducted on the two public datasets for attacking three point cloud classification models. Two recent attacking methods are set as the baselines. The results demonstrate the effectiveness of the proposed method.",
            "main_review": "In terms of the strengths, this manuscript is well written and easy to understand. The reported results show state-of-the-art attack success rates against point cloud classification models. \n\nNevertheless, I have the following concerns:\n\n1. The motivation is not clear. Why do we need to set the hard constraints on the number of perturbated points? Actually, in the real world, the number of points is decided by the scanning frequency that samples points of physical objects, and we could have different numbers of points according to the sampling strategy. I mean less number of points does not mean the generated points are more realistic. Note that, this is different from the 2D images where the pixel numbers are fixed and we need to modify the intensity of pixels. \n\n2. It is hard for me to understand why the proposed method could outperform baselines in the attack success rate with more constraints. Are the constraints help mine semantic information? or the optimization method helps achieve this goal? Why?\n\n3. In this version, the novelty seems limited. It is more like trivial skills instead of technique contributions. For example, it sets a series of hyperparameters to tune the step size and boundary. I suggest adding more theoretical analysis to highlight the importance of the contributions:\n\n3.1 why projecting the perturbation to the neighborhood of the original points could help address the local optimization? Could you provide some insight analysis?\n\n3.2 In my opinion, variable step-size attack and boundary attack is the same way to address the issue. One is to tune the step size at each step and another is to tune the maximum step size range. How do the two strategies affect each other? Why put them together could help address the issue?\n\n4. There are some typos that should be addressed, e.g., 'retaining its appearance' in Sec 3.1. ",
            "summary_of_the_review": "Overall, this work is well written and easy to understand. My main concerns are the motivations and limited novelties. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors have proposed an adversarial attack method for point clouds, which limits the modification of point cloud surfaces. In order to address this, two constraints are presented, which include hard boundary constraints on the number and norm of the modified points. As this will lead to more local maxima, the authors use a high step-size at the beginning and then gradually decrease the step-size. Experiments show the effectiveness of the method.",
            "main_review": "Strengths:\n+ Good experimental results and visualization.\n\nWeaknesses:\n- The idea of designing an adversarial attack on point clouds with distance constraints is not new. For example, Tsai et al. (2020) proposed a kNN distance constraint to obtain a robust adversarial attack, which also aims at keeping the surface unchanged. Therefore, the authors should highlight the difference between the proposed method and relevant methods. Also, the novelty is a serious concern as it's not the first time to propose an attack with surface constraints.\n\n- The writing of the paper needs to be improved. It's hard to understand the motivations and contributions of the paper from the Introduction section.\n\n- The idea of Variable Step-Size Attack (VSA) is trivial. It's a common sense that we should use larger learning rates to train the network at first, and then gradually change to smaller ones. So directly transferring this experience to step size does not excite me.\n\n- The idea of Variable Boundary Attack reminds me a series of works in curriculum learning. It's better to draw some connections with them:\n[1] Curriculum Learning, ICML, 2009.\n\n- More baseline attacks should be compared for a more comprehensive experimental evaluation. The reviewer suggests referring the experiments in this recent paper:\n[2] IF-Defense: 3D Adversarial Point Cloud Defense via Implicit Function based Restoration, arXiv, 2021.",
            "summary_of_the_review": "Due to the limitations above, this paper is unlikely to reach the ICLR level.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}