{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a method of re-balancing regression using the proposed Bayesian Posterior Debiasing method. Several train-time and test-time variants are proposed and evaluated against some existing methods of imbalanced regression.\n",
            "main_review": "Strengths:\n\n* Useful visualizations demonstrating the effect of rebalancing;\n* Experiments on synthetic as well as real data;\n\nWeaknesses:\n\nAs mentioned by the authors, Eq. 2.3 is a well-known formula. In classification, the situation where 'invariant generative probability p(x|y) is assumed' is known as prior probability shift. Overall, I do not see anything novel in the formulas provided in Sections 2.1, 2.2 and 2.3. I think the name 'Bayesian Posterior Debiasing' is misleading the reader, because although there is use of Bayes' rule, there is no choice of prior before the learning process starts. I view this simply as adaptation to shift, which for classification has been called prior probability shift in the past.\n\nA minor comment: in Eq. 2.4 it would be in my opinion more natural to use integrals instead of expectations. In this way it would become more clear that division is really just there to renormalize such that the probabilities would integrate to 1. By the way, in case of the right-hand-side formula, the denominator seems to be actually always equal to 1 because $p_train$ is already a probability distribution. Also note that these formulas are expected to read as assignment, so they should have ':=' instead of '='. Mathematically, however, it would be better to have a different notation before and after renormalization.\n\nAnother minor comment: in Eq. 2.1 the quantity I is misleading because it is standard notation for an identity matrix whereas the formula seems to be about standard regression (predicting a single real-valued quantity).\n\nThe paper seems to implicitly make an assumption that the target variable in regression is bounded to a particular range of values, but this assumption is never mentioned. How else could one treat $p_{bal}(y)$ as a constant, or is it meant to be like an improper prior? If there would be no bounds, then in Eq. 2.7, the integral in the denominator of the left-hand-side formula seems to be equal to infinity and $p_{bal}(y)$ would become a constant zero. If the assumption of being bounded is made, then does it matter how wide the bounds are, as long as they include all the data? Would further extending the bounds change anything?\n\nSection 2.4.1 uses the notation of capital sigma, $\\Sigma$ and even mentions covariance. However, I had understood from the earlier that y is a real number and not a vector, and thus the distribution over y would be a univariate distribution. Hence, the considered Gaussians are univariate and parametrised by variance rather than covariance.\n\nI do not fully understand the intuition behind the proposed methods. Why should we aim to perform well on a y-balanced dataset? Most imbalanced regression methods aim to perform well on the original distribution and hopefully on balanced distributions as well. However, artificially adjusting predictions to be balanced as the test-time method is doing seems to be solving a simpler and less relevant problem than general imbalanced regression methods. The proposed train-time methods make sense to me.\n\nThe publications which propose SMOGN (Branco et al 2017), DenseLoss (Steininger et al 2021) have been cited but the experiments do not include these existing methods in the comparisons. Thus, it is not clear whether the proposed methods improve over the state-of-the-art.\n",
            "summary_of_the_review": "There are several problems regarding the mathematical formulas and clarity of assumptions being made. The experimental part does not include some important cited methods as baselines.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a Bayesian posterior debiasing framework for imbalanced regression problem. It discusses both the train-time adjustment and test-time adjustment for matching the different label distributions of the training data and testing data. Experiments are carried out on both synthetic and real data sets. \n",
            "main_review": "- The topic of the paper is about a common issue in practice so it is targeting an important question. The proposed framework seems to be very general. However, I find I could not follow the paper, especially when it starts to present the methodology in Section 2.\n\n- The framework setup of the investigation is unclear to me. Why the training and testing labels are generated from different distributions. Are they from different data sources, or the testing set is balanced by reducing the dominating labels? \n\n- The invariant generative probability assumption seems to be very strong. Consider a binary label. If the balancing is achieved by some non uniform method such as the local case control sampling (Fithian, William; Hastie, Trevor, 2014. \"Local case-control sampling: Efficient subsampling in imbalanced data sets\". The Annals of Statistics. 1693–1724), then this assumption will not be violated. At least some discussion on this assumption should be provided.\n\n- Can I understand $x$ as the features or input variables? If so, $p_{bal}(y|x)$ is the conditional likelihood. Why is it called a balanced posterior?\n\n- It is pointless to put (2.4) in the main paper. It is trivial because the denominator is just 1, and it does not help understand (2.5), for which the derivative is provided in the appendix. Since (2.5) is the base for the proposed framework, it would be helpful to make its derivation clear. The derivation in A.1 is quite short with three steps, so it would help readers to follow the paper by moving them into the main paper. However, I am could not following the derivation. The notations of $y$ and $y'$ seems to be messed up. How is (A.3) obtained from (A.2)?\n\n- Is it reasonable to take $\\sigma^{2}_{pred}$ as a tuning parameter? It is a model parameter so would it be better to estimate it?\n\n- What are the meaning of the numbers in Table 1?\n \n- Minor issues:\n   - First sentence of page 3, \"label space Y, Y is either ...\" should be \"label space Y, where Y is either ...\"\n",
            "summary_of_the_review": "The paper proposes a general framework for an important problem of imbalanced regression. The presentation is hard to following and there is no theoretical justification. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies imbalanced regression and explores Bayesian Posterior to handle this problem. More specifically, this work proposes a normalization parameterization technique to analyze Bayesian Posterior in regression and develops several approximation methods to simulate label marginal distribution. Experimental results show the effectiveness of the proposed method. ",
            "main_review": "Positive points: \n\n1. This paper studies a practical imbalanced regression task. Considering long-tailed regression has been identified as an important future research direction [1], this task will attract increasing attention in the future. \n\n\n2. This paper explores the Bayesian posterior for imbalanced regression, and develops several approximation methods for the label marginal distribution to extend Bayesian relation from a discrete label space to a continual label space.\n\n\n3. This paper also creates a new multi-dimensional imbalanced regression benchmark on Imbalanced Human Mesh Recovery (IHMR), which is different to existing imbalanced regression datasets with the uni-dimensional label space. Such a benchmark will benefit the research area of this task.   \n\n\nNegative points:\n\n\n1. The authors mentioned that \"the key insight is that a balanced posterior can be obtained by debiasing the conditional probability with a regression label space prior.\" Although I agree with the conclusion, it seems a simple extension from imbalanced classification to regression. Note that such a conclusion has been widely used in long-tailed classification. Therefore, such insight may not impress readers a lot.\n\n\n2. The derivations of Eq. 2.5 is unclear and may confuse readers. It would be better if the authors can detail A.2->A.3 in an easy-to-understand way. Moreover, I think it is acceptable to simply assume the data marginal distributions are consistent between training and tests, which is a widely used assumption in long-tailed learning. \nMoreover, the following methods are also instantiations of Eq. 1.1 instead of Eq. 2.5. As a result, I do not see much value in the normalization reparameterization technique. Therefore, after clarifying the derivation of Eq. 2.5, I suggest the authors highlight why we need to use the normalization reparameterization technique.\n\n\n3. Considering the proposed method also explores post-calibration/test-time training, I suggest the authors review and discuss more advanced methods of post-calibration (e.g., LADE [2]) and test-time training (e.g., TADE [3]) for long-tailed learning.\n\n\nReferences:\n\n[1] Deep Long-tailed Learning: A survey. ArXiv, 2021.\n\n[2] Disentangling Label Distribution for Long-tailed Visual Recognition. In CVPR, 2021.\n\n[3] \nTest-Agnostic Long-Tailed Recognition by Test-Time Aggregating Diverse Experts with Self-Supervision. ArXiv, 2021.",
            "summary_of_the_review": "Although there are several negative points, I think the contributions of this paper will be beneficial to the long-tailed learning community. I expect authors to fix the mentioned negative points for further improving this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}