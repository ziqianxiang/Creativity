{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper conducts an extensive survey in using linear probes to evaluate how representations from a model trained to do one task can be best used to perform another. A wide range of tasks for retina images and linear models are used in this paper. The main conclusion is that the middle layers is best performing layer with the linear probes. The authors also tried to explain what they found with a synthesized image dataset.",
            "main_review": "- Strength: The experiment is very thorough. The number of linear models used and the comparisons performed in this paper is very impressive. \n\n- Weaknesses: \nMotivation of the paper is relatively weak. If the main point is to argue that linear probe is a useful tools to interpret CNN it is unclear to me why classification tasks on retina images are a good general task pool for this. The specificity of task choice here make it hard to argue this is a generalizable trend to datasets with more natural images and more complex tasks. The \"dead leaf\" image datasets also suffers from the same problem.\n\nThis paper could have applied linear probe method on other more standard CV datasets that are designed to evaluated task transfer or task relationship such as the task bank from Taskonomy (Zamir 2018). In fact this paper makes no mention of this line of work where transfer learning is used to evaluate share of information between source tasks and target tasks. A comparison is useful to gauge how good linear probe is in interpretation of CNN.\n\nThe novelty of this paper is also lacking. Linear probes, which is not that different than other linear methods for comparing representations in neural networks, has been used widely to compute similarity across train networks, in fields of both machine learning and neuroscience.\n\nFigure 5: All the lines seem to have the same thickness. It is unclear to me what readers should get out from this image. ",
            "summary_of_the_review": "This paper lacks both novelty and is not very well motivated. Despite the extensive experiments done, I don't think it communicates generalizable results that are interesting to the audience of this conference. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This presented work proposes a technique based on linear probes to analyze generalization abilities of different network layers. The introduced method is applied to retinal images and uses embeddings extracted with the Inception v3 model. In addition, a synthetic dataset of images generated using the dead leaves model is used for further analysis of the CNN behavior. ",
            "main_review": "This work addresses a problem of model interpretability in a practical application of retinal image analysis. Studies on the model interpretability and explainability are essential in multiple applications, so the topic covered in the presented work is important for the ML community. However, more work is required to improve quality and clarity of the work:\n1) It is essential to understand the used dataset first. It's important to analyze distribution of specific variables. Prediction of some variables might be more challenging than others due to the dataset nature and the variable itself. Please clarify that.\n2) The method was neither compared with the related work, nor the existing studies were explained (except a very brief summary at the end of the paper), so results are difficult to justify and novelty is not clear, as similar studies already exist and they were tested on more topologies, e.g., https://arxiv.org/pdf/1610.01644.pdf\n3) Clarity of the work should be improved. Some Figure captions differ from the legend, Figure 5 is unreadable, the heatmap figure's lables are difficult to read/interpret. Names of specific layers along with their dimensions should be provided.\n4) Could you explain the drop of performance for the left>right task?\n5) It would be interesting to show performance of each layer at the beginning and at the end of the training to determine if they are really learning. Also, how do you ensure the linear probes do not overfit? Please provide more details on the training procedure. \n",
            "summary_of_the_review": "The method was not compared with the related work and only a brief summary of existing studies was provided at the end of the paper. There are multiple existing studies addressing this topic and they include analysis of different topologies. Thus, it is not clear how this work differs from them. Also, the clarity of the work should be improved for better understanding and some important details, e.g., training procedure are missing. The quality of the presented work is not sufficient to publish it in a current form.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper uses cross-task grid of linear probes for interpreting the prediction of deep neural network. In particular, it analyzes the interpretation of networks’ representation trained for different “source” tasks while test on some “target” tasks on a common retinal dataset, using linear probes. The authors conduct experiments with 101 “source” task and ~193k models. The experimental results show that the learned features of the middle layers of the networks are more generalizable across multiple target tasks.",
            "main_review": "Strengths:\n1.\tThe idea of using linear probes for interpreting the predictions is interesting.\n2.\tOverall, the experiments are solid. There are ~193 models are conducted to demonstrate the effectiveness of the linear probes.\n3.\tThe experiments section is well presented. It is nice to see the visualization results.\n\nWeakness:\n1.\tThe novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation. \n2.\tThe motivation of the paper is not clear. It seems an experiment report about ~193k models, and obtains the obvious results, such as the middle layers represent the more generalizable features. But it is not about interpretation.\n3.\tThe writing is not clear. it's hard to read and follow the work.\n4.\t\n\nConcerns:\n1.\tWhy need 101 “source” tasks? What are these?\n2.\t101 tasks can be done on the retinal image, it is not a unified domain to do the research about model interpretation.\n3.\tThe motivation and conclusion should be clearly presented to under the contribution of this paper for model interpretation.",
            "summary_of_the_review": "The topic is interesting, but the novelty of this paper is limited, the writing is unclear.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper analyzes the relationships and shared structure among different prediction tasks on a dataset of retinal images using linear probes, and observes that representations from the middle layers of the network are more generalizable. The authors find that some target tasks are easily predicted irrespective of the source task, and that some other target tasks are more accurately predicted from correlated source tasks than from embeddings trained on the same task.",
            "main_review": "Strengths:\nThis paper studies and understands CNN model predictions on 101 different tasks based on the UK Biobank dataset of retinal images, and obtain some new discoveries.\nWeaknesses:\n1.\tJust as the title says, the paper aims to solve the interpretability of CNN model, but it does not clearly explain how CNN makes decisions on fundus prediction tasks. The author should present some visualization results of CNN, such as feature map of the middle layer.\n2.\tThe authors only adopted one CNN model Inception V3, and is there a similar conclusion when generalizing to all tasks with other network structures like ResNet or VGGNet?\n",
            "summary_of_the_review": "In general, the paper is difficult to understand and most of them is empirical and does not give a clear conclusion.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}