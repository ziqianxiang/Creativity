{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper addresses the problem of semantic segmentation through unsupervised domain adaptation. The self training (supervision) is guided by pseudo labels predicted by a model trained on a source domain as well as objectness constraint (cluster labels) computed by combining image based and depth-based clustering. The experiments illustrate that combining image and depth information might help in filtering noisy labels from a source model thus resulting in better domain adaptation performance.\n",
            "main_review": "Pros\n- Utilising depth information for better guidance of UDA seems like an interesting idea for semantic segmentation problems.\n\nCons\n- Minor contribution (the proposed additional loss is only used in the fine-tuning phase, ie. after the warm-up phase).\n- Missing the proper ablation study, no details on segmentation parameter search (e.g. depth clustering params, image clustering params etc. have not been evaluated).\n- Missing supplementary material to which there are multiple references in the main text\n- Paper contains unjustified claims\n- Missing convincing results\n\nThe paper contains an interesting idea of improving the UDA by employing depth information for better pseudo-label filtering. This indeed if properly applied could result in better UDA. \nAuthors however, are mainly based on the frameworks proposed by Zhang 2019 and Tsai 2018, building on top by only adding a new loss (objectness constraint), which unfortunately is not (imho) properly evaluated. The clustering methods that are deployed consist of multiple parameters, such as “b clusters” for depth clustering, “\\theta thresholding”, the number of “k_s” clusters for image-based clustering, etc. None of these is evaluated, so the impact cannot be really justified. Further, the evaluation for all the thresholds that are introduced for pseudo-labels and objectness constraints (e.g. superpixel threshold) is also missing.\n\nAdditionally, the paper refers multiple times to the supplementary material that unfortunately has not been added to the submission.\n\nThe paper also contains unjustified strong claims, e.g. the 3rd point in the list of contributions (P2) is “we empirically demonstrate that our proposed constraint is general enough to be\nplugged into any base self-training method and improve performance” for which there is no support in the paper. \n\nFinally, the obtained performance by the proposed method is not always better than the baseline, eg. Table 1. SAC* vs SAV*+PAC (proposed), the performance significantly drops for such categories like truck, bus and car. The authors should definitely discuss this behaviour and try to identify the culprit.  \n\nMinor:\nP7. Paragraphs Base PLST methods and Architecture and Implementation Details could be shortened or moved to supplementary material. Both of them contain too many irrelevant details for understanding the method and they break the flow of reading.\nFigure 3 should be re-organized into 4 columns: Image | GT | Prediction for Simplified-CAG | CAG+PAC (ours). Otherwise, the authors waste a lot of space and additionally do not facilitate the visual comparison.  \nFigure 3, caption “much benign -> much better” ? \nP4. Paragraph “Pseudo-label self-training”, 3L “using (4) to pseudo label” ? looks like a formatting error?\n\n",
            "summary_of_the_review": "The paper is missing important justifications of the claims presented in the paper. The proposed method can not be judged without a proper ablation analysis, which is missing. My recommendation is REJECT. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The goal of this work is to improve the accuracy of pseudo labels for unsupervised domain adaptation of semantic segmentation.\nThe main observation is that models trained on source data exhibit miscalibrated predictions on the target data due to domain shift.\nThe proposed approach is to construct depth-informed superpixels and to enforce consistency of the semantic label inside them.\nThe particular instantiation of this idea in this work leverages contrastive-based loss that pulls the features closer to the closest cluster’s mean, while pushing them from the other cluster centers.\nThe experiments show that such an approach can moderately improve the baseline in two standard benchmark scenarios.",
            "main_review": "Strengths:\n+ the issue of inheriting strong biases from the source data and the resulting miscalibration on the target data is important to address\n+ the proposed approach is relatively simple\n+ the presentation is overall accessible: one can follow the main goals and the intuition behind the approach\n+ the idea to use depth jointly with appearance-based superpixels is interesting and is a well motivated strategy to overcome some of the limitations in previous work \n+ the proposed method is empirically shown to improve the accuracy of the baseline\n\nWeaknesses:\n- the paper offers little technical novelty: (I) previous works also exploited superpixels and depth with more significant gains (e.g. Vu et al., (2019); [A]) (ii) it’s unclear how the proposed merge of depth and superpixels leads to object-specific clusters. In Fig. 2, for example, the transporter on the left is oversegmented, hence may still be labeled inconsistently using  the proposed loss. Clearly visible is also degenerate oversegmentation in the areas where depth estimation failed (e.g., in the lower left corner in Fig. 2).\n- the overall empirical improvement is rather sobering: one would expect a more significant boost comparable to previous work (e.g., as in Vu et al., (2019)). I suspect that either the highlighted issues which this method addresses are not as serious, or the method is not as effective as one would hope.\n- key details are missing. For example, depth estimation is the central component, yet it is not explain where this modality comes from.\n- the ablation experiments are on the weak side and do not provide much insight. The study focuses on just two settings (only-image and only-depth), neither of which is explained sufficiently. The qualitative results aren’t particularly informative either: one is left to wonder how and where the method improves the baseline.\n\n[A] Zhang et al., Curriculum Domain Adaptation for Semantic Segmentation of Urban Scenes. ICCV 2017.",
            "summary_of_the_review": "Overall, I find that the work tries to address a very relevant issue in self-training for UDA of semantic segmentation, yet fails to deliver a technically sound approach with compelling empirical evidence.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "To improve the erroneous pseudo-labels for the task of semantic segmentation that arise in the context of UDA, the authors regularize the existing self-training objectives  with a new contrastive loss. They define the notion of objectness in the scene which is obtained from the structure-preserving modality of that scene (depth). This objectness is agnostic to the ground truth labels of objects in that scene, thus suitable for UDA setting. The authors first compute the super-pixels in the scene by unsupervised clustering method (existing method SLIC) and then refine it with depth map histogram to better encode the geometric structure of the object. Ideally, each super-pixel is an object in the scene. Once the authors get the super-pixels, they use contrastive loss on those pixels, which pulls the same object super-pixels closer to each other and pushes different object super-pixels away from each other. It has also been shown that this regularization can be applied to any general UDA method for semantic segmentation to boost the performance.\n",
            "main_review": "Strength:\n1. The Idea and the proposed loss is novel.\n2. Loss is general enough to add with existing methods.\n\nWeakness:\n1. The work is incremental. While the idea of objectness and geometric structure are definitely useful, it is not a surprising result or a very novel idea.\n2. Results are not that great so as to nullify the concern about novelty. For GTA to cityscape result: for road and sidewalks IoU improves after applying the proposed loss upto 9.4% (highlighted and justified by the authors), but for Synthia to cityscape results: same categories road and sidewalks, it is less than SOTA by 8.5% and 12.2%. Justification is not provided here.\nAlso, though road and sidewalks results for GTA to cityscape are improving quite a bit, for car and truck it is less than 13% and 10.6% than SOTA which is quite low. Also on average, it is still less than SAC (SOTA). The result is better than SAC*, which is a simplified version of SAC (Due to less GPU resources) by a small margin of 0.6%. Due to high variabilty of performance compared to SOTA, this average improvement in IoU seems quite less.\n",
            "summary_of_the_review": "The paper is easy to read and understand. Its contribution is incremental, which could have been compensated for if the results were much better than the SOTA, which is unfortunately not the case.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work presented a new objectness constraint to tackle unsupervised domain adaptation semantic segmentation. Compared with existing methods, the authors reduced the confirmation bias in domain adaptation by introducing a objectness constraint based on structural cues. To verify the effectiveness of proposed method, the authors implemented experiments on popular datasets.\t",
            "main_review": "1. The authors claim that the confirmation bias caused by domain shift can be mitigated by introducing structural cues such as depth information. However, there is no corresponding statistical evidence to support their views.\n2. The proposed method has many unclear descriptions. First of all, the symbols in Sec.3 and Sec.4 are easy to confuse, such as |I_C^s| and |U_k| which are ambiguous and difficult to understand. Secondly, there is no flow chart to illustrate the proposed method, which greatly reduces the readability of the paper.\n3. The proposed constraint mainly consist of two parts, which are the superpixels clustering via combined modalities and the objectness constraint by L_obj^t. However, there is no ablation experiment to show the effectiveness of the proposed L_obj^t. \n4. The proposed method can be plugged into other self-training methods and improve performance with the help of depth information, which caused unfair comparison.",
            "summary_of_the_review": "This paper propose objectness constraints that can reduce the confirmations bias caused by domain shift by introducing structural cues. The proposed method can be plugged into other self-training methods and imporve performance. However, the experiments are insufficient and the description of the proposed method is not clear. Based on the strengths and the weaknesses described above, I tend to give the paper rating with marginally below the acceptance threshold.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}