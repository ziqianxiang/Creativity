{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper build on the Fusion in Decoder (FiD) model that retrieves passages using the DPR query encoder and index, co-encodes the retrieved passages with the question, and generates an answer using a decoder that independently attends to each of the encoded (question, passage) sequences. The current paper adds a passage re-ranker that is applied in two ways. Once on DPR encodings of passages to select the top 100 passages from 1000 retrievals. And once on the output of the initial L_1 layers of the (question, passage) encoder, to select the top 20 pairs that should be sent to the next (L - L_1) layers for further encoding, and ansewr decoding. The first reranking step increases the quality of the passages sent to the (question, passage) encoder, with little increase in computational cost. And the second step reduces the amount of computation needed to generate the answer.\n\nThe re-ranker is graph neural network (GNN) that operates over a graph of the retrieved passages, where the graph is created by connecting passages from pages that are linked together by some Wikidata relation (following Min et.al. 2019). There is also a comparison to a simpler multi-layer perceptron (MLP) reranker, but the details of this re-ranker are not provided. The GNN reranker is marginally more effective than the MLP reranker and, when applied both to the DPR retrievals and to the set of passages selected as input to the answer decoder, the re-ranker leads to a small improvement over the knowledge distilled FiD model (KD-FiD) on NQ-open (0.5%) and TriviaQA (0.2%). The increases are larger for FiD without knowledge distillation (1.5% and 1.1%). The re-ranker also permits some saving in computation, yielding a system that can achieve the same performance as FiD with 40% of the inference-time computational cost.\n\nAblations show that both stages of re-ranking are effective, and the DPR result re-ranking improves performance on Hits@K retrieval metrics, as well as the end answer generation accuracy. It is clear that re-ranking is a useful approach, but the comparison to a simpler MLP method does not convince me that the the GNN approach here is fully needed. Since this paper is largely motivated by the possibiilty of saving inference-time compute, I think it would be much stronger with a more thorough comparisong of the GNN approach to the MLP baseline, that takes computational cost into account as well as accuracy.\n",
            "main_review": "This paper presents a GNN based passage re-ranker for open domain QA systems that follow the retrieve-then-read paradigm. Re-ranking is applied in two ways---to select the top 100 passages from the DPR retrievals, and to rerank the (query, passage) pairs processed by the reader. The full system achieves a small gain in accuracy over the FiD system that it is based on, and it can match the baseline accuracy with less than half the computational cost for passage reading and answer generation.\n\nSection 4.3 has a thorough breakdown of the computational complexity of the reader-generator model. However, it does not discuss the re-ranker cost. While I believe the GNN reranker should be cheap, it could be applied to up to 1000^2 passage pairs in the initial reranking stage so it would be good to quantify the cost of this step. I would also like to see a more thorough comparison between the proposed GNN re-ranker and the MLP baseline that: applies the MLP re-ranker in both the retriever and reader reranking stages; includes the large FiD model; and discusses the computational complexity tradeoffs. Section 4.4 should also include a more complete description of this MLP baseline. It is not clear if the MLP is just appplied to (query, passage) pairs or if it takes (passage, passage) interactions into account somehow.\n\nOverall, this paper is showing nice gains from re-ranking. However, it's not entirely clear that these gains are properly attributable to the GNN approach or if they could be achieved with the simpler MLP baseline. The only comparison between these two systems is incomplete, but suggests that the differences are marginal when combined with the system architectures that are highlighted in the main results section (KG-FiD L_1 >= 6).\n\n**Strengths**\n- Demonstrates utilty of re-ranking for open-domain question answering with predominant reader-retriever approach.\n- Ablations provide insight into benefits of applying re-ranker in multiple ways---both directly on output of retriever and on partially encoded (question, passage) pairs.\n\n**Weaknesses**\n- Comparison to much simpler MLP re-ranker could be a lot stronger. The MLP is not properly defined, the GNN's gains are marginal, and the relative complexity of the two approaches is not discussed. It's not clear that the more complex GNN is really needed.",
            "summary_of_the_review": "- Small but consistent gains from re-ranking passages in retrieve-then-read-then-generate open-domain QA pipline.\n- Incomplete comparison to much simpler baseline raises questions about the need for the KG-based GNN.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes a new open-domain question answering (ODQA) method which is built on top of a recent work called Fusion-in-Decoder (FiD) [1]. ODQA systems has two modules, (1) Retriever: which retrieves relevant passages given a question and (2) Reader: Which predicts the answer provided a subset of passages and the question. FiD introduced a method using which the reader can take a larger number of passages as input compared to earlier methods. This paper uses the reader from FiD paper and retriever guided by knowledge graph similar to that in Graph-retriever [2]. Unlike [2] this paper proposes to use a GNN (specifically GAT) in order to capture connections between retrieved passages so as to do a better reranking of retrieved passages. Authors also use the intermediate BERT representation for reranking with the argument that reranking is an easier task compared to generating the answer and doesn’t need all the layers to do it. This is done to reduce the computation time. The experiments show that using knowledge-graph augmentation helps in increasing accuracy over the baseline FiD and FiD-KD (FiD with knowledge distillation) models. This way the proposed paper also gets the state of the art number in one of the two datasets considered.\n\nReferences:\n[1] Gautier Izacard and Edouard Grave. “Leveraging passage retrieval with generative models for open domain question answering”. arXiv preprint, 2020\n[2] Sewon Min, Danqi Chen, Luke Zettlemoyer, and Hannaneh Hajishirzi. “Knowledge guided text retrieval and reading for open domain question answering.” arXiv preprint, 2019\n",
            "main_review": "Strengths:\n1) The paper proposes a new method for open domain question answering which is the state of the art in TriviaQA dataset.\n2) Paper shows that adding extra information in the form of knowledge-graph increases the model performance on FiD models.\n3) A novel idea to use knowledge graphs and DPR together for better retrieval. \n\n\n\nWeaknesses:\n1) It feels like the improvement in EM score after applying KG tricks (which also use extra information in the form of wikidata) is smaller for FiD-KD, which is the best base model. From FiD-KD (large) to KG-FiD-KD (large) improvements are only 0.5% and 0.2% for NQ and TriviaQA datasets, respectively. These look like smaller improvements as the difference between the EM scores of the same FiD models is often bigger than this, that is comparing original paper implementation vs when implemented by authors of this paper for FiD models. One idea is to show statistical significance of improvement brought by adding KG. For this, authors could do multiple runs of FiD-KD and KG-FiD-KD experiments with different seeds and show the mean and variance of EM obtained.\n\n2) Though FiD-KD was the best base model on which proposed methods were applied, no ablation studies were done on it. Ablation studies were all done on the inferior FiD models.\n\n3) Framing of the paper could be better. Novel methods proposed are on identifying the correct passages using which the reader (FiD or FiD-KD here) can answer the question. My understanding is that the proposed techniques can be applied on any reader model, not just on FiD. Giving a lot of importance to FiD, even having the name in the title of the paper takes the focus away to the FiD reader and not the novel ideas on the retriever side. In the writing, authors could reduce the use of term FiD and probably remove it from the title as well.\n\n4) The major difference between earlier KG guided ODQA methods with this paper is given in the related work section as “However, none of them studied applying KG to improve the FiD model”. This implies that most of the novelty is just mixing KG ideas exactly as it is from an already existing paper and using it with an FiD reader, which makes it sound less novel. I guess methods described in sections 3.2, 3.3, 3.4 are novel ideas even without the presence of the FiD reader (if that is not the case, papers which proposed these ideas should be cited here). If that is the case, these novelties should be highlighted in the writing. Especially, the sec 3.4 where a DPR and GAT are used together.\n\n5) One main argument in the paper is about increasing efficiency of the ODQA methods and paper claims to reduce computation cost by 40% compared to the base model. However, no analysis comparing inference time for FiD vs KG-FiD is provided. I guess, such analysis is necessary to back the claim of efficiency. \n\n",
            "summary_of_the_review": "check for the detailed review above.\n\nQuestion and suggestion to authors:\n- Equation (5) is cross entropy loss. Authors could probably use that term for clarity.\n- Probably a graph comparing the accuracy of the model with different numbers of passages passed to the reader would be a good addition. Similar to Figure 3 in FiD [1].\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper focus on the open domain question answering problem and propose an approach to rerank the retrieved documents through their relationship in the external knowledge graph. Specifically, they build on the state-of-the-art model of Fusion-in-Decoder (FiD) which encode the question-augmented documents individually and then use an decoder to generate the answer based on the concatenation of the embeddings of the retrieved documents. The authors argue that it is not effective and efficient for FiD to take all the retrieved documents (100 in their paper) as the input for the decoder. To improve the effectiveness, the authors propose to rerank the retrieved documents and select less documents as input for the decoder. They construct a graph among the retrieved documents by mapping the documents to the entity in Wikidata. Two documents will be connected if their mapped entities are connected in the Wikidata knowledge graph. Then they use Graph Neural Network to model the relationship of the retrieved documents and predict the reranking score from the representation after GNN. Fewer documents will be selected by the reranking model to be sent to the decoder. To improve the efficiency, they adopt the intermediate layer representations of FiD encoder so that they can stop encoding the abandoned documents afterwards. Through experiments, they demonstrated that their model (KD-FiD) which sends top 20 documents after reranking to the decoder can outperform FiD. They can achieved on-par performance as FiD with only 40% of its computation cost when using intermediate layer representation for reranking and selecting top 20 documents to finish the rest encoding. \n\nThe main contribution of this paper is that they propose the leverage the knowledge graph to rerank the retrieved documents and verified that sending fewer documents to the decoder can improve the performance.",
            "main_review": "Strengths:\n* They propose a knowledge graph based reranking approach and plug in the reranking in FiD to jointly optimize them.\n* The experiments demonstrated that they can achieve better performance when sending top documents after reranking to the decoder.\n* They show that they can achieve better efficiency when using the intermediate layer representation for reranking.\n\nWeaknesses:\n* They proposed a knowledge graph baed reranking approach and show that it can improve on the vanilla FiD. But they did not compare with other reranking approaches. We do not know how the proposed approach works compared with other reranking algorithms.\n* They key idea of this paper is to reduce the noise in the documents through the reranking model. But they did not show this explicitly in the paper. I think the author can add an experiments to compare the Hits@20 in the selected documents (20) for the decoder and Hits@20 after the retrieval to show that they can improve the quality of top 20 retrieved documents by using this reranking model.\n* The authors show that their can model can achieve on-par performance as FiD model with 40% computation cost. But this is a theoretical approximation and they did not count in the computation cost of the GNN. Another concern is the computation for the rest encoder layers have to wait for the results from GNN to decide which documents to abandon and which documents to continue encode. This sequential procedures may increase the computation time. I think the authors can list the running time of each model so that we can know how larges the model can reduce the training time when achieving on-par performance. ",
            "summary_of_the_review": "This paper proposed knowledge graph baed reranking algorithm and experiments show that it can improve over the state-of-the-art model of FiD. But they did not compare with other reranking models which makes it less convincing. They can add more analysis to show that the retrieved documents have better quality after reranking. Besides the theoretical approximation of the computation cost, it would be better to give the training time. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a graph-based reranking method for open-domain question answering using a knowledge graph (KG) aligned with the textual knowledge source (i.e. Wikidata and Wikipedia passages). Similar to some earlier work (Min et al., 2019), they build a passage graph from the aligned KG to compute passage scores for reranking using graph neural networks.\nThis idea can then be applied to various spots in the ODQA retriever-reader pipeline.\n\nIn the FiD reader, which typically reads up to 100 passages, their reranking method can be applied to select a subset of, say, 20 passages to be fed to the decoder. They show that this improves accuracy of the vanilla FiD model on two ODQA datasets, NQ and TriviaQA.\nFurthermore, they can use hidden representations from intermediate layers of the FiD encoder to compute the graph embeddings instead of the last layer, which can potentially save the computation from the remaining encoding layers of those unselected 80 passages.\n\nIn addition, the reranking method can also be applied to the retriever outputs. In vanilla FiD, the top-100 passages from the DPR retriever are fed to the FiD reader, but they propose to retrieve 1000 passages from DPR, and use their reranking method to select the top 100. This gives slight improvement to the final performance.\n\nThey also provide several ablation studies that are helpful for understanding the impact of various ingredients in their method, such as doing reranking in the reader vs. the retriever, doing reranking using representations from various layers, the helpfulness of the KG, etc.",
            "main_review": "Strengths:\n- The paper is clearly written, and the method is easy to follow.\n\n- Their reranking method achieves non-trivial performance gain over vanilla FiD.\n\n- They present an extensive set of ablation studies that are helpful for understanding the effectiveness of various parts of their method.\n\nWeaknesses:\n\ni) I'm not convinced by the effectiveness of the core ingredient of the proposed method, the KG. Table 4 shows that KG is not helpful at all when used on layer 9 and above of the encoder. While it does show improvements when using lower layers, Table 2 shows that using lower layers does not provide any gain over the vanilla FiD model. This in my mind significantly undermines the main claim of the paper. Reranking helps improve FiD accuracy, but not because of KG. KG may be able to help match the FiD accuracy while saving some computation, but I'm not convinced on the efficiency argument either, as detailed in iii).\n\nii) Following i), I think this paper lacks proper comparison with baseline reranking methods. Table 4 suggests that their KG-based reranking do not outperform a very simple MLP-based reranker using the same FiD encoder output. It's important to compare their KG-based reranking method with simpler approaches that only use the textual information for reranking, especially this KG-based approach is claimed as their main contribution.\nIn addition, for retriever reranking, a more proper reranker baseline would be a cross-encoder one that reads the question and passage at the same time, instead of a bi-encoder MLP.\n\niii) I also have some doubts on the efficiency argument. First of all, the cost of the GNN reranker is not factored in the computation at all. Also, I'm not totally sold on their approximation where the decoder cost is completely ignored. It would make much more sense to do an empirical study on reader latency, or a #FLOPs comparison on real data to make me comfortable with the 60% computation reduction claim. Intuitively, with the extra KG-reranking step, the reader latency would probably increase rather than decrease. Furthermore, for retriever ranking, how expensive it is to rerank 1000 passages? Any empirical results?\n\niv) Finally, a minor limitation of this work is its generality. Since the KG reranking approach does require a KG aligned with the textual knowledge source. In practice, this is unrealistic for anything other than Wikipedia. Using predicted entity-linking results can alleviate this problem, but I'm skeptical of the end-to-end performance if not using perfectly aligned KG.",
            "summary_of_the_review": "This paper proposes a KG-based reranking method for ODQA. The method is clearly presented and shows non-trivial gains over the baseline FiD model. However, I have several major concerns and questions about this paper, as detailed above. In particular, I'm not convinced by the helpfulness of their core ingredient, the KG. The paper also lacks proper comparison with simpler text-based baseline reranking methods that do not use KG. Lastly, the argument about efficiency is not backed by any empirical results.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}