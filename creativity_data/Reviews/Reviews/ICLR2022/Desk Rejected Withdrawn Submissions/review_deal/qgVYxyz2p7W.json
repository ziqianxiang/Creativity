{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes to add several loss terms to standard Semi-Supervised Learning methods to improve performance in Semi-Supervised Image Classification in the presence of ambiguous labels, so-called inter/intra-observer variability (IIV). Numerical results are provided on small datasets (Plankton, Turkey, Mice Bone, CIFAR10-H) reporting F1 score for images that are deemed unambiguous and reporting euclidean distance between cluster centroids for ambiguous images.\n\nThe proposed method combines:\n- Previous Semi-Supervised Learning losses from literature\n- Ambiguity loss, which uses pseudo-labels generated heuristically, assumes that some percentage of images in a batch are unreliable, and maximizes the cross-entropy of the ambiguity prediction with respect to this binary pseudo-label.\n- A contrastive-like term that makes two images in the same batch have different cluster predictions. Use the ground truth label if available, otherwise uses a pseudo-label predicted by the network to determine images from the same/different classes.\n- A consistency loss term that pushes the predictions of two views of the same image together.",
            "main_review": "Strengths:\nThis paper considers an interesting problem... how do you perform semi-supervised learning with unlabeled images that can have multiple possible labelings\n\nWeaknesses:\n- Motivation is not very clear to me. It is stated the existing semi-supervised methods cannot deal with unlabeled images that can have multiple possible labelings because these methods are trained on curated data. However, all of the provided examples of \"ambiguous\" unlabeled images (e.g., Figure 3) are actually obtained from curated datasets. In fact existing Semi-Supervised Learning methods already train on images with this IIV property.\n- Metrics are not very convincing. Accuracy on unambiguous images is never reported, instead only F1 score. On so-called ambiguous images, instead euclidean distance between clusterings is reported, which also does not directly correlate with the model's predictive accuracy. Not to mention that the distinction of ambiguous vs unambiguous images is somewhat arbitrary, e.g., using some random threshold on max confidence of the associated label to create a binary \"ambiguous\" or \"non-ambiguous\" label, when there are in fact many other approaches to evaluate classification accuracy, e.g., top-k metrics, especially on datasets like CIFAR10-H that provide soft labels for CIFAR-10 images.\n- Misses much of the literature. Miss comparison with state-of-the-art methods for semi-supervised image classification, such as PAWS [a], or even MPL [b] which already performs better than FixMatch. ​Also miss the wide literature on graph-based label propagation methods and methods for weakly-supervised learning, which are especially relevant to the considered IIV setting. Also miss numerous methods on simultaneous clustering and classification with neural networks. Arguably the most famous of which is DeepCluster [e] or even recent approaches such as NNCLR [f], SeLA [g], or Divide and Contrast [h].\n- Not clear how practical the proposed method is, since it requires knowledge (or heuristic pre-specification) of a so-called ambiguity parameter $p_A$ for each dataset, especially since ablations show that correct specification of this parameter improves performance on the proposed metrics.\n- Minor point, but the citations are not keeping with the ICLR format (I assume to save space).\n\n[a] Assran et al., Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples, ICCV, 2021.\n\n[b] Pham et al., Meta Pseudo Labels, CVPR, 2021.\n\n[c] Chen et al., Comparing the Value of Labeled and Unlabeled Data in Method-of-Moments Latent Variable Estimation, AISTATS, 2021.\n\n[d] Fu et al., Fast and Three-rious: Speeding Up Weak Supervision with Triplet Methods, ICML, 2020.\n\n[e] Caron et al., Deep clustering for unsupervised learning of visual features, ECCV, 2018.\n\n[f] Dwibedi et al., With a little help from my friends: Nearest-neighbor contrastive learning of visual representations, arXiv, 2021.\n\n[g] Assano et al., Self-labelling via simultaneous clustering and representation learning, ICLR 2020.\n\n[h] Tian et al., Divide and Contrast: Self-supervised Learning from Uncurated Data, arXiv 2021.\n",
            "summary_of_the_review": "In short, I see several issues with the paper that should be addressed in order for the paper to be published. As it stands, there is a lack of evidence in numerical results that existing methods cannot deal with IIV data and that the proposed method indeed improves upon the literature. This is due to a) non-standard evaluation metrics b) non-standard datasets c) missing comparison to state-of-the-art methods from the literature. Additionally the proposed method adds non-trivial complexity to existing semi-supervised methods and requires specification of an ambiguity parameter for  each dataset.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper the authors propose a method that identifies and clusters the ambiguous samples so that the accuracy of the semi-supervised methods will not be affected negatively due to the ambiguous data samples. The method is evaluated on different datasets with different semi-supervised learning methods. Some improvements are obtained in terms of F-1 score and another metric introduced by the authors.",
            "main_review": "The authors propose a method that simultaneously learns from the semi-supervised data and at the same time identifies the ambiguous samples and clusters them. \nThe strengths of the paper can be summarized as follows:\n1) The authors propose a new method that identifies and clusters the ambiguous samples.\n2) The method is tested on various datasets and effect of each loss term is empirically shown.\nThere are many problems with the proposed method. The weaknesses of the paper can be summarized as follows:\n1) The problems start with the title. Orthogonal term is a mathematical term and it is related to the perpendicularity of the vectors or functions. If two vectors are orthogonal, they are also linearly independent (In statistics, it is also used for independent variables). Therefore, using orthogonal term for the proposed method is not appropriate at all (as if trying to combine two unrelated things). A more suitable word may be complementary but not orthogonal.  \n2) In semi-supervised learning methods, there is a limited amount of labeled data and many unlabeled data. There are robust semi-supervised methods (such as [R1]) that handle noisy labels in the labeled data and these methods are not affected badly from the ambiguous samples. Therefore, the first sentence of the second paragraph at the introduction must be corrected.\n3) In semi-supervised learning methods the ultimate goal is to assign correct labels in the testing set. Therefore, we do not care about if there are ambiguous samples in the training set or how the ambiguous samples deviate from their corresponding cluster centers. Consequently, the accuracy metric defined in (4) is irrelevant for the semi-supervised learning accuracy. But, it can be given as side information. Also comparing other methods in terms of this metric is not fair since they do not incorporate the loss terms that targets clustering. Also, I wonder why the authors use F1-score instead of classical classification accuracies that are used for testing semi-supervised learning methods.\n4) There are too many parameters that must be fixed by the user, which makes the application of the method difficult.\n5) The first two figures are not helpful at all.\nOverall, I do not think that the contributions are significant and enough for publication. A simple method can be used for identifying ambiguous samples independently just before the semi-supervised learning if the main goal is to improve the accuracy of semi-supervised learning.\nReference:\n[R1] Cevikalp et al., “Semi-supervised robust deep neural networks for multi-label image classification,” Pattern Recognition, 2020.\n",
            "summary_of_the_review": "In summary, there are problems with the title, proposed method and evaluation metrics used for assessing the performance. The contribution is limited. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This manuscript proposes an algorithm which can be extended in many Semi-Supervised Learning cases. Through the usage of loss function, this method attempts to solve intra- or interobserver variability during the annotation.",
            "main_review": "Pros:\n1. The experiments part and the ablation study have shown their approach works well in some datasets and the effectiveness of their loss function.\n2. The design of loss function in the method section is detailed and reasonable.\nCons:\n1. The novelty of this manuscript is below the bar of ICLR. This paper just presents some loss functions and introduces the concept of ambiguousness. However, the classification $p_n$, the overclustering $p_o$ and the ambiguity estimation $p_a$ are generated by the network. This reviewer suggests that the authors may need to ensure that those variabilities have physical significance by using the statistical information or by other approaches. For example, dose the ambiguity estimation $p_a$ really represents the ambiguity estimation in physical world? And so as to the $p_n$ and $p_o$.\n2. The manuscript is unclear. The reviewer does not understand the orthogonal method means, where is the orthogonality in this manuscript? Meanwhile, the captions of figures have no benefit for understanding the figures, especially for figure 1. \n3. The mathematical formulation in this manuscript is not strict and there are some minor errors for the formulations. For examples, the authors use variable $x$ representation in the subsection definitions while x becomes ambiguous or certain in equation 2. In subsection datasets, ${a’}_i=1$ should be replaced with $a_{i’}$.\n\nQuestions:\n1. $P_A$ is the expected percentage of ambiguous images in the total dataset. So, how it can be assigned with 0.6 across all experiments and for different datasets in subsection implementation details? In fact, the estimation $P_A$ in CIFAR-10H is just 32% in table 1 and the result is badly in table 4 when $P_A$ equals 0.32.\n2. How does the hyper parameter $K^’$ effect the results and how to choose it?\n3. What’s the setting of batch size in equation 2 during the experiments? If the batch size is not large enough, the function $h(x)$ may cannot be set like this.",
            "summary_of_the_review": "This submission lacks of important evidence for its assumption and novelty.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed Semi-Supervised Classiﬁcation & Clustering (S2C2) to solve the problem of semi-supervised learning with ambiguous labels. S2C2 automatically estimates the ambiguity of an image and applies the respective SSL algorithm as a classiﬁcation to certainly labeled data while partitioning the ambiguous data into clusters of visually similar images. Experiments show this approach leads to better classiﬁcations and more compact clusters across multiple semi-supervised algorithms and non-curated datasets. ",
            "main_review": "pros:\n1. This paper conduct a lot of experiments to validate the effectiveness of S2C2.\n2. This framework is compatible with all kinds of existing semi-supervised learning methods. \n\ncons:\n1. I'm afraid the contribution is limited for ICLR. $L_A$ is a pseudo-label loss. $L_{CE^{-1}}$ is taken from [31]. $L_S$ is a common consistency loss. \n2. This paper is not well-written. First, this paper did not make clear the setting -- Semi-Supervised Learning on Ambiguous Labels. What is the problem with Ambiguous Labels in Semi-Supervised Learning and why do we study them? Is there any difference in training and evaluation protocol between this setting and vanilla Semi-Supervised Learning? Second, the method is not illustrated well. I think it would be better to emphasize the functionality of each loss or component instead of just presenting mathematical symbols.  \n3. $L_A$ depends solely on the prediction of $p_a(x)$ without any ground truth knowledge of which samples are ambiguous, how can this loss enable ambiguous recognition?\n4. Why perform clustering on ambiguous samples, can the authors give a more clear motivation?\n5. I'm confused about the evaluation metrics in 3.2. Why use weighted F1-score on certain data instead of accuracy, don't we want to acquire a classifier? Why is the ambiguity determined by the network instead of some ground truth? Why should we evaluate the clustering quality on ambiguous data, is there any benefit to the goal of this setting (also, the authors did not specify the goal)?\n6. This paper emphasizes that one main contribution is that this framework can be applied to many semi-supervised algorithms. This is good but I don't think this is a 'main' contribution. The main contribution should be how this framework can solve the ambiguous label problem. \n",
            "summary_of_the_review": "Due to the limitations on novelty, I suggest a rejection.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}