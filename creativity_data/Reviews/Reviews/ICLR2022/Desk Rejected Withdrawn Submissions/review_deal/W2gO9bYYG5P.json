{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper provides constructive proof that a single ViT layer with image patches as input can perform convolution operation, with the lower bound of the number of heads to express convolution. It also provides a two-phrase training strategy of ViT where ViT attention layers are initialized with convolutional layers weights according to the construction proof. ",
            "main_review": "Strengths:\n1.\tThe paper provides construction proof of express MHSA layer as conv layer in patch setting, an extension to prior work of Cordonnier’s paper of pixel level proof.\n2.\tThe paper introduces a two-phrase training / initiation strategy to inject convolution bias into ViT. \n\nWeaknesses:\n\nThe experiment part is kind of weak. Only experiments of CIFAR-100 are conducted and only DeiT-small/-base are compared.  \t\n1.\tFor comparison with DeiT, can you add DeiT variants with the same number of layers, heads, hidden dimension as CMHSA to do a fair comparison? DeiT with 12 layers performs worse than CMHSA with 6 layers on CIFAR-100 is as expected, thus not a convincing comparison.   \n2.\tIf possible, can you add CNN models to show if CMHSA would actually make ViT performs better/on-par with CNN models, which should be the ultimate goal of training ViT in low-data regime, otherwise one would pretrain ViT on large scale dataset or just use CNN models.  \n3.\tIf possible, results on ImageNet can be more convincing of the proposed method\n",
            "summary_of_the_review": "This paper provides theoretical part of construct conv layer from MSHA layer in patch setting, an extension to existing proof in pixel setting, however the proposed application of it, which is a two-phrase training strategy does not have adequate experiments support. Adding more comparison to prior models and possibly conduct experiments on ImageNet would make this paper more solid. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper aims to show that transformer layers can perform better than convolutions. The authors provide theoretically proof and analysis to support their argument. Based on the theoretical findings, they also a new two-stage training strategy to train better  vision transformers. Experiments on Cifar100 show that the proposed approach does perform better than previous vision transformer models.",
            "main_review": "Pros:\n\n- The topic described in this paper is interesting. Most previous works just show that vision transformers can performer better than those classic CNN models, like ResNet, ResNeXt, EfficientNet but none of them provide theoretical analysis to prove that transformers are better than convolutions. This paper shows that under the certain hypothesis described in P4, a MHSA layer can express any convolutional layer.\n\n- Experiments on Cifar100 show that the proposed two-stage training method works better  than previous ViT and DeiT models.\n\nCons:\n\n- The first concern is that though theoretical analysis is provided, there are no experiments demonstrating the conclusion from this paper. For example, does a 6-layer vision transformer performs better than a 6-layer CNN? I cannot find any experiments on this. I believe these kinds of experiments are essencial. After all, the whole paper talks about the battle of transformers and convolutions.\n\n- This paper only shows experiments on Cifar100. As described in the introduction section, ViTs do not perform well when taking small scale datasets, such as Cifar, as input. I think to better support the theoretical results, conducing experiments on ImageNet or at least a small set of ImageNet is necessary.\n\n- This paper lacks of essential experiments. For example, what does the proposed approach behave when the hidden dimension is larger? Also, would the performance keeps consistent when the layer number increases? Lots of necessary experiments are missing.",
            "summary_of_the_review": "The starting point of this paper is good. They provide theoretical analysis to demonstrate self-attention with relative positional encoding can act as any convolutions. However, the experiments are thin and they cannot support a top-level conference paper like ICLR.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper explores the relationship between convolution and self-attention. The paper extends the pixel-input setting of (Cordonnier et al., 2020) to the patch-input setting and proposes a two-phase training pipeline following (D’Ascoli et al., 2021) based on the patch-wise input. Experiments show that the two-phase training pipeline can achieve better performance compared with models trained with random initialization on CIFAR-100.",
            "main_review": "Novelty is very incremental and most of the theorems, propositions and empirical observations have already been proposed in (Cordonnier et al., 2020, D’Ascoli et al., 2021). In particular, the theorem and the proof of the pixel-input setting in Sec. 3.1 are the same as (Cordonnier et al., 2020). The two-phase training pipeline is directly inherited from (D’Ascoli et al., 2021) to insert the convolutional inductive bias. The empirical observation that the injected convolutional bias makes the optimization process easier is also proposed in (D’Ascoli et al., 2021). \n\t\t\t\t\n\nThe main contribution has technical errors. In particular, the main contribution of this paper shows that a 9-head self-attention layer can express any convolutional layer with patch-wise input. Obviously, this is wrong. For example, we set patch size P= 1 and kernel size K=5. According to (Cordonnier et al., 2020) and Theorem 5, at least 25 heads are sufficient to express a 5x5 convolutional layer. The error source is in Sec. A.3, where the authors make an unreliable assumption that “In the convolutional layer, only pixels in the 9 neighboring patches (including the center patch itself) can be relevant, since P > K”, which is clearly not convincing.\n\nI think the proposed patch-wise setting (assume it holds) only applies to the first self-attention layer. For the intermediate layers, we treat each position as a ‘pixel’ and it does not make sense to apply ‘patch-wise’ input for the next self-attention layer. Therefore only pixel-input setting becomes practical in this sense.\n\nAnother problem, in conventional ViTs, each patch (e.g., with 16x16 pixels) is first projected to an embedding vector as input, then we can easily apply the ‘pixel’-input setting. It is not clear to me what’s the advantage for applying the patch-input setting. No experimental results regarding pixel vs. patch input are available. \n\nExperimental baselines are not strong enough. The paper should also compare with recent vision Transformers like Swin-Transformer. The paper must provide direct comparison with ConViT to show the benefits of patch input. \n\nNo experiments on large-scale ImageNet dataset to justify the effectiveness of the proposed method. Only results in low data regimes on CIFAR-100 are provided.\n\nSome statements are confusing. \n- The statement that ‘they only initialize the attention module to express a random convolutions, while our method explicitly transfers information from a well-learned CNN into a ViT’ is not accurate. Actually, in ConViT, they use the pre-trained convolutional parameters to initialize the positional term. \n- The description “Let the receptive field of a given patch in K × K convolution be the set of patches that contain at least one pixel in the receptive field of any pixel in the given patch” is hard to understand. \n",
            "summary_of_the_review": "The new technical novelty is incremental and the main claim of the paper is incorrect. Experiment is weak due to lack of key baselines and large-scale benchmarks. Many arguments are inaccurate. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}