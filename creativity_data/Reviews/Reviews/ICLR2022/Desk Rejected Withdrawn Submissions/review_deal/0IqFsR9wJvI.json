{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper focuses on learning node representations for continuous-time dynamic graphs. The overall approach follows the setup in recently proposed works where the state of nodes involved in an event are updated after each event and most recent node states (representations) are used for prediction and classification tasks. The node representations are updated using its previous state, features of the edge representing the event, random Fourier temporal features and its temporal neighborhood information. The key contribution of this paper is the proposal a simpler computation of the neighborhood information in an online manner for replacing the more expensive attention and message passing based techniques used in previous works. To achieve this, the proposed technique maintains two variables a_u and b_u, that account for the most recent summary of neighborhood information, decayed linearly with respect to the number of events. The authors demonstrate substantial training speedup over several state-of-art methods and also showcase slightly better or comparable performance to representative baselines for the task of link prediction and node classification. \n",
            "main_review": "Strengths:\n\n- The technique proposed to compute the neighborhood representation online is simple and intuitive.\n- The speedup advantage during training is substantial and noticeable.\n- The experiments are performed across variety of datasets and compared to state-of-art baselines along with various ablations which is a strong point of the paper.\n\nWeaknesses:\n\n- The overall approach and setup is not novel and follows various recent works sighted by the authors which makes the key contribution on the method to update neighborhood crucial. While the technique does produce speedup, it may not be generalizable to different domains. For instance, the approach assumes that importance decay of each neighbor is linear in terms of number of events on that node however, this may not be true in many cases. For instance, many edge types may have longer effect (e.g. share) vs others (e.g. like). In this scenario, the current approach would fail to correctly weight the neighbors. \n- Further, the key benefit of using message passing scheme is to use the structural information effectively and as shown by authors own experiments on graphs with no edge information, the approach does not effectively account for structural information which limits its applicability.\n- If I understand correctly, proposition 1 only holds in case linear decay and hence the approach cannot be easily extended to nonlinear temporal dependencies. Related to that, the ablation study on time information is not clear to me. The results for no time vs original time are quite mixed and it is not clear why authors conclude that this means the only required information from time is order of events.\n- Authors talk about node addition and deletion being represented as event. However, it doesn’t seem like the approach can handle node deletion. Deleting a node should have effect on its previous neighbors but there is no way to handle this in the current approach. \n",
            "summary_of_the_review": "The proposed approach is a useful update to  the already existing frameworks for learning representations over dynamic graphs and will be of interest to practitioners. However, the overall contribution of the paper is weak and also the key contribution of removing the bottleneck on computing neighborhood representation may have severe limitations when it comes to graphs with no edge features or graphs where dependence on events may not decay linearly over time as described above. Hence I believe this contribution is marginally below the acceptance threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper considers inference tasks over graphs, where the graph is dynamically evolving with time. The evolution happens in small increments where at each increment event either an edge is added or removed from the graph (though much of the discussion in the paper seems to be focused on only edge addition events). Under this setting it proposes an online graph net architecture, by which a node’s state is dynamically updated each time an edge event involving the node occurs. A key contribution in this architecture is the lack of neighborhood sampling for updating node states. It instead proposes using a hidden node state that effectively holds a summary of all updates involving the node that have occurred thus far, with preference given to recent events. As a result, the proposed method is much faster compared to baseline while providing comparable empirical performance.  ",
            "main_review": "The paper is reasonably well-written, addresses an interesting problem, and proposes a simple and effective solution. I enjoyed reading it. However, I found there are some important aspects that have been omitted in the exposition. \n- It is easy to see that OGNs would be faster compared to baselines that consider neighborhood information for making predictions. However (as has been noted in Section 6.2), OGNs risk not using graph structural information which might be harmful for certain problems. For e.g., if an edge event (u, v) happens, while r_u and r_v are updated, the states of the 2-hop and 3-hop neighbors of u, v are not updated. Moreover, if during prediction time only the local states s_u, s_v are used then it the graph information is not being used at all. It is surprising how the method is able to perform competitively given this potentially serious drawback. Are the choice of experiments such that the graph information is not that useful for predictions? \n- A discussion on how OGNs are trained (and backpropagated) is missing. For instance, it is unclear whether each training sample corresponds to a distinct temporal sequence of edge additions starting from an initial graph, or whether training can be performed “online” over a single (long) sequence of edge additions/removals. In Fig. 4 does time refer to training time or testing time?  \n- How are edge removals handled by OGN? How are node additions/removal handled? \n- In Equation 5, the righthand side does not have h_u(t). Is this a typo? \n- Equation 6 is designed in a way that older events are forgotten compared to recent events. However, for some applications an old event might contain information that is relevant long after the event has happened. How does OGN deal with such cases? Have you considered using a GRU (for example) to update r_u? \n",
            "summary_of_the_review": "Reasonable well-written paper that proposes a simple, effective solution to learning over temporally changing graphs. Some crucial aspects are missing from the discussion. It would make the paper stronger, if those points are addressed. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes Online Graph Network, a graph neural network for fast learning on temporal graphs.\nThe proposed method aims to overcome the computational burden of performing neighbor sampling and message passing whenever a prediction for a node is required. Instead, the authors propose a novel way to aggregate efficiently the neighbor representations continuously in an online manner. The idea is to keep a constant latent representation for the neighborhood of each node by adding to it the temporal representation of each neighbor that takes part in any past event. This means that the representations are updated in constant time as the events progress. The addition of a neighbor's representation is weighted based on an exponential decay term that is reversely analogous to the time between the current representation and the time of the neighbor's event.\nEmpirical results on four well-known datasets are performed to show the superiority in accuracy and efficiency of the proposed approach compared with previous methods for link prediction, and two datasets for node classification.\n",
            "main_review": "Pros: \n\n+ The motivation to increase the scalability and efficiency of temporal graphs is clear, as many real-world temporal graphs are very large. \n\n+ The idea is straight forward, which renders its adaptation easier and more interpretable for real world applications.\n\n+ The experimental section and the ablation studies are well written and sufficiently explanatory.\n\n\nCons: \n- Since the main task of the model is to focus on the efficiency for real world applications, there should be a comparison for the inference time as well. Training time is indeed helpful but it is typically run once every certain period. Inference time is of equal importance in real applications that call for fast predictions. That would clarify the benefit of OGN over other baselines. Ideally, the model would benefit by running experiments with actual large graphs, as the largest right now is only 11000 nodes, however, I understand that due to the lack of such datasets this might be a hard argument to form.\n\n- The paper could greatly benefit from expanding the experimental part. The section for node classification relies solely on two datasets, which lack node attributes and the advantages of OGN are not clear. To justify the use of OGN in this context, the authors could compare with numerous methods and datasets from the literature of temporal graph learning that can be found in the literature [1,2,3]. If OGN is useful specifically for interaction prediction on temporal graphs with no node attributes and edge attributes, it should be stated in the abstract.\n\n[1] Diffusion convolutional recurrent neural network: Data-driven traffic forecasting\n[2] PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models\n[3] GEM: A Python Package for Graph Embedding Methods\n\n- As another attempt to increase the overall body of work,  the authors could expand the methods traditionally used for evaluation of interaction prediction in previous approaches such as Rossi et al. 2020.  To my understanding the model is evaluated by predicting the probability of an edge at a given time and contrast it with the probability of a random edge. Given that the random edge might be very hard to actually exist, there are certain easy heuristics that can be used as baselines, for example, predict the edge with the highest number of common neighbors or common neighbors in the past t events.  Alternatively, one could draw a negative edge between nodes that actually have edges but in previous or future time steps, instead of totally random.\n\n-The included code contains instructions for wikipedia and reddit but not for the rest of the data. \n\n\nComments: \n* Regarding notations, in equations 1-3 it is not clear whether the L samplings are performed before the first aggregation layer or each sampling is performed right before the respective aggregation layer. \n\n* Figure 2 could benefit from depicting the computation of \\hat{G} along with the already present G.\n\n*It is not clear whether there is an argument against the use of the original timestamps. Specifically the authors rely solely on the order of the events or U-time, a uniform time difference. However the time difference between events can be important i.e. if the difference between two events is 1 week or 1 month, we would expect the prediction to be affected since it relies on aggregating the past events. If this is not the case it could be an interesting observation and should be clarified.\n\n*Is it possible to connect the proposed methodology with any theoretical findings from the literature of streaming algorithms? \n",
            "summary_of_the_review": "As the authors contend in the introduction the proposed method, though insightful, is quite simple, meaning that it has to indicate either a significant advantage over extended experiments or some theoretical justification, in order to not be considered a marginal contribution. The experiments indicate an acceleration in 3 out of 4 datasets and an improvement in precision in (different) 3 out of 4. The existence of edge features (as noted in the limitations) is catalytic, as the appendix experiments indicate that OGN performs worse than the benchmark in 3 datasets without edge features. With that in mind, I have the above comments to further justify OGN.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new approach for learning on dynamic graphs, which avoids the need of convolution for aggregating information from past interactions by maintaining a summary (of the interactions) that is updated in an online fashion with a fixed and scalable aggregation process (i.e. a weighted average based on a softmax kernel that decays linearly wrt the number of events). Thanks to the recursive formulation that the selected aggregation process admits (which allows to update the summary of a node’s neighborhood whenever this is involved in a new event), the proposed approach shows a O(1) complexity both in terms of memory and number of operations for each update and inference step. Experimental evaluations on Reddit, Wikipedia, MOOC and Twitter dataset for link prediction and Wikipedia and Reddit for node classification show the good performance of the proposed solution, which performs on par with previous approaches in most of the considered scenarios (and in some even better) while requiring a fraction of the computational cost. ",
            "main_review": "Overall the paper is rather well written, it is easy to follow and the main idea is clear and generally well explained. While the paper achieves good performance on the considered tasks, I believe the authors slightly misunderstood the formulation of TGN and proposed in their paper a simplified version of that architecture, which however ends up performing better than what presented in Rossi et al. Indeed, at the end of section 2 the authors state:\n\nIn the l-th SAMPLE function, TGN selects the last n nodes that have interacted with the leaves of the graph G_u^{(l-1)}. Subsequently, TGN encodes the timestamps with random Fourier features (Rahimi & Recht, 2008) and uses temporal graph attention for AGGREGATE. The output of the last aggregation layer is a single node, and READOUT returns its feature vector. Finally, TGN uses the identity function for COMBINE and a GRU for UPDATE.\n\nHowever, TGN doesn’t use any graph convolutional layer for determining the update of the memory cell, rather it exchanges messages across nodes that are involved in each interaction and updates the memory cell of each node based on an aggregated version of these messages (similarly to what is done in the paper with the computation of the summary of the neighbors state, just aggregating the messages that appear only in the last batch instead of the entire past history of a node). A graph convolutional layer is used only on top of these cells during inference to avoid the stainless of the memory and make a prediction for each node based on the latest state of the neighborhood (which might have evolved indeed from the last time the target node u was involved in an interaction - e.g. if a neighbor was interacting with a node different then u) - see Section 3.1 embedding of TGN’s paper for reference.\n\nFrom this perspective the approach proposed by the authors appears as a simplified version of TGN where:\n\n1) The GRU is replaced with the attention kernel proposed in the paper (the node state s and neighbrohood summary r can be considered as an analogous version of the GRU memory cells in this scenario, where the process for updating r is fixed by the attention kernel instead of being learnt by the GRU)\n\n2) The graph convolutional layer applied on top of the memory cells for producing the output is simply replaced by a MLP (which would correspond to the identity embedding presented in TGN).\n\nWhile the contribution of the paper appears thus incremental wrt what already proposed in TGN, I believe the approach is generally interesting and corresponds to a simple yet effective architecture for inference on dynamic graphs. The experiments show in this sense good performance wrt prior art and highlight whether some components (e.g. the graph convolutional layers realized in TGN or the attention layer used in TGAT) might actually not be needed for achieving good performance at least on the considered datasets. \n\nOverall, I would thus consider the paper a weak accept for ICLR and I would kindly ask the authors to reply to my points on the comparison with TGN in their rebuttal and if agreeing with me on the comparison to adjust the text of the paper to reflect the real contribution of the manuscript wrt prior art.\n",
            "summary_of_the_review": "While not presenting significant technical novelty, the paper presents some interesting insights for realizing a simple yet scalable architecture for inference on dynamic graphs. Experimental evaluation of the paper is overall sound and highlights well the benefit of the approach. The text needs refactoring to highlight the real contribution of the paper wrt prior art.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}