{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper introduces a new technique to support the release of \"sanitized\" data to an untrusted adversary, called Sanitizer. The adversary is assumed to receive a small dataset of non-sanitized data, as well as a sanitized dataset on which it tries to infer a sensitive attribute. The data is assumed to consist of a part $x$ which serves as the input for prediction tasks (in the paper $x$ consists of human face photos), and a part $y=(y_S, y_{NS})$ consisting of a list of discrete class labels, some of which are deemed sensitive ($y_S$) while the rest is deemed non-sensitive ($y_{NS}$). One of the non-sensitive labels is designated as a \"utility\" label.\nThe goal of the technique is to keep both utility (the predictive accuracy of a utility label predictor) high and information leakage (predictive accuracy of the sensitive labels) low. It also aims at maintaining the marginal distribution of the sensitive attributes\nThe paper works from a series of probabilistic requirements on utility and information leakage of the Sanitizer outputs to a composite training loss, building on a VAE architecture to express some constraints in latent space. It demonstrates the performance of the proposed architecture through experiments, including analytic experiments such as ablation studies. ",
            "main_review": "The technical approach to the proposed task is interesting and rests on well-founded technical choices; for instance the choice of a VAE architecture and the problem formulation in terms of desiderata sec3 are convincing, as well as the loss architecture adopted to respond to the desiderata in eq5. I am unclear whether the task formulation allows for the adversary to predict the sensitive attribute from all disclosed data, i.e. using not only the $x$ part, but also the entire $y$ part; I would think that this would be a reasonable threat model. \n\nThe performance of the proposed algorithm looks promisingly good in table 1, so my overall recommendation is to drastically rework the paper and resubmit, because it looks like the proposed approach has merits.\n\nThe exposition of the algorithm's task, of the threat model, of the type of data under consideration, of the expected properties of the sanitization operation, is poor overall. Some key concepts such are not formally defined until very late in the paper, such as information leakage, the fact that attributes are meant to be categorical only, \"utility attribute\" (not really defined anywhere, and sec1 \"support a specific utility attribute like identity\" is not helpful to understand what is meant), the exact threat model. The most confusing problem concerns the definition of the task. The description I have put together in the previous \"Summary Of The Paper\" section reflects my own understanding, reconstructed by guesswork and piecing together information from the experiment description, but the paper leaves me still unsure of specifics.\nThe paper starts with a generic, moderately informative, sometimes cryptic introduction. Sec3 raises hopes to formalize the task but leaves so many aspects ambiguous until the experiment description. I suggest starting with a motivating example to improve the exposition and introduce key aspects of the setup under consideration much earlier in the paper, possibly in sec1. \n\nThe paper's English writing has an average of one grammar mistake, typo, or garbled sentence every few lines. Most of these can be repaired, but some of these errors get squarely in the way of understanding some key parts of the work. The writing is generally sloppy and has numerous inconsistencies which make understanding harder. Examples follow, but I have given up collecting errors exhaustively after the first page.\n- \"We note that while this has resulted in some relevant work in privacy, there are is primarily restricted to faces and methods are designed with objective to support a specific utility attribute like identity.\"\n- sec3, top half, has repetitions and the writing flow is unclear; \"To obtain the property P2\" might be \"... P1\"\n- last line of sec3: \"while learning a classifier for the sensitive attribute\" might be \"...utility attribute...\" ?\n- \"by applying baselines on existing datasets and baselines\"\n- fig2 caption is missing 3 symbols\n- is a \"privatized\" dataset the same as a \"sanitized representation\", i.e. the output of the Sanitizer?\n- \"We report performance using Privacy-Utility tradeoff which compares the capability of an adversary to correctly infer sensitive information from the sanitized representation which is concurrently used by a user to infer task information\": what is \"task information\" in this context?\n- experiment descriptions E1, E2, E3 are not referenced anywhere after they are defined sec4.3\n- the rows table 2, table 1 do not correspond to the baselines defined sec4.2 (reader has to guess which is which)\n- table 3 titles is unclear: \"CAS on sensitive attribute estimation as utility\"\n- sec6 and fig3, based on circumstantial indications, I am guessing that information leakage is formally defined as the predictive accuracy of some predictor $x \\mapsto$ sensitive attribute, but this is such an important indication that it needs to be defined early on in the paper\n\nThe experimental setup is too unclear to let me assess its soundness. Which of the schemes sec3.1 is used in table 1? In table 3, how is the trade-off point selected? Where are results of E2 and E3? How are the ablations sec6 conducted: are functions set to identity, or are $\\alpha$ hyperparameters set to 0 ? Fig3, what does solid vs dotted line style mean? What is the (neural architecture of) parameterized functions?\n\nThe experiments do not support some of the claims of the paper. There is not demonstration on other input data than images. No experiments has more than one sensitive and one utility attribute, while the formalism always considers sets of attributes. The claim of task independence is not demonstrated; changing the attribute does not qualify as task independence in my opinion. Sec4.1 the claim of a \ncontinuously improving evaluation pipeline is unsubstantiated.",
            "summary_of_the_review": "The paper contains promising ideas, the implementation seems good and innovative, and most importantly performance seems superior to competitor methods. The experimental methodology is sound but experiments do not address the paper's claims. Overall, the paper's lack of clarity penalizes my assessment both directly and indirectly, as it leaves many aspects ambiguous.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Responsible research practice (e.g., human subjects, data release)"
            ],
            "details_of_ethics_concerns": "The authors announce release of sanitized data of already public data; it should make sure that original data has any ethics concerns, typically that face images have been collected with the subjects' consent.",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a data sanitization technique that i) splits the data into sensitive and non-sensitive components ii) replaces sensitive features with systematically sampled synthetic data.  \n",
            "main_review": "+ The idea of semantically splitting the data for sanitizing is neat.\n+ The paper is generally structured well and easy to follow.\n\nComments\n1. My main concern with the paper is the lack of formal privacy analysis. In the post DP world it is absolutely imperative to have a formal analysis of the leakage - the paper currently presents an empirical estimate as a proxy for privacy which is unfortunately inadequate. \n\n2. The paper incorrectly claims prior work has not addressed attribute DP or context aware DP.\nattribute DP- [ZOC20]\ncontext-aware DP - [ABKRS20], [CTC21], [MK19], \n3. Gender is used as both a  sensitive and utility attribute for the experiments - why this disparity?\n\n4. Typos\ni) protblem --> problem Pg 3\nii) In this work --> In this work, Pg 1\niii) The caption of Fig. 2 is ill-formed.\niv)\"Our mechanisms apply transformations that results a new synthetic ...\" --> results in a new\n\n\n[ZOC20]-Attribute Privacy: Framework and Mechanisms\n[ABKRS20]- Context-Aware Local Differential Privacy\n[CTC21]-Task-aware Privacy Preservation for Multi-dimensional Data\n[MK19]-Utility-Optimized Local Differential Privacy Mechanisms for Distribution Estimation\n",
            "summary_of_the_review": "The paper suffers from a fundamental flaw - only an empirical notion of leakage is given. As such, the paper cannot be accepted without a formal analysis.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a system to transform a dataset into a \"sanitized\" version such that useful models can still be trained on the sanitized dataset, but sensitive information is removed from each sample.\nThe system uses a VAE trained with different regularization terms that aim to disentangle sensitive attributes from non-sensitive ones, so that the former can then be removed.",
            "main_review": "This paper falls into a long line of work from both the fairness and privacy community that aims to remove pre-defined sensitive attributes from data (or from learned models or representations).\nThe approach taken in this paper is similar to many prior works, in that it learns to encode the dataset under various regularization constraints that aim to enforce that sensitive attributes are removed.\nIt would thus be helpful for the paper to more precisely explain how its techniques differ from those already present in the literature.\n\nSince the goal of this paper is to provide *empirical* privacy (i.e., there is no *proof* of any formal privacy guarantee), I would expect a lot more empirical evaluation of these privacy guarantees. In the current paper, the only privacy evaluation appears to be to report the average success rate of one attack in Section 4.3. As there are few details on what this attack actually does, it is hard to get a sense of what the privacy guarantees really are.\n\nAs with many prior papers, this work thus also provides a qualitative demonstration of privacy in Figure 4 by showing various faces with noise. While such figures clearly show that the faces change between different settings, it is very hard to even qualitatively assess what kind of privacy is really gained here. E.g., maybe the original face is still encoded in the low-order bits of the image? (probably not, but this is just an example to illustrate that qualitative assessments of privacy are at best useless and at worst misleading).\n\nAt a more fundamental level, I think that the type of approach used in this paper---which was originally proposed in the fairness literature---doesn't really make sense from a privacy perspective. Indeed, the explicit goal of the framework---both in its design and evaluation---is to reduce information leakage *on average*. Unfortunately, this does not preclude that the system might completely fail to protect the privacy of some users. For example, the sanitization process might completely destroy the sensitive information of 99% of users, but output the exact data of the remaining 1%. Such a system would be close to optimal according to this paper's goal, while being clearly not-private.\n\nA more convincing empirical evaluation of such a system would thus have to make a convincing case that *no* attack can recover sensitive information from *any* user.",
            "summary_of_the_review": "An empirical data sanitization procedure, the privacy of which is hard to assess.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a modified loss function for training a variational auto-encoder (VAE) model with the aim of releasing (partly) sensitive data in sanitized form. The method is based on using a composite loss, where different components try to enforce separate desiderata that the model should have.",
            "main_review": "Strong points:\n* The proposed method seems to work better than the existing ones.\n\nWeak points:\n* The proposed method has no formal guarantees that it will not leak any and all information in the sensitive data; as the method is based on training a DNN with a composite loss balancing different objectives, it seems like a hard problem to try and verify what information the model might actually leak.\n\n* Given that the method has no formal guarantees and only relies on empirical results, the Experiments section feels quite light (see below for specific comments regarding this).\n\nQuestions or comments for the authors:\n1) As noted in the experiments, the choices of sensitive and utility features are arbitrary for the datasets used. Have you tested different combinations than the single (?) ones presented in the paper, are the results similar? How does the model behave with multidimensional sensitive or utility targets.\n\n2) Please add some measure of variability in the test results.\n\n3) Can you clarify what do you mean by a \"semantic concept\" or \"semantic concept in latent space\" (e.g. p2)? What does it mean that the model can semantically decouple sensitive and non-sensitive representation (p2), do you mean that the encodings of the features of y_ns and y_s are (hopefully) uncorrelated or independent, or something else?\n\n4) On a related note, can you specifify what do you mean by a structured representation (why is the encoding of X structured, or did I misunderstand [e.g., \"our mechanisms apply transformation over (structured and low-dimensional) feature representation\" on p2]?)\n\n5) On p4 you state that you do not use information theoretic leakage measures due to high-dimensionality. However, if I understood the experiments correctly, you are using 1d sensitive attributes through-out. Is there some reason not to use measures from information theory here?\n\n6) Regarding h_v, on p5 you state that  the \"adversarial regularizer does not significantly impact the leakage\". Looking at the ablation study on p9 it seems that h_v has the second largest effect a bit behind dcorr. Did I misunderstood, is there a typo somewhere, or what happens here?\n\n7) Regarding Figure 4, you state that from this it should be concluded that \"our sanitizer protects image semantics while obfuscating the sensitive information\" (p9). I am not at all convinced that you should conclude anything like that based on a single example image. You can show counter-examples to providing (perfect) privacy, but cannot argue that some random example shows that your method works as claimed. I would maintain that since the other results indicate that your method does leak information on the sensitive attributes, you should also be able to pick an example where your method fails to provide privacy.\n\nSome less important questions and comments:\n8) In Figure 3 caption you state that your sanitizer \"performs better than all existing methods\". Looking at Figure 3b), it seems your method is below the TIRPDC line for some time. Can you comment?\n\n9) On p2 you state that the proposed method \"can shift the privacy-utility frontier\". What does this mean? Are you just saying that your method works better than the existing methods, or suggesting that you can shift e.g. some Pareto optimal privacy-utility frontier somehow? How does that work?\n\n10) On p2 you state that existing methods based on learning a generative model \"are unable to preserve any non-sensitive information for training datapoints since they re-sample from the distribution.\" Why could not generative models learn a joint model over non-sensitive and ssensitive data and preserve some non-sensitive information? Or are you talking specifically of the models mentioned above? Do those methods consider a different problem (e.g. releasing sensitive data without any non-sensitive attributes) or are they simply bad methods?\n\n11) On p3 you state that \"DP provides uniform protection to all sensitive and non-sensitive attributes, significantly hampering utility of transformed input.\" Why would you provide DP guarantees on a non-sensitive dataset, or are you saying that the existing methods do not consider combining non-sensitive (i.e., public) and sensitive data?\n\n12) There seems to be several missing symbols in Figure 2 caption.",
            "summary_of_the_review": "Overall, I think the paper needs to be clarified further, and I have several questions for the authors before I can recommend acceptance.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}