{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work introduces a metric learning method to assess the similarity of simulated patches of protein and lipid distribution in human cell membranes to be used as part of multiscale simulation experiments for studying RAS-RAF protein interactions. \n\nThe metric learning process itself is based on combining a 3 key loss functions: a label loss to identify the combination of RAS and RAF proteins present; a transformation loss to encode invariance to rotations as multiples of $\\pi / 2$ as well as horizontal and vertical flips; and, finally, a spatial patten loss which compares the radial maxima across each lipid channel. Each of the aforementioned loss functions are implemented as a triplet loss, and combined using a weighted sum. The model hyperparameters (including loss function component weights and the margins for the triplet loss) are selected by evaluating the model performance across three metrics and the representation dimensionality on _pre-campaign_ data to select the best model for use in the full simulation experiment. \n\nThe model is evaluated with respect to three key metrics each based on one of the loss function components, with the learnt model demonstrating that it is able produce a metric function which simultaneously minimises the three potentially conflicting constraints. \n\nAn evaluation of the metrics utility is also performed by looking at its ability to assess patch differences over spatial and temporal changes in the _campaign data_ as generated from the full simulation experiment. \n",
            "main_review": "The paper poses an interesting and challenging problem, where a metric function must be learnt using a small sample of _pre-campaign_ data for use as part of an experiment which is too computationally extensive to be repeated. As the authors highlight, this setting provides challenging circumstances in which to design an algorithm that generalises from the pre-campaign to the campaign data, as well as to assess the efficacy of the algorithm when the campaign experiment is too computationally expensive to be repeated. \n\nThe methodology that is introduced in the paper is well motivated and adapted to the problem domain. The technical novelty is reasonably limited with the label and transformation losses being reasonably common concepts in the existing literature. The radial loss does represent a novel element, although further justification could be given towards the motivation of specific choices made in the design of this aspect. \n\nTo overcome one of the key challenges of this problem – that of a lack of ground truth to compare the learned metric function against – the authors assess how the metric function results on mean lipid concentration vary between the pre-campaign and campaign data and the metrics sensitivity to changes in the spatiotemporal location of patches (fig. 6 & 7). This data shows that the metric has desirable properties. \n\nDespite this, there’s no real evidence presented in this work to suggest that the proposed metric has achieved its overall aim – allowing the campaign experiment to select a better set of patches when compared to a naive approach. Whilst the authors point out the challenges of performing this with such a time-consuming experiment, it would be beneficial to see some attempts at performing this (I.e. in a smaller scale simulation). One such possibility could be to expand the experiments performed in Figures 6 & 7 to include naive methods as comparison (such as the individual loss components). Here we would hope to see that the combined metric is able to represent the spatiotemporal changes where as other naive methods can’t. \n\nAs a minor point the mathematical notation for the radial loss could be clarified (i.e. what is the $\\mathrm{max}(\\dot)$ evaluated over on pg. 6?\n",
            "summary_of_the_review": "- Well motivated and interesting problem with challenges well laid out. \n- The proposed methodology is appropriate although only has limited novelty from a metric learning perspective.\n- Experiments demonstrate that the metric learning process itself works, but there is limited evidence to show that it has achieved its overall aim by allowing improvements in the overall simulation experiment which it is integrated into.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The specific problem is to study the signaling process of proteins embedded in the cell membrane. For this, a large number of simulations are generated with a wide range of membrane configurations. These simulated data are “patches” of multichannel images. The problem requires selecting a diverse set of patches to explore. Therefore, a metric to quantify similarities between patches are needed.\n\nThe previous approach to quantify the similarity between patches relied on well-defined norms. The drawback of this approach is that it does not correspond to the biological understanding of domain exporters about similarity. \n\nThe paper proposes a metric learning approach that takes domain knowledge into the design. The proposed loss function has three components: 1. Invariance loss, which is a triplet loss on transformation invariance of the patches. The transformations include rotations, horizontal and vertical reflections. 2. Feature proportionality loss, which is to enforce the radial profile (spatial arrangement of lipids) being consistent in the metric space. 3. Supervised classification of protein constellations (number of RAS and RAF proteins). This is formulated as a triplet loss to encourage patches with the same protein constellations being similar. The three losses are combined with weights (selected as hyperparameters). The model is a standard CNN model.\n\nIn the experiments, the paper shows that the learned metrics indeed separate patches with different protein constellations, being invariant to transformations and preserving lipid profile similarities. In addition, the metric is robust under distribution shifts and learns temporal relationships from statistical mechanics.",
            "main_review": "Strength: The proposed approach follows domain knowledge about the data. It does seem to work as designed.\n\nWeakness: The proposed approach is very specific to a narrow problem. The overall writing of the paper is also hard to follow. Therefore, it is very hard to judge the significance. \n\nDetailed comments:\n\n1. The performances metrics in Table 1 need some uncertainty estimates, through different model initializations or bootstrapped data if the model is inexpensive to train. \n2. Why are some AUC, in particular for “Standard” methods being worse than random? \n3. More clarification about the significance of this paper is needed. What scientific problems can now be better tackled with this approach compared with standard approaches? If the approach helped select a diverse set of patches, did this help study RAS signaling? If so, is there a way to quantify the contribution? ",
            "summary_of_the_review": "Overall I found the work is useful for the specific scientific problem. However the application scope is too narrow, the significance of this work is also not well communicated. I feel the paper is better placed in a biophysics venue. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors aim to develop a biologically inspired similarity metric to quantitatively measure the configuration of protein-lipid complexes which are relevant to cancer signaling. However, the current manuscript did not convince me that the current method could provide a meaningful quantification. ",
            "main_review": "Main review\nOverall, the current manuscript needs a significant amount of improvement in both writing and science. \n1.\tThe writing of the current manuscript lacks logic. The authors tended to use lots of unnecessary terminologies, introduce irrelevant concepts, be inconsistent in defining keywords, and make unsupported arguments. I’m not going to waste my time to point out all writing issues, just a few examples. (1) The authors provided multiple inconsistent definitions of “patches”, an important key concept, throughout the manuscript. (2) The motivation that a “similarity metric” could save “multiscale simulations” was not clear. (3) Why mention “mutation of RAS proteins” (paragraph 1). The authors never established any relationship between protein-lipid configuration and protein mutation. Too many to list …\n2.\tThe definition of the problem is so ambiguous. I feel the authors don’t understand what problem they were trying to solve. Could the authors state clearly what the “similarity metric” is designed to solve, eg. Similarity of protein-lipid complexes configuration in multichannel images? Then, one immediate question, what’s the relationship between what was observed in “multichannel images” and real biology? Or in other words, whether multichannel images are sufficient to reveal different configurations of protein-lipid complexes. What was observed in the multichannel images? Next question, how “similarity metric” was defined and evaluated, or why I should believe that the authors have provided a meaningful definition of “similarity metric”, as “no well-formulated or widely-accepted notion of similarity (fig. 1 legend, a similar statement can be found in another part of the text)”. A further question, what are possible configurations that the similarity metric is trying to discriminate (only number proteins, like figure 3 left)?\n3.\tIn section 4, the authors provided a formal definition of “similarity metric”. First, the author only consider cardinal axis-aligned rotations, which was insufficient to cover enough biological relevant diversity. Second, the radial profile function was defined per channel, why c was in the max function. \n4.\tWhat does figure 3, middle mean? How does this figure support the transformation invariance? \n5. The contribution of current work seems to be incremental, and the problem solved by the “similarity metric” seems to be trivial. As shown in figure 5, it seems that the spatial location of the bright spots (proteins) is sufficient for discriminating example cases. \n",
            "summary_of_the_review": "I don’t recommend publishing the current manuscript, as a significant amount of improvement has to be made. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The reviewed paper presents a novel similarity metric capable of reasonably grouping images generated by the multiscale simulations of interactions between a few proteins of interest (RAS and RAF in the paper) and bilayer lipids from the cell membrane. Although no single standard similarity metric existed in this area, experts use several intuitions to define a notion of similarity. Hence, the main contribution of this work is a combination of expert-defined intuitions into a single measure that has been shown to successfully capture the underlying biological processes as well as remain robust to potential distributional shifts. The authors argue that such a similarity metric is necessary to select a subset of simulated patches that will be analyzed in more detail using more precise mathematical models. ",
            "main_review": "The paper is generally well written using comprehensive language that is easy to follow. Except for a few minor formatting and spelling slip-offs, the presentation of the work is very good. The work is mostly technically sound, claims made are adequately supported by the evidence presented in the paper.  \n\nMain comments: \n\n- The paper seems to inadequately address the motivation of the work. While reading a number of key questions needed to put the results of this research into the larger context remain unsettled, such as: \n1) How exactly synthetic data used in experiments has been generated and what is its purpose?\n2) Why there is a need to find similar patches - how exactly this helps in the downstream analysis and what is this downstream analysis? \n3) The authors mentioned that it is important to find a diverse set of examples to dwell on, does it mean that there is an interest in different patches rather than similar?\n4) What are other applications which make use of the proposed similarity metric?\n\n- There is a significant lack of clarity when it comes to used baselines. The authors compared their proposed composite metric to three other alternatives, namely L2 pixel-wise distance between patches, L2 distance between mean concentrations, and L2 distance that is based on RDF kernel. First of all, this is important to note that neither of these metrics has been fully defined in the text of the paper, leaving readers guessing about their exact implementation. Secondly, the justification behind the choice of these metrics is nowhere to be found in the paper. The latter appears to be extremely crucial as metrics selected for comparison seem to be overly simplistic and also inadequate (the authors themselves admit this in the paper). It would seem reasonable to use more advanced similarity measures e.g. the one based on latent space produced by any of the autoencoder-based approaches discussed at the end of the related work section to convince the readership of the superior performance of the proposed metric.\n\n- The authors mentioned a gigantic amount of GPU hours that it takes to run the simulation that produces the data, it is said that due to this long execution time no comparison with other methods and metrics can be feasible. This raises a number of important questions e.g. why the results of the simulations cannot be stored to facilitate further experiments? Is it possible to run needed calculations on a reasonably small subset of this massive amount of data? Would these results still be useful? \n\n- Is there a possibility to validate the similarity metric using data that has ground truth yielding more definitive answers? It does not seem to be completely unthinkable to use a simple microscopy imaging dataset and compare patches with cells. \n\n- Although the authors made an attempt to describe potential other applications of their metric, the diversity of such applications seems to be very limited. For example, the authors say \"our framework can be easily adapted to support different types of simulations in the space of biology or elsewhere using intuitions from the application\", is it possible to provide a set of very specific examples, that will convince readers that the approach is going to be widely applied beyond one research group? It would also be beneficial to cite works that might have benefited from using the new similarity metric. \n\nLess critical comments:\n\n- in section 4 (Similarity Metric for Patches) the authors formulate classification loss. Up to this point, it remains unclear what exactly y_i and y_j stand for, while the text says 'label', it does not explicitly link the term 'label' to term 'type' introduced earlier, leaving an impression that this is some kind of another label that is used.\n\n- part of the paper related to calculating a radial profile of an image, is not easy to grasp without relevant background. A simple intuitive example would greatly help the reader to understand the concept better. \n\n- Table 1 legend mentions MARE* and MARE performance metrics, without defining nor spelling out neither of the metrics. \n\n- Figure 7, this is unclear what is the essential difference between data used for generating middle and right plots. Also, the authors refer to this figure twice in the text, without giving clear guidance as to which specific facet of the figure was meant.",
            "summary_of_the_review": "Correctness. The main claims seem to be sound and somewhat supported by the experiments presented in the paper as well as theoretically. It remains unclear whether it is possible to perform a more definitive validation of the proposed similarity metric on data that has a more reliable ground truth. Also, it seems that the approach presented outperforms simple baselines, while more complicated baselines were not used. \n\nTechnical Novelty. The proposed metric is novel, and the contributions are significant. \n\nEmpirical Novelty. It is hard to assess since no new datasets nor benchmarks are presented in the paper.\n\nOverall, although the paper presents a novel metric, the utility of this metric seems to be applicable only in a narrow field. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics related concerns were identified while examining this work.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}