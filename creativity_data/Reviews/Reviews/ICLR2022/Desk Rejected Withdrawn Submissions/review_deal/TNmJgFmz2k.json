{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper deals with a weak disentanglement problem. In conventional disentanglement setups, different generative factors are assumed to be encoded to different dimensions. However, weak disentanglement in this paper assumes a specific combination of generative factors is encoded to a certain region in the latent space.\nTo this end, the authors propose a generative model based on VAE. This new method has 2 components: an abstraction autoencoder (AbsAE) and a relational learner (ReL). AbsAE is basically an adversarial autoencoder. ReL models the relations among different regions in the latent space.\nIn the first stage, the model trains AbsAE following a standard optimization of an adversarial autoencoder. Then for each kind of combination of generative factors, 20 (or more) samples are provided. Then for each 20 samples, they are used to estimate a Gaussian in the latent space to form a subspace for this kind of factor combination. Then in the second stage, the authors construct (input, relation, output) training samples for latent codes. Then the ReL learns how to perform changes to the latent codes from one region to another. \nThe authors illustrate the proposed model's performance on 3 datasets: HWF, dSprites, and Shapes3D. Both qualitative and quantitative results are provided.",
            "main_review": "Pros:\n1. I generally agree with the motivations proposed in this paper. Separating relevant factors of variations of the data in single isolated dimensions of the latent representations is indeed too restrictive. Also, based on (Locatello et al, 2019), weak supervision (and inductive biases) could help disentanglement.\n2. The illustrations (text and fig) of the method is clear in general. I can follow the paper without major confusions.\n3. Various experiments are performed. The goals of the experiments are also clearly listed upfront. Both qualitative and quantitative results are provided. Sensitivity studies on the parameters are also included in the paper.\n4. I think some experimental scenarios are interesting, e.g. HWF.\n\nCons:\n1. It seems that each combination of generative factors requires certain amount of labeled samples (e.g. 20). Though the authors claim this is weak-supervision, the number of labeled samples could go up quickly. For instance, 3 factors each with 10 discrete values ($10^3=1000$), or 10 factors each with 2 values ($2^{10}=1024$). You might need tens of thousands of labeled samples. This limits the generality of the model.\n2. I'm not sure why authors choose 20 or 30 samples for Gaussian estimation. Maybe an additional experiment on the number of samples would make the claims more convincing.\n3. If the generative factor is continuous, how would your model handle such case?\n4. In HWF, how you model the case `6+8=14`, `6-8=-2`? I guess your model might not handle such cases?\n5. It seems that the relation learner really depends on the specific scenario. One needs to design domain-specific relation learner. This issue makes the paper a bit less general.\n6. In sec 3.1, $y_{g_i}$ should be explained.\n7. I think Eq. 9 actually requires extra supervision (e.g. how the inputs \"relate\" to the output)?\n8. Given that you used some extra supervision, is it a bit unfair to compare with unsupervised $\\beta$-VAE or FactorVAE?\n9. Though I generally agree dimension-wise disentanglement is indeed too restrictive, besides relaxing the problem to the weak disentanglement, you can also relax the problem to group-wise disentanglement [1,2]. It is worth discussing and comparing.\n\n[1] Bai, J., Wang, W. and Gomes, C.. Contrastively Disentangled Sequential Variational Autoencoder. NeurIPS 2021.\n[2] Hosoya, H.. Group-based Learning of Disentangled Representations with Generalizability for Novel Contents. IJCAI 2019.",
            "summary_of_the_review": "In general, the idea introduced in this paper is interesting and inspiring. The whole paper is also written clearly and well-organized. The combination of AbsAE and ReL is novel. In the experiments, the goals are clearly listed. However, the problem setup is still a bit too ideal. The so-called weak-supervision may not be that weak when the number of combinations grows. Also, the datasets are all synthetic and whether the relation learner can generalize to other domains is not clear. If authors can show some results on more real-world and less well-designed scenarios, the claims would be more convincing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new objective for training VAEs with disentangled representations. There are some experiments to show that the new objective learn disentangled representations. ",
            "main_review": "While the paper has promise, it has some shortcomings that need to be addressed/clarified before I can give a higher recommendation. \n\nI don’t see the difference between prior and meta-prior. I think what this paper calls ‘meta-prior’ is no different from a typical prior? \n\nThe novelty seems rather limited, as the encoder and decoder are learned by a fairly standard adversarial autoencoder objective with a Gaussian mixture prior. The main novelty seems to be the relational learner, but I have some concerns about it:\n\nFirst, how does the relational learner affect the encoder and decoder? If I change the relational network, does that affect how I train the decoder and encoder? From Eq.(1) it seems that the answer is no, then it’s unclear how the relational network benefits encoder and decoder training. \n\nSecond, learning the relational network requires paired data, the samples need to be paired in a specific way. I don’t understand how this can be done with any dataset that’s not synthetic, where you already know the correct disentangled factors. A similar shortcoming is that relational learning requires knowledge of f, which characterizes how different data modes are related to each other. Naturally, given the synthetic dataset we can design f, but given a non-synthetic real-world dataset, how should practitioners design the function f? \n\nIt seems that the prior p(z) is estimated from labeled data? However, I do not quite understand how the prior is learned based on the description in the paper. In section 4.1 (which the paper promises an explanation) the only relevant information seems to be the number of distinct modes? How do you construct a prior given the number of distinct modes? In practice, the number of modes is probably unavailable, as real datasets probably do not have a small number of distinct modes, and even if it does, the number of modes is probably not known. \n\nThe proposed method requires additional supervision (as described above), while I do not think the compared baselines (such as beta-VAE and factorVAE) require such additional supervision, so how do you ensure that the comparison is fair?  \n",
            "summary_of_the_review": "I think the paper works on an interesting problem, but there are many flaws that needs to be addressed before I can recommend acceptance. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to use a relational module to modulate the latent space learnt by an adversarial AE. The relational module utilize a small subset of labelled data to minimize a relational loss that encourages parts of the latent code to be mapped by relational latent operators.",
            "main_review": "Pros:\n1. The paper is written fluently and effectively convey the authors' ideas in a concise way.\n2. The proposed Rel-Learner seems an interesting way to encourage weak disentanglement. I think this is relatively novel.\n3. The evaluation is thorough. Experiments demonstrate the advantage of the proposed methods in terms of clustering accuracy, disentanglement measurement and relational accuracy.\n\nCons:\n1. There are constraints imposed in the method that makes it not very applicable to more real-world datasets. For example, the authors restrict the latent factors can only take discrete values, and state that each combination of latent factors is a mode in the mixture of gaussian.  First of all, the method is going to have scalability issues once more factors are included. Secondly, most of the data generating factors in real world are real-valued. I have concerns about how can the proposed module be extended to such cases. I think the authors could discuss more on this issue.\n2. Another constraint is that a set of valid relations between ground-truth latent factors need to be determined for each different dataset. For the proposed synthetic datasets, the required work is not so onerous. However for real-world cases, where relations are far less-well defined, and sometimes not binary, how can the model be adapted to be useful in this scenario?",
            "summary_of_the_review": "A technically sound, relatively novel paper with proper evaluations. The paper fall shorts in discussing some of the constraints imposed, and the method's adaptability to more real-world cases. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}