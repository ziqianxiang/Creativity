{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes the Minimax-Meta Regularization to improve the meta-generalization and adaptation generalization for meta-learning at the same time.",
            "main_review": "Strength: \n\n+ The method is simple to use.\n\n+ The paper conducts several empirical studies with both classification and regression problems.\n\nWeakness:\n\n- A major problem of this paper is that the writing in multiple places are not precise which make the reader hard to parse. Since the method proposed by this paper is heuristic, clearly explain the intuition is essential. For example: (i) \"regularizing the model during meta-testing time can enhance the generalization to the task domain\", what does the meta-testing mean here? It usually means the evaluation on a new task after all the training. (ii) \"a meta model θ that is robust to tasks distribution\", \"robust to the data distribution of the task domain T\", how to be robust to a distribution and a domain?  (iii) From the Figure 1, it is hard to understand what the proposed approach is. \n\n- The meta-generalization should be clearly defined. This paper defines it as \"generalize to the unseen tasks\". If it means making good prediction to the query set of an unseen task, how does the meta-generalization different from the adaptation generalization. \n\n- In the related work, the paper mentions the works (Yin et al. 2019) (Yao et al. 2021) are solving meta-overﬁtting, which is defined here as \"meta-model overﬁts to meta-training tasks due to the limited number of tasks\". But the overﬁtting problem studied in these related works are not because the number of tasks are limited, but rather the tasks are not mutually-exclusive. \n\n- The reviewer think the intuitive explanation of why the proposed regularization should work is not convincing yet, and there is no analytic analysis to support this intuition. The empirical advantage, as shown in Table 1, is also small. \n\n- There are several other places where the writing is inaccurate, though it only influences the understanding slightly: the proposed method \"without additional computational cost\"; really? Surprisingly, we ﬁnd (this method works); why surprising after introducing the intuition?  What is $\\mu$ in Eq.(3)? Where are the results for Sec. 5.2.2? \n\n\n\n",
            "summary_of_the_review": "Though this method is simple and easy to use, according to the clarity of the paper, the level of mathematical support, and the empirical results, it may need further study to demonstrate the effectiveness of the proposed method convincingly. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents a minimax meta-learning algorithm to address the generalization gap in standard meta-learning techniques. The proposed algorithm adds a new regularization mechanism that maximizes the regularizer in the inner-loop to encourage the adapted-model to be more sensitive to the new task, and minimizes the regularizer in the outer loop to resist overfitting. Empirically, experiments were conducted for few-shot learning and robust re-weighting. The results show that the proposed method improves the performance of the meta-learning algorithms in comparison to standard approaches.\n",
            "main_review": "**‌Originality**\n\nThe novelty of the paper is limited. The proposed approach adds an additional regularization term to the standard meta-learning objective function, and the meta-learning algorithm is trained using a minimax objective. \n\n**Quality**\n\nThe paper is technically sounds. However, experimental evaluation shows marginal gains to alternative meta-learning algorithms. Evaluation lacks experiments beyond computer vision applications, and state-of-the-art meta-learning algorithms like Meta-OPT-net and R2D2 are not included.\n\n**Clarity**\n\nThe paper is clear and easy to follow.\n\n**Significance**\n\nThe paper is relevant to researcher in the meta-learning community, however, the novelty of the paper is limited as described above.\n\n**Limitations**\n\n- The paper lacks theoretical analysis.\n- Experimental analysis is limited to sine-wave regression and image classification.\n- Gains are marginal and the novelty is limited.\n\n**Questions to Authors**\n\n- How does the performance of the proposed approach compare to other meta-learning algorithms beyond image classification and sine-wave prediction? \n- Can the authors provide theoretical guarantees of improvement for the proposed algorithm?\n",
            "summary_of_the_review": "‌‌\nThe paper presents a minimax meta-learning algorithm to address the generalization gap in standard meta-learning techniques. The novelty of the paper is limited. The proposed approach adds an additional regularization term to the standard meta-learning objective function, and the meta-learning algorithm is trained using a minimax objective.  Experimental evaluation shows marginal gains to alternative meta-learning algorithms. Evaluation lacks experiments beyond computer vision applications, and state-of-the-art meta-learning algorithms like Meta-OPT-net and R2D2 are not included. The paper lacks theoretical analysis and gains are marginal compared to alternative approaches.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper propose a new method to enhance the generalization in meta-learning problem.",
            "main_review": "Strengths:\nThe paper discusses an interesting problem for meta-learning;\nQuantitative results show the effectiveness of the proposed method;\n\nWeaknesses:\nThe presentation and writing need to be improved, some important discussions and explanations seem to be missing. \n1. How are the regularizers used in this paper designed? Do the authors propose new regularizers or simply adding existing regularizers to both inner and outer loop? Do authors try different regularizers? How will different regularizer influence the results? More detailed discussion on regularizers will enhance the paper;\n2. Discussions on meta-generalization and adaptation-generalization seems to be vague and too heuristic; The improvements in quantitative results can not be well-connected to the improvements of generalization properties, do the improvements really indicate both better meta-generalization and adaptation-generalization?.\n3. Minor problems: bottom border missing in Table 4; results have different precisions in Table 5;\n\nSome questions & comments:\n1. Are the baseline models in the experiments based on gradient descent or mirror descent? Are all the method compared with the same optimization methods?\n2. Considering the motivation of the proposed method, will the proposed method lead to more robust models w.r.t different types of adversarial attacks?",
            "summary_of_the_review": "The paper discusses a potentially interesting problem, while it needs improvement before getting published.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper study the problem of generalization in meta-learning. It considers two different kind of generalization for meta-learning: *meta-generalization* and *adaptation-generalization*. The authors argue that recent works ignore the adaptation-generalization, and they propose the Minimax-Meta Regularization for meta-learning to address both problems. Then they empirically study the effect of their proposed regularization for MAML and MAML++ on three different tasks, namely Few-shot Classification, Few-shot Regression and Robust Reweighting.",
            "main_review": "## Strengths\n\n- The separation in two different kind of generalization is sounded and interesting.\n- The proposed approach gives promising results.\n- The experiments are conducted on three different tasks, to cover different problems where meta-learning can be applied to.\n\n## Weaknesses\n\n- The quality and clarity of the writing should be improved. A lot of sentences are not grammatically correct, which hinders readability. E.g, in Section 1, second paragraph, second sentence, \"However, direct applying the regularization to the networks limited the flexibility of fast adaptation in the inner loop (meta-training) of meta-learning\". \n- The experimental details for each tasks are limited, they are not sufficient to reproduce the results. At the very least, the architecture used and the training hyperparameters should appear.\n- The experiments are only conducted on MAML and MAML++. While the authors claim that their regularization is *general* and can be applied to *all bi-level optimization formulation*, it remains to be proven that it is effective for other methods.\n- The authors do not compare their results with other methods that tackle generalization in meta-learning. Although the authors claim that previous methods do not address the *adaptation-generalization*, this claim is not proven through experiments, nor do they prove that their approach improves the method in this regard.\n- The datasets considered are limited to MNIST for the Robust Reweighting task, Omniglot and mini-imagenet for the Few-shot Classification task and synthetic Sinusoids for the Few-shot Regression task. All these datasets are considered as either toy problems or easy datasets nowadays. To really assess the effectiveness of the approach, the authors should consider more complicated datasets.\n\n### Other remarks\n\n- There is no caption to Table 1 and no explanation of the experiment in the text. This hinders the understanding of the table, even more so that the table appear before the text referring to it.\n- The figures and tables in double columns leads to really confusing captioning.\n- In Section 2, the authors denote the model by $f$, but then use $\\phi$ in the remaining of the paper.",
            "summary_of_the_review": "The idea presented in the paper is interesting and the results presented are promising. However I do not think the paper is ready to be published yet, considering that there are a lot of claims that are not supported and that the current quality of writing is low. The paper requires more experiments and comparisons to previous work to really support the contributions. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}