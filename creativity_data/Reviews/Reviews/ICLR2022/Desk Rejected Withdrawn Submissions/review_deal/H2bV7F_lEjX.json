{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper tackles the issue of training a model to generalize to data from unseen domains when there is a natural progression between domain at training and test time (eg. temporal progression). With the assumption that there is a fixed underlying transformation \"g\" which underlies the evolution of the data distributions, they propose a training algorithm based on progressive net which simultaneously learns g as well as well as a strong classifier. The resulting model can be used on unseen domains at test-time, leveraging the learned transformation to extrapolate to the new domain.",
            "main_review": "The paper proposes a sensible solution to tackle an interesting and relatively under-studied problem. Strong experimental results support the adequacy of the proposed approach on a variety of benchmarks. However, some missing baselines/ablation make it hard to interpret these results as a consequence of the proposed training algorithm as opposed to the choice of the classifier (prototypical networks). Until this is particular point is resolved, my recommendation is that the paper is borderline.\n\nPros:\n- The paper tackles an interesting and under-studied problem, which has many concrete applications in the real world\n- The problem itself is overall well-framed, as are the assumptions and use-cases of the method are clearly delineated\n- Experimental results are convincingly in favor of DPNets\n\nCons:\n- Lack of comparison to prototypical nets. As far as I can tell, there is no direct comparison with a \"non-directional\" version of prototypical nets. This makes it difficult to disentangle the effect of the proposed \"directional learning\" algorithm from the superior expressive power of PNets granted by their transductive nature.\n- I am not particularly fond of the task of gender prediction from teenage faces across decades (referred to as \"Portrait\" in the paper). What is  the intended use-case of this particular application? And what leads the authors to believe that this is an appropriate benchmark for DPNet specifically (ie what is the \"g\" function in that case)? I think this should at least be addressed in the paper or in the ethics statement (note that the authors didn't include the latter). If the purpose was to demonstrate the efficacy of the approach on a realistic, time-stratified image recognition benchmark, my personal opinion is that something like the FMoW dataset (https://arxiv.org/abs/1711.07846) would have been a better, and less questionable choice.\n\nAdditional comments:\n- There is a rather substantial body of work on distributionally robust optimization which is largely ignored in the intro/related work section. The authors do compare against group-DRO (from Sagawa et al. 2020), but I think the broader DRO literature should be acknowledged elsewhere in the paper (eg. first paragraph of Sec.2)\n- For some reason I am not 100% sold on the name \"directional domain generalization\". To me the emphasis should be less on the \"direction\" or \"directedness\" of the procedure but rather on the consistent progression across multiple domain. In my opinion, something like \"progressive\" or \"sequential\" domain generalization would be more adequate.\n- There is a typo in citations towars the end of page 4 (\"[shai-ben, mingsheng long]\")\n- I think figure 5 would be better as a 2d heatmap with \"distance of domains\" and \"# of domains\" as the two axes and the relative differences should be shown only as colors (and perhaps numerically). The elevation doesn't add much and the perspective makes reading the results along the 3 axes awkward.\n- In Table 2, I would expect that the performance of DPNet monotonically increases as the number of domains gets larger (since we can learn a more accurate \"g\"). However, this is not the case, in particular for 13 domains, the performance of DPNet plummets and is even worse than for 3 domains. Could the authors explain this counter intuitive result? The standard deviation is rather small so it doesn't seem like this is just an issue of variance due to the random seed\n- Similarly in Table 2 I am not sure if I understand why the performance of ERM drops when there are more domains. Shouldn't the addition of multiple domains act as \"data augmentation\" for ERM? If I understand correctly, for 19 domains, ERM minimizes the loss on 19 different rotations of MNIST, which should encourage it to learn rotation invariant representations. Can the authors clarify the experimental setting and explain this failure of ERM?",
            "summary_of_the_review": "The paper proposes a sensible solution to tackle an interesting and relatively under-studied problem. Strong experimental results support the adequacy of the proposed approach on a variety of benchmarks. However, some missing baselines/ablation make it hard to interpret these results as a consequence of the proposed training algorithm as opposed to the choice of the classifier (prototypical networks). Until this is particular point is resolved, my recommendation is that the paper is borderline.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Potentially harmful insights, methodologies and applications"
            ],
            "details_of_ethics_concerns": "As mentioned in the review:\n\nI am not particularly fond of the task of gender prediction from teenage faces across decades (referred to as \"Portrait\" in the paper). What is  the intended use-case of this particular application? And what leads the authors to believe that this is an appropriate benchmark for DPNet specifically (ie what is the \"g\" function in that case)? I think this should at least be addressed in the paper (note that the authors didn't include an ethics statement)",
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a domain generalization method that leverages the evolving pattern of the source domains. Specifically, if the source domains are collected as an ordered sequence with an evolving pattern (function), and if the target domain is the next in that sequence, then such a function can be learned (implicitly) to aid the learning of a new domain. ",
            "main_review": "- Strength: The idea of directional domain adaptation/domain generalization seems promising and definitely worth exploring more. The authors also demonstrate strong empirical results to support their method.\n- Weaknesses:\n    - The presentation of the paper is not clear. It introduces a (class of) function $g$ that transforms the data distribution among consecutive domains. However, it is not clear how: does it transform each data point from a domain to a data point of the other domain? Moreover, after the theoretical discussion, the function $g$ is not used in the practical implementation of the method, which makes the paper feel disconnected. \n    - The technical contribution of this paper is also limited. Based on the theorem by Shui et al., the paper presents a result regarding the loss term of the target domain, which is a direct collocation of an additional assumption about the consistency of the evolution of the domains. The assumption is also quite problematic, which I will get to later. Note also that there are similar results (to Shui et al.) in the DA and distribution shift literature with different divergence/distance metrics (total variation, H divergence, KL divergence), and I can see that the authors do not use the JS divergence in the implementation so I guess any other similar bound would work -- this is not a major concern but should be mentioned/discussed for completeness.\n    - One of the major concerns is about the problem statement. Here we assume that there is a function that transforms between the domains, and the target domain is the next in that transformation. In that sense, the target domain is already determined/known. This goes against the problem setup of Domain Generalization, where the target domain is unknown/undetermined (people also usually talk about the loss term as average over a family of target domains/ worst case domain). This assumption is also too restricted, with application mostly to some toy/synthetic datasets (including RMNIST), and does not hold true for real-world datasets such as PACS and VLCS. I can see that this assumption can somewhat be accepted for data collected over a time period. However, even in that case, it is hard to believe that there is a single deterministic transformation function, as it might be probabilistic with some complex transformation mechanism: given the data of this year, there are countless possibilities how the next year's data will look like. We can also see from the bound that, when we can't learn/ when there doesn't exist a function g that transform the data distribution between the domain well (which is very likely given that the transformation is often probabilistic and complex), the RHS would be very large, making the bound vacuous. \n    - Regarding the implementation, the authors use two functions $f$'s instead of the transformation function $g$. It is confusing to me since these functions do not transform the data density between domains. The connection of this implementation to the earlier theoretical result should be clarified.\n    - Regarding the experiments, although the results are strong, I feel like it is not a very fair comparison. The proposed method is privileged in the sense that it knows the order of the source domains (other methods are not given this information). However, this can be somewhat acceptable since this can be seen as a new direction and there are not many related baselines. I also suggest the authors to compare with [1,2], since they also use the concept of transformations among the domains, and discuss the similarities/differences. I am also not fully convinced about the result of ERM in Figure 3c, shouldn't the ERM fit the training data very well?\n\n[1] Nguyen, A. Tuan, et al. \"Domain Invariant Representation Learning with Domain Density Transformations.\" arXiv preprint arXiv:2102.05082 (2021).\n[2] Robey, Alexander, George J. Pappas, and Hamed Hassani. \"Model-Based Domain Generalization.\" arXiv preprint arXiv:2102.11436 (2021).",
            "summary_of_the_review": "Although the experiment results look promising, I am not fully convinced of the problem setup and the practicality of the assumptions made.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In domain generalization (DG), we normally assume that the target domain is static rather than dynamic. However, we might meet many dynamic target domains in the real world (e.g., domains change over time), which will cause significant prediction errors on such dynamic target domains. To avoid this issue, this paper considers a new problem setting: directional DG (DDG), where target domains will dynamically change over time/rules/directions. \n\nTo address this very challenging problem, this paper first analyses the property of DDG and defines the consistency between two near target domains. Based on this new definition, the authors propose a new generalization bound for the target domains. Such consistency definition also uses g* to connect all target domains and make the DDG solvable under some assumptions. Based on this bound, a meta-learning-inspired method is proposed to address DDG: directional prototypical network. Empirical evaluation on both synthetic and real-world data sets validates the effectiveness of the proposed approach.\n\nIn general, this paper contributes a novel problem setting and a valid solution to this setting. Although some motivations are unclear, the contributions of this paper are enough.\n",
            "main_review": "Pros:\n\n+ In the recent development of domain adaptation, researchers have noticed the situation where the target domain might change over time (e.g., images in daylight and night). Existing papers have also shown that this situation should be noticed because existing methods cannot handle it. In this paper, a new problem setting regarding DG is proposed along this line, which is novel and should be discussed in future.\n\n+ The good contributions of this paper also lie in its theoretical analysis of this new problem setting. The theoretical parts of this paper are very clear and easy to follow. It demystifies the key assumptions behind the DDG. Based on the defined consistency, we can imagine when DDG is a valid problem, which is impressive.\n\n+ This paper is easy to follow. Experiments can partially support the claims made in this paper. A plus should be that the performance gain of this proposed method is significant.\n\nCons:\n\n- Although the theoretical part is very interesting and meaningful, the solution is not well-motivated. After reading the whole algorithm, I can follow the main idea of why you have such a design. However, I cannot follow the solution when I first read it. If we just delete the meta-learning concept, would it be better for understanding? It is better to connect your algorithm to your solution. \n\n- In Figure 3, could you draw the total performance of your method? It might be very close to 3(b), is that right?\n\n- In all theorems, it seems that we can also try other distances to measure the discrepancy between two joint distributions? Is that possible to extend your theorems to more general ones? Some discussions might be needed here.\n\n- I actually cannot follow Figure 2 well. I understand your algorithm by reading the text and algorithm table instead of Figure 2. Figure 2 should be redrawn or deleted. I think I cannot directly understand the support and query in this figure. They are from meta-learning rather than your key arguments.  \n \n- One key reference is missing: gradually domain adaptation (ICML 2020). The whole idea of your paper is similar to this reference. Some discussions should be added to distinguish between your original contribution and the contributions of the reference.\n\n- The difference between your paper and the Wang et al. (2020, CIDA) should be discussed deeply. The synthetic dataset used in your paper is the same one in Want et al. (2020), is that right?",
            "summary_of_the_review": "In general, considering the significance of the researched problem, this paper can be accepted by the ICLR2022. However, some points should be clarified and strengthened in the revision.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Summary:\n\nThe paper proposes a new domain generalization paradigm called evolving domain generalization, where the source domains and the target domain are not random but have an evolving pattern.\n\nThe paper provides theoretical results on the evolving domain generalization, which bounds the target domain test error with synthetic domains.\n\nThe paper proposes a new approach for evolving domain generalization, which learns two embedding functions to derive the synthetic domains and minimize the distances between the synthetic domains and the real domains within the hidden space\n\nThe paper conducts experiments on three domain generalization datasets including RMNIST, Portrait, and Cover Type.\n\n",
            "main_review": "Strength:\n\nThe paper proposes a new problem setting of evolving domain generalization and provides rigorous theoretical guarantees. \n\nThe paper provides a new approach for evolving domain generalization based on the theoretical results.\n\nThe paper algorithm is easy to understand and clearly clarified.\n\nThe paper is organized in a fluent way.\n\nExperimental results show that the proposed method achieves state-of-the-art performance on several domain generalization datasets.\n\nWeakness:\n\nMy only worry for the paper is the practical value of the problem setting. All the experiments are done in a quite controlled setting, like the increasing rotation angles and different decades. However, in practical, I'm not sure when such controlled setting is useful.\n\nMinor:\n\nThe citation here is broken: Finally, we note two key theoretical differences between DDG and DA in evolving environments [shai-ben, mingsheng long].",
            "summary_of_the_review": "I find no major problem for the technical part of the paper. The paper is organized in a way with theoretical guarantees, description of methods and experimental results. All the parts are clearly clarified and the method is strongly related to the theoretical guarantees. However, I just find that the problem setting is quite made-up and needs some clarification of the practical use of the problem setting.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}