{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The main points of the paper:\n- the authors consider a kernel continual learning framework\n- they use deep features and a linear kernel\n- the information from previous tasks is stored in the corresponding coresets\n- in order not to store coresets the authors use VAE with a gating mechanism. The gating mechanism allows the VAE to generate coresets representing previous tasks",
            "main_review": "Comments:\n\n- section 2.1. Why do the authors use the same h(x) for all tasks? I guess, it should be either re-trained for different tasks, or we should perform continual learning for h(x) somehow in order not to forget previous tasks.\n\nI guess C_t is a coreset sample? I propose to mention this. Otherwise, the authors did not provide any definition of C_t in the beginning of the paper.\n\n- Why do we need a kernel learning framework? We use deep nonlinear features generated by the network, so the usage of the linear kernel is enough. What if we use a standard logistic regression and re-train it each time using a dataset for a new task and the datasets from the previous tasks?\n\n-  \"For the task incremental learning, the number of gates in each layer is equivalent to the number of classes while for the domain incremental learning, it is equal to the number of domains in the given benchmark.\" What if the number of tasks is not known? How can we proceed with the proposed approach? Do I understand that it can not be applied in this case?\n\n- A coreset is a subset of the given dataset which contains the most important information so that when we train the model using only the coreset, we do not loose to much in accuracy.\n\nIn Fig. 1 it is mentioned that the VAE decoder generates a corset. If we perform sampling from a prior and generate some data, it does not guarantee that the generated artificial samples will represent an accurate coreset which covers all important aspects of the previous tasks. Any guarantees? Which size of the coreset on each iteration should we use? Do we perform such generation of the coreset before or after re-training the VAE on the new task using the gating mechanism?\n\n- In page 4 it is written that “Next, we put both Ct and Dt through the encoder network qφ and obtain their corresponding internal represen- tations for learning kernels”. However, nothing is told how C_t is selected.\n\n- The authors only mentioned that to decrease the catastrophic forgetting when training VAE, they use the gating mechanism, and cited several papers with some possibilities how  such gating mechanism can be developed. However, I was not able to find any exact description of the gating mechanism used by the authors. Moreover, they mentioned that as a prior they used not a Gaussian distribution, but a mixture of Gaussian distributions. Any comments on this? How many components did they use? Any comments on how to select the number of components? How does the number of components influence the final accuracy?\n\nAlso I would expect to see the performance of the VAE with the gating mechanism. To which extent it is capable to learn continually? What if we use a standard VAE and re-train it each time using the coresets from the previous steps and a new dataset?\n\nI consider this part of the work (replay for coresets generation based on VAE with a gating mechanism) as one of the most important parts of the work. I think it has significant influence on the final results.\nHowever, the description and the experimental testing of this part of the work lack details.\n\nMoreover, some ideas (about generating coresets for new tasks, usage of a Gaussian mixture as a prior) resembles a lot ideas from the paper \n\nEvgenii Egorov, Anna Kuzina, Evgeny Burnaev. BooVAE: Boosting Approach for Continual Learning of VAE. NeurIPS 2021 (on arXiv since 2019).\n\nMoreover, the authors of this paper not only proposed an algorithm for continual learning of VAE, but also demonstrated how it works for continual learning of a classifier.\n\nI expect some discussion around this as well as comparison.\n\n- The authors uses a gating mechanism. There is another seminal paper with such idea, namely,\n\nDavide Abati, Jakub Tomczak, Tijmen Blankevoort, Simone Calderara, Rita Cucchiara, andBabak Ehteshami Bejnordi.  Conditional channel gated networks for task-aware continuallearning.  pages 3930–3939, 06 2020.  doi:  10.1109/CVPR42600.2020.00399.\n\nThey also solve classification tasks. The questions is why do we need a kernel continual learning (especially taking into account that the kernel is mainly linear)? Maybe this framework from the paper is already enough? The paper demonstrates rather competing results.\n\n- Still for usage of the proposed approach we have to perform the data replay. I consider this as a significant disadvantage.\n\n- In the description of the main algorithm it is written that \n\n“During training the variational auto-encoder, we receive in each iteration the current task data (xi,yi,ti) and the replayed data from previous tasks (xj,yj,tj) where x, y and t are the input vector, output target and the task identifier. ”\n \nWhat is the schedule of how we select data points from previous tasks? Do we select such data points uniformly randomly? \n\n- The authors used standard datasets such as PermutedMNIST, Rotated-MNIST, and Split- CIFAR100 for testing the proposed approach. However, there are too many papers/approaches using these same already rather simple datasets. Could the authors try to apply the method to some more challenging dataset? For example, in the paper on Continual Learning of VAE, cited above, the authors used CelebA dataset.\n\n",
            "summary_of_the_review": "The paper proposed some useful ideas on how to introduce scalability in the kernel continual learning framework. However, the paper lacks many important technical details, explanations and results of experimental testing, please, see the comments above. I would propose to address the mentioned issues.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose Generative Kernel continual learning as a technique for continual learning.\nThe core idea here is to build upon a recent proposal for kernel continual learning, where small amounts of data (in particular subsets or \"coresets\" of task-specific data) and local kernels are propagated through tasks to perform continual learning.\nThe authors of this work propose to modify that basic algorithmic workflow by incorporating a generative model in the form of a particular VAE which models the task-specific data and is able to generate data for kernel construction on the fly such that kernels are built based on data sampled from this generator rather than a random subset of the task-specific dataset.\n\nA secondary contribution is the incorporation of an auxiliary contrastive regularization term to improve training of the encoder and kernels asa training-only step, which the authors claim will improve performance in practice.\n\nThe authors show in results that their techniques competes or beats kernel continual learning.",
            "main_review": "Pros:\n- The authors have a useful core idea, which is to bound the amount of memory used for continual learning by utilizing a sampling distribution over kernel parameters/coresets. They also demonstrate that given such samples CL can be made to work in this scenario.\n- the authors show solid empirical advances compared to baseline Kernel Continual Learning.\n\nWeaknesses:\n1. One of the main tenants of the paper is the aim to rid kernel continual learning of its memory requirement to carry forward a representative dataset for past tasks. Generative kernel continual learning proposes sampling from a generative model (the VAE) for that. However, antithetically to its own aims, training the model per task requires finetuning/training the VAE model on the new data as well as on the 'replayed past data' R_<t, which is a reservoir of corrupted reconstructions of all past data using the VAE. This directly implies that indeed the memory requirements here are equally severe if not worse, since that data will match in quantity the total amount of data seen up to the present, and while not used to specify the kernel, is carried forward to train the VAE. As such it would appear the paper fails in its self-prescribed primary goal by construction.\n2. I am deeply uncomfortable with the utilization of the specific VAE here. I understand this constitutes a previously presented model, but in the context of this work I would be very curious to see if it is useful for the application here. For instance, since the 'replayed set' is carried forward and the model updated at each iteration,  how does this VAE fare itself with respect to catastrophic forgetting? The parameters theta appear to be re-estimated at each task, implying that the model eventually should start to perform worse for older tasks as the model moves away from old data and thus the corrupted replay data moves further and further away from the original data. Is the VAE even that useful if we are going to be carrying around such a large reservoir of data to train it to begin with? Experiments show that average forgetting increases compared to kernel continual learning. Is the end result here then really due to the generative model or maybe due to using a lot more parameters to obtain a kernel projection than in the previous work (using the projection of hidden features of the encoder)? The paper did not convince me that the VAE part is useful at all, I would not be surprised if a simple result would show that the encoder trained as a self-supervised model with a contrastive loss or so would similarly yield a good enough featurization to learn a kernel network based on without any of the reconstruction parts here, in which case the work is carried by better featurization for kernel construction and the data being carried forward (as in kernel continual learning).\n3. What is the effect if sampling data from the VAE for kernel construction? The authors discuss varying the number of samples used, but do not show multiple trials of sampling different datapoints for kernel construction and how much variance that incorporates. It would seem that if the VAE is not trained near-perfectly well to model the denisty of the training data, samples from it can be arbitrarily far away from the data manifold which might cause random samples to spuriously hurt performance at times. While this may not impact average behavior over many runs, it would introduce a ton of variance. The authors here seem to prefer reporting average performance over random seeds, I would suggest reporting error bars of some sort and also showing in detail what the effect of sampling from the VAE with respect to that aspect may be.\n4. The authors use a pretrained CIFAR network here to turn images into embeddings and perform CL on those. This is significantly less ambitious than many other works in the literature and solves half the problem of learning a representation for them. It would seem the authors could not use their method from scratch without pre-trained models in the loop, which would be a severe drawback and cast a negative light on many of the experiments here, since they rely on knowledge obtained from a highly related dataset that has been fully observed. I would want to find out if the method can be used without any of these crutches.\n5. The authors cite two related papers, but do not quite discuss them adequately or compare to them. Specifically, Variational Continual Learning with GPs is not compared to, but constitutes a method which performs a principled form of kernel continual learning without directly storing data from each task and instead learning incremental sets of pseudo-data which are chosen to maximize the marginal likelihood. It also would appear to have stronger numbers on some of the benchmarks than this paper here. I am confused why this is not taken at face value as a baseline model, potentially even over kernel continual learning, since it does not suffer from its drawbacks.\nSimilarly, \"Functional Regularisation For continual Learning With Gaussian Processes\" by Titsias et al ICLR 2020 is cited for its proposal of the forgetting metric, but not really discussed adequatelty in related work or compared to in experiments. But this paper would clearly constitute a deeply related baseline since it also relies on an elaborate construction merging neural networks to model feature space and kernel machines put on top of that with elaborate regularisation with respect to past tasks. It also seems to have stronger numbers in published benchmarks. The authors have left me with many questions with respect to their treatments of those two papers and irrespective of whether \"kernel continual learning\" is used as their departure point this work would be required to be fairly discussed and compared to.\n6. The construction of this method is quite reliant on many disjoint moving parts that feel somewhat 'cobbled together' to the reader. This is a result of both weak presentation but also of method design. How generalizeable would any of this even be to different tasks? I would urge the authors to try to condense their key idea into a model and/or an algorithms box to appeal more to the reader and maybe think through how such a technique could be used in more general systems with more modular and reusable components.\n\n\n\nMinor comments:\n- In Sec 3.1 Implementation Details you share that: The decoder pθ(x|z,y,t) is exactly the transpose of the encoder qφ(z|x). This technically cannot be happening given that the encoder has the interface q(z|x) and the decoder p(x|z,y,t) , so they differ by at least y and t.\n- In sec. 2.2 it is mentioned that \"we exploit a learnable mixture of Gaussian distributions\" when later the prior is described as independent Gaussians. Which is it? I suggest letting clear math do the talking here, I frequently got confused trying to infer what is actually implemented while reading the paper.\n- The paper has some typos, which I do not flag as criticism for the review but still suggest fixing:\nSec 1: \ndraw samples from each task’s data distribution based on data point likelihood -> draw samples from each task’s data distribution based on data point likelihoodS.\nAs a result, kernel approximation is enhanced -> As a result, THE kernel approximation is enhanced, (here the sentence itself is also unclear)\n\nSec. 2.2: \nflexible generate -> flexiblY generate\nto adjust its decoder output per each task -> to adjust its decoder output per task\n we resort the generative replay ->  we resort TO generative replay\nSec 2.1:\nIn the probabilistic perspective, the predictive prediction is denoted as follow -> In the probabilistic perspective, the predictive DISTRIBUTION is denoted as followS\n\n- Technically, there is a slight mischaracterization in Sec 2.1 Eq.1 : \nThe authors denote that the target label/prediction is given by the softmax, the issue is that one is a probability, the other a label. Maybe a better notation might be something akin to:\np(y_tilde|x') = Categorical(softmax(Y(lambda...)\n\n",
            "summary_of_the_review": "The authors propose a technique to combine Kernel continual learning with a bespoke VAE structure to bring together generative models and CL by abstracting data for each task into a condensation model.\nWhile the method seems empirically promising compared to its stated namesake baseline, it is not compared to other related techniques in the literature adequately and generally suffers from some structural and empirical weaknesses that make it hard to evaluate and attribute performance to the proposed changes.\nA key problem seems to be the underlying setup which still carries data around and uses quite a lot of machinery to somewhow squeeze performance out of that data, which appears to violate the basic premise of the paper. The paper could also be improved in terms of presentation and clarity, see some of the suggested changes.\nIt is my belief that this work would benefit from more work to flesh out the contribution, prove it works beyond the shallow comparison to kernel continual learning, and do the necessary ablations to identify the useful modular components that could be used to achieve the goal the paper sets for itself.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This submission proposes a novel extension to the Kernel Continual Learning. This work uses generative model, in particular Variational Autoencoder as a method for storing coresets of previous examples. Additionally this work proposes a new contrastive loss based regularization to improve discriminative aspects of embeddings.",
            "main_review": "Strengths:\n- Proposed method is very complex and composed of many parts, but it is generally well explained\n- Ablation studies are thorough and definitely needed for as complex method\n- Proposed contrastive loss based regularization is novel and interesting\n\n\nWeaknesses:\n- There are much better sota methods for the task incremental learning than the ones included in comparison e.g. Wortsman et al., 2020 (cited in submission) reaches over 96% accuracy on CIFAR-100 and von Oswald et al., 2020 (also cited in this work) over 82%. This means that in contrast to the main submission claims the proposed method with accuracy 72.8% is far from reaching SOTA results.\n- Novelty of proposed method is mostly  a combination of well established methods into one solution: Kernel Continual Learning (Derakhshani et al., 2021) with Generative Replay (Shin et al., 2017) based on VAE (van de Ven et al., 2020) + Gaussian mixture (Tomczak & Welling, 2018) with additional masking (Wortsman et al., 2020). The sole novelty of combining these methods with additional regularization without obtaining state-of-the-art results is not a strong, significant and insightful contribution for the CL community. \n- Experiments lack comparison on more realistic datasets (e.g. miniImageNet included in standard Kernel CL which is a method this solution is directly build on)\n- This is a highly task dependent setup. As discussed in (Massana et al., 2021) task-IL methods require new technologies to perform well in class incremental scenarios. It'd be nice to at least discuss possible extensions to more challenging class-incremental scenarios, or even better provide a solution to the problem. What if we don't have a task identity during inference?\n- It’d be nice to see actual memory requirements comparison also with other methods (not only Kernel CL). As discussed e.g. during CL CVPR workshop  https://sites.google.com/view/clvision2021 (talk 1 by R.Pascanu), “Minimal increase in model capacity and computation” is an important part of CL solution. From what I understand, the proposed method is rather memory-consuming (VAE replay model + masks + projection network).\n- I’m convinced that proposed method should remove dependence on explicit memory and might help in catastrophic forgetting and task interference, but this is not because of any novelty proposed in this work, but only other methods it’s build upon: Kernel Continual Learning Derakhshani et al., 2021, and gating mechanism by Wortsman et al., 2020.\n- I’m puzzled why contrastive loss improves results through better discriminativity of representations from different tasks, if proposed evaluations are run in task-incremental scenario.\n\nSmall details:\n- When citing what the term continual learning stands for I think we should take into account plethora of early works from 1970s\n- Why coreset selection mechanism in standard kernel cl has to be random? Can't we use any algorithm from buffer specific methods (eg. strategies described by Chaudhry 2019b)\n- Please explain better where the task identity is used in the setup. It is a bit confusing right now\n- Equations 4 and 5 can be easily merged into one\n- I’d like to see confidence intervals for ablation experiments. For some of them differences are so small that they might be a bit confusing (e.g. Table 1 SplitCIFAR100)\n",
            "summary_of_the_review": "The proposed method is in it’s majority a combination of several known solutions, it does not provide significant novel insight into CL, nor produce SOTA results even in a relatively simple task-incremental scenario. Novel contrastive regularization is interesting but has a marginal impact on the final results.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper picks up from the kernel continual learning (KCL) and replaces the coreset-based memory with VAE-based generative replay. The argument is that the memory model is bounded by storage limits and not scalable to many tasks, and that there is no guarantee that the uniformly sampled coresets are sufficiently representative of the data distribution of the respective task. The authors further propose to complement the loss with constrastive loss term to improve the discriminative characteristics of the generated samples; here they use the VAE generated data as the data augmentations. There are a few architectural tweaks aimed at improving the efficiency (none of these being new) - generative replay of internal representations, per-task gating mechanism in the decoder, learnable Gaussian-mixture prior with one class per component. The authors document superior performance over continual learning baselines including KCL on three standard tasks (permutedMNIST, rotatedMNIST, and splitCIFAR100).",
            "main_review": "(+) pros / (-) cons\n-----------------------\n\n(+) continual learning is an important active area of research, combining deep feature learning with non-parametric classification via kernel regression is a recent promising idea, adopted and further developed here\n\n(-) the paper does not propose any ground-braking new ideas or outstanding new methods or techniques, however...\n(+) it innovatively combines existing methods and ideas to solve the limitations of the coreset-based memory of KCL and improve its performance\n\n(+) the method slightly outperforms existing baselines on MNIST data variants and significantly on splitCIFAR100 data, however ... \n(-) the splitCIFAR100 encoder is preceded by layers from net pre-trained on the CIFAR10 dataset. Since this has seen the complete dataset (at least the x part), it breaks the continual learning setup. There is no dedicated discussion clarifying the effects of these additional pre-trained layers on the splitCIFAR100 experiments (e.g. ablation study). This raises questionmarks about the supposedly very good performance of the method on the splitCIFAR experiments compared to the baselines which didn't have the benefit of seing the full dataset in pre-training (including KCL).\n\n(+) I find the paper overall well written, easy to follow, with good helpful diagrams and relevant review of existing work, however ...\n(-) at points the description of the method and its individual parts is not quite clear\n1. page 2, \"Training objective\" paragraph\"\n    1.i. The model is not trained end-to-end but instead in two steps? That is in the first step you train the VAE together with the contrastive loss and in the second step you generate the coresets and use them for the kernel regression classifier? Meaning there is no information from the classification loss flowing back to the VAE learning the coreset representation?\n    1.ii. You say you put C_t and D_t through q to obtain the internal representation for kernel learning. D_t is the original data of task t and, I presume, C_t is the generated coreset from the decoder $Decoder(z) = \\hat{x}$. However, in page 3 you say you use generative replay on internal representations. From this, I would think the replay produces some $\\hat{h}$ (not $\\hat{x}$). Not sure how to reconcile these.\n    1.iii. Both the projection network for the contrastive loss and the kernel network use the same internal representation of the encoder, is that so?\n2. What is the schedule (proportion per task/class) of replay data entering the VAE loss (2). Does it matter? Have you done ablations on this?\n3. The trainable prior in equation (3) is shared accross tasks with the same classes in the domain incremental setup? Or not? If so, how?\n4. In page 3 you say the number of gates in the decoder in the domain incremental learning scenario is equal to the number of domains. That means you need to know how many problems there will be in total beforehand? Could this be somehow avoided?\n5. Page 5 - generating the VAE-based data augmentations for the contrastive loss is a decoupled operation, right? The contrastive loss is not backpropagated through these augmentation samples back to the VAE, or is it?\n6. Contrastive loss in equation (5) uses as negatives all examples (original and augmented) from the same class. For domain incremental this goes across the current as well as previous tasks which have the same class label, right? For task incremental, the only negative examples can be from the current task (previous ones don't have the same class label), right? \n\n\n\nFurther questions for clarification/discussion\n-----------------------------------------------\n1) KCL argued stronlgy for the need to learn the task kernels (there vie the ranodm Fourier features). Here you work with predefined kernels and it seems to be fine. Why is that? Is it because because the inernal reprsentation used for the kernel network is learned by the encoder/decoder framework to be coherent with the fixed kernel? But this would not be possible if there was no flow of info from the classifier back to the VAE learning the coreset representation (see clarification point above). Or would it still make sense to learn the kernels as in KCL to further boost performance?\n\n2) KCL reported forward knowledge transfer with increasing avg accuracy as more tasks were added. The avg accuracy of your method decreases. I do realize splitCIFAR has only 20 tasks but could you hypothesise what would happen for much bigger number of tasks? Would your method suffer from the need to share the encoder/decoder framework accross many tasks and hence evenutally suffering from deteriorating quality of both the replay and the gnerated coreset data?\n\n3) You claim that the method is not sensitive to the temperature hyperparameter of the contrastive loss. But you document this on the splitCIFAR100 experiment which is the task incremental problem with disjoint classes and hence no negatives from previous tasks in the contrastive loss (see point for clarification above). Isn't this conclusion skewed by this specific problem setup?\n\n\nMinor text problems / typos\n---------------------------\n* page 2: \"... infers a task-specific kernel, which is shared ...\" What does the \"which\" refer to? The kernel? I presume not. I guess rather the network. Not clear -> please rephrase.\n* before equation (1): \"the predictive prediction ...\"\n* top of page 5: \"international representations\" -> \"internal ....\"\n* middle of page 7: \"ablate generative kernel continual with\" -> \"ablate generative kernel continual learning with\"",
            "summary_of_the_review": "The idea to replace memory replay of KCL with generative replay is a logical development step (given the previous general experience of the field) which is however worth exploring and documenting. The authors combine the ideas of generative replay with the KCL efficiently, helping the generative replay through a few design tricks reported to work well in previous work (mainly van de Ven et al. 2020) and through contrastive loss to boost the classification accuracy. There are few points for which I (and I believe other readers) would like to have more clarity (see detailed comments above), and there is a worrying point about the fairness of the experimental evaluation of splitCIFAR100 related to the use of pre-trained network for which I would like to see some ablation discussion.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposed a novel method in the context of continual learning. The proposed method employs variational auto-encoders to generate samples to construct kernels for each task bypassing the need for a memory unit. Then, in addition to the ELBO objective, the authors proposed to regularize with a contrastive regularization that helps generate more discriminative samples which improves the classification. \nAs shown in the paper, the authors show substantial improvements over previous method (up to 10% gain in SplitCIFAR100). ",
            "main_review": "Overall, the paper is well written with some minor ambiguity in some sentences. For example:\n1. Sentence 3. at page 2 needs to be rephrased as it is difficult to understand. \n2. In section 2.1, it is not clear whether C_t denotes the coreset samples or not.  \n3. In section 3.2 (Generative coreset vs. uniform coreset), the authors said that “A smaller number of samples reduces the number of operations needed to compute kernels in equation 1, and as a result, it reduces the run time of generative kernel continual learning during both training and inference.” However, this is true for both methods. Can the authors be more clear in explaining the benefits of their method w.r.t uniform coreset ? \n\nRegarding the contributions of the paper, I think the paper makes important improvements over previous methods although the idea of using generative models is already proposed. But the performance obtained by the paper presents itself as a convincing argument towards acceptance. However, the performance on rotated MNIST can not be considered better (as claimed by the authors) since the method exhibits higher standard deviation than other methods. Also, is it possible to add standard deviation to Table 1 to have better comparison ? ",
            "summary_of_the_review": "Overall, I think that the paper provides good contributions w.r.t previous methods. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}