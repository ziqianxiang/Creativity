{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Authors present an effective solution to a Stackelberg game, manifested in a number of examples.",
            "main_review": "The strength of the paper lies in its attempt to leverage a previously overlooked perspective (i.e., Stackelberg games) on MARL research. There may be significant research advances and valuable insights in this direction.\n\nOne major letdown were repeated statements referring to the sequential nature of the game as an independent variable. No, whether a MARL task can be formulated as a Stackelberg game is completely out of the algorithm designers' control. Football will always be a simultaneous multi-agent game. Monopoly will always be a sequential multi-agent game. If each time step is arbitrarily further broken down into finer phases, then the model is essentially gaining \"free\" temporal resources from the increased temporal resolution. If such a thing were possible, every football team would have its players gather around the team captain every minute, discuss tactics, and then continue playing again. An n-way traffic junction (inherently simultaneous) would have its drivers come out of their cars, collectively determine who goes first (forced to be \"Stackelberg\") and then respectively re-enter their cars to drive again. No amount of algorithmic engineering would alter those facts. SeqComm should therefore not be positioned as a general alternative to simultaneous game solutions. It should not be presented as a remedy to the circular dependencies problem, which, to my knowledge, is better known as relative overgeneralization or non-stationarity, neither of which were mentioned once in the paper, to my surprise. There is good reason that MARL papers leave the given game's simultaneity property untouched and instead pursue, say, alleviating the relative overgeneralization problem, as in Stable Opponent Shaping (SOS) and Learning with Opponent-Learning Awareness (LOLA).\n\nThe following claims need good backup or must be removed from the paper:\n- \"Circular dependencies can inevitably occur when agents are treated equally so that it is impossible to coordinate decision-making.\"\nThis is not true. Randomized round-robin will avoid circular dependencies in a two-agent traffic junction.\n- \"A general approach to solving coordination problem is to make sure that ties between equally good actions are broken by all agents.\"\nThis is true only if the researcher indeed has control over whether such a tie-breaking is feasible in the given game. One cannot take an arbitrary game and turn it sequential as deemed necessary.\n- \"SeqComm, unlike previous work, allows agents to share their observations with others through broadcast.\"\nThis is not true. Among other examples, Kim2019 (cited by SeqComm authors) operates in a broadcast setting as well. Furthermore, sharing observations via broadcast is not something an MARL algorithm \"allows\". It is a given setting and should therefore not be presented as SeqComm's competitive edge.\n- \"Each predator observes the relative positions of three nearest preys, and is allowed to communicate with all other predators.\"\nThis is unrealistic due to two reasons: (i) depending on the positions of the three nearest preys, the predators will have time-varying observation horizon, and (ii) depending on the positions of other predators, the predators may have arbitrarily long communication range. I would be grateful if the authors would provide some references as to which previous works have studied the predator-prey task under such settings.",
            "summary_of_the_review": "I believe the paper could greatly benefit from a major restructuring as to the problem formulation and game properties. When applicable scenarios are identified correctly, the revised draft at a different venue in the near future would pave an insightful trail for MARL research.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a method for agents to communicate information to other agents. The main idea is that agents are divided in two levels, the upper level and the lower level.  Agents in the upper level make decisions before the lower level agents. The interactions work in two phases, negotiation and launch. In the negotiation phase everyone shares observations, in the launch phase the upper level agents make decisions before the lower level agents. The paper includes experimental results obtained in simulation in four different environments and compare the performance of their method with four other methods. The number of agents used in training is 5, the experiments are done for 3 to 7 agents.",
            "main_review": "The paper is relatively easy to read despite numerous small grammatical errors, but there are many areas in which things are not clearly specified. I have a number of questions on the paper:\n1. How is the decision to place an agent in the upper vs the lower level is made? It is not clear in the paper what criteria the system designer should use. Also, why the paper talks about upper level and lower level when it seems there are many levels? In figure 4, agents show different levels. Does it mean that there can be more than two levels? Is it possible to have more than one agent per level? \n2. The upper level agents make decisions before the lower level agents. The mechanism to order the agents in each of the level and to force the agents to make decision sequentially is based on the value of their intentions. Not much is said on how the value of the intentions is computed.  Monte Carlo methods are used to sample the space of possible futures.  Given that the agents are sorted by intention value, why not put all the agents together and sort them instead of dividing them by levels? can multiple agents be at the same level?\n3. The papers says \"The actions are executed simultaneously and distributedly in execution, though agents make decisions sequentially\".  Since the decisions are sequential, the time to make the decision will be proportional to the number of agents.  Does this limit in practice the number of agents that can work together? In the examples the numbers range from 3 to 7, which seems a very small number.\n4. The paper show results on different problems and with different algorithms, but does not say much abut the reward functions the agents use. Are the shaded areas in the plots the confidence intervals?  I did not see any explanation. \n5. There are no theoretical results/guarantees made by the method.  In the problems shown it seems to do well, but we do not know yet how well it will generalize.  Also there is no information on computational complexity or computing times.",
            "summary_of_the_review": "The paper addresses the important problem of communications in multiagent systems and proposes a way for agents to share information that will help them make decision while trying to limit communication and avoid circular dependencies.  The paper presents experimental results in different problem domains.  The performance compared to the performance (in terms of reward) of other methods show the method proposed outperforms them, but the paper does not provide any other metric for comparison.   ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns.",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a multi-agent framework for learning coordinated cooperative policies for agents of a team through multiple rounds of communication. The approach, which appears to be centralized throughout, has two communication phases. In the first phase, agents generate an independent action intention conditioned on their environment observations as well as messages received from other agents. The values of these action intentions, i.e., Q-values, are calculated and compared to determine a decision-making hierarchy. This sequential decision-making eliminates the circular dependencies among agents’ action decisions. In the second phase, agents will generate an actual action according to the determined order in the previous phase. The method is evaluated in four relatively simple domains and against three state-of-the-art benchmarks and a PPO baseline. Provided results show an improvement over baselines.",
            "main_review": "Strength\n\n- An interesting new perspective, presenting a sequential decision-making process for MARL\n- Comparison against existing state-of-the-art and ablation results\n\nWeaknesses\n\n- Although it is never discussed by the authors that what category does their approach falls in (i.e., fully decentralized, CTDE or centralized) the proposed approach appears to be fully centralized throughout training and execution. Decision-making priorities are determined centrally by having access to intention values of all agents and as mentioned several times in the paper, agents are allowed to communicate with all other agents in the environment (non-local communication). Not only does intermittent communication require centralized execution, but backpropagation into other agents’ policies would also require centralized learning. This is also how the evaluation environments are set up. All agents always have to be able to communicate with all other agents. If indeed they are backpropping into another policy, this is akin to simply have a joint policy (uni-brain of sorts) which mostly defeats the purpose of a method like this in my opinion.\n- The priority assignment is done through comparing the Q-values for intention actions. Now, if your Q function is not learned yet, technically the priorities that are being assigned are just random and the decision-making sequence is not meaningful. Thus, the approach appears to be heavily dependent on the accuracy of the Q function. \n- Although never mentioned by the authors, this method is a model-based RL. However, it is never discussed how authors obtained the model of the PP, CN, KA and SMAC domains in the experiments. The model of the environment is said to be needed to predict and evaluate future trajectories. How’s this process done in the PP, CN, KA and SMAC domains?\n- What is the content of the messages passed among agents at each phase? The paper sometimes mentions the observation and actions of other agents to be the content of the messages and observation and intention values some other times (see page 5, paragraph 2 and 3 as an example). If the actual actions are being passed (page 5, paragraph 3), how are these actions passed? As an action index or for instance as an action embedding? If only intention values are being sent, I don’t see how this would immediately allow agents to “get what actual actions upper-agents will take in execution”, as claimed on page 5. \n- Where is the observation in the second communication phase coming from? Are the communication phases occurring in two separate steps of the environment? If no, then what makes the action selection in the second round stochastic with respect to the intentions in the first round? Assume agent j selected intention action a_t with a value of Q_t(a_t) given all observations (its own and all other agents) and was assigned with the highest priority based on this Q-value. Then, the reasonable choice in the launching phase is that agent j again chooses the same action a_t since the game states has not changed and a_t had the highest Q-value, and the same goes for all other agents. Since the paper mentions that there’s only one policy (for instance agents are not equipped with a communicative policy for the intention selection and one actor policy for actual action) it is not clear how the action selection would differ from the first round.\n- On page 2, paragraph 1, authors justify their logic behind intention sharing by comparing to how humans negotiate about their division of labor. Authors should add a reference for this claim. Isn't division of labor in humans normally determined by factors such as skill set, availability, resources, etc.? It is odd to claim humans \"always\" negotiate division of labor by sharing and evaluating their intentions, without any references.\n- It is not clear how are the observations of all agents are passed into the network? Are actions of each agents passed separately so that an agent can “estimate” their actions (as explained on page 4, paragraph 3 under World Model) or is a concatenation of all observations passed into the network? Why would an agent need to “estimate” actions of other agents? Wasn’t it mentioned that actions are passed through the decision hierarchy as a message (see Figure 1 for instance)? Also, authors mention scenarios where new agents are added or removed from the environment. In such cases, how is this handled for varying network sizes? These issues are never discussed in the paper. \n- In the adopted toy domains, agents do not appear to be struggling with exploration due to limited vision (as you would expect to see in domains like PP, CN and KA) since according to the descriptions on pages 6-7, all agents always know where all the preys are. Also, the superior performance of the proposed method is not surprising since it is centralized and thus is expected to perform better than the selected CTDE baselines. Further, the presented SMAC results show that both I2S and IS performed terribly worse that a non-communicative PPO baseline. This result is very odd and does not seem to be correct. Additionally, the proposed method is a multi-round communication. Therefore, I am wondering why the multi-round TarMAC was not benchmarked against? TarMAC reports 99.9% and 97.1% success rates in Easy and Hard PP domains with only two-rounds of communication. The presented result in this paper is not showing the same. Finally, many important domain settings are missing. What environment sizes are chosen for PP, CN and KA? Are these results for the Easy or the Hard versions? What are the converged number of steps-taken for each of these methods in each domain? Such information is critical to evaluate the method against existing state-of-the-art.\n- Some of the discussions provided for ablation results are relatively weak. See discussion for Figure 4 as an instance. The order of decision-making hierarchy for PP is attributed to “encircling the prey”. This is while the described reward scheme in PP, which determines the Q-value, only models relative distance to a prey. Therefore, connecting the priority selection to encircling the prey seems to be more of a speculation. I suggest authors generate video demonstrations of their learned policies and observe the team behavior. Same applied to discussions provided for CN ablation. It would make more sense if the agents that are closer to a landmark would get assigned first so that the furthest agents have no trouble getting assigned through, specially since there’s a collision penalty. The discussion presented by authors argues the opposite.\n- The Abstract is very ambiguous, and needs is rewrite. There are several instances of unreferenced (or with ambiguous references) pronouns such as it, this, its, they, etc. Many mathematical notations are not defined (such as q and d in Eq. 2, etc.) or are presented and defined in a later section (such as AM_{lac}, AM_{neg}, etc.). Authors interchangeably use terms “agents”, “movers”, “level”, “priority”, etc. which only deteriorates the readability of the paper. The term collision suddenly appears without prior discussion that how it relates to the objective of the paper.\n\nMinor points\n\n- Communication is not an “alternative” but is a tool that facilitates coordination.\n- Last line of page 2: “On the other hand, MARL struggles in extracting useful information …” This sentence is confusing. Do authors mean the field of MARL is struggling with extracting useful information?\n- What Stackelberg equilibrium? Why is it used and how it relates to the paper? Also a reference is needed for the sentence “It is claimed that SE … ” on page 3.\n- The multi-agent path finding section of the related work seems very disconnected and out of blue since there’s no mention of collision or path finding objectives for the method before this point. \n- On page 5, authors mention “Besides, we again use an attention module AM_{lac} to handle the input” What does “handling” mean here? It is preferred to use actual technical description of the reason behind using the attention module to improve the understandability. \n",
            "summary_of_the_review": "Overall, the paper has a good structure and is posing an interesting perspective for the decision process in collaborative MARL. The proposed sequential decision-making approach, if done right, could have been potentially an interesting contribution. However, in its current form, this paper is not suitable for publication due to several major concerns: (1) the approach seems to be centralized throughout which defeats the purpose of a method like this in my opinion, (2) lack of mathematical rigor and several ambiguities regarding key ideas of the methodology, (3) there are numerous instances of grammatical and linguistic mistakes which severely impact the readability and understandability, (4) The environments are rather small and relatively easy and therefore the results may not be conclusive, and (5) the weak performance of chosen baselines is not surprising since the author’s method appears to be fully centralized and does not seem to be dealing with partial observation either, according to the presented environment settings.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a communication scheme called SeqComm for coordinting multiagent reinforcement learning.  The main idea is that in cooperative MARL, it is difficult to determine which agent should take the lead to decide what to do, so that other agents can take appropriate actions accordingly.  The authors show that SeqComm outperforms IS, TarMAC, I2C and PPO in predator-prey, cooperative navigation, keep-away, as well as SMAC.",
            "main_review": "The main strength of the paper is that the proposed SeqComm scheme outperforms other competing schemes in some of the commonly used scenarios.\n\nThere are several drawbacks in the SeqComm scheme.\n\nMy major concern is that in the negotiation phase, SeqComm requires each agent to broadcast its observations to all other agents, so that all agents are able to determine what to do based on complete information of the system (joint observation).  I believe that this fundamentally violate the principle of MARL, that is, observations should mainly be local.\n\nA relate question is that, in the launching phase, the actions taken by individual agents are NOT broadcast, hence the agents do not have complete information of the joint action.  It is not easy to understand why observations can be broadcast while actions cannot.\n\nAlso in the negotiation phase, it is not clear how many random sequence of agents should be tried in order to obtain a value that is good enough.\n\nFinally, the scheme is currently implemented as an extension to PPO.  The authors propably need to show its performance when used in other schemes as well.\n\nMy last remark is that according to the performance figures, SeqComm does not outperform other scheme too significantly.",
            "summary_of_the_review": "Since I do not think it acceptable that observations have to be broadcast in a MARL setting (while actions are not broadcast), I tend to reject the paper.\n\nI also note that the newly proposed scheme does not outperform the competing schemes too significantly.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}