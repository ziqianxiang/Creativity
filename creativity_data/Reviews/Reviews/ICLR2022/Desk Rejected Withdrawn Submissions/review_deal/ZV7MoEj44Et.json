{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The goal of this paper is to evaluate transfer learning performance in an interpretable manner. To do so, the work develops a set of control baselines and evaluation metrics to compare augmentation-based contrastive methods with non-contrastive and supervised methods. The experiments use the control baselines to revisit basic empirical questions vis-a-vis transfer learning performance.",
            "main_review": "**Strengths**\n\n- The paper is well organised; the evaluation metrics are clearly explained and the experiment results are presented in a systematic manner. The questions targeted in the empirical analysis are quite relevant to the high-level problem of measuring progress in self-supervised learning.\n\n- The three control baselines are intuitive and are combined into the calibrated risk metric to evaluate transfer leaning performance.\n\n- The empirical result that performance improvements performance on non-classification tasks is less than improvements in classification tasks is quite interesting. As mentioned in the paper, it suggests that suggests that research on SSL methods might be more biased by over-reliance on downstream performance on classiﬁcation tasks.\n\n**Weaknesses**\n\n- The novelty in methodology is fairly limited: three control baselines (scratch, maximal-supervision, blind-guess). The blind-guess baseline is more of a sanity-check rather than a competitive/useful baseline. The scratch baseline is used in Cole et al. 2021.\n\n- The novelty in empirical findings is unclear, especially in comparison to similar papers: Ericsson et al. 2021, Kotar et al. 2021, Newell & Deng 2020, Cole et al. 2021. These papers aim to benchmark and concretely evaluate similar basic questions, arguably in a more thorough manner (multiple other hyper-parameters such as dataset domain, model capacity and task complexity are taken into account). The paper would have been much stronger if the proposed metrics/baselines shed some new insight or targeted empirical questions that are not previously considered in the literature.\n\n- Experiments can be more thorough. None of the plots include standard deviation (over multiple runs) and it is unclear whether or not the differences in improvements are significant or not. For example, it is unclear to what to (conclusively) infer from the win-rate plot.\n",
            "summary_of_the_review": "Overall, the weaknesses of the paper outweigh its strengths. While the evaluation metrics / control baselines are intuitive, the novelty in terms of methodology and empirical findings is limited. The experiments can be done in a more thorough manner as well. I'd be happy to re-evaluate my score if the rebuttal addresses my concerns.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work is a study on evaluating the self-supervised pretraining methods used for transfer learning. It uses three control baselines and computes the calibrated risk by taking into account the scratch risk and blind-guess /maximal-supervision controls. Indeed models are compared based on the two curves. ",
            "main_review": "In this paper, the performance of self-supervised training methods have been evaluated through proposed curves and interesting observations have been shown regarding this type of learning models such as contrastive self-supervised pre-training are more useful for classification tasks also the difference between contrastive and non-contrastive approaches are larger in classification tasks. It looks like this approach can be very useful to fairly and easily compare the performance of transfer learning approaches that are in the center of the attention these days.\nEven though some sections are not quite clear to me:\n\nwhat does negative calibration mean and how should it be interpreted?\n\nIn order to improve the readability of the paper, I recommend to summarize the tasks/datasets and the approaches for pretraining in a table.\n\nIn order to approve a claim regarding the usefulness of this standard limited datasets and approaches are not enough. I am curious to see whether this kind of evaluation can also be valid for text-based classification and regression tasks as well and on more datasets.\n\n",
            "summary_of_the_review": "Overall, this paper and this type of evaluation standard can be beneficial the research community to compare the transfer learning performance easily and fairly if the experiments have been conducted in a vast range (like in different domains/datasets)",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes various baselines and metrics to evaluate the performance of transfer learning methods.\n\n- Baselines: \"Scratch\", \"Maximal supervision\", \"Blind guess\"\n- Metrics: \"Calibrated risk\"",
            "main_review": "# Strengths\n\n- The metrics proposed by the paper are intuitively understandable.\n- The paper's empirics are decently broad.\n\n# Weaknesses\n\n- I am not convinced that calculating ERM risks, scaled by a suitable normalization constant, is a publishable result or methodological contribution.\n- It seems like the \"calibrated risks\" can even go negative—for example, when the gain from transfer learning exceeds the loss from using less downstream task data. It is therefore hard for a practitioner to get an intuitive sense of what good results are (other than \"lower is better\").\n\n# Other questions\n\n- Eq 2: what is $R_{g s}$?",
            "summary_of_the_review": "Overall, I find that the contributions of this paper (in essence, a proposal to report a more interpretable affine transformation of the loss function in transfer learning problems) are too minor to warrant presentation at ICLR.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Authors are motivated by a lack of agreement upon a set of control baselines, evaluations practices, metrics to report. They propose an evaluation standard to evaluate transfer learning performance by defining Calibrated Risk by adjusting the empirical risk with respect to dataset bias, architecture contribution, and upper bound performance. To verify the methodology authors perform a case study to assess the effectiveness of self-supervised models. ",
            "main_review": "- The proposed methodology summarizes information about the method's performance in one measure that seems to be less interpretable. Figures 2, 3 show that there are some cases where Calibrated Loss is negative. There is a need for stronger clarification on how one should interpret such results. And compared to $cR_f-n$, there are no bounds other than the scratch.\n  - Given a note on the CIFAR100 result, the method might not be applicable in a low-data regime. \n- The results are shown for one particular run.  What will we get in the case of cross-validation or leave-one-out? \n- The experiments could address each adjustment term (dataset bias, architecture contribution, upper bound performance) separately including the variance of the empirical risk.\n- Section 3.2. The example in Motivation is trivial. For example, it is common to use other metrics as ROC AUC or consider the existence of the trade-off between sensitivity and specificity. \n\n- Section 5.1: The conclusion is too strong just by comparing only SwAV on the different tasks. Authors are missing significant results with self-supervised visual transformers as related work (e.g., DINO [1], EsViT [2])\n- Section 5.2: The training curves confirm the common knowledge that self-supervision is effective in low data regimes (data-efficient) (e.g., CPC v2 [3]). Also, it does not seem helpful to compare different tasks on the same scale. In practice, we might need to understand trade-offs between tasks. Some inductive biases are good for some tasks, but not needed for other tasks.\n- Section 5.3: There is no comparison with non-contrastive BYOL or SimSiam. The curves confirm only the common knowledge as it is known from the benchmarks that pretext tasks usually are outperformed with contrastive learning [4].\n- Section 5.4: It is not clear if it is a fair comparison when the models have been trained to the best by different parties. There are too many factors of variation (e.g., training strategy) that can benefit one method over another.\n\nOther:\n- Related work (Section 2, Evaluation Metric, and Baselines): Authors could clarify linear classifiers by relating and explaining them as linear probes [5].\n- Figures 2,4 with curves are tiny for a printable version.\n\n[1] Caron, Mathilde, et al. \"Emerging Properties in Self-Supervised Vision Transformers.\" (2021). https://arxiv.org/abs/2104.14294\n[2] Li, Chunyuan, et al. \"Efficient Self-supervised Vision Transformers for Representation Learning.\" arXiv preprint arXiv:2106.09785 (2021). https://arxiv.org/abs/2106.09785\n[3] Henaff, Olivier. \"Data-efficient image recognition with contrastive predictive coding.\" International Conference on Machine Learning. PMLR, 2020.\n[4] Bachman, Philip, R. Devon Hjelm, and William Buchwalter. \"Learning Representations by Maximizing Mutual Information Across Views.\" Advances in Neural Information Processing Systems 32 (2019): 15535-15545.\n[5] Alain, Guillaume, and Yoshua Bengio. \"Understanding intermediate layers using linear classifier probes.\" (2017). https://arxiv.org/abs/1610.01644v1\n",
            "summary_of_the_review": "Overall, the authors fail to show why this formulation is correct and why this approach is better than a standard evaluation practice for different scenarios. The methodology does confirm common findings with self-supervised learning but it is not enough to consider this approach to be generic. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}