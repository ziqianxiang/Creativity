{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors define a decidability-based loss (D-loss) to solve the Low convergence issue of the existing metric learning algorithms.",
            "main_review": "Advantages:\n1. The paper is easy to follow.\n2. The authors propose a different way to define a new metric learning loss function. \n\nDisadvantages:\n1. The experiments are insufficient. MNIST、CIFAR and CASIA-Iris datasets are not usually used for other metric learning papers. The authors should compare the proposed D-loss with other state-of-the-art methods on some widely-used datasets such as CUB-200-2011、Stanford-Online Products and Cars196.\n\n2. The ablation study is missing. The authors should discuss some important hyper-parameters such as batch size, sampling strategies of each mini-batch, and so on. \n\n3. Lots of related works such arcface[1], cosface[2], circle loss[3], N-pairs[4], etc. are missing in comparisons.\n\n[1] Deng, J., Guo, J., Xue, N., & Zafeiriou, S. (2019). Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 4690-4699).\n\n[2] Wang, H., Wang, Y., Zhou, Z., Ji, X., Gong, D., Zhou, J., ... & Liu, W. (2018). Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 5265-5274).\n\n[3] Sun, Y., Cheng, C., Zhang, Y., Zhang, C., Zheng, L., Wang, Z., & Wei, Y. (2020). Circle loss: A unified perspective of pair similarity optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 6398-6407).\n\n[4] Sohn, K. (2016). Improved deep metric learning with multi-class n-pair loss objective. In Advances in neural information processing systems (pp. 1857-1865).",
            "summary_of_the_review": "The authors define a decidability-based loss (D-loss) to solve the Low convergence issue of the existing metric learning algorithms. It is easy to follow. However, the experiments are insufficient. Therefore, I tend to reject this submission.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a D-Loss based on the decidability index and the computation is easy to implement. The authors compare the D-Loss with Triplets Soft-hard Loss, Multi-Similarity Loss, and Softmax-Loss on the datasets of MNIST FASHION, CIFAR-10, and CASIA-IrisV4. Under the same condition, the D-Loss achieved comparable verification performance against others.",
            "main_review": "Strengths:\n1)\tThe writing is easy to follow. The code is available, and the reproduction is thus guaranteed.\n2)\tThe experiments on several datasets verify the effectiveness of the proposed D-Loss for biometric learning problems.\n\nWeakness:\n1)\tSince this paper focuses on biometric verification learning, the comparison against the state-of-the-art loss functions widely used in face/iris verification should be added (e.g., Center-Loss, A-Softmax, AM-Softmax, ArcFace).\n2)\tCosine similarity score is more often used in biometric verification, so I wonder if it would work better than the Euclidean distance when computing the Decidability.\n3)\tLarge batch-size may be significant in the proposed loss. The authors conducted on three settings to select the best number of batch-size. However, it may be better to examine the performance with more settings. For example, what would happen if a small batch-size is used.\n4)\tWhy triplet loss cannot convergent on CASIA-V4? I guess many previous iris verification works have employed such loss.\n5)\tFigure 5 shows the impact of the D-Loss before and after training the model. It is suggested to compare with other losses on it.\n",
            "summary_of_the_review": "The experiments are not sufficient to support the advantages of the proposed D-Loss.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes an alternative metric learning loss that considers optimizing the statistic characteristic of data in minibatch. The authors claim their loss function has comparable performance with definitive loss functions on common metric learning benchmarks.",
            "main_review": "Basically, I believe the theory and technique that the authors proposed in this paper could not be claimed as some creative or new idea since the optimization goal of their D-loss is essentially the same as the definitive triple loss. As defined in Eq 5, their primal goal is to maximize the means while minimizing the deviation of those scores. However, the mean and deviation of those distance scores are actually already calculated in triple loss when you implement it in a batch of data. The only difference with triple loss is that this one optimizes their division factor instead. So I think the main contribution of this paper is not enough.\n\nSome other weaknesses are: (1) the writing skill of this paper is poor. (2) the algorithm is not optimized. (3) their experiment result does not show competitive results. ",
            "summary_of_the_review": "Overall, I think the contribution and academic level of this paper are not enough to be published.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a D-loss function based on the decidability index for the CNN training. Authors have evaluated this loss by calculating the recall rates on four publicly available datasets, including MNIST, fashion-MNIST, CIFAR-10, and CASIA-IrisV4, and compare it with the softmax cross-entropy loss, triplet loss, and multi-similarity loss.",
            "main_review": "This paper is easy to read and is an important positive aspect of this paper. However there are several serious technical problems with this paper and can be outlined as in the following.\n\n1)“ we propose a new loss function, the D-loss, based on the decidability index (Daugman, 2000). Daugman (2000) highlights this index as a quality measure for biometric systems, which is often used in the literature for this purpose (De Marsico et al., 2018; Luz et al., 2018)”\n\n-> Please note that the decidability index that was ‘used’ 20+ years ago has never been used as the performance or quality metric for biometrics systems. The performance metric is the ROC and accordingly the FRR at low FAR values. This can also be cross-checked from any of the IRIX reports from the NIST evaluations.  \n\n2) A fundamental assumption in such DI index computation is the assumption that the match score distribution can be modeled as Gaussian. It is impossible to assure the Gaussian distribution using such a small batch. Also, we can find that it is not Gaussian distributed as the authors' presented plot in Figure 5b. Therefore its not theoretically correct.\n\n3) The MINST, Fashion-MINST, CIFAR-10 datasets are small and low-challenging to validate a practical loss function in a convolutional neural network. Nowadays, these datasets are more used in unsupervised learning, such as MoCo (He et al., 2019) and SimCLR (Chen et al., 2020). They can also achieve similar results without any supervised label information. Therefore it cannot convince others about its effectiveness if the authors do not perform experiments on large-scale datasets, e.g., ImageNet (Krizhevsky, Sutskever and Hinton, 2012) or COCO dataset (Lin et al., 2014).   \n\n4) The authors also perform biometric experiments on the CASIA.v4 interval iris dataset, but they do not evaluate it properly. We can find that the Recall rates have already saturated for the softmax cross-entropy loss and the proposed D-loss. It is not an adequate evaluation for a biometric system if the authors do not provide the receiver-operating characteristics curves (ROC) and the genuine accept rates at low false accept rates. Also state of art methods should be used to benchmark the performance for this dataset.\n\n5)  Authors do not provide any matching protocol for CASIA-IrisV4, and it is not predefined. Without such experimental details, this work is not reproducible.\n\nReferences used in above review:- \nChen, T. et al. (2020) 'A Simple Framework for Contrastive Learning of Visual Representations', 37th International Conference on Machine Learning, ICML 2020. International Machine Learning Society (IMLS), PartF168147-3, pp. 1575–1585. Available at: https://arxiv.org/abs/2002.05709v3 (Accessed: 31 October 2021).\nHe, K. et al. (2019) 'Momentum Contrast for Unsupervised Visual Representation Learning', Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, pp. 9726–9735. Available at: https://arxiv.org/abs/1911.05722v3 (Accessed: 31 October 2021).\nKrizhevsky, A., Sutskever, I. and Hinton, G. E. (2012) 'ImageNet classification with deep convolutional neural networks', in Advances in neural information processing systems, pp. 1097--1105. doi: 10.1145/3065386.\nLin, T. Y. et al. (2014) 'Microsoft COCO: Common objects in context', in Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). Springer Verlag, pp. 740–755. doi: 10.1007/978-3-319-10602-1_48.",
            "summary_of_the_review": "This paper has theoretical flaws as explained above. The proposed loss function is not well evaluated on desired datasets and baselines. The protocol is also not provided, and therefore it is impossible for the readers to reproducible the results. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}