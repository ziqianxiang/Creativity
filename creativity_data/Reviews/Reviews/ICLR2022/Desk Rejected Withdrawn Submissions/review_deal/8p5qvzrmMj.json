{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a Confidence score Weighting Adaptation (CoWA) to tackle the source data-free adaptation. It utilized the GMM to obtain the pseudo label and refined them using the log probability gap. Further weight mixup and data augmentation techniques are used to get improved performance.",
            "main_review": "The paper has many technical and logical problems; some of them are listed as : \n\n  --Regarding the MINGAP :  The authors claim that the mingap performs well as compared to entropy and max probability, as shown in the example in the paper. \nBut I have a counterexample for this claim. \nFor example $p(x_i)=[0.6,0.2,0.1,0.1,0,0]$ \n $p(x_j)=[0.5,0.1,0.1,0.1,0.1,0.1] $ \nthen mingap: $p(x_i)= 1.1$ and $p(x_j)=1.6$ \nbut here, $p(x_i)$ should be the clear winner for class 0.\n In this case, simple max probability gives better label knowledge. \n\n --The authors used the JMDS score offline (due to heavy computational), but it may lead to incorrect labels as network weights are updated in every iteration. Authors should obtain or refine the JMDS score in some interaction (let's say ten or maybe 20).   --What is the benefit if the gradient of SCE loss reached 0.5 compared to CE, for which the max value is 0? I am not getting why this max value of gradient should be 0.5 or not should be 0. \n\n -- Regarding the mixup: The mixup idea is used here to utilize the samples with lower confidence scores, but when mixing with other data, their weights are still lower, so how the prosed algorithm claims that it can use the lower confidence data too.\n\n  -- Since this paper uses the three types of losses by their tuning parameters, there should be a study about the values of the tuning parameters  ($\\lambda_ {aug}$ and $\\lambda_{mix})$. I do not find any discussion on the setting of hyperparameters. The authors cited in many places about the appendix, but I did not find out any appendix/supplementary in this submission.\n\n--The authors trained five different source models; what is the meaning of different models? They have different architecture or different training algorithms. If they have the same architecture, data, and algorithm, how can we categorize them as best or worst based on epoch? \n\n--In Table 1, the proposed method performs better when using the worst source model for more difficult adaptation tasks ( $A\\rightarrow W$ and  $A\\rightarrow D$). It is counter-intuitive since the proposed algorithm uses the prediction ( pseudo labels).\n If predictions ( from the pre-trained model) are better, the proposed method performs poorly. But it performs better when pre-trained model predictions are bad. Would you please clarify this?\n\n  -- Minor: typos and clarification:  The eq 1 is not properly clear. What are $\\kappa_1$ and $\\kappa_2$? Define them clearly.\n-- Do LPG is calculated over a batch? what is $j$ index in Eq 4.",
            "summary_of_the_review": "The paper is not clearly elaborate its proposed method, it is not convincing how the mingap is better. The improvement of the performance is due to several components ( mixing and augmentation, which are not novel enough). So paper needs a major revision in terms of clarification and novelty.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper targets the problem of source data-free domain adaptation. The authors propose a sample-reweighting method called confidence score weighting adaptation (CoWA). The components of CoWA include the joint model-data structure confidence score, suppressed cross-entropy loss, weight mixup, and data augmentation. Empirical ablation studies demonstrated the effectiveness of each component. CoWA achieved state-of-the-art results on three domain adaptation benchmarks including Office-31, Office-Home, and VisDA-2017.",
            "main_review": "### Strengths\n- The proposed method is technically sound.\n- Experimental performance of the proposed method is impressive.\n- The motivation to refine pseudo labels with multiple views, i.e., features and predictions, is reasonable.\n\n### Weaknesses\n- Although reasonable, the proposed method lacks novelty. It combines many existing techniques, including features modeling with GMM [A], pseudo labeling, data augmentation, and mixup. Using some form of confidence to reweight samples is quite common. I think the only novelty lies in the proposed JMDS score.\n- Related works are confusing, incorrect, and missing. First, in the introduction, the authors classify related works of domain adaptation into two paradigms: minimizing the domain discrepancy and obtaining domain-invariant features with adversarial learning. There are some issues here. It is inconsistent with the content in the related works, where all related works are classified into learning domain-invariant features. Second, I think the most related works to this submission is pseudo labeling (PL). PL has been widely used for domain adaptation, to name a few [B, C, D, E, F]. But none of them are mentioned. Third, the authors mention related works of learning with noisy labels, but one of them is compared in experiments. Overall, I recommend refining the writing to make the background consistent, self-contained, and comprehensive.\n- Details about GMM training, hyperparameters, and method implementation are missing. \n\n### Other comments\n- The SCE objective is unclear. For me, it is a maximum confidence-weighted CE objective. In the paragraph before Section3.3, \"The gradient of SCE is ... compared to CE\" is confusing. According to the quadratic formulation, the minimum of the gradient is at P_M=0.5, and the maximum is at P_M=0 or P_M=1. SCE seems to suppress the medium confidence samples and encourage low or high confidence samples.\n- The source data-free domain adaptation is a quite realistic problem. Is the method easy to implement and apply to realistic DA settings? My concern is that GMM training in each epoch would be time-consuming and lots of hyperparameters would be hard to determine.\n\n### References\n[A] VDM-DA: Virtual Domain Modeling for Source Data-free Domain Adaptation, arxiv\n\n[B] Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification, ICLR'20\n\n[C] Cluster alignment with a teacher for unsupervised domain adaptation, ICCV'19\n\n[D] Confidence regularized self-training, ICCV'19\n\n[E] Source Data-absent Unsupervised Domain Adaptation through Hypothesis Transfer and Labeling Transfer, TPAMI'21\n\n[F] Unsupervised domain adaptation for semantic segmentation via class-balanced self-training, ECCV'18\n",
            "summary_of_the_review": "Overall, this submission proposes a reasonable solution to source data-free domain adaptation with competitive results. However, the current submission lacks the novelty of the method, has confusing writing, and misses necessary details of the method or implementations. Therefore, I give a rating of weak reject.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This work seeks to address the source-free domain adaptation problem. The proposed CoWA model represents the latent space structure with GMM and update such structure per epoch. By combining the data likelihood derived from GMM and classification probability from the predictive model, a JMDS metric is designed for per-sample confidence weighting. By further incorporating the technique of mixup and data augmentation, CoWA further enriches the resources of training data. On several standard benchmark datasets, CoWA consistently outperforms existing source-free UDA and UDA approaches. Also, plentiful ablation studies are conducted to verify the effectiveness of major model components. ",
            "main_review": "I admire the idea of incorporating the knowledge from both supervised predictive model and unsupervised clustering model, which should benefit from each other. However, I have some concerns on the novelty and soundness of the proposed model:\n\n1. I feel the technical contribution of the proposed method is not enough. Specifically, the main techniques, i.e. GMM, mixup training and data augmentation, are quite standard in applied machine learning. Also, the so-called suppressed CE loss is just a standard CE loss reweighted by online model confidence. Although it is nice to hear that the combination of these techniques shows superiority on source-free UDA, I do not see too much application-specific model design aiming to address the key problems of source-free UDA. For example, though the source-domain pre-trained model can produce informative representations on source domain, it is not guaranteed that its representations on target domain are also well separated. Therefore, the GMM results in the early training phase can be uninformative and even misleading. Can we use a progressive scheme that gradually enhance the reliance on LPG confidence and further promote model's performance?\n\n2. In CoWA, the GMM scheme is performed every epoch to derive latent representation structures. However, the image representations could vastly change during one epoch training, which could make the GMM results not so representative. Can we online update the GMM parameters with mini-batch statistics (e.g. mean and variance) to somehow ensure the GMM structure up-to-date? I think at least an ablation study and some GMM visualization results are needed here to explore this model more extensively. \n",
            "summary_of_the_review": "In general, I admire the core idea of this work, i.e. incorporating the supervised information from source domain and the unsupervised target domain data structure. However, I think some parts of the proposed model are not explored enough in the current draft, especially the various GMM model designs under the context of source-free UDA. I will consider to upgrade my rating based on authors’ response.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper provides a new approach for source data-free domain adaptation, where only the trained source model is given from the source domain to protect data privacy. The main contribution of the proposed approach lies in a sample-wise weighting strategy called Joint Model-Data Structure (JMDS) score. Compared to previous data-free and data-dependent methods, the proposed approach achieves better results on three closed-set unsupervised domain adaptation benchamrks.",
            "main_review": "This paper is well written and easy to follow, and the experimental results are promising, especially for VisDA dataset. Some limitations of this paper are summarized as follows: \n\n1.The novelty of this work is somewhat limited. The proposed method consists of two steps: (1) estimating pseudo-labels and their confidence via GMM and (2) updating the model by learning with pseudo-labeled data. The first step aims to correct pseudo labels, and the second step aims to robustly update the model against label noise via sample importance weighting. Therefore, this method is actually a method of learning from noisy labels. Since both clustering-based label correction and learning with sample importance weights are some well-known methods, the novelty of this study should be discussed more carefully comparing those methods. \n\n2. As we know, label noisy is also a big problem. To address label noisy, many assumptions are needed. As shown in paper Confidence Scores Make Instance-dependent Label-noise Learning Possible (ICML 2021), you should state what assumption has been used in your paper related to label noisy. In addition, source free domain adaptation also needs suitable assumptions. Could you clarity it ? That is what I am really interesting in. As I know, it is impossible to achieve a transfer without any suitable assumptions.\n\n3. MixUp technique has been used. From ablation study, I find it is important. I like this technique. Many unsupervised domain adaptation method also uses the technique such as How does combined risk affects the performance of unsupervised domain adaptation (AAAI 2020). I think it is better to use the method mentioned above as a baseline (it seems the method proposed in above paper has better performance than the baselines you have used).\n\n4.  Please discuss the technological limitations of this work. ",
            "summary_of_the_review": "I recommend reject this paper, and encourage the authors to submit their work to the next top conference, by conducting a major revision to address the limitations.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}