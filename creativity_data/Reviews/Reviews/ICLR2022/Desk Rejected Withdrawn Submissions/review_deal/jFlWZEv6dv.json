{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a new data augmentation, AlignMix, based on recent mixup methods which interpolate between two images or feature maps. Unlike existing mixup methods, AlignMix aligns two feature maps before interpolation. It calculates the pairwise similarities and finds the optimal assignments between two feature maps. It then transforms one of the feature maps by using the assignment before mixing it up with the other feature map. Experiments show that AlignMix outperforms existing methods on CIFAR, ImageNet and out of distribution detection, and makes the networks more robust to adversarial attacks.",
            "main_review": "The main advantage of AlignMix is that it is a simpler method as it does not require a decoder, while recent mixup methods which often require additional networks to interpolate between the images. Hence, it trains faster. At the same time, it also outperforms recent mixup methods without the decoder, and achieves even better performance when using a decoder.\n\nThe main idea of AlignMix is to deform one image into another based on their semantic correspondence in the feature space. However, it is very likely that the semantic correspondence between two random images, for example an image of a car and an image of a dog, is very weak. So it seems to me that this approach wouldn’t make sense in such cases. It would be great if the authors can explain how the proposed approach would be doing in such cases. So far, the authors have only shown some qualitative examples of applying AlignMix to two animal images. The authors should include some examples of applying AlignMix to two images which might not have strong semantic correspondence.\n\nThe organization for this paper needs to be improved. Since AlignMix actually includes methods from prior works as it randomly chooses one approach during iteration, it is important to show how each component contributes to the final performance. Although the authors do show such ablation studies in the appendix instead of the main paper, they should move it to the main paper.",
            "summary_of_the_review": "I think this approach is simple and effective but it doesn’t seem to be reasonable when applied to images that have weak semantic correspondence.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This work proposes a new mixup approach, AlignMix,  for data augmentation. It aligns two inputs in the feature space by finding similar feature pairs and aggregating them. The transformed and original features are mixed to form a new feature. Through visualization, it finds out that the mixed feature keeps the ‘pose’ of one image and appearance of the other. This introduced approach obtains better performance on five different tasks, classification, robustness, calibration, weakly-supervised localization and out-of-distribution detection.\n",
            "main_review": "\n- The proposed method of aligning features is interesting. Results on five different tasks have verified the effectiveness of this approach. Authors also conducted ablation studies to investigate the performance with different settings and parameters. \n\n\n\n- I feel the proposed approach is similar to cross-attention. If I understand it correctly, this approach first checks the similarity between the feature on one location in A and features on all locations in A’.  Then the matrix P is computed to obtain coefficients for aggregating in favor of similar features. It is similar to a  simple cross-attention module in terms of operations on one key in A and all queries(values) in A’, and there are no learned projection matrices. The other difference is that the proposed methods computes the P globally(i.e. for all feature pairs). I wonder if we can try the mixup with a simple cross-attention, and see the performance? Correct me if I am wrong\n- In table 8, I wonder if we could have an ablation study of having only A in a set of layers? I am curious whether x is essential here. \n\n\n- Minor:  There is a missing related work [1] which did style mixup on means and variances of two different features. \n\n[1] CrossNorm and SelfNorm for Generalization under Distribution Shifts. ICCV 2021\n",
            "summary_of_the_review": "A new mixup method is proposed in this work, and achieves better performance on different tasks. I just have a concern whether it could be achieved by using a simple cross-attention module. However, as far as I know, it is the first one to apply it to the mixup operation. At the current stage, I am on the borderline to accept or reject.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed AlignMix, an improved version of Mixup. The key insight is to compute geometrical alignment between two images in the feature space, which allows interpolating between two images. The feature correspondences are obtained via Sinkhorn distance. \n\nThe paper shows improvement in image classification, adversarial attacks, calibration, weakly supervised localization, and out-of-distribution detection. \n",
            "main_review": "**Strengths**:\n\n1. The proposed approach is simple\n2. The idea is clearly presented\n3. The proposed method provides improvement on five different tasks\n\n**Weaknesses**:\n\n1. The novelty is limited. the idea is to have a mixed image with style and content from different images, which is similar to StyleMix.  \n\n2. The proposed approach needs more computational resources to obtain good performances: 2000 epochs on Cifar10/100 (300 epochs in StyleMix). How long does the entire training take? In some cases, it might be a problem. I would suggest to precise more details about the computational resources in the paper. Moreover, as the approach takes longer to train and the inference is the same for all the competitive approaches, it is not convincing that the approach is more efficient in terms of computational complexity.  \n \n3. Table1.b Results are not comparable, some approaches seem to be trained much fewer epochs than the proposed one. For example, StyleMix only trained for 100 epochs, while the proposed approach trained for 300 epochs. It would be better to clarify this difference by also reporting results in Table.5.\n\n4. Some important training details are missing, for example, image resolution plays an important role in the performance of image classification[1]; Are there other data augmentations used during the training?\n \n5. During the training, the proposed algorithm needs to sample from 4 (5 for alignMix/AE) cases, I would suggest an analysis on these cases. For example, we could remove one case to see how the performance drops. The goal is to understand where the improvement comes from. \n\n6. It is interesting to see how the proposed approach improves over state-of-the-art approaches. For example: Deit[2], Swin transformer[3] etc.\n\n\n**Clarity**:\n\nThe approach is in general well-presented, some details are missing:  \n\n1. Table 1.b: it would be better to precise MESC/BATCH also in the caption of the table. \n\n2. PreActResnet18 seems to be different from the original paper [He et. 2016], if so, please clarify the difference. \n\n3. Supplementary B. Parameter setting. The following description is confusing: “We also train R-50 on ImageNet for 100 epochs, following the training protocol described in Kim et al. (Kim et al., 2021).”  I guess it refers to the results in table 5, but the associated text should be also there. \n\n4. The description of the architecture is confusing, the dimension of the output A should be different for CIFAR10 and ImageNet. \n\n5. The ablation study is hard to follow (Appendix C). The notations are confusing: “c $\\in$ $R^{512}$” but “e is a 128 $\\times$ 2 $\\times$ 2 tensor”. Different sets in the column LAYERS (TABLE 8) are also hard to understand. I would suggest revising this part and making the messages clear. \n\n[1] Touvron, H., Vedaldi, A., Douze, M., & Jégou, H. (2019). Fixing the train-test resolution discrepancy. NeurIPS 2019\n\n[2] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., & Jégou, H. (2021, July). Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning (pp. 10347-10357). PMLR.\n\n[3] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. ICCV 2021\n\n",
            "summary_of_the_review": "Although the novelty is limited, the overall approach is simple and effective on different tasks.\nThe paper can also be improved by adding more details and analysis. \n \nGiven the strengths and weaknesses, my initial rating is borderline accept. \n\nI could change my rating if my concerns are properly addressed. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I don't have ethical concerns about this work. ",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}