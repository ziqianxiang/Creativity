{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a new architecture for multimodal fusion by using a self-supervised module, called Refiner, that refines multimodal representations using a decoding/defusing module applied downstream of the fused embedding. Essentially, Refiner learns a joint embedding space using metric learning that brings multimodal representations of similar points closer together and others far apart.\n\nOn several multimodal fusion datasets, their proposed method achieves stronger performance.",
            "main_review": "Strengths:\n1. The paper is reasonably well-motivated and well-written.\n2. Experiments are conducted on a few datasets and results are promising.\n\nWeaknesses:\n1. Not really sure what the 'responsibility condition' is formally defined as and the motivations for this are quite vague.\n2. I would have expected ablation studies to ablate each part of the model (e.g., refiner module, multi-similarity loss) instead of just changing the percentage of available data.\n3. Not sure what figure 4 'degree of unimodality' means and the main takeaway message from figure 4.\n4. The model does show better performance but at the expense of more parameters and training time, so it would be good to discuss the trade-offs here.\n5. It would be good to benchmark more comprehensively on a multimodal benchmark like this: https://arxiv.org/abs/2107.07502\n6. I am confused why there needs to be a reconstruction module in the model, it seems like refining/similarity loss can be applied just in a supervised setting without needing to reconstruct the inputs. Some explanations and ablation studies here would be helpful.\n7. Quite a few writing and formatting issues, reference format needs to be fixed too.",
            "summary_of_the_review": "Overall, the idea is interesting and there are some promising experimental results. However, there remain major issues with the motivation, ablation studies, and quality of the writing + presentation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The submission proposes a \"REFINER\" module which can be plugged into an existing multimodal pretraining frameworks, such as VilBERT, as an additional loss. The loss aims at reconstructing unimodal features (e.g. inputs to VilBERT) from the multimodal outputs of a pretraining framework, using MSE, or contrastive loss, etc. The motivation for this is to \"ensure ... unimodal and fused representations are strongly encoded in the latent fusion space\". The authors demonstrated the effectiveness of their proposed auxiliary training objective by adding it to VilBERT and MMBT. Experiments on \"Hateful Memes\" and \"Multimodal IMDB\" benchmarks show improvement over the baselines without \"REFINER\", especially in the lower-shot setup.",
            "main_review": "+ The observation that the encoded \"fusion embedding\" by VilBERT could lose \"unimodal\" information is interesting.\n+ The proposed solution is simple and can be applied to any existing multimodal pretraining frameworks. Good performance on the included benchmarks.\n- I'm confused about the novelty claimed by the authors. Specifically, isn't the feature reconstruction loss exactly what has been implemented by VilBERT (Figure 3a in the original paper)? Although the original VilBERT only used MSE to \"reconstruct\" the encoded image and text features, there are other work (e.g. [a]) used contrastive learning objectives. My confusion also arises from Figure 6 in the supplementary of the submission, where it seems a single \"fused embedding\" is returned by the \"fusion module\", whereas most of the pretraining frameworks, due to the use of transformers, return sequential outputs. Could the authors clarify which output from VilBERT did they choose as the \"fused embedding\"? According to my current understanding of the proposed method, it appears to me that either the authors used a subset of the possible outputs from the baselines (so that the baselines performed worse), or their proposed method has already been implemented by the baselines.\n- On a high-level, why is it desirable to ensure \"unimodal and fused representations are strongly encoded in the latent fusion space\", give the goal of the additional transformer encoder is to combine the information? If a unimodal reconstruction loss is necessary to ensure unimodal information is not lost, why not directly augment the output of VilBERT with its unimodal inputs? This would make an essential baseline to the proposed method.\n- Closely related to the above point, how task-dependent are the current observations? It is especially important considering the authors introduced the OCR modality as input, and avoided the standard benchmarks that VilBERT and numerous vision-language pretraining frameworks have been using.\n- Although some are described in the supplementary, more details on the baseline approach would be beneficial to reproduce the results, and understand the contribution clearly.\n- I would avoid describing the loss as \"self-supervised\" given the unimodal representations as inputs are supervised pretrained. It would also be extremely helpful to clarify what \"responsibility\" refers to.\n\n[a] Learning Video Representations using Contrastive Bidirectional Transformer.",
            "summary_of_the_review": "The paper proposed an auxiliary loss for existing multimodal pretraining frameworks, and made interesting observations on how existing approaches might lose \"unimodal\" information. However, the reviewer has serious concerns on the claimed technical novelty when compared with the baselines, the lack of important ablation studies, and the selection of benchmarks to use. Some concerns are due to the lack of implementation and experimental details from the main submission. Hence at the current stage I recommend rejection of the submission.\n\n**Post rebuttal**\n\nI really appreciate the authors' effort to provide additional results and a detailed rebuttal. The new unimodal results are strong and relevant. However I still do not fully follow the authors' explanation (e.g. unfortunately I do not fully understand from the rebuttal why \"optimal fused representation ... should be grounded on information from upstream of the combined representation\" was not realized by earlier VL frameworks; I also had a hard time connecting \"... imposing some invariance in its representation\" with the proposed method). Finally, I still believe that results on the standard benchmarks used by many existing VL pretrained models (e.g. VQA) should be included even if the proposed method does not lead to a gain, that would help the readers understand when the proposed method would be useful.\nDue to the reasons above, I cannot recommend acceptance at the moment.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper presents a self-supervised module that refines representations after multi-modal fusion. The module called REFINER aims to reproduce the pre-fusion representations of each modality from post-fusion embedding. This ensures that the information about each modality is encoded in the fused embedding. The authors experiment with the addition of this module on top of existing fusion methods, such as VILBERT and MMBT, as well as concat, linear sum, etc. The experiments show improvement over baselines under most conditions.",
            "main_review": "The additional objectives are simple, and it is practical in that it can be applied to many fusion methods. The method shows improvements over baselines. The choice of dataset for the experiments is reasonable.\n\nHowever, it is not clear where the improvements come from. It could be that each modality embedding is captured well in the fused embedding, but it could also be just some regularisation effect. It would be helpful to evaluate on uni-modal downsteam tasks.\nThere is no consistent improvement from metric learning, which is claimed as a key contribution.\n\nThe structure of the paper is fine, but there are lots of errors in writing (e.g. networks does not, N-Vidia Volt100, image and test – text I guess?). \n",
            "summary_of_the_review": "The addition of per-modality reconstruction module contributes positively to performance. However, more analysis can be done to demonstrate the meaning of the gains.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper introduces a novel fusion module namely REFINER for multi-modal information fusion. Specifically, the method can be divided into two steps: (1) A self-supervision learning-based method is proposed for multi-modal representations learning. (2) The fused embedding is applied in the downstream task. In addition, when combined with metric learning, the proposed method can further separate the clusters across modalities and further boost performance.",
            "main_review": "Strengths :\n(1) The proposed fusion module REFINER could integrate multiple modalities such as vision, text, and image into a uniform expression for downstream tasks learning. \n(2) After being coupled with metric learning, the REFINER module can be leveraged on unlabeled data.\n(3) Compared with some baseline models, the proposed method can get better performance gains.\nWeaknesses:\nIn this manuscript, the weaknesses are mainly embodied in the following aspects: \n(1) Innovation, that I am optimistic about the proposed integration module, but I think the novelty of this manuscript is not always sufficient.\n(2) Experiments, that some recent work seems to be missed. For example, on the Hateful Memes dataset, I find that NIPS2020[1] has reached a relatively high SOTA, while the results of this article are relatively low compared to it.\n[1] Kiela, D., Firooz, H., Mohan, A., Goswami, V., Singh, A., Ringshia, P., & Testuggine, D. (2020). The hateful memes challenge: Detecting hate speech in multimodal memes. arXiv preprint arXiv:2005.04790.",
            "summary_of_the_review": "First of all, I think the novelty of this manuscript is not always sufficient. Secondly, the experimental results are quite different compared to some other SOTAs, which somewhat makes it hard for readers to convince the effectiveness of this method.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}