{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors proposed a method for self-supervised representation learning which builds upon recent attempts based on ViT. In contrast to them, the proposed framework learns from both global and local information yielding better performances. Assuming the positive patch samples given arbitrary query one can be obtained nearby regions, the proposed method first restrict the positive candidates within local window and then select hard positives based on similarity scores. To denoise false-positive samples, the authors utilize an attention-based aggregation module for pruning and summarizing the selected neighboring patches. Experiments are conducted on three datasets, COCO object detection and instance segmentation, ADE20K semantic segmentation, and DAVIS 2017 video instance segmentation, surpassing previous methods.",
            "main_review": "** Strength **\n1. It is interesting to explore better samples, particularly positive ones, for patch-level contrastive learning.\n2. Experiments are extensively conducted on various dense prediction tasks\n\n** Weakness **\n1. Novelty\n- The idea of learning with both global and local information seems to be similar to the one of DenseCL and of mining better samples in contrastive setting to the ones of  [A], [B]. It would be great if the authors acknowledge them more and highlight the differences to them.\n- Consequently, the contribution lies in mining hard positive samples, but the authors rely on cosine similarity metric which might not be guaranteed in early training as the representations are not matured. The following attention-based aggregation is also borrowed from existing works without tailored modifications.\n\n2. Experiments\n-  As the method has a component for pruning the false positive samples, I am wondering if there is any support for the assumption that restricts the candidates of positive samples within the neighborhood window. Current ablation studies fixed the size of neighborhood as 9 and the ones with respect to the size of neighborhood are missing. \n- Is there any chance to compare with the baselines where PASS applied to ViT based self-supervised methods, MoCo-v3, MoBY?\n- It would be great if there are any demonstration or visualization how the aggregation module rejects outliers along with the training time.\n\n-- References\n[A]: Contrastive Learning with Hard Negative Samples, ICLR'21\n[B]: Conditional negative sampling for contrastive learning of visual representations, ICLR'21",
            "summary_of_the_review": "Overall, the idea of exploring better patch samples for ViT based self-supervised learning is interesting, but the components seem to be lack of novelties. I would be certain after rebuttal phase.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a ViT specific self-supervised training scheme based on patch embeddings. It combines embedding-based self-supervised learning MoCo with transformer architectures. The method mainly exploits the locality of patches to boost the self-supervisory signal. The presented framework yields some improvements over baselines in object detection, instance and semantic segmentation.",
            "main_review": "+ Well written, easy to understand and follow\n+ Main technical method is quite well motivated and a seemingly novel twist on self-supervised learning with ViTs.\n- The overall experimental numbers are a bit disappointing. The improvement over baselines is within 1pt accuracy on both detection and instance segmentation, at a point that is close to 20pt removed from the state of the art. At this level, it is quite uncertain that the presented method will make an impact in these problems, and weather any improvement would just be invalidated by better and more performant backbones, detection architectures, or by now standard longer schedules. The same is true for DAVIS 2017 and to a letter degree ADE20k.\n- The ablations somehow indicate that minor changes in the overall setup (removing components Tbl 3a, or changing hyperparameters Tbl 3b) completely negate any advantage the method had. Why would removing a component or changing a hyper-parameters leads to a performance below the supervised baseline?",
            "summary_of_the_review": "The paper presents an interesting twist of self-supervised learning for patch-based transformers, but falls short in validating the efficacy of the method experimentally.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a local loss to enhance the locality during self-distillation. Therefore, the pretrained model will be more suitable for object detection and semantic segmentation. The authors show the results that outperform the DINO counterpart.\nMoreover, the authors pointed out that the attention achieved by DINO always focuses on the object only even the query is the background, and the proposed method can resolve this issue such that the proposed method improves performance on the dense-prediction tasks, especially semantic segmentation.",
            "main_review": "Strengths\n\nOverall, the idea is interesting, using class-attention to aggregate the top-k neighbors for self-distillation.\n\nWeaknesses\n\n1. Nonetheless, it is unclear to me the experiments that used to validate the proposed algorithm. The goal of this paper is to enhance the performance of the downstream tasks, e.g. object detection and semantic segmentation. However, the authors chose ViT architecture rather than other more memory-friendly architecture, like XCiT or Swin. For example, how does this method when the backbone is ViT-B/16?\n\n2. The comparison section is a little bit weak. E.g. In Table 1, Those CNN-based methods can be also applied on ViT-S/16 or the proposed method can be applied on ResNet-50 as well. Is there any constraint that the authors can not do that? It will be a more fair comparison.\nHere is a more specific example, DenseCL can be also utilized on ViT, and it can be used to compare the advantages of using neighboring regions over the corresponding patches. \n\n3. The baseline results of DINO, as DINO is trained in a different way, I think the authors should simply set their lambda to 0 as the DINO instead of using the public one. This will be able to show the advantages of the proposed algorithm.\n\n4. How about the performance on ImageNet? Is it able to keep comparative performance to DINO (again, comparing to the DINO authors trained with lambda = 0)?\n\nWhy the loss weight L^PASS is only 0.1? How does the performance change with this hyper-parameter?",
            "summary_of_the_review": "The idea is interesting but the experimental setup seems to be too simple. See weaknesses above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposed a self-supervised learning method called Patch-Aware Self-Supervision to enforce a ViT model to learn similar representations for semantically similar neighbouring patches.\nIt is shown in experiments that the proposed method can incorporate well with DINO and achieves improved results for dense prediction tasks.\nAlthough good experimental results are obtained, the proposed method is specifically designed for architectures like ViT, and may limit the impact as a self-supervised learning method.\n",
            "main_review": "Strengths:\n\n- Writing: This paper is well written and easy to follow. Overall this paper is well-organized. Figures and Tables are clear to show the methods and comparisons.\n- Novelty: This paper proposed an interesting and novel extension for self-supervised ViT models that improves the transfer performance on dense prediction tasks including object detection, semantic segmentation and video object segmentation. \n- Experiments: It seems that the proposed method can achieve better transfer performance on a variety of downstream dense prediction tasks.\n\nWeaknesses:\n1. The proposed method is specifically designed for ViT architectures. Can this proposed method be extended to other architectures? Like maybe Swin-Transformer, MLP-Mixer, or ResNets. Designing a self-supervised learning method that is specifically for the ViT architecture may limit the impact of this work on the field because new architecture may be proposed in the future and the proposed method may not be applicable to them. I suggest the author also do some experiments on the ResNet models, such as using the proposed method on the output feature map of layer4 which is a 7x7 grid (assume a 224x224 input) before average pooling.\n2. Ablation results and explanations are not so satisfying. First, at least for ViT, different input resolutions of the patches should be tested, like maybe ViT-S/8. Second, for results in table 3(b), why is the performance of k=1 so much lower than the performance of k=2?  Finally, maybe a comparison of different loss weights for the proposed PASS loss is helpful for readers to better understand the proposed method.\n3. For experiments in Table 1 and 2, why use 300 epochs to pretrain DINO and 200 epochs for other methods? and for DINO in Table 3(a), how many epochs are used to pretrain?\n4. The experimental comparison with other methods in Table 1 and 2 maybe not fair, because, for other methods, the backbone used is ResNet50, while for the proposed method, the backbone is ViT-S/16, adding results of using MoCo-v2, SwAV, etc on ViT-S/16 can better demonstrate the advantage of the proposed method.\n5. What about the training speed of the proposed method?  How long does it take to finish training for 200 epochs compared to the original DINO method?\n6. How about training for longer epochs? The experiments in the paper only train the model up to 200 epochs, but many previous works show that longer epochs for self-supervised pre-training can yield better results (e.g. MoCov2, SimCLR).\n7. Maybe the paper should add some experiments of combining the proposed method with MoCov3 and MoBY as claimed in the introduction: “... can be incorporated into any image-level self-supervision scheme...”\n\nIn the rebuttal, I hope the authors mainly address the issue of the potentially limited impact due to the specific design for ViT architectures. And provide better explanations for current ablation and experimental results. Also, the author should consider combining the proposed method with other self-supervised learning baselines to support the claim that the proposed method can be incorporated into any image-level self-supervision scheme.\n\nMinors:\n- In page 5, below eq 5, “ov erall” -> “overall”\n- In page 8, section 4.4, the pretrain dataset should be MS COCO train2017 split, not train2007 split.\n",
            "summary_of_the_review": "This paper proposed a module for self-supervised learning to enforce the agreement of representation of semantically similar neighbouring patches in a ViT model. \nAlthough the proposed module achieves a performance improvement over the baseline DINO, some comparisons with other self-supervised methods may not be fair.\nAnd more importantly, the proposed module is somehow specifically designed for patch-based models, so the applicability of the proposed module may not be as wide as other self-supervised methods.\nAlso, some of the experimental results are not very well discussed and explained.\nOverall I rate this paper a borderline reject, hope that the rebuttal can resolve my concerns listed in the weaknesses.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "Paper proposes a self-supervision approach for pertaining Vision Transformers. The core of the approach is the defined pre-training task termed by authors Patch-Aware Self-Supervision (PASS). PASS involves selecting a set of neighbors for each (query) patch and ensuring that query-neighbor representations are similar. In more detail, 8-neighborhood patches are considered as potential positives, from which a subset is then selected based on cosine distance to the query patch. These patches are then used for enforcing similarity of representations through aggregation module. The resulting PASS objective can be combined with other (more traditional) SSL objectives. The result is an approach that learns better patch-level representations as is illustrated by improved performance on object detection and segmentation tasks. ",
            "main_review": "> Strengths\n- The paper is generally well written and easy to understand\n- Results show that PASS improves performance over corresponding baselines\n\n> Weaknesses\n\n1. Some of the details of the aggregation module (Section 2.2) were a bit difficult to understand. \n\n2. The idea to use 8-neighboring patches for self-supervision is not new. This was explored in: C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual representation learning by context prediction, ICCV, 2015. Note that the task in Doersch et al. was somewhat different, where classification of the relative position of the two patches was used as supervision, but the high level idea is similar. Authors should cite the paper and discuss the relationship to the proposed approach. It would also be reasonable (and useful) to compare Doersch et al. supervisory objective to the one proposed in the paper. \n\n3. It is unclear why this specific form of self-supervision was proposed or why one would consider it better than readily available alternatives. For example, a simple baseline would be to apply BERT-like training, instead of PASS. As it turns out, there is a paper that apparently does this: H. Bao, L. Dong, F. Wei, BEIT: BERT Pre-Training of Image Transformers, ArXiv, 2021. This seems to be an unpublished paper, so I am not saying it should be considered prior work, but the performance reported in it is better than that of PASS. This begs the question of importance and significance of PASS. Irrespective of the existence of the ArXiv paper, BERT-like pre-training should have been one of the own baselines to which PASS should have been compared in the submission. One would expect any sort of proposed patch-level SSL, like PASS, to be better than BERT, otherwise, what's the point? \n\n4. The ablation results just seem incredibly bizarre to me. First, results on local patches w/o matching are SIGNIFICANTLY worse than those that use ALL patches in the entire image. This is counterintuitive at best. Second, it seems a bit crazy that improvement from 1 to 2 patches is over 200% and drops again by over a half when it goes from 4 to 8. Without an appropriate explanation of such behaviors, I view this as highly suspicious. In the very least it shows incredible sensitivity of the approach to this parameter choice. Authors present these results, but give no viable explanation to why one may expect such a behavior. \n\n5. It is stated that PASS can be combined with a number of SSL approaches, including DINO (Caron et al., 2021), MoCo-v3 (Chen et al., 2021b) and MoBY (Xie et al., 2021c). However, in experiments, it is only illustrated in combination with DINO. This is suggestive, but not conclusive. It would be valuable to see PASS incorporated in at least one other SSL model. \n",
            "summary_of_the_review": "The paper is well written and approach is sound. However, the novelty is limited (see Point 2 in the Main Review) and significance appears low (see Point 3 in the Main Review). In particular, it is not clear if the proposed PASS is any better than standard pre-training strategies such as those leveraged in BERT. The experimental results are also limited (see Point 5 in the Main Review) and ablations are rather puzzling and concerning (see Point 4 in the Main Review). ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}