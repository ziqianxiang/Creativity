{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose frequency-wise normalizing the features and input of CNNs on audio in a similar manner to instance norm. The authors perform an interesting analysis showing the mutual information between channel/freq/time statistics and the domain/task. The authors combine the proposed norm with instance norm, and show performance improvements of 3 architectures in 3 tasks. \n",
            "main_review": "\n## Section 2:\nThe analysis is very interesting to shows an estimate between the mutual information between the mean and std over  a dimension and the domain.\nSome points:\n1. I feel that the section has a bad formatting the text is not very clear. It can benefit from some revision.  Examples:\n  1. The first two paragraphs can be merged and rewritten more clearly.\n  1.  Page 3: `compared to channel statistics, MI of frequency-wise statistics (Freq-wise Stat) is superior, but that of temporal feature is inferior on all stages.` This sentence can be revised. \n\n1.  which BC-ResNet do you use? you cite two papers  (Kim et al., 2021a;b) but the one suited for (ASC) task is in the second reference BC-ResNet-Mod-1 not BC-ResNet-1,  right?\n1. Figure 1 down:  If I understood you method correctly you are reporting the classifier performance on the  real label (10 classes). In the case of $S^{(c)}$ you are calculating the mean over time and frequency for each (and the STD), just like global average pooling, this also can be confirmed that you get the same performance as in table 1, 0.6 accuracy for the scene label.\n   1.  In table 1, you have 9 devices, not 6 device as you explain earlier in the section. is this correct? shouldn't the performance be better on the 6 scene devices?\n   1.  You trained the network with global average pooling ( meaning the same features included in $S^{(c)}$ ). I think that if you train the original network with averaging over the channels (instead of freq/time) you may get different results. Therefore, I think your conclusion about comparing the channel statistics with freq/time may not be sound.\n1.  `Moreover, despite global average pooling after stage 4, audio features show the highest estimates of ` $I(s^{(F )}, y)$ How is this possible? global-average-pooling averages time and frequency  resulting in one value for each channel  removing  time and frequency information. By your definition in page 2, $s^{(F )}$ should contain only one value. Are you estimating MI before global-average-pooling? in this case I don't see how this sentence is relevant. \n2. In general, the audio part is to be expected as well. Looking at the dataset, (Heittola et al., 2020) was generated by simulating the recording devices, by convolving the impulse responses of the simulated devices with the real audio. Convolution is multiplication in the frequency domain. I cannot tell if your conclusions should hold for domain generalization in general or only for generalization to recording devices.\n\n## Section 3\n\n1.  page 4 `RFN does not use affine transformation, which means that\nthere is no additional parameter except λ.` This part is not clear.\n\n## Section 5\nThe main concern in this section is that the authors are picking one architecture for each task. The author claim that the RFN would be a plug-and-play module that can be added to any network to improve DG. This should be clearly shown in the experiments. In the current status the of paper, the chosen architectures can be hand picked to show the improvements.\n\nSome points:\n1. `We observe that the effect of RFN decreases as adding more training speakers because more training speakers, themselves, can cover the unseen speakers.` This part is not clear.\n1. Section 5.3 and Figure 3, similar to Section 2. Here the dataset is explicitly constructed by simulating recording devices, by multiplying the signal with the impulse responses of the recording devices in the frequency domains. I find it hard to draw general conclusions about DG in audio in general.\n1. Figue 4, shows that the RFN works as a strong regularizer significantly reducing the model ability to fit the training data. This also calls for more verification with more architectures. Since it's possible for the proposed method to hinder the training process.\n1. Figue 4, the baseline doesn't fit the training data perfectly which is not usual in deep NNs. The model in  Appendix A.5 does, I think these two figures should be switched.\n1. Figure 5, I suggest adding the result of `CP-ResNet c=64` to table 1 (maybe with other values c>64 as well so that the model fits the training data ). the same can be done for the KWS and SV tasks (another suitable architecture)",
            "summary_of_the_review": "The authors show that the proposed method improve the performance of selected architecture in different datasets.\nThe main concerns are:\n- Section 2 (although provides interesting analysis) can be improved to be more technically sound. \n- In the experiments, the authors  claim that their method should be a plug-and-play module that can be easily added to existing networks to improve their DG. However, they pick a specific architecture for each task.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Based on the observation that domain information in audio data is dominantly along the frequency axis, the paper proposes a novel normalization module,  RFN (Relaxed instance Frequency-wise Normalization) for domain generalization. RFN is constructed as a linear combination of layer norm (LN) and Instance frequency-wise normalization (IFN). Empirical studies are performed to show the benefit of the proposed method. Results are shown in three different tasks, (a) Acoustic scene classification, (b) keyword spotting and (c) speaker verification.\n",
            "main_review": "(+) Overall, the paper is well written. In particular, the motivation and relevant background sections are well put together. The ablation studies are fairly comprehensive.  \n\n(+) The paper borrows multiple ideas from the literature with slight extension to tackle the problem of domain generalization in neural audio processing. They show impressive results for acoustic scene classification task. \n\n(-) In ASC, unlike in other audio domain tasks, the background is more important than foreground and the task is heavily dependent on the long term statistics of audio signal. Given that the proposed method shows strong improvements in ASC, but only marginal improvements in KWS and SV tasks (e.g RFN is not the most effective approach in the large data regime in KWS and macro averaging only unseen genres suggests that mixup is better for DG than RFN in SV). This suggests further evidence is required to support the authors’ claim that RFN module improves DG in audio tasks across the board.  More specifically, experiments in ASR task and large scale acoustic event classification/detection tasks can lend better credence to the proposed method.\n\n(-) Another key concern arises from Table 6 (see item (3) in “Questions for authors”), which indicates that using the proposed module at input is almost as effective as using it after every layer. This suggests that an equivalent normalization of input features can be as effective as the proposed method. \n\n**Questions for authors** \n- In Table 3, is the “overall” column based on micro or macro statistics? Do the conclusions change depending on the type of statistics used? (same question for other Tables.) \n\n- What is the rationale for not presenting results from all 3 tasks in all tables? Example, \nTable 4 has only ASC and SV tasks, \nTable 5 has only KWS and SV tasks, \nTable 6 has only ASC and SV tasks.\n\n- In Table 6, for the ASC task, are the results for rows “Input (73.1+-0.7)” and “Input, stage 1, 2, 3, 4 (73.9+-0.7)” statistically significant? If not, does that mean that the proposed module is only required at the input?\n\n**Minor suggestions**\n- Page 5, Multi-genre speaker verification paragraph, record only at a single genre -> record only one genre \n- Page 6, Keyword spotting paragraph, Table 5.2 -> Table 2\n- Page 15, Acoustic scene classification paragraph, momentum to 0.9 -> momentum set to 0.9\n- Page 18, Automatic update of relaxation paragraph, Automatic update of \\lambda is not straight due to -> Automatic update of \\lambda is not straightforward due to",
            "summary_of_the_review": "Overall, I vote for not accepting the paper. The idea of normalizing along frequency dim is not new and is well established in audio signal processing in the form of CMVN or dynamic CMVN to combat channel effects between source and destination. The authors extend this idea by combining this with layer normalization and introducing this after every layer in a neural network. While the authors present impressive results in ASC task, additional experiments in ASR and acoustic event detection/classification tasks are required to establish the efficacy of the proposed method to fully support the author's claims. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Authors have focused on 2D CNN based acoustic models show that relevant information in the audio feature is dominant in frequency statistics rather than channel statistics. To exploit this they propose to use a normalization layer along frequency dimension and claim better performance experimentally.",
            "main_review": "Author's argument about dominant information in frequency dimension rather than channel is not true in general. \nFurther even the MI experiment in Figure1 clearly shows both channel and frequency statistics are very important as compared to temporal dimension. For MFCC channel stats are at par or better then frequency stats.\n\nFor deterministic DNNs, MI estimation is known to be problematic and not true representative of how the network is learning. In wang et al. authors employ a linear model (or a generative nonlinear model) between the variables of interest, training the model and using its performance as a proxy for MI. This is not a reliable estimator of MI, as it is likely to be biased by the choice of such auxiliary models. Furthermore, the argument used in Wang et al. that I(T;Y) is related to generalisation is contradicted by existing works.\nAjmad et al., Learning representations for neural network-based classification using the information bottleneck principle. TPAMI\nCheng et al., Utilizing information bottleneck to evaluate the capability of deep neural networks for image classification. Entropy \n\nThe relaxed version of IFN is simply an augmented layer normalization LN. The implemented weight Lambda=0.5 can't be considered as relaxed as it is a significant weightage to LN.\n\nThe study is largely experimental with no novel insights or theoretical contributions to make a case as an ICLR submission.\n\nPaper focus on 2D CNNs, what about 1D CNN based SOTA acoustic models like ContextNet. 1D convs will not have a separate frequency dimensions still they learn effectively by combining information from channels and by reducing temporal redundancies.\n\n\nExperiments:\nFigure 3: features are extracted from earlier layers. Better to use latter layers which tend to be more discriminative and also evolution of features across layers with and without IFN. Again RFN alone doesn't seems to do better than channel wise for class separation. LN seems to be quite influential here.\n\nTable4: A fair comparison would be to include LN with other normalization techniques and LN alone to understand the contribution/impact of LN in overall performance.\n\n\nI would encourage authors to add experiments on ASR. Also consider unsupervised audio representation learning for multi-task learning on downstream speech/audio tasks.",
            "summary_of_the_review": "The study is largely experimental with no novel insights or theoretical contributions to make a case as an ICLR submission. Author's claim about dominant information in frequency dimension is not convincing based on the experiments presented.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes an adaptive normalization scheme for 2D-convolutional networks specifically geared toward audio / spectrotemporal models, where the convolutional directions carry different interpretations (e.g. time and frequency) and different statistics.\nThe authors first demonstrate that frequency-positional statistics are more statistically related to \"domain\" (e.g. recording device or environment) than to target concepts (e.g. acoustic scene class), which motivates the use of normalization to focus the model on the target concept.\nThe proposed normalization method is a typical z-scoring type approach, aggregating statistics per instance and frequency index (but across time and output channel), and is then combined with layer normalization (per-instance) via convex combination.\nThe method is evaluated on several tasks, and appears to perform well compared to baselines.\nFinally, an ablation study reveals that applying normalization only on the input layer performs comparably to normalizing at all layers in their benchmark tasks.\n\n\n",
            "main_review": "The presentation of the method in the paper could be substantially clearer.\nThe authors use a fairly obtuse notation with several levels of indirection (S_i_F, k_F...) to define their statistics (\\mu and \\sigma).\nIt was only after reading the appendix that the method became clear: mu(F) aggregates across all dimensions *other than* F.\nI strongly encourage the authors to move the algorithm block in the appendix into the main body of the paper, or at least explicitly define the domain of the statistics in question; just saying \"\\mu(F) \\in R^F\" would have been enough to clarify the computation early on.\n(If the authors do move the algorithm block into the main paper, please document both the inputs *and the outputs* of the function.)\n\nI found the use of MFCC-based methods in the introduction a bit strange, as MFCCs do not have any obvious notion of vertical transposition invariance, and it's not well justified to use 2D-conv layers on MFCC inputs. (Even less so than Mel spectra!)\nThis doesn't seem integral to the paper overall, as the actual methods used in the experiments do not use MFCC representations, but it was a bit confusing to see.\n\nThe demonstration of the relaxation parameter in Figure 2 was confusing.\nNowhere is it explained what the data here is, or what behavior should be expected.\nFigure 5 does a much better job of communicating the effect of lambda.\n\nThe empirical results seem promising, though I have several reservations about the experiment setup and baseline models.\nSkipping to the end, it does seem in Table 6 as though normalization at the input layer is sufficient (at least for ASC and SV tasks; no results are presented for keyword spotting).\n(Note: the authors seem to be bolding the \"best\" model here, though almost all results including input normalization are within one standard deviation of each other and no statistical analysis is conducted.\nI think it's fair to interpret these as \"equivalently best\".)\nIn this case, the proposed normalization should simplify to a per-frequency standardization (or a combination of that and instance normalization).\nAs demonstrated by (Lostanlen et al., 2018; figs 2 and 3), this is effectively what PCEN (Wang et al., 2017) does: gaussianize (and decorrelate) the spectro-temporal magnitudes, so I found it surprising that the proposed method appears to consistently outperform the PCEN baseline in Table 1.\n\nThis observation led me to dig into the implementation details described in the appendix.\nIf I understand it correctly, PCEN is applied to the log-scaled Mel spectrograms, using the parameters specified by Wang et al.\n(Please correct me if this is not the correct interpretation!)\nThere are at least three problems with this:\n\n    1) PCEN is meant to be applied to linear magnitudes *in place* of logarithmic (decibel) scaling, not *in addition to* log scaling;\n\n    2) the parameters (specifically the time constant) are dependent on the frame rate of the spectrogram, which is different here (window=130ms, hop=30ms) than Wang's (25ms window and 10ms hop);\n\n    3) the parameters given by Wang et al. are for the keyword spotting task, and are almost certainly not a good choice for acoustic scene classification.\n\nThis leads me to think the baseline was not implemented properly.\nMoreover, the baselines were not consistently implemented across the different tasks (ASC, KS, SV), and this is particularly surprising because PCEN was explicitly designed for the KS task.\n\n- Lostanlen, Vincent, et al. \"Per-channel energy normalization: Why and how.\" IEEE Signal Processing Letters 26.1 (2018): 39-43.\n\n",
            "summary_of_the_review": "An interesting idea, but I'm unconvinced of the results.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}