{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Authors propose an algorithm to solve bilevel optimization problems with non-smooth inner problems. To my knowledge this is one of the first algorithms which show convergence toward a stationary point with non-smooth inner optimization problems.\nHowever this algorithm depend on multiple other hyperhyperparameters:\n- 2 sequences of step sizes, with at least one hard to tune\n- 6 other hyperparameters controlling the convergence",
            "main_review": "1- Paper is an extension of [1], but I cannot find the article in a open way on the internet (the only option I found is to pay the springer fees).\nSo I were not able to read the latter paper and to assess the novelty wrt the previous work. Hopefully, I guess the idea is very similar to [2]\n\n2- It seems that in Algorithm 1 the whole sequences $\\eta_w^t$ $\\eta_\\lambda^t$ have to be given as inputs.\n\n3- Since there are a lot of hyperhyperparameters, I was wondering if authors planned to release an open source package to 1- reproduce the experiments 2- have good heuristics to set these new hyperhyperparameters.\nImagine you want to make a Lasso with cross-validation without a human in the loop, I do not know if it is possible with this algorithm.\n\n4- By curiosity, since you provide a global convergence result, did you try to plot the suboptimality (ie current loss on the validation set minus minimum) as a function of time or iteration to observe potential convergence rates on the bilevel problem?\n\n\n[1] Bilevel Optimization of Regularization Hyperparameters in Machine Learning. Machine learning. 2020,\n\n[2] Akshay Mehra and Jihun Hamm. Penalty method for inversion-free deep bilevel optimization. 2019",
            "summary_of_the_review": "Overall I think the theoretical contribution is interesting. From the practical side it seems that proposed methods relies on multiple other hyperparameters, which can make it hard to apply in practice. If authors plan to release their code along with heuristics to calibrate these additional hyperparameters, I will raise my score.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose an approximation method for solving non-smooth bi-level optimization problems. The approach consists of \ni) transforming the lower-level non-smooth objective into a smooth one through a parameterized approximation and \nii) converting the bi-level problem into a single-level one by using an approximated version of the corresponding first-order conditions.",
            "main_review": "strengths:\nthe authors provide theoretical guarantees of their approach and, from the numerical simulations, the method seems to outperform various state-of-the-art techniques.\n\nweaknesses:\nThe authors could have spent some more time refining the presentation of their idea. Some sentences are a bit obscure, e.g. \"Even worse, most of them depend on approximating hyper-gradients to update upper-level variables which is the inherent reason for non-efficiency\" in the abstract.\nThe authors should have specified more clearly what is the special challenge of combining the bi-level setup with the non-smoothness of the lower-level objective. As the ideas are presented, it looks like the authors have just combined two existing well-known techniques.\nAlso, I would encourage the authors to add a few lines specifying what is the key difference between their work and the cited paper SMNBP Okuno & Takeda (2020).\n\nquestions:\n- Is this the first time bi-level optimization problems are handled by replacing the lower-level problem with the corresponding first-order conditions? \n- The experiments seem to focus on only one kind of non-smoothness, i.e. p-norms with p<=1. Is the method applicable to more general cases? And would it be possible to extend it to tackle the non-continuous case, e.g. p = 0?\n- Why in Table 4 the lowest accuracy is the best? I suggest the authors add some comments about the overall scores in this case, as there is pretty high variability in the performance over different data sets.\n- Why is the smoothness parameter decreased at each iteration? \n- It would be nice to include a brief and intuitive explanation of how Eq.2 can be seen as an approximation of a bi-level problem. As the optimization is over both the lower- and higher-level parameters, what does prevent the algorithm to fulfill Eq.3-5 by tweaking the high-level one? \n",
            "summary_of_the_review": "The method seems to perform well in practice but the originality of the main idea should have been discussed more extensively.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper is interesting in its smoothing approach to the very difficult problem of nonconvex nonsmooth bi-level optimization.\nIn this context, it should be emphasized that the general setup the paper addresses is considered extremely difficult with a very sparse literature.",
            "main_review": "In short,\nPros:\n- Interesting approach, combining smoothing, Augmented Lagrangian, and stochastic, techniques.\n- Overall written well (with a few glitches here and there)\n\nCons:\n- The paper claims theoretical guarantees that are far beyond its reach\n- The mathematical analysis is severely lacking: proper definitions, assumptions, etc.\n\nMore in depth, listed in order of appearance in the paper.\n- p3 typo in the first line\n- Sec 2.1. f and g are not even assumed to be convex? It should be easy to derive examples in which this model is not well-defined -- there should be some assumption to allow this model to make sense.\n- I think that some of the examples do not satisfy the underlying assumptions of the model(?), or Assumption 3 in Sec 4, which is required for the theoretical result Thm 1.\n- Sec. 3.2 page 4. The expression of the gradient of $c^{\\mu^k}$ is inaccurate since either chain rule is applied or it should be written as a function of w and \\mu^k (not of \\tilde{h}).\nRight afterwards, the gradient of $\\phi$ does not seem correct to me (maybe typos here) -- it should be the Jacobian of the smoothing function $\\tilde{h}$ transposed and multiplied by the gradient of $\\phi$ at $\\tilde{h} (w,\\mu)$. Since $h$ is not differentiable, the current expression is obviously incorrect. \n- The comment on solving the problem using penalty method is inaccurate. It is maybe possible to apply a penalty method (not sure if it is possible to solve the subproblems), but in terms of theoretical results, it has none in practice (the results in  Wright and Nocedal 99' are by assumption).\n- The \\Psi function, and therefore the  Augmented Lagrangian, are not differentiable since it is only assumed that smoothing function is once continuously differentiable. Hence, the optimality condition given in (3) is not well-defined.\n- The Augmented Lagrangian is nonconvex, with an unclear structure (gradients and Jacobain of nonconvex), so it is not obvious that it is even possible to obtain an \\epsilon-stationary point at all; see e.g., Bolte, Sabach, and  Teboulle. “Nonconvex Lagrangian-Based Optimization: Monitoring Schemes and Global Convergence.” MOR 2018, and related. \n- Sec. 4. is very hard to read and understand since it jumps to general subdifferential theory without preparing appropriately, in particular, it is unclear how Assumption 1 or Def 2 relate to the problem.\n- Assumption 2 and 3 are the baseline for the theoretical result, so these should appear in the problem formulation in the beginning of the paper. Some of the examples in Sec. 2. do not satisfy Assumption 3.\n- The source for Def. 2. should be cited or referenced to the appendix Lem 2 I guess.\n- Def. 2. It was not assumed that the lower level probem is convex, and it is not appropriate to insert a note in mathematical definition. That is, the definition should be written more accurately.\n- Def. 2, typo in the last line.\n- Thm 1. Considering the results in the literature on Augmented Lagrangian methods for nonconvex optimization, I find it hard to believe  that this result is correct (see the aforementioned paper).",
            "summary_of_the_review": "Overall, in my opinion, the paper should be more modest in its declarations and aim to obtain lower quality guarantees but with a higher quality analysis. If this was the case, my score would have been higher. \nAs I noted, the mathematical analysis is severely lacking.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}