{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the convergence behavior of training one-hidden layer ReLU networks with masked neurons. The random masked training setting considered in this paper is defined as (1). train randomly-selected subnetworks; (2) combine together subnetworks; (3). iteratively training. The training setting can cover three settings considered in previous work, including dropout regularization, multi-sample dropout, and multi-worker IST. The authors prove the linear convergence rate of the training error in random masked training settings when the full network is overparameterized. Experiments on a small dataset (Communities and Crime Data Set) validate the theoretical results in this paper.",
            "main_review": "Strengths:\n\n1. The random masked training setting is interesting and important in the context of distributed machine learning training. The convergence results are interesting, which suggests that we can train random subnetworks separately for each communication round and this provably converges to the optimal solutions up to an error region.\n\n2. This paper provides convergence guarantees for three settings considered in previous work.\n\n Weaknesses:\n\n1. My main concern is the proposed algorithm and the convergence analysis are too specific to the one-hidden-layer ReLU network. For example, the step size for aggregation $\\eta_{k, r}=N_{k, r}^{\\perp}/N_{k, r}$, which is very specific to the network structure considered in this paper. The analysis and algorithm do not seem to easily generalize to more general cases (such as deeper models or other activation functions).\n\n2. The convergence rate for the dropout algorithm might be sub-optimal (there is only one subnetwork is updated for each global iteration). Correct me if I am wrong, the dropout algorithm can converge to global optimal in practice. \n",
            "summary_of_the_review": "This paper provides convergence results for random masked training one-hidden-layer ReLU networks. Although the network architecture considered in this paper is simple, this paper provides some new insights on training randomly masked subnetworks.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N.A.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper establishes a linear convergence rate of the training error for shallow neural network with randomly masked neurons in the NTK regime. It applies to analyze existing methods such as  dropout training and independent subset training. \n",
            "main_review": "Strength: \n- the paper addresses a challenging problem in the NTK literature, where the training dynamics has random NTK kernels. \n- the proof seems to be very technical, suggesting a great effort spent by the authors.  \n\nWeakness:\n- certain literature is compared in a confusing way, e.g. the work of Mianjy and Arora 2020 studies the convergence rate of dropout training in terms the generalization error. It is not comparable to the training loss studied in this paper. Therefore the comparison of the convergence rate is not meaningful to me. \n- the definition of the kernel in Definition 1 is not clear. What is w_{k,t,r}? why there is x_i but no x_j in the inner product with w_{k,t,r}? similarly, what is M_{k'} in theorem 2? what is the definition of hat u_k in section 4.1? \n\nQuestions:\n- From the surrogate gradient derived on page 5, it seems that the same mask m_k^l is applied to all the data samples i. Why this is so in dropout training? It seems to me the mask is randomly selected for each i. \n- Does Hypothesis 1 imply that if t-> infty, hat u_{k,t}^l will converge to y in some sense? I would be better to define S_i directly in the proof of Hypothesis 1, as it may be hard to find it out. ",
            "summary_of_the_review": "The paper is pretty technical, and it requires further improvement on the quality of the writing. Due to some typos or confusions in the notation, certain key results such as Theorem 2 and Hypothesis 1 are hard to be verified. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This manuscripts presents a mathematical proof of the linear converge of a specific training scheme applied on networks with one hidden layer.\n\nThe training scheme considered goes as follows. \nFor every 'global' epoch, a total of $p$ independent masks are sampled randomly. Each mask defines a ‘subnetwork’ consisting of the neurons that are not masked to zero. Each subnetwork is trained independently for $\\tau$ ‘local’ epochs. At the end of each global epoch the weights of network are updated by combining the  weights of all the subnetworks.\n\nThis scheme has strong similarities with the well known dropout regularisation, which would broadly correspond to the above scheme with $p$ = 1 and $\\tau$ = 1, or with the 'multisample' dropout regularisation ($p$ = 1 and $\\tau$ > 1).\n\nHence, the authors argue that the general results proven can be taken as a first rigorous theoretical justification for why such schemes work in practice, and in fact provide specific corollaries  of the general theorem for the cases mentioned.\n\nTowards the end of the article a numerical test is presented on a simple architecture and a simple dataset.",
            "main_review": "The convergence guarantees proven can be considered important given the general lack of strong theoretical results for many heuristic learning schemes (such as dropout regularisation) that are commonly used. But I see two issues that hinder the publication of this manuscript.\n\n1) The results proven are limited to shallow networks, and it’s not clear whether similar results will hold for deep networks (more often than not deep networks behave oppositely to shallow networks), nor whether the approach used here could be extended at all on more relevant architectures. This is of course a very crucial point, since the real challenge in analytically treating state-of-the-art architectures is precisely the depth, while many results are generally available for one-layer networks. \n\n2) The results proven are *not* sufficiently tested on real or synthetic data. The only numerical experiment is shown figure 1 and it provides only a very qualitative and limited testing of the theory given.\n\n\nI can understand that point 1. can be considered a somehow unavoidable limit of all theoretical models for deep learning, but the authors should explicitly - and precisely - comment on the possibilities of extending their work to more realistic scenarios.\n\nPoint 2., on the other hand, has no excuse for not being solved. I hence think that more figures and more numerical examples should be provided in the main text (and/or in the appendix), both to better test the theory and to aid the understanding of the paper.\n\nA minor change that would also improve the readability of this manuscript would be the addition of a ‘reference table’ containing the symbols used in the theorems and in the derivations along with their meaning. While it would be better to add it in the main text, this table could also be provided at the beginning of the appendix with an appropriate reference in the main text.\n\n\n",
            "summary_of_the_review": "I appreciate the technical effort of providing rigorous convergence guarantees for a complex and important training schemes, but I am uncertain that these results will ever be useful for practical learning scenarios given that they are derived for shallow networks only and the possibilities of extending their work seem very narrow. In any case I believe that more numerical tests should be made available (in the mani text and/or in the appendix) to corroborate and confirm the theoretical results and to make the results approachable to a broader audience of computer scientists and practitioners.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In the paper, the authors look at the training and convergence of one layer ReLU networks when we mask out different neurons in the hidden layer. As the authors point out, this model occurs in practice either in the form of Dropout or as distributed training of neural networks. The authors show that under some assumptions using concentration of measure results that the neural tangent kernels are close to the infinite no mask neural tangent kernel. Using this the authors show linear convergence. ",
            "main_review": "**Strengths**\n\n1) The paper shows convergence of one layer networks under some general masking conditions. \n2) The paper shows linear convergence by showing the NTKs are close to the infinite width NTK. This gives us a lot of tools to analyze the model. \n3) The scaling of $m$ with respect to $n$ seems to have been improved. Specifically, Theorem 1 requires $m \\sim n^4$, Corollary 1,2,3 $m \\sim n^5$. Where as previous work such as Song and Yang 2020 and Du, Zhai, Poczos, Singh 2018 have $m \\sim n^6$. \n\n**Weaknesses**\n\n1) The paper is difficult to understand in certain parts. Specifically, Hypothesis 1 is difficult to understand. Also while I can infer what $\\hat{u}$ refers to, it would be helpful to define that somewhere in the text. \n\n**Comments and Questions**\n\n1) I am not sure where the first equation in Section 4.1 comes from. This might relate to my confusion about what $\\hat{u}$ is. Further, why is this true \"Since the masks are sampled i.i.d. from the joint Bernoulli distribution, the function of each sub-network is an unbiased estimation of the function represented by the large network.\" shouldn't it be off by scale of $\\xi$?\n\n2) The assumption that $\\|x_i\\|_2^2 = 1$ appears for the first time in Theorem 3. However, it seems to have been used in the proof for Theorem 2. Specifically, when it says $|h_r| \\le 1$. \n\n3) It would greatly help me, if for each of the terms, in hypothesis 1, and the formula for $B_1$ etc if the authors identified the dominant terms. This would help a reader parse the statements.\n\n4) The colorbar, on the plot should be labeled. \n\n\n\n\n",
            "summary_of_the_review": "Overall I think the paper is a good paper that improves our understanding of dropout. Hence I think it should be accepted. However, I think the presentation can be improved. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}