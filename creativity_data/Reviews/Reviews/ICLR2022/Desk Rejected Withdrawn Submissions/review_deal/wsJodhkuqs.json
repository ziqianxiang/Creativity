{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents a novel multi-agent reinforcement learning (MARL) attack against federated learning systems. They claim that unlike other attacks that are heuristic-based, and poison the gradients with respect to the immediate previous iterations, MARL enables the attackers to jointly learn an attack policy. They borrow concepts from distribution learning to estimate the average data distribution of the participating clients, and from policy learning to craft an attack that is not myopic unlike other known attacks. The paper claims to advance the state-of-the-art by integrating the above concepts that can also work in a black-box setting. Their attack has the greatest impact on increasing the error rate of the federated learning system with respect to all other known attacks even in the presence of the state-of-the-art defense techniques. \n",
            "main_review": "Strengths - \n- Their technique involves distribution learning and policy learning - both when the actual federated learning process is going on. Thus, they can estimate the average data distribution without the need for priors.\n- They show how they have been able to estimate the training hyperparameters with high accuracy, and used it to launch an attack that surpasses all other known attacks in terms of its effectiveness on the evaluated datasets.\n- They have done an impressive literature survey, and borrowed standard concepts wherever was necessary, along with a large set of standard benchmarks.\n-Their black-box attack appears to be almost as powerful as white-box attack which is impressive.\n\nWeaknesses -\n- A convincing justification for the choice of \"inverting gradients method\" was lacking. The results, however appear to be good with the chosen method. It would have been better to show some results on the quality of the reconstructed data. \n- It was not clear how the data was distributed in a non-iid manner. The evaluation of distribution learning can heavily depend on the way the data was distributed among the clients. \n- The paper describes too much of mathematical details how the different components of the attack system work, but an intuitive description is lacking. For example, the \"action\" taken by the attackers as part of reinforcement learning is only described later in the Appendix. It makes it difficult for the reader to understand the overall idea of the attack mechanism, how the attack differs from other known attacks that are called myopic. It would have been better to have a design-flow or an intuitive explanation in the main paper before describing the mathematical details. \n- In addition to the final error rates reported in Figure 2, it would have been better to describe how the temporal training plot looks like. This is with regard to the reported error rates of \"LMP\" that appear to be too small as compared to what has been reported in the actual paper. \n",
            "summary_of_the_review": "The results presented are impressive, but the paper lacks a top-down breakup of the intuition of their attack mechanism that is claimed to be so powerful. It is difficult to understand what the actual attack mechanism is, and why it is so successful. This makes it difficult for the reader to think of corner cases where the attack may fail. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors proposed a new multi-agent reinforcement learning attack framework to learn a non-myopic attack\npolicy and claimed that it can effectively compromise FL systems even with advanced defense mechanisms applied. Both white-box attacks and black-box attacks are considered, and they conducted experimental study to demonstrate that the proposed MARL method consistently outperforms existing model poisoning attacks for FL systems.",
            "main_review": "Overall, the paper is interesting to read and easy to follow, and it advances SOTA of FL and multi-agent RL literature by proposing \na novel multi-agent reinforcement learning attack framework against federated learning systems by integrating distribution learning and policy learning through multi-agent coordination. \n\nIn comparison with existing literature of attack methods for FL systems, the proposed MARL method here is novel in the sense that it enables the attackers to jointly learn an attack policy through coordinating the behavior of attackers in distribution learning, policy learning\nand attack execution, while previous attack methods only has very limited coordination (e.g., only coordinated in attack execution stage for the recent DBA attacks), or no coordination at all for most existing attack methods. Second, when compared with existing\nwork on reinforcement learning based adversarial attacks, the proposed approach is superior by considering a more realistic threat model where the attacker might not always be selected due to subsampling, nor does it have prior information about the distribution of the aggregated data. The authors also consider the scenario when the attackers have no access to the server’s configurations. In\ncontrast, previous works typically assume more powerful attackers that can attack at any time and have full knowledge about the environment. The experimental study results seem to be encouraging, and the proposed MARL based attack method seems indeed show superior performance when compared with other baseline attack methods in the paper.  \n\nHowever, to be more convincing, I think some extra details could be added/explained about the selected experiments settings/scenarios, as well as how general the conclusions in the experimental study section to be generalized to other setting scenarios. For example, about the White-box and Black-box experiment results comparison section on page 9, the provided example showed that the server’s parameters are all estimated with small errors. Is that a special hand-crafted scenario to show off the nice numbers, rather than a general more representative scenario? Those estimation numbers look too good to be true for me, for a completely \"Black-box\" attack method to estimate the server parameters. I'm wondering how the authors come up with the experiment settings, and wondering whether such an example is representative enough, or it looks more like a special easy scenario for the \"black-box\" attack demonstration. I think the authors should be more transparent and provide more clarifications about the experimental settings for getting these numbers, and clarify on how representative for the selected scenarios and settings used in the experimental study section, and  try to avoid the case to intentionally biased to some special scenario which favor the proposed MARL attack methods. In addition, it would also help if the authors could be more transparent about how much efforts the authors are tuning the experiment settings (for both the proposed method and baseline methods, as well as the env settings) to get those good comparison numbers, to ensure fair comparison and replicability. \n\n\n",
            "summary_of_the_review": "I think the proposed novel  MARL based attacker methods for FL systems is valuable to the research community of FL and MARL. The idea is interesting, and the experimental study shows some encouraging results about the advantages of the proposed approach against the SOTA methods on the existing literature. However, some improvements and extra clarifications might still be needed to make the experimental results more convincing and transparent, and to ensure the comparisons there are fair enough and the provided examples/settings are representative and not biased to special cases to favor the proposed technique. Once the authors providing satisfactory explanations & clarifications about such experiment results sections, I'd found the paper to be more acceptable for publication on ICLR. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper studies a model where a group of malicious agents coordinate to attack a federated learning system. The authors proposed a set of methods for the attackers to learn an attack policy, showed a bound on the performance loss of their methods due to inaccurate estimation of the aggregated data distribution, and conducted numerical experiments to evaluate their approaches.",
            "main_review": "The main technical part of the paper (Section 3) is very dense and hard to follow; the main idea behind the orignal approach does not stand out as a result. The overall contribution of the work looks a bit weak to me. Conceptually, I feel it not sufficiently justified why the problem is framed as a multiagent learning problem: the attackers are coordinated and they have exactly the same objective, so they could well act as a single agent and standard (single-agent) RL approaches may directly apply.",
            "summary_of_the_review": "The contribution of the paper looks below the bar of the acceptance threshold, which might partly be due to the way the paper is presented.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper introduces a novel attack strategy against federated learning using multi-agent reinforcement learning. The algorithm first learns the dynamics of training process by estimating other workers' data distributions and server learning algorithm and then uses the model to plan a policy to disrupt the learning. The paper also analyzes the algorithm in a special case and shows it is sound.",
            "main_review": "The paper is well-written and easy to follow. In my opinion, the core idea is significant and the new attack improves our understanding about the vulnerability of federated learning. This attack opens up the opportunity to better evaluate the security concerns of federated learning.\n\nThe theoretical analysis on the usage of RL in these attacks is very much appreciated and adds to the value of the paper. On top of that, the algorithm shows impressive empirical results across experiments.\n\nThere are still some room for improvement though. The algorithm is mostly taking a \"learn, then attack\" approach which is a very limited and not the common in RL. This approach is not fully utilizing the potential of RL in these attacks and I wonder how would a normal RL agent, which learns and acts concurrently would do in this problem.\n\nThe other weakness is that most of the paper is about the less interesting setting of white-box attack and the black-box attack is brief and moved to the supplementary material. \n\nIn some parts, I think the paper could better explain the design choices of the algorithm. For example, why both the current \\theta and previous \\theta are included in the state of the agent. It seems like the dynamics only depend on the current parameters. I would appreciate some insight on this from the authors.\n\nOverall, I believe that even though the idea and the results are not surprising, the paper is a nice contribution.\n",
            "summary_of_the_review": "The introduced attack showcases new insight about the vulnerability of federated learning against adversarial attacks. Even though the idea or the results are not surprising, the theory and empirical results are solid contributions.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}