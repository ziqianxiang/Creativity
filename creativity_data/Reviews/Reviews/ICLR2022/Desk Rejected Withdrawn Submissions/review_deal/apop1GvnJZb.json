{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper provides a theoretical analysis of the properties of negative sampling loss function trying to answer why it does not work well at times. In particular authors analyse the non-convexity of the NS loss function and propose ways to adjust noise distribution, number of samples and learning rate to learn a model in convex region of the loss function. ",
            "main_review": "Strengths\n1) The paper provides a theoretical analysis of an important problem particularly when it comes to KG embeddings tasks.\n2) The provided theoretical treatment appears solid.\n3) The provided results appear better than NS and also SANS for specific conditions.\n\nWeaknesses\n1) The paper seems like a minor extension of (Kamigaito & Hayashi, 2021). The bulk of the theoretical foundation is directly borrowed from there.\n2) The paper is somewhat inaccessible and is not an easy read.\n3) While the proposed approach seems to do well on one dataset, it under-performs SANS at times on other datasets. This has not been sufficiently explained.\n4) The empirical evaluation itself seems limited and should be expanded.\n5) While it is claimed that results are not limited to KGE use case but no experiments are done to show the same.\n6) The introduction is not very well written and does not do a great job in motivating the approach.",
            "summary_of_the_review": "While the paper proposes a new variant on negative sampling loss function that is shown both theoretically and empirically to do well particularly in certain situations, the theoretical contributions borrow heavily from (Kamigaito & Hayashi, 2021) making the contribution of this paper limited. The empirical evaluation is limited and no attempt is made to show its utility beyond KGE task.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper shows that NS loss function is non-convex and has a partial convex domain by analyzing the gradient of the NS loss function. They propose conditions under which the NS loss function behaves as a convex loss function. Experiments are provided to support their theoretical results.",
            "main_review": "This paper is well written and easy to read. This paper provides important characterizations for negative sampling loss function. Their result (that NS loss function is non-convex) explains why negative sampling does not work well. This is simple but very interesting result. Besides this, they also construct conditions(noise distribution, number of samples etc) in order to train a model on a convex region in the NS loss function with a scoring method that outputs only non-negative values. Last, they propose a new variant of the NS loss which could handle the unfit training examples in KGE. Their experiments also confirm the advantages of proposed loss function. This is a very interesting and impressive paper for a general problem.  ",
            "summary_of_the_review": "The reviewer think this is a very interesting and impressive paper for a general problem. Their result has a broad impact in both engineering and theory domain.   ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies properties of the negative sampling (NS) loss function in learning knowledge graph embedding (KGE). Though prior work has shown evidence in favor of NS compared to other loss functions, properties of the NS loss haven't been theoretically studied, partially because the choice of loss function is affected by the score function.\n\nThis work shows that even though the NS loss is globally non-convex, there is a local region in which the loss is convex. This paper then shows that when choosing a non-negative scoring function and when the data follows a uniform distribution, then the optimizer lies within the convex region and optimization is easy. To compensate the inflexibility of using a uniform noise distribution, the authors propose Self-Smoothing Negative-Sampling (SSNS), which outperforms the vanilla NS loss and a variant of NS on multiple KGE datasets, especially when the relation is beyond one-to-one.",
            "main_review": "**Strength**: the paper tells a complete story: motivated by a lack of theoretical understanding of the NS loss, the authors first analyze theoretical properties of NS and identify potential issues for optimization, then propose a solution named SSNS which is verified experimentally. SSNS works well especially on YAGO3-10, where there are multiple relations for each entity.\n\n**Concerns**: I'm mainly concerned about the theoretical contribution of the paper. The relation to Bregman divergence is well-known, and the propositions and analyses are fairly straightforward. Some claims need to be made more formal, for example, the comments on choosing learning rate according to which region the function lies.\n\nThe presentation can be improved. I find the writing a bit hard to follow; some places to be clarified include:\n  - The sentence before equation 1: $\\Psi(z)$ in Bregman divergence should not only be differentiable but also be strictly convex. Strict convexity is not mentioned in the current definition.\n  - When defining Bregman divergence, $f,g$ should not be called \"distributions\" since they don't necessarily normalize to one. Having $f,g$ to be real-valued and non-negative suffices.\n  - Confusing notations\n    - Overloading notation: $f$ is used both as the first argument to Bregman divergence, and a function parameterized by $\\theta$; e.g. equation 3, and multiple places in section 4.\n    - Sec 3: two functions $f,g$ are compared directly without specifying what it means for one function to be \"greater than\" another.\n  - SANS is not properly cited: RotatE is cited, but there is no reference indicting SANS is proposed in RotatE.\n  - Minor points:\n    - The score function should be explained / defined more clearly.\n    - First paragraph on page 2: phrases like \"basically performs better\" and \"does not sufficiently train models\" are very vague and should be avoided.\n    - The paragraph below the proof of proposition 1&2: \"it is desirable to use a larger learning rate...\" is repetitive.\n\n",
            "summary_of_the_review": "This paper provides interesting observations regarding the use of negative sampling (NS) by studying the gradient of the NS loss. However, I'm mainly concerned that this does not have enough technical novelty. In addition, the writing need to be made more formal and clearer.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies the problem of non-convexity present in the negative sampling (NS) loss function and investigates the reasons for training instability when using the NS loss. The authors find that noise distribution along with other parameters such as learning rate and initial value need to be adjusted in order to focus the training on the convex part of the overall non-convex NS loss function. The authors also characterize the conditions under which the model can be trained using the NS loss in the convex region and also conditions where training is more challenging. Finally the authors propose a new variant of the NS loss function termed as the self-smoothing negative sampling (SSNS) loss function which addresses the issue applied to Knowledge Graph Embedding (KGE) problem. Empirically, the authors verify correctness of their new variant on a variety of KGE datasets and observe good improvement by using the proposed variant. ",
            "main_review": "This work presents a good theoretical study of a very important problem that has a direct impact on the downstream tasks related to KGE. The authors present a very systematic understanding of the non-convex and convex components of the NS loss with clean step by step derivations. I like the characterization of non-convexity and convex regions presented in section 3 and 4. The derivation of the SSNS loss is one of the main contributions and novelty of this work. Empirical study affirms the effectiveness of this method.\n\nPros:\n- Good theoretical exposition of the NS loss - non-convexity and convex characterization\n- Novelty in presenting the new SSNS loss and its application\n- Empirical study to round up the claims about the proposed work\n\nCons:\n- I would have liked to see the authors mention how their new loss applies to methods outside of KGE. This would be help researchers from other communities also understand how to improve NS for their large scale tasks\n\nComments:\n- For the convex and non-convex characterization mentioned in Section 3, 4, can the authors provide more clarity on what kind of initialization should be used? This is important since initial value is a key factor in the NS loss training\n \n",
            "summary_of_the_review": "This work pushes the understanding of NS loss functions and conditions under which the training works well and does not. It also provides a good theoretical understanding of the convex region in the NS loss which is useful. The proposed loss also has useful applications beyond KGE in my opinion.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the convex region of the Bregman Divergence function of negative sampling, and apply it for the knowledge graph to show the computations. The paper is a theoretical study which shows that, the loss function is convex about the global min, but not convex everywhere, and especially, the gradient can tend to 0 when the variable tends to infinity, which can cause slow convergence. Thus the paper studies the mechanism that keeps the variables in the region that the function is convex, which guarantees fast convergence. Then in Sec. 4.2 a new function is proposed to eliminate the dependence on $\\nu$, with discussion in Sec. 4.3. This paper also shows the relevant experiment results to verify their discussion of limitation of the other method in Sec. 4.3.",
            "main_review": "The summary is given below. I have several specific questions as follows, which makes me unsure about the overall rating. If the questions are addressed I'm glad to increase the score.\n\n1. In Sec. 2, when the authors define Bregman Divergence and fix $f$, does that mean $g$ is the variable to minimize over? If so explicitly write that and then it's reasonable to remove the terms that does not relate to $g$.\n\n2. A lot of notions in Sec. 2 are not properly defined. Such as $\\nu, p_n, p_d, x, y, y_i, \\theta$, etc. I read Sec. 2,3 of (Kamigaito & Hayashi, 2021) and the meaning of the notions are properly given in that paper. I think at least this paper should refer to the exact paragraphs of (Kamigaito & Hayashi, 2021) and say it uses the same definitions. And I do believe that the most appropriate way is to define them again in this paper.\n\n3. Figure 1 assumes $f=1$, but in Sec. 2, Bregman Divergence is defined with respect to Distributions $f,g$. So what does $f=1$ mean? Does it mean that, the distribution is discrete and has finite number of supports, and it will be an integral, or a sum, of the same functions applied at the supports over some measure, and Figure 1 shows the function (before integral) at the support$=1$? \n\n4. What is the variable for the whole problem? From the figure, table and propositions, I can see the objective landscape and convex region when $g$ is the variable. But will $g$ be parameterized, and is $\\theta$ the actual variable to be learned? Which variable should the objective function to be convex about?\n\n5. To save space, I think Prop. 1-5 are simple facts about the first/second derivative of a scalar function, which is simple. They can be merged into a single Prop. item. Figure 1 is good for illustration. I don't think the proof (although short) for each Prop. is necessary, they can appear in the supplement.\n\n6. I think convexity can guarantee that the convergence is fast, but can the function that's weaker than convex also be easy to optimize? For example, from Figure 1, I can see that the function is Gradient Dominant when $g$ is small enough, and gradient dominant function is easy to optimize as well. Does that mean we only need $g$ to be small enough, which can be slightly larger than $f + \\sqrt{f+1}$?\n\n7. How is $\\nu$ eliminated in Sec. 4.2? Is it by defining the objective that does not involve $\\nu$, and instead involves $\\alpha$? Is it similar to the case $\\nu=1$ if regardless of $\\alpha$?\n\n8. Before Sec. 4.2, it is suggested that the new objective function ''satisfy our induced conditions''. Does the condition refer to \"falling in the set that objective function is convex\"? Is there a discussion below about how the conditions are satisfied? \n\n9. I feel Sec. 4.2 aim for the definition of SSNS and Sec. 4.3 is a comparison. Is there a theoretical proof/discussion why SSNS is good?\n\n",
            "summary_of_the_review": "I think the authors attempt to give rigourous analysis of the convexity of the objective function, and the math derivations are mostly correct.  The topic is indeed important from the perspective of optimization, because convex optimization has important rate guarantee. However, I find that the notations are poorly defined and not self-constrained, especially compared with (Kamigaito & Hayashi, 2021). I'm also not sure about several claims, which are proposed above. I'll comment on those issues partly based on my guess of their meanings, I hope that there is a extensive supplement material that addresses and explains those issues, which can make it much more reader-friendly. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper considers negative sampling (NS) loss function for classification problems with a large number of labels, including Knowledge graph embedding (KGE). In particular, it aims at providing a theoretical inverstigation into the properties of the known NS loss functions such as convexity, and what is needed for efficient model learning. The main contributions are: showing the commonly used NS loss function is non-convex but has a partial convex domain, and identified conditions (like non-negative scores on the edges in the knowledge graph) such that if they are satisfied, the loss function behaves like a convex function. The authors proposed a new objective function (SSNS) which is a variant of the original form but meets the aforementioned conditions. Then the work complements the theoretical findings with experiments. ",
            "main_review": "The paper gives a thorough discussion about the convexity properties of NS loss function, and based on the properties found, proposed another variant of the NS loss function that has the advantage of being convex. The authors provided a detailed analysis on the difference between the proposed new variant and the old one: intuitively they can be regarded as being derived based on different noise distributions. Acknowledging that the two distributions might not necessarily be close to each other, the paper then seeks to show that the new variant gives desirable results in most cases using empirical evaluation.\n\nI think this work contains some interesting ideas and has a good framework that well supports the new loss function design. However, as a reviewer from a broader community, I find it hard to judge the significance of the findings. During the introduction, the authors briefly mentioned contexts for KGE and loss functions used for it, NS and SCE. I think the authors could provide a more comprehensive overview of the KGE problem and a full comparison between common loss functions used. Especially, since the authors also mentioned that SCE basically performs better than NS in many cases, it might require a stronger motivation for studying a new variant of NS and thus claim that the findings adds a lot to this problem. Most of the theoretical claims in the paper are not technically surprising. The experiments seem to support the claim, but the design is simple and the exploration is limited.\n\nMinor comments:\n1. I think $\\nu$ is not defined when it is used for the first time? A more detailed description of the vanilla NS loss function could be helpful.\n2. The sentence \"it is desirable to use a larger learning rate\" is repeated twice on page 4.\n3. Apart from NS, is there any other loss function that applies to the datasets (e.g., what about SCE mentioned earlier?)",
            "summary_of_the_review": "I think the paper has provided new insights into the correct function to use for the KGE problem and at least partially justified the claims. Still, the paper could have done better in some aspects.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors study negative sampling (NS) loss functions in the knowledge graph embedding (KGE) area. They conclude that the NS loss is non-negative while having a partial convex domain. The authors argue that the non-convexity of the losses may degrade the performance of KGE models. Accordingly, they propose a new negative sampling strategy such that the loss function takes values in the partial convex domain and conduct experiments to demonstrate its effectiveness.",
            "main_review": "**Strength**\n\n1. This paper studies an interesting problem---the non-convexity of negative sampling loss functions may degrade the performance of KGE models. It provides a new aspect to design advanced training strategies to further improve the performance of existing KGE models.\n2. The authors provide detailed hyperparameters in the Appendix.\n---\n**Weakness**\n\n1. Some claims are not well-supported.\n\n   1. The problem caused by the non-convexity of NS losses is analyzed without experimental verification. It is unclear how much this problem affects the final performance. Besides, the analyses to conclude that we require different learning rates for different regions are quite intuitive. I think at least a reference supporting these claims is necessary.\n   2. Bottom of Page 5: “There are cases where it is **impossible** to fit a model to a training data where ....” This is a strong claim and it is unclear to me why it is valid.\n   3. Section 4.2: The authors claim that“the first problem can be dealt with by ...” and “the second problem can also be dealt with by ...”But why? The motivation is weak and it requires more discussions.\n2. Methodology\n\n   1. Propositions 1 and 2 lead to the main conclusion of this paper. Only proving them using Table 1 seems to be weak. These propositions deserve proofs with more rigorous deviation.\n   2. Proposition 6: Why is the assumption that p_d(y) is uniformly distributed reasonable? I agree that we can assume p_d(x,y) in the knowledge graph is uniformly distributed since a tuple appears at most once in a KG. However, the label y may appear many times.\n   3. Formulation (5): Why can we approximate p_d(y|x) using this formulation? Suppose that a query x has two labels y_1 and y_2, then both p_d(y_1|x)=p_d(y_2|x)=1. However, the formulation (5) cannot assign 1 for two labels due to the property of softmax.\n   4. I am not sure that the claim “unlike the NS loss, the SANS loss is not directly affected by the number of samples of negative examples” is valid. Actually, [1] has shown that using the NS loss, the number of negative samples is not related to the objective distribution.\n3. Experiments\n   1. The authors mention that they pretrain the KGE models using SANS loss while not specifying the number of pretraining epochs.\n   2. The performance improvements of the proposed SSNS over SANS are marginal. The authors may want to provide the error bar of their experiments. Although the improvements on YAGO3-10 seem to be more significant, I find the performance of RotatE with SANS is lower than the reported results in their original paper (MRR: 48.7 vs. 49.5, Hits@10: 66.4 vs. 67.0). Compared with the originally reported performance, SSNS does not show great advantages over SANS on YAGO3-10 as well.\n4. Writing\n\n   1. The fourth paragraph on Page 2: SANS appears without explanation or reference.\n   2. Section 4: the meaning of (x,y) is unclear.\n   3. Bottom of Page 4: duplicate“it is desirable to use a larger learning rate ...”\n\n\n[1] Hidetaka Kamigaito and Katsuhiko Hayashi. Unified interpretation of softmax cross-entropy and negative sampling: With case study for knowledge graph embedding. ACL 2021.",
            "summary_of_the_review": "This paper studies an interesting problem in the knowledge graph embedding area. However, a) some claims are not well-supported, b) some critical points about methodology are confusing, c) and the empirical results are weak. I do not think the current submission can meet the bar of ICLR and therefore recommend a rejection.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}