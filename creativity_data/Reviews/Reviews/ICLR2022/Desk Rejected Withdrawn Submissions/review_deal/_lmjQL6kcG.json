{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the role of the MLP projectors in self-supervised learning methods. The authors find out that the performance of the linear probe of supervised models decreases at stage 5, while the performance of the linear probe of self-supervised models continues to increase. By adding the MLP projectors to the supervised learning models, the authors can recover the transfer performance of self-supervised models or even outperform them.  To analyze why MLP projectors improve the transferability, the authors propose and validate several hypotheses with experiments. ",
            "main_review": "Pros:\n\n1)\tUnderstanding why self-supervised models are more transferable than supervised models is very important to the deep learning community. This works studies the role of the MLP projectors in this problem and the conclusion is somewhat surprising. This not only helps improve the transferability of supervised models but help understand the mysterious role of MLP projectors in self-supervised learning.\n2)\tThe experiments are reasonably designed. The authors test the transferability n diverse tasks, making their conclusion convincing.\n\nCons:\n1)\tThe theoretical analysis is limited without clearly insights. However, the empirical part of the paper is strong.",
            "summary_of_the_review": "The findings of this paper is interesting and should be known to a broad range of deep learning subareas. The experiments are solid and convincing. Therefore, I tend to accept this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors proposed to address the transferability gap between supervised learning models and unsupervised learning models. An MLP projector is added during supervised learning, which resulted in the performance improvement. The argument was that features learned this way can retain intra-class variation and are thus more robust for downstream tasks.",
            "main_review": "STRENGTHS\nThe motivations in the paper were well established. It is practically a relatively simple idea, but the main claims are validated by experiment results on the unseen class generalization task.\n\nWEAKNESSES\n1. Empirical as it is, I wonder whether there is a resemblance between \"SL vs. SL-MLP\" and \"MoCo v1 vs. MoCo v2\". In Figure 2, no results were shown on MoCo v2, and based on my understanding, one improvement in MoCo v2 is the addition of MLP to MoCo v1. In other words, is it possible that if stage evaluation is done to MoCo v2, the accuracy curve would look more like Byol than MoCo v1, which already justifies the necessity of MLP projector in USL approaches, and this proposed method is simply bringing it into the SL setting?\n\n2. It would be important to analyze how does the stage evaluation look like when SL-MLP is applied to downstream WITH the MLP part. If it there's a stage 6 and the accuracy goes down in the similar pattern as the SL curve, is it indicating that for SL pre-training, one should just use the lower-level features on stage 4 instead of 5 for downstream? Thus, adding MLP to SL is just equivalent to having a deeper network, where the features in the deeper network work in the same manner that stage 4 is better than stage 5 for downstream?\nThe results of ResNet34 deepens my concern, because it is showing that if I use stage 5 in ResNet34+MLP, it is very similar to ResNet50, and if the trend in Figure 2 remains, does it mean stage 4 in ResNet50 can be actually better than stage 5 in ResNet50, and thus better than stage 5 in ResNet34+MLP? If that's the case, this is barely saying that to get better performance, one should just use a deeper network with stage 4 features for downstream tasks?\n\n3. This is a more detailed question, in Table 4, unless I missed some information, if SL-MLP is end-to-end supervised fine-tuned by COCO and Byol is unsupervised pre-trained plus linear classifier, then the comparison is not fair because the same Byol can do many other downstream tasks but the SL-MLP trained this way is just targeted at COCO and loses its generalizability. Also, the improvement from SL to SL-MLP is marginal and not as large as on unseen class generalization task, which may also indicate that the gain over Byol is just coming from supervised fine-tuning?",
            "summary_of_the_review": "The idea is interesting, but I'm not able to justify the methodology improvement over existing works, because it might be just posing a commonly known practical trick during training in a different perspective where it can work on one task but the generalizability is limited.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper targets a supervised training setting that first train a model on a dataset with labels and then freeze the model backbone and only train a new classifier on another new dataset with labels. They claim in this setting, the accuracy of the new datasets reflects the transferability of supervised pretraining. \n\nvanilla:\n1) train backbone + classifier1 on dataset1 with labels.\n2) train freezed backbone + classifier2 on dataset2 with labels. (The classes of dataset2 are different from those of dataset1)\n\nthe proposed method:\n1) train backbone + **MLP** + classifier1 on dataset1 with labels1\n2) train freezed backbone + classifier2 on dataset2 with labels. (The classes of dataset2 are different from those of dataset1)\n\nThis paper claims the accuracy of the dataset2 testing set reflects the transferability of supervised pretraining.\n\nFigure 3 of the paper also clearly shows the proposed method. ",
            "main_review": "### Advantages\n1) The paper is easy to follow.\n2) Figure 3 well shows the idea of the paper.\n\n### Disadvantage\nthe idea is direct. In the authors' proposed setting, pre-training is supervised. In the fine-tuning stage, the model backbone is totally freezed and only a linear classifier can be trained. Since label classes of the pre-training dataset and evaluation dataset are different and there is a domain gap, it is easy to understand the vanilla. Besides the direct method in the paper, there are several direct methods: 1) in fine-tuning stage, the full backbone or just the last several layers (like the last Bottleneck of resnet50) can be fine-tuned. 2) in fine-tuning stage, directly delete the last several layers  (like the last Bottleneck of resnet50).\n\n",
            "summary_of_the_review": "This paper seems to propose an unreasonable setting and uses a direct method to solve the problem resulting from this unreasonable setting. I suggest rejecting this paper. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies transfer learning for image classification problems. The main message of the paper is a multi-layer perceptron predictor can significantly improve the transfer performance compared with standard linear predictors. The experimental results are conducted in its own setting that splits the ImageNet 1000 classes into two disjoint training and testing transfer learning sets.",
            "main_review": "I appreciate the faithfulness of the experimental results and empirical findings of this work. I will elaborate with the weakness of the paper below.\n\n1) MLP predictor improves the linear probe performance has already been largely studied by contrastive learning, especially pioneered by SimCLR and following works such ash BYOL. Studying the linear probe performance for supervised models is very minor. The essential reason that MLP is better than a linear layer is similar for contrastive models and supervised models.\n\n2) The transfer learning setting that splits ImageNet into disjoint sets and evaluates the performance with linear probe is unrealistic. It is always possible to fine-tune your full model on the new data. The finding that MLP improves the performance is conditioned on this unrealistic transfer learning scenario. If you fine-tune the full model, I guess there would not be significant differences on whether to use a MLP or not.\n\n3) The paper is motivated by the detection performance in the introduction. However, none experimental results are provided to improve the supervised models for detection transfer. ",
            "summary_of_the_review": "The paper presents a series of results for transfer learning. However, the finding that MLP predictor improves the performance is either well-known already or very minor for a paper to be published. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}