{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors study the problem of setting baseline values (meant to represent the absence of a variable) in the computation of the Shapley value, a concept from game theory that is used to measure the influence of a particular input variable on the output from a DNN. In a departure from prior work, which tries to set baseline values empirically, the authors formulate a game in which they try to find the optimal baseline values. They also show that the Harsanyi dividend can be adapted to represent multi-variate interactions and assign credit in a way reminiscent of the Shapley value. Lastly, they present experiments to support their approach.\n\n\n\n ",
            "main_review": "The Shapley value is a very nice theoretical tool for determining the impact of various parameters in DNNs, and I agree that previous work has been slightly too cavalier with the way they’ve set baseline values. I greatly appreciate this paper’s aim to formalize a method for better calculating baseline values when computing the Shapley value.\n\nThe results seem to be technically involved and correct (although I didn’t read the appendix). The experiments were also fairly compelling.\n\nHowever, I found this paper quite difficult to read; I think it would benefit from significantly more exposition particularly around the game-theoretic formulation and approximate calculation of baseline values. Also, a more formal description of this step would be nice (including perhaps formal pseudocode). \n\nQuestion: How do you set the threshold for salient patterns \\tau? \n\n\n\nMinor comments: \nPlease define the Harsanyi dividend I(S) in the body of the paper (I think it’s defined in the appendix but not the main body?). \nSection 2: AumannShaple --> Aumann-Shapley?\n",
            "summary_of_the_review": "The central question of the paper seems compelling and important for ML transparency. The technical results seem good, but the exposition needs work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper aims to efficiently and accurately approximate Shapley values for the purpose of measuring feature importance in deep neural networks. To compute Shapley values, a characteristic function must be evaluated on a subset of the features, in particular, when a variable is and is not present. The problem of how to simulate \"removing\" a feature / variable from a neural network has led to several approaches which this paper argues lead to poor approximations of the Shapley value. Instead, this work proposes leveraging the connection of Shapley values with the Harsanyi dividend to provide an alternative way of simulating \"removing\" a feature via maximizing the number of salient patterns associated with higher order interactions of features / variables.",
            "main_review": "Strengths: Overall, leveraging the connection to the Harsanyi dividend to derive an alternative perspective of computing Shapley values is interesting. And defining the \"absence state\" as minimizing the number of salient (particularly low order) interaction patterns is clever.\n\nWeaknesses:\n1) The technical term, \"baseline value\", is not defined until p. 4 in equation (2). To my knowledge, \"baseline values\" are not apart of the original theory of Shapley values, and so it would help to introduce a precise definition much earlier than p. 4. Note that \"baseline values\" are only needed when explaining deep networks with Shapley values if you insist on using a single neural network architecture that takes all features as input (contrast with approach cited in bullet 2 below). Only then do you need to find a \"baseline value\" that is equivalent to a scenario in which the neural network never \"sees\" that feature.\n2) An alternative, possibly more direct translation of a \"characteristic function\" and Shapley values is explored in \"Game-theoretic Vocabulary Selection via the Shapley Value and Banzhaf Index\" by Patel et al (https://aclanthology.org/2021.naacl-main.223.pdf). In that paper, they define the characteristic function, v(S), as the loss / output of a model trained on data which only retains features from S. The upside of this approach is that it avoids creating problematic \"baseline values\". The downside of this approach is that it requires training a new model in order to evaluate v(S), which is very expensive. I would suggest adding this paper to your related work.\n3) The connection between Shapley values and the Harsanyi dividend isn't detailed until p. 7, equation (6). The paper hinges on this connection, so this should be made much earlier. In fact, I found equation (10) in Appx. B.2 to be much more helpful in clarifying this connection. I suggest stating equation (10) soon after introducing Shapley values.\n4) How do the losses in equation (7) depend on the \"baseline values\", b, being learned? Are you learning Harsanyi dividends, I(S), via the characteristic function v(S) (eqn 4), via b (eqn 2)? Equations 5 an 7's dependence on the baseline values is not clear. The paper \"aims to formulate the problem of estimating optimal baseline values\" but baseline values do not appear explicitly in any of the loss functions, making it difficult to assess the exact approach.\n5) In the experiments, a discussion of Figure 4 suggests that your approach is performing better than the others, but I fail to see this from the figures. All except for the zero baseline and baseline in SAGE seem to perform similarly. Any performance difference seems very marginal.\n6) I see that your methods agreed somewhat with SHAP and SAGE, but I'm not sure how much I can extract from this experiment. Is that the only conclusion I am meant to draw?\n7) In equation (7), if the absolute value was around the expectation in $L_{marginal}$, would it be essentially the same as $L_{shapley}$? Is that the main difference between these two losses and why you say $L_{marginal}$ is more fine grained?\n8) The penultimate step of proving the dummy property in Appx B.1 is used frequently throughout proofs. This deserves a citation or lemma. I believe it is true, but this is a combinatorial / binomial result that I cannot easily confirm by inspection.\n9) Equation (5) gives an intuitive approximation of an absence state, but I see no rigorous argument for why V(S masked with baseline b) = V(S)?\n10) Why is the number of salient patterns the correct metric? Why not some weighted sum? Why not some other function of the salient patterns? This seemed a bit arbitrary to me.\n11) In table (6), does the left most column labeled \"Functions\" contain the Harsanyi dividends $I(S)$? I wasn't clear what \"Functions\" refers to.\n\nMinor: In some places, you refer to \"massive variables\". I assumed you meant massive \"numbers of\" variables, i.e., many features. Maybe try to reword those parts so it's clear to the reader what you mean.",
            "summary_of_the_review": "Overall, I think the approach here is interesting, but I had a hard time pinning down precisely what that approach was. I read through some of the results in the appendix, which I actually found quite interesting as well. I think some of those theoretical results should be pulled into the main body (especially equation 10). Lack of clarity on the approach and slightly underwhelming experiments led me to my score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This manuscript:\n1. introduces a method for calculating _baseline values_ in Shapley-style measures of feature importance to more closely approximative the game-theoretic understanding of an agent's absence: the 'absent' feature should add no information to the prediction problem;\n2. proposes to explore the method using an _interaction index_ based on the Harsanyi dividend;\n3. derives Shapley values using the proposed baseline values, which are computed using loss functions incorporating elements of the Harsanyi interaction index.\n\nThe problem addressed in the first point seems significant to me.  I am less convinced that the approach taken has been properly substantiated.  Overall, I would like to see the extraneous material cut to focus on making the main argument properly.",
            "main_review": "**Strengths**\n1. I believe that the problem of identifying appropriate baseline values for Shapley-style measures is an important one.  This paper presents what seems - to me - to be a novel approach to that.\n\n**Weaknesses**\n1. overall, I did not find the paper 'tight': \n    1. material is repeated (e.g. \"The Shapley value (1953) was first introduced in game theory\")\n    2. some arguments are made verbally when they should be made mathematically (e.g. the main desiderata of the baselines: \"remove all information\" and \"not bring in new/abnormal information\")\n    3. even explanations that are appropriately verbal are often unclear: e.g. \"patterns\" seems to be an misleading way of saying \"large values\" (ditto \"interaction\", \"noisy\" and \"salient\" patterns); terms like \"unbiased\" seem to be used without any real meaning (and certainly not the statistical meaning); the Shapley value isn't \"ensured\" to be \"fair\" in any objective sense, but merely satisfies what Shapley thought were fair axioms; higher order interactions don't depend on \"massive variables\" (\"massive\" variable may indicate scaling problems; constant but large variables may drop out of Shapley analyses entirely).\n    4. the empirical evidence presented is not compelling: re: Fig. 1, any alteration of pixel values necessarily 'looks' wrong to us, so I'm not convinced that the figure demonstrates any problem with existing baselines.  (For closer comparison, the proposed baseline technique should also be demonstrated on the same image.) re: Fig 4, what am I _supposed_ to see?  For the right subfigure, _should_ I see an outline of the actual digit, an idealised one, etc.? re: Fig. 5: again, what do your two methods bring, other than being different?\n    5. etc. e.g. \"We discover that when we use ...\"  The Harsanyi dividend is well established: the decomposition in (3) is probably not original? e.g. the section on adversarial examples seems thrown in, rather than advancing the main narrative (and how are the bivariate interactions \"actually related to\" higher-order ones?).\n2. as a partial consequence, too much of the main narrative is pushed into the Appendix, weakening the exposition.\n2. as another consequence, the main body is not able to properly explore issues that should be considered: \n    1. knocking out terms (in this case higher-order ones) is what e.g. LASSO does; how is this similar to feature selection techniques like that?\n    2. _should_ we use $L_{Shapley}$ or $L_{marginal}$?\n    3. my initial guess, when I understood how uninformative baselines were being thought of, was that this would be a fixed point problem: $v_0 (\\emptyset)$ would allow calculation of $\\phi_0 (mask(x_i))$, which would allow computation of an updated $v_1 (\\emptyset)$, an updated $\\phi_1 (mask(x_i))$, etc.  This would seem to offer the possibility of zeroing out 'absent' features.  Why doesn't this work?  However well/poorly it works, how does this compare to the Harsanyi dividend loss approach taken?\n3. the overall narrative is unclear\n    1. I _think_ that I understand that relationship between the Shapley value (the measure we're actually interested in) and the Harsanyi dividend terms (which are just used to inform loss functions), but am not sure\n    2. why are we using Harsanyi dividends rather than any other interaction or joint index?",
            "summary_of_the_review": "I think that the manuscript's core idea - a method for calculating uninformative baseline values for Shapley-style measures of feature importance - is an interesting and potentially important one.\n\nOverall, at this point, I think the amount of work that needs to be done to make this a strong enough paper is significant.  (For a journal, I would recommend \"major revisions\".)  I think this would only be feasible if the authors are able to devote considerable time to the paper over the few weeks available for revision.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies the problem of identifying the baseline value of Shapley value. The major contribution of this paper is to propose a method that learns the baseline value that represent the absence states of variables, following the criterion of not bring additional information. The paper performs empirical evaluation on their proposed method, and present some preliminary experiments results.\n\n",
            "main_review": "Let me first explain the context of this paper. In the application of credit assignment on neural network feature, one aims to identify the contribution of each input coordinate, and computing the Shapley value is one possible approach. The computation of Shapley value involves masking input coordinate and compare the utility different. Some previous study on this line suggest that this is a tricky problem, e.g., it is possible to use zero to mask the input variable, but this will breaks the input image and bring some useless information. The major focus of this paper is to identify and learn an approach to identify the baseline value (i.e. the value one fills in to represent the absence of a variable).\n\nStrength. The paper figures an interesting question, i.e., identifying baseline of Shapley value could be tricky. The proposed method seems to overcome part of the issue.\n\nWeakness.\n(1) The first concern is on the methodology of this paper. Though Shapley value possesses good game theoretical property and it is a good approach for credit assignment problem, however, it is by no means the only or the optimal approach. In another word, Shapley value is not guaranteed to be good and could have flaws. Actually, the author identifies one issue, i.e. the baseline value problem, though it is easy to set the input coordinate to be zero, it could breaks the input distribution (say breaks a good image).  I would rather say setting the value to zero is correct, and it is a drawback of Shapley value approach. The author proposes a method to overcome this tricky point, however, after this, it is not clear the baseline value is really a *baseline value*, it already incorporates some information and doing credit assignment. I want to emphasize that this is not the sole reason that I tend to reject this paper, but I hope the author could rethink the methodology of this paper. (2) Though the author propose an interesting issue of Shapley value, its solution is not satisfactory. The method seems not very computational efficient, and it is kind replacing one black box with another black box.\n(3) The experiment looks not that exciting. For example, looking at Figure (4), it is not clear the new method is good.\n\nSome missing reference\n[1] Ghorbani, Amirata, and James Zou. \"Data shapley: Equitable valuation of data for machine learning.\" International Conference on Machine Learning. PMLR, 2019.\n\n",
            "summary_of_the_review": "In summary, I tend to reject the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}