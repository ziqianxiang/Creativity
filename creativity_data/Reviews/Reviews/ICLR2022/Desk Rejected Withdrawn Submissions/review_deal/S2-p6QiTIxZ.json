{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper builds up in the idea of the classical most uncertain query strategy, where  the closeness of a sample to decision boundary is used / needed. However, the paper claims that when using DNNs in multi-class classification, it is harder to identify samples close to the decision boundary as the sample’s closeness based on Euclidean distance is often not readily measurable.\n\nThe paper proposes the idea of the LPDR region: a closeness measure that can be evaluated in multi-class classification with deep network as a measure of uncertainty. They assume that the most uncertain and thus the most informative samples will have their labels most “sensitive” to the smallest perturbation of the decision boundary.\n",
            "main_review": "Pros:\n\n- The paper is well written and it is easy to understand and follow. I has appropriate references.\n- Experiments seem sufficient and they are well documented in the appendix.\n- There seems to be marginal improvements wrt other existing methods\n\nWeaknesses: \n - when to sampling w for  large models with millions of parameters, I am skeptical about the quality of the estimation of the disagreement D. I guess N has to be bigger. how big it was in the experiments, I don't think I saw that in the paper. it seems like it could be more robust  for linear models with an smaller number of parameters.\n- Is the whole parameter set resampled in the experiments or only the last layer?, I guess the answer seems to be yes but did you considered resampling only a few of the last layers?\n - A concern is that the results are fine but marginal in MNIST, CIFAR10 and SVHN\n\tAdding the number of parameters to be samples by model would be useful information for the experimental results. For example: how many parameter have WRN-16-8,  is it ~11MM parameters?\n\tThe delta seems to be the same in all the dimensions of w, variance of I\\delta. Should not  the variance be scaled proportionally by the  magnitude of each parameter. Otherwise the final Deviation measure can be highly influenced and bias by some parameters based on their magnitude.  Can you please explain?\n\n\n",
            "summary_of_the_review": "The paper presents an interesting framework for uncertainty-based active learning. I  think the ideas presented are interested but there is a couple of topics that need further attention / explanation. It would be interesting to have more insight on how the size of the parameter space affects the computational complexity and the performance of the proposed algorithm and there is little of that in the paper in its current form. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new sampling strategy for active learning (DRAL) that\nstems from a new measure of informativeness of samples: LPDR. In short, LPDR of\na data point $x$ for a hypothesis $h$ is the smallest disagreement probability\nof hypotheses that disagree on $x$. It is argued that this measure is related to\ncloseness to decision boundary, hence the data point with smallest LPDRs are\nthose considered most informative.\n\n\nThe intuition conveyed in the paper is that if one \"perturbs\" the decision\nboundary a little (by selecting a hypothesis that has small disagreement\nprobability from $h$), if the label of $x$ changes, this means that $x$ has low\nLPDR for $h$, hence close to the decision boundary. \n\nAll the relevant quantities for evaluating the LPDR cannot be computed exactly\nin general, and empirical estimates for these quantities are provided in the\npaper. For estimating the disagreement probability, a standard sample average is\nused, while for estimating LPDR, one has to construct a finite class of\nhypotheses in such a way to get a good estimate. In the paper, one way of\nestimating is conjectured (empirical LPDR).\n\nIt is further conjectured that for specific perturbations of hypotheses,\nempirical LPDR is equivalent to disagreement ratio (the ratio of hypotheses that\ndisagree on $x$ with $h$ among a sampled set of hypotheses). Hence, one can\nsimply run the disagreement ratio active learning instead of empirical LPDR. \n\nThe paper provides experiments on real datasets and provides a comparison with\nSTOA. The authors provide empirical insights on their choices and all relevant\nplots are present in the paper. ",
            "main_review": "All in all, the paper touches upon an important question. \nIt provides a good list of prior works with their weaknesses. The authors\nprovide a compilation of plots in accordance to their assumptions throughout. \n\nThe experimental results are quite promising, with the proposed method\noutperforming many existing methods for active learning on neural networks. The\nauthors perform several sets of experiments which give good insight into the\ngeneral power and robustness of the methods.\n\nHowever, I am concerned about a few points which I list below:\n- The discussion about why LPDR is corresponding to the closeness to the\n  decision boundary is not very thorough. As one of the main contributions of\n  the paper, I expected to see more relations between these two quantities. Only\n  for the case of linear classifier with fixed weights, it is described and it\n  is not clear whether there is a general relation between these two (the\n  intuition I wrote above might be a good hand-wavy start, but a rigorous\n  treatment is missing) \n\n- In the proof of __Theorem 1__, I cannot follow the conclusions made by (4) and (5). Here is my understanding: \nFix $\\hat{h}, x, \\epsilon > 0$. Take $C$ large enough that Property 1 holds. By Hoeffding and a union bound, on an event $A$ with probability at least $1 - 2C\\exp(-2\\epsilon^2 S)$ one has a good uniform approximation of $\\rho(h_c, \\hat{h})$ through its empirical estimate $\\rho_S(h_c, \\hat{h})$: \n\n   $$\n    \\forall h_c \\in \\mathcal{H}_C(\\hat{h}, c), \\quad |\\rho_S(h_c, \\hat{h}) -\n    \\rho(h_c, \\hat{h})| < \\epsilon.\n  $$\n\n  The next step is to take an arbitrary $h \\in \\mathcal{H}(\\hat{h}, x)$. Then, by Property 1, there __exists__ some $h_c = h_c(h) \\in \\mathcal{H}_C(\\hat{h}, x)$ such that\n\n  $$\n    |\\rho(h_c, \\hat{h}) - \\rho(h, \\hat{h})| \\leq \\epsilon.\n  $$\n\n  I stress again that $h_c$ depends on $h$. Combining these two inequalities gives the following on event $A$:\n\n  $$\n    \\rho(h, \\hat{h}) \\leq \\rho(h_c(h), \\hat{h}) + \\epsilon \\leq \\rho_S(h_c(h), \\hat{h}) + 2\\epsilon,\n  $$\n\n  and similarly, a lower bound. The authors proceed by taking an infimum over all $h\\in\\mathcal{H}(\\hat{h}, x)$ from both sides. The left hand side becomes $L_{\\hat{h}}(x)$ by definition, while the authors claim that the right hand side becomes \n$\\inf_{h_c \\in \\mathcal{H}_C(\\hat{h}, x)}{\\rho_S(h_c, \\hat{h})}+2\\epsilon$.\n\n However, note that in the inequality above, $h_c$ _depends_ on $h$, hence, the infimum $\\inf_{h\\in \\mathcal{H}(\\hat{h}, x)} \\rho_S(h_c(h), \\hat{h}) + 2\\epsilon$ is in general _bigger_ than the infimum over all possible $h_c$. I might be confused, and would be delighted if the authors elaborate on this matter. \n\n- In the same proof, there is an asymmetry between $\\epsilon$ and $C$. Property 1 states that for every $C$ there is an $\\epsilon_C$ and if $C \\to \\infty$, then $\\epsilon_C \\to 0$. Hence, in the consistency proof (given that the computations before are correct), we get that with probability at least\n\n  $$\n    1 - 2Ce^{-2\\epsilon_C^2 S},\n  $$\n\n  empirical LPDR is $\\epsilon_C$-close to LPDR. Then it is claimed that as $\\min(S, C) \\to \\infty$ and $(\\log C)/S \\to 0$ this probability converges to 1. A problem of this argument is that $\\epsilon_C$ is not fixed; as an example consider $\\epsilon_C = \\frac{1}{C}$ and set $S = C = n$. All the conditions are satisfied by this example, while the limit of $1 - 2ne^{-2/n}$ as $n \\to \\infty$ is $-\\infty$! This means that $S$ must be _much_ larger than $1/\\epsilon_C^2$ for the bounds to be nonvacuous. I believe the rate of decrease of $\\epsilon_C$ is crucial, even for proving asymptotic consistency with the techniques used by the authors (union bound and Hoeffding).\n\n- In the proof of rank-order consistency, I do not understand how the LDPRs of $x_i$ are distinct. Also I quite did not understand how uniform convergence is implied by (5).\n\n- Statement of the Conjecture 1 is not clear for me. What does it mean to be\n  increasing in the limit? in which sense?\n\n- In the fourth line after Conjecture 1, how the argument about the diameter is concluded? ($\\max_{\\cdots} \\min_{\\cdots} |\\rho_S(\\cdots) - \\rho_S(\\cdots)| \\leq \\epsilon_C$). The assumptions made on the previous line are about expectation.\n\n- In the third line before section 4.2, why the assumption about existence of such $\\sigma_*$ is plausible? This assumption seems to be very strong (and I believe is one of the important points of the paper that DR is a good proxy for empirical LPDR) and looks like assuming the solution. I would appreciate it a lot if the authors provide a more rigorous ground on this assumption.\n\n- The conjectures made in the paper (Conjectures 1 and 2) are hard for me to relate to. It would be nice if the authors made the conjectures a bit more general and concluded some parts of their work given the conjectures. At the end of the day, a good conjecture is something that ignites research on a problem.\n\n### minor comments\n- Figure 1a might be misleading. The data distribution is of course important\n  for the LPDR, and some specific data distribution should be chosen/described for the figure.\n\n- flow: last two lines before 3.2 is detached. \n- second line of section 3.1: $h:\\mathcal{X} \\to \\mathcal{Y}$. \n- next line: disagreement metric is defined as the probability of the\n  disagreement region ...\n",
            "summary_of_the_review": "I believe that this paper is a good start, but not grounded, neither mathematically nor intuitively. The flow of the paper suggests that LPDR is going to be the method to be implemented, but suddenly, with an extra inline assumption, the whole paper suddenly shifts towards sampling a batch of hypothesis from a normal distribution over the weights and look at their disagreement ratio. While the experiments are showing that such a method might actually work, I still believe that more thoughts and rigorous arguments are needed to put this paper in the context of other active learning papers. Specifically, a simple study case (e.g., linear classifiers) with rigorous sample complexity bounds and theoretical verification of the assumptions made in the paper would shed a lot of light on this method. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new active learning technique based on the Least probable disagreement region. Basically, it proposes to measure how interesting it would be to learn a label of an instance x by measuring the lowest probability of disagreement  that a disagreeing model with $\\hat{h}$ (the currently optimal model) on x would have. The lower is this probability, the more desirable it is to elicit the label of x, as it is likely to sit on the decision border.\n\nThe rest of the paper is dedicated to how approximate this value computationally, as it is in practice hard to estimate. The resulting approximation is then tested on a bunch of data sets and deep learning models.  ",
            "main_review": "While I think the paper idea is interesting to investigate, the paper makes many assumptions, some unproved or whose plausibility was hard for me to assess, that makes the theoretical aspects of the proposal hard to really assess. In particular:\n\n* Most of the theoretical analysis and results rely on the satisfaction of Proposition 1, that requires the set of hypothesis to be large enough so that it satisfies an inequality (involving the non-estimable $L_{\\hat{h}}(x)$).\n\n* While the graph in appendix C1 are mostly increasing, this is not exactly true for all of them, for instance HAM10000 around 0 value for $\\log(\\sigma)$. As one counter-example is enough to disprove a conjecture, this puts a serious question mark around the presented conjecture. Most of the curves also do not appear to be entirely continuous. Do the authors really think that this is true, and if so what prevented them from proving them? \n\n* Conjecture 2 is even more dubious, as many graphs did not even reach the value -1, which would have been needed to have a positive example that indeed such a perfect $\\sigma_*$ could exist? It is very hard to conclude from those graphs that the conjecture would indeed be true. And something that is \"mostly true\" should not appear as a conjecture, which is usually used to replace a theorem/proposition authors think is true but could not prove? \n\nSome part of the paper are also really dense, making the paper sometimes hard to follow, for instance:\n\n* The whole paragraph after Conjecture 1 is quite hard to follow, and I must admit I missed some intuition here to fully understand it. Also, the index $(k)$ suddenly appears out of the blue (I gather this means that we are at the $k$th query)? \n\nAlso, the end-resulting approximation and techniques seems to me quite standard, as it amounts to be counting, within an ensemble of models, the number of models disagreeing with the current one. Form this follows two comments:\n\n* one may wonder why the paper focus solely on comparing with deep active learning techniques, whereas it should also show that it has good results with generic learners and recent active learning techniques not dedicated to deep learning techniques. For instance, one could look at (Nguyen, Vu-Linh, Mohammad Hossein Shaker, and Eyke Hüllermeier. \"How to measure uncertainty in uncertainty sampling for active learning.\" Machine Learning (2021): 1-34.)\n\n* It should also discuss the differences with those standard active learning techniques, and why this new one should be better and/or equivalent? Clearly, reducing the discussion to deep learning is a bit frustrating in this case (although probably fashionable). \n\nAs a final comments, I would also say that while the results seem to be promising for the considered method (but I must say I am not familiar with the other baselines, except entropy which is questionable with uncalibrated probability estimates), I am a bit dubious about having strong experts tagging thousands of examples correctly (as the framework assume that labels are given without mistakes). ",
            "summary_of_the_review": "The paper explores what could be an interesting idea, but I miss a larger discussion comparing and discussing the merits of the approach with standard or recent active learning approaches not especially focusing on deep approaches. Also, it is quite hard at times to see if the theoretical part of the paper is firmly grounded, as the support and plausibility of conjectures and hypothesis is kind of hard to assess, if not questionable to some extent. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose an active learning (AL) strategy where they adaptively select the most informative unlabeled instances on their distance to the decision boundary, and query them to improve the training of a classifier. In order to quantify this distance, they introduce a measure, the so-called least probable disagreement region (LPDR). Further, they introduce a hypothesis sampling method, which extracts the order of LPDR measures of samples without explicitly computing them, and query the labels of instances that are in the LPDR. The effectiveness of the method is illustrated on a large set of experiments. The proposed AL strategy seems to outperform the state-of-the-art methods on a variety of datasets.",
            "main_review": "*Strengths:* The idea of perturbing the decision boundary to find out the most uncertain data instances seems promising and intuitive in AL. The assumptions made in the paper are put in conjecture form, and empirically justified in Appendix. The experiments seem extensive and the numerical results seem promising -- especially given how hard it is to compete with the existing AL strategies. \n\n*Weaknesses:* The main result in Theorem 2 is a bit limiting as it concerns only the linear classifiers in the binary classification setting. I am not so clear on why this disagree ratio would be the best. _Did the authors investigate further measures such as entropy? (while sampling the classifiers the same way -- which is not the same as the classical entropy-based sampling)_ \nI also sought for some other experimental details in the main body such as some implementation details on the existing methods, or initial label size, etc. I found them mostly in Appendix.\n\n*Further questions:*\n* Could the authors clarify how did they tune $\\beta$ at the update step of $\\sigma$ in the DRAL algorithm? How sensitive is the method to this hyperparameter?\n* Is table 2 in %?\n* How was the entropy method implemented? As in, how were the classifiers formed such that the entropy of an instance can be measured?\n* As the query size is always at least the initial label size, how do the authors predict the behavior of their method when the initial label size is much smaller than what is used in the results? It would be helpful to see the results in a larger practical regime, i.e., the initial label size is 1-10% of the entire unlabeled pool, etc. This might be good to know as the method heavily relies on the labeled data so far, hence the wrong selection made at the beginning of the process might propagate.",
            "summary_of_the_review": "The idea of perturbing the decision boundary to measure the uncertainty of an instance in AL is promising, and the effectiveness of this idea is sufficiently demonstrated on a set of experiments. In my opinion, this paper is marginally above the acceptance threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}