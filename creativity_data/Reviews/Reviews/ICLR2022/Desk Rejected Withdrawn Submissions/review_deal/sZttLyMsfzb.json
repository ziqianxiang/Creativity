{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper tries to investigate an epoch-wise \"double descent\" (DD) behavior which seems to manifest in bias and variance terms under the decomposition of the excess risk. In particular, the paper first conducts experiments by measuring estimates of the bias and variance on the large trained neural nets for visual recognition. This is used as an inspiration for an heuristic objective design to detect the DD behavior. Later, the paper uses this objective to prevent overfitting in the presence of label noise by early stopping.",
            "main_review": "The paper investigates a possibly interesting behavior of the bias-variance trade-off in SGD-type algorithms, however I feel that the experimental design is somewhat lacking and might lead to unclear conclusions.\n\nThe validation loss clearly shows overfitting behavior after a certain number of steps, and I'm not sure I see any meaningful \"descent\" thereafter (fig 1). A slight downward slope might be attributed to the noise of stochastic gradient, which is unclear. Would running GD eliminate this?\n\nOrthogonal to this point, the paper investigates behavior of the bias and variance, however I'm not sure how this can be done in a meaningful way without access to the regression function / Bayes optimal classifier. I guess, to have a proper demonstration of bias and variance one would need synthetic experiments where everything is under control (e.g. Bayes optimal classifier is accessible). Moreover, bias and variance estimates have significantly larger scale than the loss on the validation set, which makes things even less clear.\n\nImportantly, these experiments do not seem to give new insights why we observe DD behavior of bias and variance. Is it due to variance of stochastic gradient? What is the effect of the label noise rate? What is the effect of overparameterization? Is it present in overparameterized linear models (which could be an excellent starting point)?\n\nSection 3 motivates the \"metric to reflect the function robustness ... to sampling noise\". Traditionally, this is done on a held-out set / by cross-validation. The paper then proposes an \"Optimization Variance\" to be such a metric, which is defined through expectations over minibatches and so one would need access to held-out data to estimate this, which already competes with a simple held-out estimate (for which we can have confidence bounds on the risk, which is a meaningful quantity, wheres it is not clear what \"Optimization Variance\" is a proxy to).",
            "summary_of_the_review": "The paper investigates a possibly interesting behavior of the bias-variance trade-off in SGD-type algorithms, however I feel that the experimental design is somewhat lacking and might lead to unclear conclusions.\n\nImportantly, these experiments do not seem to give new insights why we observe DD behavior of bias and variance. Is it due to variance of stochastic gradient? What is the effect of the label noise rate? What is the effect of overparameterization? Is it present in overparameterized linear models (which could be an excellent starting point)? I think that the paper could be strengthened by studying simple models in synthetic fully controlled simulations.\n\n",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "It was observed in prior work that for some deep learning models trained on data with label noise, the test error undergoes a double descent as training time increases. This paper investigates  empirically this behaviour  through the lens of the bias-variance decomposition of the 0-1 loss proposed by [Domingo, 2000].  The main observation is that the variance mirrors the double descent shape of the test error.  The paper also in a proposes a new generalization measure, defined as a normalized version of the variance of the logits accross training mini-batches, and suggests to use it as early stopping criterion in place of the usual validation procedure. ",
            "main_review": "Epoch-wise double descent has attracted much less attention than model-wise double descent, so I appreciate the tentative of the authors to look closer at this intriguing behaviour. The paper is a nice read, the experiments are clean and nicely complemented with ablations analysis in the appendix. \n\nMy main reservations are about the significance of the results. \n\n**On the bias-variance analysis**.  The fact that the variance varies consistently with the test error is interesting (though not entirely surprising), but does not seem to bring any insight on the origin of double descent. The authors make no tentative to address this problem. \n\nOn a related note, the analysis assumes that data sampling is the unique source of variance. Prior work [Neal et al, 2018] [Adlam & Pennington, 2020] on model-wise double descent proposed fine-grained decompositions to disentangle the role of the different sources of randomness (initialisation, stochastic optimization, data sampling). I believe such a fine-grained analysis would help turning this part of the paper to a  stronger contribution. \n\n**On the use of optimization variance for early stopping**. In deep learning applications data is often abundant and validation sets are available, so in this context the raison d'etre of the method is not immediately clear. Experiments in some low data settings showing a clear performance advantage over well-known low-data-suited methods such as cross-validation would be, in my opinion, much more convincing.\n\n**On optimization variance as generalization measure**. A wealth of generalization measures have been proposed in the recent literature (see Jiang et al, 2019 and reference therein). The observation that OV decreases with the network size in Fig. 5 is interesting, but as a correlation analysis it does not compare well in scope with analogous analysis in the literature. At the very least, I would suggest the authors to add experiments that vary a few other hyperparameters, or the level of label noise, and compare against other existing measures. \n\n\n**References**\n\nNeal et al.   A Modern Take on the Bias-Variance Tradeoff in Neural Networks. arXiv:1810.08591.\n\nAdlam & Pennington. Understanding Double Descent Requires a Fine-Grained Bias-Variance Decomposition. NeurIPS 2020. \n\nJiang et al. Fantastic Generalization Measures and Where to Find Them. ICLR 2020. \n",
            "summary_of_the_review": "Overall, despite potentially interesting observations on an intriguing and not well-explored phenomenon, I find the contributions too weak to recommend acceptance of the paper in its current state.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The work deals with a stopping criterion for deep learning that does not require auxiliary datasets (validation/test). The new criterion optimization variance (OV) is using logits and it resembles coefficient of variation (it is indeed variance/expectation). OV is the only contribution of this work. ",
            "main_review": "Strengths: the proposed OV, the idea of having a stopping criterion not relying on auxiliary datasets\n\nWeaknesses: the contributions are not significant (the expression of OV is nothing really novel), the computational experiments have gaps and it is far from that the proposed criterion is efficient",
            "summary_of_the_review": "The paper doesn't have a significant scientific contribution but despite of this I would evaluate it higher if the experiments would have been more insightful and convincing. \n\nI wonder why the datasets were artificially tweaked by randomly shuffling labels. After all, epoch-wise double descent is a known effect observed in other papers and thus the authors should have used the datasets from other papers (and without modifications). The paragraph below Figure 2 mentions no alteration, but the statement is \"still works pretty well.\" This statement is definitely not using superlative words about OV. \n\nIn Section 3.3, I wonder why the comparison is not done against the standard process (bottom of page 6). The comparison vs something that is not available in 'practice' is definitely questionable. Second, comparing the actual stopping times doesn't reveal a lot. The ultimate goal is to have high accuracy/F1 of the selected model. The section should compare along this dimension. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This work empirically explores the relationship between optimization variance and generalization. One crucial finding of the work is that the epoch-wise double descent is dominated/caused by the increase of model variance.\n\nThe paper then demonstrates the usefulness of this metric by performing early stopping without a validation set.\n\n",
            "main_review": "This is mainly an empirical exploration paper. While I am convinced that the increase in variance dominates the epoch-wise double descent, I am not sure whether this discovery is important enough to be published at ICLR. More importantly, this message is not quite new. Double descent can be exactly analyzed in the setting of linear regression, and the solution of high-dimensional linear regression already suggests the variance explosion is the main cause of double descent.\n\nAt the same time, the discovery also seems to lack practical importance. The authors showed that the proposed metric may correlate well with the early stopping point; then, one naturally expects this to help with improving generalization. If the authors can show that such an early-stopping technique can lead to state-of-the-art generalization on some tasks, I will have no problem with the importance of the discovery and the metric associated with it.\n\nWhile I agree that this paper has scientific value and is a nice collection of interesting phenomenological findings regarding double descent, I find none of the messages of the work sufficiently important/novel for acceptance.",
            "summary_of_the_review": "I recommend rejection because (1) the theoretical implication is not novel/surprising enough, and (2) the proposed practical method lacks a convincing evaluation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}