{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a simple yet effective solution called Feature Clipping to solve the vanishing gradient problem, which adds a hard constraint to the norm of the hyperbolic embeddings. The experiments show the improved HNNs can achieve comparable performance with EMMs on standard image recognition datasets while outperforming ENNs in adversarial robustness and out-of-distribution detection.",
            "main_review": "Strengths:\n1. The vanishing gradient problem is very interesting and the paper proposes a reasonable solution to solve the problem.\n2. The experiments demonstrate the effectiveness of the proposed method in a wide range of tasks.\n\nWeaknesses:\n1. The idea of Feature Clipping is not novel in hyperbolic embeddings. The Poincare ́ embeddings (Nickel & Kiela, 2017) used a similar strategy in Riemannian optimization steps. The proposed method in this paper, either the regularization term in the loss function or hard constraint in the norm, is an incremental improvement to existing HNNs.\n2. In Figure 1, the right figure shows that the Poincare ́ model can be partitioned into areas with unstable computation, vanishing gradients, larger feasible region and limited model capacity. The figure and the partition are very interesting, but there is no theoretical analysis or empirical exploration to support the statement. Although the derivation of Eq. (11) provides a sketch of the vanishing gradient problem and the first experiment shows that HNNs can be improved by Feature Clipping, I think the paper needs theoretical proof or more empirical analysis of the vanishing gradients, since it is the key motivation of the paper. \n\nMinor comment:\n\nIn Section 4 about Adversarial Robustness and Out-of-distribution Detection, do the results of HNN represent the results of HNN with Feature Clipping? If so, it is better to refer to the proposed method as 'HNNs with Feature Clipping' or 'the improved HNNs' instead of 'HNNs' to avoid confusion. The results of the original HNN should also be provided.",
            "summary_of_the_review": "The paper focuses on a very interesting problem, i.e., the vanishing gradient problem of HNNs. However, the paper does not fully explore the problem or provide detailed analyses about the problem. The proposed method is not novel enough but shown to be effective. Overall I think the paper is borderline and I am slightly inclined towards rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper suggests that hyperbolic neural networks suffer from a vanishing gradient problem. To overcome this problem, the authors limit the radius of a ball in the hyperbolic spaces available for embeddings. This improves the performance of hyperbolic neural networks on standard image classification tasks.",
            "main_review": "Strengths:\n- Very simple idea allowing for improved performance of hyperbolic neural networks\n- Motivation for this trick is provided\n\nWeaknesses:\n- The general motivation is unclear – applying hyperbolic neural networks to the field where Euclidean geometry is suitable\n- The obtained method still performs worse compared to the Euclidean baseline\n- I have some concerns regarding few-shot classification experiments\n\nDetailed comments:\n- Concern regarding motivation: hyperbolic geometry is rarely used in practical applications as it is known to give better performance for small dimensions only. For larger dimensions, the improvements over simple Euclidean spaces or dot-product similarity usually vanish, even for hierarchical data. Thus, it is unclear why we may want to apply hyperbolic networks to non-hierarchical structures where standard embeddings are suitable.\n- The authors claim that their approach improves the performance in few-show classification over the results of Khrulkov et al. (2020). However, the numbers for the baseline do not match the results in Khrulkov et al. (2020).\n- The results on OOD are not very conclusive: while hyperbolic neural networks seem to be better more often, the winner varies for different metrics and different datasets.\n- Regarding the experiments with adversarial robustness and OOD-detection, more details are needed on adversarial attacks used, on energy score, on metrics used to evaluate OOD-detection, and so on.\n\nMinor:\n- Page 8: tables are better to be placed at the top of the page since otherwise, the text between them is harder to read. Also, captions are usually given above tables.\n- Page 9: “an given input”\n- In the title of A.4 it is better to replace “R” by “radius” (since we need “r” instead of “R”)\n\nSome questions:\n- What would happen with gradients if we use another model of hyperbolic space, e.g., the hyperboloid (Lorentz) model? This model is known to give more stable results.\n- Figure 7: Is the caption correct, and the left embedding is for dimension 2? This seems to contradict the text.\n- I wonder what the word “free” is supposed to mean in the title?",
            "summary_of_the_review": "The authors propose a simple trick that improves the performance of hyperbolic neural networks. However, I have some concerns regarding motivation and experiments. In my opinion, the paper is borderline.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper studies the \"vanishing gradient problem\" of the Hyperbolic Neural Network (HNN) used in [A] for image recognition tasks. Their analysis reveals that the HNN frameworks tends to assign vanishing gradients to points that are far from the origin in hyperbolic space. \nTo solve this issue, the authors suggest to threshold the norm of the learned hyperbolic representations (see Eq. (13)). This kind of optimization framework was already proposed in other contexts such as graph representations in [B,C]. \nA regularization framework to limit the norm of the hyperbolic representations is also proposed (see Eq. (12)).\nExperiments show that the learned HNN achieves performance similar to Euclidean Neural Networks (ENNs) in standard image classification task, and improves performance in few-shot learning and Out-of-Distribution (OOD) detection task.\n",
            "main_review": "\nThe paper is well written in general and the model is clear and easy to understand. Nonetheless, I think that the novelty is limited since a similar strategy of clipping hyperbolic embeddings and gradients was already used in [B,C] even if the authors explain they consider a different context (i.e. image recognition in the last paragraph of Section 3). \n\nIn the \"training setups\" paragraph of Section 4, the ENN is trained with SGD with momentum (that exploits the curvature of the optimized function) whereas the vanilla HNN seems to only use SGD. This comparison for vanilla HNN does not seem fair, and adaptive methods such as [D] that also exploit the curvature of the optimization function should be used as a baseline. \n\nThe motivation of using HNNs for adversarial robustness and OOD detection is not clear to me. Why are hyperbolic representations supposed to work better than Euclidean ones in those tasks? Are results of the HNN without clipping reported somewhere in the paper? What are the benefits of using clipping in those applications?\n\n\n\nminor mistakes:\n\n- In Section 3. A geodesic is not a curve of \"unit\" speed but of \"constant\" speed (i.e., the acceleration is zero, and many geodesics have a velocity of non-unit norm)\n- The domain of the inverse of the exponential map is not always the manifold M but a normal neighborhood. In the hyperbolic case, M is a normal neighborhood but this is not always the case (e.g., the sphere).\n- The Poincare ball can be derived using \"stereographic\" instead of \"stereoscopic\" projection.\n\n\nReferences:\n\n[A] Krulkhov et al., Hyperbolic image embeddings, CVPR 2020\n\n[B] Nickel and Kiela, Learning continuous hierarchies in the Lorentz model of hyperbolic geometry, ICML 2018\n\n[C] Liu et al., Hyperbolic graph neural networks, NeurIPS 2019\n\n[D] Becigneul and Ganea, Riemannian Adaptive Optimization Methods, ICLR 2019\n\n",
            "summary_of_the_review": "\n\nThe paper is interesting to read and understand the vanilla optimization framework of HNNs. However, even if the applications are different, the novelty compared to [B,C] that also use the same kind of clipping seems limited. The paper also lacks a comparison with other hyperbolic adaptive methods [D] that are not even discussed in the paper. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a regularized approach for training hyperbolic neural networks (feature clipping), which is designed to mitigate an issue commonly observed when learning hyperbolic embeddings in the Poincare ball model: The learned representations are typically close to the boundary, which can cause issues in downstream applications, such as classification. ",
            "main_review": "- The paper is well-written and addresses an important problem, which is topical in the context of the recent surge of interest in hyperbolic representation learning in the community.\n- As the authors point out, the observation that hyperbolic embeddings (learned without regularization) are close to the boundary of the Poincare ball, is not new. It has been studied in the embedding literature and regularization approaches have been proposed to mitigate this. However, to the best of my knowledge, this has not been studied in the context of hyperbolic neural networks. It may be helpful for the reader to expand the discussion of related approaches.\n- The experimental analysis is extensive and convincing. A side note on the experiments (Fig. 4, Apx. A5): For the evaluation of adversarial robustness, you construct a “Euclidean attack” by adding a Euclidean perturbation in data space (following well-established protocols by (Goodfellow et al., 2014) and (Madry et al., 2017)). When testing the robustness of a hyperbolic method, it may be better to construct a “hyperbolic attack” that respects the underlying hyperbolic geometry that your model exploits. I’m not aware of much empirical work on this, but one of the references that you cite (Weber et al., 2020) has some theoretical results on this. \n- Minor: typo in abstract exceeding -> exceeds\n",
            "summary_of_the_review": "The paper is well-written and addresses an interesting problem. While related regularization approaches have been analyzed in the hyperbolic embedding literature, the results are (to my knowledge) novel in the context of hyperbolic neural networks. The experimental analysis is well-done.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}