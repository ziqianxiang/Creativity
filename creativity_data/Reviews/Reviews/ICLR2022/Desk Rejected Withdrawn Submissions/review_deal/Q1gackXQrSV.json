{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a method to learn language grounding in 2D and 3D visual scenes. The proposed method is called BEAUTY-DETR, it iteratively attend the main stream to the language stream, the pixel stream, and the object detection proposals. An objective is also proposed to supervise the model by composing language grounding examples from object annotations. Experiments on 2D and 3D datasets show that the method achieves competitive performance on the language grounding tasks.",
            "main_review": "Strengths:\n+ The paper is clearly written and the figures demonstrates the idea well.\n\n+ The proposed method is applied to both 2D and 3D scenes.\n\n+ The proposed method is more lightweight than the current state-of-the-art methods. \n\n\nWeaknesses:\n- The results on 2D datasets are not entirely convincing. Table 2 and 3 show that M-DETR performs better on both Flickr30k and RefCOCO. The authors might argue that the proposed method has fewer parameters and converged faster by using deformable attention. However, it is possible that M-DETR could still outperform the proposed method if deformable attention is employed. Hence it is difficult to conclude that the proposed method has made a significant improvement.\n\n- It is unclear if the first contribution (the three-stream cross-attention architecture) is necessary since M-DETR achieves a better performance without it. \n\n- It would be interesting to see if other models could also benefit from the proposed data augmentation (co-training with object detection annotations). ",
            "summary_of_the_review": "Overall, I think the paper has some new ideas but the contribution made is not entirely well supported. In particular, I am not sure if the first contribution (the architecture with the object proposal stream) has made significant progress over existing methods. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work proposes a new model architecture called BEAUTY-DETR that considers both language grounding and object detection. The input to the model is either a 2D or a 3D scene plus a natural language sentence/specific object words and the goal is to localize in the scene the objects that are mentioned in the sentence. The model is the standard encoder-decoder architecture where at each encoder layer the visual and language tokens cross-attend and the visual tokens attend to the detected boxes. At the end of the encoder, visual tokens are mapped to confidence scores and top-K scoring tokens instantiate query vectors that decode relevant objects.\n\nThe experimental evaluation is done on several 2D and 3D scene data sets and the model achieves performance above several benchmarks and comparable to the recent M-DETR model.",
            "main_review": "Strengths:\n\n1. The architecture builds nicely upon the previous works and nicely details how it differs from the work closest to it. The paper is well written, is simple to understand and I am not surprised that the given model works due to the cross-attention.\n\n2. The experiments seem extensive and the results are good too. I especially appreciate the SR3D benchmark.\n\n\nQuestions:\n1. What is the size and architecture of the MLP that maps the visual, word and object tokens to the same length feature vectors? In my opinion, this dependence on the MLP is important to understand.\n\n2. Some ablation studies are shown and a few more, for example, the dependence on the feed forward layers in encoder and\ndecoder would have been nice to see.\n\n3. Some future work should be discussed. How do the authors propose to extend this architecture? This is missing.\n",
            "summary_of_the_review": "I was asked to do an emergency review for this paper so I might have missed a few key details and my review is not very detailed.\nI still think that this work has merit and shows nice results in both the 2D and 3D scenes. Although this work builds heavily on previous works, I am overall positive about the paper by whatever I was able to grasp in short amount of review time. I am decreasing my confidence score for the same reason.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes an interesting approach for modulated detection and language grounding. The main contributions are i) architectures that can attend among the language, pixel, and object proposals, and ii) treating object detection as language grounding of a large predefined set of object categories. The paper conducts experiments on both 2D and 3D experiments and achieves competitive results of language grounding and referring expression on various benchmarks.",
            "main_review": "1. The paper argues that treating object detection as language grounding is a major contribution, which is already achieved in M-DETR. It makes the contribution very unclear.\n\n2. The decoder architecture is very unintuitive. The query is initialized and then cross-attended with text embedding. Then the text query is cross-attended with object proposals, and then with the visual inputs. The fusion is completed by iteratively changing the query source. Have you considered fusion methods like VilBert? \n\n3. 2D model is pretrained on a joint dataset and fine-tuned for only 1-2 epochs on Flickr30k and RefCOCO. Does that mean the main knowledge is obtained from the pre-trained data, how does the model learn adequate knowledge from these downstream data?\n\n4. The language grounding performance is slightly below the M-DETR, the only Since the framework is very similar to M-DETR, it is hard to distinguish to what extend the claimed contribution is solid.\n\n5. Is it possible to compare the 3D language grounding by changing the backbone of M-DETR to pointnet++? It could be a fair comparison.",
            "summary_of_the_review": "The paper proposes a solid method with experiments on various benchmarks, including 2D and 3D datasets.\n\nThe main concern is the main contribution is very similar to M-DETR, and the additional contributions are more like incremental and empirical techniques based on it. Another concern is the architecture design of the decoder, which is somehow ad-hoc.\n\nPlease refer to the above for more detailed comments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes an archtitecture and optimisation objectives for language grounding in 2D and 3D scenes. It combines visual input (encoded by ResNET + 2D Fourier / PointNet++ positional encodings), detected objects, and input text (encoded using RoBERTa). Results show superior performance (Top-1 accuracy) on 3D benchmarks, and more efficient training time on 2D benchmarks.",
            "main_review": "Strengths: \n[S1]: Applicable to both, 2D and 3D visual input\n\n[S2]: Performance improvement on 3D without requiring ground truth bounding boxes. \n\n[S3]: Well-designed ablation study to justify architectural choices.\n\nWeaknesses:\n[W1]: I am not fully convinced on the solidity of the experiements and resulting claim, see comments C1-C3\n\n[W2]: The claim is that using the bottom-up attention idea, the model also learns to focus on hard to detect objects. An evaluation of this claim is missing. E.g. which instances does BEAUTY-DTER solve, that are not solvable by SoA? Even shown only qualitatively in the appendix. For the examples in the appendix one would be curious how SoA models would work - e.g., are Figure 4 examples that only BEAUTY-DTER can solve or are they easily detectable.\n\n\n[C1]: A \"lighter computational footprint\" compared to SoA is claimed in the abstract, but only shown in experiments on 2D data sets (tables 2 and 3, training GPU hours) and only in comparison to M-DETR. Since models from table 1 (3D) were retrained by the authors, the data should be available.\n\n[C2]: Table 1: The value for the ScanRefer Dataset (last column) of the InstanceRefer model reported here is 40.2. The InstanceRefer paper reports 44.23 on the test set, 40.2 is on the validation set. \n\n[C3] Results on 2D benchmark: The comparison in table 3 to M-DETR uses the values with the RO1 detection of the original paper instead of the results with the ENB detection backbone which are slightly higher, essentially not reporting the highest values the M-DETR authors reported. Is there a reason for doing so?\n\n[C4] I wonder how much performance improvement on the 3D datesets can and should be attributed to the architecture changes (cross-attention mechanisms), or are due to differences in the used language models. For example, InstanceRefer used Glove embeddings, while the proposed method, BEAUTY-DTER, uses RoBERTa for language encoding. \n\nOther comments:\n- Comparison of results would be simplified for readers if precision is kept the same (e.g. 2 digits after the comma): Tables 3 and 4 use different precision for BEAUTY-DTER and the compared methods.\n- \"BY\" missing in the type \"detection BY modulated language grounding\". Or something else? I can't fully understand the title.\n- Table 5 caption: \" performance in the validation set\" -> \"on the validation set\"",
            "summary_of_the_review": "I am not fully familiar with the related work. The method seems sound and to improve state-of-the-art. The presented results leave some open questions that lead me to i) question the improvement (attributable to language models or architecture), ii) claim (computational performance only reported on 2D, but stated generally). With this open questions I see the paper in its current form below the acceptance threshold.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}