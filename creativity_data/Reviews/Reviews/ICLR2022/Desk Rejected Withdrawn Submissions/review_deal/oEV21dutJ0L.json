{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper examines visual representations in the context of reinforcement learning and generalization to data augmentations. It proposes a method for self-supervised learning of such a visual representation. The method combines 3 forms of self-supervised learning: adversarial (relativistic GAN), inverse dynamics (predicting action), and forward dynamics (predicting next latent state). The overall representation loss is a weighted sum of the losses of these three components. Results are presented from experiments using the DeepMind control suite and CARLA. Ablations show that the inverse component is the most crucial to success, followed by adversarial.",
            "main_review": "Strengths:\n* The problem area of generalizability of visual representations for RL is an important one.\n* The experimental setup with multiple forms of augmentations is interesting – especially the generalization comparisons (i.e. Figure 6).\n* The ablations (i.e. Figure 7) could be useful for determining which aspects of representation learning are most important.\n\nWeaknesses:\n* The idea of combining 3 losses for representation learning (all of which exist individually) is not particularly interesting or novel.\n* The proposed method is somewhat complicated and seems like it might require more hyperparameter tuning.\n* The experimental results seem incorrect: for example, the original SAC paper achieves a return of ~2500 on Walker-v2 in 500,000 steps, while the experiment in this paper shows a return of 100. If I understand correctly, this paper’s experiment also ran for 500k steps (500 episodes * 1000 steps per episode).\n* The introduction and related work are very broad and should focus more on the specific setting and method.\n\nOther thoughts and suggestions:\n* I’d like more detailed descriptions of the baselines and how their representations are trained.\n* What if the baselines can train with augmented observations (e.g. grayscale, color jitter)? This might improve baseline performance.\n* Showing plots of the 3 loss components (Equation 5) throughout training could be helpful.\n* Figure 6: Plotting the delta between this and the bottom row of Figure 5 would be interesting since this is the change in performance on the same test setting.\n* Figure 7: It’s not clear to me why the Adv+Fwd would work, since the output of the inverse model is fed into the forward model, and the inverse model isn’t trained in this case. The center plot is not useful. In the right plot, I’m surprised that the contrastive representation does so poorly – maybe hyperparameter tuning is needed?\n* Figure 10 (appendix): I’m surprised that Inv alone works so well, but Inv+Fwd does not. Why would this be the case?\n* Table 1: It would be nice to have a version of this for DM control in addition to the CARLA results.\n* Showing episode videos for different methods and settings would improve semantic understanding of the results.\n* It could be interesting to look at offline RL or imitation learning settings, where there is a clear need for out of distribution generalization.\n",
            "summary_of_the_review": "To improve this line of work, I would focus less on proposing a combination of existing representation learning techniques and more on a study of these techniques and which parts are important to generalization. Providing a simple takeaway for which kind of representation we should use in RL is more meaningful than proposing a complex combination, which might require more tuning. For example, Figure 10 in the appendix suggests that the inverse component is the only one that really matters.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "JS2RL proposes a representation learning framework using three different self-supervised learning methods as the input to a reinforcement learning algorithm. They suggest that these self-supervised learning methods provide better latent representations that help improve the sample efficiency and generalization performance of existing RL algorithms. \n\nTheir main contributions are the application of 1) Adversarial representation learning 2) Cross-entered learning of forward and inverse dynamics models. They empirically show the improvement in sample efficiency and generalization to unseen backgrounds.",
            "main_review": "**Pros:**\n* The paper is well-written and the authors provide a clear explanation of their objectives.\n* They show that learning background invariant features that are directly relevant to the task control is a high priority for generalization.\n* They show better sample efficiency and generalization performance over other state of the art methods such as Bisimulation (A Zhang et al., 2020), DeepMDP (C Gelada et al., 2019) and SAC (Yarats et al., 2019)\n* They show that learning the three self-supervised learning (Adversarial, Forward dynamics, Inverse Dynamics) methods synergistically helps improve performance.\n\n**Cons:**\n\n* Based on Fig 7 (right) it seems that adversarial representation learning (ARL) with image augmentations offers the most benefit to sample efficiency.\n    * While these results are great, using the suggested augmentations (Random shift, grayscale, random convolution, cutout-color, color-jitter) have previously been documented and shown to improve sample efficiency in Reinforcement Learning with Augmented Data (Laskin et al.,)\n    * Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels (Kostrikov et al.,) also shows the regularizing effect of these augmentations and improvement in task generalization.\n    * In section 2.4, the authors suggest that using multiple augmentation techniques simultaneously can improve generalization. However, it is not clear from the ablation experiments (main paper + supplement) that this is true.\n\n* While the application of generative models for unsupervised representation learning in RL is definitely interesting, some similar ideas exist in previous work such as Learning Robust Rewards With Adversarial Inverse Reinforcement Learning (Fu et al., ), GAIL (Ho & Herman, 2016) and RL-CycleGAN (Rao et al.,). It does not seem like this paper proposes a novel approach to adversarial representation learning.\n\n* Also, The neural network used for adversarial representation learning appears to be similar to Relativistic GAN (Jolicoeur-Martineau, 2018). This also diminishes the novelty of the method.\n\n* One of the claims of the paper is that cross-centered learning of the forward and inverse dynamics models provide significant value. However, from Fig 7 (center) and Figure 16 it seems that there is little difference in the average return between the crossed and uncrossed approach. Because of this, I do not think this claim is supported by the evidence provided.\n\n**Comments:**\n\n* Please provide high quality plots for Figures 5,6 and 7. Although the curves are visible, the text is blurry when zooming in.\n* This is probably suitable as future work; potentially using a longer range or hierarchical forward/inverse dynamics model could help learn the dynamics better instead of learning low level control.\n\n",
            "summary_of_the_review": "The paper is well written and the objectives are well explained. They claim that using self-supervised learning approaches such as adversarial representation learning and cross-entered forward/inverse dynamics learning helps improve sample efficiency and generalization to new backgrounds.\n\nIt is really nice to see the benefit of image augmentations in adversarial training for reinforcement learning. While the results presented are interesting, I have some concerns that the claims cannot be verified through the empirical evidence provided. Also, having recent literature that also points towards similar ideas presented limits the novelty of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This submission proposes to train task-relevant visual representations for data-efficient reinforcement learning. It combines ideas from prior work on augmentation-invariance, and forward and inverse dynamics modeling, into a new feature learning objective function. When employed in an RL framework, the experiments show significantly improved performance on Deepmind control suite tasks and CARLA visual driving, when compared to two prior methods for visual representation learning + RL.  ",
            "main_review": "**Strengths:** \n\n- Strong empirical results.\n\n- It may be of interest to the community to note that an adversarial objective for augmentation-invariance may perform better than a contrastive objective in the RL setting. Given strong results from computer vision for contrastive approaches to visual feature learning, this is a somewhat surprising result.\n\n**Weaknesses and Clarification Questions:**\n\n- Many of the core technical ideas in this work have already been proposed and are well-known in the literature. Forward prediction objectives have been commonly used before for feature learning in RL, as noted in the related work section. It has also been combined together with inverse prediction for feature learning for control tasks, as in Agrawal et al 2016, \"Learning to Poke by Poking\": https://arxiv.org/abs/1606.07419\nFinally, augmentation invariance with multiple augmentation types has been shown repeatedly in the last year to produce good visual features (RAD, DrQ, CURL) for standard visual RL benchmarks such as those in this paper. As such, the main contribution of this paper appears to be a combination of these three ideas. Other than empirical results which show benefits from the particular combination proposed, the text does not clearly lay out the motivation for picking these particular approaches and combining them in this particular way.\n\n- Empirical baselines aren't comprehensive. Looking up the results in CURL (Srinivas et al, 2020, https://arxiv.org/pdf/2004.04136.pdf) Table 1 appears to show results for several environments at 500k steps, that are comparable to the results reported here. Given that CURL also uses augmentation-invariance (with a contrastive loss, rather than the GAN here), and that it appears to perform substantially better than the two baselines reported here, this is useful information for evaluating the proposed approach.\n\n- Experiment protocol and hyperparameter sensitivity: Given that the final objective includes several terms corresponding to the various unsupervised losses, what was the procedure for tuning the weights for the different terms in the objective? Were these weights different for each environment? How sensitive were results to this?\n\n- What is the asymptotic performance of the proposed approach compared with baselines, with more environment steps? One way to understand the success of the proposed approach is that self-supervised feature learning signals provide various inductive biases, and a combination of several of them represents a particularly strong inductive bias for the representation. If this is true, and the inductive bias is good, then we would expect big gains in early stages of learning that would then vanish over time, and might even hurt asymptotic performance.\n\n- When showing results for additional tasks in appendix, I would suggest at least summarizing the key takeaways in the main paper: are they consistent with the results in the paper? If not, how/why?",
            "summary_of_the_review": "This submission combines several prior ideas for visual feature learning in RL tasks in a somewhat ad-hoc manner, without clear motivation. In my judgment, it falls a bit short of the technical contribution expected in an ICLR paper. However, it may have strong empirical results, worthy of consideration. I await author responses to my queries to help inform my final opinion.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a framework for learning a representation for vision-based reinforcement learning, that combines a set of image-based data augmentation techniques (random shift, grayscale, etc.) and a set of self-supervised auxiliary tasks. A new self-supervised task is provided which learns an adversarial representation that is robust to data augmentation techniques. The paper shows better sample efficiency and generalization to the unseen background with the proposed task on a set of simulated locomotion tasks in mujoco and a driving task in the CARLA simulator.",
            "main_review": "### Strength\n* +++ The method of learning an adversarial representation to extract a representation that is invariant to color and texture is novel and shows significant improvement in the ablation study.\n* \\+ Extensive experiments are provided in the appendix.\n\n### Weakness\n* --- As presented in the current paper, the contribution of the whole framework is a bit unclear. First, a summary of the paper's contribution or claims at the end of the introduction would be very helpful. Second, previous works (Laskin et al. '20, Kostrikov et al. '20) have shown that using data augmentation techniques can significantly improve the sample efficiency of image-based RL. However, experiments in the main paper only compare with DeepMPC and DBC, neither of which uses data augmentation. This makes the comparisons and the achieved improvement less surprising. In a second read, I found the comparison with DrQ and CURL in figure 15 of the appendix. The experiments here show that under different backgrounds, CURL achieves very competitive performance, although much worse in terms of generalization. Comparison with DrQ and CURL should be done for the other tasks as well, and be put in the main text.\n\n### Minor\n* Legends in Fig. 7 are too small to be seen clearly.\n* The figures are not imported into latex as pdf files and thus become blurry when zoomed in.",
            "summary_of_the_review": "The paper proposes a novel method combing data augmentation and self-supervised auxiliary tasks for image-based RL and achieves better generalization and data argumentation over previous approaches. While the method of learning an adversarial representation is novel, my main concern is that the contribution of the overall approach is unclear and not well supported by the experiments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a joint self-supervised learning approach for vision-based RL. The approach mainly combines 3 self-supervised components: adversarial representation, forward dynamics and inverse dynamics. Multiple data augmentation techniques are also incorporated for learning invariant features that are directly relevant to the task control. The paper tests its data efficiency and generalization in DeepMind Control Suite and Carla and shows improvements over prior methods.",
            "main_review": "Strengths:\n1. The method combines 3 self-supervised components and conducts ablation studies to show the effectiveness of each component and design choice.\n2. The paper proposes to use a cross-enter prediction method when combining inverse and forward dynamics models.\n3. The paper is easy to follow.\n\nWeaknesses:\n1. Forward dynamics and inverse dynamics are pretty common self-supervised learning techniques for vision-based RL and a synergy of these two methods has been proposed by [1]. However, the paper does not discuss this important prior work. And there are many follow-up works on the curiosity-based method [2, 3]. The cross-enter prediction proposed by this paper is the main novelty here but only provides a limited contribution.\n2. The paper mentions that the framework can replace adversarial representation with reconstruction / contrastive methods. However, given that the proposed approach is a combination of several self-supervised learning methods, I am wondering whether the authors have tried to incorporate reconstruction and contrastive (particularly useful for vision-based RL [4]) methods to the current framework at the same time (instead of replacing adversarial representation) and see if there is any improvement.\n3. The paper conducts generalization comparisons with DBC, DeepMDP and SAC. However, many strong prior works should also be compared, such as PAD [5], SODA [6], SECANT [7], random convolution [8], etc. Also, a table with reported metrics would be important for quantitative comparison. See how [5] and [7] report the numbers/metrics in a table.\n4. Table 1 shows L2 distance in the latent space between varying observations taken from almost the same situation. A t-SNE visualization will be much better for comparing such distance. See how [7] and [8] do the t-SNE visualization.\n\n\n[1] Pathak, Deepak, et al. Curiosity-driven exploration by self-supervised prediction. ICML 2017.\n\n[2] Burda, Yuri, et al. \"Large-scale study of curiosity-driven learning.\" ICLR 2019.\n\n[3] Pathak, Deepak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement.\" ICML 2019.\n\n[4] Laskin, Michael, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations for reinforcement learning. ICML 2021.\n\n[5] Hansen, Nicklas, et al. Self-supervised policy adaptation during deployment. ICLR 2021.\n\n[6] Hansen, Nicklas, and Xiaolong Wang. Generalization in reinforcement learning by soft data augmentation. ICRA 2021.\n\n[7] Fan, Linxi, et al. SECANT: Self-Expert Cloning for Zero-Shot Generalization of Visual Policies. ICML 2021.\n\n[8] Lee, Kimin, et al. Network randomization: A simple technique for generalization in deep reinforcement learning. ICLR 2020.\n",
            "summary_of_the_review": "The paper proposes to combine several self-supervised learning methods that are found useful by prior works. However, the paper’s own contribution is rather limited. Also, several strong prior works are missing from the discussion/comparison.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}