{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a method to learn disentangled representations of images, specifically (as acknowledged only in sec2) in the class vs content disentanglement setting, not in the unsupervised setting. The method is designed to be non-generative, non-adversarial, and to exhibit similar performance (but lower runtime) to generative methods. The method is based on reusing augmentations found to generate invariances in generative contrastive methods.  \nThe paper also contributes an analysis and experiments on the impact of the shape of the contrastive learning objective, justifying the shape of the objective of the proposed method (eq 3).",
            "main_review": "The paper's topic is important and justified by the intution that contrastive learning should, in principle, be possible without generative learning. The paper accordingly brings forward a discriminative objective. The method seems innovative despite the simplicity of the adaptation. It requires the identification of useful augmentations from the analysis of a VAE such as in sec3 table 1: this represents a strong dependance on an ancillary experiment; as a consequence, I am not convinced that the set of 3 isolated augmentations is universal, and certainly porting to a new type of images (e.g. photos, images with background) or other media (audio, speech...) will require new, similar experiments.\n\nThe method is based on a few simple observations for which the intuition is given clearly and through motivating experiments, which support the selected methodology well (sec3 and 4). The method exposition is clear.\n\nThe experiments are quite complete, with both an intrinsic and extrinsic evaluation, and make sense to me. Reporting is clear, but could be more detailed (eg where does the Resnet come from?) \nThe discussion of results is not well motivated by the results tables. Some results visible in the tables are not commented on, e.g. table 3, SmallNorb Ours Factors is relatively low. Extrinsic evaluation could be stronger if it were quantitative.\n\nThe paper reads relatively clearly, with a few shortcomings.\n- From the ttitle, the abstract and the first paragraph of the introduction, the paper seems to universally address contrastive learning independently of the data type. Only accidentally does the reader learn that the paper's claim is restricted to images; I would not accept to extend the claim to other data types without corresponding experiments. I suggest rewriting the title, the abstract, the introduction to clarify this fact.\n- Table numbers are wrong\n- Competitor methods are introduced late, if at all; for instance LORD's citation is only in sec5.2.1. DrNet and ML-VAE are not referenced explicitly. In several places the paper implicitly assumes that the reader has read all the cited material, instead of properly referring to it.\n- sec3 How do you identify equivariance from the results table?\n\n# Typos, grammar and spelling\n- Hyphens should be removed from several expressions: inductive-biases in the title (!), adversarial-training, generative-approaches, parameter-sensitivity, slows-down, in-principle \n- SmallNorm for SmallNORB in several places\n- inline with -> in line\n- groundtruth -> ground truth\n- sec5.1 accordingly: do you mean respectively?",
            "summary_of_the_review": "The paper rests on a simple idea and takes it through from motivating experiments to formulation and full set of experiments. The proposed method seems good from this point of view. I'm not clear about its usefulness due to its dependence on ancillary experiments to identify \"useful\" augmentations as a prerequisite step.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors investigate data augmentation as one source of inductive bias to deep generative models. They identified specific types of data augmentation that the model representation is invariant to. These data augmentations are used to learn a supervised contrastive approach to \"disentangle\" a class-specific representation invariant to the augmentation. The authors evaluated their approach on common disentanglement datasets (Cars3D, SmallNORB) and CelebA, and showed results on the predictive performance of the representation, image translation, and cross-domain retrieval.",
            "main_review": "*Strengths*\n\n* The author addresses an interesting problem, investigating augmentation as one of the inductive biases in disentanglement-based approaches.\n\n*Weaknesses (this summarizes the overall weaknesses, please see comments below for details)*\n\n* The paper lacks clarity, and the overall writing needs to be improved: The writing is often informal (e.g., \"We receive as input\", \"it may be hoped\"), variables and notation not explained, many tables are wrongly or never referenced. The paper uses terms like invariance, alignment, disentanglement. However, their definitions are vague. They are neither clearly defined nor differentiated from each other. The structure of the paper could be revised, e.g., the introduction already describes details of experiments and the results. Many claims are made without backing it in the paper (proofs or empirical evidence) or references to past work.\n* Motivation is not clear: \n    * The paper starts with arguing that VAE-based disentanglement approaches are not robust/invariant against many data augmentation. Then they continue by showing experimental results on autoencoders (not VAEs). Based on these findings, they propose a self-supervised approach for learning domain-invariant representations. The motivation and resulting proposal do not seem to make sense to me. Disentanglement-based VAE approaches want to learn disentangled representations (each factor of variation is encoded in one latent variable dimension) with a generative model. These models may not be invariant against data augmentation because the standard benchmarks do not consider data augment during training. However, the authors fail to argue why the solution to a generative model for disentanglement is a discriminative model? What are the advantages, what are the disadvantages? What are the tasks that this model aims to solve?\n    * What about other inductive biases? The paper motivates investigating inductive biases. However, the authors only look at data augmentation. There are other inductive biases. So you either change the scope and writing w.r.t. title, abstract, intro, conclusion, or expand your work to different kinds of inductive biases.\n* Significance is not clear: It is not clear how the ABCD is different from the standard self-supervised methods like SimCLR? This also has a lot of resemblance to [1]\n* Evaluation is not sufficient: Evaluation is restricted to only disentanglement-based benchmark datasets. The work does not follow any standard metrics used for disentanglement benchmark evaluation. As this method investigates class-invariant representations, any image datasets could be used, e.g., CIFAR-10, CIFAR-100, ImageNet, etc. The evaluation metrics are not defined and explained. Further, the evaluation demonstrates that the method performs similarly to LORD and does not provide any quantitative benefit.\n\n*Detailed comments*\n\n* [Sec. 1, 2nd paragraph] \"VAE-based approaches are guaranteed to learn disentangled representations [...], they are not guaranteed to learn aligned representations.\": Are there any formal proofs for both claims, or can you reference existing proofs?\n* [Sec. 1, 2nd paragraph] \"Remarkably, in practice, generative models often learn aligned representations.\" Can you show this, or has this been shown (if so, add reference).\n* Requirement for the learned representation\n    * Can you clarify the representation you are aiming to learn? Is it domain-invariant, invariant, disentangled, class-independent? How is it different?\n    * There are many terms used in this context: \"disentangled representations\", \"domain-invariant representation learning\", \"domain alignment\".\n    * It might be clearer if you narrow \"disentanglement\", e.g., class-content disentanglement or inter-class disentanglement. Disentanglement is usually disentangling every factor of variation and may confuse the reader.\n    * [Sec. 3, 1st paragraph] Definition of disentanglement and alignment: You mention in these definitions that disentanglement and alignment is related to the class of the observations. How does the domain play into these definitions?\n    * Is domain == class?\n* [Sec. 1, 2nd paragraph] \"just the denominator of the contrastive objective\": Can you present the formula, describe the objective contrastive objective, or at least reference the objective you are referring to?\n* How is your approach, ABCD, different than SimCLR or any contrastive-based approaches?\n* [Sec. 3.1, 1st paragraph] \"[...], it may be hoped this should enable recovery of the unlabeled attributes\": How? Can you elaborate?\n* [Sec. 3.1, 2nd paragraph] \"Both VAE and GAN-based disentanglement methods\": Which ones? Add reference.\n* [Sec. 3.1, 2nd paragraph] \"[existing disentanglement methods] learn a representation [...] independent of the class\": Common disentanglement methods do not do this (e.g., beta-VAE, FactorVAE, DIP-VAE, beta-TCVAE), can you give reference to the models that do so?\n* [Sec. 3.1, 2nd paragraph] \"that we learned representations $c$ s.t. $c=u$\": c is the learned representation, u is the \"unlabelled attributes\", how can this be equal? Do you mean that these are correlated, or share mutual information, or similar?\n* [Sec. 3.2] Investigating augmentation with an autoencoder: Why did you motivate the work with VAE/GAN-based approaches and then test the hypothesis with a deterministic autoencoder? Does it make no difference? Further, I found the mention of investigating \"inductive biases of generative models\" a bit of an overclaim. You investigate data augmentation as one of many inductive biases, and this should be clarified in the paper. Also, there are many other inductive biases, and it might be good to mention and discuss them.\n* [Sec. 3.2, 2nd paragraph] \"We use the perceptual loss\": Which perceptual loss did you use?\n* [Sec. 3.2, Table 1, 2nd paragraph]\n    * I am not sure whether averaging over datasets is a good idea. How high is the variance between the results of each of the datasets?\n    * Have you run multiple experiments with one dataset?\n* [Sec. 4, 2nd paragraph] \"[...] disentanglement methods learn representations c that are disentangled from the labeled attribute $y$ s.t. $p(c|y) = p(c)$\": Is this not just statistical independence?\n* [Sec. 4, 2nd paragraph, Figure 1] \"The key is to apply the contrastive objective for the images of each class separately\": Given the figure and the description, does that mean that the paired images are always from the same class? Is this similar to Supervised Contrastive Learning [1]?\n* [Table 2]: Typo \"SmallNORM\" -> \"SmallNORB\": What's the domain and content mean accuracy? Can you define the evaluation metrics used?\n* [Equation 3] \"Where $d_i$ is the domain from\": Where is $d_i$ in the equation?\n* [Sec. 4, 6th paragraph] \"A key aspect of our approach is using transformations to which the generative models were found to be invariant\": But if only these transformations are used, does that mean that the model is not invariant to other transformations and thus is not robust?\n* [Sec. 4, 6th paragraph] \"We report the results in Tab. 4.\": I think this should be Table 2.\n* [Sec. 5.1] Missing reference to the baseline models (ML-VAE, DrNET, LORD) and the datasets (SmallNORB, Cars3D, CelebA). It is unclear why the baseline models were chosen and not [2, 3, 4]. What are the labeled and unlabelled attributes for each dataset?\n* [Table 3]: Is content disentanglement the same domain, and how is that defined? Is representation quality (average prediction accuracy) the same as factors? How are these metrics defined? What is \"Majority\"?\n* [Section 5.3.1, Image Translation]: Can you give more details about how to do image translation if the ABCD model has no generator network?\n* [Section 5.3.2, Cross-Domain Retrieval, Table 5]: Are the comparisons fair? The ABCD model uses labels, whereas, as far as I know, the other baseline models do not. \n\n*References*\n\n[1] Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C. and Krishnan, D., 2020. Supervised contrastive learning. arXiv preprint arXiv:2004.11362.\n\n[2] Hosoya, H., 2018. Group-based learning of disentangled representations with generalizability for novel contents. arXiv preprint arXiv:1809.02383.\n\n[3] Locatello, F., Poole, B., Rätsch, G., Schölkopf, B., Bachem, O. and Tschannen, M., 2020, November. Weakly-supervised disentanglement without compromises. In International Conference on Machine Learning (pp. 6348-6359). PMLR.\n\n[4] Hosoya, H., 2020. CIGMO: Learning categorical invariant deep generative models from grouped data.",
            "summary_of_the_review": "The paper proposes a supervised contrastive approach for improved classification, image translation, and cross-domain retrieval. The supervised contrastive approach is trained with data augmentations that shows invariance when tested on autoencoders. The paper builds on supervised contrastive methods, and the novelty and insights in this paper are very minor. It is not clear whether only using specific augmentations improves performance or not. The evaluation does not really support the claims of the paper. The paper lacks clarity, and the writing impedes understanding the approach. In general, investigating inductive biases of deep generative models is interesting. However, I don't think the paper explored it in a general manner. Here, I hope to provide the authors with feedback and ways to improve their paper.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper investigates the disentanglement effects achieved by the contrastive self-supervised learning approach. The authors claim that providing a set of data augmentations methods that are invariant to a typical VAE model will help to learn features that are disentangled to the class of the objects. In the experimental analysis, the authors define two metrics to measure the quality of the feature disentanglement and demonstrate their proposed method has good performance.",
            "main_review": "The paper is addressing one very important problem of contrastive self-supervised learning, it shows that under certain well-designed conditions (the inductive bias), the approach could learn disentangled feature representations (possibly due to the uniformity of the embedding space). I highly appreciate this contribution, which might be impactful to this community. \n\nHowever, there are many weaknesses in the current version of this paper, which made it not solid and satisfying to read:\n\n1. A clear definition for ``inductive-bias'' needs to be made and discussed in the related work part. The initial motivation of using the content invariant augmentation should also be clearly explained based on this.\n\n2. Some claims in the papers are not well-supported. For instance, in the introduction, the authors say ``in practice, generative models often learn aligned representations''. Claims like this are extremely confusing, the types of generative models are variant and the aligned representations are not well-explained. I have to guess a lot about what the authors want to say and have no idea if these claims are correct.\n\n3. The example in section 3.1 is unclear. What are the methods that force a standard normal distribution? IMO, the GANs do not, and the VAEs learn separated normal distributions for all the hidden dimensions. Also, what does the $(P^y)^T$ mean? I have totally no idea what this example wants to prove.\n\n4. The experiments are not convincing. There are no detailed explanations on the metrics introduced (domain accuracy and content mean accuracy), though intuitively I can guess what do they mean. However, since the performance of compared methods (LORD and DrNet) are not evaluated on these metrics, it should be clearly defined and every detail of the experiments should be clearly shown. On the other hand, there are a few more works to compare (e.g. overLORD), with different tasks (the same as other papers), the current experimental results are not strong enough.\n\n5. In the conclusion, the authors say ``we made several important modifications to the contrastive loss''. So what are they?\n\n\n\n",
            "summary_of_the_review": "Anyway, I think the topic discussed in this paper is important, I will recommend accepting this paper once the authors address the mentioned weaknesses.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}