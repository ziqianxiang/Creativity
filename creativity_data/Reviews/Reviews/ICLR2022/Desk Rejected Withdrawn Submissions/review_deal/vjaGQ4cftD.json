{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper studies a new problem named referring self-supervised learning on 3D point cloud, which aims to increase the cross-domain cross-scene and cross-task generalizability for a 3D scene understanding network. Two alignment techniques are proposed to transfer knowledge from classification labels of CAD models to segmentation labels of scanned scenes. Experimental results show promising results of the proposed method while processing unseen scenes and ablation studies also justify the efficacy of the designs.",
            "main_review": "Strengths:\n- The problem setup is interesting and valuable, bridging the gap between 3D shape classification and 3D scene segmentation.\n- The paper is clearly written and easy to follow.\n- The overall pipeline is well designed. CAD models with known labels are mixed with unknown scenes properly, resulting in data with partially known labels. After some feature extraction backbone, per point feature is projected into a convex hull, which is formed by a set of learnable prototypes. A contrastive loss is used to train the whole network so that reasonable features can be learned. The pipeline bridges the cross-domain and cross-scene gap through both data alignment and feature regularization, which makes good sense.\n- The convex-hull regularization is quite a novel technique.\n- The ablation study is quite detailed, successfully validating the efficacy of various designs.\n\nWeaknesses:\n- As one of the main claimed technical novelty, the physical data alignment is just a simple variation of those augmentation techniques that crops and pastes annotated instances in 3D scenes [1].\n- There is no theoretical justification or very clear intuition why and when such convex-hull-based feature regularization would help.\n- The experimental studies are not quite sufficient. First, the paper only compares the proposed methods with their own variations as well as fully supervised oracle cases. At least some domain adaptation methods [2] could be compared with the convex-hull-based feature regularization technique. Second, the experiments are only conducted with voxel sizes of 5cm, making the reported results much lower than those with voxels sizes of 2cm. Will the conclusion still hold for high-resolution inputs in Tables 1 and 2? Third, the evaluation metrics are not consistent in Tables 1 and 2, with one using AmAP (which is not standard for indoor scene segmentation) and one using mIoU. Why is this the case? What prevents authors from reporting the mIoU in Table 1 as well? Fourth, why only four categories are selected for evaluation? Is the method general enough to handle a variety of shape categories?\n- The paper claims that the proposed method could serve as a representation learning framework. Then how does it compare with PointContrast and Contrastive Scene Contexts? The reported experiments only contain supervised training baselines, which are not sufficient.\n- The term self-supervised learning usually refers to the case where the label comes from data itself without human annotations. In the case of this submission, the label comes from annotated CAD models so I am not sure whether ‘self-supervised' is properly used here.\n\n[1] Nekrasov A, Schult J, Litany O, et al. Mix3D: Out-of-Context Data Augmentation for 3D Scenes[J]. arXiv preprint arXiv:2110.02210, 2021.\n[2] Qin C, You H, Wang L, et al. Pointdan: A multi-scale 3d domain adaption network for point cloud representation[J]. arXiv preprint arXiv:1911.02744, 2019.\n",
            "summary_of_the_review": "The paper studies a new problem focusing on cross-domain cross-scene and cross-task generalizability of 3D scene understanding networks, which is quite valuable. The proposed solution consists of certain well-designed technical components and the overall pipeline makes good sense to me. Experiments also show promising results in certain settings and the careful ablation studies justify the efficacy of different designs. Some of the techniques are not as strong as claimed and should be tuned down a bit. The experiments also need to be more solid to fully justify the claimed contributions as I mentioned in the weaknesses part. I like the paper in general but I would like to see authors addressing my concerns above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper outlines the problem of “Referring Self-supervised Learning on 3D Point Cloud”. This means that from synthetic 3D object models from a set of object categories and unlabeled 3D scenes, a network should be trained that is able to segment the object categories in unlabeled 3D scenes. To this end a data augmentation technique and a projection to a common convex hull space are applied to the new problem. The experimental results show that the contributions improve upon a self-created baseline and that the method can be used for representation learning as a pre-text task for supervised training on 3D point clouds.\n",
            "main_review": "Strengths:\n \n1. The problem seems novel and interesting as far as I can tell. The cross-scene and cross-domain transfer learning approach seems valuable to exploit vastly available synthetic CAD models for representation learning on 3D point clouds.\n2. Figures and tables are well-executed and help the story. Also, the quality of writing is overall good.\n3. The mathematical description of the method is detailed and seems (apart from some potential minor inconsistencies) correct. Also, the method borrows different aspects such as different data augmentations and the mapping to a convex hull from already established concepts (and applies them to a new problem), whose properties are well-understood and established.\n4. The ablation study in Table 1 shows how the contributions improve upon the self-created baseline method. The value of the method for fine-tuning is clearly shown in Table 2.\n \nWeaknesses:\n1. The abstract and the introduction lack clarity w.r.t. the problem description. It took me quite a while to understand that the synthetic 3D models are models of objects from specific categories. I think this should be made clearer in the abstract and the introduction. Maybe you could also give an example.\n2. The value of the proposed method over unsupervised domain adaptation (UDA) for 3D point clouds (e.g., Qin et al., “PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation”, NIPS, 2019) is not quite apparent from the introduction and related work. I think that UDA methods could in principle be applied to the problem as point clouds are the input in both source and target domain. Even the same datasets are used in the given reference. However, the discrepancy of the point cloud of one object vs. the point cloud of a whole scene will probably be a problem when applying UDA methods.\n3. The generalizability of the proposed method is not quite clear to me. To be specific only one dataset and one network architecture are considered, making the generalizability across datasets and network architectures hard to judge from the paper.\n4. The experimental evaluation and ablation in Table 1 is not completely convincing in my opinion. On the one hand there is no comparison to other methods such as an unsupervised domain adaptation method for 3D point clouds. For example the method from Qin et al., “PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation”, NIPS, 2019 could be compared to as a simple baseline. They also use the same dataset combination in their paper. On the other hand, the hyperparameter ablation study (rows 9-14 of Table 1) considers only few hyperparameter values. The method behavior w.r.t. these hyperparameters is therefore not quite clear to me.\n \nComments and suggestions:\n1. The math seems not completely accurate yet. In the last two paragraphs of Section 3.2 it is on the one hand said that \\hat{x} is mapped into \\mathbb{W} and afterwards it is said that \\hat{x} is already an element of \\hat{W}. Only one can be true in my opinion. Also, I am not sure why the index t is omitted for x and \\hat{x} in the last paragraph of 3.2. Furthermore, h_{G_i} in Eq. 3 is not defined in the text. Maybe the authors could double check and comment on these points.\n2. Section 3.2: A reference for the convex hull theory would help the interested reader.\n3. Section 3.2: To me it is not exactly clear how the learnable prototypes p_k are obtained and updated. Maybe this could be explained in more detail (possibly in the supplementary).\n",
            "summary_of_the_review": "While I think that approaching the problem of transferring knowledge from 3D synthetic object models to 3D point cloud segmentation on unlabeled 3D scenes is novel and there is value to the presented work, I think that the experimental evaluation of the method is not completely convincing. In particular, there is no experimental comparison to methods already present in related works, only one dataset combination and network architecture are considered, and the ablation study is not entirely convincing as only few hyperparameters are investigated. Therefore, I lean towards rejecting this paper, but would be open to a rebuttal addressing the mentioned weaknesses.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies a new problem of segmenting 3D objects in a point cloud of real 3D scans, based on a collection of CAD models, which is named \"Referring Self-supervised Learning\" (RSL).\n\nThe proposed learning framework is relatively straightforward, consisting of a set of augmentation and mixing procedures, a prototype-based memory module as well as elements from contrastive learning.\n\nThe paper presents extensive evaluations and ablation studies on ModelNet to ScanNet transfer, which confirm the effectiveness of each component. It also shows pretraining with this RSL task and finetuning on labeled data achieves better results than direct supervised training, suggesting this task could lead to better representations for downstream tasks.",
            "main_review": "### Strengths\n#### S1 - Clear problem definition\nThe proposed problem is clearly defined. The paper clearly states the differences between previously studied problem and the proposed problem, where the latter focuses on cross-domain (CAD model to real scan), cross-scene (single model to indoor scene) and cross-task (classification to segmentation) (although I do not quite agree with the third claim, see W4 below).\n\n#### S2 - Well-designed solution\nThe proposed solution is well-designed and thoroughly evaluated on ScanNet. None of the components is entirely new, but they all seem to be natural choices at hindsight (but certainly not trivial). The thorough ablation studies confirm the effectiveness of each component in the framework.\n\n### Weaknesses\n#### W1 - \"Referring Self-supervised Learning\" could be misleading\n- _\"Referring Self-supervised Learning\"_ does not sound very accurate. The word _\"Self-supervised\"_ here is rather confusing to me, which typically refers to learning representations through proxy tasks without direct supervision. Does this paper mainly targets at learning representations? The proposed problem seems closer to something like _\"zero-shot generalization\"_. I understand that the authors would like to distinguish it from existing work on task-specific and/or domain-specific transfer problems, but I think the term _\"zero-shot\"_ is broadly defined and surely can refer to a different task or domain (syn vs. real). If the paper indeed claims _\"self-supervised representation learning\"_, I think there are many aspects in which the model needs to be evaluated, other than the brief comparison (Tab 2) on point cloud segmentation with and without the RSL pretraining.\n\n#### W2 - Relatively simple problem setup with big claims\n- The task here is very well defined, which is to detect objects that are similar to exemplar CAD models (ModelNet) in real scans (ScanNet). However, what are the implications of the current results on this particular task? Why are they important? Is this a good benchmark for this task? What are the connections to the bigger problems (eg., the objective of RSL stated in the intro: \"how to perform the effective and efficient cross-scene cross-domain and cross-task knowledge transferring like humans on a 3D world\". How much does this paper leap forward towards this goal?)? I think since the paper is proposing a new task that might attract more follow-up efforts, it is critical to provide more discussion on these questions.\n\n#### W3 - More discussions on failure cases\n- There are a few minor failure cases in the additional examples provided in the supplementary material, but I would be interested to see more catastrophic failure examples as well as more discussions on what can be improved, since this is a new task.\n\n#### W4 - Misleading claim on \"cross-task\"\n- The paper claims this problem involves \"cross-task\" transfer (page 2, 2nd paragraph), ie., from \"classification\" (of the CAD models) to \"segmentation\" (of indoor scene point clouds), which I do not agree with. The CAD models are already \"3D segmentation\" of the objects, despite not being in larger indoor scans. The 3D shapes provide much more information than just the \"class\" labels.\n\n### Additional comments\n- In Eq (3), is $c \\neq G_i$ missing in the second term?\n",
            "summary_of_the_review": "Overall, this is a well-written paper targeting at a specific, well-defined new problem, but I find the current results on this simple benchmark falling a bit short for the claims. Most importantly, I think more discussions should be provided on the real implications of the results, or additional evaluations on more tasks.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper focuses on the problem of learning the object-class model from the set of synthetic shapes to detect those objects in the 3D scans (the goal is to find a probability of the class label of each point of the 3D point cloud).\n\nLearning from synthetic to recognize 3D scans is a very interesting problem overpassing areas of transfer learning, detection and more. The problem is not new, but very interesting and should deserve higher interest in the field. I believe that authors missed a few existing relevant methods that focused on the same problem.\n\nThe method works in the following way: First, the test shapes are physically aligned (sampling, scaling, rotation cropping); then convex hull feature alignment is performed; last the model is improved using contrastive learning to enlarge difference between different classes and increase similarity between same classes.\n\nAuthors build on the top (and compare with) of MinkowskiNet and present the effect of each subpart of their method. However it is clear how each part of the method works, it is hard to understand how the method really works due to the missing comparison with any other standard method on any standard dataset (training/testing separation).\n",
            "main_review": "Pros:\n- Very interesting problem with a lot of applications.\n- High-level overview of the whole pipeline.\n- Each part of the method is evaluated and it is shown how much it affects the result.\n- I really like the convex method for matching, but it is hard to get the details.\n\nCons:\n\n- Some parts are hard to understand in detail (alignment, nn structure).\n- Some relevant work is missing and the problem is not new as claimed by the authors.\n- Introduction and some statements have flaws and can be misleading.\n- I think the paper should be better compared with others. Although it nicely shown how each part affects the results, it is unclear how it works overall.\n\nSummary: I believe that the problem is in the high interest of the community and it looks like working on the presented example. According to the authors, the method worked better than the original MinkUNet (it will be worthy to show how it works compare others too) and it is nicely shown how each part works, but the overall performance is unclear to me. I comment on this in bellow now:\n\nI have some unknowns about the method:\n\n- Figure 1 is a bit confusing. Unlabeled input scenes occur in that figure only once as input but it is not there anymore. I believe CAD models are aligned into the input scene. Also,I understood mixed-up is a synthetic process, but it all merges together in Fig. 1.\n- Alignment: scaling, rotation, cropping.. Do you try all possible options, or is it a variable in learning (or during the testing)?\n - It is hard for me to understand the details of the method (e.g. even nn structure) and thus it may be impossible to reimplement it, but I’m not sure if this level of detail is fine with this conference and I will follow the other reviewers.\n\nSome problems:\n\n- Where did the name for Reffering Self-Supervised Learning come from?\n- Page3: ‘’..producing the possibility of each point that belongs to a specific class..’’.  I believe it is called segmentation. \n- Although authors state that their method has a cross-task generalization ability (page 2), this ability is not discussed nor investigated further.\n- The method is evaluated on SCanNet and learned on ModelNet. Even one small experiment when standard test/training splitting is used will be beneficial.\n\n\n\nSome missing work and novelty of the problem: Authors claim that their problem is novel, from abstract: ''...we study a new problem named Referring Self-supervised Learning (RSL) on 3D scene under standing: Given the 3D synthetic models with labels and the unlabeled 3D real\nscene scans, our goal is to distinguish the identical semantic objects on an unseen scene according to the referring synthetic 3D models....''\n\nProblem of training from the 3D synthetic dataset to recognize objects in the 3D real scene is very interesting. However it is a rarely researched problem, it is not new (e.g. A3,A4, A5, A6]) as is written many times across the paper (e.g. in the abstract).\n\nThere are many works that learn the model of the object class from the shape dataset and try to detect those in the 3D scans (result is usually a point-wise labeling as in the presented work). [A1, A2] can be seen as methods that should work on the presented problem, although they originally learn on the pcl of 3D scans. [A1,A2] estimates the point-wise label in the cluttered 3D scans given the 3D point cloud objects. Although they are not learning from the synthetic dataset, they are using standard testing/training separation and it will be helpfull follow this procedure to show how the presnted mehtod works on known dataset (again the only difference is \n\n[A3-A6] learn models (in case of [A3,A4] deformable) of the object classes from the synthetic shapes dataset and detect those objects in 3D cluttered scans (from scanning or SfM [A5]), the result is point-wise labelinging of the 3D scan.  [A4] has similarities with the proposed pipeline.\n\n[A1] Zhang et all, H3DNet: 3D Object Detection Using Hybrid Geometric Primitives, ECCV 2020\n[A2] Qi et all, Deep Hough Voting for 3D Object Detection in Point Clouds, ICCV 2019\n[A3] Kim et all, Acquiring 3D Indoor Environments with Variability and Repetition, ACM ToG 2012\n[A4] Ishimtse at all, CAD-Deform: Deformable Fitting of CAD Models to 3D Scans, ECCV 2020\n[A5] Knopp et all, Scene Cut: Class-Specific Object Detection and Segmentation in 3D Scenes, 3DIMPVT 2011\n[A6] Lai and Fox, Object Recognition in 3D Point Clouds Using Web Data and Domain Adaptation, IJRR 2010\n",
            "summary_of_the_review": "\nConsidering the following pros/cons: interesting problem; each part of the method is evaluated and it is shown how much it affects the result versus: several parts are hard to understand; some relevant works are missing; I’m not sure about correctness of some statements and it is hard to see if it really works overall. Thus I go for weak rejection.\n\nAs I have less experience with ICLR than CVPR/ECCV-like conferences, I’m also willing to read other reviews and I will be happy to change my decision and opinion of this paper.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper claims to propose a new learning framework, named referring self-supervised learning (RSL) on 3d point cloud, which aims to study how to transfer 3D feature learning over point cloud cross-domain (from shapes to scenes). The proposed solution includes: 1) aligning synthetic 3D models to objects in real-scans, 2) creating scenes mixing up the unlabeled scenes and synthetic 3D models, 3) projecting point features onto a convex hull and leveraging the regularization power of the convex hull theory to learn better clustering/classification via contrastive learning. Experiments are done using ModelNet40 and ScanNet datasets, and show better results than an authors-created baseline. Several ablation studies are also presented to show the power of each proposed module.",
            "main_review": "strengths:\n- the problem of how to learn generalizable point features across-scene, -task, or -domain is an interesting problem that is worth studying.\n- the proposed convex hull regularization idea looks reasonable, novel, and valid.\n\nweakness:\n- the paper writing has big problems and many points are not made clear. I really have a difficult time understanding the problem setting and many important method details. For example, \n1) how to align ModelNet40 shapes to objects in the scene if there is no label in the ScanNet scene? The authors claimed of using \"unlabeled 3D scene scans\" in the Problem Definition paragraph. From Fig. 1, it seems that the shapes are just randomly placed and the scenes are far from having realistic layouts of furniture. The \"scaling, rotation and cropping\" session needs more clarification on how the physical alignment is performed exactly.\n2) the paragraph of \"mix up with scene scan\" does not fit the section title \"physical data alignment\", since no alignment is performed for randomly mixing up ModelNet models in ScanNet scenes?\n3) the scene layouts, for example the ones presented in Fig. 1 \"Mixed point cloud\", are very unrealistic. The random cropping strategy is also very unrealistic.\n- the three claimed contributions are quite independent and they are not related? For the physical alignment, I don't understand the details from reading the paper, as mentioned above; for the random mixing-up, there are already many works using this for data augmentation as discussed in the related work sessions; for the convex hull one, I would say there may be some contributions.\n- the baseline setup is weird. The current baseline trains over shapes and of course this will generalize to take scene point clouds as inputs. Can you compare to the baseline that train over shape-augmented scenes and use contrastive learning? \n- comparisons to other mix-up based or contrastive-learning based baselines are missing. There is no comparisons to external baselines.\n- the paper claims to address cross-scene, cross-domain, and cross-task. What are the tasks you are generalizing across?\n- why only four types of objects are involved in quantitative evaluations (as stated in Sec. 4.4, the \"self-supervised learning with referring models\" paragraph), while there are 11 categories available (as mentioned in Sec 4.1, the \"ModelNet\" paragraph)?",
            "summary_of_the_review": "The paper has major writing issues or unclear statements that prevent me from fully understanding the method, especially for the physical alignment one. For the random mixing up strategy, it's the same as previous works that the authors included in the related work session. While there are contributions for the convex hull regularization, no comparisons to external baselines are presented to support the superiority of the method over previous state-of-the-art methods.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}