{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes FAIAS, an interpretable fairness enhancing technique based on the selection of sensitive-irrelevant features. More precisely, FAIAS consists of a selector g that aims at predicting the probability for each feature to be selected and a predictor f that aims to minimize the influence of the sensitive feature (sensitivity loss). The selector and the predictor are trained in an adversarial game framework where the selector seeks to maximize the sensitivity loss while f seeks to minimize it. Experimental results on three benchmark datasets indicate that the proposed approach can achieve interesting performances in terms of both accuracy and fairness.",
            "main_review": "Overall, the paper is well written, and its contributions put it in a good position compared to related work. However, they are a few points that can be addressed to improve the quality of the paper.\n\n1- Flexibility of FAIAS: it unclear how one can use the approach to reach a particular level of fairness with the proposed framework. Is there a way to use the unfairness gap as input to Algorithm 1? This is important because usually, one would like to study the unfairness-accuracy Pareto frontier.\n\n2- Comparaison: the advantage of FAIAS would be more convincing the unfairness-accuracy trade-offs were reported (e.g., see Figure 2 of \"Learning Adversarially Fair and Transferable Representations\" [Madras et al. 2018])\n\n3- Missing SOTA: the paper misses an important in-processing approach (Fairlearn \"A Reductions Approach to Fair Classification\" [Agarwal et al. 2018]) that out-performs all the works used in the current comparison. If possible, it would be more convincing if the performances of FAIAS were compared to Fairlearn.\n\n4- Missing related point: one of the novelties of FAIAS is that it preserves the interpretability of the features. However, the paper did not the few other works in the literature that also consider such a functionality. Examples of such works include (\"FairGAN: Fairness-aware Generative Adversarial Networks\" [Xu et al. 2018]) and (\"Local Data Debiasing for Fairness Based on Generative Adversarial Training\" [Aïvodji et al. 2021]). I suggest the authors consider positioning their contributions to previous works that support features' interpretability\n\n5- Minor comment: the equations are not numbered",
            "summary_of_the_review": "The paper will benefit from providing a parameter to control the unfairness gap of the approach and comparing the performances with SOTA approaches using the accuracy-unfairness trade-offs.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This study presents a framework based on adversarial networks to address the fairness inequalities in machine learning models. The framework consists of two components, the selector and prediction functions that optimize the input data and prediction function respectively. The proposed approach provides explainable results which is key to real-life applications.\nThe proposed model makes a selection of the original data optimizing the input data in such a way as to avoid sensitive-relevant features. \nThe authors compare their framework both against the SOTA model and three baseline models on three benchmark datasets. \n",
            "main_review": "I would recommend that the authors explicitly state which categories of fairness can the propose model address and which fairness metrics it can expand to.\n\nMehrabi, N., Morstatter, F., Saxena, N., Lerman, K. and Galstyan, A., 2021. A survey on bias and fairness in machine learning. ACM Computing Surveys (CSUR), 54(6), pp.1-35.\n\nThe fonds of Figure 2 are too small; please increase the size to improve readability. \n\nFrom the presented results it is not clear that the proposed model outperforms all the others. Please elaborate the discussion of results a bit more. \n\nQualitative analysis should be performed also in the other two benchmark datasets for a better understanding of the frameworks behavior.\n\nFigure 4 should be discussed in more detail.\n\n",
            "summary_of_the_review": "The paper presents an interesting idea and is evaluated against both baseline and the SOTA models. \nThe evaluation of the results should be better discussed while it would make sense to present even briefly the qualititative analysis on the other two benchmark datasets. In general the discussion of results should be improved.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new framework (called FAIAS) for mitigating discrimination on specific groups. FAIAS aims to reduce the influence of the sensitive features (such as gender or race) in both training data and a model. The main idea of FAIAS is 1) to select features that are less correlated with the sensitive feature and 2) to train the model on top of the selected features while also making the prediction robust to the sensitive feature. To this end, FAIAS proposes an adversarial training framework that jointly learns the feature selector and the final model. Experiments are performed on three real-world datasets and show FAIAS are helpful to reduce sensitive-relevant features.\n",
            "main_review": "Advantages:\n\nThe paper proposes a fair training framework (FAIAS) that focuses on both training data and the model, where the previous studies have usually focused on either data or model. FAIAS is an adversarial training framework based on reasonable ideas for 1) selecting non-sensitive features and 2) training the fair model. Also, the experiments on the CelebA dataset show that FAIAS indeed removes sensitive-relevant features in the training data and helps the model to focus on more non-sensitive features. \n\nConcerns:\n\n<Algorithm>\n\n1) It is currently unclear why the proposed algorithm is novel in mitigating the accuracy degradation compared to the previous fair training algorithms. In the introduction section, the paper explains that one of the main goals in this research is “improving fairness while maintaining the predictive performance” because the existing algorithms suffer deterioration in the predictive performance due to the fairness constraints. However, FAIAS has (implicit) fairness constraints and aims to improve accuracy by the loss $l_{pred}$, which is also used in other fair algorithms. Thus, it is unclear why the proposed framework is more suitable to maintain accuracy than other algorithms.\n\n2) Some design choices of the proposed framework are not explained well. For example, it is unclear why FAIAS probabilistically chooses the features rather than deterministically selects the features. What is the benefit of sampling with probability than a deterministic selection using the threshold? Also, how can we assume that the current loss function is sufficient to handle each group fairness definition? More explanations on the design choices would help to justify the framework.\n\n<Experiments>\n\n1) It is curious how the stability of FAIAS can be better than Adv_Deb (Figure 2, Adult), which is an adversarial training framework for group fairness. Since both Adv_Deb and FAIAS are based on adversarial training, these are expected to have similar (in)stabilities, which is different from the current experimental results. Therefore, it would be better to explain how FAIAS handles the instability issue and how the authors have tuned Adv_Deb to make a fair comparison.\n\n2) FAIAS has to handle both training data and the model training, which leads to more re-configurations than other baselines of modifying only one of the data and the model. However, the overall performance of FAIAS seems comparable to the existing fair algorithms (Figure 2). Therefore, it would be nice to see why the proposed algorithm is attractive even though it requires many configurations and shows similar results.\n\n3) The experiments can be richer to show the benefits of the algorithm. For example, an ablation study may help to show how each part of the algorithm improves fairness. The two main networks are updated with various loss terms, but whether each term positively impacts the training is unclear.\n\nThe followings are some minor comments:\n\n1) On page 4, one of the paragraphs mentions “equation 1”, but equation 1 is not explicitly stated in the paper.\n\n2) The exact fairness metrics are better to be stated in the main body, as group fairness can be measured in several ways (e.g., difference or ratio). Also, any intuitive explanations for each measure might be helpful.\n\n3) It would be much better if the clarity of the experiment part could be improved. For example, all the contents in Figure 2 are too small to recognize easily, and the algorithms in Table 1 do not seem consistent (e.g., not clear why the top row ResNet50 is not specified as baseline).\n\n",
            "summary_of_the_review": "The above comments on the algorithm & experiments are my main concerns, but the paper shows some promising results on the CelebA dataset (FAIAS helps to remove sensitive-relevant features in the training data). Thus, overall, I think this paper is marginally below the acceptance threshold.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed an adversarial framework for training a fair classifier. The proposed framework considers both pre-processing and in-processing simultaneously. One network (authors called selector) aims at removing features that influence the prediction result. On the other hand,  the other network (called predictor) wants to achieve high accuracy, while the prediction result is irrelevant to the existence of sensitive attributes.",
            "main_review": "Strength:\n- The proposed framework considers the bias from the data perspective and the model perspective simultaneously.  \n\nWeakness:\n- I comprehend that the purpose of the sensitive loss is measuring the marginal contribution of the sensitive feature by capturing the similarity between two output softmax distributions (from inputs with and without the sensitive feature), and the goal of a predictor makes two distributions similar. I am wondering why authors employ cross-entropy instead of KL divergence because minimizing cross-entropy does not make two distributions equal in general.  \n- Two networks (selector and predictor) optimize different objectives in the proposed framework. I am wondering where the alternative gradient descent algorithm converges. More in-depth discussion for the convergence is needed.  \n\nIn experiments: \n- Unfair algorithm should be included as a baseline. \n-  I think the baselines in experiments are not state-of-the-art algorithms. The comparison with baselines (e.g., Nam et al., 2020, Cho et al., 2020) you mentioned in related works should be conducted.  \n-  Also, the comparison with a naive combination of two state-of-the-art pre-processing and in-processing methods which directly regularize each fairness notion (equalized odds, equal opportunity, disparate impact) is fair and needed. I am not sure that the proposed approach outperforms the combination of two state-of-the-art methods.\n\nMinor feedback:\n-  Adding equation numbers to each equation will improve the paper.\n\n",
            "summary_of_the_review": "I have some concerns about the proposed approach, and I think the provided experimental results are not sufficient to demonstrate that the proposed approach achieves comparable or better results than state-of-the-art methods and improves fairness as well as prediction performance.   ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}