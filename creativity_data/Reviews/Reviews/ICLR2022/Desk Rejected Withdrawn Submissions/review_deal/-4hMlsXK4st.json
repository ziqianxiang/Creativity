{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper considers the robust training of classification models. The classification model is assumed to be the composition of a feature extractor function and a simple classifier defined on the latent space where the feature lies. This work proposes to add an additional regularization term to the prevalent PGD and TRADES objectives so that when the benign data points and adversarial data points are mapped to the latent space (feature space), they are close in a distributional sense. Besides, a similar distributional similarity of the predicted label is also encoded in the proposed regularization term.",
            "main_review": "This paper is built on the observation that the representations on an intermediate layer of the clean data examples and their adversarial counterparts can become highly divergent. The regularization term GOT is proposed to ensure the proximity of the representations of the benign and adversarial data points in the intermediate layers. \n\nPros:\n\nThis paper shows very good empirical results.\n\nCons:\n\nWhile enforcing the proximity of the intermediate representations in a distributional sense is indeed one option, I do not see a very strong intuition for such a choice. This is the major concern I have on this paper and is elaborated as follows:\n1. Why not directly regularize the distance between the intermediate benign and adversarial representations of the same data point, i.e. $g(\\mathcal{A}(x))$ and $g(x)$? This seems to be a strategy that is easier to implement and more intuitive. \n2. Following point 1, I think the proposed entropy regularized optimal transport based distributional regularization can be difficult to compute. To approximate the entropy regularized optimal transport problem to a reasonable accuracy with its empirical counterpart, one would need a considerable amount of samples from the input marginal distributions, which in this case are $\\mathbb{Q}_h^d$ and $\\mathbb{Q}_h^a$. While it is easy to obtain samples from $\\mathbb{Q}_h^d$, it can be computationally expensive to obtain samples from $\\mathbb{Q}_h^a$ as evaluating the adversarial operator $\\mathcal{A}$ can be expensive.\\\nWhile the operator $\\mathcal{A}$ is also required in the PGD and TRADES objectives, using a minibatch of data points still generates unbiased gradient estimator. This is not the case for computing the OT based regularization term.\n\nA minor question is that in the discussion of the Clustering view of the WS distance, why do the authors assume that in the latent space, a single class can have multiple clusters?",
            "summary_of_the_review": "This paper constructs an optimal transport based regularization term to improve the adversarial training of classification models. However, OT is incorporated in the proposed method in an artificial way. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose an regularization method using optimal transport for the robustness of the adversarial attacks . The authors argue that the optimal transport distance forces the adversarial sample to move towards the cluster of samples which share the same label. In order to achieve this, the authors used the existing adversarial attaching methods to create the adversarial data distribution and used Wasserstein distance between the true data distribution and adversarial data distribution as a regularization term in the existing adversarial training method to align the true data distribution and adversarial data distribution. For this, authors formulated the cost function of the optimal transport as the the joint distribution optimal transport (data, labels) and minimized using entropic regularized optimal transport in the dual form. In the experiments, they show that optimal transport based regularization outperforms the existing adversarial defense methods.",
            "main_review": "Strengths:\n1) The proposed regularization method is general can be applied on the any adversarial training or defense methods, and utilize the geometry of the latent space to push the latent representation of the adversarial sample close to the class where it share the same label. The proposed method showed good performance compared to the baselines used in the paper. \n2) Theoretically showed that minimizing the cost function of optimal transport with joint feature space, label distribution boosts the robustness of the classifier.\n3) Ablation study of the different components of the proposed method\n\nWeakness:\n 1) The cost function of the optimal transport using joint data (feature), label distribution already used in the literature [1], [2]. The difference is  it is used in the domain adaptation applications where they match labeled source and unlabeled target distribution instead of matching adversarial and real data distribution. The authors seems to be lacking the awareness on the related methods in optimal transport literature, and they failed to mention in the paper.\n2) There are few works which uses optimal transport in the adversarial attacks and defense [3, 5], how does the proposed method compared to [3]. Does the proposed regularization method will have advantage on the adversarial training with [3]?\n3) Pushing the (latent representation) adversarial samples to the direction with respect to minimum optimal transport distance is not completely new, the authors can see [4], where [4] used Wasserstein distance as the regularization term to push and generate hard adversarial samples with respect to the cost function. [4] applied adversarial regularization on the noisy labels, but it can be straight forwardly applied to the adversarial robustness evaluation also. I agree that the performance depends on the underlying cost function used and the authors cost function is different from [4]. The authors missed this paper in the experimental comparison and related works and discussion.  \n4) The proposed method is a regularization term with respect to a given adversary. whether the proposed regularization term can create adversarial perturbations ? \n5) how the cost function d is computed?, Is it computed with respect to the current mini-batch while training. If yes, what is the mini-match size used in the training, whether the mini-batch size has an impact in the performance of the method. Please discuss these points   \n6) In connection with above comment, what is the computation complexity of the method when compared to the PGD-AT/TRADES?, does it scale to large scale data sets (e.g. ImageNet). \n----------\n\n[1] Nicolas Courty et.al, Joint distribution optimal transportation for domain adaptation, NeurIPS 2017.\n\n[2] BB Damodaran et.al, An Entropic Optimal Transport Loss for Learning Deep Neural Networks under\nLabel Noise in Remote Sensing Images, Computer Vision and Image Understanding 2020,  https://doi.org/10.1016/j.cviu.2019.102863\n\n[3] Kaiwen Wu et.al, Stronger and Faster Wasserstein Adversarial Attacks, ICML 2020. \n\n[4] K Fatras et.al, Wasserstein Adversarial Regularization for learning with label noise, IEEE PAMI, 2021.\n[5] Mathieu et.al, Achieving robustness in classification using optimal transport with\nhinge regularization, CVPR 2021\n",
            "summary_of_the_review": "The authors proposed optimal transport based regularization method for adversarial training to increase to robustness of the adversarial attacks.  The paper claims: pushing of the latent representation of adversarial samples in the direction with respect to the minimal optimal transport distance and the use of the cost function using joint feature and label distribution. These claims are not novel and new according to my knowledge. The idea of pushing of the latent representation of adversarial samples in the direction with respect to the minimal optimal transport distance is already explored in the literature, but not studied on evaluating adversarial robustness, but it can be straight forwardly applied. However the performance depends on the underlying cost function and the authors cost function is different from [4]. The authors cost function is similar to [1], which authors failed to discuss and mention in the paper.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper enhances the model’s adversarial robustness by investigating a specific Wasserstein distance between the adversarial and benign joint distributions on an intermedia layer of the deep learning model. They minimize the Wasserstein distance of interest to push adversarial example towards the cluster of benign examples with the same label in the latent space. This paper leverages the optimal transport theory and brings some insights into latent features based adversarial defense. Comprehensive experiments demonstrate the effectiveness of the proposed method.",
            "main_review": "Pros:\n1. Although eliminating the divergence of clean data and their adversarial counterparts in the latent space is not new, the clustering view of the Wasserstein distance of interest gives interesting analyses for the effectiveness of the proposed method.\n2. From the experimental viewpoint, the GOT based methods achieve satisfactory defensive results against PGD and AutoAttack. The experiments have evaluated the performance under multiple datasets, models, and attack settings. \n3. The paper is clear and well-written.\n\nCons:\nThe main concerns are the baselines used in experiments. Why the baselines are different on each dataset or model? MART and ARN are considered on MNIST (Table 1). However, for CIFAR-10, CIFAR-100 and SVHN, the experiments only provide the baselines of PGD-AT and TRADES (Table 2). Although the model robustness has an exciting improvement over the baselines by adding the constraints of the WS distance on the latent space, the superiority of the proposed method over other defenses remains unclear. In addition, I think it is better for the authors to provide an overview of the baselines in Table 3 and clarify the reasons for choosing them for the comparison.\n",
            "summary_of_the_review": "The method is interesting and has achieved good results in defense against AutoAttack and PGD, but the baselines are different on each dataset or model. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to use optimal transport to regularize the adversarial training. The features and logits are concatenated as a representation space, where the OT distance is computed. The proposed GOT regularizes adversarial training by minimizing the loss between clean example distribution and adversarial example distribution in the mentioned representation space. The experiment result shows that the proposed GOT is better than PGD-AT, TRADES and several other adversarial training methods.",
            "main_review": "# Strength:\n\nThe proposed idea is easy to understand. The experiment settings e.g. datasets and neural networks are standard. The comparison shows the strength of GOT over several baselines.\n\n# Weakness:\n\nMeasuring distributions with optimal transport distance has already been a popular method in machine learning community. For example, Sinkhorn Adversarial Training (SAT) is proposed to improve adversarial robustness [1]. The difference between this work and [1] is that the OT is computed on the latent feature space while this work computes the OT on (feature, logit) space. Besides, in the experiment there is no comparison about the SAT and proposed GOT. So I think the novelty of the proposed GOT is limited and the contribution of this work is unclear. \n\nThe proposed method is based on the clustering view in the latent space. So I think the experiment comparison should include supervised contrastive learning [1] or Max-Mahalanobis center (MMC) loss [2].\n\nQuestion:\nAre the experiment results of several baselines in Table 3 reported in the original papers or from the experiment done by the authors?\n\n\n\n\nDespite the performance gain of GOT over several baselines, I would say that the paper is not ready to be published if the novelty or experiment comparison issue is not addressed. \n\n[1] Bouniot, Q., Audigier, R., & Loesch, A. (2021). Optimal Transport as a Defense Against Adversarial Attacks. 2020 25th International Conference on Pattern Recognition (ICPR), 5044-5051.\n\n[2] Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C., & Krishnan, D. (2020). Supervised Contrastive Learning. ArXiv, abs/2004.11362.\n\n[3] Pang, T., Xu, K., Dong, Y., Du, C., Chen, N., & Zhu, J. (2020). Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness. ArXiv, abs/1905.10626.\n\n",
            "summary_of_the_review": "The given experiment shows the effectiveness of GOT, but the paper's weakness in the novelty should be considered in the overall evaluation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigate the use of a Wasserstein distance to reduce the distance between the distribution of adversarial examples and the distribution of natural/benign examples. The authors show that it improves robustness to different adversarial attacks (PGD and Auto-Attack). The experiments are conducted on several datasets (MNIST, SVHN, Cifar10 and Cifar100) and for different commonly used architectures (Resnet-18, WideResNet-34-10).",
            "main_review": "## Strengths\n\n- The results are good, they show improvements over previous robust defenses. There are many experiments using different datasets and different architectures, proving that the approach is consistent.\n- The idea behind the method is clearly discussed and the experiments are well detailed. \n- All the experimental settings (training hyperparameters) are detailed in the appendix to help reproducibility.\n- The authors showed results with several perturbation sizes, which is important to fairly compare methods. The observed improvement is consistent accross the different sizes considered.\n\n## Weaknesses\n\n- The paper is missing important references, discussions and comparisons to Song et al. [1] and Bouniot et al. [2]. The approach proposed here is closely related to both papers, they both reduce the distance between the distributions of adversarial and original images in the latent space. Especially [2], as they also use optimal transport to do so.  \n- The entropic regularization is not clearly introduced and is a bit light on explanations. It brings an additional hyperparameter $\\theta$, but its effect on the robustness of the method is not explained and not shown in the experiments. Also, I'm not sure to understand what corresponds to $\\phi$ in practice. While it is mentioned that $\\phi$ is a neural network, no further details are provided. Is it related to the encoder $g$ ? In general, the formulation of equation (5) is confusing, as they directly mention the dual problem.\n- I'm not sure how the Wasserstein distance is computed in practice. The Wasserstein distance is known to be hard to compute online with a lot of data and in high dimension. I guess the authors used the Sinkhorn algorithm [3] , since they refer to the entropic regularization, but it is not clear from the text and I see no mention of it.\n- Which method is used to obtain Figure 6 ? I assume it was t-SNE, but it is not mentioned. I don't find this experiment convincing as it is only a projection on a 2D space and does not prove closeness in the latent space. \n\n[1] \"Improving the generalization of adversarial training with domain adaptation\", Song et al., ICLR 2019  \n[2] \"Optimal transport as a defense against adversarial attacks\", Bouniot et al., ICPR 2020  \n[3] \"Sinkhorn distances: Lightspeed computation of optimal transport\", Cuturi, NeurIPS 2013",
            "summary_of_the_review": "The idea behind the paper is well introduced and there are many experiments showing the benefits of the method and the consistency of the results. However, the approach is really close to previous works [1,2] which are not discussed and compared to. Furthermore, some experiments could be relegated to the appendix which would give more space to better introduce the entropic regularization and explain the practical details of the computation of their Wasserstein distance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}