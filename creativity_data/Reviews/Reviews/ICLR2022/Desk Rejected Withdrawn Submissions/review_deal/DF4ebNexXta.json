{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Considering the risk of information leakage about target data in standard fine-tuning, the authors explore a new learning paradigm named Fine-Tuning from limited FeedBacks (FTFB).  To this end, they propose to fine-tune the pre-trained model on the parameter distribution with the gradient descent scheme. Further, they develop a more query-efficient algorithm that refines the model layer by layer sequentially with importance weight following the idea of Exp3 algorithm in multi-arm bandit. Compared with the naive baseline of directly deploying the pre-trained model on target data, as well as standard fine-tuning and Random Search strategy, the proposed method achieve better performance on various tasks including income classification, rate prediction, and corrupted image classification.\n\n",
            "main_review": "Strengths:\n+ A new learning paradigm named Fine-Tuning from limited FeedBacks (FTFB) to alleviate the risk of information leakage about target data in standard fine-tuning.\n+ A performance-guided parameter search (PPS) method to fine-tune the pre-trained model via estimated gradients, as well as a more query-efficient algorithm that refines the model layer by layer sequentially with importance weight following the idea of Exp3 algorithm in multi-arm bandit. \n+ Competitive performance on various tasks including income classification, rate prediction, and corrupted image classification.\n+ This paper is well-written and easy to follow.\n\nWeaknesses:\n+ My first major concern is about the setup of this paper. Commonly, when we refer to fine-tuning,  the label space of the downstream task can be different from that of the pre-trained dataset, e.g., fine-tuning an ImageNet (1000 classes) pre-trained model to a CUB-200 dataset (200 classes). However, since no target data is accessible in FTFB, we cannot learn a different classifier head or regression head for the downstream task. In other words, the label space in the downstream is required to be the same as the pre-trained one.\n+ Another major concern is the suboptimal problem when we refine the model layer by layer sequentially with importance weight. Without optimizing simultaneously the whole model, we cannot guarantee the fine-tuned one is the optimal one.\n+ In Section 3, the authors analyze the communication cost and inference computation theoretically. I wonder if it is possible to give some experimental results to verify the advantages of FTFB. Theoretical results do not always reflect the experimental performance, e.g., FLOPS sometimes can not reflect the real inference time.\n+ Some presentations are not smooth. For example, \"As we have presented that 3-SG performance values cannot provide much space for data extraction in Fig. 1(b)\". What are the meanings of *Significant Figures* and 3-SG?\n\n",
            "summary_of_the_review": "A new learning paradigm named Fine-Tuning from limited FeedBacks (FTFB) but lacks some important points on implementation details.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a fine-tuning paradigm to prevent the training service provider from extracting private data from the downstream data holder by black-box optimization. The proposed black-box optimization method only needs the evaluation result from the data holder so the data leakage risk is purported to be reduced. A general black-box optimization framework, performance-guided parameter search (PPS), is first presented and the more specific Layerwise Coordinate Parameter Search (LCPS) for neural networks is proposed. In the experiment, the paper shows the effectiveness of the proposed PPS and LCPS compared to several default baselines.",
            "main_review": "Strengths:\nThe paper discusses data leakage risk in fine-tuning, which is an important security and privacy issue given the ubiquitous use of pre-training model in various applications. The proposed black-box optimization framework presumably protects the downstream data by only allowing the service provider to access evaluation results instead of data or gradients. \n\nWeaknesses:\n1) Related Work - The main idea of optimizing the DNN based on limited feedback via an “evaluation” function is essentially the same framework as using reinforcement learning (RL) for training DNNs. RL is commonly used for training DNNs with non-differentiable losses or rewards, e.g., in image captioning [Self-critical Sequence Training for Image Captioning, CVPR2017] or network architecture search (NAS). RL is not discussed in the related works section, and thus the paper misses a lot of highly relevant works. A natural question then is why can’t RL be applied to this problem?\n\n2) Limited Novelty – The main learning framework derived in (3) is the same as the REINFORCE policy gradient algorithm in RL. In (3), the evaluation function E’ and pi(theta|w) are equivalent to the reward and policy in REINFORCE. See https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html for a summary of policy gradient methods. Also see [Self-critical Sequence Training for Image Captioning, CVPR2017] for an example of REINFORCE used for training a DNN for image captioning.\n\n3) Methodology – some claims are questionable.\n\n    a) Section 3: The assumptions about the pre-trained model and the target task should be stated more clearly. The experiments mainly target domain shift problems where the target task has the same class labels, but different feature distribution. Is the proposed method limited to this scenario, or can it also handle adapting to target tasks with different number of classes and novel classes?\n\n    b) Eq 1: The evaluation function is assumed to be the “sign” function, which is equivalent to 0-1 loss. Then the paper states: “the standard fine-tuning is adopted to optimize the problem (1) by first differentiating the sign function and then tuning the model with the end-to-end back-propagation.” – this doesn’t make sense since the derivative of the sign function is the Dirac delta function, which provides no suitable gradients for training (the gradient is either 0 or infinity).  In the experiments, what evaluation/loss function is used in the experiments?\n\n    c) Section 3: the motivation for using limited feedback is stated as: “we point out the reasons for only limited feedbacks are three folds: (1) minimize the communication cost q(|Theta|+1), (2) reduce the inference computation qm, (3) prevent unexpected data leakage.\" - the first two stated reasons failed to consider that learning from limited feedback will require more iterations (number of queries q) to converge than standard federated learning. Suppose federated learning requires n iterations to converge. If n < q/2, then federated learning has less communication cost (each FL iteration sends 2|Theta| parameters). Also, in this case, federated learning will have less computation cost (each FL iteration computes a forward and backward pass, i.e., 2m). The experiments don't show the curves for FL or fine-tuning (e.g., in Fig 3), and it would be interesting to see these curves to see whether indeed the communication/computation cost of the proposed method is less than FL/fine-tuning.\n\n4) Experiments – some interesting experiments are missing, and some details need to be added.\n\n    a) Implementation details - Is the whole dataset used for each query? Or a mini batch used? Would a mini batch be better since it provides more local gradient information? What evaluation function E is used?\n\n    b) Figure 3a, 3b, 4a: Perhaps 1000 queries is not enough to learn the more complex datasets. If more queries are used does it converge closer to OPT? \n\n    c) The experiment in this paper does not validate the claimed benefit of FTFB that it “prevents unexpected data leakage”. The evaluation metrics only include classification accuracy, fairness and fault-intolerance. To validate the argument that PPS or LCPS increases the privacy of fine-tuned models, privacy attacks such as membership inference attack (Shokri et al., 2017) or gradient inversion attack (Yin et al., 2021) should also be used and compared with the baselines. Although the proposed method does not access data, the normal fine-tuning uses the averaged gradients and the input dimension in the experiment is quite small, which may not lead to worse privacy. \n\nMinor Typos:\n- Table 1 seems to have typos in the Evaluation error cell. Should “Top-1” and “Top-5” be a baseline and LCPS?\n- P5: “quires”\n\nIn summary, the paper needs to be rewritten to put the related work in RL into the context of this work and to fix the questionable claims. Experiments need to be improved, in particular to support the main claimed benefits of the proposed method.\n",
            "summary_of_the_review": "Although the limited-feedback setting appears novel, the proposed algorithm is the same as existing policy gradient methods in reinforcement learning. Some of the claimed benefits (less computation and communication cost) are questionable, and the main benefit of preventing data leakage is not supported by proper empirical study. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies an interesting problem named \"Fine-Tuning from Limited Feedbacks\". Traditional fine-tuning only involves one party, and this paper separates data holder and model provider. Data holder owns the data, and model provider should provide a good model without accessing the data explicitly. Therefore, the authors proposed a method to tune a pre-trained model with only performance evaluation, using a policy-gradient like method on the key parameter of the pre-trained model.",
            "main_review": "Strength: The paper is well-written and the logic is easy to follow. \n\nWeaknesses:\n\n1. The novelty of the method is rather limited. To be frank, the proposed Performance-guided Parameters Search (PPS) is just a policy gradient method in reinforcement learning. Actually the problem setup looks like reinforcement learning, if the feedback from Alice is regarded as a reward function.\n\n2. This paper is a very preliminary study on the so-called \"Fine-Tuning from Limited Feedbacks\" problem. Experiments are very simple without touching the interest of deep learning practitioners. To make it more interesting, popular networks like Vision Transformer or BERT should be investigated.\n\n3. The authors fail to tell the difference between fine-tuning and domain adaptation. They are two different topics. Fine-tuning transfers a pre-trained model to a target data, and typically the labels differ with labels in the pre-trained dataset. Domain adaptation typically transfers between two tasks with the same labels. (Wang. et al 2020) is a paper for domain adaptation, not for fine-tuning. The authors only mentioned (Donahue et al., 2014) for fine-tuning, which has been obsolete. New methods on fine-tuning like L2-SP (ICML 2018) / DELTA (ICLR 2019) / Co-Tuning (NeurIPS 2020) should be compared.\n\nAs a side comment, the hyperlink in this paper is broken, so I cannot click the citation to see the referred paper. What's worse, the references are not sorted, making them difficult to find. The reading experience is unpleasant.",
            "summary_of_the_review": "Overall, I think this paper studies a new problem, but the practical significance is questionable. The method is purely borrowed from reinforcement learning without appropriate acknowledgement, and it seems difficult to scale up to complex neural networks / dataset people care about.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}