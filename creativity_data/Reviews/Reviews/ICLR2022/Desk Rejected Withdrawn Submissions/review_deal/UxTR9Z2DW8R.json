{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper received splitted scores, 3,3,5,8. While reviewers thought that using reinforcement learning for state estimation is interesting, they are not convinced if the proposed technique makes sense. The paper assumes that the high-dimensional state vectors are known, in which case one may use supervised learning directly. Also there's a debate regarding suboptimality of stationarity policies, which made the reviewers confusing. Although the authors try to argue that this is mostly a clarity issue, the reviewers were not convinced and didn't change their decisions."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a method for state estimation of high dimensional dynamics by reinforcement learning. The idea is the same as that of Morimoto and Doya (2007) and the method is specialized for the context of estimating a high dimensional state vector based on a reduced dynamics model.",
            "main_review": "Strength: \nThe mathematical derivation looks quite solid.\nWeakness: \nThe reward based on the ground truth high-dimensional state error rather than observation error would limit practical usage.\nThe use of the the ground truth high-dimensional state also as the state variable would further limit its usability. The problem with reduced dynamics is PDMDP, but treating that as a time-dependent MDP may not be the best way.\nFrom the demonstrated result, the state estimator has to be learned for a particular forcing inputs, which would also limit its usage. Generalization over different forms of initial state and inputs should be demonstrated, analyzed or discussed.\n\n",
            "summary_of_the_review": "From the design and demonstration, the applicability of the method seems to be limited to known, simulated systems.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The objective of the paper is to construct an estimator for the state of a high-dimensional nonlinear dynamical system given partial observations of the state. The objective is motivated by applications in fluid mechanics or turbulent flows where the state of the system is large obtained by discretizing a PDE. The proposed approach has two main steps: \n\n(1) construction of a reduced order model. In particular, the paper proposes the dynamic mode decomposition method which only requires a single trajectory of the nonlinear dynamics.  \n\n(2) formulating the problem of finding the estimator as a MDP problem and application of RL techniques to solve it. In particular, the estimator is modeled as a dynamical system driven by a stochastic control policy that depends on the current value of the estimate and the value of observation. The objective function is modeled with running cost equal to the error in estimating the state and a quadratic penalty on the control. Then, the optimal control policy is learned using a policy gradient method by sampling trajectories from the system.  \n\nThe proposed approach is evaluated on a benchmark example that involves Burger's equation and compared with Kalman filter applied on the reduced order system. ",
            "main_review": "The problem addressed in the paper, finding an estimator for high dimensional nonlinear systems, is interesting and important. Application of RL techniques to learn the nonlinear estimator is definitely interesting and has potential. \n\nHowever, I can not recommend acceptance based on two main objections to the basic formulation of the proposed approach. \n\n1. There is no analysis on why the proposed estimate, even if the MDP is solved accurately, is a meaningful estimate:  \n\nIt is not clear what kind of estimator the paper aims to find. If the objective is to find an estimator that is optimal in mean-squared error, like Kalman filter, then such estimator does not evolve with a control that only depends on the current value of the estimate and observation. The optimal estimator is equal to the conditional expectation of the state given history of observation. It is only true in linear Gaussian setting that the estimator has the form of update law proposed in the paper, i.e. a control that only depends on the estimate and current value of observation. In general nonlinear non-Gaussian setting, which is the main motivation of the paper, one has to evolve the posterior distribution in order to compute the optimal estimate. The update law for the estimate will involve a control that depends on the whole history of the observation, not just the current observation as proposed in the paper. In short, there is no hope that the estimate proposed in the paper be close to the optimal estimate.     \n\nNow, if the objective is not the optimal MSE estimate, it is not clear how solving the proposed MDP problem can produce a meaningful estimate. The paper does not provide any analysis on this. \n\n2. The optimal control policy to the proposed MDP problem is stationary if the control is the function of the state s = (z,\\hat{x}). However, the control is parametrized as a function of observation and \\hat{x}. Therefore, it is not clear why such a formulation lead to a stationary policy. ",
            "summary_of_the_review": "I can not recommend acceptance because there is no analysis on why the proposed estimate is meaningful and the argument on why the proposed MDP problem have a stationary policy is wrong. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new state estimation method based on reinforcement learning for high-dimensional system obtained by discretizing continuous system originally modeled by partial differential equations (PDEs). The proposed method learns to correct the model errors caused by reduced order model (ROM). In the experiment, the proposed method named RL-ROE performs better than ordinary Kalman filter applied to the ROM.",
            "main_review": "I read the paper with interest. The idea of using reinforcement learning for state estimation instead of Bayesian filtering method such as Kalman filter is very interesting, though it is not completely unique as the authors said in the section of related work.\nIn the experiment, they applied the proposed method RL-ROE to the data set generated by numerically solving the Burgers equation. \nIt outperforms Kalman filter applied to ROM in the forced case where the errors due to ROM is large, while there is not clear advantage in the unforced case where the ROM error is small. This result seems reasonable.\n\nI have mainly two questions. First, why did you use the reinforcement learning for state estimation ? In the training process, it seems to be assumed that one can access to the high-dimensional state vector $\\boldsymbol{z}$. So why don't you use supervised learning method to predict $\\boldsymbol{z}_t$ based on the history of past observation with a loss function of prediction error on $\\boldsymbol{z}$ ? At least, it is necessary to compare RL-ROE with such straightfoward approach. \n\nSecondly, I think it is unfair to compare the proposed method which is allowed to fully access to the true latent vectors $\\{ \\boldsymbol{z}_t \\}$ during the training process with Kalman filter which has no training process. In my feeling, it is not surprising that RL-ROE outperforms KF with respect to the prediction error on $\\boldsymbol{z}_t$ in  the test process, because RL-ROE can use the label information in the training process whereas KF cannot.\n",
            "summary_of_the_review": "This paper proposes a very interesting state estimation method based on reinforcement learning for high-dimensional system approximated by reduced order model. However the experiment result is not so convincing enough, and it needs comparison with other data-driven approach rather than Kalman filter.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes using RL to train a policy to output correction terms in the reduced states of reduced order models in order to perform state estimation. In particular the approach is applied to DMD and Burgers equation, but could presumably be applied to other forms of ROMs and dynamical systems. The method was shown to perform well in experiments, and was found to be more robust than baseline methods in the presence of perturbations and noise.",
            "main_review": "Overall I though this was a strong paper. I think this paper presents a reasonable application for RL in a non-traditional domain, and was able  to show the approach working effectively in experiments. Having said that, my main concern is that this paper is somewhat lacking in experiments. All in all, though, I'm generally in favor of acceptance.\n\nSome questions/clarifications/concerns:\n1. I'm interested in the interplay between the Q and R terms in Eq. 6. How important is it to have the R to penalize the magnitude of actions? When does it help and hurt? My guess is R might perform some kind of regularization that allows the model to perform better in extrapolation. Can you add a comment describing what you've found.\n2. I understand that it's difficult to run too many experiments, but I think one interesting baseline would be to compare against a neural network trained in a supervised learning setting. The network could have the same inputs and outputs as the policy in this work and the loss could just be the square error between the full state and the full state estimate that you get from adding the network output to the reduced state estimate and mapping back to the state space. The reduced state estimates could be generated by the ROM alone (without any corrections applied). This experiment could provide insight into how much benefit from this method comes from its ability to make nonlinear corrections. The supervised learning approach wouldn't be able to account for the effect of its errors compounding over multiple time steps, but should be easier to train than a policy gradient approach.\n3. The system studied here is *much* lower-dimensional than many systems studied in the fluids domain. Do we have any concerns about this approach scaling to higher-dimensional systems? Will learning a policy become more difficult as the dimensionality of the reduced states grows? Have you tried applying this approach to any other problems? etc.\n4. (Minor) Using 15 modes for the dimensionality of the ROM seems a little random. Was this chosen based on some kind of criteria with the singular values? A quick explanation for how this was chosen could be helpful.",
            "summary_of_the_review": "I think this paper presents an interesting approach and demonstrates its usefulness in experiments. While I feel that the experiments could have been more extensive, I think enough has been shown to warrant acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}