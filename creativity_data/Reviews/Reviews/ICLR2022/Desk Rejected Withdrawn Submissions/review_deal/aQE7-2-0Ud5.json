{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the problem of Semantic Segmentation. The authors proposed the feature enhancement module(FEM) to fuse the features from different stages. The proposed could enhance the feature representation by reduce the representation gap between cross-level features. The experimental results on Cityscapes, PASCAL Context, and ADE20K datasets demonstrate the effectiveness of the proposed method.",
            "main_review": "+ The proposed module is simple and easy to implement.\n+ Compared to the baselines, the proposed methods could bring constant improvements \n+ The abundant experiments on several datasets are introduced to prove the effectiveness of the proposed method.\n\nThe main concerns are listed below. \n\n- The main contribution of this paper is the proposed feature enhancement module (FEM), however, the ablation study of the proposed module is missing. In Table 1, the authors only provide the ablation study of the detail flow and the semantic flow. It is not sufficient to prove the effectiveness of the proposed module. A comparison between the proposed module and the other cross-layer feature fusion module is also needed.\n\n- The detail flow and the semantic flow are closely related to PANet[a], which also used top-down and bottom-up path augmentation to boost the feature representation. The authors need to make clear the difference and the effectiveness of the proposed module.\n\n- In the proposed module, what's the effect of the detail extractor and semantic extractor? There are no visual results and numerical results.\n\n- In the abstract, the author claim that \"the proposed FEM first aligns features from different stages through a learnable filter to extract desired information\". How to align the feature and what is the desired information？\n\n- In figure 6, which stage do the original features come from? And whether the features after executing DF and features after executing SF have the different resolutions? \n\n[a] Path Aggregation Network for Instance Segmentation. CVPR 2018.",
            "summary_of_the_review": "The paper proposed a feature enhancement module (FEM) to enhance the feature representation for segmentation. However, the proposed module is not well evaluated.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposed the feature enhancement method, which fuses multi-scale feature maps. Experiments show that the proposed method improves semantic segmentation performances for various backbones. \n",
            "main_review": "**Lack of Novelty**:\nThe proposed method is designed based on \"resizing - concatenation - convolution\" blocks, which is \"the standard method\" to fuse multi-scale feature maps. Fusing multi-scale feature maps is a widespread technique in computer vision. Many research fields such as object detection, pose estimation, and super-resolution already adopt these techniques for better performance. \n\n**Lower performance on ADE20K and Pascal Context**:\nIn results on ADE20K and Pascal Context validation set, the proposed method achieves the lower performance than SOTA methods. The low performance on these datasets, along with the lack of novelty, makes it difficult to recommend this paper to ICLR.\n\n**Bold face in Table 3**:\nThe bold face in Table 3 is misleading. For example, GFFNet and STLNet achieves the best performance on Cityscapes testing set. However, they are not denoted in bold face.",
            "summary_of_the_review": "Please refer to the main review.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work addresses semantic segmentation. It proposes two Feature Enhancement Module (FEM), detail FEM and semantic FEM, to boost segmentation performance from textural information and structural information (i.e., low-level and high-level) respectively. It achieve competitive results on Cityscapes, PASCAL Context, and ADE20K. ",
            "main_review": "--About Figure 1, do you mean that the results in last column is by the first column as input (with the same network)? If so, it is amazing that you can correct the \"pole\" by brightness enhancing. If no, Figure 1 is somewhat misleading, and even incorrect, though it is just an analogy.\n\nMethods:\n\n--The motivation is clear, but does not reveal any new interesting points. Low-level and high-level enhancement have be studied by many previous segmentation works.\n\n--Figure 2 & Figure 3, they are really some simple operations, commonly used in previous segmentation works, like HRNet [a], CCL [b], DFN [c].\n\n--The proposed approach cannot well supported what you claimed in introduction. Why the FEM-D can capture textural information? Why the FEM-S can capture structural information?\n\n--\"Gated-SCNN...The result of this method highly depends on the learned deep features\" Aren't you? Does your approach not depends on the learned deep features?\n\n--“PSPNet...Despite these methods have achieved impressive results, they need to generate immense attention maps, which are computationally expensive\". First, PSPNet does not use attention. Second, your method also use PPM of PSPNet. And it seems your approach are more computationally expensive than PSPNet. Please report your FLOPs and compared with PSPNet with the same backbone.\n\nExperiments:\n\n--\"reveal its superiority over state-of-the-art competitors\", your results are lower than state-of-the-art, where does \"superiority\" come?\n\n--Table 3, Table 4, many state-of-the-art methods are missed. For example, on ADE20K, Segmenter [d] achieve 51.82, 6% better than this work; On Cityscapes, [e] achieves 83.2%.\n\nSome minor issues:\ni.e. --> i.e.,\n\n[a] Deep High-Resolution Representation Learning for Visual Recognition\n[b] Context Contrasted Feature and Gated Multi-Scale Aggregation for Scene Segmentation\n[c] Learning a Discriminative Feature Network for Semantic Segmentation\n[d] Segmenter: Transformer for Semantic Segmentation\n[e] Exploring Cross-Image Pixel Contrast for Semantic Segmentation",
            "summary_of_the_review": "The main concern is the technical novelty and significance. The contributions are neither significant nor novel.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper aims to addresses two common problem. First is insufficient ability in handling key but low-contrast/tiny details missing; another is diverse patterns of same class misunderstanding (large intra-class variance). To this end, the authors introduce a general module namely FEM to effectively strengthen desired information in deep features. \n",
            "main_review": "+ Strengths\n1.  The proposed FEM is plug-in, and can be adapted to different backbones. \n2. Extensive experiments are conducted to verify the effectiveness of the proposed method.\n\n+ Weaknesses\n1. The Section 3.2 is hard to follow, which makes it difficult to understand the FEM directly from the text. For example, the function\nOper($F^t$, $F^a$).\n2. Limited novelty. Fig. 2 presents the detailed structures of FEM-D and FEM-S. This implementation is actually a simple combination between residual connection and common cross-level integration. \n3. The authors rephrase the common decoder design. Although the FEM is plug-in, it presents incremental novelty. Please give evidence to explain your novelty. ",
            "summary_of_the_review": "My preliminary rating is based on limited novelty. The authors can give reasonable evidences to demonstrate its uniqueness.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}