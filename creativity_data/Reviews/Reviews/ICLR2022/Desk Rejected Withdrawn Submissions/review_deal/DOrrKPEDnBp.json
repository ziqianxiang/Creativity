{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper studies data augmentation methods for contrastive learning with audio-visual data. They experimentally investigate which types of augmentation are effective when they are applied in combination to both modalities. For this, they draw on recent methods proposed in the literature for single-modality contrastive learning, like Feichtenhofer et al., 2021 and Al-Tahan & Mohsenzadeh, 2021. They also propose 4 spatio-temporal augmentations.  They also study how these augmentations can be applied to both modalities, while preserving (or not preserving) the alignment. They find that augmenting both modalities improves the quality of the feature representation (and that not performing alignment can improve results), and consider which combinations are helpful. They evaluate their method on a variety of different contrastive learning frameworks, including both MoCo and SimSiam, where they find that negative-free methods take longer to converge. \n",
            "main_review": "Positives:\n- The problem of determining which augmentations are suitable for audio-visual self-supervised learning is interesting.\n- The paper looks at a large number of augmentation strategies, and a wide range of learning frameworks.\n- The paper is generally well-written. \n- The experimental observation that alignment is not necessarily helpful is interesting. \n\nConcerns:\n- The method was evaluated only on the AVE dataset. This is a relatively small dataset that is not widely used for this task, which makes it difficult to put the quality fo the results into context. It is unclear understand how well the proposed augmentations will generalize to other datasets (eg, as the paper points out that in AVE, the audio events are mostly visible on-screen). I think that evaluating on AVE could be acceptable if some effort were taken to \"calibrate\" the results with large-scale experiments on more widely-used datasets, such as AudioSet, Kinetics, or VGGSound, but this was done. \n- The proposed approach is rather incremental. The augmentations themselves are similar to those proposed in papers cited as previous work, like (Wang and van den Oord, 2021) and (Al-Tahan & Mohsenzadeh, 2021). The proposed alignment approach is also an incremental extension.\n- The intro gives the impression that the four spatio-temporal augmentations are something that they introduce in this work (second bullet point in the list of contributions). However, they are not particularly novel, and a few seem to have been considered in other work. For instance, closely related time masking and time shifting (i.e. cropping) methods have been considered in (Wang and van den Oord, 2021), which they cite as prior work. It would be helpful if these claims were clarified.\n\nMinor typos:\n- \"in-tales\"\n- \"Because the gap...\"\n",
            "summary_of_the_review": "While this is an important topic, the method is quite incremental. The experiments are also on a relatively simple and small dataset that may not be representative of other audio-visual learning problems. Given that the questions being asked by the paper (i.e. an experimental evaluation of different augmentation strategies), I think that this is a significant shortcoming. Due to these two issues, I lean towards rejection.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors present a contrastive framework for self-supervised audiovisual representation learning. Specifically, they focus on spatio-temporal augmentations and investigate several different augmentations with corresponding analyses. Experimental evaluations on the AVE dataset show the effectiveness of the proposed augmentation strategies. The main contributions of this paper are the proposed data augmentation methods for audiovisual representation learning.",
            "main_review": "### Strengths:\n\n\\+ The proposed data augmentation methods are shown to be effective on the AVE dataset. They could be potentially useful for audiovisual representation learning research.\n\n\\+ The authors did extensive experimental evaluations of their proposed augmentation methods from different perspectives.\n\n\n### Weaknesses:\n\n\\- The contribution and novelty of this work are a bit limited, considering there are already prior works (e.g. [*3]) that work on a very similar study. Please clearly clarify the differences and the main contributions of this work, with comparison to prior works. Please also compare with [*3] if possible.\n\n\\- The authors mentioned that they choose the AVE dataset because the audios and videos are well corresponding. But why this is the reason to choose this dataset? What if the audio and video are not always corresponding?  Does that mean the proposed method can only be applied to the perfectly aligned data? Why not use other audio-video datasets?\n\n\\- Since the main contribution of this work is the data augmentations, there should be a comparison between the proposed augmentations and the commonly used ones (e.g. those used in [Qian et al. 2021] and in those audio-video representation learning methods [*1, *2, *3]). But such a comparison is missing.\n\n\\- It is unclear how would the proposed augmentations pipeline helps existing audio-visual representation learning methods. An experimental evaluation of with and without applying the proposed approach on state-of-the-art audio-video representation learning methods (e.g. [*1, *2, *3]) should have been provided.\n\n\\- It is unclear what downstream task was used to evaluate the performance. Is it action recognition? The detailed experimental setting is also missing, e.g. on which dataset(s) the methods were evaluated?\n\n\\- The authors found that \"not aligning the augmentations temporally produces better performance\" (page 7). But there is no analysis to justify the reason.\n\n\\- It is unclear what does the comparison in Table  5 reveals. What is the motivation to do this experiment and what is the derived conclusion/insight? The authors just summarize what has been shown in Table 5, without any analysis.\n\n\\- It seems that the authors only present evaluations on video representation learning and only on one downstream task. As audio and audio-related augmentation are key components in the proposed method, it would be better to evaluate on the audio-based tasks as well as more other downstream tasks to validate the effectiveness of the proposed method.\n\n\\- The writing needs to be improved.\n\n\n\n[*1] Morgado, Pedro, Ishan Misra, and Nuno Vasconcelos. \"Robust Audio-Visual Instance Discrimination.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n[*2] Morgado, Pedro, Nuno Vasconcelos, and Ishan Misra. \"Audio-visual instance discrimination with cross-modal agreement.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n[*3] Patrick, Mandela, et al. \"On Compositions of Transformations in Contrastive Self-Supervised Learning.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.",
            "summary_of_the_review": "This paper mainly focuses on data augmentation for audiovisual representation learning. Experiments show the effectiveness of the proposed method and this could be useful for related research in the community. But the contribution is not significant, the novelty is also a bit limited especially considering similar prior works (see above)  have been presented. The experiment is also insufficient to validate the claims and there are some unclear statements that need further clarification. Overall, the weaknesses overwhelm the strengths and as a result, it is not ready for publication in its current form.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigates composition of temporal augmentations suitable for learning audiovisual representations under the contrastive learning frameworks and finds that lossy spatio-temporal transformations that do not corrupt the temporal coherency of videos are the most effective. Experiments on AVE dataset provide extensive study of the effectiveness of various augmentations. ",
            "main_review": "Overall, I think the experiments are not sufficient for publication. \n1) This paper only conducts experiments on the AVE dataset, in which all the events are audible. Besides, this dataset has only 4,143 videos in total and is a bit small for a self-supervised learning method. How the augmentations work on general datasets like Kinetics-400 or Moments-in-Time is expected. \n2) Only contrastive learning methods for self-supervised image representation learning are considered. However, there are already many existing works focusing on audiovisual self-supervised learning, such as [a,b,c], that have publicly released the code. So how these methods benefit from the augmentations should be added. \n3) [c] has proposed various data augmentation approaches for audio signals and investigated their effectiveness. However, appropriate comparison cannot be found in the paper. Especially [c] also has proposed the time masking, but I have not found any citation in the paper. \n4) This paper only adopts the contrastive learning framework, so the \"self-supervised representation learning\" maybe not suitable for the title. \n5) For Table 1, it is interesting that applying the auditory augmentations before the temporal augmentations delivers better performance. However, no explanation or analysis for this phenomenon has been provided. \n\n[a] Morgado, Pedro, Nuno Vasconcelos, and Ishan Misra. \"Audio-visual instance discrimination with cross-modal agreement.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12475-12486. 2021.\n[b] Asano, Yuki M., Mandela Patrick, Christian Rupprecht, and Andrea Vedaldi. \"Labelling unlabelled videos from scratch with multi-modal self-supervision.\" NeurIPS. 2020. \n[c] Patrick, Mandela, Yuki M. Asano, Polina Kuznetsova, Ruth Fong, João F. Henriques, Geoffrey Zweig, and Andrea Vedaldi. \"On Compositions of Transformations in Contrastive Self-Supervised Learning.\" In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9577-9587. 2021.\n[d] Park, Daniel S., William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le. \"Specaugment: A simple data augmentation method for automatic speech recognition.\" arXiv preprint arXiv:1904.08779 (2019).\n\n\n ",
            "summary_of_the_review": "In my opinion, the experiments in this paper are limited to a small audiovisual dataset and a few contrastive learning frameworks for image representation learning, while some state-of-the-art video approaches with publicly available code have not been considered. So it is hard to assess the contribution of this paper. And in experiments, only phenomenons are illustrated while the reasons behind have not been discussed. The citations are not complete. So I think this paper is not ready for publication. But I would be happy to change my rating if I have misunderstood any content. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes to add four different temporal augmentations, i.e., resample, fade in/out, time masking, and time shift, to both video data and audio data in the self-supervised audio-visual representation learning framework. Extensive experiments are conducted to find out the best temporal augmentation combination. The results show that the resample data augmentation achieved the best performance, which improved the linear classifier by 6.5% on AVE dataset.",
            "main_review": "Pros: \n+ It is interesting to add data augmentations to both video and audio modalities. \n+ Four data augmentations are analyzed through extensive experiments with different self-supervised learning frameworks.\n+ Some promising results are provided to validate the effectiveness of adding more data augmentations.\n+ Figure 1 gives a clear illustration of different temporal augmentations with straightforward examples.\n\nCons:\n- The downstream task is unclear. AVE dataset is used to evaluate the learned representation, which is not a commonly used dataset. How does it evaluate the learned features, by action classification, audio classification, or audio-visual event localization? More discussion should be added to introduce this dataset. \n- Will the proposed approach also work with other datasets and other evaluation methods, such as fine-tuning and retrieval? Only linear classifier is not convincing enough. More experiments are needed to validate the proposed approach.\n- How does this proposed approach compared with other approaches? There is no such comparison. \n- Table 1 is quite confusing. First, what is the baseline of Table 1? The bold results of pitch shift are the same as the bold results in Table 3. What are these two Tables about? Second, Table 1 analyzes the effectiveness of two audio augmentations, colored noise and pitch shift. But it is not stated in the methodology section.  What does this experiment try to investigate and prove? \n- Table 2 also lacks baseline. what will happen when \\alpha=0?\n- The presentation of Table 4 also needs to be improved. What do these experiments try to investigate and prove?\n- Many experimental settings are missing and some of the details are incorrect. For example, what are the network architecture details of the video encoder? In sec. 3.2, the authors illustrate that “we utilized ResNet-18 for both streams”. Do the authors use a 2D-CNN to learn video representation? In Sec. 3.4, “uses base lr=30”? Please check the learning rate again. \n\nMinor comments: \n- Figure 3, 2st augmentation -> 2nd augmentation.\n- Sec. 3.2, 4 and 8 GPU? 4 GPU or 8 GPU?\n- Sec. 3.3.1, affect all the video -> affect each frame in a video clip.\n- Sec. 3.4, contrastive learning pre-text task? Contrastive learning and pre-text task are two different self-supervised learning approaches.\n- It is better to use a figure to illustrate the result of Table 5.\n",
            "summary_of_the_review": "Overall, I vote for rejecting. It is worthwhile to investigate the data augmentations in both audio and video modalities. However, this work lacks rigorous experimentation to validate the proposed approach. Besides, the illustration of the experimental details and discussion of the results should also be improved.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}