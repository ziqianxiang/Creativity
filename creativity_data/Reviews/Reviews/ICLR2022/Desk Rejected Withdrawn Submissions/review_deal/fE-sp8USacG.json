{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, authors propose a masked weight adversarial training method (aka DropAttack) which claims to improve the generalization ability of neural network model and overfitting. In DropAttack, the binary mask weighted by the attack scale. Experiment are conducted on IMDB, PHEME, AGNEWS, MNIST and CIFAR10 datasets for sentiment analysis, rumor detection, news classification and image classification. Theoretic analysis is included as well to support the proposed idea.",
            "main_review": "strengths:\n1. in general, paper is well constructed and easy to follow.\n\nweakness:\n1. The novelty of this work is not high. To reviewer, it is like the combination of dropconnect [1] + adversarial training. The idea is also similar with [2], which uses Gaussian noise scaled with learned parameter on both weight and input to further generalize the model. \n\n[1] Wan, Li, et al. \"Regularization of neural networks using dropconnect.\" International conference on machine learning. PMLR, 2013.\n[2] He, Zhezhi, Adnan Siraj Rakin, and Deliang Fan. \"Parametric noise injection: Trainable randomness to improve deep neural network robustness against adversarial attack.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n\n2. In terms of the experiments, the reviewer question the experiments setup. It is known that, for adversarial training, the model normally trade off its clean data accuracy for perturbed data accuracy. In other words, after adversarial training, the clean data accuracy normally is lowered. However, in table 2, it is seen that baseline accuracy is 98.95 and 84.67 for MNIST and CIFAR-10, and baseline+PGD is 99.1 and 85.57, which improves the accuracy, which means the model is heavily overfitted due to over-parameterization. For example, even the out-dated ResNet-20 can easily achieve >90% accuracy on CIFAR-10. Reviewer should choose some state-of-the-art compact models.\n\n3 Other techniques like model pruning can effectively shrink the model size (reduce the model inference latency on hardware) and avoid overfitting. However, the proposed method intentionally choose the significantly  over-parameterized model, and the proposed method still keep the same model size. The reviewer can barely foreseen the adoption of proposed method in practical scenario.\n\n4 The reviewer doubts the training overhead of given model, which is not include in the proposed method. Besides, what the scalability? for large-scale dataset, does the proposed method has the same issue of conventional adversarial training that significantly increase the training time",
            "summary_of_the_review": "The reviewer question this paper in term of novelty, experimental setup and practical usefulness",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper introduces an adversarial training method to improve the generalization of the deep neural networks. The proposed adversarial training method builds upon existing adversarial training method but differs in the way perturbations are added. First of all, perturbations during the training are added on both the inputs and the model parameters, and the perturbations are randomly masked using bernoulli distribution. Experiments are conducted on five public datasets (IMDB, PHEME, AGNEWS, MNIST, CIFAR10) using the proposed method to compare the generalization performance. \n",
            "main_review": "Strengths:\n\n- The proposed method is well explained.\n\n- The ablation studies to illustrate effectiveness of the proposed method with less training samples is interesting, and adds value to the proposed method. The analysis to connect the proposed adversarial training method to gradient regularization is also useful.\n\n- The method has been shown to improve the generalization performance on various small scale natural language processing and computer vision datasets and outperforms most of the existing counterparts of adversarial training.\n\n##########################################################################\n\n\nWeaknesses:\n\n- The novelty of the proposed method is limited since the proposed method is basically an extension of existing adversarial training methods with adversarial robustness applied to weights as well and masking some noise.\n\n- In Table 2, Baseline+DropAttack(H) should also be added to really observe whether we really need any input gradient regularization to see this improvement in generalization performance.\n\n- The results presented on the small scale datasets in the paper should be reported with multiple random seeds to showcase whether the performance is really due to the proposed mechanism or due to randomness.\n\n- A major weakness of the paper is the experimental setup used to demonstrate the effectiveness of the proposed method. Since the paper is mostly empirical, the experiments sections should be stronger to increase the confidence. At the moment all of the experiments are performed on very small scale datasets with very small networks. The proposed method should be tested and compared on Imagenet with networks such Resnet, MobileNet to showcase the effectiveness of this method in improving the generalization.\n\n- Since the method is actually a modification of adversarial training, the comparisons should be added reflecting how DropAttack fares on the problem of adversarial robustness.\n\n- It is common to observe a drop in performance of clean datasets after adversarial training [1] but most of the adversarial training methods (except Baseline+FGSM) are better than clean training in Table 2. This is a bit surprising to me.\n\n- The attack parameters used for different experiments are not mentioned clearly. These should have been mentioned in Section 4.1. It's important to mention the attack parameters for even the compared adversarial training methods in Table 2. This will definitely give more information on why the performance of clean training is inferior to adversarial training in Table 2.\n\nReferences:\n\n[1] Madry, Aleksander, et al. \"Towards deep learning models resistant to adversarial attacks.\" arXiv preprint arXiv:1706.06083 (2017)",
            "summary_of_the_review": "The proposed method is an interesting modification to existing choice of adversarial training methods and shows some generalization performance improvements but being an empirical contribution of paper, experimental section of the paper is not strong enough. Please look at the weaknesses mentioned in the Main Review.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper introduces DropAttack, a form of adversarial attack to improve generalization in neural networks. DropAttack finds computes the gradient of loss with respect to both the network parameters and the input, randomly masks them at every iteration, and simultaneously applies them during training. ",
            "main_review": "**Strength**: the idea of combining parameter and input perturbations to train a neural network models is reasonable and is well justified given the prior literature. \n\n**Weakness**: The novelty of the proposed method needs to be better clarified and discussed. Also, several important baseline methods should also be considered to help justifying the state-of-the-art claim. There are also some inconsistencies in the methods section that need to be clarified. See comments below for detailed feedback.  \n- On page1: \"characteristics of deep learning models is the ease to overfitting\" \n  It has been shown that surprisingly neural networks are quite good at evading overfitting unlike classic models (Belkin et al 2019, Nakkiran et al. 2019), even in the absence of regularization terms that are designed for this purpose. \n  Belkin, M., Hsu, DThe proposed method is reasonable and is similar to some other recently proposed approaches.  The paper is written mostly clearly but there are a few key parts that there seems to be inconsistencies in the equations. The novelty of the proposed method needs to be better explained, and a more detailed comparison with similar methods like Adversarial Weight Perturbation would be helpful. The results show consistent improvements compared to baseline methods, however, some important alternative baselines were not considered that may challenge the state-of-the-art claim on generalization performance. Also comparison of computational cost with alternative baselines would be helpful. ., Ma, S. & Mandal, S. PNAS 116, 15849–15854 (2019).\n  Nakkiran, Preetum, et al. \"Deep double descent: Where bigger models and more data hurt.\" arXiv preprint arXiv:1912.02292(2019).\n\n- page1, typo: \"have previously demonstrated\" , delete \"have\" \n\n- page3: \"will still cause high convolution and non-linear loss surface\" , it is unclear what this statement means. \n\n- page3: \"we are the first to propose an adversarial training method that simultaneously attacks the input of the model and the weight parameters\". The proposed method has some similarities with another recently proposed method \"Adversarial Weight Perturbation\" which also simultaneously perturbs the network parameters and the input. This claim needs to be updated to accurately reflect on the similarities and differences with this method. \n  \n  Wu, Dongxian, Shu-Tao Xia, and Yisen Wang. \"Adversarial weight perturbation helps robust generalization.\" arXiv preprint arXiv:2004.05884 (2020).\n\n- In Algorithm1: \"g\" is not defined. \n\n- DropAttack uses a randomly generated mask to filter through the perturbations at every iteration during training. However, it wasn't shown whether this randomness is needed during training or even if it leads to better performance compared to the case where the perturbations are applied without masking. A comparison between these two possibilities seem important for giving an intuition about necessity of randomness in the perturbations. \n\n- In Algorithm2: \n\t1. the first four lines under the most inner loop (t=1,...) can be moved outside the loop.  \n\t2. the line where g_\\theta is computed, it is unclear why the perturbation term M_\\theta . r_\\theta is added to the input. \n\t3. the way the algorithm is currently written, r_x^(K) is not used in the final computation of g_theta and g_x. there has to be an additional step for updating g_x and g_theta based on the final values.  \n\n- Page 5, line3: similar to the comment on Algorithm 2, it is unclear why the \\theta perturbation is added to the input x. \n\n- In equation 7, the \\theta perturbation seems to be added to the network parameters \\theta which is reasonable but contradicts the equations in Alg2 and on line 3 page 5. Were those equations written in error? \n\n- Given that DropAttack perturbs the parameters and the inputs, it seems necessary to provide comparisons with alternative methods in terms of computational cost. \n\n- typo on page 5: essentially-> essential\n\n- Section 4.3: it is difficult to say whether these results indeed are state-of-the-art or not. \n  There are various forms of input regularizations that were not mentioned here e.g. CutOut and AutoAugment that often leads to substantial gains in generalization performance. Also, a comparison with somewhat similar adversarial weight perturbation method will be appropriate.\n  \n  DeVries, Terrance, and Graham W. Taylor. \"Improved regularization of convolutional neural networks with cutout.\" arXiv preprint arXiv:1708.04552 (2017).\n  \n  Cubuk, Ekin D., et al. \"Autoaugment: Learning augmentation policies from data.\" arXiv preprint arXiv:1805.09501 (2018).\n\n- A more appropriate reference for adversarial training with PGD is Madry et al 2018.\n  Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. International Conference on Learning Representations, 2018.   \n- Table2: some of the differences with other baselines are very small (e.g. MNIST and CIFAR10) and it is difficult to judge whether the proposed method is indeed better or not. I suggest rerunning the experiments  multiple times to get a sense of variability in results across different runs and to better judge whether the differences are indeed significant. \n- Table4: it would be good to see the variance of results along with the average values.\n- Line before eq. 9: missing verb before \"second-order\" \n- Page 9, paragraph below eq. 13: \"Gradient penalty pushes the gradient of some parameters and inputs to approach zero,\". How does the proposed approach evades the problem of gradient obfuscation which seems to be an important issue faced by many similar approaches. \n  Athalye, Anish, Nicholas Carlini, and David Wagner. \"Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.\" International conference on machine learning. PMLR, 2018.\n- paragraph below eq. 14: \"a flatter loss landscape usually means better generalization\", it would be appropriate to discuss opposing views to this statement too Dinh et al. 2017: http://proceedings.mlr.press/v70/dinh17b.html\n",
            "summary_of_the_review": "The proposed method is reasonable and is similar to another recently proposed approach.  The paper is written mostly clearly but there are a few key parts that seem to be inconsistent. The novelty of the proposed method needs to be better explained, and a more detailed comparison with similar methods like Adversarial Weight Perturbation would be helpful. The results show consistent improvements compared to baseline methods, however, some important alternative baselines were not considered that may challenge the state-of-the-art claim on generalization performance. Also comparison of computational cost with alternative baselines would be helpful. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduce a method to stochastically and adversarially perturb both the weight and the input of a deep neural network to improve its generalization performance. Experiments are conducted on some NLP and CV datasets to show it can find the flatter minima in the loss landscape and improve the generalization performance over the vanilla training.",
            "main_review": "**Strength**\n\nIt is interesting to incorporate adversarial examples or adversarial weight in order to improve the generalization performance on the clean data. In addition, the authors have conducted ablation study to show the validity of the method over vanilla training.\n\n**Weakness**\n\n[Baselines] My main concern is the overclaim and abuse of \"state-of-the-art\" about the experiment results. As far as what I know, the baselines in this paper, both in NLP and CV datasets, are far from the state-of-the-art. For NLP tasks, the experiments should be conducted on Transformer or XLNet; for CV tasks, the experiments should be conducted on ResNet or DenseNet. The performance of baselines in this paper is much lower than the true state-of-the-art. For example, ResNet can achieve ~95% accuracy on CIFAR10 and Transformer can achieve ~97% accuracy on IMDB. The VGG and LSTM used in this paper is too weak to be convincing.\n\n[Diversity of Attacks] In addition, I think the authors should conduct ablation study on how the random masks help the generalization performance, because the authors claim that the diversity of the attack is the strength of the proposed method. Furthermore, all the perturbations are in the $l_2$ norm geometry, I suggest authors include $l_\\infty$ and $l_1$ norm attacks to further diversify the attacks.\n\n[Performance Variance] Regarding the experiment results, consider the gap between baselines and the proposed methods is not big (~1% in Table2), the authors should run each settings multiple times and report the variance as well.\n\n[Theoretical Analysis] The contribution of the theoretical analysis is very limited on top of Ref[A], which uses adversarial perturbation to find the flat minima in the loss landscape. In addition, formula (9) is the first-order Taylor expansion instead of the second order.\n\nRef[A] Yao, Z., Gholami, A., Lei, Q., Keutzer, K., & Mahoney, M. W. (2018). Hessian-based Analysis of Large Batch Training and Robustness to Adversaries. Advances in Neural Information Processing Systems, 31, 4949-4959.\n\n[Relative works] The Ref[A] above utilize a similar philosophy to improve the generalization, so it should be discussed. Ref[B] also adversarially perturbs both the weight and the input, so it should also be discussed.\n\nRef[B] Wu, D., Xia, S. T., & Wang, Y. (2020). Adversarial Weight Perturbation Helps Robust Generalization. Advances in Neural Information Processing Systems, 33.\n\n[Writing and Presentation] I regret to say this paper is not well-written and needs many edits. There are many typos and grammar mistakes, such as \"GIFAR10\" in page 6. In addition, \"K\" in the second paragraph of page 2, \"Algorithm B\" in the last paragraph of page 4 are not defined. Furthermore, there are many unfounded claims without either reference or supporting experiments in this paper, such as \"naive perturbation value may destroy the distribution of the original data\" in page 6 and the discussion about the difference in the input perturbation between CV and NLP tasks in the paragraph under Figure 2. Furthermore, many claims are not correct like \"Moreover, the existing adversarial training methods add perturbation to EVERY element in the input tensor...\", actually there are some attacks using parse perturbations, such as $l_1$  attack and SparseFool.\n\n\n",
            "summary_of_the_review": "To summarize, the baselines in the experiment part is too weak, the theoretical part has very limited novelty and the writing needs much improvement in this paper. I cannot recommend acceptance based on the current manuscript.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}