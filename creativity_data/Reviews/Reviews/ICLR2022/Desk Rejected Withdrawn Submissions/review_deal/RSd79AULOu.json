{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper applied the CVaR-based approach to design a fairness-aware federated learning algorithm. The proposed definition marginally differs from the standard CVaR-based fairness definition, and the non-smooth objective function is approximated by replacing the max function with the softmax. Some experimental results are provided to support the claim. ",
            "main_review": "Major comments\n-- Def 1: Please elaborate on the relationship between this definition and Equation (29) of [Williamson and Menon, 2019].\n\n-- \"Subgradient optimization method is supposed to solve this kind of problems. However in the federated learning setting its convergence guarantees can not easily be derived. In fact, smoothness is required condition to prove convergence in federated optimization literature.\" => I am not sure if this is true. See for instance [\"Federated Composite Optimization\", Yuan et al., ICML'21]. I believe that one may be able to use a variant of existing FL algorithms to minimize the studied non-smooth objective function.\n\n-- please provide empirical evaluations of the simple subgradient-based methods.\n\n-- Thoerem 1: Isn't the convergence of rFedFair obvious given the results of [Khaled et al. (2019)]? How is your proof different from theirs? What's the difference between the off-the-shelf guarantee and this theorem? \n\n-- Sec 6: Is the value of q optimized for q-FFL? What about subgradient methods? What is the choice of approximation parameter \\mu for the proposed method? \n\nMinor comments\np4: αi → 1, αi → pi: why limits? why not just equality?\n\np4: Add a pointer to Lemma 1's proof\n(Also, in the appendix, restate Theorem 1 of Rockafellar et al. 2000.)\n\nTypos\np2: it is not a fairness-aware objective and as pointed out by Yang et al. (2020) is less effective in ensuring better fairness under heterogeneity. => as \"it\" is?\n\np3: a underrepresent client => an \n\np4: reduce to a single point -> the set of a single point or \"singleton\"?\n\np5: eq(4), eq(5) -- why does \\hat{F} still depend on w and \\eta? Isn't it a constant?\n\np6: Algorithm 1's pseudocode seems like the standard FedAvg with the specific (approximate) objective function derived in (5). It doesn't seem necessary to repeat all these details here. \n\np6: the second half of this page seems also not much informative.\n\np7: again, (w*, \\eta*) = argmin \\hat{F} -- isn't \\hat{F} a constant?",
            "summary_of_the_review": "The theoretical contributions are not clear to me, and the empirical evaluations are not fully convincing.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, a fair federated learning algorithm is introduced to address the data heterogeneous issue across the clients. A modified conditional value at risk called heterogeneous conditional value at risk is firstly introduced to define the fairness across the heterogeneous clients. Then an optimization framework called FAFL is proposed to minimize clients' loss while taking into account fairness. Some theoretical results and convergence analysis of the proposed method have been established.",
            "main_review": "I have the following major concerns to accept this paper.\n\n1. The motivation for using HCVaR is not clear to me. Since the data distribution is heterogeneous, we could expect the models are different across the clients to a unified fair performance on all the clients. However, in the proposed method, a unified model is trained with empirical weights to achieve the different levels of fairness across the clients.  \n2. In addition, how to set proper $\\alpha_i$ to guarantee the desired fairness across the clients? What's the reason to have different $\\alpha_i$? Why will the clients accept these different settings? \n3. The ablation studies on the parameter $\\alpha_i$ are not completed systematically. It is hard to observe the trade-off between fairness and accuracy in the current experiments.\n4. The paper is not well-written. For example, in the beginning, FAFL is defined as a framework, and HCVaR is defined as a risk value but the conclusion claims that these two terms are equivalent. \n",
            "summary_of_the_review": "1. The motivation of this work should be further clarified.\n2. The experiments need to be done more systematically. \n3. The writing should be improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new federated learning objective for promoting fairness. The new objective introduces client level hyperparameters $\\alpha_i$, which in turn can recover federated averaging by setting $\\alpha_i = 1$ and agnostic federated learning (AFL) by setting $\\alpha_i = \\frac{1}{p_i}$ where $p_i$ is the client-level weight. The paper draws connections between the proposed objective and a generalized form of Conditional-Value-at-Risk (CVaR). Then, the paper proposes a federated solver for the new objective, proves its convergence at full client participation as well as partial client participation, and proves generalization bounds for the objective. The paper ends with experiments in federated benchmarks where it demonstrates the superior performance of the proposed algorithm compared to FedAvg, AFL, and q-FFL (Li et al., 2020).",
            "main_review": "This paper proposes a new federated learning objective for promoting fairness. The new objective introduces client level hyperparameters $\\alpha_i$, which in turn can recover federated averaging by setting $\\alpha_i = 1$ and agnostic federated learning (AFL) by setting $\\alpha_i = \\frac{1}{p_i}$ where $p_i$ is the client-level weight. The paper draws connections between the proposed objective and a generalized form of Conditional-Value-at-Risk (CVaR). \n\n\n- It is unclear to me what the operational meaning of the vector $\\alpha$ is, beyond being a set of hyperparameters that reweight CVaR. In particular, what is the operational meaning of the new CVaR variant, the so-called heterogeneous CVaR, beyond it being the dual of the proposed objective?\n- There are intimate connections between distributionally robust optimization, and that of CVaR (one being the dual of the other). These connections are known, and pointers to existing literature on that front are missing. For example, (Curi et al., 2020 -- Adaptive Sampling for Stochastic Risk-Averse Learning, NeurIPS 2020) and the references therein.\n\n- One of the most closely related pieces of prior work to this paper is (Laguel et al. 2020), which is on solving superquantile optimization in federated learning. This paper is only referred to very casually on page 6, and is missing from the rest of the paper. Tilted empirical risk minimization (Li et al., 2021) also establishes connections with super-quantile optimization which is not discussed here.\n\n\n\nThen, the paper proposes a federated solver for the new objective, proves its convergence at full client participation as well as partial client participation, and proves generalization bounds for the objective. However, unfortunately, the relationship between this solver and existing work has not been properly established.\n- As the paper explains, the proposed solver is different from the min-max solver of AFL. The paper claims that this solver is more efficient than that of AFL, however, this claim is neither theoretically nor empirically substantiated.\n- There are existing solvers for CVaR, again see (Curi et al., 2020 -- Adaptive Sampling for Stochastic Risk-Averse Learning, NeurIPS 2020) and the references therein. It is not clear to me that the proposed solver in this paper is better in any way (theoretically or empirically).\n- The convergence results are only derived for convex losses. This limitation needs to be clearly discussed. What is the difficulty of extending the results beyond convex losses?\n- Can you please add a discussion on comparison between the convergence results with those of FedAvg and AFL? \n- It is not clear to me that the bound in Theorem 3 is better than the bound by (Mohri et al., 2019) in any way, especially given the multiplicative factor $\\tau_\\alpha$ in (6). Can you please compare Theorem 3 with that of AFL in a concrete setting?\n- Please make the discussion at the end of Section 5.1.1 explicit, as one needs to have a recipe for choosing the hyperparameter $\\mu$, which is not provided here.\n\nThe paper ends with experiments in federated benchmarks where it demonstrates the superior performance of the proposed algorithm compared to FedAvg, AFL, and q-FFL (Li et al., 2020).\n- First, the baselines (Li et al., 2021) and (Laguel et al. 2020) are missing.\n- *Most crucially about this paper*, given that the proposed algorithm has $n$ hyperparameters, where $n$ is the total number of devices, it is crucial to devise a hyperparameter tuning strategy which is completely missing from this paper. This is in contrast to AFL (with no hyperparameters) and q-FFL (with 1 hyperparameter).\n- The performance of the proposed strategy vastly varies with hyperparameters as can be seen from Table 3, which makes it even more crucial to devise a systematic strategy for tuning the vector $\\alpha$.\n- How did you choose the hyperparameter $q$ in q-FFL?\n- The paper mentions the following statement: “Our framework is flexible in that it allows each client to select different $\\alpha_i$  to trade-off between average accuracy and fairness.” This is misleading as if a client optimizes their $\\alpha_i$ for their own best average accuracy, that will probably cost other clients their performance which is the opposite of fairness. Please either justify or remove this claim.\n\nOther comments\n- Please clarify what this means on page 2: “ Instead of optimizing the model for a specific (uniform) distribution.\n- New notation is introduced in different places, e.g., end of page 3 or beginning of Sec 5.1. Please move all to the notation subsection.\n-  The objective at the end of page (4) is distributionally robust optimization (DRO). Please make the connection explicit.\n- Please change deviance (pg.5 first line) to deviation.\n- Please consider changing the name of the algo, rFedFair, to something that is more closely related to the objective. Too many names are floating around.\n\n====== update ====\n\nUnfortunately, the authors have not revised the paper. While the authors provide partial answers to some of my questions (many remain unaddressed), as I mentioned before, there are major revisions needed before this paper could be published. First and foremost, the paper needs more comprehensive comparison with existing CVaR solvers; and also it should compare with  (Li et al., 2021) and (Laguel et al. 2020). In its current form, it is unclear what the advantage of the proposed solver is compared to existing art. Hence, I remain my reject vote.",
            "summary_of_the_review": "In summary, this paper provides a new objective that can be viewed as an extension of FedAvg and AFL (and recovers them both at extremes). The paper develops an algorithmic framework for solving the objective, provides guarantees about the solver, and shows its good performance in federated benchmarks. However, the paper suffers from major shortcomings. Most crucially, the proposed framework comes with $n$ (number of clients) hyperparameters and it is not clear how one should choose these hyperparameters in practice. Some pointers to existing literature about the objective and the solver are missing, and it is unclear whether the proposed solver has advantages over existing solvers. Some of the most closely related literature are missing from comparisons, namely (Laguel et al. 2020) and (Li et al., 2021), which have a far smaller number of hyperparameters. There are major revisions needed to address some of these explicit issues I have raised, hence my reject score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work proposes a variant of Conditional Value-at-Risk (CVaR) in the context of federated learning by considering different weights of each device loss. It provides convergence guarantees for the solver, and empirically demonstrates the effectiveness of the objective on standard benchmarks. ",
            "main_review": "* Title is too general: any work in the area of fair FL can be called fairness-aware federated learning\n\n* The submission describes the flexible tradeoff between accuracy and fairness as one of the major contributions of HCVaR throughout (e.g., after Definition 1, in the experiments, etc), which is misleading. The flexible tradeoff is brought about by CVaR, and HCVaR inherits such properties because it is a direct extension of CVaR. In other words, this benefit is not due to the fact that HCVaR allows for different alpha_i's, which is the only difference between CVaR and HCVaR. For instance, the example given under Eq (1) where FAFL reduces to previous objectives is a property of distributionally robust optimization (or its dual, classic CVaR), not a new property of HCVaR. \n\n* HCVaR changes alpha in CVaR to alpha_i's. I understand it is scaling the probability of each device's loss by some constant, but it is hard to reason about the effective behavior of HCVaR_{1-alpha}. Compared with CVaR, which only has alpha as an interpretable hyperparameter, alpha_i's introduce one hyperparameter per device.\n\n* The motivation/benefits of using the smooth approximation are not clear. Standard solvers for CVaR (e.g., those mentioned in https://arxiv.org/pdf/2010.05893.pdf) are readily applicable to solve HCVaR. For general convex functions, this approximation does not seem to improve convergence rates (1/\\eps^2) theoretically. Empirical benefits of solving this variant (if there are any) are also missing. In addition, the convergence error (related to mu) might be large---there is a 1/mu in L and mu in the last term in Theorem 1. \n\n* By assuming convexity of f_i's and therefore convexity of HCVaR with respect to (w, eta), there are no challenges in the convergence analysis (i.e., can readily use previous federated optimization analysis). In addition, the algorithms and analysis do not support stochastic cases, which is important for FL.\n\n* The dual representation, the approximation, and the bounds on the approximation (lemma 2) are standard and well-studied in previous literature. Using alpha_i's everywhere does not add additional challenges, and the empirical benefits are not very convincing (see below).\n\n* Experiments should also compare with the objective of directly reweighting losses and CVaR, which are two components of HCVaR.\n\nOthers:\n\n* Mischaracterization of several works:\n\n-- 'q-FFL is not a fairness-aware objective' is not justified. The reference Yang et al. (2020) mentioned in the submission studies how system heterogeneity (defined and simulated in a slightly more fine-grained way) impacts various FL algorithms, and I do not think its takeaway is 'q-FFL is less effective in ensuring better fairness under heterogeneity'.\n\n-- For TERM, sampling from a Gumbel-Softmax distribution is not expensive, and they propose efficient stochastic solvers for the objective.\n\n-- 'Li et al. (2021b) do not have fairness guarantees for the fairness benefits except on a linear problem' is correct, but the submission does not have any fairness guarantees on any problem instances.\n",
            "summary_of_the_review": "It looks like the proposed formulation does not offer new methodology, techniques, or empirical insights to the studied problem. My main concerns are listed in the comments.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}