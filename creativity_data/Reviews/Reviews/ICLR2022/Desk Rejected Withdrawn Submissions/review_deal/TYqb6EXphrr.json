{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a space time recurrent networks with explicit memory that stores useful information from the past. Specifically, when processing a new frame t, the model will select one memory slot to vacate, distill its information into another selected slot, and then store the feature of the new frame t to this vacated slot. The authors have ablated several designs for the mechanism for selecting the slots for removing and distillation. In terms of experiments, the authors applied their method on video object segmentation on DAVIS 2017 and video prediction on Humans3.6M and KTH. On both tasks, the method compares favorably to several previous methods. \n",
            "main_review": "Strengths:\n+ This paper aims to tackle the important problem of modeling long-range space-time memory of video understanding.  \n+ The authors have conducted experiments on two different tasks to validate their approach. \n+ I like the writing of the Intro section. \n\nWeaknesses at concept level:\n- First, as also discussed in the Related Work section, the idea of using space time memory is not new, there are many work on this both using implicit and explicit memory mechanisms. Like many previous work, the main memory update equations in this paper (Eq. 3-9) also resembles previous gating-based update rules, the only conceptual difference to me is the use of a attention module (FAM in Eq 4 and 7) which is not ablated in experiments. With this, I don't find this paper contribute much new insights on building space time memory models. \n- As the authors mentioned on Page 2 (\"we believe that this kind of memory structure has more potential to scale up to long videos beyond a few seconds\"), the goal of the paper is to build a model that can scale to long videos, but the task it embarks on like DAVIS 2017 are videos with only a few seconds long, without empirical validation, I don't see how is this goal achieved with the proposed method. \n\nWeaknesses regarding the technical approach:\n- One big issue I have with the memory selection rule A-D introduced in Section 3.2 is that, none of these four designs use the input feature X_t when selecting the slot to remove and distill. Wouldn't this be an obvious source of information for the model to decide where to discard (e.g. it can discard slots that are already close to new frame X_t to minimize loss of useful information)? To me, this at least should be tried and compared to those 4 rules proposed by authors. \n- Specifically, for rule A and B, the authors wrote \"use it to update other templates\" and \"to update the remaining templates\", how exactly is this done? From all following equations (Eq. 3-9), I can only tell how to distill one slot into another one, not sure how to distill to multiple slots/templates in the memory. \n- Overall, the design of Eq. 3-8 seems arbitrary and there is no good ablation on why it is designed in this way. For example, one of the most important ablation would be removing the FAM module (and replace with conv stacks) and see how does that change the performance. \n- Another important ablation is to validate the claim that \"it might be nice to keep it before blending it with the long term information\", I see the authors conducted an experiments in Section 4.1.3, but the description of this experiment is not clear to me -- how exactly does the author update the input to memory? This needs to be elaborated. \n\nWeaknesses regarding the results:\n- It would be good to briefly introduce what does J, F and Avg refer to as metrics in Table 1 and 2. \n- In Table 1, it would be good to have the exactly number of size of memories used by different approaches, for example, comparing GC and STReMN, what's the absolute size of memory slots used for each? Without this, it's hard to draw conclusions saying the proposed memory mechanism is better (it could just be that it uses a big memory slots). \n- Also, it would be nice to have actual runtime number for different methods in Table 1 and 2, this can show the speed benefits brought by keeping a constant memory. \n- Page 7, \"Post-processing was also employed to remove some false positive predictions\", can you elaborate on what post-processing is used? \n- Section 4.1.2 second paragraph, there is no analysis on results from Table 1? What can we learn from this (other than numbers are higher)? Also, though I understand why the authors do not compare to many methods that does fine-tuning on the test set, it would be good to show results of a few representative work along this line in Table 1, to give better contexts to the audience.   \n- Section 4.1.2 third paragraph, I don't buy the argument about the comparison to KMN and CFBI. To have a fair comparison to KMN, the authors could apply the same data augmentation as KMN, or is there any obstacle to do that? For CFBI, the authors could simply train a model with R101 backbone for comparison. \n- For video prediction experiments, it would be good to have some qualitative results to visualize the predicted frames. \n\nWritings/Figures:\n- The writing of the approach section is not clear. As an example, in Section 3.2, before introducing different memory slot selection rules (A-D), there is no introduction of what does X^M_i and X^M_j refer to, this makes it hard to read.  \n- Figure 2 is too small to read. \n",
            "summary_of_the_review": "Although tackling an important problem, with all the weaknesses I listed above, I don't think this paper is ready for this venue. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethical issues spotted in the paper. \n",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper introduces a memory augmented Transformer architecture for spatio-temporal tasks. The core idea of the proposed method is to make use of constant-sized memory for handling long video sequences. Empirical evaluation on video object segmentation and video prediction benchmarks against other baselines suggested the advantage of the proposed method.  ",
            "main_review": "Strengths:\n1. Most parts are easy to follow and the motivation is clear. \n2. The discussions about the relation and difference to some related literature in VOS domain are helpful.\n3. The performance is reported over two different spatio-temporal tasks. \n\n\nWeaknesses \n1. My main concern is the limited novelty. There are numerous techniques that have been proposed to address the overhead problem of global attention in Transformer by using different memory designs [1-6]. However, these efforts are totally ignored.  As Transformer-style self-attention computation scales quadratically with sequence length, how to remedy this issue has naturally been a critical theme in later Transformer-related literature.  And many follow-up efforts are devoted to improving the memory design of Transformer. \n\n[1] The memory-augmented transformer\n\n[2] Mart: Memory-augmented recurrent transformer for coherent video paragraph captioning\n\n[3] Space-time Mixing Attention for Video Transformer\n\n[4] Addressing Some Limitations of Transformers with Feedback Memory\n\n[5] Is Space-Time Attention All You Need for Video Understanding\n\n[6] Higher-Order Recurrent Space-Time Transformer for Video Action Prediction \n\n2. The main idea is to combine the fixed-size episodic memory (Lu et al., 2020) and Transformer. However, the authors do not notice there are many memory-augmented Transformer variants that have been proposed and fail to position this work with these previous closely related efforts, making many statements overclaimed ad misleading. The authors should rewrite the Abs, Intro, and literature review sections using fact-based statements, and focus on the contribution in the context of VOS, and the title should be better revised. \n\n\n3. Experiments are conducted on video object segmentation and video prediction. However, the claimed advantage of handling long-term image sequences should be better examined, as the videos in DAVIS are short (usually 3-5 seconds). Again I refer the authors to [1-6], which have more insights into the computation and memory efficiency issue of Transformer with theoretical and empirical discussions. \n\n\n4. The experiments are not convincing. The experiments on Youtube VOS (in suppl.) show the performance of the suggested model is far behind current SOATs, and even worse than STM. Experimental results on DAVIS, reported by the main body, only suggest marginal improvements.",
            "summary_of_the_review": "I lean to reject this paper due to the limited novelty and unconvincing experiments. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this work the authors propose a space-time memory network. The model is designed to maintain a constant-sized memory, and thus is scalable to longer sequences. The model is evaluated over two tasks, i.e., video object segmentation and video prediction. ",
            "main_review": "Pros:\n1. The method is conceptually simple and easy to understand.\n2. Studying constant-sized memory to strength model scalability is an interesting venue.\n3. A good set of numerical experiments are performed to support the method.\n\nCons.\n1. The methodological contribution is marginal/incremental. The proposed method is a straightforward extension of STM, with different memory update strategies (Sec. 3.2) designed to maintain a constant-sized memory. However, all the four updating strategies are very trivial, with minor contributions.\n2. Is the memory being updated at each frame? Since the computation of affinity (Eq.2) is expensive, updating with a high frequency may increase the cost. Thus, It is essential to compare the computation cost in this aspect with STM. It may be also interesting to study how the update frequency affects the performance of the proposed method.\n3. While STM benefits a lot from a linear-growth memory, the proposed method does not. As shown in Table 9, the gains become marginal when the number of memory slots is larger than 4. More discussions are needed for this.  \n4. The method shows much worse performance on YouTube-VOS against STM (77.2 vs 79.7). What are the possible reasons for the result? \n5. Several unclarities - The update function $G$ in Eq.9 should be well explained. - Some descriptions are not clear: \"create higher order interaction among memory cells\", \"This allows our model to reason on high order information of past frames\". Please explain \"higher order\" in these cases.  - \"compared to transformer-based networks which do not perform aggregation\" some references of transformer-based networks should be given.\n\n",
            "summary_of_the_review": "Overall, the paper is heavily incremental, and the improvements are also trivial. The experiments show only comparable performance. For these reasons, I tend to a rejection. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "A novel memory update mechanism is presented for sequentially processing video. The memory is fixed size by design, maintaining a constant number of memory slots . The memory update is: choose slot to free, distill selected slot into most similar other slot, insert from new frame into freed slot. Experiments are done on video object segmentation and video prediction tasks with public benchmarks.",
            "main_review": "Strengths.\n\nThe paper is easy to read and clear. The figures complement the description text of the method and well depict the memory mechanism.\n\nThe memory update is build from interpretable choices for gate, fuse, update, which makes the design explainable. It is a parameter-free function of 3 CNN blocks f,g,h which might produce strong gradients for learning. \n\nWeakness.\n\nMemory update is done without considering the new frame t+1. The update is independent of the content of new frame, which seems suboptimal design choice for the memory slot selection and distillation.\n\nThe choice for S in eq.1 is rather trivial. For example, it is pixel-wise and does not account for possible structures in the memory templates, nor for any temporal smoothness that might come in from the video input. \n\nDecoder is uninformed about the encoding stage.\n\nI would like to ask the authors to clarify on higher-order: \"This modification also allows the reasoning within the memory and create higher order interaction among memory cell\", page 6.\n\nMaybe authors want to add one sentence at the end of first text block of section 3.2 to introduce the bullets.\n\nThe reported performance is not clearly superior than existing approaches on the chosen tasks and datasets.",
            "summary_of_the_review": "The paper has some merits, the design is transparent and easy to reimplement. But at the same time, more exploration on design choice and ablations would make the stronger contribution. The results do not clearly improve over sota but i am not overrating this in my evaluation. Given the high expectations from ICLR, I am undecided about whether the developed work is at its best with the current paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}