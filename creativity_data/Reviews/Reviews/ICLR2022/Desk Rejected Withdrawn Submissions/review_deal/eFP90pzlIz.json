{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors proposed adversarial training for moderate size perturbation (e.g. $8 \\leq \\epsilon \\leq 16$ on CIFAR-10). The main idea is to use standard PGD attack for Oracle Sensitive Adversarial examples and regularized PGD with LPIPS distance for Oracle Invariant Adversarial examples. In the experiments, the proposed method, so-called Oracle-Aligned Adversarial Training, improves robustness for standard thresholds (e.g. $\\epsilon = 8$ for CIFAR-10) and moderately large thresholds (e.g. $\\epsilon = 16$ for CIFAR-10).",
            "main_review": "### Strengths:\n- Novel definition of Oracle-Sensitive and Oracle-Invariant attacks.\n- Hypothesise and show that high contrast images are Oracle-Invariant at large perturbation.\n- Relative improvement of 10-15% for small attacks and medium strength attacks in experiments on CIFAR-10 and SVHN datasets.\n- Exemplary and comprehensive experimental comparison of adversarial robustness according to the research standards.\n\n### Weaknesses:\n- The authors claim to explore the $l_p$-norm robustness beyond perceptual limits. To strengthen their claims, the authors can include experiments with human annotators to show that the predictions of the model are indeed perceptually aligned, as it was done in [1, 2] on experiments with human annotators.\n- The method combines many different techniques and has many different hyperparameters ($\\alpha$, $\\lambda$, probability of AutoAugment, schedule for $\\epsilon$, $\\alpha$, $\\lambda$, early stopping). The final accuracy and robustness are highly sensitive to these various hyperparameters.\n- Given the various hyperparameters, it is not clear how to scale the proposed defence to ImageNet as it will require fine-tuning each hyperparameter. Maybe authors can comment about the stability of the method to the final hyperparameter choice and overall budget for the hyperparameter search used in experiments.\n\n### Questions / comments:\n- $l_{\\infty}$-AT models usually are non-robust to other $l_p$-norm perturbations. Can the authors include experiments for other attack norms?\n\n[1] Matyasko, A., & Chau, Lap-Pui, Improved network robustness with adversary critic, In Advances in Neural Information Processing Systems (2018).\n\n[2] Laidlaw, C., Singla, S., & Feizi, S., Perceptual adversarial robustness: defense against unseen threat models, In  International Conference on Learning Representations (2021).\n",
            "summary_of_the_review": "The paper introduces several novel ideas. The authors conduct a comprehensive and exemplary evaluation of the proposed defense to show its effectiveness. The language and presentation are also quite clear.\n\nOverall, I recommend accepting this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies adversarially robust learning in the setting where relatively large perturbations are allowed, i.e., the perturbations are large enough that they can change the true label of an image according to an oracle (say, a human). The authors claim that learning in such a setting boils down to ensuring that the model is invariant to perturbations that do not change the oracle's prediction (Oracle-Invariant; OI) yet is sensitive to perturbations that do (Oracle-Sensitive; OS). They then propose a training scheme based on this intuition. Concretely, they train the model on two types of examples: (a) standard white-box adversarial examples but with a soft label that corresponds to a convex combinations of the models prediction on the original and perturbed example, (b) OI adversarial examples which are constructed by regularizing the LPIPS metric between the original and perturbed sample. Evaluating their method, the authors find that it offers a small improvement over existing methods on standard benchmarks. Finally, the authors draw a connection between the contrast of an image and its intrinsic robustness according to an oracle.",
            "main_review": "The paper studies an important problem: how can we approach robustness to perturbations that are not guaranteed to be imperceptible. However, the paper lacks focus and rigor at a number of places:\n\n- Sections 3.3: From what I understand, the robustness goal can be stated simply as \"the model should agree with the oracle\". Why is the distinction between OI and OS examples important at this point?\n- The major challenge here is obtaining access to an oracle the we can incorporate into training. The authors consider a very crude approximation based on the LPIPS metric. However, there is no rigorous evaluation of how well this metric performs or whether the examples produced are actually invariant for a human. Additionally, this approximation of an oracle by the LPIPS metric is first mentioned in page 6, which feels misleading given how central this approximation is to the work.\n- The title is somewhat misleading. \"Beyond perceptual limits\" implies that the model is more robust that what humans can perceive.\n- The connections to image contrast is interesting yet tangential and specific to the specific benchmarks used. If the goal of the paper is to propose a general methodology for incorporating an oracle into training then image contrast seems overly specific (it is not a general methodology for estimating per-image epsilon values).\n- Based on the abstract and introduction, the goal of the paper is to develop methods for training and evaluating models in a setting where robustness is defined with respect to an oracle. However, the treatment of the topic ends up being too narrow, and boils down to training with LPIPS-regularized adversarial examples and PGD-examples with soft labels. It is thus not clear why this is a significant departure from previous work.\n- There is no rigorous study of whether the OI examples created for evaluation are actually OI. For all we know, they might be actually flipping the oracle label and the fact that the model is robust could hint towards the existence of invariance-based adversarial examples.\n- The description of the methodology could be improved. For instance, in page 6, experimental details (exact eps thresholds) are interspersed into the description of the general algorithm thus masking the key intuition.",
            "summary_of_the_review": "Overall, while the premise of the study is interesting, the manuscript lacks the clarity, rigor, and focus that is required to make a significant contribution.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper aims to develop a defense algorithm beyond perceptual limits, which is defined with a small epsilon ball. To this end, the author proposes Oracle-aligned adversarial training that defines the label of generated adversaries with a new scheme (claim as an Oracle), e.g., it utilizes LPIPS representation as approximated Oracle. The author provides extensive experiments on several robustness benchmarks and shows strong empirical robustness.\n",
            "main_review": "**Strength**\\\nThe perspective of this paper is interesting and will give a new insight to the readers.\\\nThe paper provides extensive experiments on several robustness benchmarks.\n\n**Weakness**\\\nThe overall scheme is ill-defined. Due to the unclear definition of “Oracle,” the proposed method is hard to believe the proposed scheme is the best choice.\n* I understand the purpose of the author, but the author should rigorously define the “Oracle.”\n* Hard to understand the purpose of an Oracle-Invariant attack, i.e., LPIPS. As the adversarial feature is well-transferable across architectures [1], the defined attack will reduce the robust feature of the attack. Due to this reason, it seems like the LPIPS do not affect adversarial robustness much (in Table 4).\n\nThe comparison is unfair, as the method strongly utilizes the strong empirical benefit of {weight averaging, augmentations, scheduling}.\n* Especially, augmentations [2] and weight averaging [3] are known to be beneficial to improve robustness.\n* Also, the scheme needs careful engineering, such as scheduling. \n* For a fair comparison, removing the arguments is needed rather than adding them into AWP [4].\n\nAlthough the paper aims to achieve robustness beyond the perceptual limit, it still considers the careful design of small epsilon ball constraints. If the author defines a training scheme w.o any epsilon ball constraint, it will be more convincing.\n\nAdditional adversarial training is required for the model in LPIPS regularization.\n* While the motivation is to define Oracle prediction (to achieve beyond perceptual limits), this network needs careful design of epsilon ball size.\n\nThe experimental results slightly differ from the original paper’s results [4]. \n* Comparing the result with the reported value of [4], it is hard to claim the proposed scheme is state-of-the-art.\n* Considering an additional experimental setup will strengthen the paper, i.e., running 200 epochs (the author only considered it in CIFAR-100).\n\nAdditionally, I realize that in that main text, the authors are referring to some important analysis in the appendix, e.g., Fig 8 and ablation study. If the analysis is important in understanding the paper, please move the section into the main text.\n\n**Questions**\\\nCan the author provide the computation time, i.e., training time? I believe adversarial training requires much more time [5] than standard training, and the proposed scheme might suffer from such constraints. \n\n**References**\\\n[1] Adversarial Examples Are Not Bugs, They Are Features, Ilyas et al., NeurIPS 2019\\\n[2] Relating Adversarially Robust Generalization to Flat Minima, Stutz et al., ICCV 2021\\\n[3] Robust Overfitting may be mitigated by properly learned smoothening, Chen et al., ICLR 2021\\\n[4] Adversarial Weight Perturbation Helps Robust Generalization, Wu et al., NeurIPS 2020\\\n[5] Fast is better than free: Revisiting adversarial training, Wong et al., ICLR 2020\n\n",
            "summary_of_the_review": "I recommend weak rejection.\\\nI agree that the perspective of this paper is interesting. However, the overall scheme is ill-defined, and hard to agree on the claim due to the weaknesses. I believe another round of revision is needed for better clarification and believe it will largely strengthen the paper.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work aims to achieve adversarial robustness within larger bounds against perturbations that may be perceptible, but do not change Oracle prediction. The authors propose a novel defense, Oracle-Aligned Adversarial Training, to align the predictions of the network with that of an Oracle during adversarial training. The method incorporates separate training losses for Oracle-Invariant and Oracle-Sensitive samples in alternate training iterations. In particular, Oracle-Sensitive samples are generated by maximizing Cross-Entropy loss in a PGD attack formulation. In the alternate iteration, the authors use the LPIPS metric to efficiently generate strong Oracle-Invariant attacks during training. Experiments show the improved performance over strong baseline. The paper also shows that High-contrast images are Oracle-Invariant even at large perturbation bounds, whereas Low-Contrast images are Oracle-Sensitive at lower perturbation bounds.",
            "main_review": "Strengths:\nThe paper proposes to and tries to study unrestricted adversarial example, which is good.\n\nWeaknesses:\n1. I agree with the authors that \"While low-magnitude l_p norm based threat models form a crucial subset of the widely accepted definition of adversarial attacks (Goodfellow & Papernot), they are not sufficient, as there exist valid attacks at higher perturbation bounds as well.\" That is to say, designing unrestricted adversarial example and its corresponding defense is an important research question. However, the paper fails to study general unrestricted adversarial example, but instead still falls into the category of l_p norm based threat model (though with larger perturbation radius). To me, this is a less interesting problem to study. After all, Oracle-Invariant and Oracle-Sensitive sets are rather subjective objects to be defined for different images and datasets, and I do not think l_p norm can characterize them well.\n2. There seems little convincing justification or theory to show why the proposed method works. The novelty of the paper is limited.\n3. The experimental improvement is marginal compared with the baselines, with at most 2-3% improvement. Given there might be stronger attack than autoattack, the true improvement can be even smaller or just zero.\n4. Different images should use different radiuses to define Oracle-invariant and Oracle-sensitive. The paper uses a constact radius across different images in the training and evaluation phases, which seems inappropriate.",
            "summary_of_the_review": "Though good experimental performance, the paper lacks convincing justification why the proposed method is better. The improvement in the experimental result is marginal. Given the arms race between empirical defender and attacker, it is hard to believe that the method brings true improvement.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}