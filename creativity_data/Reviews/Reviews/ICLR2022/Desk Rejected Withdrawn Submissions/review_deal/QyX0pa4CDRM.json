{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a low-rank factorization of neural network parameters for continual learning. Each tasks adds additional low rank weight factors to the model, and a diagonal weight factor selector is learned for each task. This separates out the model weights for each task, eliminating forgetting. Experiments on a variety of continual learning benchmarks demonstrate better results for the proposed model, while also using less memory.",
            "main_review": "Pros:\n1.\tEfficient memory usage vs number of tasks, and reuse of old factors for knowledge transfer; memory advantage versus other methods clearly shown in experiments\n2.\tNo forgetting; additional tasks can be learned without any impact to performance on previous tasks\n3.\tNo replay buffer\n4.\tAcknowledges limitations\n5.\tClear presentation\n\nCons:\n1.\tNovelty: shared low-rank factorization formulation is very similar to [1] (uncited)\n2.\tRequires knowledge of task ID at test time (task incremental), which is easier than class incremental learning\n3.\tIncreased computation as the model learns more tasks\n4.\tExpansion rate of model must be hand-tuned\n\nDetailed Comments:\n\nNovelty, comparison with prior work (Cons 1,3,4):\nThe proposed approach is extremely similar to [1], also a continual learning work. In particular, the shared low-rank weight factorization with a task-specific diagonal matrix to select factors is identical to [1] (see Figure 1 from both papers, which share clear similarities). The primary difference in methodology is that [1] uses the Indian Buffet Process (IBP) and Bayesian inference to assign weight factors, while the proposed method learns the weights with gradient descent, and that the proposed method allows weight factor ranks beyond 1. \n\nNote that the proposed method has several clear weaknesses compared to [1], which are advantages of the IBP. 1) Unlike a factorized model with an IBP prior, the proposed method lacks a sparsity constraint in the number of factors used by subsequent tasks. As such, the model will not be incentivized to use less factors, leading to increasing number of factors and increased computation with more tasks. 2) The IBP prior allows the data to dictate the number of factors to add for each task. The proposed method has no such mechanism, requiring setting the growth rate by hand using heuristics or a pre-determined schedule. Either is liable to over- or under-utilization of model capacity. Table 4 in the Experiments show that this does indeed have a significant impact on performance.\n\nOverall, I think this is an example of convergent ideas rather than plagiarism, but a discussion of the connections is warranted.\n\n\nTask incremental learning:\nThis method requires knowing the task ID at test time to pick which factor selector weights to use. Without it, the proposed method doesn’t know which subnetwork to use, and would likely have to resort to trying all of them, which isn’t guaranteed to produce the right results. Recent continual learning methods are often evaluated in the more challenging class incremental setting, where task ID is not known.\n\n\nExperiments\n1.\t(+) Experiments are conducted on a good set of datasets\n2.\t(+) Error bars are shown\n3.\t(+) The proposed method mostly outperforms the baselines, especially on the more complex datasets.\n4.\t(-) More baselines should be compared against, particularly dynamic architecture approaches, as that’s the category that this method falls under. Many of the compared methods don’t operate on the same set of continual learning assumptions as this paper; in particular, the replay-based methods are often using replay because they consider class incremental learning.\n5.\t(-) Why are the results of Multitask learning so bad for S-CIFAR-100 and S-miniImageNet? My understanding is that it trains on all the data jointly, which should actually be the upper bound for a single model.\n6.\tIt would have been nice to visualize the factor selection matrices S for each task in order to visualize knowledge transfer.\n\n\nMiscellaneous:\n1.\t\\citep should be used for parenthetical citations.\n2.\tInitial double quote “ is backwards (Related Works).\n3.\t“the first task,rk,1”\n4.\tFigure 3 caption: “abd”\n\nQuestions:\n1.\tHow would you apply the weight factorization to 4D convolutional kernels?\n\n[1] “Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors”, AISTATS 2021\n",
            "summary_of_the_review": "The proposed method is very similar to a prior work [1], and has several strong weaknesses compared to said work. While the experimental results are promising and do highlight several advantages of the proposed method (advantages shared by [1]), there aren’t enough comparisons to other dynamic architecture continual learning methods to feel completely confident that the proposed method is superior. As a result of these factors, I recommend rejection, but I encourage the authors to further develop the idea. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes a continual learning method to circumvent catastrophic forgetting in deep neural networks. \nIn particular, the authors incrementally learn low-rank matrices per task that are summed over time. To prevent forgetting, the authors stop training of all parameters of old tasks. Since task id is given during test time, the authors simply choose the correct (summed) weight matrices for the neural network used for prediction. The method achieves comparable or improved performance wrt to related work.",
            "main_review": "The paper fails to compare to important baselines that lead me to recommend a reject. \n\nI am missing several crucial ablations as well as comparisons to methods very closely related.\nThe following methods all chose in some way or another subnetworks during test time while requiring task id:\n\n1. Overcoming Catastrophic Forgetting with Hard Attention to the Task - https://arxiv.org/pdf/1801.01423.pdf\n2. Continual learning with hypernetworks (not cited) - https://arxiv.org/abs/1906.00695\n3. Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization (not cited) - https://www.pnas.org/content/115/44/E10467\n4. Supermasks in Superposition - https://arxiv.org/abs/2006.14769\n\nFurthermore, a crucial ablation study is missing: How does the method compare when I simply use a single rank one matrix (not summed) per task. This basically means I would train a single network (with low memory) per task, save it and test it after training on all. The current method either has to convincingly show that it outperforms this ablation or show forward transfer when summing the weight matrices.  \n\nWithout these comparisons and ablations, the method can not be evaluated.\nAdditionally, the paper/appendix does not offer any details of how the results (of related work) were obtained.  ",
            "summary_of_the_review": "Important comparisons and ablations as well as citations missing. Technical quality is not sufficient. Reject. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Continual learning is a more challenging task than the multi-task learning counterpart in that it does not observe the data samples in previous tasks. This work introduces a continual learning strategy using low-rank factorized weight tensors. It is a dynamic network architecture method that increases the size of the network as the number of incoming tasks increases, and it trains a new separate set of weights for a new task while freezing the learned weights. Experiments on several datasets show that the presented approach performs competitively compared to some existing approaches.",
            "main_review": "The paper presents a continual learning approach without forgetting learned knowledge based on the dynamic network update strategy. The idea is simple and easy to follow. \n\nHowever, I am concerned about this method because the approach increases the size of the network as a new task comes in and will not effectively learn a long sequence of tasks. \n\nThe motivation is rather weak. I was wondering why it is particularly useful to update only the diagonal matrices of old tasks. The authors call them selectors, but it seems they are not binary values. So the statement written in the paper makes me confusing. Figure 1 does not show updating the diagonal matrices for old tasks when learning a newer task.\n\nA similar work, Orthogonal Subspace, exploits low-rank subspace, but it is introduced without analyzing the clear difference between them. The authors should clearly show the benefits and strengths compared to the work.\n\nEven if the proposal is a dynamic architecture method without zero forgetting, it is not compared with another strong competitor, PackNet, which learns multiple tasks in a single network without task overlap (it does not share parameters for different tasks, giving zero forgetting). It is required to compare it with PackNet using the large-scale datasets presented in the compared work.\n\nIn the early of Section 4.4, the authors mentioned that ‘multitask learning does not have enough capacity to learn all the 100 classes’. Multitask learning is generally considered an oracle method, but the experiment performed rather poorly. Thus, the authors should conduct experiments for a larger-scale and popularly used network as the main experiment, not such a three-layer network. Table 6 in Appendix shows only results for S-CIFAR-100. Note that the performance gap between the proposal and Ortho sub and AGEM in Table 6 (using a larger-scale network, ResNet) decreases significantly from the results in Table 1 (using a three-layer network). So what can we expect the performance difference when we use a larger network?\n\nI wonder why Table 4 excluded S-CIFAR-100 and S-miniImageNet results. Besides, it should also contain the memory consumption accompanied with accuracy for a fair comparison.\n",
            "summary_of_the_review": "It would be better to rigorously compare with the strong competitors (Ortho subspace, PackNet) theoretically and empirically. \nMore experiments using larger-scale networks and datasets to show the effectiveness of the proposed method are required to make the paper stronger.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a solution for the catastrophic forgetting problem arising in continual learning. More specifically, the authors propose to factorize the linear projections in every layer (e.g. fully connected or convolutional layers) as a linear combination of low-rank matrices (factors). During the first task, only a single factor is present. Every new task, a new low-rank factor is added and all previous ones get frozen. During inference, the model requires the task label for every example, and, with that, can isolate the correct subset of factors and construct the weight matrix for the proper task. Experiments with task-incremental learning on 5 datasets showcase the suitability of the model as compared to several baselines.",
            "main_review": "PROS:\n\n- the paper is very clear and well written, and a first read is sufficient to understand deeply the problem tackled, the proposed solution and the corresponding assumptions. The mathematical formalization seems correct to me and the underlying principle is sound. Algorithms 1 and 2 help the reader understanding how the model works.\n- to my knowledge, the proposed solution to the problem is novel. Factorizing weight matrixes as low-rank factors has become standard in other fields such as neural network compression, but it has never been applied to sequential learning problems. It represents a new way to increase the capacity of the network sub-linearily with the number of tasks, somewhat akin to masking methods (e.g. PackNet, HAT) that seek suitable parametrizations for each task within the network itself. Similarily to them, the model grows in parameters for every new task, but settles its complexity and runtime below the backbone itself. However, this model differs from them as it focuses on very different principles, namely weight factorization rather than masking.\n\n---\n\nCONS:\n\n- I find the framing of the experimental section somewhat misplaced. The model can only tackle task-incremental settings, as it explicitly requires to be conditioned on the task label in order to carry out inference on a test example. Despite the fact I do not consider this as a drawback per se, I think the experimental section lacks comparison with masking methods (what authors refer to as \"dynamic network architectures\"). Indeed, the authors claim \"zero forgetting without replay buffer\". However, in task-incremental settings, most masking methods can achieve that (e.g. [1,2,3,4,5]). For these reason, I find comparisons with these kind of models more relevant that the ones reported. For instance, iCaRL was proposed for class-incremental learning, and its use in this setting is not strictly wrong, but slightly amiss. \nMoreover, as solutions that solve forgetting completely already exist in task-incremental learning, I think the paper should focus more on second-order benefits, such by experimenting more with the computational complexity or the memory overhead required.\n- The authors rely on MLP models even for complex computer vision datasets such as CIFAR-100 and miniImageNet. This choice is clearly suboptimal as, as discussed by the authors, the multi-task learning \"baseline\" (in my view, actually an upper bound) underperforms severely with respect to CL models. This behavior is very weird to me as in continual learning works the model trained without sequential learning constraints should be considered as the best reachable performance. In this case, the backbone scores 16.4 and 4.21 accuracy for CIFAR-100 and miniImageNet respectively. In my opinion, this setup sort of disqualifies experiments on these datasets. I wonder thy the authors decided to use MLP, given that in other papers the use of ResNets for those datasets is standard [5,6,7] for continual learning in vision datasets.\n- Both P-MNIST and R-MNIST are typically referred to as domain-incremental tasks. Here are however used with a model that can only tackle task-incremental settings. Authors should clarify the exact setting under which these datasets are used. Is the task label available at test time (my guess is that this is the case). Are different classifiers employed for every task or a single one is used (if so, how is it regularized against forgetting)?\n- About competing methods: are they exploiting the task label fully? For instance, in the nearest neighbor inference in iCaRL, are stored examples from the unrelated tasks removed? Or are unrelated classes removed during inference in the classifiers of EWC and AGEM?\n\n---\nReferences:\n- [1] Rusu, Andrei A., et al. \"Progressive neural networks.\" arXiv preprint arXiv:1606.04671 (2016).\n- [2] Serra, Joan, et al. \"Overcoming catastrophic forgetting with hard attention to the task.\" International Conference on Machine Learning. PMLR, 2018.\n- [3] Mallya, Arun, and Svetlana Lazebnik. \"Packnet: Adding multiple tasks to a single network by iterative pruning.\" Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 2018.\n- [4] Mallya, Arun, Dillon Davis, and Svetlana Lazebnik. \"Piggyback: Adapting a single network to multiple tasks by learning to mask weights.\" Proceedings of the European Conference on Computer Vision (ECCV). 2018.\n- [5] Abati, Davide, et al. \"Conditional channel gated networks for task-aware continual learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n- [6] Lopez-Paz, David, and Marc'Aurelio Ranzato. \"Gradient episodic memory for continual learning.\" Advances in neural information processing systems 30 (2017): 6467-6476.\n- [7] Buzzega, Pietro, et al. \"Dark experience for general continual learning: a strong, simple baseline.\" Advances in neural information processing systems (2020).",
            "summary_of_the_review": "RECOMMENDATION:\n\nOverall, I think the submission is below the standard for ICLR. Although the proposed formulation is interesting, I think the experimental validation can be greatly improved, as explained in the cons section above, by including baselines from the dynamic architecture family and by relying on a different backbone for more complex vision tasks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}