{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper provides a practical training framework to ensure both adversarial robustness and differential privacy of the trained models. The framework combines standard practices of both communities (adversarial training from the robustness community, and gradient perturbation from the DP community). Experimental results demonstrate that this training framework indeed preserves robustness and privacy.",
            "main_review": "**strengths**\n\n* The paper provides a concise framework which is compatible with existing and potential future efforts (i.e., the modules from either community can be flexibly plugged into the framework.)\n\n* The paper provides a clear review of related work in both robustness and DP regime as well as in their intersection.\n\n* Authors conducted experiments on multiple datasets and investigated several properties such as transferability and calibration.\n\n**weaknesses**\n\n* I appreciate the conciseness of the framework, yet it feels like stitching two methods together which happens to give good results. First of all, the motivation for stitching them is not strong---why we care about having robustness and privacy simultaneously? There are other desired model properties such as fairness, smoothness, etc. Why don't we involve those? Second, the authors did not explain why they chose adversarial training and gradient perturbation (probably because they are very popular and standard?) and why it would work out. Similarly, for SecureSGD, the authors only mentioned it is \"empirically less competitive\" but did not provide any explanation for why it is poor. (By the way, SecureSGD can provide certified robustness while DP-Adv can only provide empirical robustness. Authors did not mention this difference in the paper, which feels a bit unfair to me.)\n* Under remark 3.1, authors said using both benign and adversarial examples \"makes the privacy accounting difficult due to the complicated sensitivity analysis\". If I understand it correctly, the authors mentioned a challenging problem but did not tackle it. Then what is the purpose of this part?\n* In section 3.3, the authors discussed two privacy accountants in two corollaries, but it is not clear in the later text which accountant is used where.\n* In section 4.2, the sentence \"non-robust training (SGD or DPSGD) has high clean accuracy above 96%\" is wrong. It is not DP-SGD but Adv that has clean accuracy over 96%. The rightmost subfigure in Fig. 3 is also similarly wrong---\"DP-SGD, clean\" does not match the middle subfigure.\n* In section 4.3, comparing w/o DP and w/ DP in clean testing and under adversarial attacks, I wonder why DP hurts quite a lot under attack? Also, DP added to non-adversarial training does not seem to hurt as much.\n* In section 4.4 Fig. 4, I would appreciated it if the y-axis of the three subfigures are aligned.\n* In section 4.5, why compare MNIST (without pre-training) with CIFAR (with pre-training)?",
            "summary_of_the_review": "The paper provides a concise framework and conducted several experiments to support it. However, I feel that the paper is not enough motivated and that many important questions are not explained or simply overlooked. The presentation of the paper is not clear, and there is also flaw in the statement.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes DP-Adv, a training technique that combines adversarial training with differential privacy to address both privacy (e.g., membership inference) and security (e.g., adversarial attacks) risks of machine learning models. In a nutshell, DP-adv consists can be viewed as a min-max problem, where the inner maximization problem aims at generating adversarial examples from benign samples in a non-differentially private manner, while the outer minimization consists of training a differentially private model using the generated adversarial examples. DP-Adv has been compared (on MNIST) to existing SOTA approaches that combine adversarial training and differential privacy. Additional performances results (without comparison to SOTA approaches) have been reported on CIFAR10 and CELEBA.",
            "main_review": "\nOverall, the paper is very well written. The approach proposed shows promising performances.\n\nMain comment:\n---\n\n- The comparisons are only made on MNIST. Ideally, the comparisons should be made on the three datasets to dissipate the doubts about how DP-Adv will compare with SOTA approaches on larger datasets.\n\nOther comments:\n---\n\n- If possible, I suggest reporting averaged results over several runs \n\n- Privacy and security are strongly coupled in the current implementation. That is, a correction on a security vulnerability will ruin the privacy guarantees of the model. Discussion on the impact that such design choice can have on model updates would be helpful. For instance, other design choices (e.g., model agnostic private learning) could have been discussed. \n\n- On page 2, first paragraph: \"Therefore, DP deep learning is different from the regular learning in terms of the optimizer of the same optimization problem.\" This is not necessarily true. Other examples/differences of DP deep learning include bounded activation functions or model agnostic private learning.\n\n- On page 2, first paragraph: \">70% with pre-training on CIFAR100\". A range of the accuracy would be more insightful",
            "summary_of_the_review": "Overall, the paper shows promising results. However, it will greatly benefit from positioning its performances on larger datasets with respect to the performances of SOTA approaches.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents a new adversarial training algorithm with differential privacy (DP) to achieve DP model parameters with certified robustness bounds against adversarial examples. They key difference with existing work is the replacement of each benign example with exact one adversarial example crafted during each training iteration. This is an important research line. However, there are several concerns about the privacy analysis and experimental results.",
            "main_review": "This paper presents a new adversarial training algorithm with differential privacy (DP) to achieve DP model parameters with certified robustness bounds against adversarial examples. They key difference with existing work is the replacement of each benign example with exact one adversarial example crafted during each training iteration. Experimental results conducted on MNIST, CIFAR10, and CelebA datasets shown some promising results.\n\nThis is an interesting and significant research line to address the trade-off between robustness and DP in training DNNs. However, there are several concerns as follows:\n\n1)\tThe authors indicated that Line 4 in Algorithm 1 does not affect the DP guarantee of the Algorithm 1. However, this may not be true since the adversarial examples are crafted from benign examples at each training step. As a result, a benign example $x$ could be used to generate different adversarial examples at different training steps. All these adversarial examples are used in the training process causing a group privacy problem. In fact, if the benign example $x$ are changed following the definition of neighboring datasets in DP, it will affect all the adversarial examples associated with $x$. It is unclear how we could bound the privacy risk in this scenario using the default moment accountant? The reviewer does not think that addressing this problem is trivial and straightforward.\n\nThe reviewer carefully checks the StoBatch mechanism and noticed that the adversarial examples crafted in StoBatch are DP. It is not suffered from this problem, since it is a post-processing step from a DP benign example. \n\n2)\tIn the experiments, the authors pre-train a two-layer CNN on CIFAR100 for 10 epochs before privately train on CIFAR10. Would this be fair in comparison with other baselines? This may also trigger privacy risks, since CIFAR100 is in the same domain with CIFAR10. Also, why StoBatch and SecureSGD are excluded from this comparison?  \n\nFrom the Table 2, given the MNIST dataset, the improvement of the proposed DP-Adv over StoBatch is unclear. For instance, at $\\epsilon = 0.2$, StoBatch achieved 82.7\\% compared with 74\\% of DP-Adv in terms of robust accuracy. The performance at the tight privacy protection is significant compared with larger values of $\\epsilon$, as indicated in a recent work [1]. Similarly, at $\\epsilon = 2$, StoBatch achieved 89\\% compared with 89.1\\% of DP-Adv in terms of robust accuracy. The performance of StoBatch in CIFAR10 likely to be very competitive. Thus, demonstrating a more convincing experimental results would significantly improve the paper. Stronger attacks need to be considered as well, e.g., $ l_\\infty(0.1), l_\\infty(0.2), l_\\infty(0.3), l_\\infty(0.4)$.\n\n[1] Adversary Instantiation: Lower Bounds for Differentially Private Machine Learning. IEEE S&P’21.",
            "summary_of_the_review": "Overall, this is an important research direction. I would recommend the authors to carefully rework on the privacy analysis and experimental results addressing the concerns above.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors propose a preprocessing step to DP-SGD (and variants) where the input is modified through an optimization problem to obtain an adversarial input which is then perturbed using DP noise to provide DP guarantees. ",
            "main_review": "Pros:\n\n1. Comprehensive set of experiments to validate the ideas.\n\n\nCons:\n\n1. No novel insight provided. I suspect the formulation provided by the authors is incorrect. \n2. Writing could be improved.\n\nDetailed Comments: \n\n1. The primary contribution of the authors remains unclear. Neither technical advancement (i.e., adversarial training or DP training) is attributed to the authors. While it is an interesting engineering feat to marry the two, it is unclear if this work provides any novel algorithmic insight. While algorithmic insight alone is insufficient to judge the quality of work, the results presented are not surprising: the combination of DP noise and adv. training results in significant utility degradation. Several important discussion elements, such as the run-time of the approach (since it requires solving a minmax optimization and clipping gradients per example), are not provided. I believe that the problem of studying if the noise added to obtain adversarial examples itself can provide DP guarantees is interesting, or one where the authors attempted to reduce the accuracy degradation would provide the community with new insight. \n2. I think the formulation is also incorrect. Obtaining the adversarial example relies on calculating the loss of the function which processes it (which in this case, is the ML model which is trying to be learnt in a DP manner). Thus, since there's a reliance on the ML model to obtain this gradient, there should be some noise to the input to provide DP guarantees (and thus the gradient obtained will also be DP by post-processing). This will result in a different accounting mechanism, invalidating the theorems provided by the authors. Could the authors clarify if my understanding is incorrect?\n3. The statements made about PixelDP are inaccurate; the solution does provide provable DP bounds and hence provides DP of the input.\n4. For sections 4.4, 4.5 - what is the value of epsilon achieved?",
            "summary_of_the_review": "I would urge the authors to rethink the privacy analysis and rephrase their contributions accordingly.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}