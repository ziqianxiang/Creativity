{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose a projection step on the gradient of final, fully connected layers to incorporate second-order information during SGD.  The projection chosen is a kernel matrix.  Under this setting, convergence guarantees and rates are provided.  Experimental results are provided evaluating the proposed method on several datasets and comparing to second-order solvers L-BFGS and ESGD, as well as vanilla SGD and SGD with momentum.",
            "main_review": "The projection idea and theoretical results are interesting.  However, the paper contains problems with benchmark comparisons, unjustified design choices, and a potentially misleading description of the proposed method.\n\nWith regards to benchmark comparisons, several relevant works are not compared to (or even mentioned).  In particular, while the paper claims:\n \".., updating the Hessian matrix while training the networks is practically infeasible in most second-order optimization,\"\nthere has been significant progress in recent years to develop second-order optimizers for SGD in deep learning, most notably [1, 2].  Furthermore, the comparison to L-BFGS, as implemented in PyTorch, presented in the paper is extremely questionable; this version of L-BFGS operates in full batch mode, which incurs significant latency due to device-host memory transfers, thus resulting in the inflated observed runtimes (note, such memory transfer latency is negligible for small batch sizes, evidenced by SGD runtimes).  Additionally, in practice, large batch sizes in SGD have been observed to significantly decrease generalization performance [3,4], which is observed in the both the performance of (full batch) L-BFGS and for the proposed method (in Figure 4).  A fair comparison would be against L-BFGS designed for SGD from [2] with the same hyperparameters as kernel SGD, and AdaHessian should be compared to as well.\n\nFor the second point, several design choices for the method require further exploration.  In particular:\n-\" The last layer of the network can be regarded as the inference layer which has a similar decision function as general kernel machines. Thus, it is more natural to apply the update rule (4) only in the last fully connected layer, and the rest of the layers are updated using the original SGD.\" <- This is a hypothesis.  Did the authors attempt using Kernel SGD on all intermediate layers?  If so, could you please comment on the results of using the projection on all layers versus only using on the final layer?  If not, this is a large jump in logic without evidence to support it.\n-A single RBF kernel is used.  However, an important point of exploration in this line of work would be how different kernel functions affect performance (even just a linear kernel).  Did the authors try other kernels?  And if so, could you please comment on this?\n-In the convergence theorem proof, this result holds for any positive semi-definite matrix.  Thus, without further experiments, it isn't possible to assess the utility using a kernel matrix (which is unrelated to the Hessian of the actual DNN objective, see below) versus just random PSD projection matrices.\n\nFor the last point, note that while the authors claim that their set up conveniently sidesteps a number of difficulties encountered directly computing (or even estimating) the Hessian matrix in SGD, kernel SGD does not actually incorporate (or estimate) the DNN's Hessian.  The fact that the kernel matrix is the Hessian for kernel machine objectives does not equate to the very same kernel matrix providing Hessian-information for SGD with a completely different objective function.  The fact that the kernel matrix is called the Hessian matrix throughout the discussion of SGD is potentially misleading and may result in the reader to misunderstanding that the Hessian for the DNN objective is, indeed, being calculated in some form.\n \nOther comments:\n-\"As for L-BFGS and ESGD which do not compute the Hessian matrix of\n neural networks explicitly, we recorded the memory for the components\n which are used to approximate the Hessian matrices in L-BFGS and\n ESGD.\" <- How exactly were these recorded in PyTorch?  This is an\n important, nontrivial detail and requires explanation.\n \n\"Therefore, the computation cost for Hessian matrix is rather small compared with the training of the\nwhole network.\" <-The described method is potentially burdensome in terms of both diskspace and loading the kernel matrix (and inverse) of each mini-batch into memory.  The kernel matrix (and inverse) for all minibatches is unlikely to fit in even large GPU device memories (e.g., 32 GB for V100) for modest problems and standard mini-batch sizes.  For instance, a mini-batch size of 512 requires a transfer of > 262k  parameters for a single step of SGD, repeated n/512 per epoch.  While only a mini-batch size of 64 was evaluated, what is the latency impact of this loading on speed as the mini-batch increases?\n\nI have not seen primal/dual relationship for kernel machines explained\nas it appears in Section 2.  Even w.r.t. the Keerthi paper referenced\n(which deals with RBF SVMs), the explanation given is very different.\nThe notation and description of kernels is particularly confusing;\ntypically, one either solves primal problem to determine the w's or\nthe dual to determine the \\alpha's... it is also extremely strange\nthat regularization occurs in the dual space (e.g., SVMs are typically\nL1 or L2 normalized, which occurs in the primal objective space).\nNone the less, it is easy to see from the dual of an unregularized SVM\nthat the Hessian is thus the kernel matrix.  Note that, in the case of\na regularized SVM, this is not true (the regularization parameter\noften crops up as a diagonal component).\n\nReferences:\n[1] Yao, Zhewei, et al. \"ADAHESSIAN: An adaptive second order optimizer\nfor machine learning.\" arXiv preprint arXiv:2006.00719 (2020).\n[2] Bollapragada, Raghu, et al. \"A progressive batching L-BFGS method for\nmachine learning.\" International Conference on Machine Learning. PMLR,\n2018.\n[3] Keskar, Nitish Shirish, et al. \"On large-batch training for deep\nlearning: Generalization gap and sharp minima.\" arXiv preprint\narXiv:1609.04836 (2016).\n[4] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep\nlearning. MIT press, 2016.",
            "summary_of_the_review": "Very interesting idea and theoretical results.  However, relevant second-order optimizers are not compared to, there are issues with comparing against included benchmark competitors, relevant experiments justifying design decisions are not included, and the description of the method as incorporating second-order/Hessian information for the DNN objective function of interest is misleading.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper explores second-order optimization for deep learning by leveraging kernel machines.",
            "main_review": "1. Please correct me if I am wrong; but the use of the term “linearly separable” sounds incorrect to me here: “A kernel machine uses the kernel trick to map the non-linear problem into a feature space where the problem is linearly separable.” The data might still be linearly inseparable in the higher dimensional space.\n2. The expression in Eq. (1) is the Lagrangian. Is the dual problem stated somewhere?\n3. Assuming c in Eq. (1) denotes the labels; did you mean the loss \\ell(f(X), c) is a convex function and not affine?\n4. In Figure 1, the values of the contour lines are so close to each other that the difference could be due to numerical precision error or noise. Hence, I’m not sure if this is a fair statement: “Kernel SGD found a better solution than the existing methods.”\n5. I think the argument that “In the transformed space of Kernel SGD, the optimum tends to be closer to the initial position, and hence Kernel SGD is more likely to converge to a better solution with a higher speed” is not thoroughly justified. In particular, I find that it is not fair to argue that it converges with a higher speed since the optimum being closer to the initial point does not imply any information about the convergence speed. Maybe a result on convergence rate is needed?\n6. Is the regularization coefficient set to 10^{-4} for all the methods? If so, do authors have an argument why this parameter shouldn’t be tuned for every method separately? I believe this approach may lead to unfair empirical comparisons. \n7. There is a typo here: “Figure 2: Trianing loss of each optimization method”.",
            "summary_of_the_review": "In my opinion, some of the “main” claims made in the paper require further justification/proof. Hence, my evaluation for the paper is negative.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a stochastic gradient descent for kernel machines, that uses the Hessian to precondition the update. The Hessian is the same as the kernel matrix, hence can be computed quite fast. The paper presents experimental results on several small datasets showing that kernel SGD achieves accuracies similar to existing second order algorithms, but substantially faster.",
            "main_review": "The main contribution of the paper is the Kernel SGD algorithm. The authors prove that the algorithm converges, and also show that the algorithm works in practice on small datasets.\n\nMy main concern with the paper is the presentation.\n- Please include the pseudo-code for the algorithm.\n- In all cases, please state the dimensions of the matrices.\n- What is the kernel you use? In section 3.2, you say that the input to the last layer is the \\phi(x), but then you also state that the Hessian, which is the same as the Kernel SGD matrix is constant throughout training. If K(x, y) = <\\phi(x), \\phi(y)>, and \\phi(x) is changing as a result of training, how can the Hessian stay constant?\n- You assert that the Hessian for equation (1) is the Kernel matrix. However this was not clear to me, even after I read the supplementary material. \n- Your convergence theorem is a non-increasing theorem - it does not show convergence as stated.\n- The proposition on page 5 is a very vague qualitative statement - it needs to be precise.\n- In the proof of the proposition, you say \"When sqrt(n) || ... || < 1\", do you have results that show that this is indeed the case?\n- Minor point --- on page 5, you flip the sign of the last term in ai −aj −yi +yj.\n\nExperiments:\nYour experiments got a test accuracy of 97.86 on MNIST with Resnet-18 --- this is very surprising --- even a 4 layer network can get to 99% on this dataset. Similarly, Resnet-18 can easily get to 93% on Cifar10 (https://github.com/kuangliu/pytorch-cifar), yet your numbers are closer to 83. Thus these results are not convincing.\n",
            "summary_of_the_review": "The paper was quite hard to follow, as the presentation was not very clear.\nThe experimental results were not convincing - the baselines used were very poor.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "I have read the authors' response and other reviewers' comments, and thus remain the evaluation unchanged.\n=====================\nThis paper proposes kernel SGD for efficient second-order optimization with theoretical guarantees, in which the Hessian matrix is substituted by kernel matrix, which benefits from computational and memory efficiency. The experimental results demonstrates the superiority of the proposed kernel SGD.",
            "main_review": "Pros:\nThis paper is well-written. The proposed algorithm is simple and exact, benefiting from kernel methods. The optimization problem can be transformed to a new space via the Hessian matrix of kernel machines. The cross entroy loss can be guaranteed to be non-increasing when compared to the original optimization problem.\n\nCons:\nI’m familiar with kernel methods but not an expert in optimization. I concern the following issues:\n1, this algorithm is only suitable to kernel methods based (second-order) optimization? If the optimization problem is not related to kernel methods, the Hessian matrix is not the kernel matrix?\n\n2, there are many efficient second-order optimization, e.g., Hessian matrix approximation based algorithms, BFGS’s variants. It would better to take them into discussion for theory and experiments.\n\n3, it appears a little strange why L-BFGS only achieves 87.77% accuracy on MNIST. The result is too low.\n\n",
            "summary_of_the_review": "This paper introduces kernel matrix in second-order optimization with theoretical guarantees and experimental validations.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}