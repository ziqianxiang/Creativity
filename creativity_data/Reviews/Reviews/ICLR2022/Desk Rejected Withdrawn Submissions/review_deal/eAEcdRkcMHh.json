{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper uses the ADMM method to jointly optimize N:M sparsity and Integer quantization. Integer quantization and N:M sparsity are two important directions to accelerate the heavy DNNs.\n\nThe authors evaluate the proposed method on the GLUE benchmark to show that the ADMM method can outperform existed methods (ASP+PTQ, ASP+QAT).\n\nBesides, the authors conduct the experiments to analyze the importance of pruning criteria (magnitude-based and Grad-based) and two fine-tuning methods.",
            "main_review": "Weakness:\n\n1. The motivation of this paper is clear, However, the motivation is similar to [1] and this paper exploits the same method [1] to solve the problem. The non-convex optimization using ADMM has been addressed in many related works. \n\n2. The authors only apply N:M semi-structured sparsity and quantization on fully connected layers in Transformer, similar to [3], for transformer, the attention module (QK^T, matrix multiplication between attention score and V) should be considered.\n\n3. Then the authors observe the magnitude-based pruning work better than the grad-based pruning, ref[2] pointed out the same phenomenon in Figure 7.\n\n[1] A Unified Framework of DNN Weight Pruning and Weight Clustering/Quantization Using ADMM\n\n[2] WHAT IS THE STATE OF NEURAL NETWORK PRUNING?\n\n[3] accelerating sparse deep neural networks",
            "summary_of_the_review": "This paper adopts ADMM method to jointly optimize N:M sparsity and Integer quantization. But the analysis and experiments have some issues (see weakness) and the technical contribution is limited. I will give a borderline due to those concerns and change it accordingly based on rebuttal.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The work uses the alternating direction method of multipliers (ADMM) and additional heuristics to learn structured 4:2 sparsity (2 zero elements every 4 elements) on a finetuned transformer. A pretrained transformer is first finetuned and then the sparsification+quantization method is applied. Integer quantization is induced by scaling the ADMM integer value through multiplication and offsetting to the floating point value used in training.\n",
            "main_review": "\n\n*Significance*: This paper tackles an important problem, making pretrained transformers more accessible by using both sparsification and quantization. Inducing 4:2 is a significant contribution since the additional acceleration during inference can make pretrained more useful for research that uses these models in a zero-shot way.\n\n*Quality*: The scientific quality of the manuscript is poor. My main criticism is that weak points are hidden so that the reader needs to read very carefully and understand the literature to understand that the work is less significant than it claims in the introduction. Main points include:\n- The intro talked about not needing post-finetuning training, and the presented method does not necessarily require this, but the main results seem to be for post-finetuning training\n- The paper only sparsifies and quantizes fully connected layers in the transformer. This is obfuscated throughout the paper as the paper talks about general transformer sparsification and quantization, but then in a few sentences explains the exceptions. If feels like the paper is hiding what it is doing to seem more impressive.\n- GLUE performance is reported for *the maximum performance seed* across 5 random seeds. This is a very unreliable measure for GLUE which is a suite of tasks that has considerable variance\n- The work uses a heuristic introduced by Dettmers and Zettlemoyer (2019), without citing this work\n\nFurthermore, I find it strange that sparsification + quantization results are better than results for mere sparsification. What is the explanation for this?\n\nAdditional and more detailed comments:\n- \"these layers constitute the vast majority of inference wall-time\" -- this is only true for GLUE-like tasks. For language generation, this is not true\n- \"this layer is often less than 2% of the total execution time\" -- this depends on vocabulary and model size and it is not true for multi-lingual models where this operation can be a source of significant overhead\n- \"we take a product of the absolute value of a parameter gradient momentum and the weight\" this is the same method as Dettmers and Zettlemoyer (2019), but this work is not referenced. If your method is different, then present the mathematical details to make this clear.\n- in the intro it sounds like your method does not require post-training for quantization, but your method section states that you finetune first and then you apply quantization and sparsification in a post-training (post-finetuning if you will) step. This feels inconsistent and misleading to me\n- similarly, it sounds like you apply general N:M sparsity, but it is only applied in the feedforward layers. It would be easy to mention that you only apply sparsity to these layers rather than say your whole transformers is sparse and then list the exceptions\n- the relative error of 0.6% is misleading in this case. I would recommend to report the absolute drop in performance since the accuracy on GLUE is already close to human performance. Alternatively, relative increase in error rate would also be acceptable (2.8%)\n- reporting the best result on the validation set is not a good practice for GLUE. I would recommend reporting the median performance across 10 random seeds for each task and then report the average cross tasks with 95% confidence interval\n- why is performance with sparsity worse than with sparsity + quantization?\n- it is unclear if post or direct-finetuning is used for the results in Table 1 (it seems to be post-finetuning, but this is never mentioned and obfuscated between intro, method, and analysis)\n- ADMM could be introduced in 1-2 sentence what this method is at the beginning of the background/related work section. The main references to this method are missing (in the related work section) and only methods that apply this method are references, but not the original source.\n\n",
            "summary_of_the_review": "While the contributions in this paper are significant enough to be accepted into ICLR, I currently recommend rejection based on (1) uncertainty about how reliable the results are, (2) unclear writing which makes it seem the paper makes more contributions than it actually does.\n\nI would be happy to increase my score significantly if:\n - variation of performance on GLUE is reported (in some way) rather than max performance\n - problems with \"overclaiming\" are addressed\n - sparsification heuristic and how it fits into the literature is clarified\n\nAdditionally, I could be convinced that the scores are more reliable, if it is explained why sparsification+quantization is better than sparsification.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors proposed a unified, ADMM-based approach to achieve N:M sparsity and INT8 quantization in pretrained Transformer weights. The authors first provide a unified fine-tuning objective with both sparsity and quantization constraints. Then, by introducing auxiliary variable $Z$ and applying augmented Lagrangian, the authors decompose the objective into two separate subproblems: 1) optimizing weights under the standard finetuning objective and a quadratic regularization term and 2) solving for the auxiliary variable $Z$. The authors solve 1) with Adam optimizer and solve 2) with analytically defined Euclidean projection. The authors claim to achieve better performance on the GLUE benchmark than the previous “train, prune, fine-tune, then quantize” approach.",
            "main_review": "Strengths:\n1. The paper is clearly written. The derivation is easy to follow.\n2. The idea of applying ADMM to decouple the N:M sparsity & INT8 quantization constraints is clean and straightforward.\n3. The ablation studies provide insights on important design decisions such as importance estimation strategy for N:M sparsity and fused vs. unfused constraints.\n\nWeaknesses:\n1. The paper claimed in the abstract that HoloFormer is “able to execute on newly released hardware effectively” and in Figure 1 caption that “The resulting compressed model can be trivially converted to the deployment format for compatible hardware”. To demonstrate that one of the advantages of HoloFormer is its compatibility with commodity hardware, the readers would ideally expect to see benchmarks on the target hardware, such as the improvement in inference speed.\n2. How are operations such as LayerNorm handled? If they are kept at FP32 precision, will they prevent HoloFormer from utilizing the sparsity capacity of A100 effectively? Mentioning such details would help improve the completeness of the paper.\n3. The authors mentioned that the Adam optimization steps and Euclidean projection are performed in an alternating manner until W and Z converge. It would be helpful to include the training details including the number of Adam steps in each iteration and the convergence criterion. Showing how W and Z converge as training progress and detailing how the number of Adam steps is selected would help the reader gain more insights.\n4. “For activations, we directly apply uniform symmetric quantization on activation tensors, solving for the scale by setting it to the smallest possible value such that the largest weight parameter can still be represented.” - How would the largest weight parameter translate to the range of activation? Is this a typo?\n5. Minor: should the $x$ notation in Equation (8) be $W_{i}^{k+1} + U_{k}$?",
            "summary_of_the_review": "In general, the paper presents a clean and simple approach to produce N:M sparse quantized Transformer from pretrained weights. The main contribution of the paper hinges on empirical insights and improvements. The authors demonstrate the effectiveness of the proposed method in preserving floating-point performance. Though I am still not completely clear how the contribution would position in the bigger picture of other methods that can achieve negligible accuracy loss under the 50% sparsity and INT8 quantization constraint. Thus, I would encourage the authors to produce benchmarks on A100 for rebuttal, so that the approach has a better practical edge.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed a solution for compressing the pre-trained transformers by jointly pruning and quntizing the model weights. The authors first formulate the joint compression problem as a constrained optimization problem, with two constraints on N:M sparsity and quantization, respectively. Then, ADMM is adopted to solve this optimization problem. ",
            "main_review": "First, I think the novelties of this paper should be explained more precisely. There are previous works [1, 2] discussing how to jointly prune and quantize the model weights. What are major differences between this work and these works? Can we use these methods to sparsify and quantize pre-trained Transformers? The authors should compare the proposed method with them and detail the differences in explanation and experiments.\n\nSecond, how to set the hyper parameter \\rho is not specified. This hyper-parameter may have a large impact on the model's convergence, therefore, the authors should provide more information, e.g., training curve, to demonstrate it. \n\n[1] Yang, Haichuan, et al. \"Automatic neural network compression by sparsity-quantization joint learning: A constrained optimization-based approach.\" *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2020.\n\n[2] Wang, Ying, Yadong Lu, and Tijmen Blankevoort. \"Differentiable joint pruning and quantization for hardware efficiency.\" *European Conference on Computer Vision*. 2020.",
            "summary_of_the_review": "Overall, this paper explores jointly sparsifying and quantizing pre-trained Transformers for effective compression and achieves good results in terms of performance. However, there lacks thorough discussion on comparing this method with previous joint-optimization methods. Besides, there also lacks discussion the convergence of ADMM, e.g., how to choose hyperparameter \\rho and how long does it take to converge. In all, I intend to reject this submission.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a unified framework to enforce N:M sparsity and integer quantization for pre-trained Transformer using ADMM with corresponding sparsity and bit-precision constraints. The sparsification constraint is met with magnitude-based pruning; the quantization constraint is met with integer quantization on both weights and activations with layer-wise granularity, where the selected sparse and quantized patterns are 4:2 and INT8. The following experiment shows that Transformer with such quantization and sparsification will lead to comparable or even better performance than the vanilla model.",
            "main_review": "Strengths:\n* The paper proposes a systematic ADMM framework for compressing the Transformer with pruning and quantization without sacrificing the performance.\n* The N:M sparsity that allows the pruned model to be supported by commercial devices (e.g., A100) is promising.\n\nWeakness:\n* This paper is motivated by the efficiency of NN implementation in the commercial hardware platform (e.g., A100) but I never found the corresponding efficiency measurement in the experiment session, in which way it is not self-contained and cannot well close the loop.\n\n* The novelty of the adopted techniques needs to be justified. For example, in Sec. 3.2, the author analyzes two ways of pruning for solving the N:M constraints, while finding that naive magnitude-based pruning works very well for all tasks. The adopted quantization method is also very commonly used integer quantization and I am not surprised that INT8 for weight and activations will not drop the accuracy. There are quite a lot of quantization-aware training methods that can instead improve the accuracy, e.g., FracTrain (NeurIPS'20), CPT (ICLR'21).\n\n* The considered baselines are not strong enough. For example, for sparsification and quantization of Transformer, there are related works:\n    * Chasing Sparsity in Vision Transformers: An End-to-End Exploration, NeurIPS'21\n    * Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers\n    * Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model\n\n* Also, I am curious whether the methods can well be extended to the vision Transformers developed recently apart from BERT.\n\nWith the above questions addressed, I believe this paper would be a stronger submission.",
            "summary_of_the_review": "In short, this paper develops a compression framework for the Transformer with both quantization and sparsification. While the efficiency is not demonstrated. Also, the chosen baseline and model/datasets are not enough.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}