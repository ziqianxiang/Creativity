{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work proposed the Adaptive Graph Capsule Convolutional Networks (AdaGCCN), which utilizes RL to adjust the CapsGNN model architecture at runtime. Specifically, an RL assistant module is proposed to search the optimal architecture during training. Another focus of this work is to speed up the proposed method with parallel processing.",
            "main_review": "Strengths:\n1. Details of the proposed method are provided clearly.\n2. Section 4.3 that discusses limitations of the proposed method is appreciated.\n\nWeaknesses:\n1. It's not clear what the main claims are. The proposed method shows neither a speed advantage nor a clear performance boost.\n2. It's not clear why the proposed method is coupled with CapsGCN if the RL module is to tune depth and width of convolutional layers.\n3. It's not clear how the proposed method is different from NAS.",
            "summary_of_the_review": "I will vote for rejection. The current version of this paper fails to emphasize its main claims and contributions clearly. The motivations and experimental results are also weak.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "because they use scalar-valued neurons, this paper improves on\nexisting graph capsule network, CapsGNN by adapting the neural network\narchitecture in the runtime. More specifically, the authors propose\nAdaptive Graph Capsule Convolutional Networks (AdaGCCN) which\nleverages Reinforcement Learning (RL) to design an assistant module for continuously selecting the optimal modification\nto the model structure through the whole training process. In\naddition, it determines the architecture search space through analyzing the impacts of model’s\ndepth and width. Evaluations demonstrates that AdaGCCN achieves\ncomparable accuracy results with the state-of-the-art GNN and outperforms\nCapsGNN almost on all datasets from bioinformatics and social networks.",
            "main_review": "Pros:\n\n+ Leveraging Reinforcement Learning (RL) to design an assistant module\nfor adapting the model structure during the training process is an\ninteresting idea.\n\n+ The proposed AdaGCCN is clearly explained and presented in details.\n\n+ Evaluation results are promising.\n\n\nConcerns:\n\n  - Using RL during the training process to search for and modify the\n    neural structures may create stability issues. I would have liked\n    to see some theoretical justifications or \"proofs\" of convergence,\n    or at the minimum, provide some empirical analysis on the\n    stability of the training process.\n\n- The evaluation can be done more thoroughly, with ablation studies to show how effective the RL is in adapting the network structure , and the runtime complexity, and so forth.\n\n\n",
            "summary_of_the_review": "The paper proposes Adaptive Graph Capsule Convolutional Networks (AdaGCCN) which improves\non  CapsGNN by adapting the neural network architecture during the\ntraining process. Leveraging Reinforcement Learning (RL) to design an assistant module\nfor adapting the model structure during the training process is an\ninteresting idea. The evaluation results are promising.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this manuscript, the authors proposes an assistant module (AM) that is able to dynamically change the model structure of a GNN model by adjusting the depth and the number of neurons. The AM relies on the reinforcement learning framework and exploits the loss decrease rate and the accuracy results on the validation data set to select how to modify the GCN hyper-parameters. Exploiting this component and the capsule GNN model, the authors defined a novel GNN model dubbed Adaptive Graph Capsule Convolutional Networks.",
            "main_review": "The paper proposes an interesting methodology to dynamically choose some hyperparameters of the model. The use of the Am model recalls some concepts related to the meta-learning methods and hyper-network models. The paper presents two main weaknesses: the motivations are not well discussed, and the used experimental methodology is not well detailed. In particular, this second point makes complex to reproduce the reported results.  Moreover, the claims of the authors are not always supported by the experimental evidence.\n\nIn the introduction, the authors argue the complexity of the model architecture can be reflected by the number of graph convolutional layers. Moreover, they propose an example of the influence of the number of layers on the accuracy of two graph classification datasets (fig. 1). Reporting these results without reporting the model used to achieve them and how the other hyperparameters of the used model have been tuned makes the results non-significant. Obviously, the performance of the model highly depends on all the parameters of the models, and reducing the evaluation only to the depth without considering the other hyperparameters makes the comparison of the accuracy achieved by the various considered models with different depths not fair. In my opinion, reporting these examples without a complete explanation turns out to be misleading for the reader.\n\nThe authors state that a dynamic model with adaptive structure through training would improve the classification performance significantly but the results reported in Table 2 do not always support this claim. \nIn terms of the experimental methodology followed to empirically evaluating and comparing the proposed approach there are some major issues:\n\n-The authors do not report any information about how the models considered during the experiments were validated. In [1] it has been proven that the policy used to validate the hyperparameters of a model highly impacts the model's performance. Therefore performing a fair validation phase turns out to be crucial to obtain experimental results that really prove the strengths and the weaknesses of a methodology. \nIn this regard, in section 4.2.1 the authors state: “...some hyper-parameters in AdaGCCN could be tuned to present better model performance in the future.``. Obviously, to fairly compare the results of the models involved in the comparison, all the models have to be validated following the same procedure. Moreover, the used validation policy should be as fair as possible. The one reported in [1] would be a good choice. That validation has to be used also for the proposed AdaGCCN, considering all the hyperparameters that are not tuned by the AM.\n\n-Another important point is that in table 5 the authors do not report the standard deviation of the accuracies. Since the initialization of the hyperparameters of the models exploits some random procedures, performing more than one run turns out to be crucial to correctly assess the model performance. Note that in almost all of the papers the propose the models considered in the comparison the standard deviation is reported. That is also important because some results are very close and so the difference between them can be within the variance range. Therefore it would be better to perform a statistical significance test,  in order to identify which improvements are significant. Note that the methodology followed to obtain the results and validation policy influence also the variance of the accuracy detected for each method.\n\n-The results reported in Table 5 suggests that using the AM to optimize the D and W do not worth the increase of complexity due to the use of this external model since the results of the AdaGCCN do not differ significantly (and some times are even lower)in comparison with the accuracy of the other models considered in the comparison.\n\n-For what concerns the results reported in table In table 5 please note that some results that were published in the literature outperform the ones reported by the authors. For instance, the results reported in [1] are higher for Enzymes datasets and in [2] for NCI.\n\nAnother issue that is not completely discussed is the performance of the model in terms of memory consumption. According to section 4.3 using AM seems to impact significantly the memory consumption and/or the training time of the model, making it infeasible to run the model on some datasets. Therefore in order to clearly outline the strengths and the weaknesses of the proposed model the author should report a comparison in terms of time and memory consumption between all the various models involved in the comparison.  Note that authors used high-performance hardware, in particular, they used an Nvidia v100 with 32GB of memory as GPU, which in my experience is more than sufficient to train the GNN models considered in the comparison on D&D and REDDIT in a reasonable time. Therefore the analysis reported by the authors in section 4.2.2 (and in figure 4) should be significantly extended.\n\nIn section 6, the authors state that adjusting the model structure through an RL-based searching process improves the explainability of the model. This is a very interesting point that should be further discussed. My suggestion is to show some empirical results and analyze the correlation between the selected D and W and the features of the task/dataset. In this regard along with the accuracy (reported in table 5), it would be important to report also the W and D selected for each dataset by the AdaGCCN and compare them with the one selected via validation process by the other considered models (or at least by capsule GNN).\n\n\n[1]F. Errica, M. Podda, D. Bacciu, A. Micheli, A Fair Comparison of Graph Neural Networks for Graph Classification.\n\n[2]Pasa, L., Navarin, N., & Sperduti, A. (2020). SOM-based aggregation for graph convolutional neural networks.\n\n[3]Brockschmidt, M. (2020, November). Gnn-film: Graph neural networks with feature-wise linear modulation\n\n",
            "summary_of_the_review": "The paper proposes an interesting methodology to dynamically choose some hyperparameters of the model. The use of the Am model recalls some concepts related to the meta-learning methods and hyper-network models. The paper presents two main weaknesses: the motivations are not well discussed, and the used experimental methodology is not well detailed. In particular, this second point makes complex to reproduce the reported results.  Moreover, the claims of the authors are not always supported by the experimental evidence.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a reinforcement learning based strategy for adaptively adjusting capsule graph convolutional network architectures throughout training. To that end, they introduce an assistant module which continuously adjusts the depth and width of the network based on the loss/validation accuracy reduction rate over a predetermined training epoch window.\n\nThe search space is defined by the authors as a list of m combinations of depth and width variables D and W, and an action a_t at epoch t consists of choosing one of the m depth/width architectural changes. The current training epoch with architecture depth D_t and width W_t is treated as the state, and the reward of an action at epoch t is revised via the relative decrease in training loss/validation accuracy over 3 epochs.\n",
            "main_review": "Strengths:\n\n-The approach is interesting and relevant to the target research community. \n\n-The proposed method is evaluated on multiple graph classification datasets, and is compared against various baseline models including shallow ones.\n\n-The paper is reasonably easy to understand with only a few typos here and there.\n\n-The detailed algorithm blocks included can be useful for practitioners looking to reproduce the paper.\n\nWeaknesses:\n\n-The capsule graph neural network section (2.2) is somewhat shallow and asks a lot of the reader, it could be beneficial to include a gentle but more detailed introduction to the topics discussed to make the paper more self-contained.\n\n-Lacking in ablation experiments such as on the epoch sliding window size and choice of search space for D and W. It's also not clear how the method would perform without the capsule components, i.e. is the assistant module strategy practically useful for other architectures.\n\n-Concerns of reporting test accuracy throughout training in figure 3, models could be overfit to the test set rendering the results potentially incomparable to previous works.\n\n-No standard deviations are reported in the results in table 2, and generally only marginal improvements are observed despite the increase in runtime complexity of the proposed method.\n\n-Lack of meaningful runtime comparisons between the proposed method and the various previous works without the assistant module. E.g. what about comparing to a parallel cross validation version of CapsGNN in figure 4 etc.\n\n-In the discussion the authors claim the proposed approach improves the explainability of the model, but I don't see how that is the case. I recommend you either elaborate on this or exclude it from the paper.\n\n--------------------------\n\nGeneral note: \n\nAs far as I understand, the purpose of using dynamic routing in capsule networks is to adaptively modify the network connectivity based on the input. With that said, doesn't that mean we could reduce the search space by only considering depth D and not width W, since dynamic routing already takes care of \"pruning\" W. Would be interesting to hear the author's thoughts on this. \n",
            "summary_of_the_review": "Due to the concerns raised above, I believe this paper is not yet ready for publication at this conference. However, I believe the authors may be able to significantly improve the quality of the paper by taking the actionable feedback provided into consideration. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}