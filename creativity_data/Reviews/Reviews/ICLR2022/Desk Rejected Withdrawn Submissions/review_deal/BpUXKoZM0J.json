{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes to study the SP tradeoff in context of lifelong learning, specifically focusing on the importance of individual examples. The paper proposes the MetaSP algorithm that enables them to obtain a better SP trade-off for LL. Paper includes evaluation with multiple baselines on three datasets.",
            "main_review": "## Strengths\n\n1. The paper is focusing on an important and relevant problem - balancing forgetting with plasticity.\n\n2. Comparison with a large number of baselines which suggest that the proposed approach works well in practice (some caveats apply, eg point 8 in questions).\n\n3. The writing generally flows well, though there are some statements that are confusing (check below)\n\n## Area for improvement / Questions\n\n1. Maybe this is just a matter of choice of words but SP tradeoff is a \"tradeoff\". Statements like \"to approach the Stability-Plasticity (SP) trade-off\" or \"SP is equivalent to the evaluation on the whole testing set after training well.\" make it sound like a metric. Am I missing something here?\n\n2. Several statements are confusing. For example, \" the task-level SP can be transformed to the gather of example-to-example SP of every training datum in probability\" or \"Last, two gathers of weights on behalf of Stability and Plasticity respectively are obtained and can be used to adjust the training\" or \"Examples are different, previous studies have proven one example may influence the training (Koh & Liang, 2017; Wang et al., 2018; Ren et al., 2018) and can be even used to attack (Biggio et al., 2012; Yang et al., 2017). \"\n\n3. Is equation 2 a defintion or can be derived form other definitions? To be clear, the part that is confusing me is $p$ - how does the data likelihood come in the picture,\n\n4. Isnt stability basically forgetting and plasticity basically current task accuracy? If yes, why not stick to the more common terminology?\n\n5. What is the computational overhead of using the proposed approach?\n\n6. In eq 9, we are minimizing the loss on both the old and new dataset. Since theta_S is focused on forgetting, shouldnt it only focus on old datapoints B_old. Similarly, shouldnt theta_P focus minimize the loss only wrt new datapoints? \n\n7. I did not understand equation 11 - We want to minimize the $\\gamma$ weighted combination of model's performance on the old (buffer) and new tasks. So we pick up a $\\gamma$ that minimizes that combination - is that correct? If the model was highly unstable (i.e. forgetting things very quickly), it could just set $\\gamma$ = 0 but that does not mean the model suddenly becomes a good LL model. Basically why is minimizing eq 11 (wrt $\\gamma$) going to lead to a better LL model?\n\n8. Could the authors add standard deviation for the results? Also report which results are statistically significant/ \n\n9. In table 1, why would the results on A_1 change as the size of the replay buffer changes? Isnt A1 computed just after finishing the first task?\n\n10. The results for the GDumb baseline does not match the results reported in the paper (https://www.robots.ox.ac.uk/~tvg/publications/2020/gdumb.pdf). It maybe the case with other baselines as well. Could the authors explain this discrepancy?\n\n11. Have the authors considered using the proposed influence function to determine which examples should be saved in the replay buffer?",
            "summary_of_the_review": "Please note that my initial review of the paper is based on what I understood so far. I look forward to the author's response and interacting with them to understand their approach better. I encourage them to initiate a conversation sooner rather than later. I would be very happy to change my scores as I better understand the contribution/significance of the work.\n\nI think the work is quite interesting and the results look largely positive (though I have some additional questions about the significance of the results). The writing needs to be improved - while the overall flow is good, there are some paragraphs/subsections that are confusing.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper studies the problem of lifelong learning with rehearsal, and in particular the stability-plasticity dilemma. The main idea of the paper is to investigate how each input example (in the online batch and in the replay buffer batch) impacts the performance on a validation batch, and build a rehearsal-based method that leverages this information to get better performance. Authors take inspiration from the literature on Importance Functions but propose their own meta-learning-based approximation to avoid the expensive calculation of the inverse of the Hessian. The authors compare the algorithm on three common continual learning datasets and show that they get better performance than baselines in terms of accuracy.",
            "main_review": "\nStrengths:\n- Disentangling and understanding the stability-plasticity dilemma is an interesting research direction and investigating this issue further should be beneficial for the community. In particular, trying to find the Pareto frontier of stability and plasticity seems like an interesting formalization of CL research goals.\n- The methods achieve good performance in terms of accuracy on the tested datasets, outperforming the other tested methods. I would say that the range of tested baselines is satisfactory, and although there are certain interesting works that were not discussed (e.g. Dark Experience Replay), the evaluation setting is sufficient.\n- The authors measure not only the final accuracy of the model but also provide separate metrics for measuring forward transfer and forgetting. This allows the reader to better understand the properties of each method.\n\nWeaknesses:\n- The motivation of the proposed method has certain important limitations that were not discussed.\n  - The authors propose their method as an approximation of the Importance Function which itself approximates how each training example impacts each testing example. Calculating the Importance Function requires calculating the inverse of the Hessian of the loss function, which is understandably too computationally heavy to be performed during training, so the authors decide to move to a lighter, meta-learning-based approach. However, I don't see a direct connection between the Hessian-based approach and the meta-learning update function, i.e. how well the latter approximates the former. It would be helpful to showcase this even on a toy problem where the Hessian is easier to calculate. Currently, the method itself seems to be disjoint from the theoretical background provided in the paper.\n  - I'm not convinced by the argument that stability/plasticity can be represented as the sum of stability/plasticity of each example in the training set (Definition 1), as the relationship between examples can be quite complex. Imagine for example, that the training dataset contains two datapoints (let's call them A and B)  which are exactly the same - this assumption is not unrealistic, as some duplicates tend to appear in real-world datasets. If we asked \"what is the impact of A\" in this framework, the answer would be \"A has no/very low impact on the dataset\", as B contains the exact same information, so removing A does not hurt us that much. Similar logic can be applied to asking \"what is the impact of B\". However, if we were to remove both of these examples, the impact on the result could be much more severe. While I agree that studying the impact of each example in separation is more tractable, the paper seems to imply that this decomposition is the perfect solution and does not discuss the limitations of the performed analysis, which I think is needed.\n- Empirical evaluation is not thorough enough at times and doesn't take into consideration important properties of the proposed algorithms.\n  - The paper does not discuss the computational complexity introduced by the proposed algorithm. MetaSP performs three updates on the weights $\\theta$ (two virtual ones) and that by itself would triple the computational cost of a standard ER algorithm. Additionally, calculating per-example gradients in the pseudo-updates and the meta-gradient over $w$ also seem very expensive. The issue of computational expenses is important in the continual learning setting, as we want to build learners which are explicitly limited in terms of computation and memory, and as in this paper the authors consider a setting with a fairly limited computational time (5 epochs), this is even more crucial. As such, I think a thorough discussion of this point is needed.\n  - The results do not include standard deviation so it is difficult to say if the improvements are significant enough. Looking at some of the related work used as baselines in this paper, the standard deviation may reach even around 2 percentage points, which may raise questions about the significance of the improvements.\n  - The discussion of how the baseline results were obtained should be extended. For example, in the original GDUMB paper, the results on the CIFAR-10 dataset are much higher than in Table 1 in this paper. In particular, the GDUMB paper reports 45.8% accuracy with 500 samples, and Table 1 in this work reports 33.24%. Discussion of these discrepancies would be useful.\n  - This is less of a weakness, and more a suggestion to improve the paper in the future, but performing some analysis (both quantitative and qualitative) on what kind of examples are important for plasticity and stability, would be interesting if we want to understand this phenomenon. Potential questions that I personally find interesting include: what is the average distribution over the obtained weights $w_S$ and $w_P$? Is the distribution closer to uniform for $w_S$ than $w_P$ (i.e. does the need to be example-specific is stronger for stability than for plasticity?). How do the weights of examples relate to their classes, are the weights higher for certain classes?\n- Clarity and technical correctness of the paper should be improved in several ways. Most of the corrections presented (chronologically) below are minor, but in combination, they obfuscate certain points about the method and make the paper more difficult to understand. I've skipped issues concerning grammar/language as they should not impact the final score according to the guidelines, \n  - In Section 3.2 the sentence \"The three metrics evaluate the capability to approach Stability, Plasticity, and SP trade-off, respectively.\" appears before \"Finished Accuracy\" and \"Mean Average Accuracy\" are even defined.\n  - In Section 3.3 the authors quote \"Lemma 1\" which does not appear anywhere in the text. I assume you meant \"Definition 1\"?\n  - In Algorithm 1, line 1, there is a dot product between the weights $w_S$ and a gradient w.r.t $\\theta$, however these do not have the same dimensionality. I guess this means that the loss function $l(B_{old} + B_{new})$ outputs a vector $\\in \\mathbb{R}^{|B_{old} + B_{new}|}$, and that gradient is meant to be understood as per-example gradient w.r.t. to $\\theta$. However, the same function $l$ in other places (e.g. eq. (1)) returns a scalar, loss averaged over examples in the batch. This is inconsistent and confusing.\n  - Is Eq. (11) a constraint or a function to be minimized? It seems to be a constraint, but in the next line, you write that $\\gamma^*$ is the argmin of $l_{SP}$.\n  - In Eq. (11) gradients w.r.t $w_{old}$ and $w_{new}$ appear and these symbols were not defined, I assume you mean $w_S$ and $w_P$?\n  - In Fig. 4 you do not say what dataset is presented in the figure.\n  - In Section 5.3 you write \"(...) First Accuracy $A_1$, Finished Accuracy $A_\\infty$ and Mean Average Accuracy $A_m$. The three metrics evaluate the capability to approach Stability, Plasticity and SP trade-off, respectively\". Did you mean \"Plasticity, Stability and SP trade-off\" (different order)?\n  - In Algorithm 2 in the Appendix $B_{old}$ and $B_{new}$ are not explicitly defined \n  - In Appendix A.2 the proposed proof for SP decomposition seems to be wrong. E.g. how is the probability of the dataset the sum of probabilities of each test datapoint? I could see an argument for it being a product of the probability for each datapoint if we assumed that each datapoint is independent, but I don't see how it can be decomposed as a sum.\n  - Since the code is not provided with the paper, I was unable to check the implementation for clarification of these issues.",
            "summary_of_the_review": "Although the main idea of the paper is exciting (investigating the stability-plasticity dilemma by looking at per-example contributions in the dataset), I cannot recommend acceptance of the paper in the current form. The theoretical motivation of the proposed method should be extended to show how the meta-learning approach ties in with the presented theoretical background of Importance Functions. The empirical evaluation section lacks certain important points, such as a discussion of the computational complexity (which seems to be quite higher than for other methods), standard deviations in results, and a discussion on how the baseline results were obtained. Given the current state of the paper, I recommend rejection.",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a novel and interesting algorithm for rehearsal-based Lifelong Learning (LL) over a sequence of datasets/tasks to balance the Stability and Plasticity (SP) at fine-grained level. The proposed algorithm is based on the hypothesis that examples can be very different even if they belong to the same underlying distribution. Therefore, it dives deep into the rehearsal mechanics and decomposes the influence of training data on test data to each example (fine-grained) and tries to achieve a better trade-off between Stability and Plasticity (SP) by jointly optimizing Stability-aware and Plasticity-aware weights as a multi-objective optimization problem. Therefore, the weight of each example during training process is determined on the basis of its contributions to SP.",
            "main_review": "Strong points:\n - The motivation is well founded and the claims are sound.\n - Theoretical formulation of \"example level influence function (IF)\" and \"MetaSP algorithm to simulate IF\", individually, is well-formed and detailed.\n - This paper provides improved quantitative results for \"First Accuracy\" ($A_1$) and \"Mean Average Accuracy\" ($A_m$), to show the effectiveness of the proposed algorithm in acquiring new knowledge in LL.\n\nWeak points:\n - It seems to me that \"Finished Accuracy\" ($A_{\\infty}$) is actually the average of forgetting on all past tasks when training is finished for all tasks. By definition of catastrophic forgetting, $p(D_t^{tst}|\\theta_T)$ should be less than $p(D_t^{tst}|\\theta_t)$, hence the final average value of \"Finished Accuracy\" ($A_{\\infty}$) should be a negative value but it is positive for all datasets in table 1. Why? Also, if I assume that a norm is implied in $A_{\\infty}$ then also higher values suggest higher forgetting which means model is performing worse in remembering past tasks.\n - No formulation and comparison of computation complexity of task-level vs example-level rehearsal-based LL via MetaSP algorithm.\n - It is not convincing to me how \"example-level influence function (IF)\" in equation 8 is directly related to / influenced \"MetaSP algorithm to simulate IF\" as one coherent narrative.\n - No definition of probabilistic representation ($p$) used in equation 2.\n - Confusing terminologies: \"step\" (used in algorithm 1) and \"iteration\" (used in Figure 2 caption).\n - A table of all notations used can improve the readability.\n - Incoherent use of notation for loss ($l$) as a scalar value in equation 1 and as a vector in equation 6.\n - Incoherent use of notation for $w_{old}$, $w_{new}$ in equation 11 and $w_S$, $w_P$ in all other equations respectively.\n - Malformed sentences: first paragraph after equation 3 on page 4.\n - Figure 4, 5: missing axes descriptions/notations.\n - Missing references for proofs/details in appendix for equations 5 and 8.\n \nTypographical errors:\n - Equation 1: $D_t^{tm}$ -> $D_t^{trn}$\n - Last sentence (\"The three metrics evaluate the capability to approach Stability, Plasticity and SP trade-off, respectively.\") in the definition of \"First Accuracy\" needs to be before the definition of all three accuracy metrics.\n - Section 3.3 first para: \"Lemma 1\" -> \"Definition 1\"\n - Line after equation 4, in equation 5, in equation 8: $x_i^{tm}$ -> $x_i^{trn}$\n\nFurther Questions:\n- What are the conditions of well trained model as mentioned in abstract?\n- What if instead of forgetting of past tasks, training on new tasks actually improves performance on past tasks?\nIn that scenario should this algorithm be applied in the same way because then it would be restricting to a lower optimization point while it could achieve higher?",
            "summary_of_the_review": "Overall, I vote for rejection. Although, the motivation for this work is sound and its theoretical foundations are detailed. I have encountered certain problems (specified above) with theoretical formulation, evaluation metrics and final quantitative results which contradicts the positive results mentioned in the paper. I look forward to read the clarifications from authors on my above-mentioned queries and update my score accordingly.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}