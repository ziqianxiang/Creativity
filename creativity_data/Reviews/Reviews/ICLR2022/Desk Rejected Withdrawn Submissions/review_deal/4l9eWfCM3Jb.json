{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose a framework for modeling controller adaptation in the presence of novel test-time disturbances. They subsequently propose a method that, with access to samples from the test-time environment for fine-tuning, “jointly optimizes” both the model and the controller for performance in the test environment. The proposed method is then evaluated on a series of experiments, where the performance is compared to that of existing model-based and model-free methods in the literature.",
            "main_review": "**Concerns**\n1. There is a lack of discussion regarding model-based methods that directly train policies (e.g. MBPO [1], SAC-SVG [2]), and the novelty of the proposed training framework in the context of these existing works is unclear. Furthermore, given the existence of methods in the literature that learn a model for policy optimization, the theoretical derivations and following taxonomy of methods seem superfluous and do not provide particularly useful insights (additionally, see point #3 for correctness concerns).\n2. The usage of “joint optimization” terminology is a bit questionable here, since the algorithm trains the model and the policy separately using different objectives. In the literature, “joint optimization of the model and policy” usually refers to expressing the return directly as a single (differentiable) function of both the model and the policy, so that the model and policy can be jointly optimized to maximize return (as opposed to training the model using a separate maximum likelihood objective), see [3, 4, 5, 6].\n3. The exposition relating to the problem setting suffers from significant clarity and correctness issues. Among these:  \n    1. (Correctness) The definition of the optimal controller in (4) cannot be correct - the optimal action at time $t$ should also depend on the distribution of $(s_{t'}, \\nu_{t'})$ for $t' > t$. That is, the optimal action should be defined using the $Q$-function (as is standard in RL), not the instantaneous cost at time $t$. In the context of the problem setting, the best action seeks to not only get as close to the current reference point, but also make it easier to reach future reference points.  \n    2. (Correctness) The direction of the inequality in (5) is incorrect. If one were to try to quantify the infeasibility of a reference point from a current state, then any action $a$ would have to incur a minimum cost (not a maximum as suggested by Equation 5).  \n    3. (Clarity) It is unclear where the reference $\\nu$ comes from, and how it changes over time - is that determined by the agent, or the environment, or the policy (or something else)?  \n    4. (Clarity) The notations used to introduce the policy $\\pi$ and $\\hat f$ suggest that both of these objects are deterministic, and thus it is unclear what the random objects involved in the relevant expectations are.  \n    5. (Clarity) In the definition of the optimal controller in (4), why is there an expectation over $(s, \\nu)$, when it seems like the goal is to define the optimal action for a fixed $(s, \\nu)$?  \n    6. (Clarity)$\\hat{f}$ is defined as the fixed dynamics model obtained from the source environment, but in section 2.2, $\\hat{f}$ is trained to be close to $f^\\ast$.\n4. Experimental setup concerns:\n    1. The choice of fine-tuning samples provided to each algorithm seems rather arbitrary in Table 1 - why are there no reward curves provided for each method?\n    2. How are MBMF and PETS trained? Are the pre-trained models retrained on test environment data, or are they trained with an additional residual model?\n\n**Minor comments**\n1. The discussion regarding imitation learning methods in observation 3 does not seem to be particularly relevant, and is not referenced anywhere else in the text.\n\n**Typos**\n1. Algorithm 1: References use $r$ instead of $\\nu$ in numerous places.\n2. Introduction, third paragraph, first sentence: adaption → adaptation.\n3. Section 2: Many definitions involving $\\nu$ have a $v$ instead.\n4. End of page 3: “Equation equation 8” → “Equation 8”\n\n[1] Janner, M., Fu, J., Zhang, M., & Levine, S. (2019). When to Trust Your Model: Model-Based Policy Optimization. Advances in Neural Information Processing Systems, 32, 12519-12530.  \n[2] Amos, B., Stanton, S., Yarats, D., & Wilson, A. G. (2021, May). On the model-based stochastic value gradient for continuous reinforcement learning. In Learning for Dynamics and Control(pp. 6-20). PMLR.  \n[3] Amos, B., Rodriguez, I. D. J., Sacks, J., Boots, B., & Kolter, J. Z. (2018, December). Differentiable MPC for end-to-end planning and control. In Proceedings of the 32nd International Conference on Neural Information Processing Systems (pp. 8299-8310).  \n[4] Eysenbach, B., Khazatsky, A., Levine, S., & Salakhutdinov, R. (2021). Mismatched No More: Joint Model-Policy Optimization for Model-Based RL. arXiv preprint arXiv:2110.02758.  \n[5] Okada, M., Rigazio, L., & Aoshima, T. (2017). Path integral networks: End-to-end differentiable optimal control. arXiv preprint arXiv:1706.09597.  \n[6] Srinivas, A., Jabri, A., Abbeel, P., Levine, S., & Finn, C. (2018, July). Universal planning networks: Learning generalizable representations for visuomotor control. In International Conference on Machine Learning (pp. 4732-4741). PMLR.  ",
            "summary_of_the_review": "I vote for rejecting the paper. As I outline in my list of concerns, the paper does not adequately address existing literature on training policies using learned models, and has correctness/clarity issues in the provided theoretical derivations/exposition.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies sensorimotor controllers which are often subject to unpredictable/sudden model variations. It aims to reduce sample complexity for policy adaptation, in order to quickly overcome such perturbations. To do so, the proposed algorithm jointly runs system identification and planning submodules, which should result in more adaptivity when deploying the trained policy from sim to real. Experiments are conducted on Cartpole, 3D navigation with a fixed-wing aircraft, and trajectory tracking with a quadrotor. ",
            "main_review": "*Strengths*\n\nThis work tackles a challenging problem that is yet to be solved, at least in the RL setting. It aims to make standard baselines such as PPO and MBMF more realistic by running experiments on sensorimotor controllers. It also has a strong motivation and proposes an intuitive solution (=joint learning) to tackle few-shot policy adaptation. \n\n*Weaknesses*\n\nAlthough this work is well-motivated, it raises three main concerns that lead me to a « reject » decision. \n- Firstly, theoretical contributions seem a bit trivial to me, as most results are derived from simple triangular inequalities. I think this comes from the problem formulation, which also raises some issues. \n- Secondly, regarding the experiments, it is not clear to me why PPO and MBMF were specially chosen as relevant baselines. PPO in particular does not tackle policy adaptation, as far as I know, and there are many other baselines that would have been more relevant to this work.\n- Thirdly, comparison to related work seems incomplete to me, particularly to RL works which I am most familiar with. Specifically, this work does not position itself w.r.t. meta/transfer-learning works, or more recent ones that employ change-point detection on top of policy training. ",
            "summary_of_the_review": "I detail and justify my concerns below. \n\n- The problem formulation is not standard for an RL setting, and I would expect the authors to provide a few references supporting their setting. The only reference provided there (Sec. 2, Borelli et al.) is on MPC, whereas the proposed solution is an RL technique. Not that these two different settings are incompatible, but I expect the challenges encountered in each of them to be quite different, so the authors should provide strong arguments for plugging an RL method into this MPC problem. \n- More precisely, I have a few questions on this problem formulation:\n    - What is the role of the reference state $\\nu_{t+1}$? Except for the additional control policy, what is the difference between the objective function in Eq.(1) and a mean squared error (letting $p=2$)? If I understand correctly, this objective function may be translated as an agent aiming to copy a trajectory by minimizing the additive errors induced by his control policy. As such, I see this as a particular RL problem with a more specific reward function. Thus, since all baselines run in the experiments are RL methods, in my opinion, it seems more natural to formalize the setting as an MDP.\n    - Sec. 2 - \"with a possibly different state-space\": What does it mean? In which state-space does $\\nu$ live? How is a distance measure meaningful if, say, we originally work in $\\mathbb{R}^3$ but test our policy in $\\mathbb{R}^4$ as our target space?  \n    - Only a finite horizon is considered here. How does the problem translate for infinite horizon settings? Such discussion is missing here. In fact, the objective function in (1) implicitly takes a discount factor $\\gamma=1$, which would not be possible in an infinite horizon. On the other hand, taking $\\gamma<1$ in Eq.(1) would result in the following objective inside the expectation: $\\sum_{t=0}^{T}\\gamma^t C(f^*(s_t, \\nu_{t+1}), \\nu_{t+1})$, which does not seem to have a logical interpretation for the considered problem (e.g., why would immediate errors be more penalizing than long-term ones? etc.).\n    - All analytical derivations come from inequalities that are quite trivial to me, which questions the fundamental theoretical contributions of that work. \n- On the algorithmic aspect:\n    - Joint learning has already been proposed in a few papers such as [1, 2], where a VAE is trained simultaneously to a policy that aims to adapt to changing environments. This work needs to be compared with those ones, among others. \n    - MBMF sometimes outperforms the proposed method («  Finetuned » part of table 1 and a few runs in Figures 2-3). Why is it expected? Why does not this question the quality of your method? \n    - Other algorithms would have been more relevant as baselines (as mentioned before, I do not understand why PPO appears in this work). These are: LILAC [2], variBAD [1], Prognosticator [4], DQN-URBE [3], among others. \n- The related work section mentions related approaches, but it does not justify why, fundamentally, the proposed method is more efficient. It even says in the first paragraph of Sec. 4 « leveraging uncertainty methods for sim-to-real scenarios may be an interesting opportunity for future work». \n- Other fields in RL would have been relevant to be mentioned and compared with: meta-learning, transfer-learning, robust RL, POMDPs, etc., which seem very much related to this work.  \n\n*References*\n\nIf I am not mistaken, all of these references do not appear in related work. \n\n[1] Zintgraf, L., Shiarlis, K., Igl, M., Schulze, S., Gal, Y., Hofmann, K., & Whiteson, S. (2019). Varibad: A very good method for bayes-adaptive deep rl via meta-learning. ICLR 2020.\n\n[2] Xie, Annie, James Harrison, and Chelsea Finn. \"Deep Reinforcement Learning amidst Continual Structured Non-Stationarity.\" International Conference on Machine Learning. PMLR, 2021.\n\n[3] Derman, Esther, et al. \"A bayesian approach to robust reinforcement learning.\" Uncertainty in Artificial Intelligence. PMLR, 2020.\n\n[4] Chandak, Yash, et al. \"Optimizing for the future in non-stationary MDPs.\" International Conference on Machine Learning. PMLR, 2020.\n\n[5] Arcari, Elena, Andrea Carron, and Melanie N. Zeilinger. \"Meta-learning MPC using finite-dimensional Gaussian process approximations.\" arXiv preprint arXiv:2008.05984 (2020).\n\n[6] Lecarpentier, Erwan, and Emmanuel Rachelson. \"Non-Stationary Markov Decision Processes, a Worst-Case Approach using Model-Based Reinforcement Learning, Extended version.\" NIPS (2019).\n\n\n*Minor comments*\n- Sec. 1: « and too dangerous » —> «  and may be too dangerous»; « adaption problem »—> «  adaptation problem»; «  we learn a model … jointly»—> « we jointly learn a model… »\n- Sec. 2: «  this encode» —> «  this encodes»; «  which is often added »—> remove «  which is». \n- Sec. 2, Observation 2: « iLQR or MPC »: these acronyms were not introduced before. \n- « equation Equation n » in a few places (maybe use \\eqref to also make parentheses appear in the reference)\n- Fig. 1 and Algo. 1 are referred to in the text but not really explained, which makes it hard to fully understand them. An explanation of both is welcome. \n- Please provide a name for your algorithm (e.g. using acronyms). It would be much easier than calling it \"yours\" or \"the proposed method\".\n ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper deals with the use of models for data-efficient RL. It considers the embodiment that does not use a value-function (or Q-function, or critic). Furthermore, the embodiment in which the model and the policy are jointly optimized by gradient descent is considered.",
            "main_review": "Strengths\\\nAn overview of the diverse approaches of value-function free, model-based RL methods is useful.\n\nWeaknesses\\\nThe overview is incomplete and especially does not consider older work.\n\nThe novelty of the presented method is, in my opinion, low and not sufficiently distinguished from existing approaches.\n\nE.g., further work on gradient-based optimization of model and policy:\n\nB. Bakker et al., A robot that reinforcement-learns to identify and memorize important previous observations, 2003\n\nB. Bakker, The State of Mind: Reinforcement Learning with Recurrent Neural Networks, 2004\n\nA.M. Schaefer et al., A recurrent control neural network for data efficient reinforcement learning, 2007\n\nD. Wierstra et al., Recurrent policy gradients, 2010\n\nS. Depeweg et al., Learning and policy search in stochastic dynamical systems with Bayesian neural networks, 2017\n\nFurther comments\\\nThe sentence \"To account for this limitation, model-based RL approaches learn a model of the system and a control policy jointly\" is not correct in this form, since not all model-based RL methods do this simultaneously.\n\nThe sentence \"in contrast to traditional modelbased RL, we optimize the model and control error via gradient descent\" is misleading. There are already quite a few papers that do exactly this.\n\nI cannot agree with the statement \"We empirically show comparable or better results with respect to prior works in terms of performance on the downstream task and sample complexity in the target domain.\", it seems to me that MBMF is clearly superior on CartPole, Quadrotor, Fixed-wing.\n\n\"30N\" -> \"30 N\" (space between number and unit if it starts with a letter).\n\n\"m/s\", \"m\" not italicized\n\nThe sentence \"We use our framework to review existing methods and motivate a novel approach which jointly minimizes the control and identification cost with gradient descent. \"could be misunderstood to mean that the present work is presented as the first that \"jointly minimizes the control and identification cost with gradient descent\".\n\nIs it necessary that both „Wanxin Jin, Zhaoran Wang, Zhuoran Yang, and Shaoshuai Mou. Pontryagin differentiable programming: An end-to-end learning and control framework. arXiv preprint arXiv:1912.12970,\n2019.“  and „Wanxin Jin, Zhaoran Wang, Zhuoran Yang, and Shaoshuai Mou. Pontryagin differentiable programming: An end-to-end learning and control framework. 2020.“ are referenced?\n\nI find it inconsistent to write \"e.g.\" and \"i.e.\" in italics and \"et al.\" not.\n\nPlease check the bibliography for accidental lower case letters, like „gaussian“, „mpc“\n",
            "summary_of_the_review": "The paper has only limited novelty.\n\nSome relevant existing work is not mentioned. \n\nThe novelty of the own method is not sufficiently justified against existing approaches.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}