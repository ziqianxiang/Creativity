{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper considers a new definition of neighboring datasets for a DP definition. The neighboring relation is called user-entity where datasets can differ either in one user and a sensitive entity. The paper proposes a DPSGD like approach to satisfy this definition. It experimentally shows that utility of their mechanism is better than that of user-level DP.",
            "main_review": "Strengths:\n- the paper proposes a privacy definition that may be more relevant for NLP settings than prior work\n- experimental section is very well written and provides empirical evidence of the approach \n\nWeaknesses:\n- the new definition does not have enough motivation\n- formal presentation and analysis could be improved as currently it seems that the mechanism is data-dependent\n\nThe definition: It is not clear how U and E are related to D. As a result it is hard to say how would one satisfy this definition.\nComparison of the definition with prior work: The paper makes a really good point in trying to separate itself from existing (related) definitions. However, it would help to express previous definitions using U, E, D, A, f to make it clear how the new definition differs.\n\nMotivation: The introduction states examples of entities. It is not ideal for privacy to use an automatic way of defining what is sensitive or for users to highlight what they consider sensitive. It would be good to see a bit more motivation for where this would apply and where correlation with something user missed in identifying as sensitive may be leaked.\n\nAlgorithms: It seems that parameters that algorithms choose in Section 4 are data-dependent and the guarantees are satisfied only if a bound holds for all users. Smooth sensitivity maybe needed to provide formal guarantees.  That is, what if the parameters estimated using sampling in Lines 3,4 are not correct?",
            "summary_of_the_review": "Summary: The paper makes an interesting and relevant contribution and provides a great experimental evaluation. My main reservation is due to technical details and how this would translate to privacy guarantees if assumptions on parameters made by the paper do not hold (explained above).",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new definition for adjacency between databases called User-Entity Adjacency. It further proposes an algorithm for federated learning that provides differential privacy under this notion of adjacency relation.",
            "main_review": "Strength: \n+ The problem the paper aims to address is an important one. Machine learning with differential privacy is a very active research area, and this paper could be a valuable addition to the literature in this field.\n\n\nWeakness:\n- Significant portion of important technical details is left in the appendix. For example, the table for notations used in this paper (Table 2) and the main technical contribution of the paper (Algorithm1, the UEDP-Alg) all appear in the appendix, as well as all the proofs. This has negative impact on the readability of the paper. The appendix should not be used to “save space”.\n\n- The User-Entity Adjacency definition does not seem realistic. Under this definition, two databases are adjacent if one can be obtained from the other by removing one user (and all his/her samples) AND one entity (all samples that contain this entity, which may come from different users). However, imagine in an extreme case that all samples contain the same entity. Removing such an entity will result in no samples left. In reality, for large databases (e.g., social media data), removing one entity could remove an unbound number of training samples from the database. It’s hard to imagine that good utility could be achieved under this model.\n\n- The experimental results are questionable. The paper claims that, on large models like BERT or GPT-2 with hundreds of millions of parameters, they can match the accuracy of a non-private noiseless baseline with epsilon as small as 0.19 (see Figure 3 and 4 on Page 8), while providing user-entity level DP which, if I understand correctly, is much stronger than sample level DP. This result is highly counter-intuitive. More details are needed to explain the setting for the experiments, e.g., which parameters are fine-tuned in those large language models, the whole model, or a selected number of layers. \n\n- The main algorithm (Algorithm 1 in Appendix C) is unclear. It contains several non-standard hyper-parameters (three sampling rates and one ad hoc parameter called z). The paper does not have sufficient discussions on how those parameters affect model privacy/utility, as well as how to choose their values. Also, the notations used in Algorithm 1 are inconsistent. Both left arrow and = are used. Do they both mean assignment? The \\forall symbol is used in Line 29. Does that mean a for loop? From Line 21 and 28, it seems that the clipping is done to the gradient times the learning rate, not just the gradient. Does this affect the privacy accounting?\n\nTypos:\nPage 7:\nParagraph 2: “(ber; Devlin …)” -> “(Devlin …)”\n“…which has a much fewer parameters…” -> “…which has much fewer parameters…”\n",
            "summary_of_the_review": "For the reasons mentioned above, I think this paper needs further polishing and do not meet the standard for publication on a top tier conference like ICLR yet.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper introduces “user-entity” DP and a DP-SGD variant to go along with the new notion. This notion is applied to NLP models trained on textual data from users. The paper includes some empirical results showing improved privacy-utility trade-off.\n",
            "main_review": "Strengths:\nThe work is well-motivated by a real-world problem\nThe writing and presentation tends to be sufficiently clear\nThe trade-off seems numerically favorable based on experiments\n\nWeaknesses:\nThe privacy notion offered by user-entity DP is potentially problematic\nIt may require extensive manual labeling effort to identify all sensitive items and some content might be considered sensitive by the user even if not by the system\n",
            "summary_of_the_review": "Overall, I think the paper is addressing an interesting setting with real-world application. The paper defines UeDP as a DP notion that prevents sensitive texts from being revealed. In this sense, (as I understand it) noise is only added to gradients containing sensitive entities. While this makes sense, the issue is that the authors propose to solve it by essentially having a big lookup table of everything that is a sensitive entity. In my opinion, there are a ton of practical issues with this approach, and the authors do not give strong evidence that these practical matters can be overcome. \n\nThe privacy notion fails to address any type of long-term sensitivity. It is obviously possible that an entity alone would not be deemed sensitive, but it is in fact sensitive in the context of a particular passage. This is not addressed by the privacy notion. Thus, this is an extremely superficial type of privacy compared to DP. Nevertheless, at least the authors show a nice improvement in privacy-perplexity trade-off. The statistical performance (perplexity) attained by the language models is quite good for such a small epsilon.\n\n\nWith that being said, I think that the problem the authors are addressing is quite important, and that the privacy definition explored by the authors is novel. \n\nI might recommend the authors explore a variant of their algorithm that simultaneously guarantees an eps-DP or eps-delta-DP guarantee at a higher epsilon with an UeDP guarantee at a small epsilon. In this situation, the algorithm will protect all user privacy at least a little bit, but will increase the protection for sensitive entities by focusing the noise there. This could be a practical solution for this problem.  \n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}