{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors proposed to study weakly few-shot instance segmentation as a new problem. The authors used BoxInst as the basic framework and followed the evaluation setting proposed by MTFA (Ganea et al., 2021).",
            "main_review": "The presentation of this paper is clear and the content is quite easy to understand. The paper itself is well written and polished. However, I consider the position of this paper a bit weird. Despite being new, the research problem seems a created one given the nature of few-shot learning in object detection and instance segmentation.\n\nI'm okay with methods being frustratingly simple, as long as there are good insights motivations. A major issue of this work, however, is that I don't see the introduction of instance segmentation, or the introduction of box supervision, brings much new things to the few-shot representation learning itself. Recent instance segmentation methods (such as CondInst/SOLO) are often formulated as a bottom up (feature branch) meets top-down (RoI feature), and it is well known that the bottom up branch is open set generalizable and the top-down part is often the bottleneck for few-shot generalization. From this aspect, I don't see the work proposes anything new over the existing work of two-stage fine-tuning (Wang et al., 2020). The proposed work is a naive combination of BoxInst and two-stage fine-tuning, and most of the contribution in few-shot learning comes from (Wang et al., 2020). To me I do not see a clear role of instance segmentation in improving few-shot generalization.\n\nIt is also not entirely clear where the advantage over previous methods comes from. It seems that the gain may not have come from the specific designs/inductive biases from this work (using box supervision). Rather, it may come from the special formulation and strong performance of FCOS. Without fair comparison on the same backbone, it is difficult to draw concrete conclusions.\n\nMissing important reference\nLan et al., DiscoBox: Weakly Supervised Instance Segmentation and Semantic Correspondence from Box Supervision, ICCV 2021",
            "summary_of_the_review": "Right now, my main concern to the paper is that the position is weird and the experimental settings are not convincing. But I will be open to response from the authors.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes the weakly-supervised few-shot instance segmentation task and a first baseline.\nStarting from instance segmentation (IS), the task's goal is to train an IS model without full annotation but using bounding boxes.\nMoreover, the dataset is composed of two subsets: a dataset with a large set of samples of base classes and a dataset containing K samples per each novel class (K=1,5,10).\nThe baseline is very simple: the IS architecture is first trained on the base dataset using available bounding boxes and is then finetuned, freezing the Mask Head, on the few samples.\n\nThe paper assesses the model performance on the VOC and COCO datasets, focusing in particular on the latter. Since the method is the first in the setting, the authors propose a simple baseline that applies GrubCut on top of the detection architecture.",
            "main_review": "1 - The technical contribution of the paper is minor: the paper proposes to train an IS model using the loss proposed by Tian et al in the base training phase and then, using the same loss, to fine-tune it by freezing the mask head. \n\n2 - The paper analyzes and proposes a relevant task, with concrete applications in the real world. However, it would be better to see also the behavior of the method in more relaxed settings, e.g. when many images are available with bounding box annotation (that is, not in a few-shot scenario). While I acknowledge that this task would have a different scope, it would validate the experimental results and disentangle the effect of having only a few images and the one of weak supervision.\n\n3 - Moreover, the methods' performances on the novel classes are very low (Table 1, 2, 3), even when considering the fully-supervised baselines and the performance on detection. For example, when considering K=1 in Table 1, MTFA obtains 2.89 FG-AP and 6.14 FG-AP50 on Detection and 2.4 and 4.15 FG-AP and FG-AP50 on Segmentation. To me, the setting appears already very challenging for the object detection task, so it is unclear why it should be even complicated by adding the segmentation task.\n",
            "summary_of_the_review": "The paper proposes a novel and interesting setting but (i) the technical contribution is minor and (ii) the proposed setting is very restrictive and experiments on a relaxed setting would help to understand the setting challenges and method contribution.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper claims to introduce the problem of weakly supervised few-shot segmentation in the sense that supervision is weak in all phases of the learning, i.e. during meta-learning/base training and fine-tuning. Weak supervision is defined in terms of class and box annotations. A method to address the problem and an evaluation metric are proposed. Results are shown on COCO and PASCAL VOC with a refined test-split.",
            "main_review": "The claimed contributions cover task, method, and evaluation. Regarding the task, the contribution is trivial in the sense that weak annotations and FSS existed before and the task is a combination of both. Also, some confusion in terminology occurs as some previous works (cited ones such as Raza et al. and uncited ones such as Siam et al., IJCAI 2020) use the same terminology, but conduct the meta-learning step/base training using dense annotations. Furthermore, other authors sometimes refer to sparse labels instead and the cited work by Rakelly et al. uses those both during meta-learning and FSS.\n\nThe method contribution is mainly a combination of FCOS (Tian et al. 2019) and a weakly supervised loss (Tian et al. 2021) during base training. This particular combination is, to the best of my knowledge, novel. However, I found no stringent motivation to use exactly this single combination and an ablation on this aspect has not been performed.\n\nRegarding the experiment part, I found the organization of the paper confusing. The most relevant question to address is what performance the new method achieves compared to SOTA methods on standard datasets and using standard evaluation protocols, but I had difficulties to extract that information from the included tables. This information is essential, even if results are sub-SOTA. The only results of this type seem to be in table 2 \"base\" column? But also here, the query samples seem to be drawn form novel classes, too. I consider it mandatory to conduct experiments with standard data and evaluation, even if alternatives are suggested. Changing the evaluation at the same time as introducing a new method does not allow to determine the origin of the improvement of results.\n\nSome of the issues above could be addressed in a revision. In a potential response, the authors are requested to clearly state what changes would be made (e.g. renaming of problem definition, structure of the experiments section) and in particular with respect to the experiments, which results to be added.",
            "summary_of_the_review": "The paper formulates a slightly different problem than what has occurred in the literature before, without clearly delineating terminology. The method to address the problem is a combination of previous approaches. The experimental evaluation is confusing and lacks clear SOTA comparisons in standardized settings.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper is about few-shot instance segmentation using the bounding box supervision in both training and testing. The main idea is to adapt BoxInst -- a box supervision instance segmentation -- to the few-shot setting by using two-stage training. BoxInst is first trained on base classes with abundant labeled examples then fine-tuned on few-shot examples of new classes with the whole network frozen except the last few prediction layers. The authors also propose a new metric for evaluating named FG-AP which presumably disentangles the effect of each head. Also, the authors also introduce a new evaluating split that focuses on the novel classes only. ",
            "main_review": "Strengths:\n1. This paper introduces a new setting of few-shot instance segmentation where we only need box supervision for both base-class training and novel-class fine-tuning stages. \n2. Introduces a new metric for evaluating named FG-AP \n3. Proposes a new test split which only includes images having at least one novel object instance. \n\n\nWeaknesses:\n1. This paper introduces a new setting but it is arguably redundant since one of the main goals of few-shot learning, in general, is to transfer knowledge from base classes that have abundant labeled examples to novel classes that have a few labeled ones. And nowadays, we have very large datasets with instance segmentation annotation such as OpenImage, LVIS, NuSense, BDD100K, etc which cover a good amount of everyday scenarios including indoor and outdoor scenes. Therefore, the need for a new setting with box supervision is very low. \n\n2. This paper basically adapts BoxInst designed for standard box supervised instance segmentation to the few-shot setting with no modifications at all specially designed for the few-shot setting. The fine-tuning approach for few-shot learning is already proposed in prior work including (Raghu et al. 2020; Wang et al, 2020)\n\n3. The new proposed metric FG-AP is not well described in the paper and it is not clear what is the difference between it and the standard AP since the standard AP only focuses on the foreground object classes only.\n\n4. The new proposed test split is quite unrealistic since the testing of the false-positive rate is also important as given an image without any object of new classes the models should provide zero predictions.",
            "summary_of_the_review": "The lack of novelties in both problem setting and approach are the main drawbacks of this paper. Also, the description of the new proposed metric FG-AP is not well written. Finally, the introduced test split is unrealistic as well. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}