{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces an active learning strategy for multi-label classification and object detection using logic constrains among classes. The authors show the knowledge-based data selection worked better than random sampling and model uncertainty-based methods, especially when the knowledge of the domain is complete (i.e., have a list of logic constraints among classes). The paper is generally well-written. But I do have some concerns regarding the generalization of the proposed method as well as why it works.",
            "main_review": "Strengths:\n1. Select samples based on the logic constraints instead of model uncertainty is somewhat novel.\n2. The proposed method is well-written and easy to follow.\n\nWeaknesses:\n1. Define a set of rules (logic constraints between classes) for the task seems a bit unscalable and ad-hoc. Even for scenarios with only partial knowledge, the list of rules seems quite long and covers all the classes in the dataset. Does such a list have to be manually generated for every task? What if there is no clear logic constraints between certain classes? \n2. The improvement of KAL over the uncertainty-based method is not as clear as the authors claimed. The proposed method has a diversity-based metric built-in, which forces the KAL select samples from different groups. As a consequence, the KAL has better ability to discover new data distribution compared to the uncertainty selection. It would be more interesting to see a comparison between KAL and an uncertainty-based method paired with a diversity metric.\nMinor: Should it be \\argmin in the uncertain criterion (Eq. 5)?",
            "summary_of_the_review": "The proposed method is limited to certain type of problems. The experimental results are not convincing enough.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Traditional deep learning algorithms are overconfident in their predictions and thus are unable to accurately quantify the uncertainties in order to identify data distributions lying far from training data. Thus, uncertainty-based approaches for active learning face a challenge in selecting samples. This paper proposes a new algorithm that uses domain knowledge to select samples for active learning. Specifically, domain-knowledge present in the form of First-Order Logic clauses is translated into real-valued constraints/scores that can be used to select samples during active training. The proposed method is compared against baseline methods which include a random method (selecting data samples at random), an uncertainty-based method (selecting data samples closer to decision boundary), and a supervised method which is the upper bound of performance that can be achieved. A synthetic XOR-like dataset and two real-life type datasets (Animals and PASCAL-part) dataset is used for evaluations.",
            "main_review": "Overall, incorporating domain knowledge as a constraint is an interesting idea that has been proposed previously. This paper uses the domain knowledge in identifying inconsistent samples during active training. My major concern is about the experimental results and clarity of the paper.\n\nPros:\n1.\tThe paper proposes the use of domain knowledge in the context of active learning which is an important task.\n\nConcerns:\n\nMajor Concerns\n1.\tThe key concern of the paper is experimental results. Although, the idea of using the domain knowledge is interesting, the results seem very close to the other baselines especially in the Animals and Pascal-part dataset. Further analysis based on robustness of the algorithm, ease of use needs to be done. \n2.\tUsing first-order-logic to design loss functions has been proposed in other papers (Gnecco et al., 2015; Diligenti et al., 2017). The main contribution of this paper is the idea of using it to identify uncertain samples in active learning. However, the benefits of the method over the uncertainty-based methods are not clear due to the results being close to the other baselines. This paper lacks the comparison with other uncertainty-quantification methods like MC-dropout, Bayesian approximations, etc., which are more state-of-the-art. In addition, comparison with active-learning strategies is also essential as the simple uncertainty-based method shows quite close performance with the proposed method. Since this method is constrained by the availability of domain knowledge, comparison with more SOTA baselines is essential as they do not require the domain knowledge to be available and thus are more generalizable.\n3.\tAdditionally, the algorithms depend on the reliability of the individual parts classifiers, and thus under-trained parts classifiers can lead to faulty signals of FOL violations.\n\nMinor Concerns\n\nThe writing in the paper lacks clarity which makes it difficult to understand. A few examples are listed below:\n1.\tSection 2, Paragraph 2: “Let us illustrate this with an example of the first case” and “Now, let us provide an example for the second case”, which are the cases being referred to here. Based on the FOL, I assume that the first case is of object detection and the second case is of multiclass classification. But at the beginning of the paragraph, these cases are introduced in a reverse manner.\n2.\tSection 2, Paragraph 2: “where x1(x); x2(x) respectively represent the logic predicates associated to the first and second feature”. Here the use of the feature is ambiguous. I assume “feature” here refers to the parts/attributes of an object while the feature is introduced in a different sense at the beginning of the paragraph.\n3.\tIn equation 1, K should be replaced with Kappa.\n4.\tSection 2.2, the derivation of the constraints Phi_1 and Phi_2 should be provided in the supplementary material as it is not trivial and not present in the referred article. Moreover, state clearly which of the constraints (Phi_1 and Phi_2) models the forward or backward implication. I think Phi_1 models the backward implication i.e. f=>(x_1 & ~x_2)|(~x_1 & x_2) and Phi_2 models the forward implication. For clarity and better readability, it will be good to state these clearly.\n5.\tThere are several other typos present in the paper.\n6.\tFigure 4 d, please label the axis of the plot as it is difficult to identify which axis is supervision loss and knowledge violation loss.\n",
            "summary_of_the_review": "This paper proposes an interesting idea of identifying/selecting uncertain samples for active learning. Due to the lack of sufficient analysis and comparison of baselines (please see the Concerns), the paper in its current state is not ready for the ICLR acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors describe a framework where first-order-logic knowledge is converted into constraints and their violation is checked as a natural guide for sample selection. They empirically demonstrate that knowledge-driven strategy outperforms standard strategies, particularly on those data sets where domain knowledge is complete. Furthermore, the authors also show how the proposed approach enables discovering data distributions lying far from training data.",
            "main_review": "the strength: domain knowledge is first introduced into active learning for sample selection. If the decision on a sample is not consistent with   the rules, it should be selected. This strategy is claimed as the main contribution of this work.\n\nthe weaknesses: 1. it is usually difficult to get the rules in real-world applications.  Statistical rules learnt from data may be feasible. 2. the experimental section is a little weak. More experiments are required.\n",
            "summary_of_the_review": "The authors describe a framework where first-order-logic knowledge is converted into constraints and their violation is checked as a natural guide for sample selection. The idea is simple and easy to understand. However there is a big problem how we can get enough rules. To my knowledge, we can obtain some rules in few cases. So the work is not limited. Moreover, the experiments are not enough and the authors just show two examples.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors present a new active learning algorithm tailored for multi-label classification (and possibly for object detection) problems. The proposed `knowledge-driven active learning’ algorithm exploits domain knowledge presented in the form of logical relationships between different target classes. Specifically, the authors propose to instantiate such logical relationships into first-order logic (FOL) and to measure how the corresponding FOL constraints are violated. This general framework was evaluated on Animals and Pascal part datasets (as well as a synthetic XOR dataset) where the corresponding FOL formula are defined based on existing studies on specifications of object parts, attributes, and their relationships.",
            "main_review": "I think this paper makes a borderline case for the reasons given below\n- This paper presents a new and interesting approach to exploiting multi-label class contexts. To the best of my knowledge, this is the first attempt to build an active learning framework specialized in multi-label classification.\n- Given the idea of using FOL constraints, the derivation of the actual algorithm seems rather straightforward.\n- The application domain of the proposed method seems to be limited to cases where well-defined `knowledge’ presented in the forms of attribute- or class-level relationships is available: Please advise me if I am mistaken.\n- Experiments focus on comparisons with Random, Supervised, and Uncertain only. Existing active learning algorithms (e.g., VAAL, Learning Loss, and BADGE) are not tailored to multi-label classification problems but they can still be evaluated on the Animals dataset and therefore they should be compared with. On the Animals dataset, the proposed method looks only comparable to Random.\n",
            "summary_of_the_review": "A borderline rating for\n1) interesting new idea;\n2) limited technical novelty given the main idea;\n3) potentially limited application domain;\n4) weak experiments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a knowledge-driven active learning strategy, which converts the heuristically designed first-order-logic knowledge into constraints. The violation of these constraints serves as the objective for sample selection, i.e., the samples that most violate the constraints are selected. To increase the diversity of the selected samples, the number of samples that violate the same constraint is limited by a pre-set maximum number. The proposed active learning strategy is evaluated on the XOR problem where the domain knowledge is given and complete, the multi-label image classification problem on a subset of the ImageNet dataset, and the object detection problem on the PASCAL-Part dataset.",
            "main_review": "The strengths of this paper include:\n+ The proposed active learning strategy seems to be technically sound when prior knowledge can be obtained and formulated into first-order logic.\n+ Code is provided for better reproducibility.\n\nConcerns:\n- It seems that the novelty of this paper is limited. The proposed active learning strategy follows the learning from constraints framework in (Gnecco et al., 2015) and adopts the triangular norms in (Klement et al., 2013) to convert the first-order-logic knowledge into numerical constraints. These constraints are aggregated into the loss for sample selection. Therefore, it seems that the proposed method is a straightforward application of existing techniques to active learning.\n\n- There are doubts about the significance and generality of the proposed strategy. The first-order-logic knowledge needs to be manually pre-determined, which is quite heuristic and highly depends on the target problem and data. In the toy XOR problem where knowledge is complete and known, there is no need to apply active learning to solve this problem. In the image classification problem, the knowledge defined in (Winston & Horn,1986) is directly employed. However, when classifying other classes and attributes, and in other real-world problems, it is not trivial to design and formulate proper logic-based knowledge, where expensive manual labeling may be required.\n\n- It is not clear how the proposed strategy is applied in the object detection problem on the PASCAL-Part dataset. It is claimed in Section 3.1 that “a Faster R-CNN network (Ren et al., 2016) is trained on the bounding boxes extracted from each mask”, does it mean that the Faster R-CNN needs to be pre-trained from the ground-truth annotations? Is the proposed active learning approach used to select the region proposals by faster R-CNN that most violate the constraints? If so, uncertain-based active learning method can also be applied to classify the regions (It is claimed in Section 3.4: “As anticipated, it is not straightforward applying UNCERTAIN (Eq. 5) sample selection to the object-detection context, thus, in this case, we restricted the comparison to the other methods only.”).\n\n- Other experiments are performed on toy and small-scale datasets where FOL rules are known or have been defined in the literature.\n\n- The proposed strategy is only compared with three simple baselines, including random selection, the standard uncertain-sample selection, and a supervised strategy which serves as the upper bound. It is difficult to evaluate the effectiveness of the proposed strategy without comparisons with state-of-the-art active learning methods.\n\n- It is not clear how to determine the hyper-parameters such as the maximum number of selected samples per constraint and the number of initial labeled samples, which are set differently in different experiments. From the experimental results, it seems that thousands of actively selected samples are required to obtain satisfactory results even for the toy XOR problem.\n\n- Some notations in Section 2 are difficult to follow, e.g., x^1(x), x^2(x), Muzzle(x), Dog(x), etc.",
            "summary_of_the_review": "The contribution of this paper seems to be incremental, and the empirical evaluation is not convincing enough.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}