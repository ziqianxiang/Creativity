{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper develops a new reinforcement learning algorithm for constrained MDP problems. ",
            "main_review": "Strengths: \n\nNew performance bounds GAE for general functions. \n\nWeaknesses: \n\nThe design of the algorithm is not new. The algorithm includes two steps: policy improvement and projection to a safe policy. The new GAE bounds are interesting, but overall the design is not new. The second step is computationally expensive because it is a constrained optimization problem. ",
            "summary_of_the_review": "Some interesting ideas but the contribution is incremental. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors establish a useful lemma that portrays the generalized performance difference using the generalized advantage estimator, apply it to the constrained Markov decision processes, and propose a conservative update policy method. The authors conduct a series of computational experiments to verify the effectiveness of the proposed method. \n",
            "main_review": "Strengths:\n\n(1). The paper establishes a generalized policy performance difference regarding the generalized advantage estimator in Theorem 1. This development further leads to lower/upper bounds on the reward/cost value functions, respectively. I recognize this development is important in both theory and practice.\n\n(2). The proposed conservative update policy is featured by the use of the generalized advantage estimator. In theory, the authors show better policy improvement and constraint violation if compared to existing results. This is a significant improvement. \n\n(3). Some comparison experiments are provided to show that the proposed algorithm achieves the best performance among several baseline algorithms.  \n\n\nWeaknesses:\n\n(1). It might be useful to establish any performance guarantees on the implementation of the proposed conservative update policy method. \n\n(2). Is the dependence on $|S|$, $|A|$ necessary in your theory? This could break down your theory. Do you observe the effect of the size of $|S|$, $|A|$ in experiments?\n\n(3). It is informative to compare with standard methods for (23) in your experiments. \n\n============================\n\nPOST-REBUTTAL: I have read the response from the authors and other reviews. After I read your revised paper and tried your code to reproduce experiment results, I opt to downgrade my original decision for the following two reasons.\n\n(1). I have tested the supplemented code of CUP for Circle Humanoid and Velocity Humanoid, using the default parameters. In both cases, CUP has a lower average reward than FOCOPS after 1e7 interactions, although they share a similar average cost. This contradicts the performance indicated in the last line in Table 1. I have consistently seen this bad performance of CUP in other environments velocity ant, velocity Cheetah, velocity walker, velocity hopper, except that they share a similar performance in velocity swimmer. This result reproduced by your shared code contradicts the observation made in Table 1. We note that the result of FOCOPS indeed matches the result in the paper: First Order Constrained Optimization in Policy Space. This makes me worry about the generalization of the performance in Figure 1 to some other standard environments. My initial judgment mostly considers the consistently improved performance of CUP in both theory and practice. Now I feel at least the practical performance is still questionable. In theory, I still feel uncomfortable about the $\\mathcal{S}\\mathcal{A}$-dependence that seems to be a disadvantage of the generalized advantage method. However, this does not appear to be an issue in experiments. \n\n(2). The related work on Local Policy Search and Lagrangian Approach is missing a lot of recent results. Some references are cited inaccurately, for example, the paper: A primal approach to constrained policy optimization: Global optimality and ﬁnite-time analysis does not address the problem Eq. (23) using the Lagrangian Approach. This reference's two versions repeatedly appear in your reference. Other recent references on primal-dual policy optimization should be carefully discussed, for example, the paper: Natural policy gradient\nprimal-dual method for constrained Markov decision processes which studies a primal-dual policy gradient method. Also, recent references on local policy search are not covered yet, for example, the paper: Accelerating Safe Reinforcement Learning with Constraint-mismatched Policies. I feel it might be a chance to miss one or two references, but unfortunately, this is not the case here. \n\n\n\n\n",
            "summary_of_the_review": "I agree with a new general constrained policy optimization. However, unfortunately, I think more efforts to be made to explain the consistency of the results in theory and practice, and to study the generalizability of the algorithm performance across environments. Therefore, I opt to lower my original rating. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies the problem of learning under constrained Markov decision processes. It proposes to solve this problem by an RL algorithm based on generalized advantage estimator (GAE), which they call  Conservative Update Policy (CUP). Although existing works have already proposed GAE-based algorithms, the current paper is the first to provide theoretical foundations for this type of algorithm. Specifically, it proves bounds on performance difference for GAE-based algorithms and one-step improvement for CUP. Numerical experiments have been conducted to showcase the performance of the proposed algorithm.",
            "main_review": "Pros:\nThe paper justifies a GAE-based algorithm with both theoretical and empirical results.\n\n\n\nQuestions and concerns:\n- The theoretical results and their proofs seem to be incrementally adapted from existing ones. \n- In Theorem 1, the lower and upper bounds are functions of \\psi, while the middle quantity is independent of \\psi. This appears elusive to me. Could the authors provide intuition for the validity of these inequalities?\n- Table 2 shows the variance of CUP is often higher than other algorithms. This is quite counter-intuitive to me given that CUP is based on GAE which, as the author claims in multiple occasions, is supposed to reduce variance. Could the authors provide justification for this?\n- The paper mentions that all experiments were performed over 3 random seeds. I'm a bit concerned that 3 seeds may not be sufficient to lead to substantial statements about the performance of RL algorithms, which are well known to be volatile across trajectories. \n- It is unclear to me what hyperparameters were used for the benchmarking algorithms.\n- The paper has many typos and grammatical errors. I list some of them below.\n\n\nTypos:\n- Abstract: we develop them at least three aspects\n- pp1, Line 5 from bottom,  bases on ->  based on \n- pp2, Paragraph 2, Line 5, while maintains -> while maintaining\n\n\nPost rebuttal:\n\nI appreciate the response from the authors. Unfortunately, I still feel that \n- The theoretical results are a bit incremental\n- The experiment results are not convincing since they are based on only 3 random seeds\n- The issues raised by Reviewer nTtg add on to my concerns about the experiment results\n\nTherefore, I decided to maintain my score.",
            "summary_of_the_review": "The paper can be improved in both aspects of theory and experiments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper considers reinforcement learning, a common model which has seen success recently in many areas (e.g. games, robotics, autonomous vehicles).  Prototypical reinforcement learning algorithms explore the action space in order to maximize their rewards as much as possible, potentially ignoring impacts of the chosen actions or safety constraints.  Typical to many real-world scenarios, though, are constraints on the selected actions allowing the algorithm designer or practitioner to enforce certain constraints on the selected actions in the environment (e.g. ensure that the autonomous vehicle stays within the dictated lines on the road, etc).  This is typically done by introducing a cost-function and adding an additional constraint that the resulting policies have long-run discounted cost upper bounded by a given constant.\n\nWhile many RL algorithms have been designed in this setting, they mostly focus on either taking lagrangian relaxations of the given policy constraints, or constructing convex approximations to the non-convex objectives to optimize (which also in turns adds to additional complexity in constructing higher order moments of the objective for optimization).  In contrast, the authors in this paper take a different view, via the following:\n1. Construct tight upper and lower bounds on the difference in value functions for arbitrary policies with respect to arbitrary functions $\\phi$, a given additional discount parameter $\\lambda$\n2. Instantiate the upper bound with $\\phi$ taken as the value function to get a lower bound on the difference in value between any two policies\n3. Instantiate the lower bound with $\\phi$ taken as the cost-value function to get an upper bound on the cost-difference between any two policies\n\nOnce these bounds are established, the algorithmic approach is simple.  In the first step they perform a performance improvement step to improve the performance of the policy by maximizing the lower bound on the difference in value between the two policies.  The second step is a projection step, where after lagrangianizing the constraint set they minimize the lower bound on the difference in costs between the two policies.  The authors show a guarantee on per-step improvements on the rewards and cost for this proposed approach assuming exact maximization / minimization of these objectives.\n\nTo be more specific, the authors consider the typical RL model with an MDP characterized via $(S, A, P, r, \\gamma)$.  They consider the goal of maximizing the expected long term rewards $r$.  However, there is an additional cost function $c$ and they add an additional constraint that the long-run discounted cost is upper bounded by a given constant $b$.  The hope is that the learned policy picks actions which satisfy the long-run average cost constraint while simultaneously maximizing the expected rewards.\n\nThe authors first start-off by proposing a novel bound, generalizing the policy performance difference $J(\\pi_\\theta) - J(\\pi_{\\theta'})$ between any two policies.  In particular the difference is upper and lower bounded by two terms.  The first term can be interpreted as the expectation between the TD errors of $\\pi_\\theta$ and $\\pi_{\\theta'}$ where TD errors are computed for an arbitrary function $\\phi$.  The second part is the discounted distribution difference between the two different policies.  These bounds are then instantiated with different functions $\\phi$ and parameters in order to get lower and upper bounds on the performance difference between the policies with respect to the costs and rewards of the problem.  Once these lower and upper bounds on the performance difference are established, the algorithm falls naturally by maximizing the reward lower bound and minimizing the cost upper bound.  This is then theoretically justified by providing a per-step improvement with respect to the two different objectives.  \n\nTo complement the algorithmic framework, the authors present a set of synthetic experiments to compare the efficiency and constraint violation of the resulting policies of their method and others in the literature.  In particular, they test the policies on three different environments and compare to several other existing algorithms in the literature (more on this later).  They show that their algorithm and approach dominates others in terms of performance while simultaneously ensuring the cost constraints.",
            "main_review": "### Originality:\n\nThe authors present a novel generalized policy performance difference in terms of an arbitrary function $\\phi$ and discount factor $\\lambda$.  In particular, when instantiating this bound with $\\phi = V^{\\pi}$ then the bounds give a lower bound on the performance difference, which illustrates a worst case approximation error in terms of the GAE and distribution mismatch between the induced distribution under the two policies.  Moreover, instantiating with the cost value function you get an upper bound on the cost difference again in terms of the cost-GAE and distribution mismatch between the two policies.  These two upper and lower bounds naturally give rise to a simple algorithm, termed 'Conservative Update Policy' which performs in two steps, first optimizing the rewards before then projecting back to the safe policy set (by having explicit bounds on the cost of the policy).  They show that these steps give rise to a natural, implementable algorithm with theoretical guarantees, and through experiments show competitive performance to existing algorithms in the literature.\n\n### Quality:\n\nThe submission is technically sound and the theoretical claims are well-supported, but there are some slight aspects which could help improve the clarity of the presentation (as described later).  The authors are honest and upfront in the new techniques used in their modeling and algorithm development, namely:\n- new generalized policy difference lemmas\n- allowing for more easily-estimatable quantities instead of just fixed under a given policy\n\n### Clarity:\nThe submission is well-organized. In particular, I enjoyed how the authors included the related work after the algorithm description, as it made it easier to relate their approach to existing approaches in the literature (although see the questions section for some comments on this).  \n\nHowever, the submission has a couple sections which could be improved:\n\n- Please explain the algorithms in the experiment section instead of just the acronyms for readers like myself who aren't super familiar with the related work (maybe a table in the appendix highlighting acronyms and the high level approach)\n- The generalized policy performance results seemed a bit 'useless' (for lack of a better word) as written in terms of arbitrary functions $\\phi$ and the parameter $\\lambda$.  It might be helpful to at the beginning of that section give a brief interpretation for $\\lambda$ and possible instantiations for $\\phi$ and how that will be used in the algorithm design.\n\nMoreover, there are some confusing sentences and grammatical errors, including:\n\n- Acronym w.r.t is used frequently, might be more helpful to just rewrite those sentences or expand the acronym\n- \"develop them at least three aspects\" in abstract\n- \"so good\" in abstract\n- \"troubles\" in introduction - not referring to anything explicit\n- \"compact\" is never really described\n- \"derive the CUP bases on some new\"\n- Interchange CUP and CPU acronym\n- \"using difference bound to replace\"\n- \"Our new bounds in refine classic difference bounds\"\n- \"Provide the necessary details\"\n- Last couple sentences of section one is a bit confusing\n- $s'$ notation in section 2 looks a bit off\n- \"still lacks a theory analysis\"\n- Equation 11 should be $V_{\\pi_\\theta}$ instead of $V$\n- Missing a $t$ in superscript for expectations in equation before (7)\n\n### Significance:\n\nThe theoretical results with the generalized policy performance theorem is well-justified, and leads to the author's novel conservative update policy technique in terms of the GAE estimators.  Their result has per-iteration guarantees on the objective improvement, which justifies their approach.  Moreover, the experimental results help justify their approach by showing the algorithm has improved returns while still satisfying the cost constraints.\n\n### Strengths:\n\nThe main strengths of the paper are as follows:\n- novel bounds on the policy performance difference between two policies in terms of arbitrary functions $\\phi$ which can be instantiated to improve upon existing performance bounds in the literature\n- easy to compute policy updates by simply appealing to simple estimators of the GAE and first-order optimizers\n- theoretical bounds on the per-step improvements on the performance of the policy while simultaneously giving bounds on the cost of the policy\n\n### Weaknesses:\n\nThe main weaknesses of the paper are as follows:\n- the theoretical contributions and policy performance bound can be better explained and positioned in the literature by first giving a high level interpretation of the different functions and instantiations of the theorem and how it leads into their algorithm\n\n### Questions:\n\n- What is the meaning of the phrase 'compact' as used when comparing the different bounds?\n- The bias-variance trade-off of the GAE versus other estimators for policy gradients is never-fully described.  As this serves as a main point to the algorithm, section 2.1 is a good section to sell GAE as an approach in comparison to the related literature instead of just repeating the same phrase relating bias and variance with no explanation in the paper.  Could you expand more on this trade-off versus other approach?\n- What is the difference between lower case $c$ and capital $C$? Later in the paper you mention that an advantage is 'not needing to evaluate cost function' - but that seems also true with your approach too?\n- One complaint for existing bounds is it requires the data to come from a fixed policy $\\pi_\\theta$.  Could you expand more on this and how your approach doesn't additionally have this same issue? (i.e. your GAE must be evaluated under a fixed policy as well)?\n- The Practical Implementation comment feels a bit weird.  Yes - your approach can appeal to first-order optimizers with easy to calculate gradients - however you have no condition which says you are 'actually' maximizing the function which is written down.  In contrast, at least with a convex relaxation they can give explicit convergence guarantees for the optimization procedure. \n- In the related work, the policy updated by \"Lagrangian approach may be infeasible\" is established as a separation of your approach.  However, your policy projection step also lagrangianizes the constraint and the given bound on the cost of the policy is always increasing, so do you not run into the same issue?",
            "summary_of_the_review": "The paper focuses on developing novel generalized policy performance differences in terms of arbitrarily chosen functions $\\phi$ for calculating the TD errors.  With instantiating the $\\phi$ with different terms the authors get an upper bound on the performance difference and a lower bound on the cost difference, which leads naturally to a simple safe policy performance improvement step for designing an algorithm (which the authors coin as CUP).  While the approach is simple and intuitive given the generalized bounds, the authors provide concrete theoretical performance improvements per step.  Moreover, they include robust experimental results.  The paper is easy to follow (with the caveat of some slight notation issues and confusing sections) and helps illustrate their theoretical and experimental results.\n\nEDIT: I appreciate the authors for providing detailed feedback with respect to some of the points raised in my reviews.  In particular, my major complains were with respect to the clarity of a couple aspects, including:\n1. Detailed comments on the variance reduction from using GAE estimators versus typical Advantage functions\n2. Discussion on \"compactness\" of the bounds vs the others in the literature\n3. Comment on data from their estimate coming from fixed policy versus on policy.\nAll of these the authors have addressed and will be included in the updated version.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}