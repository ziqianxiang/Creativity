{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose BLUnet, which aims to encode DNNs as look-up tables (LUTs) in FPGAs or ASICs. Unlike other DNN accelerators, BLUnet does not use programmable weights - instead LUTs are directly programed with the functions that map a layer's inputs to its outputs. BLUnet's main innovation is to time-multiplex different bit slices of the input. This avoids having to flatten all the activation bits leading to huge fanins and area costs. The evaluation focuses on energy usage and latency. In both cases BLUnet achieves huge improvements over a baseline consisting of serial FP32 adders. The authors demonstrate a small MNIST network mapped using BLUnet onto a real FPGA and achieve energy and latency improvements over spiking DNN accelerators and other LUT-based accelerators.",
            "main_review": "I think this is an interesting direction of research, and the main idea is novel and interesting. However, many of the comparisons in the paper are strawmans (i.e. comparing against a poor baseline), and there are many issues and drawbacks which the paper does not mention or address.\n\nStrawman comparison issues:\n - The comparison in Fig 1 and Table 1 is a strawman. If x is 1-bit then we should be able to quantize the weights down to very low precision (2-bit?) and still maintain exact functionality. There's no reason to keep the weights in FP32 and the comparison to a FP32 serial adder is not appropriate. At the very least you should compare against a fixed-point adder. Same concern with the result table (Table 2).\n - In the real FPGA comparison (Table 4), TrueNorth, Shenjing, and Tianjic are spiking neuromorphic accelerators which are highly experimental. It would be nice to see the comparison against conventional DNN accelerators.\n - In Table 4, Umuroglu et al is another LUT-based accelerator, which doesn't seem to use bit serialization to address fan-in. I would expect BLUnet to be more energy-efficient, easier to route, and achieve higher Fmax, but suffer in terms of latency due to serialization. But this is not reflected in the results. Could you explain why this is? I would also like to know the exact FPGA devices you and Umuroglu used, could this be a factor in the comparison?\n\nIssues that need discussion:\n - Because the weights are encoded in the LUT functions, it seems that BLUnet has to fit an entire network onto a chip. If we can only fit a portion of the network, we'd have to program a new bitstream each time we want to execute a different portion of the network. Figure 3 shows that the LUT usage grows with N^4, where N is the number of neuron inputs. BLUnet does not seem like it would scale to modern DNN sizes. This is fine, but I would like to see an acknowledgement/analysis of the scalability issue.\n - Are there registers in the LUT tree design shown in Figure 3? I expect the design to be pipelined to maximize throughput. It would also be helpful to know the critical path and throughput bottleneck of the LUT tree design.\n - I don't understand the latency numbers in Table 2. Is this the total time to process the network? What is the target frequency and cycle count? This is not the way hardware papers compare performance. Typically throughput is the more important metric since DNN accelerators are pipelined.\n - Energy and latency in Table 2 appears to be estimated without considering placement and routing. Since you have a real prototype, could you compare the estimates to the real measured numbers to see if they are accurate? The estimates are used with large scale networks (VGG, ResNets) while the prototype is for a tiny MNIST network. Could problems with P&R, register placement, etc arise when trying to scale to those larger networks?",
            "summary_of_the_review": "The paper studies how to directly map DNNs to LUTs, which is an interesting direction of research. Their main idea to use bit serialization to solve fan-in issues seems novel and useful. However, the techniques also have many limitations and drawbacks that the paper does not mention or discuss. The experimental results have serious issues with strawman comparisons, and relies on estimates that do not seem to account for placement and routing.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to implement the computations of deep neural networks using lookup tables. To mitigate the overhead of lookup tables and improve the scalability of their solution, they suggest to use bit-serial technique for performing the computations.",
            "main_review": "The idea of bit-serial computation and lookup tables have been extensively studied (see the additional references) and it is not clear to me what the main contributions of this work are. The scalability of the solution is questionable. Basically, for the comparison the authors assume that they can use `maximum parallelization`, however in real scenario that is not a practical assumption. Some of the important works for baseline comparisons are missing.\n\n**Questions**\n\n(1) Can you clarify the contributions of your work over other bit-serial computations?\n\n- [Bit fusion: Bit-level dynamically composable architecture for accelerating deep neural network](https://ieeexplore.ieee.org/abstract/document/8416871)\n- [Stripes: Bit-serial deep neural network computing](https://ieeexplore.ieee.org/abstract/document/7783722)\n- [BiQGEMM: Matrix Multiplication with Lookup Table For Binary-Coding-based Quantized DNNs](https://arxiv.org/pdf/2005.09904.pdf)\n\n(2) How scalable your solution is? For example, for ResNet-150 and BERT-style models how the results would change?\n\n(3) A fair comparison with prior work would be to compare under same area budget. What is the performance/energy numbers under same area budget?\n",
            "summary_of_the_review": "The proposed techniques (quantized training, using lookup tables, and bit-serial computation) have already been explored and studied extensively in the community and the contributions are not clear. The comparison with some important baselines are not covered. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper aims at eliminating multiplication operations from NN inference with LUT(look-up table)-based accumulation. While the weight precision is full-precision (32bit), the activations are quantized into 6~8 bits. A small LUT stores possible cases of six FP32 weights (i.e. 2^6) and a lot of small LUTs are hierarchically structured to calculate outputs of a layer. This paper also includes experimental results w/o accuracy degradation on ImageNet and CIFAR-10. Moreover, its simulated results show significantly reduced energy consumption.",
            "main_review": "### Strength\nThis paper proposes a new way for efficient inference using LUT with full-precision weights and binary activations while most model compression techniques first reduce the precision of weights. This reviewer agrees with LUT-based inference may be a great game-changer for efficient deep learning.\n\n### Weakness\nThis reviewer does not think describing H/W issues in detail is a proper scope for this conference, and the algorithm side should be evaluated first. It is hard to discuss H/W issues, metrics, or implementations within 9 pages. After reading this paper, this reviewer is very confusing about the LUT-based inference and compressed models. Maybe it is because the scopes of this paper are so wide and mixed. In that sense, this reviewer’s concerns and questions can be listed as below: \n- On the algorithm side, discussing 32bit weights and x-bit activations is needed. Most papers and techniques have put much effort into weight quantization than activation quantization because the precision of activations is more sensitive due to softmax, residual connection, or generative layers. So, more detailed arguments and experimental results seem to be needed for the quantization format. This reviewer thinks contents on the algorithm side are more important than h/w or implementation side for this conference. In addition, it is hard to the quantization format of activation. For 2-bit activation quantization, are the quantized values only 0,1,2, and 3? How about scaling factor (maybe beta?)? How about batch normalization? Descriptions about quantized model architectures in this paper are so insufficient. \n- This reviewer has a question about the pre-computation cost for LUTs. There is only one mention for the pre-calculation, it is straightforward. But, weights are changed over and over. So pre-calculation steps should be performed every time. To reduce the portion of pre-computation costs, reusing the pre-computed values is crucial. With model architecture, this reviewer thinks it should be clearly described.\n- Throughout this paper, the comparison target (model and h/w) is full-precision (FP32). Since there are many studies about compression and light-weight architecture w/o degradation. For instance, w8a8 quantized models are fully accelerated by H/W (including general-purpose CPU/GPU) w/o performance degradation. And FP16 training/inference schemes are widely used for plenty of applications. In Table 1/2, there exist FP32 results and h/w descriptions for comparison. This reviewer thinks other schemes should be added for a fair comparison. Indeed, as this paper suggests new h/w implementation using ASIC/FPGA, many studies on low-precision computation can exist using new h/w designs.\n- The outperformed results about accuracy in Table 2/3 are not surprised for this reviewer because full-precision weights and 6~8bit activations can be not harmful to model accuracy. It could also lead to higher generalization effects. Longer fine-tuning from the pre-trained model could also make an effect on the accuracy. For Table 3, the precisions of the comparable models are different from this paper’s suggestion. So, it is not fair. \n- For Table 2/3, energy and latency estimation seem to be naive. It is hard to evaluate this part because we don’t know the details of assumptions and h/w designs. To simulate and estimate the performance of h/w, there should be many assumptions for the designs. But, this paper has only numbers about that. For example, Does the LUT-6 model have no MAC operations? How about the first and last layers? How about Batchnorm? Readers cannot know the implementation details of Table 3. \n- For Table 4, unfortunately, the real result on only two-layer MLP is not sufficient to evaluate this paper. In order to know the applicability of this method to current DNNs, the target network should be complicated. The two-layer MLP is too small and simple.\n\n### Minor\n- Figures 3 and 4 are so confusing for this reviewer. For example, hidden layers are colored green and the first stages of LUTs are also colored green. The final neuron and other stages of LUTs are colored blue.\n- Page 2: DNN were studied → DNN was studied",
            "summary_of_the_review": "Unfortunately, this reviewer thinks this paper should be refined in the aspect of algorithm, model architecture, and so on. The current contents are so wide and insufficient to prove the proposed work. But, this reviewer is fully open to other reviewers' opinions.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a method that can reduce computational complexity of deep neural networks (DNNs). More precisely, multiply-accumulate operations in DNNs are replaced with lookup table (LUT) operations (i.e., indexing and addition operations). LUT operations on multi-bit activations are performed by serializing their representative bits into binary time series inputs, which removes the need for multiplications. Using the proposed computing method, operations of different DNN architectures can be performed efficiently in terms of latency and energy consumption while yielding the same accuracy of the full-precision model. ",
            "main_review": "Strengths: \n1) The paper is easy to read even for general reader. The contributions and goals of the paper are clear and well-explained.\n2) Extensive experimental results (on both accuracy performance and hardware performance) and their comprehensive discussions are another strength of this paper.\n Weaknesses:\n1) I believe the main weakness of this paper is its theoretical contents. In my opinion, they are not justified and it's not clear why the proposed computing method should work. The main goal of this paper is to perform Eq. (2), which is the main computation of DNNs, using LUT operations. This equation is followed by several more to finally rewrite it as Eq. (9). However, no justification is provided to support Eq. (2-9). My main question is why they hold true? Or more specifically, why is Eq. (2) equal to Eq. (9)? Since the proof is missing (or at least it's not obvious to me), I cannot verify the correctness of the proposed method. Moreover, if there is any error associate to the equality (or approximation) of Eq. (2) and Eq. (9), this error should be explained and the contributing factors should be determined. Finally, what is M in Eq. (5) and (9)?\n2) My second concern is the memory usage of the proposed method. Let's consider the first example provided in the paper (i.e., Fig. 1), where we have six binary inputs (or activations), 6 full-precision weights and a binary output. In this case, if we want to use LUT, we need to consider all possible combinations between inputs and weights. In other words, the memory footprint for weights are increased by a factor of ~11x (2^6/6). Such an increase in memory size undermines the whole point of energy reduction. Please elaborate more on your implementation. Ideally, it would be great to have the detailed architecture of the proposed LUT. Moreover, is beta a constant value for the whole layer or does it vary for every neuron?\n",
            "summary_of_the_review": "This paper suffers from unsupported equations and also the lack of discussion on the memory footprint of the proposed implementation in my opinion. Without justifying the provided equations, it's hard to verify the correctness of the proposed method. Moreover, no discussion is provided on the memory footprint overhead of the proposed method. Energy consumption of DNNs is dominated by the memory accessed; therefore, any increase in size of DNNs will result in larger number of memory accesses and consequently larger energy consumption.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}