{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This works solves the knowledge graph completion problem by tensor completion. Existing works suffers from the overfitting problem, and this work propsoes to solve this issue by using low rank tensor regularization.\n\n\n",
            "main_review": "This work has several issues:\n1. Generally, the contributions of this work are a bit weak. The main contribution is to introduce a low-rank tensor regulrization which has been well studied and has many applications. \n2. The used low n-rank regulrization is not new. But it motivation is not very clear. Why the corresponding tensor is of low n-rank? Why not low CP rank? This observtion should be from the structre of data. The authors need to verify the low rank property of data.\n3. After introducing the low n-rank regulrization and its convex surrogate, sum of nuclear norm, the authors further introduce an upper bound of the sum of the nuclear norms and uses it in the models. This is not a reasonable relaxation. It seems that the upper bound is not tight. It is not necessary to introduce the upper bound since the sum of nuclear norm regularization is solvable. The used upper bound is a loose approximation of the tensor n-rank. It cannot guarantee the low-rankness of the obtained solution.\n4. There has no theoretical support of the proposed model.",
            "summary_of_the_review": "The motiation of low rank tensor regularization may be reasonable. But the finally used upper bound function is not a tight approximation of tensor n-rank. The novelty is limited and there has no theoretical support. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies a general tensor decomposition based (TDB) model for knowledge graph completion (KGC). The proposed model is based on a Tucker decomposition (Eq. 1, 2), which can be regarded as an extension of many previous TDB models. Then, to avoid overfitting, the authors proposed a regularization (Eq. 4), which is an upper bound of the tensor N-rank. Experimental results on real data show the proposed model outperforms many TDB models. Also, the authors conducted experiments to show the advantage of using the proposed norm.\n",
            "main_review": "Strength\n\n-The proposed model and the regularization term is general, which include many previous frameworks.\n-The authors demonstrate the proposed model can handle symmetric, anti-symmetric and inverse rules.\n -The proposed regularization is based on Frobenius norm, hence is computationally friendly.\n\nWeakness\n\n-The motivation of the regularization in Eq 3 is not very clear to me. This regularization consists of two upper bounds of the nuclear norm. What is the advantage of summing  these two bounds? How about using only one bounds, e.g., setting \\lambda_1 = \\lambda_4 = 0 or \\lambda_2 = \\lambda_3 = 0?\n-As the authors claimed, the regularizer in the DURA model is a  special case of this paper. Why did not the authors compare with DURA in experiments?\n",
            "summary_of_the_review": "The new regularization term for low-rankness of tensor is novel. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work studies the knowledge graph completion problem through the lens of\nthird-order binary tensor completion. The authors present a general framework\nbased on a weighted Tucker decomposition that unifies several existing methods.\nThey ensure the completed tensor is \"low rank\" to prevent overfitting, where\n\"low rank\" refers to the vector of the ranks of each unfolding of the completed\ntensor. They show that it is both theoretically and computationally reasonable\nto upper bound this value by the sum of nuclear norms over all the unfoldings\nsince this is the tightest convex relaxation to matrix rank. Last, the authors\ndemonstrate the effectiveness of this idea experimentally and compare the\nresults to all of the key tensor-based methods this approach generalizes.",
            "main_review": "**Strengths.**\nThis paper is well-organized and clearly presents how this work fits into the\nexisting literature. The idea of casting all previous tensor-based works as\nTucker decompositions as Tucker decompositions subject to core tensor\nconstraints is relatively straightforward but convenient nonetheless. The use of\nan upper bound for the sum of nuclear norms of each unfolding is nice because\nit (1) encourages a \"low-rank\" decomposition and (2) can be written as the sum\nof several (easily computable) Frobenius norms.\n\n**Weaknesses.**\nThe main weakness of this work is its lack of technical depth. The paper would\nbenefit from a couple sentences describing the proof instead of immediately\ndeferring to the appendix (though I did read it). The experiments build on those\nin related works and draw nice comparisons. This section could benefit from\nmore discussion and additional analysis (e.g., comparing running times,\nconvergence rates, etc.). Less important: It would be valuable to define the\nMRR and Hits@N metrics in the appendix to make the paper self-contained.\n\n**Typos / suggestions.**\n- [ 1] Minor: KGC is only defined in the abstract, but is then used in the\n  introduction.\n- [ 2] Typo: tensor n-rank is defined --> tensor $n$-rank is defined\n- [ 2] Typo: n-rank regularization (TNRR) --> $n$-rank regularization (TNRR)\n- [ 3] Suggestion: consider using bold script letters for tensors (like in\n  Tensor Decompositions and Applications by Kolda and Bader, since this is sort\n  of the convention).\n- [ 4] Suggestion: Consider using inner product symbol (i.e., \"\\langle\") instead\n  of \"<\" and \">\". It makes it a little easier to read due to better spacing.\n- [ 4] Missing word: which can be [a] parameter tensor or predetermined...\n- [ 4] Typo: come from two part --> come from two parts\n- [ 4] Suggestion: What does \"the computational complexity\" refer to? It is not\n  stated explicitly.\n- [ 5] Typo: the relations should be \"symmetric\" and \"anti-symmetric\"\n- [ 5] Suggestion: Using $r$ for relation could be confusing given that $\\textbf{r}$\n  is a vector.\n- [ 5] Typo: n-rank --> $n$-rank\n- [ 5] Typo: :low- rank\" --> \"low-rank\"\n- [ 8] Typo (word should be capitalized): ... logical rules. We analyze ...",
            "summary_of_the_review": "This paper makes limited technical contributions from a theory point of view.\nThe experiments are good, but they are not enough to carry the paper. I *do not*\nthink this work meets the bar for acceptance as written, but it has potential,\nespecially if it provides more novel and comprehensive experiments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}