{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper explores using language models (LM) to incorporate future information into simultaneous translation tasks. Their motivation comes from that human translators can anticipate the immediate future words using linguistic information and domain knowledge. They focus on the monotonic attention, a kind of adaptive policies, which performs read/write decisions based on the partial source and target sequences and the corresponding computed monotonic energy. An external LM is used to first generate a pseudo target token based on the emitted target tokens, and then the pseudo target token is also counted when computing monotonic energy. The experiments on MUST-C En-De and En-Fr datasets show that the LM can help on improve the quality-latency tradeoff, by increasing more than 1 BLEU in similar latency. It also shows that large-scale multilingual language models (XLM-RoBERTa) cannot further improve compared to small language models trained with in-domain data. Although external language models might introduce extra computational aware latency, their model with small language models still have lower computational aware average latency than baselines.\n\n",
            "main_review": "Strengths:\n1.\tThe motivation of the work is very clear and the applied method is described well.\n2.\tThe method with small language models show great improvement compared to the baselines.\n\nWeaknesses:\n1.\tThey only compare with the MMA baselines, and do not compare with other policies like wait-k or chunk-based policies\n2.\tThey fail to give persuasive analysis into their experimental results. For example, why large language models like XLM-RoBERTa cannot further improve the performance compared to small language models? How the language models help on improving quality (maybe a case can be shown)? I cannot find the answers through the paper.\n",
            "summary_of_the_review": "The paper propose an easy way to incorporate future information with language models into MMA-based simultaneous translation. The experiment shows considerable improvement derived from their method. But there are some concerns on their experiment design and analysis.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper used an external language model to aid monotonic attention to improve the read/write decisions in simultaneous neural machine translation. Specifically, the external language model was used to predict a plausible target word condition on the generated target fragment at each time step. The plausible target word was as an additional input to the MMA model to perform the read/write decisions. Authors hoped that the external language model can provide more sufficient information (e.g., linguistic information and domain knowledge) for the monotonic attention to better make read/write decisions. The proposed model was evaluated on the MuST-C English-German and English-French speech-to-text translation tasks.",
            "main_review": "Strengths: \n\n1. The proposed approach gained improvement over the baseline systems in terms of BLEU scores.\n\nWeaknesses:\n\n1. The motivation is debatable to some extent. The plausible target word is only a current translation option instead of future information for predicting the final target word. Therefore, the proposed approach is more like a refinement of each target word to be generated.  \n\n2. There lack of some necessary experiments to verify key parts of the proposed approach, for example, sufficient information, read/write decisions, linguistic information, and domain knowledge.\n\nQuestions:\n\n1. What is the expected sufficient information?\n\n2. Which does one of LM itself and better read/write decisions this improvement come from?\n\n3. What about if the distributions of LM and SNMT are directly ensembled in the output layer？\n\n4. What are the differences between the plausible target words and final target translation?\n\n5. How to evaluate the proposed method to make better read/write decisions than the original SNMT?\n\n6. Figure. 6 let me confused. There is an additional LM to predict the plausible target word. This means that the proposed approach should be more computation time than the baseline system. ",
            "summary_of_the_review": "There is a mismatch between the claimed problem and the proposed method, and a lack of some necessary experiments to verify key parts of the proposed approach",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper reports on efforts to use LM predictions for simultaneous neural machine translation (SNMT). SNMT systems begin translating before the complete source sentence is available. Most approaches model the trade-off between latency and quality by predicting read/write decisions at each time step; \"read\" corresponds to waiting for the next source token, \"write\" to predicting the next target token. The core idea of this paper is to use a target-side LM to predict a plausible single token extension of the current translation prefix, and use it for a more informed read/write decision. Fig. 4 demonstrates that using LMs in that way can improve the Latency/BLEU efficiency curve.",
            "main_review": "The latency/BLEU improvements reported in Fig. 4 might have the potential to justify the increase in system complexity. However, I have concerns about (a) clarity and (b) what exactly is causing the gains.\n\n(a) There are a number of issues with the formalisms. Some examples:\n- Sec. 2.1: x_{<j} \\in x is not the correct notation - x_{<j} is a subset/sequence, not an element.\n- Eq. 2: I believe the A(h_j) notation is not entirely correct - s_i does not (only) depend on the j-th source position: no j on the left hand side.\n- Sec. 2.1: \"Instead of hard alignment of c_i=h_j\": for which i and j? presumably if i=j\n- h refers to the h-th attention head and h_j to the j-th encoder state, sometimes even in the same equation (Eq. 7)\n- What is k in d_k (Eq. 7,9)? d is not indexed with k anymore in Eq. 14\n- |x| in Eq. 10 and 11 should be bold\n\nMost of these points are rather minor, but together they do interfere with clarity. More importantly, I feel that not all equations - even if correct - contribute to a better understanding of the approach. For example, Eq. 7-11 seem to me like (a small variant of) normal scaled dot-product attention, but the important aspect - the fact that the training loss incorporates latency - is not shown but merely mentioned in the text. Explaining the exact meaning of the lambda factors instead of just referring to the original work or formalising what \"monotonic\" means in terms of e_i,j would be a more helpful use of the available space.\nSec. 2.2.1 could also be clearer. Is this only necessary for XLM or also for SLM? A 1:1 correspondence between MT and LM tokens would make reading the results clearer, and at least for SLM it seems to be an easy fix. Also, how do you handle token boundary mismatches in situations when the NMT token y_{i-1} ends \"in the middle\" of an LM token (example: y_{i-1}=foo as NMT splits \"foobar\" into \"foo\" and \"bar\", but the LM normally uses a single \"foobar\" token)?\n\n(b) It is important to note that the predicted future token is on the *target* side: the LM is not used to artificially extend the source context by one token. Furthermore, the additional target side token is just used to inform the read/write decision, not for the translation itself. Both design decisions seemed surprising/counter-intuitive to me, so I would have hoped for a more careful analysis of how the gains come about. Compared to the MMA baseline I see multiple factors:\n\n- using a language model per se (trained with an LM objective)\n- the use of more training data\n- the prediction of an additional target token\n\nThe use of additional (LM) training data alone makes the results in Fig. 4 a bit questionable without further analyses. It would be interesting to either test an LM trained on the MT training set only, or compare with the alternative approaches mentioned in the related work that use LMs in NMT. Another ablation study could be an oracle experiment in which \\hat{y}_i=y_i is always the correct token.",
            "summary_of_the_review": "If solid the reported gains would be sufficient enough for publication, but the underlying reasons for the gains could have been analyzed more thoroughly, and the approach described more clearly.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}