{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose to systematically test the robustness of perceptual similarity metrics using various adversarial attacks. There are multiple contributions: \n\n1. adapting adversarial attacks to perceptual similarity metrics\n2. a new distortion-based attack (FlowAdv)\n3. evaluation of attack effectiveness and robustness of perceptual similarity metrics\n",
            "main_review": "### Strengths\n\n* Clear and well-written\n* Many adversarial attacks are compared\n* Many perceptual similarity metrics are compared\n* A new attack is proposed\n\n### Weaknesses\n\n* The imperceptible aspect of the attacks is not systematically assessed by experiment with human participants\n* (minor) Figure and table font size should be similar to the main text font size\n* (minor) Relevant scores in the tables should be highlighted\n\n### Detailed comments\n\nPage 4: I didn't understand why s_other / (s_other + s_prey) is approximately 1 if the metric correctly predicts the human ranking. It has to be compared to 0.5 as stated below. I understand that if it is close to 1 it means that s_prey is approximately 0 and therfore that I_prey is the most similar to I_ref but being above 0.5 is sufficient, right ?\n\nIs it possible to control the intensity of the attack ? It would then be interesting to use these attacks to progressively distort an image and systematically test it in a psychometric experiment on human participant. In other words, it would open the door to finer grained BAPPS-type datasets. \n\nFigure 6 would benefit from having the larger size example for the reader the make sense of the distortion generated by the attack. If the instensity of the attack can be controlled, it would be great to have a large size example.\n",
            "summary_of_the_review": "This is a well-conducted work with clear contributions and an extended empirical analysis.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper explores the robustness of perceptual image similarity measures to small adversarial perturbations. The authors focus on the BAPPS dataset and propose attacks to flip the ranking of similarities assigned by various metrics to images in the dataset. The attacks considered are FGSM, PGD, one-pixel attack, and a stAdv-type attack. The authors find that most similarity metrics are vulnerable to attack at least some of the time. They further explore transferring attacks on one similarity metric to another in a black-box setting and find that the metrics are vulnerable in this case as well.",
            "main_review": "The writing in this paper is mostly clear; I found the methodology for the most part easy to follow. The authors also did a nice job of highlighting related work.\n\nIn terms of the experiments, I appreciated the extensive number of similarity metrics considered and the variety of attacks. This allows readers to get an overview of the robustness of many of the perceptual metrics from the literature. \n\nI think there are also a few major weaknesses of the arguments presented. For some of these weaknesses, I will compare to Kettunen et al., which is the most closely related work; they investigated the adversarial robustness of LPIPS and introduced E-LPIPS.\n\nFirst, I think the authors could do a better job of motivating why the robustness of perceptual similarity metrics is important, beyond simply arguing that they are commonly used. For instance, what downstream applications are affected by the lack of robustness of these metrics? Are these results relevant to the work referenced (Laidlaw et al. and Wang et al.) that use similarity metrics to bound adversarial attacks? Ideally, the relevance of the results in this work could be motivated by experiments showing negative affects on downstream applications.\n\nSecond—and I think this is the major methodological shortcoming of this work—it is unclear if the attacks proposed actually preserve the similarity ranking of the images in BAPPS. The perturbations in the BAPPS dataset are mostly small and nearly imperceptible; they are comparable in magnitude to the adversarial perturbations the authors propose. Thus, I would not be surprised if these perturbations altered the ground truth human judgements. To fix this issue, I think the authors either should do a perceptual study with human subjects to show that the humans judgements are not flipped by their attacks, or they should demonstrate the attacks on a dataset where their adversarial perturbations are much smaller compared to the dataset perturbations. In comparison, I find the results of attack (A1) in Kettunen et al. much more convincing (e.g., in Figure 1) because they show that an image of a completely different object can be made to be very close to a source image under the LPIPS distance. \n\nI think there are also some statements in the paper that could be clarified. The authors repeatedly compare \"geometric distortions\" to \"advanced adversarial perturbations\" (e.g., in the introduction and also in the related work). I found this a bit confusing, so maybe some extra explanation or an example would help. Also, the authors argue that \"no study has been reported with more advanced adversarial perturbations... we seek to address this critical open question.\" However, it seems like Kettunen et al. investigated exactly this question, so I think the claim is somewhat misleading. Finally, I thought the \"imperceptibility\" section starting on the bottom of page 7 was not clear or well-motivated. The authors claim the attacks are imperceptible by using the RMSE and PSNR between images, but aren't these similarity metrics themselves? Why should we expect them to be robust when other metrics are not? Furthermore, is the RMSE any different than the L2 distance? It would be helpful to have further definitions and motivation of these metrics.\n\nOne more potential weakness is the motivation of the FlowAdv attack. I don't understand why it's necessary to use a CNN to produce the flow vectors instead of just following what stAdv does and optimizing the flow vectors directly. It would be helpful if the authors could better motivate the need to complicate this attack, for instance by showing that it is superior to other flow-based attacks like stAdv in some way. While the authors present FlowAdv as an additional contribution, I think it actually distracts from the main results and could make it more difficult to compare to other work using stAdv or similar attacks.\n\nOverall, I think this is a promising work; it would be valuable to the community to have a comprehensive comparison of the adversarial robustness of different perceptual similarity metrics. However, I think that the paper needs better motivation of the importance of attacks on similarity metrics and of the experimental setup, where it is unclear if the attacks are actually preserving the ground truth ranking. Of course, I am open to changing my score based on the authors' responses.",
            "summary_of_the_review": "Overall, I think this is a promising work; it would be valuable to the community to have a comprehensive comparison of the adversarial robustness of different perceptual similarity metrics. However, I think that the paper needs better motivation of the importance of attacks on similarity metrics and of the experimental setup, where it is unclear if the attacks are actually preserving the ground truth ranking.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This work aims to systematically examine the robustness of these metrics to imperceptible adversarial perturbations. They first show that all metrics are susceptible to perturbations generated via common adversarial attacks. Then they propose a new attack method, called FlowAdv, to attack the widely adopted LPIPS metric.\n",
            "main_review": "Strengths: \n1. The author shows a relatively comprehensive literature review in section 2.\n2. The proposed attack method is technically sound to me.\n\nWeakness:\n1. Motivation of this work: a) It is interesting to show perceptual similarity metrics can be easily fooled. However, it is not significantly novel since the popular metrics are often based on deep neural networks. b) When do we need a strong attack method like FlowAdv? The motivation for such an attack method is not clear to me.\n\n2. It is well known, the adversarial examples created with many iterations can be overfitted to the source model [1]. In this work, the author combines FlowAdv with PGD(20) to improve the transferability of the created adversarial examples. How sensitive the iteration is in the proposed combination? What about FlowAdv + PGD(10), FlowAdv + PGD(40)\n\n3. There is only visualization of a single image, More visualization with high-resolution images should be provided (e.g. in Appendix) since this work focuses on the perceptual metrics.\n\n4. As stated by the authors, similarity metrics are also used in optimizing, constraining, and evaluating adversarial attacks. The perceptual metrics are closely related to the adversarial robustness itself. It would be interesting to discuss insights revealed by this study on the adversarial vulnerability problem. E.g., do the conclusions in Laidlaw et al. (2020) still hold?\n\n[1] Xie, Cihang, et al. \"Improving transferability of adversarial examples with input diversity.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.",
            "summary_of_the_review": "\nGiven the concerns listed in the weakness above, I tend to reject this paper. I am happy to raise my rating if the concerns are well addressed.\n\nSome minor issues for authors to improve the presentation quality:\nThe citation style is not consistent across the paper. some with parenthesis while others not, e.g. Dolatabadi et al. (2020), (Rezende & Mohamed, 2015)\n\nFig 1 is a nice illustration. However, the readers have to spend time to figure out the difference between I_0 and I_ref. In figure1, please mark/tell the difference somewhere.\n\nIn the introduction, the author states that ‘analysis of more advanced adversarial perturbations’ has not received considerable attention. How does this work focus on this point? More elaboration on this will improve the readability.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper observes the robustness of perceptual similarity metrics against imperceptible perturbations.  They use several adversarial attacks (FGSM, PGD, and one pixel attack) and introduce a spatial attack FlowAdv to attack perceptual metrics.  They find that these attacks can cause perceptual metrics to flip the distance ranking of 2 images and FlowAdv and PGD attacks can transfer to other metrics.",
            "main_review": "Strengths:\n- Interesting novel direction: the authors explore the direction of adversarial robustness of perceptual similarity metrics which has not been explored by prior works\n- Related works is comprehensive\n\nWeaknesses:\n- Experimental design choices unclear:  There are some portions of experimental design that seem unclear to me.  For instance, the authors define the loss function they use for FGSM and PGD as $J(\\theta) = (\\frac{s_{other}}{s_{other} + s_{prey}} - 1)^2$.  Why use this formulation instead of a simpler loss function like $J(\\theta) = -s_{prey}$?  Is there a difference in performance?  Another part that I found confusing was the use of RMSE to measure imperceptibility.  I'm not too sure what is meant by taking the RMSE between images; is that not just a constant multiple of L2 distance, which is a metric that is being attacked?  Since FGSM and PGD are $\\ell_{\\infty}$ constrained, $\\epsilon$ itself should be a measure of imperceptibility, so I'm not convinced measuring RMSE is necessary.\n- Examples of adversarial images make the significance questionable: The paper only provides a few examples of $I_{ref}, I_{other}, I_{prey}, I_{adv}$ (namely Figure 6).  However, this Figure also makes the significance questionable.  In these examples, $I_{ref}, I_{other}, I_{prey}$ are already very similar so flipping the ranking between does not seem very important (at least I can't think of applications which would require robustness at such a fine grained scale).  I am wondering, have the authors tested with $I_{other}, I_{prey}$ very different and $I_{prey}$ is some noisy version of $I_{ref}$.  In that case, do perceptual metrics get fooled by these attacks?\n- Additional experiments and measurements for context: The authors experiment only with adversarial perturbations.  Instead of doing PGD/FGSM, what is the flip rate when perturbed with a random perturbation of that size?  This would measure the sensitivity of these metrics to small perturbations in general and address whether these metrics are generally bad when it comes to those perturbations.  Additionally, I was wondering if the authors could measures of the distance between $I_{other}, I_{prey}$ and $I_{prey}, I_{adv}$ for images in Figure 6 for reference.",
            "summary_of_the_review": "I vote to reject this paper.  My main concerns are that it looks like the authors tested with $I_{other}$ and $I_{prey}$ initially very close which limits the significance of the results and that the authors only experiment with adversarial perturbations and its difficult to contextualize these results since there is no comparison to randomized perturbations.  There are also some confusing aspects in the experimental design.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}