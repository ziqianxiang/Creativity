{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a perturbation based fingerprinting method to protect the multi-exit DNN models, by using perturbations to modify the inference time of the samples.",
            "main_review": "The idea is interesting. However, the paper suffers from insufficient motivations, incomplete literature review, insufficient elaborations on the methodology, insufficient experiments, etc. In general, I am afraid this paper is one major revision away from being accepted.\n\nComments:\n1. The paper may benefit from revising its typos, grammars and informal presentations.\n2. I am afraid the paper has not given enough emphasis on the importance and applications/popularities of the multi-exit models. I think the model compression methods or lightweight models are more widely adopted when people has limited computing powers.\n3. In Sec. 2.2, I am afraid the paper has not reviewed the DNN watermarking methods comprehensively.\n4. In Sec. 3.3, I am wondering why the CW optimization technique is used to generate the perturbations? Why not consider other perturbation generation methods? I don't quite believe that there is only one method which can be used to generate perturbations with little transferability.\n5. For (2), I am actually confused that why the generated verification samples should consider the stealthiness? I suppose they will only be used when a verification is needed?\n6. In the experiments, I am wondering why the maximum RAD is chosen to 15%? Is there any intuitive explanations?\n7. For the model pruning rate in the experiments, since the paper only use up to 0.4 rate, I am wondering what will happen if the pruning rate is higher? maybe 0.6-0.8?\n8. For the adversarial modification part in Sec. 4, I am wondering how exactly the adversarial training is performed? Why only PGD with the perturbation constraint 8/255 is used for adversarial training? What will happen if the adversarial training is performed with random noise? or other adversarial attack methods? Will they affect the performance of the proposed method?\n9. For Sec. 3.3, since the proposed method still uses perturbations to change the exit point of the benign input sample. The added perturbation may still affect the performance of the benign model, though the performance drop may not be as large as common adversarial perturbations.\n10. For the number of epochs selected for the finetuning and adversarial training, I am wondering why this specific number of epochs are selected? Is there any standard protocol?\n11. For Table 1, it seems that the pre-defined thresholds are quite different for different datasets, even for the same target model. If the pre-defined thresholds cannot be fixed for a target model, I am wondering how can this verification be effectively performed in real situations?\n12. For Table 2, I am wondering why the result of Resnet-56+C10+Indep. is relatively similar to the result of Resnet-56+C10+Stolen? Is there any explanations?\n13. For Sec. 4.4, I don't actually think the visual similarity can be accurately measured by L-2 scores. The paper may better use the metrics which consider human perception.",
            "summary_of_the_review": "The idea is interesting. However, the paper suffers from insufficient motivations, incomplete literature review, insufficient elaborations on the methodology, insufficient experiments, etc. In general, I am afraid this paper is one major revision away from being accepted.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a fingerprinting approach for multi-exit DNN models, and the paper is well written, though there are some typos etc.\nThe empirical results show that the proposed method is effective and beats one baseline method.",
            "main_review": "Overall, I think the paper is interesting while the technical novelty is a bit limited.\nIn particular, the paper tries to optimize a pattern for instance x such that the manipulated instance won't exit in the middle, which can therefore be viewed as a fingerprint. However, this optimization is straightforward without any guarantees, and the authors don't compare it with other state-of-the-art fingerprint approaches, such as the adversarial examples-based ones given that they are both optimization-based pattern generation approaches.\n\nIn the main optimization objective function (1), there is no decision variable \\delta_x.\n\nEmpirically, it would be interesting to show the effectiveness of the fingerprint approach considering different exits. It may not be the case that \"do not exit early\" is the best optimization/verification criteria for the fingerprint.\n\n\n\n\n",
            "summary_of_the_review": "Overall I think the paper is well written and easy to read, but the technical novelty is a bit limited, and it would be good to compare with other fingerprint baselines to show the effectiveness of the method. In the meantime, it would be interesting to see if some model properties are enforced, such as smoothness, whether such fingerprint is still effective, since in such as case the optimized pattern may not be sensitive any more.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a technique for protecting the intellectual property of a multi-exit model. The method uses a set of slowdown samples crafted to bypass as many internal exits as possible. The paper uses those samples as a proxy to measure the exit behaviors of an illegally deployed multi-exit model. In evaluation, the paper shows some effectiveness of their approach and robustness to modifications available to the adversary.",
            "main_review": "**Strengths:**\n1. The paper studies an interesting problem.\n2. The paper conducts an extensive evaluation.\n\n**Weaknesses:**\n1. The threat model is unclear.\n2. The intuition behind this approach is not clear.\n3. Technically, the method is the same as the prior work.\n4. The extensive evaluation doesn't support the central claims.\n5. The paper is poorly written.\n\n**Detailed review:**\n\nI like the idea of using the inference timing behaviors as an artifact for fingerprinting multi-exit models. However, this paper does not provide convincing answers to their research questions, and it also does not ask several research questions that I would ask. Thus, even if this paper has some potentials, I don't think the paper is ready to be published.\n\nHere, I provide detailed comments, feedback, and suggestions.\n\n[Unclear Threat Model]\n\nI don't understand what is the intellectual property that this paper protects. Is it the multi-exit locations (the locations where a victim attaches internal classifiers), the ICs, or the backbone model? I suggest making this point more clear. I assume that there's no need for a victim to protect the backbone model---it's the same as the existing work. So, it will be either \"the ICs' parameters\" or \"the IC locations.\"\n\nOnce the authors' clear about what they are protecting:\n\nIt would be much better if this clarification comes at the second paragraph in the introduction, as I don't think what this paper protects is the same as what Tramer et al. and Orekondy et al. did. \n\nIt also needs to be articulated more clearly in the threat model section. The questions I would ask and answer there are: what do we protect? what would be standard practice if someone steals a multi-exit model? what happens to what we want to protect under the standard practice?\n\n\n[Unclear Motivation for the Proposed Techniques]\n\nI am also not clear about what intuitions behind the proposed defense. Do the authors think there are some characteristics in multi-exit models preserved, even in adversarial settings? If so, I would put them in a convincing manner (perhaps, with some experiments) in the paper. Right now, I only see the two challenges in protecting the (unclear) IP, but I cannot see why the protection should be done in the proposed way.\n\n\n[Technical Weakness]\n\nThe second concern is that there is no technical contribution.\n\nThe technique proposed in this paper is exactly the same as what the prior work proposed [1]. To bypass the early-exit points, this paper adapts the C&W attack, while [1] adapts the PGD attack for the same objective, which is essentially the same.\n\n[1] Hong et al., A Panda? No, It's Sloth: Slowdown Attacks on Adaptive Multi-exit Neural Network Inference, ICLR 2021.\n\nAlso, the metric used to quantify the protection rate is the same as the prior work, and this is not even well-motivated.\n\nThe objective for crafting adversarial examples is to make these samples exit as late as possible. But, ironically, the paper uses the EEC AUC score for measuring fingerprinting effectiveness. Why do we need to quantify the exit behaviors rather than just compute the adversarial ratio that bypasses all the exits?\n\nAs I pointed out above, clarifying the IP this paper protects will help with motivating and coming up with the objective function different from [1].\n\n[Weak Evaluation]\n\nI am not sure that the robustness study in Sec 4.3 reflects real-world cases. I don't think the adversary will do adversarial training with the stolen multi-exit models. It could lead to many undesirable outcomes, like accuracy degradation or reducing computational savings [1]. The same applies to pruning. Why does the adversary want to be robust against adversarial examples and to increase the efficiency more?\n\nIn Sec 4.4, the same problem persists. Why does the adversary need to make the fingerprinting samples human-imperceptible? For the fingerprinting purpose, the defender is the only one who observes the input. Thus, it's not necessary.\n\nMoreover, it's hard to read the results. What are the bold numbers in some tables? Why do some other tables not have any bold numbers?\n\n\n[Writing Quality]\n\nIt's extremely difficult to read this paper and follow the authors' logic.",
            "summary_of_the_review": "I like the idea itself. However, I am 100% sure the execution shouldn't be done like this paper. There are many flaws (that I mentioned in the main review), and I don't think they can be addressed in a short revision. Thus, I am leaning towards rejecting this paper.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes to generate slowdown attack adversarial examples as a set of fingerprint samples for multi-exit model, so that the ownership can be verified by checking the inference time. The author claims the work to be the first work for IP protection of multi-exit models by fingerprinting, and conducts extensive experiments to verify the uniqueness and robustness of the proposed method.",
            "main_review": "## Strengths\n1. Motivation: the idea of fingerprinting a multi-exit model with the inference time of certain example is well-motivated.\n2. Emperical evaluation: Extensive experiments are performed to prove the validity of the proposed method. Though the significance of the results are hindered by the unfair experiment design, as discussed in the weaknesses. \n3. The paper is overall easy to follow.\n\n## Weaknesses\n1. Novelty/significance: It should be noted that both slowdown adversarial attack on multi-exit models (https://arxiv.org/abs/2010.02432) and the idea of using adversarial examples for fingerprinting (as cited in the paper) are already well explored. This work just make a naive combination of the two, showing slowdown attack can fingerprint multi-exit model, which only provide limited contribution to the field.\n2. Fairness of experiment design: When comparing with AE-based fingerprinting baseline, the paper claims proposed method is significantly better under adversarial training, as shown in Figure 5. However, this comparison is unfair. As adversarial training serves as an adaptive countermeasure of AE, a comparable countermeasure for slowdown attack should be adversarial training with the slowdown attack examples, not the original AEs. Only in this way the robustness comparison is fair.\n3. Citing related work: The fingerprint generation process discussed in Section 3.3 is largely similar to the [slowdown attack](https://arxiv.org/abs/2010.02432) proposed by Hong et al. in ICLR 2020. The paper does not mention its relationship with the slowdown attack paper, nor provide any discussion on the similarity and difference between the proposed fingerprint generation technique with Hong et al.'s slowdown attack. \n\n## Additional questions\n1. The experiment setup section mentions the C10 and C100 models are trained for 50 epochs, which seems to be a very small amount comparing to common design choices (varying from 150-300). Is there any reason behind the number of training epochs? Is this training performed from scratch or from a pretrained backbone model?\n2.  Table 2 and Table 3 seem to indicate a poor performance of ResNet-56 model on CIFAR-10 dataset, any comments on why is this the case and what can be done to metigate such poor performance?",
            "summary_of_the_review": "In summary, my major concern of the paper include limited novelty and unfair comparison (see previous section for detail), which limits the technical merit of this work. Thus, I would suggest rejection for now.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}