{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes to reuse the attention scores for the heads across multiple layers in a transformer network. The reuse mechanism is inspired by the redundancy of attention maps in transformers. The similarity of attention maps between the heads of different layers are analyzed both empirically and theoretically. A lemma is proved that random attention has less similarity. The theoretical analysis reveals that the redundancy of attention maps does not come from the architecture itself but from the training process. Based on the empirical and theoretical analysis, a simple attention reuse mechanism is proposed. Basically, starting from the second layer, K heads across P layers are shared. Thorough experiments are done for BERT, T5, ViT, and machine translation.",
            "main_review": "Pros:\n1. The idea of this paper is based on the fact that neural networks have redundancy. The idea of reusing attention maps is simple to understand. The exploration along this direction is valuable.\n2. The theorectial analysis in this paper helps to understand the reason why reusing attention maps is possible.\n3. The paper is fairly well-written. \n\n\nCons:\n1. The main concern of this paper is the benefit brought by the reuse mechanism. As shown in the experimental results, the reduction of number of parameters and FLOPs is not severe. This shows that the reuse mechanism only has a limited effect.\n2. The experiments are done on transformer architectures that uses global attention. Yet, recently transformers with local attention are proposed, where the computational complexity of local attention is greatly reduced. The effect of the reuse mechanism is questionable.\n3. The reuse mechanism is based on the redundancy in neural networks. Actually, reusing some operators in neural networks is discussed [2-3]. A reference to those works is necessary and helps the readers to understand the wide existence of redundancy and reuse mechanism in neural networks. \n\n[1] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n\n[2] Towards Efficient Graph Convolutional Networks for Point Cloud Handling\n\n[3] Convolutional Neural Networks with Layer Reuse",
            "summary_of_the_review": "The idea of this paper is simple to understand. It is based on the wide existence of redundancies in neural networks. This leads to the rationale of reuse mechanism in transformers. Yet, the actual reduction of model complexity in terms of number of parameters and FLOPs is very limited (less than 10%). In addition, the recent local attention mechanism also reduces the computational complexity of attention, which further limits the applicablity of the proposed mechanism.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed the Reuse Transformer, a modified Transformer architecture which saves on redundant attention computation to deliver reductions in both compute and memory usage. The derivation of the Reuse Transformer is based on the investigation on the redundancy of attention scores computed by a Transformer model. The authors empirically found a high degree of similarity of attention matrices between different layers, and this finding inspired the idea that using a mix of standard exact computation attention heads with \"reuse heads\" which reuses attention heads computed in previous layers. Extensive experiments across different domains (language and vision tasks) and different settings (from-scratch training, pre-training and finetuning) demonstrate the efficiency and effectiveness of the Reuse Transformer.",
            "main_review": "**Strengths**\n\n* The motivation is straightforward and clear. This paper is well written, and figures, tables and pseudo codes are very illustrative.\n* The authors provide extensive experimental results across language and vision tasks, which helps readers comprehensively understand the efficiency and effectiveness of the proposed Reuse Transformer.\n\n**Weaknesses**\n\n* Regarding the novelty of this paper. The main contribution of this paper is the observation of the high degree similarity of attention matrices between different layers (especially for adjacency layers) and the proposed Reuse Transformer. However, there is existing work [1,2] which has the same motivation \"the attention matrices are similar in different layers, especially in the adjacent layers\" (in [1,2]) and proposes similar methods (reuse attention matrices to reduce redundancy computation). However, the authors do not include these works in this paper for discussion and experimental comparison. \n\n* Regarding the experimental results. The results should serve as evidences that the Reuse Transformer has better efficiency and effectiveness trade-off than the standard Transformer, i.e. the Reuse Transformer has better performance with less or matching computational requirements (params, flops, etc.) compared to the standard Transformer. However,\n\n  * For the BERT pre-training task, the improvements of the Reuse Transformer compared to the baseline BERT model are very marginal considering both the pre-training/finetuning metrics and the efficiency metrics. Besides, the MNLI accuracy 85.32 of the baseline BERT model seems to be lower than that (85.72/85.67) is reported in [1], which may be due to the difference of the pre-training settings.\n  * For the ViT task, the Reuse Transformer (Reuse 13L) has only 0.18% relative improvement compared to the standard ViT model which has matching computational requirements, while the \"Steps/Sec\" metric even shows that the Reuse Transformer (Reuse 13L) is slower than ViT model. Models in other settings of the number of reused heads (K) and reused layers (P) have inferior performance compared to the standard ViT model.\n\n  \n\n* Minor issues:\n\n  (1). In fig 3, the authors show the similarity in adjacent layers for different heads and the similarity of each head is computed with its closest matching head in the previous layer. Do these matching heads distribute separately or concentrate on just one or two heads in the same layer? \n\n  (2). X_i^\\top * W_q1 * W_k1^\\top * X_j --> X_i^\\top * W_q1^\\top * W_k1 * X_j in proof of Lemma 1.\n\n[1] Ying et al. \"LazyFormer: Self Attention with Lazy Update.\" *arXiv preprint arXiv:2102.12702* (2021).\n\n[2] Xiao et al. \"Sharing Attention Weights for Fast Transformer\", IJCAI 2019.",
            "summary_of_the_review": "Overall, I vote for rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this paper, the authors first show that the pairwise attention scores used in the transformer model are considerably redundant, especially adjacent layers showing high similarity. Based on this observation, the authors further propose Reuse Transformer, that reuses attention scores computed in one layer in multiple subsequent layers. Experiments in multiple tasks show that by doing this, the performance is equivalent to or better than a standard transformer, while reducing both compute and memory usage.   ",
            "main_review": "Strengths: \n\n1) Writing: The paper is generally well written, and easy to follow and understand. The reviewer enjoys reading the paper.  \n\n2) Novelty: The observation that the attention scores are highly redundant is quite interesting, which is also a novel contribution. The proposed Reuse Transformer is also interesting, which also contains certain level of novelty. \n\n3) Experiments: The authors have conducted comprehensive experiments on multiple benchmarks, which are non-trivial efforts.  \n\nWeaknesses: \n\n1) The reviewer's major concern lies in the experimental results, which seem not ideal. On one hand, the reviewer appreciate the authors conducting so many experiments; on the other hand, both the claim of saving compute and memory usage and improving performance is quite weak. \n\na) For example, in Table 1, the savings of parameter counts and FLOPS when comparing BERT and Reuse are quite marginal. \n\nb) If we compare BERT-base/large vs Reuse-13L/26L, it also remains unclear whether the improvement is significant. Or, whether it is worthwhile to use this \"kind of complicated\" Reuse Transformer but only bring marginal performance improvement.\n\nThis similar observation also holds for other tables, such as results of T5 and ViT. Comparing both a) and b), it is unclear what's selling point of Reuse Transformer, and why should we care about/use it?\n\nc) The authors also conducted experiments on LRA. However, it remains unclear why Reuse Transformer can help in this case. The Reuse Transformer seems not designed to deal with long sequences?\n\n2) Clarity: Overall, the paper is clearly written. But there are also some places that need further clarification. \n\na) For example, in Figure 4, what does Random Training Data mean? \n\nb) In section 4, as to the design of the reuse layers, why always reusing the first few layers? Why not considering some other strategies? For example, reusing the attention scores every k layers? The reviewer tries to understand the design philosophy.  \n\nSide comments:\n\nThe ICLR paper format seems have been changed a little bit. For example, the margin has been made smaller, which is not the original paper format. I am not sure whether this should be considered as violation of the submission policy, and I think it is worthwhile to bring it to AC and the authors' attention. ",
            "summary_of_the_review": "In summary, this paper is well written, and proposes an interesting transformer variant. However, the performance, especially the claim of saving compute and memory usage is not ideal. Given the fact that there are so many transformer-related submissions, the reviewer decides to give a Borderline Rejection recommendation. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}