{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a self-supervised learning method that is based on aligning class distributions of two augmentations $(P^1, P^2)$ of the same image, while avoiding all class labels to collapse. KL divergence is used for aligning, together with a sample-wise entropy term for each class distribution(corresponds to cross entropy loss). In order to avoid collapsing issue, another negative entropy term is added, this time, taken over the mean distributions on different samples. The authors present results across many different tasks such as unsupervised, semi supervised classification, object detection, segmentation. Their results look better than the benchmarks by a large margin. The authors claim that their method is advantageous because of the following characteristics: end-to-end training, no need for pseudo labels, single loss function. \n\nIn terms of contributions, the method has many similarities with this paper, (for the details, please see two paragraphs below(Amrani & Bronstein, 2021)). They are both simple, end-to-end, use CE between $(P^1, P^2)$. The difference mainly lies in their solutions to trivial outcomes. (Amrani & Bronstein, 2021) slightly modifies CE term to include uniform prior. On the other hand, this paper has an extra term that maximizes entropy of the mean of $P^i$'s over classes. (still, they both tackle the same dimension of data - across classes). In addition, they use batch norm before the softmax, which highly improves the performance. \n\n",
            "main_review": "Strengths:\n\n* In general, experiments are convincing in the sense that they are various, detailed and sufficient ablation study is included. \n\n* The paper is written clearly and easy to follow (although a quick proof reading would not be bad).\n\nWeaknesses:\n\n* I liked the analysis of column and row statistics, highlighting the collapsing problem(which is important in self-supervision right now, mainly for nonconstrative methods), the concept of \"matching class distributions of the augmentations\". However, they are already known or included in the previous work.\n\n*  Although (Amrani & Bronstein, 2021) shows a lot of similarities in terms of the ideas and model, it is only cited once and under a wrong context(this paper is presented as using an example paper which uses pseudo labels and do not have a single loss function -- but the opposite for both). They had better talk about this paper in the related work section and compare two methods, their loss functions and state this paper's contributions clearly(For example, in Relations to previous methods section, and when presenting Equation 3). The last paragraph in this section is a bit vague(For example, it was Eq. (3) differentiates our method from previous clustering-based methods -- in which way and from which methods exactly?). \n\n* Self Classifier method does not contain multi crop augmentations (and self labeling) by default. The augmentation types better be matched for a fair comparison in Table 1. (Although this paper's results are probably still better) \n\n * The authors emphasized their method being end-to-end as a differentiating factor. However (Amrani & Bronstein, 2021) and VicReg is also end-to-end at the very least.\n.\n* A couple of small fixes: \n\n    * \"probability distributions\" can be replaced with \"class probability distributions\" while presenting equation 1 and 2, for clarity. Fig 3 plot is labeled wrong, needs a fix.\n\n    * Table 8, 9, 10, Fig 3, 4 do not contain experimental details(which data&task?), could you briefly include them?\n\n    * [1] can be added as a previous work(2021 ICLR paper).\n\nQuestions:\n* Do you have any intuition why using BN+3rd term worked better compared with(Amrani & Bronstein, 2021) column softmax + l1 norm? Did you try using adding BN before softmax in other methods that uses softmax? I would wonder how row and column standard deviations change in (Amrani & Bronstein, 2021) since they attack the same problem and dimensions?\n\n[1] Li, Junnan, et al. \"Prototypical contrastive learning of unsupervised representations.\" arXiv preprint arXiv:2005.04966 (2020).\n\n",
            "summary_of_the_review": "Overall, I find the contributions of this paper is very limited (having a slightly different term for class diversity + BN before softmax). The paper needs to at least mention Self-Classifier paper and compare with it, reorganize related work and state their contributions more clearly. Providing more intuition about why BN and diversity term should be chosen like in this paper (at least compared with Self-Classifier) would be helpful. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose the Twin Class Distribution Estimation (TWIST) as a self-supervised framework to learn representation from unlabeled image data. Specifically, the authors adopt a siamese neural network with softmax to compute the class distributions of two views of a given image and propose the TWIST loss to enforce consistency between, sharpness, and diversity of the views. The proposed methods are evaluated on common self-supervised learning evaluation protocols.",
            "main_review": "The paper is overall clear-written and easy to follow. The proposed method is interesting. And a major contribution of this work is to enable the end-to-end self-supervised training in the unsupervised classification setting, unlike previous methods who may require an additional clustering & alignment step. However, there are several concerns raised. I would like to update my evaluation if the concerns are addressed.\n\n\n1. Although the method is interesting for its end-to-end characteristic compared to the self-training (or clustering-based) methods, the novelty is limited when compared to existing contrastive methods, especially the Barlow-Twin. Sharing similar frameworks and losses, TWIST is more like an adaptation of Barlow-Twin (as well as the idea of information bottleneck) from representations to classification probabilities. \n\n\n2. The proposed TWIST loss has an interesting intuition. The empirical discussions and analyses are appreciated. However, it would be helpful and add more originality to the work if the authors can provide further theoretical insight on the proposed TWIST loss, especially compared with the contrastive losses. In addition, a discussion on the diversity term in the TWIST loss would be appreciated, as there are many ways to enforce diversity and it is not clear why the current one is employed.\n\n\n3. The proposed framework requires a pre-defined number of categories (C) during the self-supervised training. It adds additional assumptions (a known number of classes) and hence narrows the applicable scenarios of the proposed method. For example, when the downstream tasks are agnostic, the number of classes is during pre-training, or the class has a long-tail distribution, the proposed method seems inapplicable. One possible solution can be to replace the classification layer in the siamese network, which reduces to typical contrastive frameworks with a projection head discarded during fine-tuning.\n\n\n4. It’s great to see that the experimental results on semi-supervised and transfer learning settings are promising. However, I have some concerns about the linear evaluation regarding the multi-crop. In Table 2, the authors compare multiple methods with or without multi-crop. In the “without multi-crop” section, TWIST underperforms BYOL and BarlowT, whereas, in the “with multi-crop” section, BYOL and BarlowT are not compared. Although TWIST with multi-crop achieves the best performance among the compared methods, the current results are unable to show the effectiveness of TWIST, as I would consider BarlowT and BYOL as the closest baselines of TWIST (as stated in comment 1). The improved performance, compared with BYOL and Barlow-Twin, is mainly due to the use of multi-crop. Therefore, it would be more convincing if more results with multi-crop are included.\n",
            "summary_of_the_review": "The paper is overall clear-written and easy to follow. The proposed method is interesting. And a major contribution of this work is to enable the end-to-end self-supervised training in the unsupervised classification setting, unlike previous methods who may require an additional clustering & alignment step. However, there are several concerns raised regarding the novelty, originality, application scenario, and experiments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This submission introduces a new self-supervised learning method, called TWIST. TWIST trains the neural network model by enforcing the Siamese-structure network to produce consistent predictions over two augmented versions of the input image, with an additional sharpness term and diversity term. The proposed approach is simple but empirically works well for unsupervised learning, representation learning and pre-training neural network model for diverse downstream tasks including detection and segmentation. ",
            "main_review": "Strengths:\n\n1. The proposed method is simple and straightforward, yet working very well. \n\n2. This work presents comprehensive experiments to evaluate the proposed method. Extensive experiments provide solid evidence for the effectiveness of the proposed method.\n\n3. The paper is written well and clear to follow. \n\nWeakness:\n\n1. For CNN and ViT-alike model training, the work uses different training strategies (self-labeling vs. momentum encoder). This kind of hurts the simplicity of the method and requires the users to adapt the method for different backbones. It will be better if the authors can provide a single training strategy for TWIST that works fairly well for both backbones. \n\n2. This work has large similarity with DINO. Authors are encouraged to explain the difference with DINO clearly. \n\n",
            "summary_of_the_review": "Overall, I think this paper makes a solid contribution to self-supervised learning field. The proposed method is simple yet works well. My only concern is the method seems to need extensive hyper-parameter tuning and training strategy for achieving higher performance. I would rather like to see the method with a single training strategy, even though the performance may slightly drop. If the rebuttal time is sufficient, authors are encouraged to provide results of ViT with self-labeling + TWIST. My another concern is the similarity to DINO. Authors should explain the difference more clearly. I would like to raise my rating if the two concerns are well addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a method for self-supervised learning by estimating twin class distribution. It uses Siamese network to predict the probability distributions of two augmented views of a single image, and then try to minimize the distribution differences between them. In addition, they regularize the class distributions to ensure the sharpness and diversity. Some reasonably good results have been shown in this paper.",
            "main_review": "The major weakness of this paper is the similarity to DINO, which also tries to minimize the differences of two probability distributions using cross-entropy loss. Also, some centering and sharpness tricks are used in DINO. This makes this paper very similar to DINO, but this paper tries to avoid mentioning these similarities in this paper. Also, the multi-crop trick, although it is was firstly used in SWAP. And the improvements over DINO are quite marginal, less than 0.5 points in some cases and no gain or worse in other cases. \n\nOverall, I don’t see much value, novelty and significance from this paper. So I recommend reject.\n\nSome minor points\n-Tab. 10 in page 6 should be tab. 3?\n-no comparison with DINO in table 5.\n-what training strategy used in table 8? Don’t find the 70.6 accuracy in the paper.",
            "summary_of_the_review": "The novelty is very limited; the paper doesn't mention the similarities to DINO, which is not appropriate; the experimental improvements are quite marginal.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}