{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an efficient backdoor sanitation method that only takes 6% and 2% of training time on CIFAR10 and ImageNet, respectively.  The sanitation method, called Feature Grinding aims to increase the distance between predicted features of clean and trojan samples from the same target class by applying a transformation with a random factor (random permutation, random rotation matrices, randomly sampled) which should keep in secret from the adversary. ",
            "main_review": "​Strengths:\n- The idea is quite interesting and meaningful. Authors tried to sanitize models with a restricted resource as models become low-cost and computing resource becomes a critical problem.\n- The runtime decreased significantly yet the performance still remains roughly the same.\n- Authors proposed a new metric to measure the effectiveness of sanitation.\n\nWeaknesses:\n- The methodology is a bit confusing. \n  * In Sec 4.2, the authors only reset the model's head. I am wondering how to define the top of the feature extraction layer. Do different models have to reset a different number of layers? How many parameters have to reset? Some further explanations would be needed to explain it.\n  * As the authors described in 4.2, the activation layer should be recorded for every data. I am curious about how much storage needs and whether it is affordable when the resource is restricted (which is the scenario author described), especially when sanitizing an ImageNet model.\n  * An algorithm would be needed to better explain the methodology. Some details are missing (e.g. (1) why should we record the activation layer? I think it is for φM(x) but it is hard to understand without any further explanation. (2) The authors only mention that there are two stages; however, there's no explanation when to switch from one to another.\n  * Three types of transformation are proposed in section 4; however, there's no experiment to explain which one is better and why. \n- I'm wondering the differences between transforming data and transforming features. Why transforming features works. In my opinion, the same method can be applied to data and get the results.\n- Some questions about the \"Training Time\" part of the evaluation:\n  - Does the time of Feature Grinding include recording the activation layer? \n  - As I knew, Feature Grinding is quite similar to NAD, both feed data to model and get activation layer(s), compute the losses, and fine-tune the model. What if we also record the feature map of NAD, will the runtime become the same as the time of Feature Grinding?\n  - Furthermore, Feature Grinding reset some parameters and re-train these layers. How many epochs are needed to fine-tune the model? Is it faster than NAD? (In NAD, they claimed only few epochs are needed (less than 5 iterations), see Figure 9 in their appendix). As some parameters have to be reset in Feature Grinding, will it converge as fast as NAD?\n- In the AUPOC part, \"There is an apparent correlation between both metrics, where a low CDA predicts a low ASR. For example, assume the defender randomly assigns all weights of the victim model (CDA is equivalent to random guessing), then the ASR is expected to be no higher than random guessing as well.\" The idea and the example are not convincible. Some further evidence is needed to support this assumption. We can always train a backdoored model with a strong attack and perform poorly, which conflicts with the above idea.\n\n",
            "summary_of_the_review": "The paper appears to be incomplete and needs further work. Some details and experiments are missing. I do not think it is ready for publication in its current state.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a \"feature-grinding\" method to defense against backdoor attacks. The idea is to increase the distance between the clean and backdoor samples of the same target class in the latent space by assuming access to the clean samples. The paper also proposes a new metric, called AUPOC, to assess the effectiveness of the defense methods. Experimental results show some effectiveness of the proposed feature-grinding method.",
            "main_review": "The paper is generally easy to follow and the motivations behind the proposed method are ok. However, I think that there are a few important limitations of the paper:\n\n* The main thesis of the paper is that given limited computational resources, training is outsourced. Thus, a backdoor defense that relies on less computational resource is generally better. For such a reason, Feature Grinding is proposed. However, the discussions and experimental results in the paper are not providing convincing evidence of this proposal. Below are some reasons:\n    * Assuming access to the entire training (end of Section 3) is a pretty strong assumption in the backdoor domain. For example, Neural Cleanse assumes a subset of the clean, test data. I also argue that a likely reason for outsourcing the training process is because of the large amount of often proprietary training data. Thus, this assumption is impractical.\n    * The experiment w.r.t runtime for Neural Cleanse (NC) is not convincing. While it is true that NC trains a different model for each class, thus higher number of classes requires a longer training time, each training w.r.t one class can be run in parallel and separate from the other classes. Furthermore, the proposed method will update all model's parameters (thus requiring storing gradient during training, and this could be expensive for large models), while NC does not, which also represents an advantage of NC. I would argue that for larger models, NC will not have a problem while the proposed model will need more-memory GPUs. \n    * In general, it is hard to see the advantages of the proposed method in terms of computation given the current motivations and empirical evaluation. \n* I am also concerned about the theoretical connection between the proposed approach and the discussion in Section 4.1. I'm not entirely sure how the feature grinding will achieve the desired effect, either from a theoretical or empirical perspective. Furthermore, there are not enough details about the proposed 2 phases. What are the objective of the 2 phases? How long do you train the 2 phases?\n* Some recent types of attacks are not investigated, for example, Doan et al. ICCV 2021 or Nguyen et al. ICLR 2021. These methods have a different attack mechanisms than existing attack methods and should also be considered.\n\nMinor: I also find it difficult to view and understand Figure 1 and 2. \n\nDoan et al. ICCV 2021. LIRA: Learnable, Imperceptible and Robust Backdoor Attacks\n\nNguyen et al. ICLR 2021. WaNet -- Imperceptible Warping-based Backdoor Attack\n",
            "summary_of_the_review": "While the paper seems to be well-motivated, the discussion, including both theoretical and experimental analysis, in the paper is not convincing and strong enough. I think that the paper should be improved significantly  before another consideration.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new backdoor removal technique by transforming the activations from a feature layer (the penultimate layer) and fine-tuning the trojaned model. Specifically, it records all the activation values of the feature layer from all the clean samples in the entire training dataset and applies a pre-defined transformation on these activations. It then fine-tunes the trojaned model by minimizing the difference between the activations from the model and the transformed ones. The classification loss is also considered during fine-tuning. The evaluation is conducted on two datasets, CIFAR-10 and ImageNet. The experimental comparison with four baselines shows that the proposed technique has better performance on removing backdoors.",
            "main_review": "\nIt is interesting to use transformations on internal features for backdoor removal. The performance of the proposed technique is compared with four baselines. There are a few aspects that need improvements.\n\n1. The threat model of this paper is impractical. It requires the defender having the entire training dataset. If the defender has all the data, there is no need to use a model trained by a third party. The defender can directly train the model by herself. Even with the assumption of the defender having limited resources, the defender can use transfer learning to fine-tune her model on an existing well-trained feature extractor. The resource usage is no different from fine-tuning the trojaned model. Also, this paper claims \"we do not put hard constraints on the defender’s computational power\", which makes the setup of this paper more impractical. As this is the base of all the evaluations and comparisons with other methods, it is particularly important to justify the threat model.\n\n2. The intuition why the proposed approach works is not clear. The paper uses a conceptual explanation in Figure 1 to illustrate the intuition behind the proposed approach. From the figure, it can be observed that there are half of the poisoned samples still falling into the target class, meaning it can only mitigate half of the backdoor effect. The transformations used in the paper are mainly changing the ordering of feature dimensions. Although the approach records activations of clean samples, the feature ordering also change correspondingly for backdoor samples. It is not clear why the proposed approach would work.\n\n3. The novelty of the proposed approach is limited. The idea is straightforward by constraining the internal features with transformed counterparts. It is very similar to NAD, where the student model's features are constrained by matching with those from the teacher model. The transformations seem interesting. But they are just standard matrix transformations. Have those transformations been used in model training in existing works?\n\n4. The evaluation is only conducted on two datasets and two model structures. There are a large set of (thousands of) pre-trained poisoned models from the TrojAI competition. Those models are trojaned with various backdoor settings, including different backdoors such as polygon triggers and filter triggers, and different attack models such as universal attacks and label-specific attacks. It is important to see the performance of the proposed technique on extensive test cases with comparison to baselines.\n\n5. Figure 2 and 3 are hard to interpret. Different subfigures do not have a consistent value range for both the x-axis and y-axis, making the results hard to compare across different approaches. The presentation can be improved by showing the results of different approaches in the same figure. The results on different attacks can be placed in separate subfigures.\n\n6. Typos and minor issues.\n- In abstract, \"five other sanitation methods\" -> \"four other sanitation methods\".\n- What is the task loss L_t? Is it the cross entropy loss?\n- On page 7, the learning rate range for Neural Cleanse and TABOR is \"[0.00002, 0.00001]\". The upper bound is smaller than the lower bound.\n- On page 8, for CIFAR-10, \"Feature Grinding performs has\" -> \"Feature Grinding has\".",
            "summary_of_the_review": "1. The threat model is impractical.\n2. The intuition why the proposed approach works is not clear.\n3. The novelty is limited.\n4. The experiments need to be evaluated on a larger set of trojaned models.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes an efficient model reconstruction based backdoor defense, which intends to apply a transformation that increases the distance between predicted features of clean and poisoned samples from the same target class on the penultimate layer. The proposed method is based on an assumption that the transformation will move clean samples while maintaining the position of poisoned samples, and therefore disentangling two types of samples. ",
            "main_review": "\nPros\n1.\tThe topic is of sufficient significance and interest to ICLR audiences.\n2.\tThe paper is well written and easy to follow.\n3.\tTechnically, the proposed method is moderately novel.  \n4.\tThe proposed method seems to be more efficient, while having performance on par with baseline defenses.\n\n\nCons\n1.\tAlthough I recognize the efficiency and the effectiveness of the proposed method, why the proposed method works need further discussion. At least, the author should empirically justify the correctness of the assumption since it is not natural and necessarily true. I will increase my score if this concern can be well addressed.\n\n\n2.\tMissing important technical details. \n-\tHow the baseline attacks are implemented needs more details provided in the Appendix. For example, the latent backdoor attack is designed for transfer learning. How it can be used to evaluate the proposed backdoor defense which not targets transfer learning?\n-\tHow the running efficiency of baseline defenses is calculated? The running time of many defenses (e.g., Fine-Pruning and NAD) relies heavily on the running epoch. How the running iterations are determined needs more details. Using their default settings is not acceptable since the defense may have converged far before the last iteration.\n-\tIt seems that the proposed defense is implemented after the attacked models are received. I think the author should provide the CDA and ASR of all baseline attacks before the defense to ensure that those attacks are trained well.\n\n3.\tMissing important experiments.  \n-\tNo discussion about the selection of adopted transformations. How to select suitable transformations needs more discussion.\n-\tNo ablation study about the grinding and restoration modules.\n-\tNo ablation study about the effects of hyper-parameter \\alpha",
            "summary_of_the_review": "An interesting backdoor defense whose mechanism and experiments need further discussion.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}