{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents an approach to learn latent rule embeddings based on an existing knowledge graph embeddings model to explain the inferences of the KG embeddings model. The work uses knowledge distillation -- making the output of KGE as soft labels and the training data as hard labels. Eventually learning parameters of a model that can output latent rule embeddings and its appropriate confidence scores. Noting that the evaluation of such explanations are hard. The work evaluates based on two criteria (a) faithfulness evaluation: comparing the performance of rule embeddings on knowledge graph completion on wordnet and freebase datasets, and (b) Explanation evaluation: where statistical confidence is measured in comparison to other rule learning approaches. ",
            "main_review": "The paper and the work surely has value. They get to the heart of black box models in knowledge graph embeddings and try to address it by explaining those black box models. It is also important to acknowledge the difficulty in evaluation such works and the authors have tried their best to do so. \n\nHowever, I have some major concerns with the overall paper. (a) Motivation (b) Clarity, and (c) Evaluation. Overall, while I agree with the value of the work, it is important to state and be clear in the paper regarding the value (explained below in Motivation and Clarity). Furthermore, the work seems very close in performance to KGEs which is good but also raises some concerns explained in Evaluation below.\n\nMotivation: Existing KGE approaches are in-turn hard to explain. However, there is no clear motivation provided by the authors on the lack of explainable approaches for knowledge graph completion and why there is a necessity for explaining them. It would be good have a few sentences related to this in the introduction and the related work. A clear WHY THE PROBLEM IS IMPORTANT is necessary to motivate the paper.\n\nClarity: The paper is hard not straight forward to follow -- in particular on some decision being made. For instance: Equation 5 you cite Sun et al for self-adversarial strategy. Is there a reason why you chose this? If it is something that is existing in TransE and RotatE, it is necessary to keep the focus of the work and not explain L(q, a) since it is standard and does not add value to the flow of the paper. Feel free to rebut this point. My primary concern is reading this work, it will eventually get non trivial to discover the novelty.  \n\nEvaluation: It is important to know how different are those rule embeddings and what value it brings in. Although I acknowledge that evaluation is hard -- the work needs some clear baselines in comparison to the rule-embeddings based approach. For instance one of the papers (COMPLEX QUERY ANSWERING WITH NEURAL LINK PREDICTORS) that use Beam Search over Neural Link Predictors can be applied as a baseline for multi hop rules for head to the entity using KGE. In other words, it is important for this work to be compared to a rule generation using the corresponding KGE and see how it differs in performance. \nNext, the statistical confidence methodology seems biased in evaluation because you are looking at most popular paths for a particular relation in the knowledge graph. This might not be the best paths to explain what is going on with the KGE model. ",
            "summary_of_the_review": "I have some major concerns with the overall paper. (a) Motivation (b) Clarity, and (c) Evaluation. Overall, while I agree with the value of the work, it is important to state and be clear in the paper regarding the value (explained in Motivation and Clarity). Furthermore, the work seems very close in performance to KGEs which is good but also raises some concerns explained in Evaluation. Overall, this work might need more thinking and clear way to justify the novelty and evaluate the work. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to create an explainable auxiliary model, M, that mimics the prediction of a trained Knowledge Graph Embedding (KGE) based model, like TransE and RotatE. For every relation r, M first uses a learnable rule embedding and a shared LSTM to generate an implicit first-order path in the form of a sequence of relation embeddings. Following this sequence in the embedding space, one arrives at the embedding of the answer to a query q = (h,r,?). This embedding sequence is then decoded to generate an ordered list of explicit first-order rules along with their confidence. The paper uses knowledge distillation to achieve high fidelity (faithfulness) to the underlying KGE model. \n",
            "main_review": "The paper proposes an interesting differentiable rule learning method based on the predictions of an embedding-based link prediction model. It brings together ideas from different areas, e.g. loss motivated by knowledge distillation; converting summation over paths in equation 8 to differentiable summation over steps in equation 12 as done in NeuralLP; and finally, a decomposition of the transformation function f, that enables it to use similarity in the embedding space to compute the confidence scores. \n\nThe main concern with the paper is the evaluations in section 5 and the overall readability, which is very poor. \n\n**Readability and Mistakes:**\nThe paper is tough to read, with many mistakes and typos, even in equations and algorithms. For example: \n\n1. In equations 12, 14, and 15, summation should be over j.  \n2. In equation 3, RHS should be $\\Phi(q,a)$ and not $\\Phi(a,a)$\n3. Equation 5 is incorrect: sigmoid function $\\sigma$ should take only one argument as input. \n4. Equation 5 should have $\\Psi$ and not $\\Phi$. Otherwise, there is no learnable parameter in it.\n5. In the algo, line#6, the reference should be of equation 13 and not equation 11. \n\n….\n\n\n**Introduction:**\nThe paper claims to learn a \"local\" surrogate model (Introduction, para 3, 2nd line), which means that a new surrogate model should be learnt for every prediction to be explained. However, this is not what KGExplainer is doing. Instead, it learns a global interpretable model that mimics the KGE model. \nKGExplainer, as described in the paper, predicts an answer independent of the KGE prediction. What happens when this answer is different from the prediction of KGE model? Ideally, an explainer should take the answer from KGE model as an input and then generate the path as an explanation. Instead, KGExplainer takes only the query (h,r,?) as input, and outputs both the answer and the path.\n\n**Related Works:**\nThere are many relevant works that haven't been discussed in the paper. For example, \n1. Stadelmaier and Pado, ACL 2019, considers paths for explainable KBC and uses an underlying embedding based method.\n2. Guu et al, EMNLP 2015, was the first to propose the idea of traversing KGs in the embedding space. It hasn’t been mentioned in the paper.\n3. Nandwani et al, AKBC 2020, present an explanation engine for KGE models.\n4. Zhang et al, WWW 2019, propose IterE, which learns embeddings and then induces rules iteratively.\n5. Fact-checking literature (e.g. Shiralkar et al, ICDM 2018), which aims to generate paths for a given triple (h,r,t), is altogether skipped. The paths given by a fact-checking model can be thought of as explanations for an answer 't' of a query (h,r,?). \n\n**Joint training of KGExplainer and KGE model:**\nIn the way it is presented, KGExplainer needs an already trained KGE model. Why can't we train both KGExplainer and embeddings jointly? If the KGExplainer performs as well as the KGE model, do we then have an explainable path-based link prediction model that performs as well as a sota embedding-based model?\n\n**Evaluation of the explanations in section 5.2:** \n\n*Stronger Statistical Baseline:* The evaluation method uses statistical confidence scores as a metric and compares KGExplainer with two statistical approaches. \nWhy can't we create a baseline which mines rules directly by maximizing the confidence scores? For example, Section 5 in Yang et al, ICLR 2015, describes one such method for extracting rules. Similar to KGExplainer, their method also uses the learnt embeddings. Mining only two length rules is not computationally expensive as well. \n\n*Stronger Neural Baseline:* Why not compare against more recent neural rule based methods, e.g. DRUM ?\n\n\n**Case study:** The paper should also describe an example where the explanation paths are not good. For example, what happens when the path rated at the top by KGExplainer doesn't exist for a given head entity in the KG? Does KGExplainer resort to a path with low confidence? Are model predictions often incorrect when a path with low confidence is returned? Is there any difference in the average path confidence when KGE prediction is correct vs incorrect?\n \n**Other minor comments / mistakes / typos:**\n\n1. Equation 5: In rotatE paper, negative sampling changes only the head and tail entities and keeps the relation fixed. However, negative samples h',r',t' in equation 5 can potentially sample a different r.\n2. Related works, para 2: g: E x E → R\n3. The Spelling of Traverse and Allocation in  **Baseline Approaches**.\n\n**References:**\n\nGuu et al, EMNLP 2015: Traversing Knowledge Graphs in Vector Space\n\nStadelmaier and Pado, ACL 2019: Modeling Paths for Explainable Knowledge Base Completion\n\nZhang et al, WWW 2019: Iteratively Learning Embeddings and Rules for Knowledge Graph Reasoning\n\nNandwani et al, AKBC 2020: OXKBC: Outcome Explanation for Factorization Based Knowledge Base Completion\n\nShiralkar et al, ICDM 2018: Finding Streams in Knowledge Graphs to Support Fact-Checking\n\nYang et al, ICLR 2015: Embedding Entities and Relations for Learning and Inference in Knowledge Bases\n",
            "summary_of_the_review": "The overall approach is interesting, but there are significant issues with the write-up and the evaluation of the proposed model.\n\n\n---Post Rebuttal ---\nThanks for your responses. My concerns w.r.t. experimentations and mistakes in the write-up have been addressed. I am happy to increase my rating to 6. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper focuses on the KG link prediction research field. The work proposes a multi-hop reasoner KGExplainer which learns latent rules with RNN and KGE models. Besides, it works well on providing explanations for link prediction results.",
            "main_review": "Strengths:\nThe interesting multi-hop reasoner learning latent rules and behave similarly to KGE models with knowledge distillation strategy for link prediction.\n\nWeaknesses:\n1. A few related works are missing, e.g., RNNLogic [1] also employs RNN to learn logic rules for KG reasoning. RPJE [2] introduces rules and paths into KG embeddings for link prediction. \n\n2. The experiment is not sufficient enough.  \n2.1. The baseline models are not sufficient enough. (1) For the differentiable rule learning baselines, they only choose NeuralLP rather than its advanced approaches such as DRUM and Anyburl. (2) For the KG embedding baselines, they only select TransE and RotatE rather than other SOTA models such as HAKE and DualE. (3) Some reinforcement learning-based multi-hop KG reasoning models should be compared with the proposed approach, such as DeepPath, MINRVA and RLH.\n2.2. The proposed model cannot achieve more performance gains than the original KGE models from the link prediction results shown in Table 2, which is unable to explain the advantage of the developed model compared with KGE models except for explanation.\n2.3. The case study shows the paths as grounded explanations similar to the reinforcement learning-based approaches. However, there are no explicit logic rules for explanations. Thus, compared with the existing models based on reinforcement learning, the proposed rule learning approach does not show obvious advantages in interpretability.\n\n3.\tIn the KGEXPLAINER section, only TransE and RotatE are introduced to design KGExplainer, which limits the generality of the proposed model since it cannot be applied to other KGE models such as ComplEx and DualE.\n\n[1] Meng Qu, and et al. RNNLogic: Learning Logic Rules for Reasoning on Knowledge Graphs. ICLR 2021.\n\n[2] Guanglin Niu, and et al. Rule-Guided Compositional Representation Learning on Knowledge Graphs. AAAI 2020.",
            "summary_of_the_review": "This paper proposes a novel framework of multi-hop KG reasoning and the paper is well written. However, a few related works are missing both in the introduction and the experiments. Besides, the evaluation results on link prediction could not illustrate the advantage compared with the other KGE models. Thus, it is suggested to add more discussions and experiments to demonstrate the effectiveness of the proposed model.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NO",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents a method for explaining the predictions of a large-class of knowledge graph embedding (KGE) methods, that the authors call the \"two-function class\" of KGE methods. The methods in this class assign a real-valued truth-score to a \"(head, relation, tail)\" triple by passing it through two functions as score(head, relation, tail) = f2(f1(head, relation), tail). The KGE method heavily uses the decomposition of the score function into two parts \"f1\", \"f2\". There is an additional multi-linearity assumption in the second-argument of \"f1\" which is obeyed by two of the existing methods such as TransE, and RotatE but not by Hyperbolic embeddings and others.\n\nFor such a \"two-function\" KG Embedding method this paper presents a system called the KGExplainer. For each relation, the KGExplainer learns a set of rules, where each rule is parameterized by an embedding. This embedding is fed into an LSTM for T steps, to generate a sequence of intermediate relation embeddings. These intermediate embeddings are then used as input to the function \"f1\" to generate intermediate \"head\" embeddings. For example, the head is first unrolled to get head' = f1(head, lstm(relation)), and then in next step we get head'' = f1(head', lstm(lstm(relation))). The outputs of the LSTM are the \"intermediate\" relation embeddings.\n\nThis LSTM is executed once for each possible rule, and a separate relation-conditional score-distribution is learnt for each rule as well. These prior values are parameterized by $\\alpha$ and they are computed by using a dot-product between the intermediate relation embeddings and a learnt relation embedding matrix R.\n\nHowever there is an issue in the computation of the final $\\alpha_{z_r}$ values that they do not sum up to one, which is required by the initial interpretation in equation (7).\n\nFinally the whole system is trained end-to-end using back-propagation.\n",
            "main_review": "## Strength\n1. The paper presents a few interesting empirical results on knowledge base prediction and explanation tasks.\n2. The method used for encoding self-loops and inverse relation was an interesting trick.\n\n## Weakness\n1. The system is applicable only for explaining the predictions from two models TransE, and RotatE and the empirical results do not seem to justify the complexity of the new architecture. The results are only shown on small knowledge graphs such as worndnet and FB15K where the value of learning a knowledge graph explanation system is not very clear\n\n2. There is also an issue in the computation of the final $\\alpha_{z_r}$ values that they do not sum up to one, which is required by the initial interpretation in equation (7).",
            "summary_of_the_review": "This paper presents a method for explaining the predictions of a large-class of knowledge graph embedding (KGE) methods. Although the paper initially frames their problem space very generally, additional linearity requirements imply that the particular method in the paper is only applicable to two previous models: TransE, and RotateE. The overall architecture is interesting, but ultimately quite convoluted and the benefit of all the additional complexity is not evident. The paper will be much improved by through more careful ablation studies, and by streamlining the notation, often the notation for a rule, and an relation is confused and sometimes a vector is denoted by a bold-faced symbol, and other times it is denoted by a normal plain-faced symbol.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}