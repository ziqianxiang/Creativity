{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors address the problem of domain generalization in time series classification, where the goal is to learn models that are robust to domain shift without access to target domain samples. This is done by assuming that multiple source domains sharing the same label space are available during training. Their approach, selective cross-domain consistency regularization, encourages similar predictions on subsets of source domains that are estimated to be more similar to each other.",
            "main_review": "The authors present a relatively simple strategy for cross-domain robustness in time series classification consisting on regularization by domain similarity and data augmentation. The regularization term in (1) though well motivated is somewhat underwhelming due to the fact that i) only accounts for domain-class averages, rather than distribution matching, and ii) that the regularization weights also depend on domain-class averages, which question whether the weights are necessary or whether the hard constrain on both fixed and learned selection (zeroing out some domain combination weights) are responsible for the performance gains and not the weighting itself. That being said, the authors consider both regularizing on features and soft labels and different number of clusters in the ablation study. However, it will be interesting to see the performance of the model without the weighting term.\n\nThe results though comprehensive considering that the authors implement a a wide range of baselines not necessarily proposed for time series classification or domain generalization as well as examining the impact of regularization/augmentation, regularization options and number of clusters, seem underwhelming considering that i) the contribution of data augmentation is minimal as shown in Table 5, and ii) the performance of the proposed model is only slightly better than RSC.\n\nHaving results on additional datasets will improve the paper considering that in one of the datasets considered the performance gap between the proposed approach and ERM are very close.",
            "summary_of_the_review": "The paper introduces a regularization term for cross-domain consistency that though well motivated is realized heuristically, which questions the generalizability of the proposed approach beyond the datasets considered in the paper, more so considering that in one of the datasets (HHAR) the proposed approach does not perform to differently than the baseline ERM and does not overperform the existing RSC.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a domain generalization method for time series classification. Their method consists of an additional loss term that equalizes the distribution of logits for each class across select domains. In order to choose domain pairs for consistency regularization, the authors propose either using prior expert knowledge, or estimating it by computing the distance between logits during training. The authors also improve robustness by using simple time-series data augmentations. They benchmark their method against a wide variety of baselines on two time-series datasets, and find that their method outperforms prior work. They then dissect their performance gains in detail through ablation studies.",
            "main_review": "## Strengths\n\nThe paper is generally well-written and is easy to follow. The authors conduct a robust experimental evaluation against a wide array of baselines, and their method does seem to improve performance. The authors also conduct interesting further analyses to dissect these performance gains.\n\n## Weaknesses\n\n- The authors do not motivate their proposed loss term very well. It is unclear to me why it is desirable to equalize the average class conditional logits across domains, and what types of distributional shifts this would allow the model to be robust to. The authors should provide a simple synthetic example, or mathematical justification, for why this loss term works.\n\n- The authors should try to be more mathematically rigorous in their explanations/justifications. For example, by \"label-preserving, but does not preserve class relationships\", do the authors mean that $P(Y|X)$ can change across domains, but $P(Y)$ stays the same?\n\n- It seems to me that penalizing the logits in this fashion would lead to poor model calibration. Is this the case?\n\n- There does not seem to be anything in the loss proposed in Section 3.2 that is specific to time-series data. Is there a reason why the authors chose to frame the paper as being specific to time-series, as opposed to evaluating this loss term on other data modalities (e.g. common image-based DG benchmarks)? \n\n- The time-series augmentations seem very simple and rudimentary relative to other methods which have been used for time-series augmentation in machine learning (e.g. see [1]). \n\n- In order to improve the difficulty of the experimental evaluation, I would recommend running a set of experiments where all of the domains for a particular user or phone model are left out (for HHAR, and similarly for Bearings). This seems to be a much more realistic use-case, and would be a bigger challenge for domain generalization methods.\n\n- As these two datasets are less known in the domain generalization field, the authors should consider reporting basic dataset statistics like sample sizes and class distributions for each domain in the appendix.\n\n- Both of the datasets that the authors test on are based on high-frequency sensor measurements (long time series with few features). Do the authors believe that their method would work just as well on other time-series datasets with different characteristics, such as clinical data with fewer time steps but many more features (e.g. [2])?\n\n[1] https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0254841\n\n[2] https://arxiv.org/abs/1908.00690",
            "summary_of_the_review": "The paper presents a method for domain generalization which seems to do well experimentally. However, the proposed method is not very well motivated or justified mathematically. I believe that the paper is borderline, pending author responses and fixes.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a regularization to keep consistent predictions across similar domains in time series classification tasks. Different regularizers are used when meta data of domains are available or not. It also introduces data augmentation to enhance robustness. Experiments on two datasets show that the proposed method outperform a series of baselines.",
            "main_review": "## Strengths\n1. The idea of the paper is clearly presented and easy to follow,\n2. Extensive empirical studies are performed, including thorough analysis of the results\n\n## Weakness\n1. First of all, the ultimate purpose of this work is to increase generalizability in spite of domain shifts, as the introduction and the experiments indicate. However, the logic between this goal and the proposed consistency regularization is not clearly presented. By making consistent predictions in similar domains, how can it help making accurate predictions in a target domain that is potentially very dissimilar to all the source domains? In other words, the entire work focuses on data transformations ( the domain shift in fact ) that do not twist class relationships too much, as it always assigns zero weight to dissimilar logits. And what if the shift that occurs in the target domain is not that \"mild\"?\n2. The method in case of unavailability of domain meta data is not convincing. The learned selection is dependent on the feature extractor and classifier. It could be an extreme case where dissimilar domains produces close logits initially, and is induced to be closer. The author should explain how to address such a cold-start issue, where reliable weights $w(d^{(i)}, d^{(j)})$ are not available at first.\n3.  Essentially the proposed method aligns the soft labels between domains that are evaluated to be similar. Equivalently, it attempts to obtain domain-invariant features using available data in a subset of source domains. In this sense, I don't think the paper provides sufficient novelty in technical aspect. \n\n## Suggestions\nTo improve the paper, I think the authors should clearly demonstrate the necessity and feasibility of the proposed consistency regularization for domain generalizability in addition to empirical results.",
            "summary_of_the_review": "Overall I think the paper needs considerable revisions before it can be published.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper addresses the domain generalization problem in the context of time series classification application. \nThey propose to learn representations such that the predicted class distribution is similar across close domains.\nThey regularize in two settings: fixed and learned selection. \nIn the fixed setting, domain closeness is defined through domain's meta-data and human judgment.\nIn the learned selection setting, closest domain is defined as the nearest neighbor in the logit space. \nTheir regularization based algorithm is shown to perform better than ERM and many other DG methods on two publicly available time-series datasets. ",
            "main_review": "## Strength   \n- Sound empirical evaluation strategy on the two datasets considered. \n- Detailed visualization.\n\n## Weakness  \n- Important aspects of the algorithm are not well argued. See below. \n- Marginal empirical gains on 1/2 datasets: HHAR.\n- Comparisons on only two datasets is limiting. \n- The paper focuses on time-series, but the authors did not argue well why time-series classification is different enough to warrant a separate study. I.e. why does the existing DG methods fail on time-series and why would their method work only on time-series? Unlike time-series, the DG benchmarks in the case of image applications are well-established. Without a good reason for their algorithm's specialization to time-series classification, I will have to assume the worst that the authors are only trying to avoid evaluation on well-studied image/text DG benchmarks. \n- The intuitive explanation for why the method works is unclear to me. In the learned selection setting, the divergence in logits is minimized between a domain and its closest neighbour (closeness captured again by some distance in the logit space). Why minimize logit divergence between two domains that are already close?\n\nThe selective regularization aspect is the only novel aspect of the proposed algorithm as far I see. Algorithms that regularize logits or its normalized version are known [1, 2].\nHowever, the authors did not establish convincingly the need for selective regularization through extensive ablation or comparisons with the existing work. \nThe following further experiments can provide more evidence:\n\n- Compare with [1] or [2] on the two datasets. \n- Is the difference between 2 and 1 cluster cases of Table 7, i.e. 87.1 and 85.6 significant? What is the p-score?\n- Conduct ablation studies where the closest domain is picked randomly in the learned selection setting.\n- Demonstrate empirically the negative transfer between the domains and ill-effects of domain invariance across all domains (without selective regularization). \n\n\n## Minor  \n- I do not understand why the authors pick 80% samples randomly from the target data to make the test set as described in last para of Page 5, why not use them all? \n- Why is proposed (fixed sel) of Table 3 and 2 cluster of Table 7 Avg. numbers differ (87.9 vs 87.1)?\n\n## References\n1. Ahmed, F., Bengio, Y., van Seijen, H. and Courville, A., 2020, September. Systematic generalisation with group invariant predictions. In International Conference on Learning Representations.\n2. Arpit, D., Xiong, C. and Socher, R., 2019. Predicting with high correlation features. arXiv preprint arXiv:1910.00164.\n",
            "summary_of_the_review": "More work is needed to establish the need for selective regularization. The empirical gains are marginal and the evaluation is limited to two datasets that are not well studied in the literature. \nAlso, unclear what is the unique nature of time-series classification over image/text DG. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}