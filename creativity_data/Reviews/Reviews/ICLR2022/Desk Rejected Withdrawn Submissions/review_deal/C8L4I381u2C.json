{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper performs an empirical study of the transfer learning properties of value based model free DRL algorithms on Atari. They observe that transferring the pre-trained network in this case is challenging, resulting in negative transfer in many cases and also negative transfer even when the source and target tasks are the same. The paper tries to hypothesize the reason for these observations.",
            "main_review": "Strengths\nThe paper looks at an important question.\n\nWeaknesses\n1. There is a lot of work on transfer learning in RL, including works in Deep RL. Please refer to “Zhu et.,al, Transfer learning in deep rl; a survey”. The paper fails to situate/compare its work in this literature on transfer learning in Deep RL. Note that transferring the weights directly is only one way of transfer, other ways such transferring knowledge about the dynamics, directly the policy, values, reward shaping etc exist. Also within representation transfer, there are works like progressive nets, pathnets and model based transfer (section 4.5.1 in the above paper) which the paper fails to evaluate. \n2. Given above, the contribution of the paper is very unclear to me. \n3. I feel the paper needs more work before it's ready for publication. The current observations need a lot more discussion and further analysis and evaluation.  \n\n\nQuestions\n1. Note that when a pretrained network is used as an initialization, and allowed to change in the new task, lot of knowledge about the previous task can be overwritten/forgotten during the initial stages of learning of the new task where the agent almost acts randomly. By the time it has learned the basic the knowledge from the previous task might be present in the way intended to perform/evaluate transfer. Any thoughts on this? Was something along these lines observed?\n2. When measuring transfer, how many runs were each transfer experiment was? Was there any significant difference in the transfer when the random seed for the source or target task is different and nothing else changes? \n3. From the experiments in Table 1, can anything be inferred about the relationship (qualitative/properties) between the source and target that results in a positive/negative transfer? Discussion on this would be useful\n4. The notion of similarity suggested in the paper (Figure 1) may not be true for DRL agents learning in just those tasks. The semantic similarity that humans observe when we look at the images having corridors and passes may not transfer to the similarity in the features learned by CNNs that are being transferred in DQN style methods. That might be the reason why we still observe negative transfer between the two games in Figure 1 (as observed in Table 1)\n5. The results in Table 3 (negative transfer) are surprising as mentioned in the paper. What are the exact differences between fine tuning (which gives relatively less performance) and just continued training in source task (which gives good performance)? Is it that the optimizers are reinitialized when moved to the target task? Replay buffer reinitialized? Anything else? Which among these changes if any are responsible for the difference in performance? Discussion on this would be useful.\n6. The experiments in catcher domain alone feels insufficient to justify the hypothesis and arguments made in the paper section 4. \n7. Why is transferring the head from a different network and fine-tuning better than just fine-tuning the whole network?\n8. It is not clear to me why the hybrid version with body taken from the negative transfer part and head from the positive transfer part is expected to work better that just fine tuning? Was this tried in Atari games setup? Could you explain this please?\n\nOther Comments\n1. Should be a bit careful about the wordings/claims in some places. The observations made in the value based model free RL setting may not necessarily transfer to all of DRL. In some places the paper makes claims general for entire DRL.\n2. I would recommend the authors to provide the key findings and observations in the introduction and also early on in the paper. The current style of explaining information in the order in which they happened with suspense may not be the best presentation style for sharing the progress made. \n\n",
            "summary_of_the_review": "Look at an interesting/important question. Misses utilising/evaluating/situating itself among many related works in the literature. Needs more work before it's ready for publication. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper is motivated by the goal of transferring the representations learned by deep Q functions across tasks, which is certainly of interest to the ICLR community. The authors conduct an empirical evaluation of the transfer learning performance of DQN agents pre-trained and fine-tuned on many different pairs of Atari Learning Environment tasks, more so than existing work (to my knowledge). They devise an additional set of simplified control experiments to conduct ablation experiments focused on different fine-tuning regimes and their impact on transferability.",
            "main_review": "Strengths:\n- Evaluated on many pairs of source/target task, compared with prior work\n- Writing is mostly easy to follow, with just a few exceptions\n\nWeaknesses:\n- Lack of details about the experimental setup make interpreting the results difficult. For example, are rewards normalized across Atari tasks? If not, it's hard to know if the differences in the learning curve ratios are significant at all. In addition, the authors do not mention the number of random seeds used for the Atari experiments; if only one was used, the significance of the results is greatly diminished.\n- The scope of the paper is very limited, to 2 particular algorithms for Q learning with neural networks. The authors themselves state that some of their main results are \"are inherent to this family of algorithms only.\" It would be useful to at least perform experiments on a wider range of algorithms, rather than just using many task pairs.\n- Some claims seem questionable and not sufficiently backed by the experimental evaluations, such as the claim that \"The poor TL performance observed throughout this work could therefore be the result of using models where the feature extractor component of an agent, as it comes as pre-trained, is too detached from the respective final layer of the network, which is randomly initialized instead\" or \"This [positive transfer] is not due to the representations that are learned by a pre-trained network but rather because of some specific dynamics within the target MDP MT.\"\n- Broadly speaking, insufficient attention is given to background concepts and related work, such as the RL and transfer learning paradigms in general and DQV and DDQN in particular\n\nOther comments:\n\n- \"a property that allows them to successfully learn optimal value functions\" Neural networks don't typically do this, as far as I know\n- Equation 1 doesn't make sense to me- is $\\mathcal{M}_S$ the task or the learning curve?\n- Is it possible the impaired performance is due to the partial random re-initialization? Ablations with and without this would be very useful\n- I don't understand what the left side of Figure 6 is trying to show",
            "summary_of_the_review": "Overall, this paper is motivated by the interesting question of how Q functions approximated by neural networks transfer across tasks. While this is a noble goal, the paper's experiments do not provide significant new insight into this problem on account of their limited scope (only considering two algorithms), insufficient details regarding setup (number of random seeds and reward range/scaling), and simplicity (most of the ablation experiments use a single simple domain).",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper discuss why deep Q-network transfer poorly. It presents a large set of transfer learning experiments, where in each experiment the authors pre-train on one Atari task and then transfer to another. The authors convincingly show that DQN does not transfer well, and analyze the dynamics of transfer learning.",
            "main_review": "Strengths\n* The authors present a large-scale experiment, convincingly showing that DQN does not transfer well, and the negative result is specific to DQN but not deep RL. For example\b, DQV-learning on some environment can result in positive transfer. \n* The paper reports interesting observations from well-designed experiments: the control experiments show that positive transfer in DRL does not arise when source task is about the same to or more complex than target task. This could be different from what most people would expect.\n\nWeaknesses:\n* The main story of this paper appears to be a negative one. Although the authors try to analyze the learning dynamics in Section 4, the hypothesis for why DQN does not transfer well appears to be hand-wavy, i.e. \"We believe that the poor TL performance observed throughout this work is, therefore, the result of models which could not find a balance between a randomly initialized head and their respective pre-trained layers which are too biased towards the source task.\". I also don't find there is a strong analysis to support the hypothesis, not empirically, intuitively, nor theoretically.\n\nPresentation issues:\n* The paper format has problem. Headers and the place-holding author lines are missing. \n* Unnecessary acronyms like TL (transfer learning), DRL (deep RL), etc, making the paper difficult to read. I also don’t think DQV is well-known enough that it doesn’t need to be spelled out in the main paper. \n* References to figure 6 and figure 7 in the main text are interleaving. It first refers to figure 6 (left), figure 7, and then figure 6 (right). Quite difficult to read.\n",
            "summary_of_the_review": "I think the findings presented in this paper are interesting, i.e. DQN doesn't transfer well. I wasn't aware of any existing work that reported similar findings. However, I don't see too much insight or a potential fix beyond the negative result. Therefore, I'm not convinced to recommend acceptance at this point.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper empirically studies transfer in deep RL using DQN algorithms and Atari-like environments. Surprisingly, they find that transferring the feature extractor and fine-tuning the entire policy usually is outperformed by training from scratch. They conjecture that this phenomenon is due to conflicts between the transferred feature extractor and the policy head. They also contribute a group of Atari-like environments for fast experimentation, called Catch.",
            "main_review": "# Strengths #\n\n- The design of the four Catch environments, with a hierarchy of difficulty. In many existing empirical works using Atari-like environments, it is not clear which environments are harder than others, as some games are not comparable. Using a hierarchy of environments makes analyzing the relative performance of algorithms more rigorous.\n- $r$ is a good measure of transfer performance, as it is not sensitive to reward scaling.\n- Interestingly, this paper shows that positive transfer does not always occur in deep RL, and quite likely depends on specific algorithmic choices, e.g. how much of the policy is transferred and fine-tuned. The same observations can also be found in Tyo & Lipton (2020), although that paper interprets them differently and focuses on the case where features are frozen instead of fine-tuned.\n\n# Weaknesses # \n- The main conclusion of section 3.3 seems to be that positive self-transfer between the Catch environments occurs only when just the policy head is tuned. Just tuning the head is a common practice in transfer learning for supervised learning. Therefore, a natural question to ask would be whether the same practice should hold for deep RL. To answer it, we could for example rerun the experiments in section 2 and 3.2, only tuning the head.\n- Figure 6 is missing the left side.\n- Some of the claims are vague and warrant further exploration. The description of the phases at the bottom of page 8 are a conjecture drawn from figure 7; however, to make it stronger, some analysis of the features or policy head should be done. Similarly, the comment that specific dynamics of the target environment cause differences in transfer is too general and IMO is a good direction for research.\n\n# Minor #\n\n- The row/column names appear to be incorrect in table 2.",
            "summary_of_the_review": "In my opinion, the paper has potential but is not yet ready for publication. The results drawn are interesting but are not exact enough and/or require further experimentation. I believe currently the most valuable novel contribution of the paper are the Catch environments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}