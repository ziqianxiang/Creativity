{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper characterized the ReLU networks that interpolate a given dataset D in the simple setting of weakly `2-regularized one layer ReLU networks with a single linear unit and input/output dimension 1. Moreover, the authors show that to assign labels to unseen data, such networks simply \"look at the curvature of the nearest neighboring data points on each side\". This geometric description led to sharp generalization results for learning 1d Lipschitz functions. The conclusion may open some directions for future investigation, such as extending the results to deeper networks and beyond fully connected architectures.",
            "main_review": "Strong points:\n\n1. The paper is generally well organized. It is easy to understand the purpose and contribution of this paper.\n2. The literature review is clear. The relation to prior work is explained in detail.\n3. The author provides a clear illustration of ideas through a series of figures.\n\nWeak points:\n\nA.The setup in this paper is not general enough.\n\n1. It only works on Shallow Relu networks with 1D.\n\n2. (P8 Corrollary 3.3) The generalization property is only held in the domain from [0,1], which is different from the domain of f(x) in the main conclusion Thm 2.1 and Thm 3.1. In addition, x takes the value only in i/m. This makes the generalization not \"strong enough\". \n\n\nB. The theoretical contribution is not significant enough. Most parts of the proof section are concerning about technical issues about curvature and its transformation.\n\nC. Many terminologies are not strictly defined or clearly claimed. \n\n1. (p2) Network function is referred to before definition and setup. \n\n2. (p2) Concave and convex need mathematical definition, especially between concave and strictly concave. In mathematics, concave (convex) contains linear. The lack of mathematical definition makes the conclusion somewhat ambiguous.\n\n3. The paper discussed Lipschitz function several times. However, there is no definition and properties of Lipschitz function. It makes some of the conclusions not clear enough.\n\n4. The title claims, \"RIDGELESS INTERPOLATION WITH SHALLOW RELU NETWORKS IN 1D IS NEAREST NEIGHBOR CURVATURE EXTRAPOLATION\". However, the \"NEAREST NEIGHBOR CURVATURE EXTRAPOLATION\" is not clearly defined in the paper, which is not reader-friendly.\n\nSuggestions:\n\n1. Have a more detailed discussion about the theoretical contribution in the proof.\n\n2. Have more mathematical definitions about the key terminologies in the paper.",
            "summary_of_the_review": "The initial recommendation is weakly rejection of the paper.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, a one-layer ReLU network with 1-D input is considered. Authors study an interpolant of all training samples that minimizes the $l_2$ norm of all weights. A description of the function learned by such a model is provided. The description depends on specific values of training data and consists of several steps. First, intervals between two successive training data inputs are divided into several categories according to the discrete second derivative of training samples. Then, based on different categories of intervals between two adjacent training inputs, the function should be convex, or concave, or affine. Based on such description, this paper provides a generalization upper bound on Lipschitz functions when the 1-D input is uniformly spaced in [0, 1].",
            "main_review": "Although this paper shows some new results, the current presentation of this paper is not satisfactory. My overall feeling is that the setup (1-D input with 1-layer ReLU) is oversimplified and thus lacks enough useful insights on realistic problems. The following are some detailed comments.\n\n1. Minimizing the l_2-norm of all neuron weights may lack motivation. In the abstract, the authors claim that \"such networks can intuitively be thought of as those that minimized the MSE plus an infinitesimal weight decay penalty\". I highly suspect such a statement and I don't find any evidence in this paper, neither theoretical nor numerical.\n\n2. The generalization error bound shown by Corollary 3.3 is extremely restrictive, since it only works for the deterministic situation that all training inputs are uniformly spaced in [0, 1]. In reality, training input is usually random and cannot be precisely uniformly spaced. Thus, the importance of Corollary 3.3 is questionable.\n\n3. The description of the learnable space RidgelessReLU(D) shown by Theorem 3.1 is very complex and counter-intuitive. In order to determine whether a function is in RidgelessReLU(D) by the methods shown in Theorem 3.1, the discrete second derivation of every training sample has to be calculated and treated differently. Therefore, I find it difficult to determine which \"categories\" of functions belong to RidgelessReLU(D). For example, does quadratic functions always in RidgelessReLU(D), regardless of D?\n\n4. There exists no simulation result in this paper, which makes readers doubt the correctness of the conclusion.\n\n5. Fig. 2 and Fig. 3 take too much space and are repetitive, yet do not provide too much useful information.",
            "summary_of_the_review": "Although this paper shows some new results, it does not provide enough contributions and the oversimplified setup deteriorates the usefulness in providing implications on realistic learning scenarios.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies two-layer ReLU networks (with an additional residual unit) that interpolate a one-dimensional dataset with the minimum possible $\\ell_2$ penalty (not applied to the residual unit or to the biases). Existing results by Neyshabur et al. (2014), Savarese et al. (2019) and Ongie et al. (2019) have showed that these networks minimise the total variation of the derivative of the resulting predictor. This paper goes one step forward and proves a more concrete characterisation: if the curvature estimates at two consecutive training data points have different signs, then the estimator is linear; if both the curvature estimates are positive (resp. negative), then the estimator is convex (resp. concave). \n\nAs a consequence, the authors give a (relatively simple, but still insightful) generalization bound on Lipschitz functions, which is tight up to a universal multiplicative constant. The authors also note that this bound could have been obtained just from the existing characterisation, albeit with a sub-optimal multiplicative constant. ",
            "main_review": "STRENGTHS\n\nThe paper looks at an interesting problem (understanding how the predictor resulting from a two-layer ReLU network that fits the data with minimum norm of the weights looks like), and it proves a novel characterisation which is more concrete than the ones existing in the literature. As an immediate consequence, a generalization bound is also provided. \n\n-------------------\n\nWEAKNESSES\n\n(1) The paper is incremental in nature. The authors take an existing characterisation (based on the minimisation of the total variation of the first derivative), and prove a new one essentially via a case-by-case analysis. The fact that the predictor satisfying conditions (1)-(2) of Thm. 3.1 minimizes the total variation is a rather simple observation (formalised in Proposition A.13). The converse statement (any $f$ minimising total variation satisfies properties (1)-(2)) requires a more elaborate proof, but no fundamentally new ingredient seems to be necessary.\n\n(2) The characterisation is still somewhat implicit and many open question remains. E.g., does gradient descent implicitly select one of the functions characterized by Theorem 3.1 and, if so, which one? Does the same result hold if we regularise also the biases and/or the parameters of the residual unit? Furthermore, as a consequence of their result, the authors give a rather simple generalization bound. I wonder if there are any other interesting consequences of the characterisation presented here. I realise that addressing these questions is somewhat out of scope, but pursuing (any of) these directions would lead to a less incremental paper.\n\n(3) The proof of Corollary 3.3 and the discussion in page 9 appear to contain some typos and, in general, are a bit compressed. I will now elaborate on the typos:\n\n(i) How do you get (8)? There seems to be a typo here, perhaps on the RHS you have $\\epsilon_{i+1}$ instead of $\\epsilon_i$? Right now, the formula is trivial for $\\epsilon_i=0$ and it does not seem to depend on either $\\epsilon_{i+1}$ or $\\epsilon_{i-1}$.\n\n(ii) How do you get the $L^\\infty$ bound on $Df$ below Eq. (8)? Are you ever using (either here or in the proof of Eq. (8)) the property (2) of Theorem 3.1? I could not follow here.\n\n(iii) A picture would help for the visualisation of the left, right, and central portions of $D$, and for the left, right, and central versions of $f$ as well.\n\n(iv) How do you get the lower bound on the TV of $Df$ in terms of the sum of the TVs of $Df_L$, $Df_R$ and $Df_C$? Furthermore, are you sure about how $f_L$, $f_R$ and $f_C$ are defined? Right now, the changes in $Df$ in the central part ($x\\in (x_{i_*-1}, x_{i_*+1})$) seem to be repeated three times in the RHS of the bound mentioned above, while they appear only once in the LHS. Are you using any other property of $f$ to obtain this lower bound, or does it hold for any $f$?\n\n(v) Please elaborate on how you obtain also the next lower bound on the TV of $Df$ (in terms of $f_{D_L}$, $f_C$ and $f_{D_R}$). In particular, I guess that the TV of $f_{D_L}$ should actually be the TV of $Df_{D_L}$. Furthermore, I can see that the TV of $Df_R$ is at least the TV of $Df_{D_R}$. However, I do not see how one can relate the TV of $Df_L$ to the TV of $Df_{D_L}$, unless $i_*$ is replaced by $i_*-1$ in the definition of $f_L$.\n\nLet me conclude by mentioning that, if the authors need additional space to clarify the points raised above, they could just scratch the informal statement in Theorem 2.1: anyway the formal statement (Theorem 3.1) is quite similar, and it appears just in the following page.",
            "summary_of_the_review": "I am mildly positive about the paper, since its result provides a novel, simple, clean and crisp characterization. At the same time, the paper is a bit incremental and the consequences of this new characterisation are not very well explored. Also, there are a few technical issues (detailed in weakness (3)) which should be resolved.\n\nA few more typos are below:\n\n(a) In formula (1), $b_i^{(1)}$ should be $b_j^{(1)}$ \n\n(b) Page 3. \"chosen arbitrary\" --> \"chosen arbitrarily\"\n\n(c) Page 5, step 5. $\\epsilon_{i+j}=1$ or $\\epsilon_{i+j}=1$ (one of the two is $-1$, I guess)\n\n(d) Page 12. \"Finally, establish\" --> \"Finally, we establish\" \n\n(e) Statement of Proposition A.1. \"For every $i=1, \\ldots, m-1$ and $Df$ is monotone\" --> \"For every $i=1, \\ldots, m-1$, $Df$ is monotone\"  \n\n(f) Third line of the proof of Corollary A.3. $f(y_i)$ should be $f(x_i)$ ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies weakly regularized two-layer 1D ReLU networks and provide a characterization for the function fit based on geometric arguments. The authors also show that interpolating ReLU networks in the specified regime obtain the best possible generalization for learning 1d Lipschitz functions.",
            "main_review": "The paper is mostly well written and seems theoretically sound. I also believe the paper contains valuable characterizations for the induced function as a result of training one hidden layer ReLU networks on 1D data. Please see my detailed comments below:\n\n* What happens in the case of a binary classification framework. I think the same analysis should also apply to that (assuming there is squared loss + weakly regularized weights). Thus, the authors should be able to characterize the decision boundaries for such classification tasks. Could the authors comment on this?\n\n* Is there a specific lower bound on the number of neurons to have the function characterizations in the paper? If so, the authors should explicitly state this assumption at the beginning of the paper, e.g., before the statement of Theorem 2.1?\n\n* Do the authors have any intuition for (weakly) regularized training problems with penalized bias terms. I know that this is not a common case in practice but I am still wondering if regularizing bias terms makes any significant changes for the resulting function. If this is not possible, the authors should briefly explain the challenges preventing this analysis.\n\n* Is this analysis valid only for squared loss?\n\n* Could the authors explain the relation to the linear spline interpolation results in [1]? \n\n\n**Minor comments**:\n\n-At the top of page 3 $b_i$ should be $b_j$\n\n-It is better to use the standard notation, i.e., $n$ for the number of data samples and $m$ for the number of neurons\n\n[1] Ergen, Tolga, and Mert Pilanci. \"Convex geometry and duality of over-parameterized neural networks.\" Journal of Machine Learning Research 22.212 (2021): 1-63.",
            "summary_of_the_review": "The main contribution of this paper is incremental considering the previous studies and being limited to one hidden layer ReLU networks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}