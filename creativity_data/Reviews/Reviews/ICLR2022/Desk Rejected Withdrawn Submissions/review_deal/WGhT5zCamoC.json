{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents an algorithm to do sequence matching using a tokenizations of the original sequences, and evaluates it in three different settings. The test environments are non-standard setups of public databases, with a closed vocabulary of sequences, i.e. training, validation, and testing sets contain the same sequences.",
            "main_review": "Strenghts\n-------------\nThe paper approaches an interesting problem, how to effectively search over sequences such as raw audio in an efficient way, by learning quantized representations that specifically take into account the sequence similarity problem (rather than just a frame-based criterion)\n\nWeaknesses\n-----------------\nThe description of the work is very dense and often confusing. Authors employ three test sets, three baselines, and multiple variants of the proposed system, but as none of these correspond to known and established configurations, I do not have a sense if the proposed approach does well, or not. Despite that variability (and the corresponding density in the descriptions), it seems that the test set actually only contains sequences that were seen during training - so I cannot tell if the algorithm would be able to classify an unseen input sequence reasonably well, or not. This generalization ability is an indispensable requirement for a meaningful contribution to sequence classification, because otherwise we could simply do closed-set classification.\n",
            "summary_of_the_review": "The paper tackles an interesting problem, and has interesting ideas, but the presentation is too dense to be useful, the experimental setup is not standard, and the results are therefore not likely to be of interest to the general audience. I can therefore not recommend acceptance of this paper.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to speed up search for similar sequences in cases where data is continuous valued and similar sequence search is computationally expensive.  It proposes to convert continuous valued sequence to a sequence of discrete tokens.  The token vocabulary and assignment of features to tokens is constructed via k-means clustering and evolves during the model training process.  \nResults are presented on two tasks: QBSH (query by singing/humming) and TIMIT speech search and keyword spotting, demonstrating that the proposed approach yields better results as compared to baseline DTW methods, and is also more robust to input distortions.  However a comparison with previously published results is missing.",
            "main_review": "Pros:\n\na) Significant speed up in similar sequence search.\n\nCons:\n\na) The technical presentation and formulations in the paper are difficult to follow.  For instance:\n* The problem statement in Section 3.1 is incomplete.  Simply requiring that edit distance is minimized can be trivially achieved by mapping all sequences to a fixed token sequence.\n* Model training happens by concatenating encoder generated feature vectors for similar input samples (if I follow it correctly), tokenizing these vectors, and then creating prototypes for tokens.  It is not clear how this imposes the constraint that similar sequences should generate same sequence of tokens in same order so that edit distance between token sequences for similar samples is minimized.  That is, what is the inductive bias in this training procedure that leads the model to hypothesize similar token sequences for similar input samples.  Perhaps there is a lack in my understanding to the training procedure, please clarify.\n* Did the authors try approaches that more directly aim to minimize the token edit distance between similar sequences?\n* Initial centroids for the tokenizer are estimated via k-means clustering of features generated by a freshly initialized encoder.  Wouldn’t it be better to estimate the centroids (at least association of features to centroids) based on input features that are not processed by a randomly initialized network?\n\nb) In the baseline approaches\n* For MIPS it is stated that model is trained with an augmented replica.  Unclear why an augmented replica is being generated when similar pairs of samples are known.\n* In Wav2Vec2.0 approach it is also not clear how the pairs of similar samples are being utilized, how are the distractor features being obtained.  A more detailed description is needed.\n\nc) The experimental results are not compared with previously published results, making it very hard to determine the value of proposed approach.\n\nRelatively minor edits:\n* What is ’s’ in paragraph after Eq. (5)\n* z^{org} in MIPS baseline needs to be defined\n",
            "summary_of_the_review": "My rating is based on:\n\na) Lack of clarity and precision in presentation.  As discussed above, the problem statement is incomplete and the inductive bias in the proposed model needs to be made clear.  Also description of few baselines as they relate to the task of similar sequence retrieval is unclear.\n\nb) Lack of comparison with previously published results to put the proposed approach in context.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a data-driven tokenizer whose goal is to convert a continual-valued (raw) sequence into a sequence of tokens. The proposed tokenizer is trained from pairs of raw sequences and token sequences, without any pre-defined tokens.",
            "main_review": "Pros:\n+ The proposed data-driven tokenizer trained from the pairs of semantically similar sequences is novel and helps to improve the performance on sequence retrieval tasks in the speech domain.\n\n+ This paper provides extensive experiments on three tasks (query by humming, speech search, and keyword spotting) with ablation studies to prove the superiority of the proposed method.\n\nConcerns:\n- The concern about this paper is the generalizability of the proposed method. That is, the proposed method is mainly designed to solve sequence retrieval tasks in the speech domain. According to that, this paper tries to minimize the edit distance between two token sequences as the objective of the proposed method. Even most sequence retrieval tasks in the speech domain are applied to this formulation, however, all sequence retrieval tasks are not regarded as the edit distance minimization like similarity text retrieval task.\n\n- In the introduction, this paper insists that the proposed method can be used for text search methods. However, the proposed tokenizer is not applicable to a sentence or document tokenization because many tokenizers used in NLP tasks do not suppose that query and target sequences have a one-to-one mapping relationship. (e.g. subword tokenizer is not an one-to-one mapping between a raw sequence (sentence) and a sequence of tokens (subwords)). Nevertheless, the proposed tokenizer supposes that query and token sequence are one-to-one mapping.\n\n- The paper is diffciult to follow. The motivation of the proposed method “relevant landmarks appear in the same order in both sequences” is not clearly intuitive. Giving some examples would make a reader easy to understand intuitively.\n\n- The proposed method is highly dependent on pairs of semantically similar feature sequences. In a practical situation, it is questionable how to collect these “semantically” similar sequences.\n\n- The baselines used in the experiments do not utilize the pairs of similar sequences to train their models. The reviewer thinks that this comparison seems unfair because the proposed method is trained with more information derived from the pairs of semantically similar sequences.\n\n- The title of this paper is “Seq2Tok: deep sequence tokenizer for retrieval”. The title is too general to specify the proposed method because the proposed method is valid for “sequence” retrieval tasks in the “speech” domain. It would be better to change the title to focus on the main part of the proposed method.\n\n- In “Transformer as Encoder” experiments, the performances of “TokB + Trans” are worse than ones of the BiLSTM model. However, BiLSTM is required to read all sequences, but Transformer is not. It would be better to show the speed comparison.\n",
            "summary_of_the_review": "The proposed data-driven tokenizer is novel, but the manuscript is too general and experiments are not convincing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the problem of information retrieval that searches similar items to a given query. The paper introduces a new tokenizer and claims that Seq2Tok can convert a continuous sequence (e.g. audio) to discrete tokens. The purpose of the conversion is to enable easier retrieval. The paper conducts extensive experiments and claims that Seq2Tok achieves consistent performance on audio retrieval tasks. ",
            "main_review": "Main Review\n\nThe paper tries to introduce a new method, called Seq2Tok, to convert a continuous sequence to discrete tokens. The new method is evaluated extensively in the experiment section. Results show that Seq2Tok achieves impressive performance that outperforms several existing methods (see Table 1). However, the paper lacks explanations on why Seq2Tok is superior. The current draft contains many mathematic formulas and tables, but none of them are about the input data format, output format, and baseline methods. It is difficult for readers to understand the baseline methods and the contributions of the work over baselines.   \n\n\nPros:\n\n1. The paper presents a new method that converts a continuous sequence to discrete tokens to simplify similarity search. The design of the method is reasonable that takes sequence pairs to train a neural network. When the neural network is well trained, it can be used to project every continuous sequence to discrete tokens. \n \n\n2. The proposed method achieves impressive performance. According to Table 1, Seq2tok outperforms a weak baseline by a large margin. For example, Seq2tok achieves 0.84 in row TokB DTW and column Normal V, while the baseline Triplet DTW achieves only 0.278 under the same setting. In addition, Seq2tok can outperform strong baselines. For example, under the above setting Seq2tok achieves 0.84, which outperforms the 0.75 of Wav2Vec 2.0 DTW. \n\n\nCons: \n\n1.\tThe presentation of the paper is not clear. For example, table 1 is difficult to understand. Some numbers are marked in bold but the table caption does not explain why. Are numbers in bold mean good performance? The paper should explain that. Also, there are many undefined terms including “Normal”, “V”, “TS”, “PS”, “DTW” and “Compressed”. It will be good if the paper can define these terms formally in the paper or use other terms that are known by the public. \n\n2.\tThe technical contribution of the paper remains unknown. The paper lacks a background section between Related Work and Methodology. The background section is necessary because it explains the logic of baselines. Without knowing the data processing flows of baselines, it is hard to understand the technical contributions of the paper. \n\n\n3.\tThe performance of seq2tok is unstable. In table 1, it seems seq2tok does not always outperform baseline Wav2Vaec 2.0. For example, the numbers In row TokA ED are always smaller than numbers in row Wav2Vec 2.0 DTW. It will be good if the paper can explain why.  \n",
            "summary_of_the_review": "The paper has merits that introduce a new method to project a continuous sequence (e.g. audio) to discrete tokens. However, it is unclear whether the new method advances existing methods. The paper should add one paragraph in the Introduction section to articulate the contributions explicitly. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}