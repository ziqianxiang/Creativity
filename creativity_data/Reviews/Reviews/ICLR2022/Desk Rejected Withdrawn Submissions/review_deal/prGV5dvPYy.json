{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper derives a Riemannian gradient flow on the product manifold over Euclidean and SPD manifolds, where the SPD manifold is characterized by the Bures metric. The paper derives the Riemannian structure of the product manifold under the above product metric, as well as derives the Riemannian structure of probability distributions on the product manifold using the Wasserstein-2 metric with the ground metric as the above Riemannian metric. The flow is then derived via optimizing over an objective defined by kernelized MMD for which two discretization strategies are proposed. Experiments are provided on small scale datasets and demonstrate promise. \n\n",
            "main_review": "Strengths:\n1. The paper is relatively well-written and easy to follow, except for a few variable-overloads that make the reading a bit confusing after page 3. \n2. While, the empirical structure and practical motivation for this work is perhaps sub-optimal (see below), I think there is value to the proposed idea in general.\n\nWeakness:\n1. As I understand, the feature-Gaussians are defined via the push-forward of the data features \\phi(x) using their labels y. Thus, I wonder how can one treat the x (the data) and \\phi(x)_\\#v_y as independent in deriving the gradient flow? Won't moving along a flow trajectory for x already change the distribution \\mu_x and \\Sigma_x on \\phi(x)? In that sense, it is unclear to me why considering a product manifold on x and the sufficient statistics of \\phi(x) is meaningful? \n\n2. While, the paper argues for applying the proposed idea to domain adaptation problems via minimizing the MMD between source and target distributions (page 3), the experiments use only a single example from the target distribution (Section 5.2), which is perhaps too less to define any form of distribution on the target set. \n\n3. The experiments are overall very weak in my opinion, however the theoretical contributions perhaps outweigh this shortcoming.\n\nOther comments:\n1. Why did the paper chose the Bures metric for the distance on the SPD matrices? Why not consider other metrics such as Jensen-Bregman logdet divergence, KL divergence, affine-invariant metric, etc.? What are the advantages?\n\n2. Page 3, \"In the first step, we would lift ... to a higher dimensional space\". It appears that the paper uses the T-SNE embeddings for lifting? Isn't that just 2 dimensional? Why is this choice of TSNE embedding sufficient to model \\phi? Why not use some larger embedding from say some autoencoder? \n\n3. Page 3: \"In real-world settings, the conditional moments of \\phi(X)|Y are sufficiently distinct for y\\neq y' and thus the representations using (\\mu_y,\\Sigma_y) will unlikely lead to any loss of label information\". Isn't this a very strong assumption to make for the subsequent derivations? If the distributions are sufficiently discriminative, then why not just use a nearest neighbor matching for the optimal transport instead of deriving the flow? \n\n4. Eq. (3.2), what is the logarithmic map used to find the symmetric matrix? And what precisely is the V matrix in the Lypanov equation? \n\n5. Gradient and divergence: Is the d used in \\nabla_d the same as the Riemannian metric in (3.1)? The \\nabla notation is confusing, it uses \\nabla_d in one place, and the uses \\nabla_z where z is the argument at another place. Also, \\phi is overloaded as well (used as the lifting operator early on).\n\n6. Eq. 3.8 needs better explanations and notation for A(\\rho_0, \\rho_1). \n\n7. For the kernel MMD, it is unclear to me why the Riemannian geometry is preserved in the kernel embedding space. \n\n8. As for the experiments, how does it make sense to transport fashion MNIST items to improve the performance of MNIST digits? Some motivation for this will help appreciate the idea.\n",
            "summary_of_the_review": "Overall, the paper has some technical contributions that could be of interest to a diverse audience. However, the paper needs better motivations and experiments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethical issues as far as I can see.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper addresses the problem of transfer learning with limited target labels by transforming the source data into the target data. Concretely, they adopt the gradient flow to minimize the MMD distance on the feature gaussian Riemannian manifold.  \n",
            "main_review": "**Pros**:\n\nAs far as I know, this is the first paper to adopt the feature-gaussian manifold to generate target data in transfer learning. Besides, the paper is technically sound.\n\n**Cons**:\n\nThe motivation and significance of transfer learning are unclear. After carefully reading the paper, I feel rather confused about the whole proposed approach and its motivation. It is quite unclear the theoretical and practical benefits of transfer learning. \nBesides, the paper is not self-contained and several parts are rather confusing, a major revision of the paper structure and organization is highly expected.  \n\n**Conclusion** \nI do think the author introduced several new techniques but the proposed techniques are seemingly not beneficial for transfer learning. Based on this, I would like to reject the current version but encourage the author to rethink other proper scenarios.\n\n------------------------------------------------------------\nDetailed Review:\n\n- About the motivation of generation or transformation data.\n\nActually, this idea is quite popular in transfer learning or domain adaptation, based on the recent advancement of the deep generative model. For example, CyCADA [1] exactly generates the data through mitigate the domain shift. As for the gradual data transformation with time $t$, the idea is essentially not novel in the generative model. For instance, one can adopt the recent scoring function or diffusion model to gradually generate the target samples. From this perspective, the motivation or proposed perspective is not novel.\n\nIt is highly expected to state **why the proposed generation approach is important and preferable, compared with the existing approaches**.  Otherwise, it will be a plug-in idea through several complicated theories without real benefits for the transfer learning community.\n\n- About the proposed method. The significance of the proposed approach is rather unclear from theoretical and practical aspects.\n\nFrom the theoretical perspective, it does not even prove the target risk bound, i.e, how does the generation approach improve or control the target risk theoretically? How does it improve the sample complexity in the target domain? \n\nThis paper tried to propose several theoretical results such as weak convergence of the $t$, which did not provide useful information for understanding its insights. The author seems to fail to prove the convergence behavior of the proposed label projection, where is a core part of the proposed approach and requires rigorous demonstrations. \n\nAs for empirical evaluation, it is far from sufficient. First, the dataset is rather toy data and only evaluated on MNIST KMINST, etc.  The source samples are set as 200 X 10 =2000 and target 50 X 10= 500, which is almost rarely adopted in real-world practice. Therefore, the empirical contribution is rather limited. The scalability to the large scale and real-world data is quite doubtful, where real-world transfer learning occurs. Besides, the real-world transfer can have a highly label imbalanced or ill-defined problem, where some target class is missing, the behavior of the proposed approach is also questionable.\n\n- The technical details and clarity\n   \n 1. The relation of MMD and W distance is rather confused throughout this paper. The paper seems to minimize the MMD distance but why (or how) the W-distance and Riemannian geometry are also introduced? Why is important or beneficial? Why not directly solve the euclidean distance? \n\n2. In sec 3.1, why do we need to introduce the Lyapunov equation?\n\n3. Sec 3.2., there is a small typo “To define a gradient low”, -> “gradient flow”\n\n4. Eq (3.7), what is the definition of $\\phi_t$, $\\rho_t$, the coupling distribution are $\\phi$ and $\\rho$, right? There seems to be some inconsistent notation here.\n\n5. An important question is the role of Sec 3, it seems the introduction of the basics. Unfortunately, the introduction makes me more confused about the whole, due to the improper organization. (Since Sec 2 is more consistent with Sec 4) I would like to suggest the author carefully revise the background part.\n\n6. The same notation problem in Sec. 4, what does $\\nabla_{d}$ mean? What’s the meaning of the d? Again the $\\rho_t$ seem undefined?\n\n7. In Sec 5, why adopt T-SNE as $\\phi$? The idea is quite wired and lacks motivation, T-SNE is essentially not the best embedding and can be quite different when the perplexity is different.\n\n8. The assumption of Gaussian seems quite hard to satisfy in real-world practice. Even in the simple dataset in the paper, the author adopted different heuristics or ad-hoc methods such as K-clustering, which makes me hardly believe the theory and math are really effective in the real-world problem. The authors should carefully rethink the more suitable scenarios for their proposed ideas. \n\nRef:\n[1] CyCADA: Cycle-Consistent Adversarial Domain Adaptation. ICML 2018\n\n\n\n\n",
            "summary_of_the_review": "The author introduced several new techniques in generating/transforming datasets, but the proposed techniques are seemingly not beneficial for transfer learning. Although I am not an expert in geometric machine learning, the proposed approach seems to have a limited impact on the transfer learning community. \n\nThe theoretical guarantee in transfer learning is lacking (despite this paper being full of math).  The proposed method lacks a clear theory to show the improved sample complexity in the target domain, which is the core concern in transfer learning. There exists a large number of papers in data transforming in transfer learning. The author did not compare or even discuss the related work. The experiments are conducted in the toy data, where the scalability to the real-world data is quite questionable.\n\nBased on these, I would like to reject the current version but encourage the author to rethink more suitable scenarios rather than transfer learning. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Although the author missed the discussion of the Ethics concerns, this reviewer has ethics issues, where the author is highly encouraged to provide additional and detailed explanations/statements. \n\nThis paper proposed a generalized conditional data-generation approach, which can cause the negative impacts:\n\nThe transformation can induce or transfer the data bias from the source to the target. For instance, if the decision in the source is correlated to stereotype, the data transformation will surely deliver the bias to the target domain, which generally has limited samples. Generally, transfer learning is adopted in limited data scenarios, which can cause severe fairness issues for the minority group with limited data.\n\nPlease take note that ethical concerns are not the reasons for my decision. The recommendation is merely based on technical concerns, but the proposed approach can indeed cause potential fairness issues.\n\n",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper dealt with the the sparsity of labeled data and learned a metric on the feature-label space. A gradient flow minimizing the MMD was proposed for domain adaptation. Riemannian gradient of the loss function was proposed for implementing an optimal transport metric.",
            "main_review": "Pros:\n1. To handle the problem of discrete value in label, this paper represented the datasets to find the distributions in the feature-Gaussian space, and explored the detailed Riemannian structure of the feature-Gaussian manifold.\n2. Unlike the existing gradient flows almost on linear structure in the flat and Euclidean spaces, this paper developed the curved Riemannian manifold of Gaussian distributions.\n3. Some theoretical derivations were provided to support the method they developed in this paper. \n4. This paper presented the demonstration for the trace of particles from source domain/distribution to target domain/distribution. For transfer learning tasks, this paper improved the classification accuracy even in one shot setting. This paper examined the transfer of the pretrained classifier.\n\nCons:\n1. Since the axis in source sample and transformed sample were not the same in Fig 4 and Fig 5, this may cause some confusion about the performance.\n2. The comparison on accuracy over different methods is not clear and complete.\n3. The descriptions on experimental settings are not cleared. The experimental justification needs to be further strengthened. ",
            "summary_of_the_review": "This paper presents an interesting work with theoretical justification. The experimental setting and comparison can be further strengthened.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper is about transfer learning. The authors proposed a new method based on\nRiemannian geometry to transfer a set of N labeled samples into a target domain,\nwhere another set of M labeled samples are available. The overall framework is\npresented by in the flowchart in Figure 1.\n\nThe so called \"gradient flow\" is computed with respect to the optimization\nproblem at the end of page 3, which minimizes the MMD between two distributions\nin an intermediate representation space Z, corresponding to the input and output\nsamples. Thus, the novelty of this work on this intermediate feature-Gaussian\nspace and its Riemannian structure, which give efficient solution to the\ndistance minimizing problem. The method essentially takes a few steps\nalong the gradient of this MMD, where the gradient is defined with respect\nto the Riemannian geometry, and the steps are further computed by the Riemannian\nexponential map.\n\nIn the experiments, the author showed that the proposed approach can\nimprove the accuracy of one-shot or few-short learning by incorporating\nthe transferred samples.",
            "main_review": "Overall, I tend to believe this is an interesting contribution based on\nprofound math.  The science bits can meet the publication standard, and the\nnovelty should be recognized. However, I have the following concerns regarding\nthe writing and the practical aspects, which is the reason that I vote for \na weak rejection.\n\nWhile the authors did a good job in introducing the background application \nand what they are trying to achieve through the flowchart, it is not clear\nwhat is the motivation to use the representation space Z, and the MMD?\nWhat are some simple alternatives which can serve as a baseline?\nWhy the optimization problem on page 3 can achieve transfer learning?\nThe authors briefly discussed using MMD versus KL, but this is not\nsufficient without some symbolic formulations. Overall, I don't think the\npaper is well motivated.\n\nThe paper assumes the readers are already knowledgable of Riemannian geometry.\nMany concepts are introduced in a way that is not well self-contained.  ICLR is\na deep learning community. The math background can be better written to\nfit the general audience.\n\nThe proposed method depends on a hyper-parameter $T$. If seems that $T$\nis a sensitive parameter. If $T$ is too small, than the output is close\nto the input data; if $T$ is too large, than the output becomes the output\ndata. I scanned through the paper but did not find any recipe to choose\nthe parameter, or related experimental study. In any case, related\nsensitivity study should be discussed in the experimental section.\n\nFor the related work, there is a literature of Gaussian embedding in deep\nlearning that can be discussed. Bures distance has been applied in optimal transport\nand graph neural networks. I leave the author to do a more detailed literature review.\n\nThe complexity is dominated by O(N^2), where N is the number of particles.\nIn the experiments, the authors use $N=200$. It seems that the proposed\nmethod suffers from scalability issues. Therefore I have some doubts on the overall\nusefulness of this method as a practical transfer learning method.\n\n\nMinor comments:\n\nThe notion \"gradient flow\" should be introduced in more detail, when it first\nappeared in the paper. In deep learning we also have other flows such as\n\"normalization flow\". It is better to make this notion clear at the beginning.\n\n\nP1 \"We here adopt a gradient flow method\" needs citation\n\nP1 \"flow each source sample towards the target data\"\nHere, what \"flow\" means is not clear\n\nThese two paragraphs before the \"Contributions\" are not clear \nfor readers who are not familiar with the background.\nThey can be rewritten to be more self-contained with\nsome simple formulations.\n\nP1\n\"minimize a certain loss function.\"\n-> \"minimize a certain loss function with respect to ....\"\n\nfigure 1\nx_i^T is easy to be confused with x_i^\\top\n\nP3\n\"the law of conditional probabilities allows us to identify the conditional distribution...\"\nThis sentense is not clear. What is \\nu_y in any case?\n\nP5\n\"Given the Riemannian metric (3.3), ones can\"\n-> \"Given the Riemannian metric (3.3), one can\"\n\nAdd a sentence to remark on the difference between Z and P(Z)\n\nP5\nTo define a gradient low\n-> To define a gradient flow\n\nP8\n\"The kernel $k$ is thus characteristic by (...)\"\nrewrite this sentense\n\n",
            "summary_of_the_review": "Pro:\n\n- the mathematical statements and the writing are of good quality\n\nCon:\n\n- Not friendly to general reader\n- Not scalable\n- Experimental evaluation should have more detail and baselines",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}