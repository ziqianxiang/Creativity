{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors present hypotheses on the causes of overparameterization and overfitting in neural networks in support of smaller, more efficient networks. A discussion of how subnetworks within larger networks lead to overfitting and how softmax normalization can contribute to these problems is presented.  A log likelihood ratio activation and corresponding loss are proposed to use in place of softmax outputs and a proposal to use weight decay to promote smaller weight magnitudes is also made.  Snapshot based adjustment of non-weight parameters in the networks is further recommended after pruning to smaller network sizes.  These proposals are explored through a brief mathematical breakdown and empirical results on the MNIST and CIFAR datasets for fully connected and convolutional networks.  ",
            "main_review": "The paper provides strong motivations for composing networks that perform more generally and with fewer resources.  The ideas proposed are generally sound and warrant further investigation.  The results presented on MNIST and CIFAR10/100 highlight the benefits of the proposed log-likelihood ratio as a target over softmax due to a reduction in parameter count with improved accuracy.\n\nWhile small datasets are necessary to get an idea of how well the proposed techniques work, I'd prefer to see results beyond MNIST / CIFAR to provide additional support of the methods.  I believe the results presented in this paper are further too limited to draw demonstrable conclusions as made by the authors.  For example, the authors claim that relu activations lead to overfitting and that this is exposed in the paper -- I struggle to see any direct evidence of this from the results presented.  The authors note that \"overfitting caused by the ReLU function\nis more prominent than Softmax normalization\" but the context around this claim (Figure 1) does not support that -- which networks used relu vs. softmax, where was that change made, and ultimately how did performance change on those?   I'm left wondering if the distributions presented in Figure 1 left are substantially different enough to cause notable performance differences / parameter counts. \n\nThere is a large issue with the proof presented in A.1 that calls into question the authors' support for softmax normalization being a problem causing overparameterization. The normalization summation in the divisor $\\sum_{k}{\\exp{y_{0,k}+y_{1,k}}} = \\sum_{k}{\\exp{2y_{0,k}+\\beta_{1,k}}} $ is indexed on the same $k$ as $q_k$, which I believe has led to confusion on pulling out the $\\beta_{k,1}$ term for removal.  If we instead use $j$ as the index it is clear that $\\beta_{1,j}$ cannot be pulled from $\\sum_{j}{\\exp{2y_{0,j}}\\exp{\\beta_{1,j}}}$ to cancel $\\beta_{1,k}$ in the numerator without modifying the other terms in the summation.  This mistake has led to oversimplication and removal of constants which I do not think is correct and is repeated further into the second section of the proof.\n\nIt is not immediately clear to me how the work of Mishkin and Matas (2016) as referenced by the authors supports the idea that subnetworks are born of non-deterministic effects caused by weight initialization.\n\nEvidence of Loshchilov & Hutter (2018)'s AdamW algorithm increasing overparameterization would strengthen the anecdote at the end of S2.3.\n\nThe paper claims A.3 contains parameters (maybe thresholds?) for pruning but I cannot find these thresholds in the appendix.  I understand they may be proportional to the variance of trained weights but what is that proportion and how does it need to change for differing types of networks?  Was this empirically derived?\n\nFigure 3 right is a repeat of left as mentioned by the authors on the forum but unfortunately I could not see the updated figure.\n\nWeight sharing and reduction as a structural element such as convolutional networks clearly has benefits in reducing overfitting and providing trainable networks (hence deep learning), but there isn't any additional insight provided here as to what types of problems the sharing is necessary for.\n\nMinor grammatical tweaks are necessary in this paper and a proofread needs to be performed.\nNoted issues:\nS2.2: paragraph “linear correlated” -> “linearly correlated”\n“representation combining” -> “representation combined”\n\n",
            "summary_of_the_review": "Though the ideas on overparameteriation and overfitting presented here in the paper are interesting and deserve investigation, I do not believe the paper has performed a thorough enough exploration into the ideas yet.  Unfortunately I believe this is largely because the paper may be attempting to address too much in the short format and it would be perhaps more useful to focus on these ideas across multiple papers.  There are notable errors in the math presented that needs to be addressed and the empirical results presented are not complete enough to support the claims / hypotheses made in the paper.  While starting with small networks and datasets is necessary, I believe the work would be considerably strengthened with further investigation, be it into additional deeper architectures and more complex datasets or through a more microscopic view of what parameters in trained and pruned networks are doing (along the lines of Figure 1). ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper examined a few known techniques for reducing the model size of image classification models. The experiments suggest that models with the log likelihood ratio representation in the output layers perform the best comparing to the softmax baselines.\n",
            "main_review": "Strength\n1. According the authors, the code is open sourced and the experiment results were averaged over at least 10 different runs.\n\nWeakness\n1. This paper will become much readable if authors provide more details in experiment setups. It's unclear to me how the authors various the model size in figure 2-4. It's better the detailed steps to reproduce is described clearly in the paper.\n\n2. The two graphs in Figure 3 (for ResNets and DenseNets) are strikingly similar to each other.  For example, DenseNet100-bc should perform much better than ResNet56 (4.5% top-1 error vs 6.6%) in cifar10 top-1 error rate from the literature. But error rates shown in Figure 2 from these two models are both around 7%.\n\n3. The experiment section is only loosely connected to the rest of the paper.  It's unclear how techniques in Section 2 are used to generate models in the experiment section.  And the discussion and conclusion section is a bit vague and not quite based on the experimental results. For example, authors' claimed key contribution is that reducing overfitting is critical for obtaining efficient neural networks. However, it's well known that reducing overfitting is critical to every aspect of machine learning. Moreover, to study overfitting, one has at least shows the correlations between training set metrics and evaluation set metrics, which is not present in the current experiments.\n\nAdditionally, the authors claimed the snapshot-based pruning is preferred over the iterative pruning and retraining. But such claims are made without proper experiment supports.  There is no iterative pruning baseline shown in this paper.\n\n4. There are a lot of known facts and techniques that are listed in section 1 and 2. It's unclear to me what are the novel contributions from this work. ",
            "summary_of_the_review": "From the experimental results shown in the paper, I can only conclude that the log likelihood ratio representation in the output layers for image classification models outperforms the softmax counterpart. There is no enough clear description in how the models are setup and trained in the experiments. Finally, I found it hard to connect the contributions claimed by this paper with the experiments author showed. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper addresses the problem of learning efficient neural architectures: the addressed issue is relevant and of enough interest to this audience. The article is well written and understandable.\n",
            "main_review": "At p1, abstract \"weight decaying and structural regularization can also effectively reduce overfitting\": parameter regularization (often, L2) is well known no avoid a model to overfit on the training samples.\nStill in the abstract, teh authors claim that \"The second contribution is that we discovered a well-trained network without overfitting can\nbe effectively pruned using a simple snapshot-based procedure – pruning unimportant weights and connections first, and simply adjust remaining non-weight parameters using the BP algorithm.\" : it is not clear to me where in the experimental section this claim is proven.\nAt p 2 \"Overfitting may improve training accuracy, but it can cause overparameterization\": indeed, it is known that some architectures need to be overparametrized to learns specific datasets, however once the task is learned, pruning is commonly used to reduce the model footprint.\nAt p 5, sec 2.5 \"Furthermore, the underlying principles regarding the retraining procedure is not well understood\": this statement needs t be better motivated, as it is is groundless. Either motivate this claim or tone it down.\nConcerning the experimental setup in Sec 3, its main problem is that it is only lightly discussed, so it is hard to express an opinion of merit on these numbers.\nConcerning Fig 3.1, I assume the authors indicate with \"Softmax\" a reference scheme without weight decay and traditional softmax as activation function. I also assume that LLR is as Softmax, yet with LLR in place of SM and \"Weight Decay\" is akin to Softmax, yet with an (L2?) regularization component in the cost function J.\nAbout Fig1-left, it is true that LLR yields lower deviation than SM, however the difference is very small: I d like to see corresponding  deviations here.\nAbout Fig1-right, the result of the experiment is correct according to existing literature and my previous experiment, however it is as correct as predictable, so it does not add much to the existing body of knowledge.\nConcerning Fig 2, it is not clear how the authors measure the model sparsity, as they do not seem to perform any thresholding as for example in Han et al.:  did the authors apply some equivalent form of thresholding, and in the case with what threshold value ? And the dots in the graph, what do they represent ? Epochs of training ? That is confusing as I thought the proposed approach was  one-shot ? I will assume the authors bin the wights and drop those below a threshold. \nThe most interesting result seems to lie in Fig 2-right, where we see that when weight decay is employed, the LLR yields better test accuracy for marginally lower sparsity only: I would like to see tis result discussed more in depth.\nConcerning Fig 3 (including the updated Densenet), also here the graphs clearly show some advantages of LLR  over SM, that is promising, however without a bit of context on this experiment it is hard to judge on its merit. And why two different regularization factors of 5*10-4 and 10^-4 ? The authors shall focus more on this aspect of their work.\n",
            "summary_of_the_review": "All in all, this paper reads well and could be a good tutorial to get started on the topic of learning efficient models, however while some results are a bit obvious, others as the effect of the LLR function are not enough explored. I would suggest to the authors to focus on the LLR function as that s the element bearing  more novelty: I suggest the authors however to better  explain the experimental setup and discuss more in depth the results. Other aspects such as the role of regularization are well known in literature and deserve less focus.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper tries to obtain an efficient deep neural network under the assumption that the deep neural network consists of several decomposed subnetworks. To this end, various methods are mentioned to remove redundant parts of the subnetworks. The methods include using the log-likelihood ratio layer instead of the softmax, analyzing the weight decay, and model structure restriction. The paper empirically verifies their hypothesis using three small datasets.",
            "main_review": "** Strength **\n1. The paper is well-structured and the main idea is clearly demonstrated.\n2. The paper tries to interpret the deep neural network efficiency via subnetwork decomposition. The reviewer thinks this kind of approach is very interesting. \n\n** Weakness **\n1. The paper empirically verifies their idea. One concern is the experiments are performed on small datasets such as MNIST/CIFAR10/CIFAR100. The reviewer thinks the paper can be enhanced if the authors add some experiments with large datasets such as the ImageNet.\n2. The paper claimed that the complex neural network can be decomposed into subnetworks. It would be great if the paper could clearly justify this argument. E.g., mathematical backgrounds or visualizing a loss surface of the model.\n3. The authors emphasize in the main script that the efficiency of the trained network can be judged through the pruning, but related results were performed only for the MNIST dataset, and the result is also included only in the Appendix. It would be better if this part will be improved.\n",
            "summary_of_the_review": "Overall, the paper is well written, quite thorough, and well-cited. I commend the authors’ idea to interpret the efficiency of the deep neural network with subnetwork decomposition. However, the empirical experiments are performed only with small datasets, which it makes hard to determine the proposed idea can be expanded to the general cases. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}