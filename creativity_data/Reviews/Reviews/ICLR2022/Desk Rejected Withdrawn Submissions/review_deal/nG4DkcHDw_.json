{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper 1) investigates when the trade-off between the backdoor robustness and adversarial robustness of an adversarially trained network happens and 2) proposes a new backdoor defense approach (a data preprocessing technique) by taking the trade-off into account.\nThe authors found that the trade-off holds when the perturbation budget used by an adversarial training algorithm is smaller than the magnitude of the backdoor triggers injected into the training dataset by an attacker. The authors also observed that a poisoned data point usually takes more PGD steps to become an adversarial example. Based on this observation, the authors proposed a new backdoor defense approach that, before adversarial training, removes the points taking more PGD steps to become an adversarial example from the training dataset. Experiments are conducted and the results show that the defense can improve the backdoor robustness of an adversarially trained model.",
            "main_review": "The paper studied an important and interesting problem, and the finding on when the trade-off holds could benefit the field. However, the current version of the paper has the following issues:\n\n* Biased standpoint and conclusion. In one of the listed contributions in the Introduction, the author claimed that the \"trade-off claim between backdoor and adversarial robustness is not correct.\" I have read the work (Weng et al. 2020), and this is what I see: \"the trade-off exists ... in our experiments.\"  It seems that (Weng et al. 2020) only reported the \"existence\" of the trade-off instead of declaring that the trade-off is universal, and your findings actually validate the existence of such a trade-off rather than proving it is wrong. In fact, the discussion in Section 3 of this paper is very similar to the conclusion in Section 3.2 of the study (Weng et al. 2020). The presentation and the conclusions of this paper is unnecessarily confrontational, and I suggest the authors focus on \"when the trade-off holds\" from a more neutral standpoint. \n\n* Impractical experiment settings and misleading results. The entire paper tries to give an impression that the trade-off holds only under \"special\" conditions, which is not true. For example, in Figure 2, the authors show different poisoned images with different triggers. Among the 9 triggers used, only 2 triggers have largely perturbed pixels falling out of the normal perturbation budget. This gives wrong impressions and misleading results. In practice, a backdoor attacker who is aware of the trade-off will almost always use the triggers that go beyond the perturbation budget. The attacker can inject largely perturbed pixels because a backdoor trigger does not need to cover an entire image. I cannot imagine any practical reasons why an attacker would limit a trigger within the epsilon perturbation ball when it already knows that the network will be trained to defend the perturbations within that ball. The paper will be much more valuable if the authors focus on the triggers with largely perturbed pixels rather than the opposite.\n\n* Limited trigger types. The paper did not study some common trigger types such as global watermarks, space distortions, and channel shifts. The impact of the size of a trigger is also neglected. Does a low-pixel-frequency, large-sized triggers make a successful backdoor attack even if its perturbation magnitude falls within the perturbation budget of adversarial training?\n\n* Worse performance and missing baselines. Table 1 shows that the performance of the proposed defense is worse than the other two baselines in many situations (it wins only under an unrealistic setting where alpha=0.8). Furthermore, the authors only compared their work with two baselines that are both based on data-preprocessing. The study (Weng et al. 2020) has shown that adversarial training could weaken the defenses that are based on data-preprocessing and, on the other hand, enhance the defense based on neural cleansing (Qiao et al. 2019). How does the proposed method compare with this type of defense under realistic settings?\n\n* Missing discussion on defense-aware backdoor attacks. Normally, a defense paper should discuss what will happen when a backdoor attacker knows the defense. Will the proposed defense still work if the attacker tries to work around the assumption that a poisoned data point usually takes more PGD steps to become an adversarial example, for example, when it uses the PGD to perturb a non-transparent checkerboard trigger a few steps before injecting it into the dataset? \n",
            "summary_of_the_review": "Overall, while this paper presented some interesting and useful observations, I think the paper should undergo a major revision before being published in any venue. In particular, the reviewer suggests the authors to make the following changes:\n1. To present your findings from a neutral standpoint and be less confrontational.\n2. To conduct the experiments in a more practical way and focus on the triggers with larger perturbations than the budget as it is a norm rather exception.\n3. To extend the experiments by considering more types of triggers and compare the proposed method with other baselines.\n4. To discuss the defense-aware backdoor attacks and your look-ahead defense strategies.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "The work is about backdoor security but present their results from a biased standpoint.",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper assesses the tradeoff between adversarial robustness and backdoor robustness. While prior work appears split on the actual relationship, this paper attempts to settle the dispute by breaking down the correlation between two regions -- when perturbation budget is low v/s when it is high. The authors demonstrate that both the trends observe independently in these regimes and provide an intuitive justification for the same. They also propose a detection scheme and an overall method to train a model that is free from backdoor vulnerabilities. ",
            "main_review": "## Strengths\nI have listed the key strengths of the paper in the main review. Other points:\n1. For their detection scheme, the authors show how during training, adversarial attacks typically require more steps to modify the model output on poisoned samples, as compared to that on clean samples. They use this as motivation to propose their defense. The demonstration of this phenomenon is convincing.\n2. Experiments are conducted across various settings (including varying poisoning rates and types, trigger shapes and sizes, and architectures). \n\n\n## Suggestions / Weaknesses\n1. Confined to a single Dataset: All results are based on the observations of the authors on CIFAR-10. It is difficult to assess the generalizability of the results without analysis on other types of datasets.\n2. $\\ell_\\infty$ robustness is not the same as Adversarial robustness: The paper aims to settle a dispute about whether adversarial robustness and backdoor robustness are allies or not. My expectation from such a paper would be that it looks at adversarial robustness broadly -- in terms of $\\ell_2, \\ell_1$ robustness, perceptual/semantic robustness -- and not just $\\ell_\\infty$ robustness to make umbrella claims on the broader class.\n3. Comparison with prior detection strategies: The authors do not highlight why their method should be adopted in practice as opposed to prior works. Apart from a single table, we do not have much information on this front. How does this change when the value of $\\alpha$ or $p$ change? This table provides very less information because almost all methods are perfect. PGD defense typically performs worse than other forms of defense\n4. Knowing the poisoning rate appears to be a strong assumption that is not required in defenses that do not rely on detection.\n5. It would be also good to talk about the cost incurred on clean accuracy when achieving backdoor robustness via adversarial robustness (since this requires a considerably higher $\\epsilon$ threshold).\n6. Discussing prior work with similar hypotheses about the tradeoff: *\"Peri et al. (2020) claimed that adversarially trained feature extractors yield robust features resistant against clean-label data poisoning attacks such as feature collision attacks (Shafahi et al., 2018) and convex polytope attacks (Zhu et al., 2019) in the transfer learning. Zhu et al. (2021) indicated that AT encourages DNNs to be locally constant in the neighborhood of correct data and prevents them from memorizing the corrupted labels.\"* There is no further discussion about the same apart from this brief paragraph.\n\n## Questions\n1. *However, as the perturbation budget exceeds some threshold and is able to modify the trigger pattern, the backdoor trigger becomes a non-robust feature instead. Thus, AT hinders the model from learning the backdoor-related feature.* ——> How does this happen when the perturbation budget is still less than the transparency $\\alpha$? (Since this is an $\\ell_\\infty$ attack and not $\\ell_2$)\n\n2. Why call it PGD defense when there is no projection operation (apart from the standard [0,1] constraint)\n\n3. Appendix C, Figure 9 is incomprehensible\n\n\n\n## Writing\n1. Meanwhile, we also find that when AT mitigates backdoor vulnerability, the minimum number of PGD steps is unable to identifying poisoned samples (see Figures 5(c) and 5(d)) anymore.  —> identify\n\n",
            "summary_of_the_review": "The paper does provide some interesting findings of the relationship between backdoor attacks and adversarial robustness -- particularly by breaking down the observations into different scenarios. However, the overall work seems underprepared for a conference submission -- such as the limitation of results on just one dataset, or projecting $\\ell_\\infty$ robustness as *adversarial robustness* in general. There is little discussion/comparison with relevant works that have also suggested that there is no tradeoff between adversarial robustness and backdoor robustness. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigates the relationship between train time (backdoor/poisoning) and test time (adversarial perturbation) attacks on neural network classifiers. In particular, the study is motivated by conflicting results about whether adversarial training (AT) increases or decreases the effect of data poisoning. The authors find a relationship between the perturbation budget used in adversarial training and the size of the backdoor trigger; when the budget is smaller than the trigger, AT increases the effect of poisoning, while when the budget is bigger it decreases the effect. They hypothesize that this difference is due to the trigger being a robust feature for small budgets and a nonrobust budget for larger budgets, and experimentally verify this by computing the number of steps needed to successfully attack inputs in both settings. Finally, they propose a method for detecting poisoned inputs using this observation and show it is successful at producing classifiers that are both adversarially robust and resistant to poisoning.",
            "main_review": "Overall, I think this is a strong paper. The writing clearly flows from the empirical question to the experiments and then the proposed methods. The motivation is strong, as there is significant interest from the community in both adversarial perturbation robustness and backdoor robustness. I am not so familiar with the poisoning literature but from what I can tell the observation the authors make is novel.\n\nI appreciated that the authors showed that their main observation—the effect of the relationship between AT budget and trigger size on backdoor success rate—was robust across poisoning rates, trigger types, model types, and various other covariates. I also thought the results shown in Figure 5 were great to back up the hypothesis that adversarial training latches on to the robust feature of the trigger. The authors made a helpful connection to Ilyas et al. (2019) in regards to this.\n\nThe PGD defense method was clearly explained and the results seem strong. Since all defenses performed comparably across the experiments shown in the paper, I am not sure how much better or worse it is than existing methods. It might be helpful if the authors show more cases where there is greater variation among the methods, although it could be that in cases not shown the effect of poisoning is so small that detection is not necessary. Regardless, I think the PGD defense and proposed training pipeline reinforce the findings of the rest of the paper and seem like valuable tools.\n\nThe paper could be strengthened further if the authors could explicitly/mathematically characterize the relationship between adversarial training budget and backdoor magnitude. Is there some measure of the size of the backdoor trigger such that one can calculate the AT budget above which the poisoning attack will stop working?\n\nThere are also some small grammatical mistakes throughout the paper. I don't think they distract too much from the writing but it might be worth proofreading, running the paper through a grammar checker, and/or asking a native English speaker to look over it if the authors are not already.",
            "summary_of_the_review": "This paper demonstrates an interesting and important empirical phenomenon through extensive experiments. The findings are novel as far as I know, so I recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the relationship between adversarial robustness and backdoor robustness.\n\nExisting work by Weng et al. (2020) demonstrates that there is a trade-off between adversarial and backdoor robustnesses. However, this paper finds that such a trade-off only occurs when the perturbation budget of adversarial training is relatively smaller than the magnitude of the backdoor trigger. Indeed, as the adversarial robustness increases, the backdoor robustness will first increase and then decrease.\n\nBesides, the authors also find that compared to clean data, backdoor data are usually farther away from the decision boundary.\nAs a result, it would take much more steps to generate adversarial examples from backdoor data with PGD.\nBased on this observation, a new backdoor attack detection algorithm is proposed based on PGD, in which an example that takes more PGD steps for generating adversarial examples is more likely to be detected as a backdoor example.",
            "main_review": "First of all, I think the topic of this paper is not interesting. Many works [r1, r2] have already shown that adversarial training can break poisoning attacks. Since the backdoor attack is a specific type of poisoning attack, it is not surprising that a strong adversarial perturbation budget can make the backdoor trigger invalid.\n\nBesides, current experiments results do not suggest that the proposed backdoor detection algorithm (Algorithm 1) is more superior to existing methods. For example, in Table 1, the proposed detection algorithm only beats the existing methods when the poisoning rate is $5\\\\%$ and the $\\alpha$ factor is $0.8$. It is recommended to conduct more comprehensive experiments (for example, conduct experiments with more different $\\alpha$ factors) to verify the effectiveness of the proposed algorithm.\n\nOther comments are listed as below:\n- Algorithm 2 is just almost a standard adversarial training algorithm, which is redundant and can be removed.\n- There are too few backdoor detection baseline methods for comparison. It is recommended to involve more baselines in the experiments.\n- In Algorithm 2, an amount of $1.5p \\cdot n$ samples are identified as backdoor data and removed. I am wondering why the $1.5$ factor here is proper?\n- From Figure 5, it seems to indicate that when the $\\alpha$ factor is $0.2$, the proposed detection method could not effectively detect backdoored examples? What about the detection performance of other baseline methods in this situation?\n\n**References**\n\n[r1] Fowl et al. \"Adversarial Examples Make Strong Poisons\". arXiv 2021.\n\n[r2] Tao et al. \"Provable defense against delusive poisoning\". arXiv 2021.",
            "summary_of_the_review": "Based on the previous comments, I think this paper lacks novelty and does not meet the requirement of ICLR. Thus, I recommend rejecting this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}