{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this work the authors describe in some detail a recently published dimensionality reduction method coming from the computational physics community, and they test it on two toy datasets: a low resolution hand written digits dataset and a trajectory of a small molecule.",
            "main_review": "The method described in this work is interesting and convincing, the results presented are encouraging, and the manuscript is fairly well written.\n\nHowever, in my opinion the following issues clearly make the manuscript unsuited for publication in ICLR:\n\n1) The method presented and tested was published on another journal more than one year ago. It is *not* novel.\n\n2) Point 1. alone would not completely hinder publication if the method was to solve a challenging and pressing problem. But this is not the case: the model is tested solely on toy datasets.\n\nMoreover, I believe that the exposition is too much tailored for an audience of physicist with background in molecular simulation. In fact, the manuscript focusses on many community-specific challenges such as  collective variables estimation, enhanced sampling, biasing of molecular dynamics simulations and so on, which, albeit very important, are of limited interest to the typical reader of ICLR.",
            "summary_of_the_review": "In spite of the correctness of the results, and of the appeal of the  method used, I think this article is not suited for publication for the severe lack of novelty both of the presented method and of the empirical findings reported.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper applies a similarity based method to learn a low dimensional representation from potentially high dimensional data. \nThe approach is validated over 2 datasets: digits, and a physical dataset.\n",
            "main_review": "The paper proposition aims at aligning a multiscale representation of the data computed using predefined transformation (Gaussian similarity) of the observed features .\nThe construction of the method is clear and straightforward despite some discussion on the features $x$ and the parameters $\\alpha$ could be interesting and help the understanding of the method.\nMoreover, the experimental settings, seem to confirm the authors choices.\n\nHowever, despite being interesting from a statistical mechanic standpoint, I  don’t really understand the influence of section 2) in the construction of the method. At least, from a machine learning perspective. Indeed, despite being interesting, the method can be presented as matching two symmetric matrices accounting for the data distribution, thus making section 2) a little cosmetic in a venue such as ICLR.\n\nLimitation: The construction of the training data mainly follows [2] and the general method only slightly differ from [1] and t-SNE method.\nQuestions :\n- How does the training method scale with the training set size ? What is the cost of computing the whole laplacian? \n- Since the mapping $f:x\\mapsto z$ is learned by a neural network, I don’t get in what sense the representation is stochastic.\n- Some more details on the influence of $\\alpha$ in eq 9 would be interesting, as it defines the relative importance of both datum $x_i,x_j$ on the data\n- Despite the method being interesting and quite unsupervised, can the author detail on the robustness of the method to noise ?\n- Can the authors compare their method to t-SNE ?\n- How would the method perform on higher dimensional data such as pixel-pendulum or sea surface temperature data ?\n- What is the influence of the $D_W$ term in the training ?\n\n\n[1] Multiscale Reweighted Stochastic Embedding: Deep Learning of Collective Variables for Enhanced Sampling (Jakub Rydzewski and Omar Valsson)\n\n[2] Diffusion maps (R.Coifman, Lafon)\n\n",
            "summary_of_the_review": "The proposition amounts to t-SNE based on NN to enable latent state estimation. \nDespite being grounded and interesting, I find the novelty in the approach or the results limited for a venue such as ICLR.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper discusses a recently proposed method, Multiscale Reweighted Stochastic Embedding (MRSE), that learns a low-dimensional representation of a quantity of interest (ML dataset, physical system, etc.).",
            "main_review": "The paper addresses an interesting topic, namely finding low dimensional representations of high dimensional data. The authors look at the recently proposed MRSE method [RV_2021]. The authors state that their goal is to demonstrate the ability of MRSE to learn low-dimensional representation of high dimensional data and for that purpose two numerical examples are presented.\n\nThe main issue that I have with the paper is the high degree of similarity with respect to the [RV_2021] paper. Bar the UCI Handset example, everything else in this paper is also found in the [RV_2021] paper. More so, the [RV_2021] paper contains a more detailed description of the involved concepts and of the approach than the current one. Consequently, the current paper does not bring sufficient new information to warrant acceptance.\n\nI understand that it is important to share ideas found in other fields as it can foster progress, however, the paper should also present sufficient original material. An example of such original material would have been a comparison with other [state of the art] methods to perform dimensionality reduction. \n\nRV_2021 - Jakub Rydzewski* and Omar Valsson*, Multiscale Reweighted Stochastic Embedding: Deep Learning of Collective Variables for Enhanced Sampling, J. Phys. Chem. A 2021, 125, 6286−6302",
            "summary_of_the_review": "Paper too similar to existing body of work, does not bring enough new information/knowledge to warrant acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "Yes, Other reasons (please specify below)"
            ],
            "details_of_ethics_concerns": "The main issue I have with this paper is that it is too similar with an existing body of work, https://pubs.acs.org/doi/pdf/10.1021/acs.jpca.1c02869.\n\nThe authors do cite the similar paper and they correctly identify that they are just demonstrating the ability of the method proposed in the paper they are citing. However, I find a bit disturbing to have 4 pages of content out of a total of 6 that mainly reproduces the work in the cited paper. Plus, in the remaining 2 pages covering numerical examples, one of the presented examples is also found in the original paper.\n\nI don't know if I can call this plagiarism, as they do cite the original paper and they identify their contribution, however, the similarity with respect to the original paper is very disturbing for me.",
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The work investigates a new technique to learn low-dimensional representations of noisy data; the utility is demonstrated with handwritten digits and biological measurement data. ",
            "main_review": "This work focuses on improvements of a previously proposed model in terms of noise treatment, and aims to suggest improved ways to search for low-dimensional representations of noisy real world data across various domains. The novelty focuses on the application-oriented adjustments of an earlier method, and performance demonstration with standard data sets.  The reporting is clear but could benefit from more extensive intuitive justification of the choices in the methods section. The benchmarking tests seem promising performance but given that the main novelty focuses on the applied use, the work could benefit from more extensive benchmarking tests.",
            "summary_of_the_review": "Strengths: \n- Addresses an important problem of noisy data representation\n- Contributes new analysis of noise treatment in stochastic embedding\n- Using two different application domains provides a more comprehensive analysis on the applicability of this method\n\nWeaknesses\n- Limited methodological novelty; focuses on analysing applied performance of a previously proposed method using standard examples\n- Relatively little space is given to the applications & benchmarking, although this is where the main contributions are supposed to be\n- Ethics statement is very limited\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Other reasons (please specify below)"
            ],
            "details_of_ethics_concerns": "Authors only cite external ethical guidelines without reflecting how these related to this particular work.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}