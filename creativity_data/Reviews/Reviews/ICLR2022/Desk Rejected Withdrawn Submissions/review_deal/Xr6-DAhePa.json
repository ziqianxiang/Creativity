{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The proposed SSL compares the positive and negative samples by maximizing the mutual information between the representation of the first view and the latent variables (i.e. cluster indices), while minimizing the mutual information between the other view’s representation and the latent one. Experiments on different types of classification architectures confirm that the proposed approach is better than existing one, including SwAV. ",
            "main_review": "**Strengths**\n* Experiments on both CNN and ViT show that the proposed method has a potential to improve the existing approaches. \n\n\n\n**Weaknesses**\n* I failed to understand the motivation of the proposed objective clearly. The proposed method has a component to minimize the mutual information between the other feature and the latent variable. However, the role of this part for the final performance is not that clearly described. In my opinion,  this is counter-intuitive in some sense, because having a good representation of an image means that the representation well explains the augmented images from different policies, unless the semantic of augmented images is very different from the original one. \n* It seems that the main component of the proposed approach is GMM. However, the importance of the cluster assignment in the latent space has been extensively explored in SwAV. Interpreting this as the Bayesian framework is straightforward, and I’m not sure how much this interpretation improves the final performance. \n* In experiments, the authors may consider comparing the proposed approach with MoCo v3, which greatly improves the performance of SSL algorithms on ViTs. \n* It seems that the manuscript is submitted in a hurry, it definitely needs to be revised significantly. I’ve found several missing references and many errors. Below is some examples: \n  * The broken reference exists even in the first line in the introduction. In addition, I’ve found the duplicate sentences, “Features from different augmentations are forced to infer the other one and the latent variable together.”, in the introduction.\n  * In “B also proposes to existing…”  in the related works, the reference to B is missing. \n  * The reference to figure 1 cannot be found in the main text. \n  * In the derivation, the authors assume that there exists the conditional independence of z_1 and c given z_2. However, it is not clearly shown in Figure 2, because this is not a formal graphical model. \n  * In the first equation in Eq. (4), c is missing in the condition. ",
            "summary_of_the_review": "This paper improves the existing SSL methods in linear evaluation and low-shot classification tasks. However, the motivation on the method is not that clear, and the manuscript should be revised. The current form is not ready for publication. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a method for self-supervised learning based on the information bottleneck principle. Formally the objective to maximize is as follows: I(C, Z1|X) – beta * I(C, Z2|X), where I() denotes the mutual information, X is an observation, Z1, Z2 are features generated from different augmentations of X, C is a categorical latent variable, and beta a real-valued hyperparameter. The features Z follow a Gaussian Mixture model with C as a latent variable. The main motivation behind this formulation is to reduce representation redundancy between different augmentations of a given observation. The authors experiment with ResNet50 and VisionTransofmer (ViT) encoders to infer the features Z, and they evaluate the proposed method on various tasks, including Image classification, object detection and segmentation. The experiments also include some qualitative results and an ablation study to assess the impact of some components (e.g., the beta hyperparameter) of the proposed method. ",
            "main_review": "Strengths.\n- Relying on the information bottleneck principle to design a self-supervised framework is interesting.  Reducing the redundancy between different features of the same observation is of a great interest from a representation learning perspective. \n \n- Empirical results show that the proposed method outperforms the chosen baselines in several cases. \n\nWeaknesses.\n- The writing of the paper requires substantial improvement owing to language and typo issues.\n\n- The paper is hard to follow. The intuition behind the proposed method and several claims are unclear. For instance, it is not clear what kind of redundancy the proposed method is trying to reduce. From the objective function, Z1 is encouraged to explain the clustering variable C, while Z2 is encouraged to be independent of C. However, nothing is preventing the model from encoding information (other than C) in Z1 and Z2 simultaneously.\n\n- The current title is misleading and should be revised to be consistent with the content of the paper.\n\n- The experiments show that the proposed method outperforms the chosen baselines on various tasks. However, the results do not support the main claim of the paper regarding redundancy reduction. I would recommend running targeted experiments to demonstrate that the proposed method does indeed reduce redundancy, which has yet to be defined clearly.\n",
            "summary_of_the_review": "The idea of relying on the information bottleneck principle to develop a self-supervised learning framework is interesting, however there are major concerns with the paper writing, method clarity, and more experiments are required to support the main claims of the paper.  ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper tackles an important problem in self-supervised learning of image\nrepresentations: How to learn, from different random augmentations of the same\nimage, representations that capture the semantic redundancy but ignore the\nappearance redundancy due to different augmentations being relatively similar,\ncompared to real world variability. This is an interesting and important angle\nto the contrastive learning problem, since the augmentations are the only\navailable source of supervision. In this paper, the authors propose an approach\nbased on applying the Information Bottleneck (IB) principle between two views of\nthe same image. Intuitively it makes sense to apply the IB to extract only what\nis common between different views of an image. The authors also propose to model\nthe latent embedding space as a GMM, which also seems like a reasonable\nmodel. However, to the best of my understanding, the way the authors apply the\nIB principle here requires further clarifications.\n",
            "main_review": "The main issue that I see with this paper is that the application of IB is\nnon-standard, and thus should have been more elaborately justified. If C is the\nlatent class, and Z1 and Z2 two representations obtained from images of class C,\nthe basic objective is to \"maximize\" $I(C,Z1)-\\beta I(C,Z2)$ (page 2, note\ntypical IB is formulated as minimization over one latent variable between input\nand output). I understand the intuition that class C should be the only\ninfomation shared between Z1 and Z2. However, the authors developments arrive to\nan objective function that seems counterintuitive: in Figure 1, and final Loss\n(17), authors seem to suggest the combination of 1) penalizing the agreement of\nlatent class predictions between the two different views, and 2) matching the\npredicted latent class from one view to an uninformative class prior. I fail to\ngrasp the intuition on why this combination leads to useful\nrepresentations. Puzzlingly, the experimental results show that it does.\n\nI tried to follow the authors formulations arriving at their objective, to the\nbest of my understanding. Here is a list of technical points that were very\nconfusing, at least to me:\n- In the graphical model in Figure 2, and in (2), Z1 depends only on Z2 and not\n  on X. This seems like a very strong modeling assumption that in my opinion\n  should be better justified.\n- I think (3) is missing Expectation over x in the l.h.s.  Same for many\n  subsequent equations (5, 11, 13, 14, 15). Maybe for clarity it is better to\n  omit this sum over x?\n- (4) drops the dependence on $c$ in the first equality, which is confusing because\n  $z_1$ should depend on $c$  and not $x$ according to the proposed graphical model.\n- In (7) $p(c)$ drops the dependence on $x$ in the r.h.s. Why?\n- I might be missing something but in the r.h.s. of Equation (12) the\n  multiplication of $p(c_i)$ in the numerator and denominator seems\n  problematic. Unless the $i$ in $p(c_i)$ is fixed, in which case the numerator\n  is not the marginalizing sum.\n- In (15), it is puzzling that the dependence on $x$ is dropped for $q(c)$, the\n  variational approximation to $p(c|x)$. I think the role of $q(c)$ in general\n  requires more elaboration, since it is part of the final loss. How it is\n  actually implemented?\n- Modeling the latent space as a GMM seems like a good idea, but requires\n  knowing the number of classes. I think the paper could use more detail on how\n  the GMM is fitted.\n\n\nA positive point of the paper is that the experimental results seem very\npromising. I only wished the method was better presented to fully grasp where\nthis improvement is coming from. Also, it should be noted that the competing\nmethods do not require the number of classes.\n",
            "summary_of_the_review": "The application of the Information Bottleneck principle to the self-supervised\nproblem seems like a good idea but I think the current version of this work\nrequires several justifications and clarifications as I listed above.\n\n I'm looking forward to the authors clarifications in their rebuttal.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Review comments:\n\nThe paper considers incorporating the variational bottleneck principle into the self-supervised learning framework. The resultant loss function turns out to be the cross entropy term of augmentations regarding the similarity score prediction between an updated component mean c_i and the augmentation feature itself. \n\n",
            "main_review": "Strength: empirical results show superiority of the proposed method. The estimated variance $\\sigma_i$ seems to be an adaptive temperature given the current variational parameter estimate. This gives me confidence that the method could be potentially superior than the SOTA methods, as SOTA method usually use a fixed temperatures. Although I think the resultant loss function shares spirit with [a], in which the distributional discrepancy between the similarity prediction score of two augmentations are penalized in KL term (versus. your cross entropy term between cluster mean and augmentation), I admit that these two works are motivated differently ([a] aims at supervised learning). Also, I think the clustering idea is very reminiscent of SwAV, although the loss is now modulated with adaptive temperature given the uncertainty measurement. \n\nWeakness: 1. It seems the method needs to update of the $c_i$, $\\sigma_i$ and $q(c)$ iteratively. How do you update these parameters in an end to end training manner? Does this increase your training complexity, how is the complexity compared to SwAV and DINO (e.g., in hours)? I think you could definitely hide most of your derivations into the appendix, whereas the space shall be left for the update rules in a succinct manner. This might significantly help improve the clarity and readability of your paper.\n\nWeakness: 2. Training batchsize of the proposed method is not specified in the Table 1. This might cause unfair comparisons between the proposal and SwAV, DINO (which I think essentially is very close to this SVIB method) due to the sensitivity of these SOTA method regarding batchsize.\n\nWeakness: 3. In my personal opinion, I suggest that the authors reconsider the claim: \"In this paper, we make the first attempt to remove the redundancy between augmentations with the introduction of the information bottleneck\", since SOTA method Barlow Twins [b] has explicitly discussed the impact of feature redundancy for the self-supervised learning tasks. \n\nWeakness: 4. The presentation quality needs improvement. \n\nIf possible, please clarify my concerns in your rebuttal. \n\n[a] Mitrovic et al., “Representation Learning via Invariant Causal Mechanisms”  ICLR 2021.\n\n[b] Zbontar et al., \" Barlow Twins: Self-Supervised Learning via Redundancy Reduction\", ICML 2021. \n",
            "summary_of_the_review": "I think the method introduced the adaptive variance term into the objective, which intuitively could improve the method over the SOTA method. The method also shows empirical significance over the SOTA. I therefore recommand weak acceptance of the paper. But still, please clarify my concerns during the rebuttal time. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose a self-supervised variational information bottleneck framework which relies on redundancy reduction via entropy minimization.\nA Gaussian mixture model is learned to approximate the variational posterior distribution. \nThe model is shown to deliver state-of-the-art performance on a range of tasks. \n",
            "main_review": "The idea the authors propose is interesting as far as I understand it. However, unfortunately, I found the paper difficult to read and I am not sure I have fully understood the argument the authors make. This is in part due to (in my opinion) unclear writing and sloppy mathematical derivation. \n\n__Method:__\nI found the paper misses a proper set up for the problem it needs to solve. It assumes that the reader is very familiar with all relevant work. It would've been helpful to have a more detailed description of the problem setting and desiderata of a good method. \n\nSection 3 that introduces the methodology starts by pointing to Fig 2 that I find very confusing. After looking at the conditional independencies used later in the text, I assume that the 3 non-annotated arrows describe a DAG which in turns denotes the dependence structure, but the annotated arrows (min MI, max MI) have no meaning in terms of the graphical model. Given that the arrows have the same design this is very confusing and requires a lot of parsing from the reader. \nThe rest of section 3 just consists of a list of derivations, without any motivation as to why these calculations are carried out. I would’ve liked more information towards what the authors want to achieve. \n\nI have tried to follow the derivations, but I could not verify every step. For example, in equation (4) the authors first write $p(z_1 | c, x) = \\int p(z_1, z_2 | x) dz_2$ where $c$ disappears on the RHS. It appears later again, but then $x$ vanishes and we obtain \n$$\n  p(z_1 | c, x) = \\int \\frac{p(z_1 | z_2)p(c | z_2)p(z_2)}{p(c)}dz_2.\n$$\nDoes this imply that $z_1 \\perp x | c$? I tried to see whether I can derive that but only ended up with \n$$\n  p(z_1 | c, x) = \\int \\frac{p(z_1 | z_2)p(c | z_2)p(z_2|x)}{p(c|x)}dz_2\n$$\nusing the factorization (2). If the authors could clarify this I would appreciate it. \n\n__Novelty:__\nThe proposed approach is new to me (i.e. I can see it as a small but new extension of previous methods), but I don’t know the relevant literature very well. \nA discussion of Zbontar et.al 2021, Barlow Twins: Self-Supervised Learning via Redundancy Reduction seems useful.\n\n__Experiments:__ \n- The experiments show good results throughout, which supports the method. \n- Why are the authors reporting the mean accuracy over 5 runs, but not the standard deviation?\n\n\n__Writing:__\n\n- The manuscript suffers from many typos, unusual phrasing and inconsistent writing:\n-I don’t see how $Z_1 \\leftrightarrow Z_2 \\leftrightarrow C$ is a Markov chain (the arrows go in both directions and Markov chains are usually thought of as sequences on the same space to make sense of notions such as equilibrium, convergence etc. \n- Equations are missing proper punctuations, e.g. full stops\n- The use of $p(X)$ and $p(x)$ is inconsistent. The authors should choose one of those. (If capital letters are intended to mean random variables and lower case for their realization, I would recommend using lower case, $p(x)$). \n- Same is true for random variables in expectations, e.g. equations 3, 5, 6 etc. where random variables are denoted with lower case letters inside the expectation operator, but with capital letters in the MI. \n\n_Minor:_\n- Specifically, We apply apply Gaussian Mixture Model (GMM) -> Specifically, we apply a Gaussian Mixture Model (GMM) \n- Existed methods -> Existing methods\n- Moreover, directly calculate the -> Moreover, directly calculating the\n- Hjelm et al. (2018); Bachman et al. (2019) learns -> Hjelm et al. (2018) and Bachman et al. (2019) learn \nand employee inpainting -> and employ inpainting\n",
            "summary_of_the_review": "The proposed method has an interesting underlying idea, but the writing makes it very difficult to follow. While the results are generally good, I don't feel the paper would be helpful to a wide audience in it's current state, which is why I cannot recommend accepting at the moment. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}