{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents a deep learning based model for multi-step forecasting in multi-variate time series data. The authors assume presence of temporal covariates as well. The proposed architecture has components that are motivated from causal inference. The model operates in a sequence-to-sequence setup, where it encodes a window of observations from the past and, subsequently, decodes it to produce the output window of predictions (forecasts). Results on four different time series forecasting tasks show improvement over the state of art.",
            "main_review": "The paper presents a deep learning based model for multi-step forecasting in multi-variate time series data. The authors assume presence of temporal covariates as well. The proposed architecture has components that are motivated from causal inference. The model operates in a sequence-to-sequence setup, where it encodes a window of observations from the past and, subsequently, decodes it to produce the output window of predictions (forecasts). Results on four different time series forecasting tasks show improvement over the state of art.\n\nIn general, the idea is interesting, and the authors have sought inspiration from the causal inference literature to justify their architecture, which is reasonable. The proposed architecture does appear to give some performance boost on the data sets used in this paper. \n\nThe paper has following weaknesses, in my opinion:\n1. The connection between the model and the causal inference scheme in Section 3.3 is very unclear. The relationship of the discussion to the proposed framework in 4.1 is not clearly explained.\n2. Mathematical notations are introduced without explanation in several places, e.g., do(R), `n' in Section 3.1, etc.\n3. I did not quite understand the practical use of a forecasting system that requires the knowledge of covariates for the forecasting horizon as well. In my opinion, this significantly limits the applicability of such a method, and it is not clear where it would be applicable. Somewhere in the experimental results, the authors mention that the covariates for future can be inferred from the past, but it is not clear if that is specific to the related works or for the proposed method. Moreover, it is unclear why it is justified to assume that the covariates can be inferred from the past. \n4. The paper is lacking details about the data sets, which makes it hard to understand the relevance of Figure 5.\n5. The paper has several grammatical inconsistencies that should be fixed.",
            "summary_of_the_review": "The paper has an interesting idea of using concepts from causal inference to motivate a deep learning model, however, the presentation is unclear and there are some issues with the experimental evaluation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper addresses the problem of multi-horizon and multi-time series forecasting. It proposes a causal inference perspective using structural causal modeling, and introduces the notion of confounder as common sense knowledge. They create representations with recurrent neural models and attention modules (self-attention and transformer-like attention) to extract local and global information and suppress noise and unnecessary information. Then, they parametrize the probabilities found in their causal modeling with neural networks to finally predict a prediction interval for each target in the future. They present results in benchmark datasets, compare with state of the art baselines, perform ablation studies, and explore the interpretability benefits of their attention modules.",
            "main_review": "Strengths:\n- The paper proposes an interesting causal inference perspective that naturally accounts for some information that hasn’t been explored which is learning long-term patterns under the umbrella of a confounding variable (what they refer to as common sense knowledge). \n- The methodology is well explained and motivated. \n- The selected baselines and datasets are adequate for demonstrating the paper’s hypothesis.\n\nWeaknesses:\n- The paper doesn’t seem to provide a good representation of the previous work of causal inference in time series (see details below). This raises doubts on the contributions of the paper.\n- Evaluation doesn't clearly demonstrate what are the practical benefits of the approach (see details below).\n\nContribution wrt previous work:\n- A quick search in Google scholar for causal inference in time series shows multiple results, including one survey paper [1], which cites several papers in this domain. However, there is no discussion on what these do and how his paper builds on them. If the authors can clarify the contributions of the paper, this would make it pass the acceptance bar (given that the evaluation point is also addressed -- see below).\n- Figure 1 introduces an interesting causal relationship, but, as the related work in causal inference for time series is not summarized, I am not sure how much of this is new. \n- Pattern attention seems to be borrowing ideas from functional neural processes [2] for selecting relevant instances at inference. I’d suggest making that connection to add value for the reader. See [3] for a time series forecasting example of this idea.\n\nEvaluation comments/issues:\n- Their results show their method outperforms the baselines by a small margin, and they do not present any confidence intervals. When the authors refer to these results, all they say is that they achieve the best performance thanks to the causal inference. Therefore, it is fair to ask: what are the practical implications of achieving the best performance with such a small margin? Is improving 0.001 of P50 loss in the traffic dataset significant from a practical standpoint? If not, then the value of the approach needs to be addressed in a different way. If the authors present confidence intervals along with a discussion of *practical* significance of their results, the results section would be in good enough shape to pass the acceptance bar.\n- Why should we focus only on q-Risk? Are metrics like RMSE and CRPS (and even correlation coefficient) not relevant? The premise of the paper is multi-horizon forecasting, therefore, the evaluation should be broader. While not a dealbreaker, if you run the baselines by yourself, this step should be trivial and would considerably strengthen the results section.\n- Why are not all baselines run in all datasets? Couldn't find a sentence explaining this. \n\n[1] Guo, Ruocheng, Lu Cheng, Jundong Li, P. Richard Hahn, and Huan Liu. \"A survey of learning causality with data: Problems and methods.\" ACM Computing Surveys (CSUR) 53, no. 4 (2020): 1-37.\n\n[2] Louizos, Christos, Xiahan Shi, Klamer Schutte, and Max Welling. \"The Functional Neural Process.\" Advances in Neural Information Processing Systems 32 (2019): 8746-8757.\n\n[3] Kamarthi, Harshavardhan, Lingkai Kong, Alexander Rodríguez, Chao Zhang, and B. Aditya Prakash. \"CAMul: Calibrated and Accurate Multi-view Time-Series Forecasting.\" arXiv preprint arXiv:2109.07438 (2021).\n",
            "summary_of_the_review": "The algorithms and method are technically sound and contribute towards better modeling of time series. However, the lack of discussion of related work raises doubt on the novelty of the technical contributions to the approach. In addition, the evaluation is not demonstrating the practical benefits of incorporating the causal perspective that was presented (method outperfoms baselines by a small margin, no confidence intervals presented, no discussion on the practical significance).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper describes a method for time series forecasting using front-door adjustment. For this purpose the model uses three type of attention mechanisms. The method uses a mediator artificially created by training a model in a supervised manner. The paper evaluates the method based on prediction accuracy.\n",
            "main_review": "Main concern: The supervised training of the model resulting in the mediator violates the assumptions necessary to use M as an adjustment set. The front-door adjustment is invalid.\n\nThe assessment of the method is based on predictive performance, what is also incorrect. We cannot assess if the model really working according to the causal principles just because accuracy is better. In the contrary, usually accuracy is lower in a pure prediction task when causal restrictions are applied, but the model is more robust to distribution shift. \n\nSee here:\n\"Instead of directly relationship R → Y , there exists one mediator M, which refers to the knowledge extracted from original input R and used for the prediction of target values Y , i.e., R → M → Y .\" \n\nOne cannot just invent a mediator. It needs to be observed to use the front-door criterion. In case of the front-door criterion the mediator M is the part of the so called adjustment set. Nothing guarantees the synthetic M is not contaminated with the confounder.\n\nThe paper uses unusual terminology, that makes understanding somewhat difficult:\nFor example, the author refer to multi-horizon, multi-series as a special case of time-series analysis, while majority of the recent work actually addresses this problem without explicitly stating it. Sometimes \"multi-series\" prediction just simply referred to as vector valued time series prediction.\n\nThe paper is difficult to read, It took lot of effort to understand what the author really intended to communicate as the statements are not formal enough, and some places quite ambiguous.\n\nCausal terminology is used imprecisely, eg \"deconfounding the unobserved confounder.\", you can deconfound a given causal relation not a confounder, you need to define which relation are you referring to. \n\nor in this case:\n\n\"if we directly train the model based on the correlation P(Y |R) without intervention on confounders, no matter how large the amount of training data is, the model can never identify the true causal effect from R to Y\"\n: To quantify the causal effect R->Y, the one should intervene on the candidate cause to measure P(Y|do(R)), not on confounders. I guess this is a mistake in terminology again and not mistake in understanding.\n\n\"This confounding relationship can cause harmful bias that misleads the time series model to focus on the spurious correlations in data and thus reduce prediction accuracy. \" : Why? Prediction accuracy is usually improved by confounding, except if distribution shift occurs. I guessing the author assumes non-stationary behavior, as it can be deduced from statements like \", these common senses usually are only applicable for part of the time points.\". Again, no formal statement. Also how the experimental evaluation measures this?\n\nOn statements like:\n\"front-door adjustment (Pearl, 1995) that does not require any knowledge on the confounder\" OR\n\"Alternatively, we adopt front-door adjustment that does not require any knowledge on the confounder \" :\nTechnically true, but you need to know the causal graph, you need to be sure the mediator is unaffected by the confounder directly! Again one cannot just make up a mediator. Also it requires knowledge on the mediator in exchange: it needs to be observed.\n\nThe Description of attentions in 3.2 are obscure, not really clear what is the difference between temporal and transformer attention. In my opinion this section not really helps the intuition in this current form. \n",
            "summary_of_the_review": "The base assumption of the method is wrong, as it performs adjustment with inappropriate variables. The statements are hard to understand in several points, they are ambiguous and  imprecise. The paper uses the causal terminology wrong in some cases.  ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}