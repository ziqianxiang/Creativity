{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors consider a one-class classification problem statement.\nThe drawback of the standard one-class classification approaches based on kernels is that they require feature engineering. The authors propose an approach how to introduce deep-learning based feature extraction into the one-class kernell-based classification approach (IVDD) and train the objective in an end-to-end fashion.\n\nThe main novelty is:\n- the authors used IVDD as a base approach\n- instead of standard input features they used features from VAE\n- also, they add the VAE loss to the IVDD loss to train them simultaneously\n- they modified slightly an argument for the logistic model (so that it is scaled with respect to the distance of a sample from the center of the sphere in the feature space)\n- also they proposed to optimise not the  radius of the sphere, but the KL between some prior distribution and the distribution, provided by the logistic model\n\nThe question is why not to use the same modifications for any other anomaly detection method, e.g. for Deep SVDD?\n",
            "main_review": "Comments\n\n1) “The training process (of the Deep SVDD algorithm), as Authors discuss, can degenerate to trivial uninformative solutions if carried without a proper management.” I propose to comment on why this happens. It is not clear why this does not happen with the proposed approach.\n\n2) There are many deep anomaly detection methods, see the detailed review https://dl.acm.org/doi/10.1145/3439950\nThe author did not mention these approaches. They did not provide any comparison with them. Moreover, the experiments are done only for MNIST and Cifar datasets, however there exists many other datasets that have already become standard testing benchmarks for deep anomaly detection (I know about experiments on several UCI datasets in the appendix, but this is not enough). The demonstrated results are not convincing without such comparisons.\n\n3) It is not clear how did the authors define the prior in (12). What is x? How did they define mu and b? There is some motivation about why to use such prior. However, it is not clear how the claim “there is no reason to have a convexity change in this probability shift process” supports that the Gaussian prior is not appropriate. I would propose to make the ablation study and check whether the Gaussian prior is not OK.\n\n4) The method contains a lot of hyper parameters - mu, b, C, kernel width, lambda, etc. I did not find any significant investigation whether the anomaly detection results are sensitive to the selection of these hyper parameters. Moreover, it is not clear how to select them. The text does not provide a clearly defined algorithm for this,  only general comments like “If learning is too fast and the range is skipped as the hypersphere is growing too fast, we step back to the previous epoch parameters and decrease the learning rate. ” I would propose to compose a separate subsection on how the hyper parameters can be automatically selected and demonstrate that the algorithm is robust.\n\n5) The description in Algorithm 1 is not well defined. What is sigma? What is W? Do they pass gradients w.r.t. VAE parameters through the kernel?\n\n6) There are a number of misprints, e.g.\n- p. 1. “uses an hypersphere as SVDD”: an -> a\n- formula (9): problems with index of \\hat{x}_i\n- etc.",
            "summary_of_the_review": "The paper contains several receipts that can be useful in practice. The authors demonstrate on a somewhat limited set of examples that combining these receipts inside an anomaly detection approach can improve it. However, the text is poorly written. A lot of important things are not discussed. The experimental section is weak. The proposed method itself is a combination of small improvements to some existing technique.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors proposed a method for one-class classification that can produce probability of normality for examples. It adopted a loss function that combines KL divergence of predicted probability distribution and desired distribution, and the reconstruction loss in auto encoder, which was included to prevent the representation collapse. The results from one of their experiments do show the proposed method was successful to produce the desired probability distribution on the training set. ",
            "main_review": "The motivation of the paper is not clear. The benefit of having probability over score in practice should be discussed. A continuous score can be relative easily transformed to probability if needed. Should the probabilities of all inner samples be 1? Why should it follow a Laplace distribution centered at 1?  Why are those Gaussian shaped distributions in Figure 1 considered as suboptimal? The discussion of the importance of these should be emphasized, because it seems that’s the main contribution of the work. And the aspect of their method superior over existing ones is primarily about the capability of producing probability and argued through their experiment the probability distribution that they consider more desired. \n\nThe experiment results are rather limited. As indicated in Table1, the proposed method does not have the best performance. Even though, more desired distribution of probabilities (the “desire” itself is questionable though) was obtained on MNIST dataset compared to other methods, this is not as successful on other datasets. Moreover, there is lack of comparison of several most recent one-class classification methods, such as P-KDGAN: Progressive Knowledge Distillation with GANs for One-class Novelty Detection, 2020, and HRN: A Holistic Approach to One Class Learning, 2020.\n\nThe writing of the paper has large room to improve. Awkward sentences and poor choices of words and phrases overspread in the entire manuscript. In addition, there are many places requiring clarification. For example, (1) how the nl (number of landmark samples) was specified? (2) the \\pi (\\pi_low, \\pi_high) in the main text on page 5 is not properly defined before being used; (3) how were the \\mu and especially, b in the Laplace distribution determined. \n",
            "summary_of_the_review": "The motivation of the paper is not clear. The experiment results are very limited. The writing of the paper has large room to improve and does not meet the publication quality.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper builds upon recent work on Import Vector Domain Description (IVDD). IVDD is a kernel logistic formulation of the one-class classification problem, similar to SVDD (OC-SVM), with the advantage of estimating the probability of a sample not belonging to the class of interest rather than a \"distance\" based score in prior work. This work extends IVDD by replacing the kernel with neural networks (Deep IVDD). It also proposes another variant by replacing the hypersphere radius in the optimization objective with the KL divergence with respect to a reference Laplace distribution (which indirectly controls the radius). The methods also add an additional VAE-based reconstruction loss for regularization. The authors provide qualitative and quantitative comparisons of the proposed variants against DeepSVDD, and the kernel-based versions of IVDD. The proposed methods match the performance of the baselines quantitatively.",
            "main_review": "### Strengths\n+ **Relevance**: The paper considers the one class classification problem, which is a fairly well-studied and relevant topic, with numerous applications, as well open problems of interest to the community [11].\n+ **Reporoducibility**: The authors have taken several steps to make the results and the methods easily reproducible. In addition to clear description of the algorithmic procedure and details for the experiments, the authors also provide code to reproduce the results. I briefly took a look at the code provided and it is fairly well documented and has single step commands to reproduce the results from the paper. The experiments are run with 10 seeds and the authors also include these results of the experiments presented in the paper. Only small suggestion here would be to add some more details on the hyperparameters (e.g. $\\beta$, $\\hat{C}$), how they affect the method (in practice), and maybe some strategies to work with them practically, especially considering the empirical nature of the work. (More on this below)\n\n### Concerns\n* **Clarity and Presentation**: \nThe paper would benefit considerably from better organization and more clarity in the presentation. \n    * A core contribution of the paper is providing a deep one-class classificiation method that can provide probabilities, rather than \"distance\" based anomaly scores, but provides little motivation for _why_ that is important, or why explainable one-class classification methods like [8] are not enough. \n    * Figure 4, which gives a high level view of the method would be useful to have somewhere in the beginning of the paper (It can be smaller).\n    * The experiments on tabular data should be included in the main text rather than the appendix. \n    * Figures 1, 2, 3 could be compressed, and some parts can be shifted to appendix (more reasons for this below). \n* **Novelty and Experimental Results**:\nA major concern I have with this work is regarding the novelty of the approach and the experimental results. \n    * The novelty of the key contributions of the paper: \"Deep\" version of IVDD and a modified optimization objective, is not significant enough. The paper motivates the contribution in the context of representation collapse (mode collapse) in DeepSVDD [12]. However, the paper provides no thorough analysis of the claims for being robust to such failures. The authors rely on analysis from previous works [2][13] to back the claims, but I am not sure if these results translate trivially when using neural network features, and the modified objective. \n    * The experimental results presented also do not provide any insights on the claims of robustness to representation collapse. The experiments are run on MNIST and CIFAR. While these are fairly standard benchmarks in this area, they do not demonstrate the mode collapse issues with DeepSVDD, and thus do little to justify the claims. \n    * In addition to this, the method only manages to match the performance of DeepSVDD at best. While just looking at metrics might give a somewhat myopic view of the work, it is still important to consider when looking at the method wholisitically.\n    * Figure 2 aims at providing some qualitative comparison of the samples classified as normal and anomalous. I might be missing something here, but there doesn't seem to be anything visually significant to differentiate between the methods. \n    * While the authors do provide a very high level ablation with different variants of IVDD, the paper lacks any detailed ablation studies with respect to the hyperparameters of the method. \n    * Another set of experiments would help to understand the contribution from the VAE. What happens if we do not have the VAE regularization? ($\\lambda$=0) or what happens if the VAE is not pretrained? I suspect the pretraining on the entire datasets introduces some information regarding the distribution, which would influence the boundaries learned by the method. SO it would be useful to know since (requiring pretraining on anomalous samples) would significantly limit the applicability of the method. Also if the method relies on the VAE for preventing mode collapse then it would also be susceptible to the failure cases for VAEs.\n\n* **Missing literature and baselines**:\nAnother major concern is the lack of discussion of relevant literature and absence of baselines. \n    * In the introductions the authors briefly discuss anomaly detection approaches. This discussion misses several broader approaches (consequently, also lacks comparisons to these approaches). While the problem of representation collapse is used as a motivation, the authors do not talk about previously proposed methods [6][9] for tackling this issue. The authors also don't mention recent transform-based approaches for learning one-class classifiers [4][5][7], energy based models [10]. The method also relies implicitly on additional training data (since it requires data to pretrain the VAE), so there are also connections with semi-supervised approaches like [3]. References to [14][15], which are some of the initial works using AEs/VAEs for anomaly detection, are also missing. \n    * The only baselines used for comparison are IVDD and DeepSVDD. The experiments lack comparisons to major approaches mentioned above, as well as some simple methods like Nearest Neighbours [1] which can outperform sophisticated methods on CIFAR [6]. For the tabular experiments, there is no comparison with classical methods or more recent methods like [5][6][7]. The tabular experiments also use datasets which are not seen in previous literature, which makes it even harder to put the performance in context. Without a holistic set of baselines, it is hard to make any conclusions about the method and this significantly limits the impact of this work. \n\n\n**References**: \n\n[1] Gu, Akaglo, Rinaldo, \"Statistical Analysis of Nearest Neighbor Methods for Anomaly Detection\", 2019\n\n[2] Decherchi and Cavalli, \"Fast and Memory-Efficient Import Vector Domain Description\", 2020\n\n[3] Hendrycks, Mazeika, Dietterich, \"Deep Anomaly Detection with Outlier Exposure\", 2019\n\n[4] Golan and El-Yaniv, \"Deep Anomaly Detection Using Geometric Transformations\", 2019\n\n[5] Bergman and Hadid, \"Classification-Based Anomaly Detection for General Data\", 2020\n\n[6] Goyal, Raghunathan, Jain, Simhadri, Jain, \"DROCC: Deep Robust One-Class Classification\", 2020\n\n[7] Qiu, Pfrommer, Kloft, Mandt, Rudolph, \"Neural Transformation Learning for Deep Anomaly Detection Beyond Images\", 2021\n\n[8] Liznerski, Ruff, Vandermeulen, Franks, Kloft, Muller, \"Explainable Deep One-Class Classification\", 2021\n\n[9] Chong, Ruff, Kloft, Binder, \"Simple and Effective Prevention of Mode Collapse in Deep One-Class Classification\", 2020\n\n[10] Zhai, Cheng, Lu, Zhang, \"Deep Structured Energy Based Models for Anomaly Detection\", 2018\n\n[11] Ruff, Kauffmann, Vandermeulen, Montavon, Samek, Kloft, Dietterich, Müller, \"A Unifying Review of Deep and Shallow Anomaly \nDetection\", 2020\n\n[12] Ruff, Vandermeulen, Goernitz, Deecke, Siddiqui, Binder, Muller, Kloft, \"Deep one-class classification\", 2018\n\n[13] Sergio, Rocchia, \"Import vector domain description: A kernel logistic oneclass learning algorithm\", 2016\n\n[14] Zhou and Paffenroth, “Anomaly detection with robust deep autoencoders,” 2017\n\n[15] An and Cho, \"Variational autoencoder based anomaly detection using reconstruction probability\", 2015\n",
            "summary_of_the_review": "To summarize, I believe this work has marginal value in terms of novelty, lacks rigorous empirical evaluation (in terms of the experiments as well as the baselines used for comparison), and can benefit significantly from some restructuring and re-evaluation of the claims. The issues highlighted are related to the core contributions of the paper and would require significant time and effort to rectify. Thus, I recommend rejection.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors study one-class classification. It uses the Import Vector Domain Description (IVDD) in a deep networks, to improve existing deep SVDD. To make IVDD feasible in combining with deep networks, the authors further uses Nystrom approximation for the kernel. The experiments are conducted on MNIST and CIFAR10, and the authors show the improvement over deep SVDD, while lack of the comparisons with other deep one-class works. ",
            "main_review": "Strength: \n\n* Instead of reinventing the wheels, studying how to leverage old wisdom is an important direction. The authors study how to extend the existing non-deep IVDD to a deep network extension seems novel. \n\nWeakness:\n\n* It is not clear why the probabilistic IVDD is a better alternative than SVDD. It would be better to elaborate more. In the applications the authors demonstrated, I don't find a demand of having a probabilistic output.  More explanation and empirical demonstration would be very helpful.  Second, why does having such probabilistic output outperform the deterministic SVDD?  Providing some explanation would also be helpful. \n\n* In the paper, one claims of combining deep models with IVDD is to leverage the representation learning from deep models. Recently, there are many progress of using deep representations for one class classification. Unfortunately, the authors do not discuss those. For example, instead of using AE/VAE, those works use self-supervised learning results in promising one-class classification performance \n\n\n[A] Deep anomaly detection using geometric transformations, NeurIPS 2018\n\n[B] Using self-supervised learning can improve model robustness and uncertainty, NeurIPS 2019\n\n[C] Classification-Based Anomaly Detection for General Data, ICLR 2020\n\n[D] Learning and Evaluating Representations for Deep One-class Classification, ICLR 2021\n\n[E] CutPaste: Self-Supervised Learning for Anomaly Detection and Localization, CVPR 2021\n\n\nIn particular, the proposed algorithm can be treated as AE/VAE + IVDD with and end-to-end training. An unanswered question is, how about compared with the two-stage approach as studied in [D]? whether you can learn representations with any pretext tasks (e.g. [A,B,C,D,E]), then combine with any one-class classifiers, either deterministic or probabilistic.  What's the advantage of the proposed e2e IVDD compared with the two stage approach? Although the two-stage approach look suboptimal, [A-E] demonstrate strong performance.  I would to see some discussion and comparison with those works. ",
            "summary_of_the_review": "The main weakness of the paper is the lack of comparison and discussion with representative works on deep one class classification. Also, it is not clear the benefit of using a probabilistic classifiers. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this paper, the authors address the problem of how to perform feature extraction and one-class classification at the time. To achieve this, the authors propose to combine two existing approaches: IVDD for one-class classification and auto-encoders for feature extraction. The proposed method has the following advantages.\n\n1. The proposed method is able to automatically learn low-dimensional features.\n2. Compare to the deep SVDD method that has the same advantage in 1, the proposed method is able to provide an anomaly score which can be used for the anomaly detection, which is a byproduct of the one-class classification.\n",
            "main_review": "\n**Strengths**\n1. The paper in very clearly written, the idea and the motivations are clearly outlined.\n2. I appreciate the authors clearly stated the experiments setting, the code is publicly available and experiments seems to be reproducible.\n\n**Weaknesses**\n1. The novelty of the paper is not significant.\n2. Too many hyper-parameters in the loss function, especially for the Conceptron.\n3. The quantitative results on MNIST and CIFAR in Table 1 do not show the advantage of the proposed method. For the experiment in the appendix, it seems like the proposed method works well on some classes, it might be interesting to investigate why the proposed method works better on some of the classes and does not perform as good as existing method on the rest of the classes.\n4. How does your method compares with the approach where I use a pre-trained CNN to extract features and apply the IVDD for one-class classification?\n\n\n\n**Minor Issues**\n1. The paper needs to be proofread more clearly. There are several typos. For example, the sign before $\\hat{C}$ in equation(7) and equation (1) are not consistent. After equation (4), should be \"then\", rather than \"than\".\n2. Suggestion in the writing: \n -  (6)-(8) seems a bit redundant since they are already given in (1)-(5).\n - It might be easier for the reader to follow if the name for different loss function is highlighted.\n3. Despite the reasons that the Laplace prior is chosen, the $L_1$ distance terms may cause extra numerical issue during the optimization based on the reviewer's experience, is this an issue in your experiment?",
            "summary_of_the_review": "The paper is overall very clearly written. However, I find the novelty is somehow limited, and the experiment results do not show the superiority of the proposed method over the existing ones.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}