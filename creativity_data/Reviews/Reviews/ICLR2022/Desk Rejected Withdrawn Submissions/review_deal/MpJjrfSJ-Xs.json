{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper aims to address the problem of cross-domain cross-set few-shot classification. The main contribution is that it deals with a scenario where the domain shift exists in the target task between the support set and query examples. To this end, it modifies existing cross-domain few-shot setting, by introducing support and query sets with domain shift in both meta-training and meta-test stages and providing additional unlabeled target data in the meta-training stage. A prototypical compact and cross-domain aligned feature representation is then learned to reduce the domain shift. The feature representation is learned by minimizing the point-to-set distances in two directions – from source features to target prototype and from target features to source prototype. Augmented samples are further introduced to perform such alignment. The approach is tested on two benchmarks adopted from existing domain adaptation datasets, including DomainNet and Office-Home datasets, and compared with competing results.",
            "main_review": "Paper Strengths:\n\nThe authors tackle an important and challenging problem of cross-domain cross-set few-shot classification. The proposed approach is simple. Experimental evaluations clearly demonstrate the effect by introducing the bi-directional prototype-based domain alignment framework for learning compact and aligned feature representations.\n\nPaper Weaknesses:\n\n1) My main concern about this paper is the proposed setting. To me, the proposed cross-domain cross-set setting is unrealistic and inconsistent with the general assumption in few-shot learning, in particular the use of unlabeled target data in the representation learning stage. First, the key of few-shot learning is the lack of data or annotated data in the target domain. However, the proposed setting and the proposed approach strongly rely on the unlabeled data from the target domain. Second, the representation learning needs both source and target data. Often, in few-shot learning, the representation learning stage aims to learn generalizable features without accessing to target data. Here the involvement of target data makes that for each potential target task, the representation learning needs to be re-performed.\n\n2) Also, the proposed approach, bi-directional prototypical alignment, seems just a purely domain-adaptation technique that is wrapped on top of and applied to the few-shot benchmark. There is no specific contribution to few-shot learning problems.\n\n3) The proposed approach, bi-directional prototypical alignment, pseudo-labels the target data using the source classifiers. Therefore, a key assumption is that the target data belong to the same label space as the source data. However, in few-shot learning, often the source and target label spaces are different. Why does such pseudo-labeling make sense?\n\n4) From the comparison tables, it seems that the improvements are marginal, especially compared with STARTUP.\n\n5) More comparisons with the state-of-the-art domain adaptation and semi-supervised learning methods are needed to show the benefit of the proposed bi-directional prototypical alignment.\n\n6) How does the performance change with respect to the size of the unlabeled data?\n\n7) How is the data augmentation strategy in the baseline methods?",
            "summary_of_the_review": "This paper addresses a cross-domain cross-set few-shot scenario, where the domain shift exists in the support and query sets and an additional unlabeled target data is available during base training. The main issue is that the proposed setting is unrealistic and inconsistent with the general assumption in few-shot learning.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a new setting to Few Shot Learning (FSL) by enforcing a domain gap between the support set and query set. The authors also propose a method named *stab*PA, which is featured for comparing the features in one domain against the prototypes of the other domain. The proposed *stab*PA surpasses the baselines on their datasets.",
            "main_review": "**Strength**\n1) The paper is generally well written and easy to understand.\n2) The proposed bi-directional prototypical alignment strategy sounds reasonable for solving cross-domain tasks. \n3) Experimental comparison and comprehensive ablation validate the effectiveness of *stab*PA.\n\n**Weakness**\n\n**1) The proposed setting is ill-posed**. There are two reasons: \n\na) *This setting contradicts the characteristic of few shot learning*. An important advantage of few shot learning is to learn from very few support samples (*e.g.*, 5 samples per class in 5-shot setting).  Given this advantage, collecting and annotating samples with similar / close style  (as the query samples) is very easy. In another word, one can easily avoid incurring significant domain shift between the support samples and the query samples. \n\nb) *This setting requires a harsh prerequisite, *i.e.*, there are abundant unlabeled samples for inferring the target domain*. In this paper, the authors use pseudo-label-based method to learn from these unlabeled samples and consequentially mitigate the domain gap between support set and query set. Collecting abundant unlabeled data and ensuring that they have same domain as the target domain are much more difficult than re-collecting some few support samples (to make the support samples have similar domain as the query images).\n\nIn a word, the proposed CDCS problem can be easily avoided, while fulfilling the prerequisite for CDCS is actually more difficult. \n\n**2) A critical process on assigning the pseudo labels is unclear**. Since the class space of the unlabeled dataset only intersects (rather than fully overlap, as illustrated in the last paragraph in Section 3) with the base classes, how do you deal with those out-of-distribution unlabeled classes when assigning the pseudo labels? Or you actually make the unlabeled dataset have exactly the same class space (or sub-space) as the base classes? \n\n\n\n",
            "summary_of_the_review": "A major weakness of this paper is the low realistic value of the proposed setting. Since the support samples contain very few samples, it is easy to collect and annotate samples in the desired domain. In contrast, the proposed CDCS setting requires a more harsh prerequisite: there are abundant unlabeled samples for inferring the target domain (Moreover, it seems the authors further assume that all the unlabeled samples belong to some already-known classes).  To satisfiy  these assumptions and prerequisites is more difficult than avoiding the domain shift during support set collection. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper studies few-shot learning under domain shift. Compared to cross-domain few shot learning setting that assumes domain shift between meta-training and meta-testing stage, this paper introduces a new setting, named cross-domain cross-set few-shot learning, where the additional assumption is that there is a domain shift between support and query set in the meta-testing stage (either support set is from the source domain of base classes, or query set is from the source domain of base classes). Additionally, the setting assumes that the unlabeled data from the target is available during meta-training stage. The paper proposes a new method to address the problem. The key idea is to use prototypical alignment between source and target domains during pretraining stage. During evaluation, pretrained feature extractor is fixed and linear classifier is fine-tuned with the support set. The approach is evaluated on DomainNet and Office-Home datasets.",
            "main_review": "Strengths:\n- Paper is well written and well organized.\n- Prototypical alignment strategy idea is an interesting approach to removing domain shift between the domains.\n\nWeaknesses and concerns:\n\n- Concerns about the setting:\n -- While the paper claims that the new setting is more challenging and realistic than cross-domain few-shot learning, it assumes that unlabeled data from the target domain is available during training, while such assumption is not present in cross-domain few-shot learning (e.g., Tseng et al ICLR’20) which requires cross-domain generalization. Therefore, I do not think that the statement that this is a more challenging setting is true. Also, in many real-world applications it is unrealistic to expect to have access to the unlabeled target data. \n--  My other concern is that the assumption that during meta-testing either source or target domains are from the base classes appears also unrealistic.  \n- Concerns about the related work:\n-- Universal domain adaptation (e.g. [1,2]) methods which do not assume that source and target domain contain the same set of classes are relevant for this work, but not cited.\n\n[1] Saito et al. Universal Domain Adaptation through Self Supervision. NeurIPS ‘20\n\n[2] You et al. Universal Domain Adaptation.  CVPR ‘19\n- Concerns about the approach:\n--  Given that the classes in source and target domains are different, why would a classifier learned on the source domain work well on the target domain? And related, does this setting assume that the source and target domains have the same number of classes and that each class in the source domain has its “matching” class in the target domain? For example, if the classifier is trained on the source domain that has dog breeds, and then applied to the target domain that has sketches of cat breeds, why would such classifier generate meaningful pseudo-labels and why should their prototypes be forced to align?\n-- Another concern is that this paper really boils down to solving the domain shift problem between labeled source and unlabeled target domain where source and target domain have different class categories. So the main focus of the paper is really how to effectively align two domains (related to universal domain adaptation as explained above). To adapt to few-labeled examples, the approach just adopts a simple-linear classifier at that stage. In that regard, assuming that the number of categories between source and target domain is the same and they match seems unrealistic.\n-- There are many hyperparameters (e.g., T_max, alpha, lambda, m) and it is not clear how to set them and how sensitive the method is to their selection.\n\n- Concerns about the evaluation:\n\n -- The selected baselines are weak. For example, the method is compared to the inductive few-shot learning methods while comparison to transductive would make more sense since they assume availability of unlabeled data which is not given to baseline few-shot learning methods.\n\n -- Given that the paper solves the domain shift problem between labeled source and unlabeled target domain, more baselines from unsupervised and universal domain adaptation are needed. Currently, the authors compare only to Ganin et al. ’16 which is not a state-of-the-art method.\n\n",
            "summary_of_the_review": "Many concerns regarding the setting, approach, related work and evaluation arise. First, the setting assumes availability of large amounts of unlabeled data from the target domain which is not realistic. Secondly, the method generates pseudo-labels using a classifier trained on the source domain, but it is not clear  how such classifier can generate meaningful pseudo-labels on the target domain. Third, universal domain adaptation line of research is relevant, but not mentioned in the related work. Finally, more state-of-the-art domain adaptation methods are needed in the evaluation, as well as comparison to transductive few-shot learning methods.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}