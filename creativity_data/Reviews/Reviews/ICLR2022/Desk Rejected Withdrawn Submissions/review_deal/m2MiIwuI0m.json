{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes and analyzes an attention-based GNN for embedding molecular graphs. It shows that the proposed method can learn walk statistics and that the new attention scheme has learning guarantees.  Extensive experiments are conducted to demonstrate the effectiveness of the method. ",
            "main_review": "Pros:\n1.\tThe attention mechanism is natural, and the theoretical justification is sound. The remark and illustrative example in the end of Sec. 5 is especially helpful.\n2.\tThe experiments are comprehensive, and AWARE achieves good performance compared with many competitive baselines.\n\nCons:\n1.\tMy primary concern is that the paper may appear incremental or at least it could be more explicit about its original contributions while crediting other's to avoid being misleading. Compared with the Liu et al. ICLR 2019 paper N-Gram Graph: Simple Unsupervised Representation for Graphs, with Applications to Molecule, AWARE only changes its unweighted neighborhood aggregation to a weighted one using the simple self-attention mechanism. In terms of technique, AWARE is more like a good hack to Liu’s paper rather than something fundamentally novel (though the hack can be well justified, which is good). In terms of writing, it is a little disturbing to see the paper’s Section 3, 4, 5 have great resemblance with Section 2, 3, and 4 of Liu’s paper, with many places differing just up to a straight paraphrasing. This paper may want to cite Liu’s paper not only when it compares with them and highlights its own advantage, but also throughout its Section 3, 4, 5, e.g. when it uses the same problem setup, and the very similar Algorithm 1, Definition 1, Theorem 1, etc. \n\n2.\tCompared with GAT (and many other existing GNNs), AWARE removes all learnable projection matrices, including the one for transforming features in each layer and those for computing attention. In principle, this could render AWARE much less expressive than GAT (or its close variance). Therefore, though Theorem 1 is interesting, it seems to me that many existing GNNs should also be able to create node embeddings that are linearly correlate to the walk statistics. \n\n3.\tBased on the second point, it is a little surprising that AWARE significantly outperforms GAT and GIN in experiment. What graph-level readout function do the baselines use?  Also, while the result is very stunning, it would be very helpful to extensively discuss what helps AWARE achieves much better performance with much fewer parameters. If it is because of alleviation of overfitting, does the advantage fade out when dataset is large?\n",
            "summary_of_the_review": "In summary, this paper presents an extension of the previous walk-aggregating GNN by adding self-attention. While the attention is natural and improves empirical performance, its contribution may appear incremental. In particular, I am especially concerned with the paper's great resemblance with its previous counterpart, which I believe should be given more credit. Therefore, I do not vote for acceptance of this paper at this point. However, it is possible that my score be improved after having my concerns addressed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to improve walk-aggregating GNNs by applying different weighting schemes on vertex-, walk-, and graph-level for graph classification. Key idea(s) are to add an additional non-linear mapping on vertex- and graph-level, and to apply an attention mechanism on the walk-level. The paper presents a theoretical analysis that shows provable guarantees for a simplified version of the proposed idea (more details below). Furthermore, it provides an extensive empirical analysis that shows improvements of the proposed idea over a fairly large set of reference methods. ",
            "main_review": "Strength:\n- The paper is well-written and easy to follow.\n- The paper performs a large set of experiments and reports the average of 5 different runs, which improves the robustness of the findings.\n- The paper reports good results on many datasets. Furthermore, the ablation study presented in Table 3 reveals that all three proposed modification contribute to the performance.\n- The ideas presented in the paper can easily be implemented. Hence, I think there is a chance that the proposed ideas can be used by other researchers and practitioners.\n\nWeaknesses:\n- The supplementary material contains git information that can reveal the author's identity. If possible, please re-upload the supplementary material without this information.\n- The theoretical analysis in Section 5 assumes a linear activation function, which greatly limits its usefulness since using non-linear activation functions is a crucial ingredient for deep learning. Hence, it is questionable how useful the theorem is. The model used in the experiments also uses a non-linear activation functions (ReLu), which means that the theorem does not apply for the model in the experiments. To be more precise, the paper claims: \"We present provable guarantees for AWARE, ...\", which is not correct since AWARE uses a non-linear activation function.\n- The paper claims to \"aggregate the walk information by means of weighting schemes at distinct levels (vertex-, walk-, and graph-level)\". However, I think only at the walk-level, a proper attention mechanism is applied. The paper states that there is also a weighting at vertex- and graph-level. However, this weighting seems to be a linear mapping followed by a non-linear activation function. Hence, there seems to be no weighting. Only at the walk-level, an attention mechanism is applied.\n- Related to the point above, the input and output dimension for F_(1) can be different (r and r'), which means that the function does not perform a weighting as described in the sentences before (\"some directions in the vertex embedding space are likely to be more important ...\").\n- I did not find the idea presented in the paper to be very interesting from a scientific point of view. Essentially, it the idea is to add two non-linear functions, one at the vertex- and one at the graph-level, and to add attention at the walk-level.",
            "summary_of_the_review": "The paper proposes an application of the attention mechanism to different walks in walk-aggregating GNNs and to add additional layers at the vertex- and graph-level. I think the ideas are not particularly novel, but the good results could be interesting for others. I did not find the theoretical analysis to be very interesting since it makes a very strong assumption (linear activation function).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors develop a novel attentive walk-aggregating GNN: AWARE and also provide theoretical analysis for their method. Extensive experiments are included to show the outstanding performance of AWARE.",
            "main_review": "The article is well written and the developed method is reasonable and technically sounds. However, I am really worried about the novelty of this work.\n\nCompared to the previous work N-Gram Graph published in NeurIPS 2019 [1], AWARE only introduces two attention layers in the walk-aggregating GNN, where one for node-level and another one for graph-level. Moreover, the attention mechanism in these two attention layers is quite normal in recent GNNs and there can be a lot of method replacements for the attention layers in AWARE, but the authors choose a quite simple one.\n\nThe theoretical analysis is quite attractive, and I really appreciate the authors can provide so detailed analysis for their developed method, although the novelty of the method is quite marginal and straightforward.\n\nThe following is additional comments on this article:\n\n1.It will be better if the authors can provide an overview of the architecture of their method. It’s quite hard for the reader to follow the pipeline of their method, which is also hard for me if I don’t read the paper for N-Gram Graph [1].\n\n2.The discussion between AWARE and N-Gram Graph are missed in Related Work Section, and the authors should better highlight their contributions when compared to N-Gram Graph.\n\n3.The theoretical analysis is quite interesting. I wonder why the authors want to prove that the AWARE can highlight important features and depresses irrelevant ones for prediction. Is it the most basic common sense in the field of attention mechanism.\n\n\n\n[1] N-Gram Graph: Simple Unsupervised Representation for Graphs, with Applications to Molecules. NeurIPS 2019",
            "summary_of_the_review": "I really appreciate the authors can provide so detailed analysis for their developed method, but the novelty of the method is quite marginal.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper analyzes of the effect of introducing weighting schemes into walk-aggregation GNNs, and present an Attentive Walk-Aggregating GRaph Neural NEtwork (AWARE) for graph prediction, in which the vertex-level, walk-level, and graph-level weightings are incorporated to aggregate the walk information. Partial theoretical analysis is developed. Experiments conducted on molecular property prediction and social networks demonstrate some improvements.  \n",
            "main_review": "Strengths:\n+ The idea of introducing extra weights is somewhat novel. \n+ Partial theoretical results are developed. \n\n\nWeaknesses: \n- $W_v$ is viewed as a weight matrix. It in fact is a full connection layer, in which $F$ is the input and $\\sigma(\\cdot)$ is the activation function. The goal of introducing $W_v$ is to find out or emphasize the subset of the vertex attributes (\"Some directions in the embedding space\") that are useful to the prediction task. \nHowever, it may not be fulfill be the proposed weighting scheme because, there is no any regularization on $W_v$ but merely a supervision loss as the training objective. Rather than finding out the desire subset of the vertex attributes, it is more likely over-fitting to some complicated combination of the attributes. Introducing some regularization on $W_v$ is a need, e.g., sparsity, low-rank, group sparsity, or support of a given size. \n\n\n- In Eq. (2), the function $S$ seems undefined. $W_w$ is used to learn the relationship between vertex $j$ and vertex $i$. The learned relationship is normalized with softmax in Eq.(3) and then is used to element-wisely weight the adjacency matrix $A$. (It seems a typo in the formula, i.e., $S_n$ should be $S_{(n)}$? ) Moreover, $W_g$ is further introduced to compute a weighted sum of the latent vertex representation $F_{(n)}$ to obtain walk set embeddings. \n\nThe proposed AWAKR is a complicated model, but what a disappointing is that the theoretical analysis is developed for an over simplified case.  The theoretical analysis looks like a strawman after the simplication assumption: $W_v = W_g = I$, $C =1$, and $\\sigma(z) = z$. It is not clear how RIP could be hold for a generally learned $W, W_v, W_g$.\n\n- The weighting schemes aggregate walk information at distinct levels, e.g., vertex-level, walk-level, and graph-level. It is not clear what about the effect of the weighting at different level on the performance improvement for different task? Is there any preference on a specific level for a specific task? While it has been emphasized at the motivation part, neither the theoretical analysis nor the empirical results answer this question yet.\n\n- While some improvements could be observed in some cases, the results in Tables 2 or 3 are mixed. \n\n- The presentation or organization of the contents is somewhat vague and difficult to read. All equations should be enumerated. All symbol or functions should be defined when it is in use. \n\nMinor issues:\na) Footnote in pp.5:  \"Singular Vector Decomposition\" --> \"Singular Value Decomposition\".\n\nb)  \"Some directions in the embedding space\" --> Should that be a subspace in the embedding space? ",
            "summary_of_the_review": "While analyzing the effect of introducing weighting schemes into walk-aggregation GNNs is an interesting topic, and the current work did great effort in the submission, it is still unclear some critical aspects: a) lacking some regularization in model, b) over simplified theoretical result, and c) insufficient or mixed empirical results. Thus the reviewer ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}