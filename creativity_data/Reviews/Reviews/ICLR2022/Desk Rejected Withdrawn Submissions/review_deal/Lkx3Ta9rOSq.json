{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper demonstrates a theoretical equivalence between multi-distribution density-ratio estimation (multi-DRE) and class probability estimation. They show this equivalence via a multivariate Bregman divergence identity, that generalises known results from the binary (two-class) setting. They use this equivalence to derive different loss functions for multi-DRE that they then validate for a range of experimental scenarios.",
            "main_review": "**Framework + Theory**\n\nThe core contribution of the paper, culminating in Theorem 2, is to demonstrate a theoretical equivalence between multi-distribution density-ratio estimation and class probability estimation. As acknowledged by the authors, the paper seeks to translate the results from Menon & Ong [1] to the multi-distribution setting.\n\nUnfortunately, the authors seem to be unaware of the work by Nock, Menon & Ong [2], which was published at Neurips 2016, and contains extremely similar results. Specifically, Lemma 2 of Nock, Menon & Ong relates the expected Bregman divergence between the class-probability vectors and the expected Bregman divergence between density-ratio vectors. As far as I can tell, this makes Theorem 2 of the current work largely redundant. Moreover, the authors' proposed Bregman identity in Lemma 1 appears to be a special case of Theorem 1 in Nock et al.\n\nHowever, I have not examined the proofs in either paper, and so I would be interested to hear the authors’ thoughts on the potential added value of their results and proofs over what already exists in Nock et al.\n\nOne contribution of this paper that is novel to my knowledge is the variational representation of multi-distribution f-divergences in Equation 16. However, I believe this result is the multi-distribution equivalent of a result in Nguyen, Wainwright & Jordan [3], which should therefore be explicitly cited.\n\n**Examples of the method**\n\nI liked the inclusion of sections 5.1 & 5.2, which gave an accessible overview of how to construct novel loss functions either via a choice of convex function or a proper scoring rule. Multi-class logistic regression is widely used, and a unified presentation of alternative losses (and their connection to density-ratio estimation) is a novel valuable contribution.\n\nThe authors claim that “In terms of modeling flexibility, the curvature of different convex functions encode different inductive biases that may favor various downstream applications”. I would have liked to see a concrete example of how to encode a particular inductive bias by choice of convex function, since this would equip the reader with more intuition about how to use the framework.\n\nSome of the proposed methods (Power, Quadratic and Logarithm pseudo-spherical score) have free parameters, but there is no advice given for how to set those parameters, nor a clear explanation of which values were searched over in the experiments. In particular, the Quadratic method has a large number of parameters, making a simple grid-search infeasible. This issue is particularly concerning, as Table 2 shows that rather specific choices of these free parameters were sometimes used (e.g. Spherical with alpha=1.8), and that even with such precise tuning, the methods could be unstable (i.e. the results marked with ‘/’, denoting extremely poor performance).\n\n**Experiments**\n\nThe diversity of experimental applications was good, as well as the large number of methodological variants compared. However, there was limited evidence that these new variants were reliably better than multi-LR (more details below), which is the well-known gold-standard. This detracted from the main message of the paper i.e. that a ‘unified framework’ leads to new loss functions that are to be preferred for some applications.\n\nIt was interesting to see that Brier score was often competitive with multi-LR, and that, in some cases, other methods like Power, Spherical etc. were also competitive (Table 2). However, overall, there was weak evidence that any of these new methods were robustly outperforming multi-LR. Whilst LogSumExp and Quadratic are better in the off-policy evaluation experiment, their volatile performance (see ‘/’ entries) makes them unappealing. This leaves Power as the only method to outperform multi-LR for off-policy evaluation, whilst still being reliable in the other experiments. I think this is an interesting finding that partially validates the core message of the paper, but stronger results are required to fully convince me.\n\nI do not think that the Cifar-10 experiments constitute ‘OOD detection’, at least not in the usual sense implied by that term. As explained in the appendix, you solve a 4-way classification problem between three “superclasses” (each of which is a union of multiple Cifar-10 classes) and a special fourth class that is a mixture of the three superclasses. You then use the learned ratios to predict which of the three superclasses a test image belongs to. If I were to feed Gaussian noise to your method, it would be forced to place it in one of the 3 superclasses. The method cannot tell me that Gaussian noise is ‘out-of-distribution’, it can only tell me the most likely superclass.\n\n**Clarity + prior work**\n\nThe paper is very well written and structured. It was a pleasure to read.\n\nThe discussion of prior work was generally decent, except for the glaring omission of Nock et al. already discussed above.\n\n**Minor issues**\n\n- ‘Multi-class experiments’ is  a strange title for Section 2.1, which contains no experiments.\n- ‘Remark on notations’ → ‘remark on notation’\n- In the definition of the multi-distribution link function (Eq 7  & 8), where is the dependence on x? It appears to have been dropped without explanation.\n\n**References**\n\n[1] Aditya Menon and Cheng Soon Ong. Linking losses for density ratio and class-probability estimation. In International Conference on Machine Learning, pp. 304–313. PMLR, 2016. \n\n[2] Nock, Richard, Aditya Menon, and Cheng Soon Ong. \"A scaled Bregman theorem with applications.\" Advances in Neural Information Processing Systems 29 (2016): 19-27.\n\n[3] Nguyen, XuanLong, Martin J. Wainwright, and Michael I. Jordan. \"Estimating divergence functionals and the likelihood ratio by convex risk minimization.\" IEEE Transactions on Information Theory 56.11 (2010): 5847-5861.",
            "summary_of_the_review": "The paper’s primary contribution is Theorem 2 (and the results leading up to it). Unfortunately, it seems to me that this result is, for the most part, not novel, since it was previously published by Nock, Menon & Ong (2016, lemma 2). Unless the authors can persuade me that I am wrong, this is cause for rejection.\n\nHad the theoretical results been novel, then I would have advocated for a weak-accept, since the remainder of the paper is well communicated and reasonably novel.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Density ratio estimation is the problem of estimating ratios of densities from samples. This paper considers density ratio estimation from three or more distributions p_1, p_2, … p_k. The approach estimates ratios r_i(x) = p_i(x)/ p_k(x), so that p_i/ p_j is estimated as the ratio r_i/r_j. The density ratio estimation problem is formulated as empirical Bregman divergence minimization, with respect to a strictly convex function. The minimization problem is shown to be equivalent to variational estimation of multi-distribution f-divergence. Additionally, density ratios are connected to class probabilities via a link function. The main theorem relates multi-class classification regret (with a strictly proper loss function) to density ratio estimation under expected Bregman divergence minimization, generalizing a known result in the binary case. \n\nThe methods provide quite a bit of flexibility. First, since the Bregman divergence minimization problem includes a choice of function f. Also, any strictly proper scoring rule l composed with the link function can be used for DRE. Several choices of f or l are proposed. The experimental section includes synthetic data, as well as an evaluation of how well the methods aid downstream task (out of distribution detection, sampling, reinforcement learning policy evaluation). \n",
            "main_review": "•\tThe paper provides an approach for multi-distribution DRE, which has received limited attention in the literature, yet has numerous important downstream applications. The new approach builds on established approaches for the binary setting, including by extending theoretical results.\n\n•\tIf f is separable, then is Equation 14 equivalent to solving k-1 binary DRE problems?\n\n•\tThe choice of estimating density ratios with respect to p_k is non-symmetric. How stable is the approach when you renumber the distributions? \n\n•\t“Results are averaged across three random seeds.” Does this mean a given experiment was run only three times?\n\n•\tI don’t see runtimes provided. How practical are the methods?\n\n•\tIn the proof of Proposition 1, there is a change of variables replacing s(x) with the gradient of f at \\hat{r}(x). But in the expression for f-divergence, we minimize over all s which are functions from \\mathcal{X} to \\mathbb{E}^{k-1}. Isn’t the change of variables restricting the possible functions s?\n\n•\tI didn’t follow Equation 16; it seems expectation and maximization were flipped? Please provide a proof. I suggest turning Equation 16 into a lemma.\n\n•\tIt would be great to include a discussion about the advantages of multi-distribution DRE versus solving k-1 independent binary DRE problems to arrive at estimates of p_i/p_k.\n\n•\tI prefer the notation e.g. \\mathbb{E}_{p_k} rather than \\mathbb{E}_{p_k(x)}.\n\n•\tThe equivalence results are given for true quantities rather than empirical quantities. For example, Theorem 2 treats the true regret of a given classifier \\hat{\\eta}. The main approach itself minimizes an approximate form of Equation 14. It would be valuable to understand how well the empirical versions of certain quantities relate to the ground truth. (This is more of a suggestion for future work, but it would be great to include some indication in the current paper.)\n\n•\tIn Equation 11, I think the minimization should be on the rhs as well.\n\n•\tMiddle of pg. 5: “In the following, We briefly discuss”\n\n•\tpg. 9 “Bregmand divergence”\n\n•\tpg 13: “Rearranging the term”\n\n•\tpg. 15, line denoted by (ii): the first quantity in square brackets should have u_i’s in the denominator I think.\n",
            "summary_of_the_review": "This paper makes an important contribution to multi-distribution DRE by providing an optimization method and extending theoretical results from the binary setting. I have questions/concerns about the correctness of Proposition 1, as well as the experimental evaluation (runtimes, number of trials). It is not completely clear from an empirical standpoint that multi-distribution DRE should be preferred over running binary DRE multiple times.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper handles the problem where we are given samples from k distributions p_1...p_k and the goal is to estimate the relative density between each pair p_i(x)/p_j(x).\nThe paper shows a reduction between class probabilities in a multi class classification problem and density ratio estimation. It then shows that many methods for density estimation fall under a general framework where the goal is to minimize the expected Bregman divergance of a convex loss function. The choice of the loss function specializes the optimization being used. This follows closely previous work for the binary case, most notably Menon-Ong 2016 and Sugiyama2012..",
            "main_review": "It is always a good thing when multiple algorithms could be presented and analyzed through a single framework. However in this case I see three main weaknesses:\n1. The added contribution from the binary case is not significant.\n2. The framework itself does not offer any form of guarantee as to the quality of the solution. Since the loss function is proper we know that if the true densities are realized by the model, then the correct solution would be obtained, but this is a hard assumption and there are no guarantees when they are not met.\n3. The paper ignores (both in theory and implementation) the natural approach of approximating each ratio separately (see comments below).\nAnother general comment relates to the quality of exposition. The paper does not follow a cohesive narrative where the contributions are clearly summarized and contrasted with related work.\n\nDetailed comments:\n- Page 2 remark on notation: I disagree that a proper loss function 'encourages' the estimator to match the truth. It means that the true probabilities minimize the loss function, but if they are not realizable there is no guarantee.\n- Please explain the significane of Theorem 1 and how it plays in what the paper tries to achieve (it is not a contribution of this paper).\n- section 2.3: until this point it is never said that connection class probabilities and density ratios is a goal, nor why it is important. The paper lacks a logical narrative structure.\n- page 4 top: what is optimal convergence. Convergence of what to what? \n- Page 4 top: I would think that the naive approach for the problem would be to estimate k-1 binary densities, or perhaps even more than r-1 densities and then use some regression to make them consistent. \n- Page 4 middle: This is not the place to detail an application for the main problem presented in the paper.\n- Page 4 bottom. I'm not convinced that estimating only k-1 densities avoids compounding errors. Perhaps estimating more than k-1 densities would be better.\n- Section 3.2 seems to me like a simple (almost syntactic) generalization of the Menon-Ong2016. If not please contrast. The characterization as a variational estimation is well known in this problem domain (if not for this exact formulation) and appears already in NJW10\n- Section 4 first paragraph: The writing is confusing, who inspired whom? also, the citation of Bickel et al is useless without context. This should go to the related work section (which is missing).\n- Lemma 1: Try to avoid phrases such as 'We can show that' in the body of the theorem.  \n- Lemma 1: Isn't it a simple generalization of Menon-Ong?\n- Theorem 2 needs to be explained, what is its significance? \n- What have you learned from the experiments besides which loss function is good for the specific data you used?\n- Note that only for the most simple case of k Gaussians do you actually know what the true densities are. In other cases the density ratio estimation is used for other purposes (anomaly detection) and it is impossible to gauge how accurate it actually is.",
            "summary_of_the_review": "I find the paper weak both in substance and in presentation. The contribution beyond the binary case in previous work is not significant. A natural heuristic (of using k-1 binary estimations) is not discussed. The writing is confusing and lacks a coherent contrasting with prior work.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper tackles the problem of multi-distribution ratio estimation, i.e. estimating $p_1/p_k, \\dots, p_k/p_k$, through a generalization of the approach of Sugiyama et al (2012).\nSection 3.2 presents the unified framework of the authors, based on expected Bregman divergence minimization. Proposition 1 presents the relationship between the author's approach to DRE and multi-distribtution f-divergence. Section 4 relates with Theorem 2 the problem of multiclass classification to the minimization of a specific Bregman divergence, showing that the density estimators derived from solving a classification objective can also be obtained in their unified framework.\nIn Section 5, the authors propose in 5.1 convex functions for the Bregman divergence that can be interpreted as multi-class classification objectives. In 5.2, they propose methods based on solving the density ratio estimation problem by solving the classification problem (proba density estimation).",
            "main_review": "I believe that the most salient contribution (for ICLR) of the paper is the extension\nof the binary DRE approaches proposed through the framework of the authors and that\nit could be set forth more aggressively.\n\nProposition 1 prooves the opposite of what is written (in parenthesis). Indeed, we use\nthe solution of DRE to derive a solution to variational estimation\nas $s(x) = \\nabla f(\\hat{r}(x))$ which cannot be inverted.\n\nThe experiments do not compare the authors' results with any other known result\nin the literature and consider toy problems. Although they cover a lot of possible uses\nof DRE in practice (OOD detection, data-dependent sampling, policy learning),\nthe experiments are not motivated by real practical use-cases.\nCould we use Multi-DRE to solve a classification/regression problem involving several samples\nwith different distribution to the test dataset? (as does the KLIEP paper with only one training\nsample.)\n\nIn the experimental details of the appendix, you should precise how you do a uniform\nmixture of the distributions. As such, your experiments are not replicable.\nDid you split the dataset in two: 1) one split to be used for all distributions\n$p_1, \\dots, p_{k-1}$, and 2) the other split to get the last $p_k$?\nAlso, is the mixture of MNIST uniform also?\n\nPresentation:\n- The paper is written like a chapter of a mathematics course: it is hard to see the value of the contributions before one finishes the paper and is familiar with the concepts described in the paper. Maybe start by putting in perspective the extensions of KLIEP provided, i.e. the important new techniques set forth by the paper.\n- The mathematical notation is heavy, but precise which is good. I don't have practical advice on how it could be simplified. Maybe the preceding work can help with this. The expression could be improved, as the paper contains disproportionately long sentences (e.g. before section 3.2) making the paper painful to read sometimes\n- Some typos (e.g. @ end: Bregmand div)",
            "summary_of_the_review": "The paper proposes an important extension to the multi-sample case of the theory on density\nratio estimation. The most important contribution to my eyes is the extension of existing\napproaches, such as that of the KLIEP algorithm (multi-KLIEP).\nFor that reason, I vote to accept the paper. However, I vote weak accept\nbecause: 1) the presentation of the paper could be improved to make the paper\nmore impactful for the community, and 2) although the authors tried to cover\na lot of ground for density ratio estimation and propose a novel technique (making\ncomparison harder), the empirical contributions are weak.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}