{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper tackles image anomaly detection by CNNs as feature extraction and sparse dictionary learning as its classifier solved by KSVD. In reference, the sparse coding can be effectively solved by OMP and its reconstruction errors denote the anomaly scores of images. As this paper states,  it is the first work to use a sparse dictionary learning algorithm to handle deep network features. Also, the proposed method achieves the sota performance.",
            "main_review": "Strengths:\nThis paper well describes the main pipeline and one can easily tell the technical details without confusion. Overall, it is technically correct. In addition, the ablation studies and comparison on multiple benchmarks are sufficient to evaluate the proposed sparse dictionary learning pipeline.\n\nWeaknesses:\nAs far as I know, sparse coding and dictionary learning has been well investigated on anomaly detection of both images and videos, though deep neural networks as feature extraction are rarely adopted. The KSVD and OMP used in this paper are classical algorithms on sparse coding and dictionary learning. Thus, I am much concerned about the technical novelty of the proposed pipeline, which simply combines deep features and sparse coding formulation.\n\nAlso, the sota performance claimed in this paper is unfair because recent advanced work on MVTec is missing.\n\n   ",
            "summary_of_the_review": "I prefer to reject this paper with the concerns of technical novelty and sota performance claimed in this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents an anomaly detection and localization method based upon sparse dictionary learning on the activations of a pretrained neural network.  The method uses the K-SVD algorithm and modified OMP to solve the sparse coding problem.  The method is compared on industrial anomaly datasets and shows state of the art performance on detection w/ respect to other published work.  \n\nThe contributions of the paper are the use of dictionary learning on the output of a pretrained CNN.\nState of the art anomaly detection on industrial datasets.\nExtensive justification of hyper parameter/layer decisions of the model.",
            "main_review": "This paper reads very well and tackles a niche problem of industrial anomaly detection.  The methods are well defined and experiments support the conclusion.  \n\n+I believe the main contribution of this paper is the use of dictionary learning on the output features of a pretrained CNN.  The paper claims to be the first to investigate this methodology for anomaly detection.  To my knowledge, I also do not know of published methods that have done this, and I view this as a novel contribution.\n\n+The paper states that the best scores are coming with a combination of activation maps at different layers.  I think this makes intuitive sense.  Dictionary elements learned at multiple scales should contribute different information to the anomaly detection method.\n\nMy main criticism of the work was applicability, scope, and impact.  While the authors do a good job scoping the paper to this problem, I am left wanting more.  Yes, the contribution is there and it is publishable.  But I am not convinced of the impact of the work to the extend that it belongs at ICLR.  \n\n-The paper could benefit from additional experimental results on real world examples.  At least one more datasets over the industrial type datasets, something at a larger scale. \n- While there are many empirical experiments to justify the hyper parameters of the system, I feel that it detracts from novelty.  Parameter grid search is a fine thing when optimizing a system, but I would have liked a stronger discussion on the benefits and contributions that dictionary learning has/ and could have in this context.  What does this give you conceptually?  Sparse coding gives you sparse vectors, it gives you selectivity of elements, and it gives you greater expressiveness... how do these concepts transform the problem and impact how or why this should be used in the community?  I feel like this would be the connection I would need for me to get really excited about this paper.\n\n\n\n",
            "summary_of_the_review": "I have expressed most of my justification in the main review.  Basically, I like the paper, I think it is scoped really well for this problem.  It makes incremental contributions and novelty and is comparable to the state of the art.  It is publishable in its current form.  However for ICLR I would have liked to see additional impact and discussion on the innovativeness of the method.  I would de-emphasize the grid search of hyper parameters and rather see why this is a good idea in the first place, why it should be adopted in this type of problem in general, and how this could impact the field at large.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I have no concerns on ethics.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes an anomaly detection algorithm based on dictionary learning on the representations obtained by pre-trained deep convolutional networks. More precisely, the authors consider location-specific signals from a collection of training images, extracted via pre-trained neural networks of different depth and types. They then train a dictionary via K-SVD to obtain sparse representation for these patches, which are then modeled via multivariate Gaussian distribution. At testing time, the representation for each patch is computed, and a metric is computed over all patches to declare an image as an outlier/anomalous, or normal.\n",
            "main_review": "\nThe presented method is intuitive, and interesting. This reviewer found the following concerns, which I would appreciate the authors responding to:\n\nThe formulation of the dictionary learning problem in Eq.(1) is problematic. First, there is a dimension mismatch (since the dimensions of DX do not match those of S). Second, one of the key advantages of dictionary learning is that they allow for flexibility. In particular, they allow for overcomplete dictionaries, which in turns allows for sparser representations than say, complete, basis. That is, typically one has that the dimension of the signal, n, is smaller than the number of atoms in the dictionary, m; i.e, n<m. Here, the authors chose to make the dictionary square. Why was this choice made? Perhaps the authors wanted to limit the number of atoms in the dictionary based on computational constraints, but then why not choose m<n (which is also possible)?\n\nThe comment on page 3 that \"the dictionary learning [problem] is an underdetermined linear system problem\" is incorrect. The dictionary learning problem is a bi-linear (and non-convex) problem, which is not underdetermined as long as the number of examples is greater than the number of dictionary atoms (as is the case here).\n\nThe authors mention that, to their knowledge, they are the first in employing dictionary learning ideas on representations computed by neural networks. I do not think this is correct, in light of the following works, not in the context of anomaly detection but in computer vision problems in general (which resulting from a quick search - arguably many more exist):\n\nTang, Hao, et al. \"When dictionary learning meets deep learning: Deep dictionary learning and coding network for image recognition with limited data.\" IEEE Transactions on Neural Networks and Learning Systems 32.5 (2020): 2129-2141.\nZarka, John, et al. \"Deep network classification by scattering and homotopy dictionary learning.\" arXiv preprint arXiv:1910.03561 (2019).\nThe reported experimental results seem good, but the experimental validation is not fully convincing: i) Why was not PatchCore inclueded in the comparison on Table 4, or on the BeanTech Anomaly Detection Dataset? In the same vein, why was not PaDim included in the BeanTech Dataset? ii) The authors mention that the feature extractors used for their method and the ones they compare to are different. In light of this, it is difficult to understand whether the difference in performance comes from just a better feature extractor. iii) On section 3.2.2. the authors mention that the \"seeds have been changed from run to run\". This is somewhat concerning, as it seems to imply that the seed of the random number generator might have been fixed in previous experiments. Please clarify on the experimental setting.\n",
            "summary_of_the_review": "The approach is intuitive, though some technical aspects and choices unclear. Some aspects of the numerical experiments likewise require clarification.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work combines sparse representation learning and a pre-trained neural network for classification purposes. Firstly, meaningful representation features are extracted and a sparse dictionary of the most expressive ones is built, which only come from the images without anomalies. Then, the images having anomalies will either contain a non-sparse representation as linear combination of the dictionary elements or a high reconstruction error. ",
            "main_review": "Whilst this is a workable solution, the paper in fact is a continuation of the original work of Napoletano et al. (2018). While Napoletano's paper built their dictionary out of a PCA and K-Means of latent representation features,  the current paper uses K-SVD, which is quite straightforward in terms of research concepts. \n\nThe authors of the current paper argue that their approach is more robust than the other state of the arts, enabling them to drop the one-class learning scenarios and dealing with more significant variances. However, intermediate results and reasoning/proof of why the introduction of K-SVD helps reducing the variances are missing in this paper. \n\nOn the other hand, the authors argue that combining different layers activation maps is to maximize the detection capability of the pipeline. It is not clear whether or not the performance boosting is due to the transfer learning itself or the combination of different features. The ablation experiments are not set up to answer this question.\n\nFinally, the reviewer believes that this paper is an examination over the integration of several state of the arts schemes instead the presentation of a piece of fully novel idea. If this is true, then the impact and generalisation of the current paper are limited.",
            "summary_of_the_review": "Due to the lack of novel and significant theoretical and experimental discovery, this paper will not be accepted for publication. Moving forward, it is expected that this paper may be driven towards investigating how the extracted features influence each other in the context of deep learning architectures, and what features may play a major role on the performance boosting. The structured or stochastic behaviours of different features may be worth investigating. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a.",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}