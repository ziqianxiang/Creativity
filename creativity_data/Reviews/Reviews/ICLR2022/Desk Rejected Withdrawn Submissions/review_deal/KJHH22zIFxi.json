{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper is trying to address the data deficiency problem in deep Q-learning methods which results in stochasticity in reward or transitions. The authors propose a composite Q-learning algorithm in which they decompose a TD learning problem into a bunch of truncated Q-functions that captures the return for a fixed n-step temporal horizon, and Shifted Q-functions to represent the return after the n-step roll-out. The authors indicate that this composite Q-learning algorithm benefits from separating these two formalisms in terms of hyper-parameter tuning, data efficiency, and robustness in stochastic environment and reward functions which are the result of allowing for a higher learning rate. They compare their proposed approach with two baselines, TD3 and TD($\\varDelta$). ",
            "main_review": "Strength: \n- The paper is well-structured and well-written.\n- The idea is clearly explained.\n- The paper contains some interesting discussions and analytical conclusions. \n- The code was made publicly available. \n\nWeaknesses: \n- Limited methodological novelty. This work uses previously established methods by turning them into new notations, and only make minor advances either in the methodology or in the interpretation of existing methodology.\n- Limited evidence for some of the stated conclusions: \n- Limited (not enough) discussion: please refer to the questions below.\n\nMajor comments and questions: \n- Dividing the bootstrapping into several truncated and shifted Q-functions eventually result in increasing the model complexity due to the increase of the number of optimization parameters. My main concern is that this would result in higher variance while obtaining the optimal solution and eventually more noisy parameters with a higher risk of suboptimality? Also does not this approach reduce the generalization of the optimal policy and increase the overfitting into the seen data? \n- In this setting, the number of roll-out steps seems to be playing a critical role since lowering the value of 'n' means higher variance (lower bias) and enlarging it means higher bias (lower variance) given]a fixed T. How do you select the best value of this hyper-parameter?\n- In the inequalities of (7) and (8), the authors claim that a later initial point in time leads to lower variance of shifted Q-functions. Even though, this might be true about shifted Q-functions for many cases (since the covariance condition probably holds in many environments), they still have not talked about the effect of increasing the initial point on the variance of truncated Q-function. Would not that later initial point increase the variance of truncated Q-functions as opposed to the shifted ones?\n-  The authors claim that inequality (8) holds whether the variance of the reward function is zero or larger than zero. Then they argue that Shifted Q-functions allow them to have lower variance in the gradient and thus a higher learning rate. But they have not considered the role of the truncated Q-functions in their composite methodology.\n- The experiments have been done only on 8 runs. This number of runs is not sufficient in RL problems due to the biased results. \n- My understanding from the paper is that one of the baselines, TD3(\\varDelta), has been applied as a single-step method, while composite TD3 is over the whole horizon. We know that n-step methods have the advantage of considering further steps on the horizon resulting in less biased estimations. Moreover, applying more reward values directly in their update rules instead of action-value estimations is another benefit of applying such methods. Therefore, n-steps are generally expected to be performing better than one-steps, especially, when the true reward function is provided. So better performance of composite TD3 vs. TD3(\\varDelta) in higher learning rates is not necessarily due to the introduction of Shifted Q-functions, rather could be due to the multistep vs. single-step approaches. Hence, I believe Figure 3(a) does not support any specific advantage of the composite approach over TD(\\varDelta). Also, I could not find similar results on the other two environments neither in the Appendix nor in the paper. \n- The authors claim that TD($\\varDelta$) still suffers from a stochastic environment and reward. They have not indicated clearly how their proposed approach alleviates both of these issues other than the possibility of having a higher learning rate for Shifted Q-functions.\n- Corresponding results in Figure 3(b) for the Humanoid-v2 environment is also missing. \n\nMinor comments:\n- In equations 2 and 3, the assumption to have $(T-1-t) mod n = 0$ as the only restriction for a cleaner notation is not enough. Either T or t also should have the mod n of zero as a second restriction. \n",
            "summary_of_the_review": "Overall, the paper investigates an interesting problem but offers limited technical and empirical insights. The work can be strengthened by a more in-depth discussion about the pros and cons of this approach, more theoretical guarantees, as well as more precise experimental results. Yet, I would be open to increasing my overall score based on the authors' responses since the idea behind the paper is interesting.",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces a new Reinforcement Learning algorithm (Composite Q-learning ) based on a novel off-policy Temporal Difference formulation of Truncated and Shifted Q functions that represent the return for the first n steps of a target policy rollout and the farsighted return after the n steps.\n\nThe motivation is to use two separate learning rates for the truncated and shifted Q functions. The method is implemented within the state-of-the-art continuous RL TD3 method (resulting in Composite TD3) and evaluated using a set of MuJoCo benchmarks . Also, TD(\\Delta) off-policy variant is introduced based on the previous work by Romoff et al., 2019. ",
            "main_review": "After reading the paper, I am convinced that the proposed method is sound and improves the existing state-of-the-art in off-policy continuous RL. I like the ideas implemented in the paper. The contribution is mostly technical (new algorithm). \nThe introduced algorithm is well presented in general, but the authors could improve some aspects of the writing, and I provide my suggestions below.\n\nThe benefit of implementing truncated and shifted Q-functions with separate learning rates is obvious, as shown in the presented MuJoCo benchmarks. Adjusting the separate Q-learning rates is being argued using an example stochastic MDP and tabular Q learning. There are many empirically shown advantages of the presented approach. Especially, achieving better stability thanks to reduced variance in the latter phases of training is striking.  AFAIK it is a significant achievement in continuous RL (TD3/SAC) to reduce the high variance of checkpoint performance in the latter training phases.  Thanks to reduced variance, the algorithm performs better within a larger time-step regime, and also in the noisy rewards setting.\n\nAnother algorithmic innovation introduced in the paper is the off-policy TD(\\Delta) variant based on the on-policy TD(\\Delta) from (Romoff et al., 2019. \n\nThe formal presentation of the Approach from Sec. 3 presents the intuitions behind the method, and derives the fundamental quantities. The algorithm design is based on the observation that the Q-function can be decomposed into multiple truncations, that can be evaluated off-policy and model-free. The theorems proving some basic properties of Composite TD3 are presented in Appendix B, and are based on inductive arguments. However, at the first reading, I could not grasp the details of the approach. The issue might be the new notation introduced by authors to handle the multiplicity of truncated/shifted Q-functions and targets. I suggest formalizing the notation and providing definitions wherever appropriate, this should make the reading of the paper smoother.  \n\nThe theoretical derivations on the (co)variance of the remainder (Eqs (6), (7), (8)) should more formal than that. I mean there are loose statements like _\"The covariance of reward and future return is likely to be positive if the immediate reward depends on the next state\"_. Rephrasing those derivations as a formal lemmas/theorem would definitely do good for the impact of the results, as it would explain theoretically the reduced variance visible in benchmarks. \n\nThe authors describe the entropy regularization scheme only in Appendix D , it is not at all mentioned in the main part of the paper. Is there any motivation of deferring this important algorithmic choice to the appendix? It also makes the Composite TD3 more Soft-Actor-Critic-like. Also, why Composite SAC was not evaluated then?\n\n### Additional Questions\nI will be grateful if the authors clarify the questions below that I had.\n* Is there any intuition on the optimal number of truncation horizons on the environment, and how to adjust this value. Also why a constant value, not a time-dependent/learnable one ?\n* Why only those three MuJoCo environments, not including e.g. HalfCheetah and Ant ?\n* Is there any interpretation of the deterministic/stochastic MDP chains in Appendix C ?\n* The critic architecture is different and much larger than in other off-policy methods, and the Leaky-ReLU activations are used, it is interesting to see a (table) comparison of optimized architecture designs. It would be nice to comment on the increased computational complexity.\n\n\n### Remarks:\n* This reference is broken (see how it is referenced in the text) Anonymous Author(s). Adversarial skill networks: Unsupervised robot skill learning from video. In ICRA, pp. 4188â€“4194. IEEE, 2020a;\n* Please precisely define $y_{j,i}$, $y_{j, i<n}$ and $y_{j, i>n}$ for both $y^{Tr}$ and $y^{Sh}$ , also it would make the paper easier to read if the used notation is formally defined in the main part of the paper;\n* p. 18 eq. (16) there is  a function missing after the gradient symbols, I guess $\\mathcal{H}$ should be on the right of the bracket.",
            "summary_of_the_review": "The strong points of the paper overweight its limitations. Overall, I think that after minor writing improvements, this paper is a strong accept. ",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose a novel Q-learning algorithm based on merging truncated Q-function and shifted Q-function paradigms.\nThe theoretical analysis can be summarized as showing that the composite Q-target is a complete representation of the full return (in Theorem A.3.). The claims of robustness and samples efficiency are not theoretical guaranteed, and are based on empirical validations, by simulations. ",
            "main_review": "I found the idea of the paper to be as simple as correct. Indeed, the idea of combining truncated and shifted Q-function evaluation is interesting. The proofs of Theorems A.1-A.3 seem correct. \nMy main concern with the paper is the lack of analysis of the two most important claims of the paper, namely, the robustness claim, and the data efficiency claim. Indeed, there are no theoretical analysis of these two claims, yet the authors keep repeating all over the paper that this new composite method guarantees strong robustness w.r.t. noisy reward functions, and fast convergence.\n\nI would advise that the authors revise these strong statements, by underlying that there are no proofs in this work of such statements, and that they are merely empirical observations, based on the tested examples.\n\nOther (minor) issues are:\n\n- Please define what you mean by stability in this paper, when you mention 'learning stability' ?\n- Figure 2 is unnecessary; it does not convey much information.\n- Please  make sure to emphasis each time when you use the term robust that it is robust w.r.t. noisy reward function. Indeed, other type of robustness, is the one dealing with model or state transition uncertainty, which is not being considered here, not even empirically.\n",
            "summary_of_the_review": "Interesting idea, simple but correct general proof of convergence.\nLack of theoretical guarantees with regard to the two main claims of the paper; robustness (to noisy rewards), and data-efficiency.\nGood empirical results, obtained by simulation. ",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new approach for learning a q-function that is potentially more robust to stochastic environments. The main idea is to decompose the Q-learning to two parts:\n\n1- learning n truncated Q-functions for first n steps of the policy rollout\n2- learning shifted Q-functions for the rest of the rollout \n\nThis procedure is called composite Q-learning. Authors claim that this allows to use different learning rates for each part which results in speed up and robustness to stochasticity of the reward.\n",
            "main_review": "Pros:\nThis paper considers a relevant and important topic for reinforcement learning \n\nCons: \n\nThe proposed algorithm is more complex compare to the baselines. It requires maintaining multiple function approximators. Therefore i would expect authors demonstrate significant gains theoretically and empirically over baselines to justify the complexity. More concretely my main concern is on evaluation of the algorithm. The idea of composite Q-learning in a high level makes sense however there is not enough support for the claims.:\n\n1- Evaluations are done on a few relatively easy Gym tasks and I'm not convinced with the reported gains as the evaluations are done only for 4e5 (walker and hopper) and 2e6 (humanoid) environment steps. I would like to see the comparisons for more environments and for when all the learning curves converge. \n\n2- The main claim of the paper is that composite Q-learning is more robust wrt the noise in reward function. However there are no theoretical results supporting this claim. Empirically there are no enough baselines in the paper to evaluate this. This particularly makes it difficult to judge the progress made in this paper. \n\n3- Comparisons with TD3 and TD3(\\vardelta) in terms of sample efficiency is not fair given that these approaches use 1 step estimate for Q-function and composite TD3 is using n-step estimate. I would be interesting to see what happens when simply n-step estimate is used for Q-learning in context of Q-learning.",
            "summary_of_the_review": "The idea presented in this paper makes sense in a high level. However paper can benefit from evaluating the algorithm better both theoretically and empirically. See my comments above for more details.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}