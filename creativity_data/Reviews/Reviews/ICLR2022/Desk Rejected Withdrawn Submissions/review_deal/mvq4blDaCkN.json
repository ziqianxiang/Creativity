{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors propose a new semi-supervised adversarial training framework that focuses on maximizing AUC (rather than accuracy) in the adversarial setting (lp-norm bounded perturbations). The authors also provide a method to make the learning process fast and not reliant on an inner optimization process.",
            "main_review": "Pros:\n* The work focuses on optimizing AUC (rather than accuracy). This is particularly interesting when datasets are imbalanced. I haven't seen this particular setup in the adversarial training literature before.\n* The authors provide 3 methods with varying degrees of AUC and training time.\n\nCons:\n* The experiments are limited to binary classification and MNIST and CIFAR-10. Given that the method is \"efficient and scalable\" I expected to see ImageNet (or TinyImageNet).\n* Results are difficult to compare with prior art. Demonstrating the the method can recover classical adversarial training performance in the multi-class balanced setting could be useful.\n* The presentation is hard to follow and could be improved.\n\n## Main comments\n\n1) The paper tackles unlabeled data as well as AUC optimization in the adversarial setting. Unless, the \"adversarial AUC\" setting has been considered before, I'd recommend the authors to focus on that particular aspect. The addition of unlabeled data seems to play a secondary role.\n\n2) Eq (10) seems to hint that one doesn't need to preform any inner optimization (such as finding adversarial examples), but then Eq (11) requires an optimization. Hence, I am questioning whether the proposed technique is indeed faster. The experiment seem to suggest that S2AT-AUC(M) is not faster than VAT and UAT. Only S2AT-AUC(K) is faster but this seems to be due to another method (from Shi et al.).\n\n3) S2AT-AUC(K) aims to be robust to adversarial examples without necessarily computing adversarial example. As such, a discussion and comparison with CURE [1] seems warranted. Similarly, it might be useful to compare to other fast adversarial training techniques [2,3,4].\n\n4) The attacks considered in the paper are relatively weak (PGD10, ZOO). Could the authors use stronger suites of attacks (e.g., AutoAttack [5])? It is also important to verify that there is no gradient obfuscation and I recommend the authors to go down the checklist in [6].\n\n## Minor comments\n\n5) The introduction states that UAT (Uesato et al., 2019) requires to estimate labels in advance. This is not true for UAT-OT (where online targets are computed).\n\n6) I'd refrain from using wording like: \"ingenious\" and \"superiority\".\n\n7) In Eq (2), what is $\\mathcal{D}_p(x)$? I suppose the authors meant $\\mathcal{D}_p$.\n\n8) Could the authors detail the performance of the pseudo-labeler model using for UAT and VAT? Also, which version of UAT are the authors using?\n\n9) I am surprised that VAT and UAT perform so poorly when the dataset are closer to be balanced (e.g., Fig4(e)). Can the authors explain this? Maybe adding the $N_n/N_p \\in \\{1, 2\\}$ could help.\n\n10) Can the authors explain the difference in clean accuracy between UAT/VAT and the other methods?\n\n11) How are the datasets created? When unbalancing the datasets, are the authors downsampling or upsampling samples?\n\n[1] S.-M. Moosavi-Dezfooli, A. Fawzi, J. Uesato, and P. Frossard, “Robustness via curvature regularization, and vice versa,” 2018\n\n[2] E. Wong, L. Rice, and J. Z. Kolter, “Fast is better than free: Revisiting adversarial training,” 2020\n\n[3] C. Qin et al., “Adversarial Robustness through Local Linearization,” 2019\n\n[4] M. Andriushchenko and N. Flammarion, “Understanding and Improving Fast Adversarial Training,” 2020\n\n[5] F. Croce et al., “RobustBench: a standardized adversarial robustness benchmark,” 2020\n\n[6] N. Carlini et al., “On Evaluating Adversarial Robustness,” 2019\n\n\n",
            "summary_of_the_review": "The problem tackled is interesting and the proposed methods are relevant to that particular problem. However, the experiments lack in clarity and the limitations of the kernel method (S2AT-AUC(K)) are not discussed in enough details. The method is faster and better than what is considered the state-of-the-art (UAT/VAT/RST) and it is worth investigating whether this is due to gradient obfuscation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "- This paper tackles the adversarial robustness problem under a semi-supervised learning scenario, an important problem for deploying DNNs to the real world. To address the performance degeneration from the bias on the estimated pseudo-labels, the authors propose a new semi-supervised adversarial training method via maximizing AUCs. As the proposed method is also formulated as a minimax problem, it can be solved similarly to the usual adversarial training method with a simple modification. The authors further present an equivalent minimization problem based on the kernel perspective to reduce the computational cost from multiple gradient steps. Empirical results on the binary classification problems (constructed from MNIST and CIFAR-10) are provided to demonstrate the effectiveness of the proposed method.",
            "main_review": "**Pros.**\n\n- **Clarity.** Overall, the writing is clear and easy to follow. The organization of the main draft is well-established.\n- **Interesting idea.** The introduction of the kernel perspective to reduce the computational cost is an interesting idea along with Theorem 2.\n\n**Cons.**\n\n- **Limited empirical evaluations.**\n    - The authors only provide the empirical results of the binary classification on MNIST and CIFAR-10 with artificially chosen two classes. For the fair comparison of previous works and the verification of extendibility to multi-class scenarios of the proposed method, they should report the experimental results on the original dataset (with all classes).\n    - Also, the recent state-of-the-art attack methods should be considered for evaluation, such as AutoAttack (Croce et al., 2020) and Adaptive Attack (Tramer et al. 2020) (i.e., directly attacking the proposed method S2AT-AUC itself).\n    - For each dataset, only one fixed perturbation size $\\epsilon$ is considered. The robustness on various sizes should be verified.\n    - Although the proposed method seems to rely on the trade-off parameter $\\beta$ crucially (in Equation (5)), there is no analysis about this parameter.\n- **Limited motivation.** In Section 1, the authors motivate using AUC (instead of accuracy) based on the imbalanced problem. However, the considered problems seem to be far from that scenario; hence the provided sentences are not sufficient to answer that \"one should use AUC instead of accuracy for a semi-supervised adversarial learning problem.\"\n- **No mentation about computational cost.** As S2AT-AUC requires multiple adversarial examples following Algorithm 1, the huge computational cost seems to be unavoidable. How much are larger computations exactly needed for the proposed method?\n\nCroce et al., ICML 2020, \"Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse Parameter-free Attacks\"\n\nTramer et al., NeurIPS 2020,\"On Adaptive Attacks to Adversarial Example Defenses\"",
            "summary_of_the_review": "- Although the paper tackles the important problem with an interesting idea, 1) the motivation for the idea is quite weak, and 2) the empirical justification is insufficient to validate the proposed idea. The score could be raised if the author can provide additional results on the standard multi-class setups with stronger attacks.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Based on the semi-supervised AUC optimization proposed in  Zheng & Ming (2018), the authors proposed a framework for semi-supervised adversarial training. The proposed method can be applied to both DNN and Kernel SVM.",
            "main_review": "Semi-supervised learning for adversarial training has already been quite extensively studied and has been proved to be effective[a-d]. This paper uses a new AUC based loss function to implement such a task by fusing AUC loss and MINMAX strategy. The ideas seems OK to me. The main problem of this paper is the weak experimental validation. The authors only provide very limited results on binary classification which is quite uncommon and unfair. According to the author, their method can be 'easily' extended to multi-class classification. Why not simply provide the full results so that the comparison with existing methods such as UAT and VAT can be at least fair? Also, adversarial training on large scale dataset, such as imagenet should be provided to demonstrate the method's generalization to more classes. \n\n[a] Adversarially Robust Generalization Just Requires More Unlabeled Data, Arxiv 2019\n[b] Are labels required for improving adversarial robustness? NeurIPS 2019\n[c] Unlabeled Data Improves Adversarial Robustness, NeurIPS 2019\n[d] Robustness to Adversarial Perturbations in Learning from Incomplete Data, NeurIPS 2019",
            "summary_of_the_review": "The experimental results are far too weak to support the authors' claim.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}