{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "To facilitate understanding the decisions of deep learning models at a semantic level, this paper introduces an interpretation method based on Morris' method. The authors claim that previous interpretation methods did not distinguish between the features in input space that are individually important and features that are important because of their interaction with other features. ",
            "main_review": "*** Strength\n1. The method which uses Morris' method is quite new in the domain of interpretation of deep neural networks.\n2. Improving understanding of the decision of deep neural networks at a semantic level is an important direction\n\n\n*** Weakness\n1. Evaluation is limited. This paper does not provide any comparison with other interpretation methods, which makes it difficult to figure out the strength of the proposed method.\n2. Experiments are conducted on a small dataset. The Biased MNIST and partial ImageNet are used in the experiments. However, only 80 images are used for the case of ImageNet. It is too small to verify the effectiveness of the method.\n3. Standard benchmarks for quantitative evaluation of feature attribution are missing in this paper. For example, sanity checks (Adebayo et al 2018), Sensitivity-N (Ancona et al 2018), image degradation (Schulz et al 2020) are examples. By adding quantitative evaluation on the sensitivity results, it will be possible to understand how good the proposed method is.\n4. According to Figure 7 (qualitative results), sensitivity results are not attractive. I am concerned about how informative the proposed method is. What kind of information the users can exploit and how does this improve the understanding of modern deep neural networks.\n5. The experiments are only conducted on the small image dataset. However, the authors claim that the method can be applied not only to image datasets but also to other modalities such as language, audio, and video. The authors need to show experiments at least one more modality to claim it. \n6. This paper is missing important references. The feature Interactions were also discussed in Contextual Decomposition (Murdoch et al 2018), Integrated Hessians (Janizek et al 2021). More discussions and comparisons with these studies will be informative to the readers.\n\n[R1] Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M. and Kim, B., 2018. Sanity checks for saliency maps. NIPS\n\n[R2] Ancona, M., Ceolini, E., Öztireli, C. and Gross, M., 2018. Towards better understanding of gradient-based attribution methods for deep neural networks. ICLR\n\n[R3] Schulz, K., Sixt, L., Tombari, F. and Landgraf, T., 2020. Restricting the flow: Information bottlenecks for attribution. ICLR\n\n[R4] Murdoch, W.J., Liu, P.J. and Yu, B., 2018. Beyond word importance: Contextual decomposition to extract interactions from LSTMs. ICLR.\n\n[R5] Janizek, J.D., Sturmfels, P. and Lee, S.I., 2021. Explaining explanations: Axiomatic feature interactions for deep networks. Journal of Machine Learning Research, 22(104), pp.1-54.",
            "summary_of_the_review": "Although this paper targets an interesting problem, the experiments and evaluation are limited in the current version of the manuscript. Also, more literature needs to be introduced and discussed as mentioned in the main review. I think these need major revision of the paper and I would suggest the authors resubmit a paper to the next venue.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a saliency-based method to visualise what parts of an image (or what user-defined parameters) contribute to a neural network's classification decision. The method distinguishes between parts/parameters that contribute significantly to the decision on their own vs. parts/parameters that are important due to interactions or non-linear effects. An evaluation of different CNN ImageNet classifiers suggests that most models use similar image regions to make classification decisions, but deeper models rely more on non-linearities or interactions between regions.",
            "main_review": "The method seems to be novel. The evaluation is reasonable, although it focusses only on showing that the method works as intended (successfully identifies features that are independently important to the output vs. features which are involved in interactions). In particular, the evaluation on a toy dataset with known generating features is a very nice way to validate the method, which I rarely see in XAI papers.\n\nHowever, the evaluation of the method is limited; there are no comparisons to other saliency-based methods or user studies to assess the interpretability of the saliency results.\n\nA major weakness of the method is that it requires the user to define an input partition, and the degree to which the results are interpretable or meaningful would depend heavily on this input partition. If the generating factors for the data are known (as in the MNIST toy example), it's easy to choose a correct partition that matches the actual generating features and obtain meaningful results that give valuable insights into how the generating features interact. However, it's not clear how to interpret the results of this method if the input partition is poor, and it's not clear how robust the method is to poor partitioning.\n\nThe choice of input partition seems, to me, to undermine the experimental results with the ImageNet classifiers. Since the generating factors for images are probably quite complex, the authors simply divide the images into (w x w) grids and treat the pixels in each grid cell as a \"feature.\" The results show that different architectures tend to rely on a similar subset of grid cells to make decisions but more recent architectures are more likely to rely on interactions between grid cells. I'm not sure if this is simply showing that most models use the central part of the image to make decisions (since that's usually where the object is located in ImageNet images), or whether the \"interactions\" reflect the receptive field sizes of these different models or actual interactions between higher-level features in the neural network.",
            "summary_of_the_review": "This paper proposes a novel method to generate saliency-based explanations for neural networks that reflect the degree to which features contribute to a decision independently or through interactions with other features. However, I think this method may not be generally useful because it relies on the user correctly identifying features for the model.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper identify key weaknesses in previous XAI methods, and propose a Morris Screening based sensitivity analysis method using MoSIP, which allows quantifying which parts of input s are individually important and which are potentially important due to correlations. ",
            "main_review": "Strengths:\n+ Explainability is an important research topic and highly relevant.\n+ This paper addresses an important issue existing in former method.\n\nWeaknesses/Concerns:\n- The paper is hard to follow.\n- The methodology is not easy to understand, Figure (1) is not straightforward. More explanations are required for Morris Screening and Input partitioning.\n",
            "summary_of_the_review": "Overall, I vote for rejecting, with lots of concerns unsolved.\n\nThis paper is not clear to me, more explanations for methodology is needed before I consider increasing my score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a feature attribution method that can highlights compound (interaction of) and individual features that trigger a model's prediction. The method is based on a sensitivity analysis method (Morris method) that does a per-feature sensitivity analysis in a sequential fashion on the input space. It defines two main metrics (mu_i* and sigma_i) from which it can be determined if the i-th input dimension has a zero, linear, or non-linear influence in a prediction. This method is evaluated using Biased-MNIST and a random selection of 80 images from Imagenet.",
            "main_review": "## Strengths\n- The paper highlights an important problem regarding feature attribution methods, namely the inability to systematically identify latent semantic features behind a prediction.\n- Exploring potential uses of the Morris method as an efficient alternative for doing feature attribution.   \n\n## Weaknesses\n- The grammar of some passages, and the use of non-standard formal terminology (e.g., lump values, designs, levels) make it difficult to understand what the contributions are or how the method works. \n- Formal notation is often hard to follow and possibly inconsistent.\n- The proposed method does not provide any baselines showing that it is better than random chance. For Biased-MNIST, there are no evaluations showing how MoSIP performs when none of the controlled features (background color, foreground color and digit position) are uncorrelated. Figure 3a even suggests that the metrics based on the Morris method do not change much when certain controlled features get more correlated to the class.\n- Results on Imagenet are based on an almost insignificant sample (80 images from 4 classes). Results are unlikely to be significant.\n- The premise of the paper regarding the identification of \"individual features\" and \"interacting features\" is not evaluated. Similarly, the use of \"semantic features\" is left open to interpretation by the reader.\n\n### Detailed Comments\n- The paper uses the expressions \"non-linearities/interactions\" and \"individual features\" without providing a strict definition of what they are. Without one, it is impossible to know a priori what counts as individual feature (e.g., is the color yellow an individual feature or an interaction between the red and the green channels in RGB? Are circles individual features or are they an interaction between non-linearities of the x and y coordinates?)\n\n> 1. We present, what is to the authors' knowledge, first application of this method to Deep Learning models.\n\n*There's at least an application of the Morris method to hyper-parameter tuning for DL [1]*\n\n> Given a model, f(x), that acts on an input, x ∈ R^n, the Morris method samples N values x.\n\n* What does it mean \"to sample N values x\"? To sample N values **from** x? To make N (possibly noisy) copies of x?\n\n- Equation 1: What is the relation between N and R^n? If N != n, how can f() work for both \"designs\" and for x? If N = n, how can two samplings of N elements from x be different or how is x_{1, r} different from x?\n\n- Equation 1: What is $\\Delta_i$? What are levels?\n\n- Section 2.2: the notion of semantic features is left undefined and open to interpretation.\n\n- Section 2.3: introducing the definition of terms like \"input partitioner\", \"model wrapper\" and \"screening method\" beforehand will greatly improve readability. Neither figure 1 nor the description of the section help in understanding what the pipeline is.\n\n- Section 3.1: if the \"partition input\" is modeling the possible combinations of (finite values for) background color, foreground color and position, why is x defined in a continuous space R^3?\n\n- Section 3.1 (appendix): why is the CNN model defined with a batch-norm as the first layer of the network? Are input samples normalized before being passed through the model or are they kept in RGB space i.e., [0, 255]?\n\n- Section 3.2: what are lumped values?\n\n- Figure 2b: what does the y-axis represent? How are the \"Normalized standard morris method results\" (sic) computed? \n\n- Section 5: results are based on 80 images from 4 classes (out of 1.2M images from 1000 classes). The size of this experiment is not enough to produce significant evidence for the proposed method.\n\n>  If the Morris method is accurately selecting regions of importance/non-importance, we expect that the score and accuracy should decrease by more than 20% when we mask the 20% most important regions.\n\n*This statement implies that size of the occluded area is a lower bound of the decrease in accuracy, something that methods like ROAR [2] or even 1-pixel adversarial attacks [3] disprove.*\n\n- Figure 4: no baselines are provided (e.g., removing 20% of the input region at random) showing that MoSIP works better than random chance.\n\n- Validating the Morris Method Globally: it is known that ImageNet samples have a center bias i.e., the object of interest is most likely in the center of the image. Without any further analysis, it is plausible that the results of the global analysis are simply exploiting this center bias and selecting regions that are located towards the center. A baseline that test this hypothesis is needed before arguing in favor of MoSIP.\n\n### Grammar\n> The feature important and has significant interactive and/or non-linear effects\n\n> In this section we discuss the data and models we will apply the Morris method to, as well as the how we partition this data.\n\n> and the image generator serves as extra arguments capable of generating raw input space from the partitioned values.\n\n> The parameters used for the Morris method are N = 40, n_partition,x × n_partition,y = 8 × 8, amd m = 8.\n\n### References:\n[1] Taylor, R., Martino, I., Nicosia, G. and Ojha, V.  (2021) Sensitivity analysis for deep learning: ranking hyper-parameter influence. In: 33rd IEEE International Conference on Tools with Artificial Intelligence (ICTAI2021), 1-3 NOV 2021, Online.\n\n[2] Hooker, Sara, et al. \"A Benchmark for Interpretability Methods in Deep Neural Networks.\" Advances in Neural Information Processing Systems 32 (2019): 9737-9748.\n\n[3] Su, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. \"One pixel attack for fooling deep neural networks.\" IEEE Transactions on Evolutionary Computation 23.5 (2019): 828-841.",
            "summary_of_the_review": "The paper has an interesting premise and the Morris method provides valuable properties that can lead to an efficient alternative for otherwise costly attribution methods. However, the paper overall lacks coherence and scientific rigor. Experiments are insufficient and even the simplest baselines are missing.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}