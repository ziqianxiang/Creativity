{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an approach to address the domain generalization problem - by disentangling the feature space into content and style subspaces. The key idea being that the domain variance is captured in the style subspace and the content subspace remains domain invariant. This consistency is being imposed during training using a domain regularization objective. Experimental results are presented on PACS and DomainNet benchmarks.",
            "main_review": "Strengths:\nThe paper is easy to read and clearly written. The figures are easy to follow and complement the text well. The motivation behind the domain regularization loss is explained well. \n\nWeaknesses:\n1. The independence assumption that the authors expand in Section 4.2 is not a novel construct - it has been around since the early works of domain adaptation. The challenge has been to achieve such disentanglement in a principled manner without losing content accuracy. \n2. In the current work, the proposed approach is essentially a conditional StarGAN - multimodal translation using the provided domain information. The authors do some nice engineering to enable the training of such a big system. Training a complex system involving feature extractor and generative models requires a big computation overhead that compares unfavorably against feature based approaches such as [1].\n3. The evaluation section follow a generic pattern of testing on the same benchmarks that are not large scale. This issue is explained in detail in [2]. The authors do not test on the full domain net - The WILDS benchmark which is a large scale benchmark for domain generalization is missing. The transformations provided in PACS or even DomainNet are unrealistic when applying such solutions to real world setting. The WILDS benchmark (not recent, but published a year ago) captures that. \n\n[1] Learning to balance specificity and invariance for in and out of domain generalization\n\n[2] In Search of Lost Domain Generalization\n",
            "summary_of_the_review": "The paper falls short in that while the problem being addressed is interesting enough, the proposed solution is not highly novel and the evaluation + the analysis of the model performance is middling. Results on the WILDS benchmark can spruce up the evaluation section quite a bit. I am very interested in seeing where the generative models fail and how the proposed approach can adapt to that failure. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper addresses the domain generalization topic when multiple source domains (labeled data) are available for training while no target data are accessible. The proposed approach tackled this task by data augmentation, in particular, 1) by learning how to disentangle the source data in 2 factors of variation, content-based and style-based, of which only the former affects the semantics (i.e., the class label), 2) generating augmented images by combining such factors, 3) and learning a classifier taking into account source images and these new generated data. To this end, a new loss is designed, which showed to improve performance in domain generalization datasets, more specifically, by using 3 domain adaptation datasets (PACS, OfficeHome, mini-DomainNet), leaving one domain out each time (out-of-domain generalization). Also, in-domain generalization tests are performed proving good performance in these cases as well.",
            "main_review": "The paper is well organized and explained, and mostly clear; the approach is simple: in a few words, it is based on a) feature disentanglement -- content-specific (class) and style-specific (all the rest, the actual domain, which should not affect the semantics), b) the recombination of these factors to produce new (augmented) data, and c) learning a classifier invariant of the style change.\n\nThe point (a) and (b) are carried out using an image-to-image method in the literature, namely StarGAN, which is modified to generate data of the specific classes considered. The point (c) is a standard learning procedure using cross-entropy (CE) term and a simple function (weighted by a hyper-parameter lambda) aiming minimizing the L1 distance between the classifier probability outputs of the considered sample and the corresponding newly generated image.\n\n1) I like the simple structure of the overall approach, but my first main concern is that I don't see much novelty in it because disentangling features in this way is not new, and it's done by a known GAN method, whose variant is quite straightforward, just adding a classification loss in the original StarGAN formulation. So, most of the work is done by StarGAN. Moreover, the learning of the final classifier f is also quite simple, CE plus the difference between the classifier (probability) outputs.\n\n2) Second, I have some remarks also for the justification of the method: in the end, if the method is aiming to generate new \"plausible\" data by combining different style factors with the same content in order to learn a more robust classifier, invariant to such style factors, why not using the same data from the different source domains directly to make this learning invariant? In other words, why the generation of the new data with different styles (taken anyway from the source domains available) should work better than only using the data from the other source domains in the same formulation?\n\n3) This aspect also relates to the results and the provided ablation analysis, and that is why, my third main concern is considering the experimental part.\nAccuracies are overall pretty good, competitive wrt state-of-the-art methods, showing that this approach is valuable in some way, but the provided accuracies alone are not sufficient, and more ablation should have been reported. Standard deviations should have been reported too as authors correctly made more (5) runs of the algorithms, but only average accuracies are reported. \nThe results refers to the number of generated images M=1, but it could have been interesting to see how the method behaves when M>1.\nThe final formulation of the classifier learning is using CE + a comparison between the outputs of the same model processing a sample and its augmented version, which is resembling contrastive (self-supervised) methods (e.g., simCLR): it could be interesting to see how this formulation could help in this case, or at least have a comment on it.\nL1 distance is a quite simple distance to use, and other distance functions could be tested: as probability distributions should be compared, L1 does not appear the more suitable distance to use, it can be a baseline.\nThe use of standard augmentation is not fully clear, as well as its relevance in the context of the approach, e.g., are generated data also augmented in the usual stardard way? Have you tried to change the standard augmentation functions and assess how much this affects the results?\n\nAs minor, I think that eqs. (7) and (8) should be checked out: eq. (7) about considering all source domains S, and eq. (8) in relation ot the pedices used (i,j for the # of samples and domains, see also eq. (7)).\n",
            "summary_of_the_review": "My main remarks regard points 1-3 reported in the main review box above, in particular, in relation to novelty, justifications and results/ablation.\nThe first 2 points are quite important to my opinion, and the latter should be structured to support them.\n\nIn these conditions, I cannot assign more than a reject rating for the moment, but I'd like to see authors' rebuttal and the comments of other reviewers for possibly revising my evaluation.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work aims to improve the model generalization ability by expressing and formalizing the real-world variations which have changes in style. Specifically, this paper leverages I2I model C-StarGAN to capture latent style-specific features from content-specific features of data, then proposes a domain-invariant regularization (DIR) loss function that enforces the learner (classifier) to be invariant to the former. \nExperimental results on several widely used datasets demonstrate that the proposed method achieves better performance than SOTA methods.",
            "main_review": "Strengths\n1. The proposed MI2I model C-StarGAN can capture the latent style-specific features from content-specific features of images, which allows to perturb the style information and generate new perturbation samples. Visualization of generated images in Fig. 5 and ablation study in Table 4 shows the effectiveness of the C-StarGAN model.\n2. They introduce a new regularizer (DIR) that enforces the classifier to be invariant to the underlying style-specific features of the data using L1 distance, the effectiveness of which has been shown in  Fig. 4 when lambda is 0.\n\nWeaknesses\n1. This work is based on the assumption that the style-specific feature space and content-feature space are disentangled, it would be better to visualize the content-feature z_c and style-feature z_s generated by the C-StarGAN model respectively to show whether the two feature spaces are indeed disentangled.\n2. Will replacing the L1 distance in DIR loss with other distance metrics further boost the model performance?\n3. The two main contributions of this work, the MI2I model and DIR loss function seem to be existing, which lack innovation.\n4. The key idea of this method is more like data generation/augmentation, which separates the style information and content information by some means and then perturbs the former while keeping the output robustness. There is a lack of comparison with these brunch of DG methods (e.g., mixstyle[1]ï¼Œfact[2]).\n [1] ICLR2021 : Domain generalization with mixstyle\n [2] CVPR2021 : A Fourier-based framework for domain generalization",
            "summary_of_the_review": "The two main contributions of this work seem to be existing, which lack innovation. There is a lack of comparison with the data generation/augmentation brunch of DG methods.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}