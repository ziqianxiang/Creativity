{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors proposed a new model for deep metric learning. The main claim of this paper is that actual models of DML are neglecting the relationship between manifolds/batches while considering only samples or samples to manifold relations. The proposed model goes in this direction combining a GCN that captures the relations between nodes and considers/tests multiple losses to constrain the model in learning a metric. The resulting model outperforms considerably the state-of-the-art on three publicly available benchmarks.",
            "main_review": "Strengthens: \n- The underlying idea is quite simple (GCN+MSLoss) but new in the field of DML.  \n- The reported performances, assessed on three publicly available benchmarks, outperform the SOTA.\n- The paper is appropriately written, easy to follow, and the literature is updated.\n\nWeaknesses:\n- The paper is a combination of known models (a GCN with an MSLoss); this partially downgrades it in terms of novelty.\n- I wonder why you are using just one GCN layer when your goal is to propose a model that should embed entire manifolds. One GCN layer = one-hop, so you are considering only the direct neighbors of each node within a minibatch, neglecting the relationships that are more distant (in terms of hops). I would have expected an ablation on this point.\n- If I am not mistaken, you are learning manifold-to-manifold embeddings, which means that you are learning embeddings that are robust across batches. To do so, you are using a GCN that captures local similarities. For instance, the Group Loss is doing the same, considering the similarities in a batch, propagating labels, and updating the embedding consistently. It is unclear how you prove (apart from the empirical results) that you are doing something different from other manifold-based approaches (GroupLoss, ProxyGML…). Maybe you can better highlight the differences with respect to other models in section 2.3 or provide a counterexample in which your competitors miserably fail while your model does not.\n- How did you find the best $\\epsilon$?",
            "summary_of_the_review": "I ranked this paper as “marginally below the acceptance threshold” due to its limited novelty (combination of existing models) and the doubts raised in the weaknesses. Is worth noting that the achieved performances on several benchmarks are good, so I hope the doubts can get clarified.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes appending a GNN on top of the backbones for deep metric learning; it formulates the proposed approach (GLE) as a generalization of the MLP case (which amounts to a \"graph\" with only self-connections, ie an identity affinity matrix). Substituting the MLP with a GNN on the kNN graph of all points inside the batch (regardless of class), the train their model for metric learning. During testing, they construct a similar graph by multiple augmentations of the query. They experiment on common metric learning datasets and report results when using their method on top of a number of popular metric learning loss functions.",
            "main_review": "**Strengths:**\n\n* The paper presents a straightforward metric learning method that uses a graph neural network to perform local message passing along nearby points on the feature manifold.\n* The paper presents interesting ablations and combinations of the proposed method with 4 popular metric learning losses.\n* The paper shows that creating a graph at test time using multiple augmentations of the query gives gains over the simpler single-query variant.\n\n**Weaknesses:**\n\nW1) The paper present the GLE approach as a generalization of the common MLP head and \"sample-to-sample\" methods. Yet, as they also note, there have been recent methods like MPN (ICML 2021) or ProxyGML (NeurIPS, 2020) that perform \"sample-to-manifold\". Even their one sentence summary points to the fact that  they \"identify the missing neighborhood relationships\" - however these have been exploited in the related works above, and this is not clear from the text. The differences and relations to these methods can be emphasised more.\n\nW2) Local neighborhood in testing: The authors propose to use 10 query augmentations when testing to create their graph. When they don't do that, the proposed method is denoted as GLE* and doesn't really show very large performance gains over linear and performs on par with the state-of-the-art (eg MPN a method that performs message passing on a fully connected graph). \n\nTest-time augmentations and averaging over multiple crops has been common practice eg for ImageNet classification benchmarking - yet, and given that this is a common way of having higher accuracy, results are reported separately for the cases with 1 crop and 10 crop. As no other methods use test-time augmentations (please correct me if I am wrong) the fair comparisson is between GLE* and the other methods (and not GLE)\n\nAlthough I understand that the proposed method has a nicer way of fusing results from multiple augmentations at test-time compared to simply averaging, CLE can only be fairly compared to other methods with the same test-time augmentation strategy. Note that there is a discrepancy here with training: at test time the complete graph comes from one class (while during training from P) \n\n\nW3) Quality of the graph and performance gains: According to the motivation and presentation of the method, a \"better\" graph would give better results. It is unclear to me if that is the case as:\n* The method seems not sensitive to batch size (Table 1), despite the fact that the graph is constructed within the batch. This is unintutive and makes me think that the quality of the graph is not that important - gain (eg over linear) could come from the added parameters of the model. Where do the authors attribute this? The manifold construction method is used for Table 1 is unclear.\n* Regarding not restricting the graph to neighbors (there are class labels during training): How does performance change if you restrict the neighbors to ones from the same class? I am not really convinced with the reasoning in 3.2 - assuming that results are not better than the unrestricted case, this also points towards the quality of the graph not really correlating with the performance gains. See also Q4 and Q5 below.\n\n**Notes and questions:** \n\nQ1) Wouldn't an MLP be another baseline for Table 4? \n\nQ2) For table 4, what would be the performance of Linear* ie a variant  with a linear head and 10-augmentation averaging at test time?\n\nQ3) Why only add one GCN layer? Does performance drop when adding more layers?\n\nQ4) How \"clean\" are local neighborhoods? eg before and after training - Some visualizations with things that are close-by in the manifold and therefore exchange information with the \n\nQ5) (srory if i missed it but) what is the value selected for k for the k-NN graph? An ablation for the k and epsilon is missing.\n\n\n",
            "summary_of_the_review": "Although interesting and straightforward, the method presented in this paper is not presented, compared or ablated in a fair way. There are gains over using a linear head, but there seem to be no gains over using a transformer head as in [MPN] (W2). There are some results that make it unclear to me if the gains come from the right place, ie the graph and local connections (W3) while visualizations and insights are missing. Looking forward to the authors' response.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes the Graph Local Embedding for deep metric learning, which comprehensive identify the missing neighbourhood relationships in the traditional matric learning framework. Specifically, the proposed method explores the local relationships and draws on the graph convolution networks to construct a discriminative mapping for embedding learning. Such strategy can enhance metric learning by exploring the manifold-to-manifold relationships. Extensive experiments illustrate the effectiveness of their proposed method on popular fine-grained retrieval benchmark dataset.",
            "main_review": "Strengths:\n\n1、\tThis paper proposes a metric learning framework by constructing a robust GCN embedding which considers the local manifold relations.\n\n2、\tThe proposed method achieves comparable results or significant performance gains over other SOTA pairwise metric learning techniques on the fine-grained image retrieval benchmark datasets.\n\nWeakness:\n\n1、\tIn section 3.1, what’s the difference between Eq.(1) and Eq.(2)? Why multiplying an identity matrix on Eq.(1) can address the problem “each data point is associated only with itself and does not link to others” ? This does not clearly described in the manuscript.\n\n2、\tWhat’s the difference between the proposed manifold-to-manifold embedding learning with the sample-to-sample embedding, if the sample is selected from each manifold by some hard mining technique ?\n\n3、\tIn section 3.3, the authors describe that “Comparing with the typical graph convolution operation, the obtained local manifold G is sparse and exhibits essential local neighboring relations. It can consequently result in a robust embedding more relevant to the subsequent metric learning.” Why the local manifold G is sparse? If the G is sparse, it seems that the graph G may contain less local structure information.\n\n4、\tHow to integrate Eq.(5) into the GCN framework is not clearly described;\n\n5、\tThe loss function used to train the metric learning is “Sn-Sp”, Do you use the margin?\n",
            "summary_of_the_review": "This paper proposes proposes a metric learning framework by constructing a robust GCN embedding which considers the local manifold relations. However, this manuscript is not well-written, there contains many points which didn't clearly described and  make me very confused. Specifically for section 3.3 and 3.1, some details should be provided, such as using some fugures to illustrate some important part. I believe I am an expert in the field of deep metric learning.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this work, the authors propose to mitigate the missing neighborhood relations in prior works and propose a graph local embedding approach for deep metric learning. They propose to explore local relationships using graph convolutional networks to construct discriminative mapping for embedding learning. The authors first illustrate the inefficiencies in prior works and then show how graph convolution embedding improves performance on metric learning task across three popular deep metric learning benchmark datasets.",
            "main_review": "**Strengths**\n\nThis work has the following strengths:\n1. The overall idea of using manifold-to-manifold relation instead of sample-to-manifold and sample-to-sample relations to preserve local neighborhood relations is interesting.\n2. Overall, the paper is easy to follow (except for a few mathematical notations as pointed below).\n3. Experiments conducted across three popular metric learning papers are extensive.\n\n\n**Weakness**\n\nI appreciate the author’s efforts in developing this idea. However, this work has the following issues which I would like the authors to address. Under each subsection, the weaknesses are marked as P$_n$ where P is priority and n is its number (lower number indicates higher priority)\n\n***[Method]***\n\nP$_1$: *[Graph embedding vs. proxies]* The authors illustrate through Figure 1(c) that Proxy Anchor (Kim et al., 2020) does sample-to-manifold matching and the proposed method does manifold-to-manifold by using GCN embedding. But this manifold-to-manifold relation can also be achieved by using only proxies and does not explicitly require one to use GCN embeddings for this purpose. For e.g. one can construct a per-class proxy and achieve manifold-to-manifold relation and do a contrastive loss between proxies without having a sample-to-manifold relation. So the use and motivation of GCN embedding over proxies is not clear. Furthermore, the authors do not empirically show the difference between GCN and proxies in terms of performance.\n\nP$_2$: *[Mining strategy]* In section 3.1 and 2nd paragraph of the introduction, the authors claim that the effectiveness of prior methods is based on effective mining strategies (Also claim-1 in Introduction). However, by using k-nn and $\\epsilon$-ball to construct the affinity graph, the authors also heavily rely on hard mining strategy. So I don't see the mining strategy used by prior works as an “inefficiency” as the authors claim, since they also use it in their work.\n\nP$_3$: *[Affinity graph]*: In section 3.2, the authors construct an affinity graph by using cosine similarity between images $x \\in B$. However, this approach would be computationally expensive. A feasible approach would be to use similarity between embeddings which is much faster since it is just a vector dot product.\n\nP$_3$: *[Inter-class overlap]* In Section 3.2, the authors compute the inter-class overlap in the image space rather than the embedding space. The reason for this approach is unclear.\n\nP$_4$: *[Notation issues]* In Section 3.1, the authors mention that the deep features X “is an element” in $\\mathbb{R}^{n \\times d}$. This means that the deep feature $X$ is obtained from a linear layer (Fully-connected) since the authors define $n$ as batch size. I can assume that $d$ is the embedding dimension. So why are these deep features being passed through another MLP? Do the authors use 2 FC layers in their architecture? This is unclear.\n\nSecondly, $F_{linear}$ is an element in $\\mathbb{n \\times m}$. The authors did not define what $m$ is. I would urge the authors to redefine these variables correctly using proper mathematical notations. For e.g. $W : \\mathbb{R}^{n\\times d} \\rightarrow \\mathbb{R}^{d\\times m}$ etc.\n\n\n***[Experiments]***\n\nP$_1$: *[Unfair experimental settings]* In Section 4.2, the authors claim that they train a one-layer GCN embedding \\emph{and} an embedding of dim=512. However, this is an unfair comparison with other baseline methods, since these baseline methods use only an embedding of dim=512 and do not use additional information (e.g. GCN embedding in this case). Could the authors please clarify this.\n\nP$_2$: *[Loss functions]* Could the authors please clarify what loss functions were used in Table 1, 2, 3 and 5 because we observe different numbers as compared to Table 4. \n\nP$_3$: *[Effect of batch size]* In Table 1, the authors show the effect of batch size when using GLE. However, based on the claims of authors that using GLE preserves local neighborhood relations, it is difficult to map it to the results we observe from Table 1. Intuitively, we should observe that increasing batch size should increase the performance since the method would identify semantically meaningful relations, but this does not happen. Could the authors please elaborate on this observation?\n\nP$_4$: *[Missing comparisons]* In Table 5, the authors miss comparisons with Proxy Anchor (Kim et al, 2020), ProxyNCA++ [4] and EPSHN [5]. It would also be interesting to understand the effect of using GLE with Proxy Anchor loss in Table 4.\n\n\n***[Introduction]***\n\nP$_1$: *[Unsupported claims]* In the first paragraph, the authors claim that works like Proxy Anchor (Kim et al., 2020), calls for rigid training and does not generalize to unseen classes. I find this to be untrue because Proxy Anchor supports faster training (empirically shown in their work) and also generalizes well to unseen classes (evaluated on fine-trained retrieval datasets such as CUB etc.). Furthermore, the authors also claim that training with random sampling causes too many redundant pairs, but works like Multi-Similarity loss (Wang et al., 2019) propose a simple mining strategy to mitigate redundant pairs. In Section 2.3, the authors claim that ProxyGML (Zhu et al., 2020) uses multiple intra-class proxies. However, the authors of ProxyGML claim that their method uses fewer proxies. Could the authors please clarify this discrepancy. I would urge the authors to support such claims empirically or by citing works that have such claims. \n\n***[Minor issues which need to be fixed]***\n\n1. *Error in references*: The references of contrastive loss and triplet loss are incorrect. Please use [1] for contrastive and [2] for triplet loss. Also, please cross-check the references of all deep metric learning loss from [3].\n2. *Description of triplet loss*: In section 2.1, the description of triplet loss is missing details about the margin. I would urge the authors to rephrase and include the significance of margin in triplet loss.\n3. *Figure 1 (a), is not clear*: Could the authors please clarify what blue, yellow and orange dots at the output of the encoder indicate? Does each point represent the embeddings from the mini-batch of images as anchor, positive and negative? \n4. *Grammar*: Section 3, 1st para - in pairwise DML,..which are less informative and cause unexpected... - needs to be corrected as - In pairwise DML,.. which are less informative, cause unexpected…..\n\n[1] Hadsell et al., Dimensionality reduction by learning an invariant mapping, CVPR 2006.\n\n[2] Weinberger et al., Distance metric learning for large margin nearest neighbor classification, JMLR 2009.\n\n[3] Musgrave et al., A metric learning reality check, ECCV 2020.\n\n[4] Teh et al., Proxynca++: Revisiting and revitalizing proxy neighborhood component analysis, ECCV 2020\n\n[5] Xuan et al., Improved embeddings with easy positive triplet mining, WACV 2020.\n\n",
            "summary_of_the_review": "This work has several claims which are incorrect - about Proxy Anchor and other deep metric learning methods as pointed above. Furthermore, the experimental settings and comparisons are unfair. Based on all the above mentioned weaknesses, I recommend a *reject* as my initial recommendation since the authors need to improve the work in its current state. \nI would also request the authors to please justify all the above mentioned weaknesses.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}