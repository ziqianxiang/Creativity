{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents a mixture of experts approach to hierarchical time series forecasting. The main contribution of the paper is to use a gated network to combine predictions from specific expert models. An additional contribution is the addition of quantiles using a model-free approach that avoids quantile crossing.\n\n ",
            "main_review": "The paper does present novel and coherent contributions. The empirical results present quite a detailed exposition of experiments in both paper and supplementary material.\n\n- The paper is well written and structured\n-Method is compatible with other state of the art forecasting methods such as FBProphet\n \n\nThe main weakness of the proposed model is that the overall probabilistic forecasts might still be subject to the dominance of weak experts at test time. Have the authors' considered sparsity control at test time perhaps on the basis of some measure of predictive quantile width? This is likely to be an issue in low data regimes. \n- authors also do not thoroughly investigate the quality of uncertainty estimates emanating from their method using NLLs etc.\n\n",
            "summary_of_the_review": "The paper presents a novel method in hierarchal time series forecasting. This is an understudied area in literature. The paper provides an empirical evaluation of the method. Two potential pitfalls of the work include the potential dominance of weak exports and the lack of evaluation of the quality of uncertainty estimates.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a method that combines forecasts from different models and aims to produce coherent forecasts subject to hierarchical constraints imposed in advance.\n",
            "main_review": "Overall the paper was well written and attempted to solve very practical problems. \nHowever, I am unsure about a few key basic assumption of the methods: (1) why the point forecast should be ideally subject to additive constraints? (2) misalignment between the evaluation losses and the training objective functions.\nThus I think the paper could be a stronger publication in future if those key design was addressed.\n\nSome more details comments:\n(1)\tOn page 1, ‘A well-generalized forecasting model should not only...’, it’d be good to either add references to elaborate why ‘accuracy’ is not sufficient – was it because generalization concerns or was it because application requirements?\n\n(2)\tOn page 3, the condition following ‘Ideally, the forecasts should satisfy’is not well motivated – it suggests that for any given point forecasts x, we should consider the constraint that the point forecast should follow additive condition. However, this claim was not sound – the claim would be correct assuming all point forecasts are mean estimates by the linearity of expectation, but what if the point forecast is a quantile forecast? In the quantile cases, the equality doesn’t have to hold unless further assumptions.\n\n(3)\tIn Section 3.1, should ‘confidence intervals’ be replaced with ‘prediction intervals’?\n\n(4)\tIn Proposition 1, it makes a strong assumption of ground truth should follow in the intervals of min and max of point forecasts. Such assumption was impractical and not well motivated – for example, if the point forecasts are all mean forecasts, the probability of the realization falling into the intervals of X mean forecasts could never be 1 and thus the assumption would never guarantee to hold. With such unrealistic assumption, I could not see the value of the proposition. This issue was consistent with the findings in Section 4.3 ‘we found it not easy to eliminate the coherent loss...’.\n\n(5)\tIn Section 4.1, it’d be good to add what objective functions were used for some of the point forecast models.\n\n(6)\tIn Section 4.1, need to add reference for the four public datasets.\n\n(7)\tIn Section 4.1, it’d be good to add why MASE was chosen to evaluate the forecasts – if MASE was used for evaluation, why can’t it be used as an objective function into the models? What about evaluating for quantiles other than median estimates?\n",
            "summary_of_the_review": "Overall the paper was well written and attempted to solve very practical problems. \nHowever, I am unsure about a few key basic assumption of the methods: (1) why the point forecast should be ideally subject to additive constraints? (2) misalignment between the evaluation losses and the training objective functions.\nThus I think the paper could be a stronger publication in future if those key design was addressed.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper deals with hierarchical forecasting using a mixture of heterogeneous experts. To do so, the proposed method learns hierarchical relationships during the training stage to produce point forecasts. Multiple quantile estimators are used to producing distribution-free and nearly coherent probabilistic forecasts. The authors also discuss an extension for situations where change points happen in the future. A comprehensive evaluation is conducted on both point and probabilistic forecasts which show increased accuracy compared to baselines.\n",
            "main_review": "Strengths:\n- The combination of multiple forecasts for each series is a challenging problem in hierarchical forecasting (especially if the resulting forecasts are coherent).\n- Another contribution is the extension to the probabilistic setting with quantile forecasts.\n\nWeaknesses:\n- This paper tries to do too much. I think it is better to do less, and better motivate and explain each component of your method. In particular, Section 3.2 is hard to read. It introduces multiple new concepts with few explanations.\n\n- There are inconsistencies in your statements. First, you say \"However, this approach does not perform very well in our implementation when combined with mixture-of-experts.\". But later, you say \"In addition, we intended to compare with Rangapuram and Taieb but their state-of-the-art implementaitons are not available.\".\n\n- Similarities and differences with existing methods are missing. In fact, your method can be seen as a generalization of the method proposed by Taieb (ERM method).  Under a gaussianity assumption, MLE reduces to Least-squares. Expression (2) is a (regularized) ERM objective. The main difference is that you are considering a dynamic and nonlinear combination of the forecasts and you allow coherency errors. In other words, if MECATS uses static linear combination (no hidden layers) and lambda_K goes to infinity, it is equivalent (or closely related) to ERM. This also suggests that you can easily compare with the ERM objective. This should be discussed in the paper. \n\n\nOther comments:\n- Section 3, you say: \"Ideally, the forecasts should satisfy ... \", but this is not a constraint. In other words, probabilistic forecast coherency is ill-defined in your paper.\n- \"Note that other reconciliation methods like MinT, requires forecasting model at every vertex to be unbiased.\". Reconciliation methods do not require unbiased forecasts. Forecasts are often biased in practice and these methods still work. However, the theoretical properties are valid only under unbiased forecasts. Since you do not give any theoretical guarantee for your method, you cannot give that as a disadvantage of reconciliation methods. Please update the text accordingly.\n- How did you compute CRPS? Please add a link to the appendix.\n- L_v_i in (2) is not explicitly defined.\n- Furthermore, you say: \"Note that it is straightforward to extend linear aggregations to the non-linear case\". Are you talking about problem definition or forecasting or both?\n- \"A model set with both high-variance and low-variance models\". I am not sure this sentence if useful.\n- \"it is unreasonable to simply combine their conﬁdence intervals.\". What do you mean by confidence intervals? Are you not trying to generate forecasts? This sentence needs clarification.\n- The expression after expression (6) uses an equal sign. Should it be an approx. sign?\n- Section 3.2 is poorly written. The DCT algorithm is not introduced. Please add a reference. Also, later you talk about \"type-II DCT transformations\" without introducing it.\n- Figure 5, what is quantile coherency?\n- \"Building this connection to the combined point forecast regularizes each quantile estimation to be more coherent.\". Why is that the case? Did you prove it theoretically or experimentally?\n- \"to eliminate the coherent loss\". What does that mean? To make it equal to zero?\n- Clearly define an \"uncertainty wrapper\".\n- What do you report after the \"+/-\" in the Tables? (exact formula)\n\n- Proposition 1: what is \"coherent error\"? Why are the weights between 0 and 1?\n- What do you mean by \"ideal value\"?\n- Table 4-20: why all CRPS values are in bold font?",
            "summary_of_the_review": "\nA major weakness of this paper is that it introduces too many concepts and goes in too many directions. There is a lack of focus. Clear and strong justifications for each component of your method are missing. As a result, it is hard to pinpoint the true contribution of the paper. Furthermore, when looking at the results, it is not clear that this complexity is needed given the small improvement compared to the (much simpler) existing methods. Finally, the paper is not easy to read. Writing should be improved. In its current form, I do not think the paper is ready for publication.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}