{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work addresses the problem of efficient semantic code search where the database consists of code snippets and the queries are in natural language. It proposes a hybrid approach that ranks code snippets in two stages where in the first stage a fast retrieval model is employed to retrieve a small set of candidates from the database which are reranked in the second stage using a slow classification model. Such a hybrid approach would naturally give a trade-off between query response time and retrieval efficacy that might be desirable when either the fast retrieval model in itself is not very effective and the slow classification model is too slow to be useful on its own.",
            "main_review": "This work addresses the problem of efficient semantic code search where the database consists of code snippets and the queries are in natural language. Tools that provide such a search feature can potentially be useful in increasing the productivity in software development by allowing reuse or repurposing of existing code snippets. \n\nExisting solutions to the problem of semantic search either use relatively less effective but fast retrieval models or somewhat more effective but slow classification models. The current work proposes a hybrid approach that ranks code snippets in two stages where in the first stage a fast retrieval model is employed to retrieve a small set of candidates from the database which are reranked in the second stage using a slow classification model. Such a hybrid approach would naturally give a trade-off between query response time and retrieval efficacy that might be desirable when either the fast retrieval model in itself is not very effective and the slow classification model is too slow to be useful on its own.\n\nHowever, the idea of using a two stage approach to retrieval ranking is not new. It is a fairly old and well-worn technique in Information Retrieval literature. See, for e.g., [1] for an application of this technique in the area of Natural Language Processing. \n\nIn order that the hybrid approach give the desired trade-off between query response time and retrieval efficacy, it is necessary that the first stage involving the fast retrieval model has very high recall (but not necessarily very high precision) and the second stage involving the slow classification model has very high precision. The current work ensures the first by making use of a competent fast retrieval model and also the second through a competent classification model. Both the fast retrieval model (encoder) and the slow classification model (classifier) make use of Transformer architecture. A contrastive loss, infoNCE, is used in the training of the encoder whereas cross-entropy loss is used in the in the training of the classifier. Both make use of bimodal dataset consisting of positive pairs and negative pairs.\n\nOne consequence of using two different models in a hybrid approach such as the one proposed in the current work is that the memory footprint of the models is substantially increased. This issue has a trivial solution in the form of sharing of model parameters which is exactly what the current work also employs. Parameter sharing comes at the expense of some drop in retrieval effectiveness.\n\nExperimental results on CodeSearchNet dataset shows that the proposed hybrid approach gives marginal improvement in retrieval efficacy (as measured in MRR) over the baseline fast retrieval model (0.76 vs 0.74 with shared parameters and 0.77 vs 0.74 with separate models). However, this marginal improvement comes at significant increase (2.5 X to 7X) in query response time over the fast retrieval model. Compared to the slow classification model, the hybrid approach is slightly less effective (with same model size) or nearly the same (with double model size).\n\nA key concern in the context of the proposed work is the gap (in MRR) between fast retrieval model and slow classification model is actually small - 0.74 vs 0.78. The proposed work attempts to bridge this small gap using a hybrid approach but at the cost of slower query response time. It is conceivable that future improvements in the fast retrieval model or even better parameter tuning of the current model might make the fast retrieval model as effective as the slow classification model while not losing its efficiency advantage over both the slow classification model and the hybrid approach. It might also be possible to get better retrieval efficacy by modifying the loss function suitably to boost precision. \n\nSome comments/questions:\nEven with K = 10, the classifier phase of cascade takes significantly more time than fast encoder search. Why?\nLeft parenthesis is missing in Equation 1.\nIn Section 3, 'separat' should be corrected to 'separate'.\n\n[1] A Cascade Ranking Model for Efficient Ranked Retrieval\nhttp://www.jimmylin.umiacs.io/publications/Wang_etal_SIGIR2011.pdf\n[2] Hashing-based Approaches to Spelling Correction of Personal Names, EMNLP 2010\nhttps://aclanthology.org/D10-1122.pdf",
            "summary_of_the_review": "The proposed work attempts to bridge the gap between fast retrieval models and slow classification models using a hybrid approach that uses both. However, the gap is rather small and will quite likely be bridged by enhancements to fast retrieval model by way of better parameter tuning, loss function fine tuned for the application, more data or a combination. Compared to the fast retrieval models, the proposed hybrid approach is significantly slower while providing only marginal improvement in retrieval effectiveness. Further, there is no appreciable novelty in the proposed approach. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose a cascaded approach of semantic code search based on establishing fast and slow models. A transformer encoder model first creates an index for fast retrieval, and a classification-based reranker fine-tunes the top rankings. Experiments are conducted on six datasets of different programming languages. The experimental results show that the proposed method outperforms several baseline methods.",
            "main_review": "Strengths:\n\n* The transformer-based encoder models both natural language and programming language, thereby providing the foundation for indexing and classification-based re-ranker.\n\n* The fast encoder efficiently filters out irrelevant candidates so that the re-ranker can focus on the ones that are more relevant. \n\n* Publicly available datasets for reproducibility.\n\nWeakness:\n\n* The idea of using a fast retriever to select limited candidates for reranking is not novel. Especially in the field of deep learning of ad-hoc retrieval, it is almost impossible to use the ultimate deep models to derive relevance scores over the whole corpus.\n\n* The description of the method is unclear. Lots of the details are missing, such as how to derive KNNs and essential hyper-parameters.\n\n* The experimental setup is also unclear. For example, how the slow classifier (and the ones without pre-filtering) derive the final ranking?\n\n*  The writing also needs to be polished. There are several grammatical errors and formatting issues. \n\n",
            "summary_of_the_review": "Due to the limited novelty and unsatisfactory quality of description and writing, I would recommend \"5: marginally below the acceptance threshold\". ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper contributes an approach for retrieving semantically relevant programming code, in response to a natural language query. The approach consists of a jointly trained transformer encoder (to retrieve from the index) and a re-ranking component (to improve top-k performance). The authors find improvements over SOTA baselines in terms of effectiveness (MRR, recall) and efficiency (runtime approximations).",
            "main_review": "Strengths:\n- The experiments use known SOTA baselines and a benchmark dataset that is available to the community. \n- The components of the architecture are also known to the community and described in a way that can be, within reason, approximately reproduced.\n- There is clear motivation for the task of searching programming code, and also for the specific architecture used in this work.\n- The paper is overall clearly & coherently written.\n\n\nWeaknesses:\n- In section 3, the authors state that when using a transformer, the interactions between the natural language (NL) and programming language (PL) tokens in the self-attention layers can help improve the precision of the authors' approach of jointly encoding NL & PL, as opposed to encoding them independently. It is well known however that self-attention in transformers is in fact more messy than that. For instance, while some heads learn to assume indeed salient roles, other heads learn to assume non salient roles and can be pruned as redundant without impacting performance, while other heads, when pruned, can even lead to improved effectiveness (indicating potential noise in their learned roles). See Michel et al. NeurIPS 2019 and Voita et al. ACL 2019, for example. \n\n- In section 4, the authors state that they use a version of the CodeSearchNet code corpus that has been previously used by Guo et al. 2021, where low-quality queries are filtered out and the retrieval set is expanded, so that the corpus is more challenging and realistic. Removing low-quality queries renders a dataset neither more challenging, nor more realistic. Quite the opposite.\n\n- In section 4, the two effectiveness evaluation measures used are MRR and recall@K. MRR considers only the rank of the first relevant retrieved item, which means that it is created for search tasks where we only care about the first correct answer. It is not argued in the paper why code search as a task should be evaluated with this measure precisely. It is not clear that code search is a single-hit type of retrieval task. I understand that prior work has also used this measure and that re-use of it facilitates comparison to prior work (which I agree with). However, the driving force for choosing an evaluation measure should be its fit to the task. This should be argued in the paper, because it is not self-evident. The second evaluation measure, recall@K, is defined as the number of retrieved items in the top K divided by the total number of relevant items in the corpus. Recall@K assumes that all relevant items, for each query, have been identified in the corpus. It is not clear if this is the case or not here. Is the whole corpus (including its modification by Gun et al.) exhaustively annotated for relevance? Finally, the combination of MRR and recall@K as the two sole evaluation measures of effectiveness is not optimal. On one hand, MRR disregards any relevant items (their number and their ranking) except the single top retrieved relevant item. On the other hand, recall@K ignores the rank position of retrieved relevant items, and just considers their number within the top K (i.e. recall treats the ranking as a set). Ranking is an important quality in search, which is why so many ranking-based evaluation measures are used to measure search effectiveness (e.g. NDCG, MAP, bPREF, etc.). \n\n- In Table 3, some missing SOTA baselines are GPT-C by Svyatkovskiy et al. 2020 https://arxiv.org/pdf/2005.08025.pdf and C-BERT by Buratti et al. 2020 https://arxiv.org/pdf/2006.12641.pdf.\n\n- Are the results reported in Section 4 the average over several runs? More details should be provided. Also, no statistical significance test results are reported. This should be amended.\n\n",
            "summary_of_the_review": "The work is interesting in itself. The topic is timely and has merit. The experimental evaluation is somehow limited, in the choice of evaluation measures, lack of statistical significance testing, choice of baselines, and lack of precise details about some testing methodologies. Overall, it is not enough to run experiments and report scores. Scores should be analysed to make sure they are indeed indicative of the claims made in the paper. There is not much experimental analysis of that type in the paper or in the appendix.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper focuses on the code samples retrieval problem that can be used to improve code reachability inside organizations - given a set of the code samples, retrieve the ones which are relevant to a text description of the code. Authors argue that the task has a reasonable value for practical applications. \nPaper discusses improvement of technical details over the previous works, like longer training/bigger batch etc and employ cascaded approach based on the 1) embedding of code samples into a constant vector space and retrieving top-K elements which are closest to the query; 2) re-rank the retrieved elements using a heavy ranker, which is typical for most practical information retrieval solutions.\nStudied solution gives better performance on the CodeSearchNet dataset exhibiting higher metrics compared to previously reported numbers as well as reasonable latency.\n",
            "main_review": "Strengths: \nPaper is clearly written, provides a seemingly reasonable solution for the code search problems and shows improvement over the previously reported results. I guess it might be interesting for those who build practical code search systems.\n\nWeaknesses:\nThe main weakness with the paper is lack of context from general information retrieval works as the problem is very similar. There are production-ready solutions that can do the same two proposed steps (and overall the concept of candidate generation and heavy ranking is very old and used ) and even more, e.g. late fusion between query and the candidate items [arxiv 2004.12832]. I would point to the vespa's blogpost discussion of an already working production solution https://blog.vespa.ai/pretrained-transformer-language-models-for-search-part-1/ \n\nIt also not very clear what is the performance of the slow part of the proposed encoder alone (applied to the whole corpus). There is a plot in Fig. 4, but it is hard to read only only up to 6K samples. It is not clear if the observed bump in metrics for heavy classifier is due to filtering from the fast encoder or due to a better heavy classifier.\n\nFrom a minor point, it would be nice to add actual time in Fig. 2, so the reader can understand what is the improvement without looking in the table.\n",
            "summary_of_the_review": "The paper lacks comparison/context from the information retrieval and the proposed solutions with fast candidate generation and heavy transformer-based re-ranking that are now available as out-of-the-box production systems.\nI think that after adding the information retrieval context the paper might fit to more specialized or practical conferences, but lack of novelty makes it hard to recommend for ICLR.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}