{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper introduces an encoder to the GANs setting to alleviate the mode collapse problem. The encoder is designed to behave like the inverse of the generator during training. This property of the encoder, allows the model to find the latent distribution of the real images during training by just simply passing images through the encoder. Having the real latent distribution and trying to make it converge to the standard normal distribution (the distribution of the generator at the test time), allows the model to avoid mode collapse. The authors provided illustrative examples on some toy data and large-scale experiments on CIFAR-10, CIFAR-100, and STL-10.",
            "main_review": "Pros:\n\nThe paper is well-written and easy to follow.\n\nI like the main idea of the paper, although there are several very closely related methods both in GAN and VAE.\n\nCons:\n\nThe main problem, I have with the paper is the lack of experimental analysis. The authors claim that their method can be regarded as an orthogonal plug-in for existing GAN models that can improve their performance. However,  they even did not try to apply their method to SN-GAN which is a well-accepted baseline method. The current experimental results in terms of FID and IS are even inferior to the methods proposed two years ago like SN-GAN. The authors need to apply their method to different architecture BigGAN, SN-GAN DCGAN to show the empirical effectiveness of their method.\n\n\nSince $G^{-1}$ is mathematically ill-defined for the $x$ which are not in support of the $G$, therefore proposition 1 is not applicable to the hard mode-collapse problem which has more importance in the literature than soft mode-collapse.\n\nThe effect of \\lambda's in equation 7 has not been investigated enough empirically.",
            "summary_of_the_review": "Please consider the above comments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposed to solve the mode collapse issue of GANs from IID sampling perspective. In particular, a cycle consistency loss for reconstructing data and latent vectors and a Gaussian loss to align the inverse distribution of real data with a standard Gaussian are added onto the objective of original GAN. Extensive experiments are conducted to show the effectiveness of the proposed method.",
            "main_review": "I think the main claim of the paper is not convincing and the proposed idea has already been explored by the community. To be specific,\n\n1) I am not fully convinced by the authors’ argument that generation with IID property will avoid mode collapse. From what the authors discussed, Page 3 “Based on Def. 1, … the generated samples {G(zi)} can also be viewed as IID from β due to the equal probability”. The IID is only a necessary condition for mode completeness. In addition, intuitively, when the generator collapses to few mode, partial of the whole distribution, which could also maintain IID property. For example, the whole distribution is a mixture of standard Gaussians. The generator converges to only one of the Gaussians,  i.e., a standard Gaussian, which maintains IID property.\n\n2) The proposed method is very similar to [1], which also has a loss to align the inverse distribution of real data with a standard Gaussian and a cycle consistency loss. Thus this paper has very limited novelty.\n\nSome minor issues:\nTypos in equation 1, the equation in the last line of Page 3; inconsistency of the denoted mapping in proposition and proof.\n\n\nReference\n\n[1] Ulyanov, Dmitry, et al. “It Takes (Only) Two: Adversarial Generator-Encoder Networks.” AAAI, 2018, pp. 1250–1257.",
            "summary_of_the_review": "The authors’ argument that generation with IID property can avoid mode collapse is not convincing. The proposed method has been explored by the former work.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to alleviate mode collapse in GANs by training an additional network to approximate the inverse generator, that would encourage the generator to produce samples are i.i.d. by optimising a cycle-consistency loss.",
            "main_review": "Overall, the approach of trying to ensure i.i.d samples in data and latent space of the generator is interesting. The method presented here and appears to produce better results than existing methods (VEEGAN, BiGAN, etc).\n\nHowever, I have some concerns about the novelty of the approach: \n1. The method presented in the paper appears to be a variant of VEEGAN or BiGAN with a different loss function. Although there is extensive discussion of related work in section 2 and in parts of section 3, these explanations are quite confusing. For example:\n- while describing  the work of Nguyen et al (2017), Ghosh et al (2018), the argument for IID-GAN versus these methods is summed up by \"these models do not essentially solve mode collapse\"; \n- while describing the connection to VEEGAN in section 3, \"the essential difference is that VEEGAN does not take an IID perspective\"; \n- while discussing cycle-consistency loss in section 4.2, \"it  cannot help avoid mode imbalance [...] individual data points studied here.\" \nSuch sentences make arguments presented in the related work sections more tautological than illuminating.\n\n2. Without the benefit of clear explanations regarding previous work, the claim that most of these methods are orthogonal to the method presented here, and can be combined to improve stability also needs more clarification.\n\n3. The paper also claims that the VEEGAN discriminator enforces $G^{-1}(x)$ to be close to 0: I did not find an explanation for this in the VEEGAN paper. Could the authors please clarify?\n\n4. The paper uses the 2-Wasserstein distance for the consistency loss, even though Appendix B presents other alternatives (p-norm, KL divergence etc.). Could the authors provide an explanation for the choice of the Wasserstein distance over the other possibilities?\n\n4. There is an overall lack of clarity in the writing: clarifications of related works are spread out over sections 2, 3, and 4; the results section is also quite confusing to follow since the tasks on which the methods were fit are described before the results of the experiments; and explanations of some of the analysis (inverse samples, generalisation with bad initialisation) are either too brief to understand their purpose or confusing.\n\nMinor comments:\n- Font styles are inconsistent across figures\n- There are odd light grey lines bordering Figure 6, and the axis labels are hard to read.\n- There were many typographical errors throughout the paper -- the manuscript would benefit from a thorough edit.\nSome other concerns ",
            "summary_of_the_review": "There is a lack of clarity in the writing, and I have concerns about the novelty of the method presented here. I would be happy to re-evaluate if there were clearer explanations of connections to previous work in conjunction with the method presented here.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a method to improve the mode coverage of GANs, by regularizing the cost function using the IID sampling perspective of target distribution. The main idea is that a generator satisfying the mode completeness property should necessarily make the inverse samples from target data to be IID as well in the source distribution. By training an additional inverse mapping of the generator and enforcing the inverse samples of the real data to follow a Gaussian distribution in the latent space, the proposed scheme achieves a better mode coverage. ",
            "main_review": "Strength-\n\n* Idea is clean: the proposed method regularizes the GAN training to make the inverse of training data to satisfy IID property, which turns out to be helpful in improving the mode coverage in some setups.\n* Synthetic data experiment: The effectiveness of the proposed idea is well demonstrated with simple synthetic datasets, including Ring dataset and Grid dataset.\n\nWeakness-\n* Lack of theory: theoretical justification of the proposed idea or heuristic arguments that the proposed idea does not hurt the quality of sample generation of GANs will improve the manuscript much. Even though IID property of inverse samples of the real data can be interpreted as a necessary condition for mode completeness, it is not clear whether the idea may hurt the overall performance of sample generation of GANs. \n\n* Scalability is another issue: The proposed cost includes additional computational overhead to make the reconstruction error and the Gaussian consistency loss small. This relatively-high computational overhead may make it difficult to apply the proposed idea to large state-of-the-art GANs.\n\n* Lack of real-data simulations with SOTA GANs: the experimental results are limited to basic GANs and not applied to SOTA GANs such as StyleGAN2. For example, FID of CIFAR-10 with SOTA GANs are much lower, say less than 20, than the reported number, which is above 60.\n",
            "summary_of_the_review": "Even though the proposed idea to improve GAN by using the IID perspective of training data could be helpful in improving the mode coverage, the overhead in computation seems to be high and the simulation with real data is not enough to support that the idea is scalable to SOTA GANs. Moreover, theoretic justification of the proposed idea is not very strong, since it is only based on the necessary condition to support a very strict mode completeness assumption. Overall, I think either more complete theoretical justification or simulations with real data on SOTA GANs will be necessary to improve the current manuscript.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}