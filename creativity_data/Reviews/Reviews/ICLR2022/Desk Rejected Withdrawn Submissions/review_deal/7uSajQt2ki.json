{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "\nThe paper presents a model to perform cross-lingual Conversational Semantic\nRole Labeling (CSRL a newly proposed task with a Chinese annotated dataset),\nand a method to apply CSRL in the target language to conversational\ntasks. The results of both models are positive.\n\n",
            "main_review": "\nStrengths:\n\n- The paper is well-written in general, and presents extensive experiments\n  with positive results. \n\n- The presented model is transferred successfully to English, and CSRL shown\n  to improve results on two downstream tasks. The fact that predicate\n  argument structure discovered across utterances in a dialogue help\n  conversational tasks is an appealing one.\n\n\nWeaknesses:\n\n- From the current writing, it is not clear whether the model (except the\n  use of XLMR instead of BERT) is novel. In the same vein, it is not clear\n  whether the method to add CSRL information in conversational tasks is\n  novel. In this sense, the paper seems to show for English what (Wu et\n  al. 2020; 2021) already showed for Chinese.\n\n- Regarding model transfer, the use of XLM-R to build zero-shot crosslingual\n  models is straightforward (replace the underlying monolingual pre-trained\n  language model like BERT with a multilingual model like XLM-R) and well\n  established, with multiple examples in several NLP tasks. The fact that it\n  works also for CSRL was to be expected, the same way it was shown to\n  succeed for regular SRL (Fei et al. 2020; 2021). In this sense, it is not\n  clear why the conclusions are different from those for cross-lingual SRL.\n\n- Regarding the strong results on the two downstream tasks, the baselines\n  predate pretrained language models and most of the improvement comes from\n  the use of BERT, not from the added value of CSRL. The authors should\n  clearly indicate this and tone down expressions such as \"impressive\" when\n  referring to CSRL results.\n\n- In other words, the main contributions are not clearly stated: is it the\n  CSRL model that is stronger than previous models (then the paper should\n  have focused on what makes the new CSRL model perform better than previous\n  models) or is it that the authors present a stronger method to fold-in\n  CSRL information in downstream tasks (then the paper should have focused\n  on what makes the new method perform better than previous methods).\n  \n- The evaluation dataset for CSRL in English does not seem to be available\n  or expected to be released.\n\n- The explanation of the model is not totally clear in places (see below for\n  details).\n\n- Most of the improvements are relatively small, but there is no information\n  about the standard deviation across the five runs. It could be that the\n  improvements shown over the average are not so strong after all, specially\n  seeing that the datasets in table 5 are on the small side. BTW, please\n  indicate the size of the train, dev and test splits of DuConv. In the same\n  vein, from the numbers in Table 1 it seems that cross-sentence arguments\n  are just a small percentage of all arguments, please add the statistics to\n  Table 5.\n\n\nOther comments:\n\n- The related work seems to miss very relevant work on SRL beyond sentences,\n  including implicit SRL (Laparra and Rigau 2013) and full document event\n  argument extraction (Lou et al. 2021).\n  \n- The explanation of MTrans is not very clear, please be more explicit. Note\n  that such an architectural change should be better motivated and checked\n  empirically.\n\n- It is not clear how the algorithm knows which tokens correspond to\n  predicates in the PA-Encoder.\n\n- The two paragraphs on Semantic Argument Identification are difficult to\n  follow.\n\n- It is not clear what is the CSRL training objective. Is it sequence label\n  over tokens?\n\n- Given that English data is used to train SPI and UOR it is not totally\n  zero-shot.\n\n\n\n\nReferences:\n\nLou, D., Liao, Z., Deng, S., Zhang, N., & Chen, H. (2021). MLBiNet: A\n  Cross-Sentence Collective Event Detection Network. arXiv preprint\n  arXiv:2105.09458.\n\nLaparra, E., & Rigau, G. (2013, August). Impar: A deterministic algorithm\n  for implicit semantic role labelling. In Proceedings of the 51st Annual\n  Meeting of the Association for Computational Linguistics (Volume 1: Long\n  Papers) (pp. 1180-1189).\n\n",
            "summary_of_the_review": "\n\nGiven the weaknesses above, it isn't up to the standards of ICLR in its current form, so I'm recommending rejection. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The work described in this paper consists in a system for cross-lingual conversational semantic role labeling. A multilingual language model is pre-finetuned on Chinese/English translation, parallel phrase classification, speaker role identification, utterance reordering, and non conversational SRL. Then, it is finetuned to perform Chinese conversational SRL, and tested in zeroshot fashion on English conversational SRL. Results show that the approach leads to non-trivial improvement on two English datasets compared to a multilingual LM not pre-finetuned. It also improve the state of the art for Chinese conversational SRL. An ablation study shows that the origin of the improvements is not completely clear. Three additional experiments are presented: low-resource CSRL where 30% of the training data (1000 dialogs) gives most of the improvement when using pre-finetuning, rewriting of pronominal references in questions and multi-turn dialog generation. Those two applications lead to improvements compared to seq2seq baselines.  ",
            "main_review": "Strengths:\n- the work addresses a combination of difficult problems: SRL, on conversations, in a zero-shot cross-lingual setting\n- main results are seconded by three additional experiments that might be papers in themselves\n- the paper is clearly written and easy to read\n\nWeaknesses:\n- the paper would stand without the application experiments, which themselves are not thoroughly described, and in which a full range of baseline results is not presented \n- the accumulation of non-standard components makes it difficult to untangle the origins of the effect\n- the contribution is rather incremental\n\nDetailed comments:\n\nThe proposal of using MTrans in eq. 1 tends to blur the argument. What is the performance of this approach with standard components?\n\n\"the resulted utterance representations\" => resulting\n\nIt is confusing to use both s^j_(i,k) and s^i_k in eq. 1 and 2. Eq 2 should be rewritten so that indices keep their role.\n\nWhat is meant by \"we feed source and target sentences twice in different orders instead of resetting the positions of target sentences\"?\n\nIs the pre-training architecture a Siamese network with parameters specific to the pretraining tasks?\n\n\"After pre-training, we employ the transposed speaker role decoder and utterance order decoder as the speaker role embedding and dialogue turn embedding in CSRL model.\" What is meant by transposing the decoders?\n\n\"objective to strength the correlations between \" => strengthen\n\nWith hierarchical pre-training, model parameters are frozen by training stage. Why not apply end-to-end training? What would happen?\n\nWill the test sets for Person-Chat and CMU-Dog be made available?\n\nWhich language was used from CoNLL 2012 data? Arabic, Chinese and English are mentionned.\n\nWhat is the difference of support for F1_cross versus F1_intra? According to SimpleXLMR DuConv F1_cross in Table 1, the impact on F1_all is very small. \n\nWhat explains that cross-lingual performance for F1_cross is much more affected than the other metrics compared to the monolingual case?\n\nCould the CANARD task be solved with a coreference resolution component alone?\n\n\"we can highlight the words pick up by the CSRL parser\" => picked up ",
            "summary_of_the_review": "The paper is clearly written, the proposed approach is sound, and experiments are well described. However it is difficult to untangle the source of the effect in order to apply the findings in other contexts. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes an effective and novel model architecture and several novel pre-training objects for zero-shot cross-lingual CSRL. In detail, the model consists of Cross-lingual Language Model, SC-Encoder and PA-Encoder, where SC-Encoder is hierarchical with word-level and utterance-level embedding. The improvement on DuConv, Persona-Chat, CMU-DoG and CANARD proves the effectiveness of proposed model for CSRL and elated downstream English conversational tasks.",
            "main_review": "Strengths:\n1.\tThis paper conducts extensive experiments to demonstrate obtain acceptable improvements and the effectiveness of the proposed model, where the area of experiments includes CSRL and related downstream English conversational tasks.\n2.\tThis paper proposes a series of novel or effective pre-training objects for the challenges in CSRL, as well as a hierarchical architecture which integrates word-level and utterance-level representations.\n\nWeaknesses:\n\n1). This paper lacks novelty. The three modules of the final model are not originally proposed by the authors. The cross-lingual language model is proposed and trained by (Conneau et al., 2020) and (Devlin et al., 2019), the idea of incorporating speaker and dialogue turn information is from (Xu et al. 2021), the hierarchical encoding practice is widely used in dialogue modeling area (e.g., Filling the Gap of Utterance-aware and Speaker-aware Representation for Multi-turn Dialogue, Liu et al. 2021). The first two pre-training objectives are from other papers.  \n2). More ablation studies need to be performed. The authors do not provide key ablation studies to analyze the source of performance gains, such as replacing SC-Encoder or PA-Encoder with naive methods (or remove them), or replacing the Concat & Norm trick with the original Add & Norm. Besides, do the improvements come from the additional number of parameters? Will simply adopting additional Transformer layers (without the speaker and turn information) improve the performance? \n3). More kinds of languages should be studied. The authors conduct experiments only on English corpus as zero-shot study, which is insufficient to demonstrate the effectiveness of the zero-shot transfer ability of the proposed model. \n\nMore detailed comments and questions,\n1.\tSome details in the model are vague, please see below questions:\na)\tHow does the SC-Encoder obtain dialogue turn embedding t and speaker role embedding r (to obtain timestep encoding s)? It seems that they cannot be generated from UOR and SPI pre-training objects, due to step for generating timestep encoding is before the step for UOR and SPI pre-training.\nb)\tWhat type of activation function do you use in Formula 2 and 4?\nc)\tDoes the symbol ⊕ mean vector connection in all the formulas? Related description is missing.\n2.\tThe proposed model is much more complex than the baseline with more parameters and additional data for pre-training of CLM module, so that:\na)\tThe overall performance improvement cannot match the increased parameter volume and additional pre-training consumption.\nb)\tA considerable proportion of improvement may come from the increased parameter volume and additional pre-training consumption, instead of the proposed methodology and architecture.\nc)\tThis paper does not report the parameter volumes of proposed model and its baseline.\n3.\tSome detail analysis of the ablation result (Table 1) is missing or not rigorous enough:\na)\tWhy do the performances of model w/o SPI and UOR improve in F1_all and F1_intra for Persona-Chat and CMU-DoG?\nb)\tThe performance of model w/o TLM and HPSI increases in F1_intra for CMU-DoG, compared to all obiects model (59.24 v.s. 58.82), so “removing TLM and HPSI objectives hurt performance consistently but slightly across all metrics on all datasets” is not rigorous enough.\n4.\tLack of some ablation studies:\na)\tThe ablation of model architecture is missing, e.g., w/o SC-/PA-Encoder.\nb)\tThe ablation of replacing u’ to u in SC-Encoder.\n5.\tMore compared models and statistics application in downstream English conversational tasks are need:\na)\tMore comparable models with their performance in other typical works are needed in Table 3 and 4, instead of only Seq2Seq.\nb)\tThe authors claim that questions with required information completion/ responses containing entities mentioned in histories benefit from CSRL, but do not provide the detailed (error/correct) case statistics or analysis.\n6.\tThere are some typos in grammar. For example, in Page 4, lin2 2, denote->with denoting.\n\n",
            "summary_of_the_review": "\nThe paper has its merit of good experimental results, but it lacks novelty in method itself and some important experimental analysis. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents a new approach for zero-shot cross-lingual conversational semantic role labeling (CSRL). The authors pointed out three challenges of cross-lingual CSRL, i.e., (cross-lingual) latent space alignment, conversation structure encoding, and semantic arguments identification. To tackle these challenges, they proposed five pre-training objectives, i.e., translation language modeling (TLM), hard parallel sentence identification (HPSI), speaker role identification (SPI), utterance order permutation (UOR), and semantic arguments identification (SAI). Experimental results show that the proposed model outperforms baselines in both monolingual and cross-lingual settings.\n",
            "main_review": "Pros:\n- This paper is well organized and easy to follow.\n- Strong empirical results on three CSRL datasets, i.e., DuConv, Persona-Chat, and CMU-DoG.\n\nCons:\n- Compared to baselines like CSRL-XLMR, the proposed method requires more resources (e.g., IWSLT’14 En↔Zh,  CoNLL-2012) for pre-training.\n- Some details in experiments are missing. Regarding the *Back-translation* baseline, the authors mentioned that Google Translate and a pre-trained CSRL model are used. Please specify which CSRL model is used here. For the cross-lingual test sets, please give more details about the data collection process rather than simply mentioning \"we manually collect two out-of-domain CSRL test sets based on English dialogue datasets Persona-Chat (Zhang et al., 2018) and CMU-DoG (Zhou et al., 2018)\".\n- Although this work aims to solve the zero-shot cross-lingual CSRL problem, I do not see any new zero-shot cross-lingual transfer learning ideas other than applying XLM-R and TLM which have been proposed by previous work. Overall, I feel this work is trying to combine massive existing techniques and lack of focus.\n\n ",
            "summary_of_the_review": "An empirical work with several limitations (e.g., missing experimental details, limited technical contribution, lack of focus). ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}