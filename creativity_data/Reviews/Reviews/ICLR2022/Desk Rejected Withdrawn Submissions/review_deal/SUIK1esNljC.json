{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces a method for automatically dropping the learning rate. It proposes to use the angle between subsequent steps as the signal to determine whether to drop the learning rate or not. The method is motivated by looking at the noisy quadratic model and then a network trained on CIFAR-10. Some theoretical analysis is done to prove that the learning rate converges for SGD. Lastly, experiments show that the proposed method is competitive compared to most hand-tuned learning rate schedules.",
            "main_review": "I am unsure how I feel about this paper.\n\nOn the one hand, I think the results are encouraging. It is interesting to see how the angles between displacement are such a clear signal as to the convergence behaviour of gradient descent, and being able to eliminate hand-tuned learning rate schedules can be of great use of practitioners. Many people have tried learning rate scheduling before, and this is a relatively simple but seemingly effective approach.\n\nHowever, I find the paper lacking in its delivery. On the one hand I would like to see a stronger motivation. The \"angular velocity\" signal kind of comes out of nowhere, even though I can think of a variety of interesting connections to highlight: When SGD is close to the optimum the noise dominates, which means that the sampled gradients are effectively just noise and hence orthogonal (i.e., at 90 degrees from each other). On the other hand, a common picture for GD is that if the step size is too high, the algorithm \"bounces\" back and forth on either side of the local minimum. This would imply an angle greater than 90 degree (I would love to get an intuition as to where 120 comes from though). Before convergence most gradients agree and the angle should be less than 90 degrees. Although this is very hand-wavy, I suspect that the authors could have made this connection more concrete.\n\nI think it would also have been clearer if the angular velocity would have been introduced for GD or SGD first, in which case s_i is actually equal to the (scaled) gradient, which makes it clearly what is actually being measured (i.e., agreement of gradients over time).\n\nI don't fully agree that the proposed algorithm 1 is a \"straightforward extension\" of the idea of dropping the learning rate when the angle reaches threshold, as it adds quite a few things: To get a non-noisy signal, the network is trained for an entire epoch before the displacement is calculated; the threshold for the angle (θ) is actually increased throughout training; and when the threshold is met iterate averaging is performed before dropping the learning rate. Not all these choices are clearly motivated, e.g., the paper states that \"iterate averaging is commonly done by practitioners\", but it doesn't explain why it is needed to make AutoDrop work beyond saying it is \"to stabilize the learning process\". Iterate averaging should particularly be explained in this context because it is usually used as an alternative to driving the step size to zero, so why are both needed in this case?\n\nThe theory is nice, but I feel that perhaps it detracts from the main paper. It is nice to have a proof, but I suspect that it could have been moved to the appendix given that the main proof method does not seem too surprising (upper and lower bound the learning rate schedule by some function that is O(1/t) from which the classical results for SGD follow) or relies on very strong assumptions (namely the exact behaviour of the angular velocity as presented in equation 11). This would have made space for more experiments, which I feel are lacking.\n\nFor the final experiments the authors only present test error (except for figure 6, top right). As this optimization method is first and foremost presented as a way to reduce the training loss and achieve convergence, I find that this muddies the waters. Is AutoDrop performing well on the training error, test error, or both? As a first experiment, I would prefer to see experimental validation of whether AutoDrop can successfully drive the learning rate of SGD to zero and achieve convergence for a toy problem. Then, as a second set of experiments, it would be interesting to compare the generalization behaviour of AutoDrop to other methods.\n\nI didn't thoroughly check the related work section, but I did notice that, e.g., Maclaurin (2015) is not cited for hypergradient methods, which makes me concerned about the quality of the literature review.\n\nA minor comment is that when citations are used in the text \\citet or \\textcite should be used so that it reads \"Jin et al. (2020) propose\" rather than \"(Jin et al., 2020) propose\".",
            "summary_of_the_review": "All in all, I find that this authors have some interesting ideas, but the paper requires significant work: Several parts of the algorithm require stronger motivation or explanation, I find the experimental validation insufficient, and the presentation is a bit messy.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a method to automatically drop the learning rate based on the “angular velocity” (direction change in two consecutive gradient updates). Using the noisy quadratic model, the authors show that the angular velocity appears to saturate when the training is about to converge, which could indicate that the learning rate should be dropped. Experiments are conducted on CIFAR and ImageNet datasets using variants of the ResNet architecture. The proposed method have matching or slightly better performance compared to the baselines.",
            "main_review": "Strengths:\n* Angular momentum as an indicator of when to drop the learning rate is an interesting idea.\n\n\nWeaknesses:\n\nGeneral:\n* The NQM experiment shown in Figure 3 is not very meaningful. Given the simplicity of NQM, many learning rate decay schemes can reduce the loss to near 0.\n* The experiments are only done on a limited selection of architectures on a few image classification datasets only. Given the lack of strong theoretical justification, I would want to see strong empirical results on a wider range of problems (e.g. language models)\n* The experiment results are not strong. Considering the confidence interval, the AutoDrop results don’t appear to be necessarily better than the baselines (as can be seen from Figure 6). Also, ImageNet experiments are only with 1 seed.\n\nClaims not well supported:\n* There are many hyperparameters in Algorithm 1. How come the authors claim in the introduction that they introduce no additional hyperparameters? \n* Page 4: it claims “tracking the saturation of angular velocity is more plausible than tracking the saturation of the loss function because the angular velocity curves have a much harder saturation pattern”. Two issues: 1) just that the angular velocity saturates does not justify learning rate decay; 2) unclear why a “harder saturation pattern” is helpful. Angular velocity saturates before the loss has converged -- should LR decay immediately?\n* Page 5: the paper claims that the fundamental difference between NQM and DL models is the noisy angular velocity at saturation. I don’t see any supporting evidence for why this happens, and why this is fundamental.\n* Section 4 claims that the recommended hyperparameters “guarantees good performance for a wide range of model architectures and datasets”. “Guarantee” is a strong word and I expect to see extensive empirical (if not theoretical) evidence. However, experiments are only done on CIFAR and ImageNet with a few ResNet variants, and I don’t think these account for “a wide range of model architectures and datasets”.\n* Section 5 begins with the claim that it will theoretically show decreasing the LR when angular velocity saturates guarantees sub-linear convergence rate for (momentum) SGD methods. However, **this is simply not true**. Section 5.2 merely builds a heuristic model v(t), and proceeds to use that in place of the angular velocity.\n\nClarity & writing issues:\n* Theorem 1 is rigorous. According to the paragraph below, C* is only approximately between ½ and 0. A theorem should quantify this rigorously, and definitely not claiming C* \\in [-½, 0].\n\n\n\nFeedback for improvement:\n* The first two paragraphs in the introduction are too long as a general motivation for deep learning. Also, when you mention statistics (e.g. how much is needed for training a large model), you should always include details such as the type of problem and architecture. Avoid identifying individual companies unless there is a good reason to do so.\n* Definition 1 is a bit out of place in the introduction. Consider moving it to the background or method section.\n* Should cite (Zhang et al., 2019) “Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model.” for noisy quadratic model related contents. (currently, the Zhang et al. 2019b citation is for a different paper).\n* Figure 1&3, don’t abbreviate “iteration” in the labels.\n* Section 5.1 It’s odd to call a convergence analysis “state-of-the-art”. \n",
            "summary_of_the_review": "I recommend rejection. \n\nThe reasons are that contrary to what is claimed in the abstract & introduction,\n* The authors have not shown convergence guarantee for the proposed algorithm (details above in the main review).\n* There is little evidence that the angular velocity is a good indicator for learning rate decay.\n* Empirical results are weak and limited (only image classification, and a few ResNet variants).\n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper explores adaptive learning rate scheduler based on the angular velocity between difference of parameter iterates in the training process. Based on the empirical observations (in convex as well as non-convex setting) that the angular velocity correlates to the convergence behavior, authors propose an adaptive scheduler, AutoDrop that can be used on top of any existing optimizer. Theoretical treatment of the simplified AutoDrop shows the convergence of scheme. While empirical evaluations show the usefulness of this scheme.",
            "main_review": "\nI really liked the idea that a quantity such as angular velocity can be helpful in the adjusting the learning rate schedule for DNN training. However, I feel the execution of the idea is not on par. Below, I list out some of the concerns.\n\n**Strengths**:\n1) Idea to use angular velocity for learning rate schedule is novel.\n2) Theoretical treament of albeit simplified version, shows convergence of the proposed scheme.\n\n**Weakness**:\n1) Baselines such as Resnet and DenseNet have been only using a multi-step learning rate schedule (i.e. reduce their learning rate at pre-defined epochs), while the correct baseline for comparison would have been to equip Resnet and DenseNet with cosine scheduler or some other continual learning rate schedule.\n\n2) Its not clear why AutoDrop requires an exponential moving average (EMA). Since EMA has been studied in the literature and has been shown to improve the performance of the DNNs irrespective of the learning rate schedule (see for ex. FBNetV3, EfficientNet, MobileNetV3, etc.). That being said, if in AutoDrop you still use EMA, the correct baseline to compare the proposed method would be to use the EMA for Resnet as well as DenseNet. Also, EMA in AutoDrop can be thought of as the stochastic weight averaging as proposed in https://arxiv.org/abs/1803.05407 , where they trigger the averaging towards the convergence / saturation regimes.\n\n3) Looking at the empirical results, if the authors equip baselines with cosine scheduler and use EMA, it would hardly surprise anyone that their bleak gains would become worse and the baseline would be somewhat near the proposed method.\n\n4) The paper starts off posing the question that hyper-parameter tuning is costly and we should explore alternative strategies for automatic tuning. While the paper eliminates scheduling learning rate in the DNN training, it introduces other hyper-parameters and at least one of them has been shown to change the performance drastically (\\rho hyper-parameter). \n\n5) One of the claims in the main text is that AutoDrop accelerates training. However, I do not see any training time comparison between AutoDrop and any baselines. \n\n\n**Questions for authors**:\n\n1) Currently the angular velocity definition looks at the angle between difference of two parameter iterates, i.e. < x_{t+1} - x_t,  x_t - x_{t-1} >. Did the authors experiment with the angle between gradients at these two locations? I would expect the gradients to behave as per the transition at 90 degree, its surprising that the difference between parameters also has similar effect ( maybe since its a crude approximation to the gradient ).\n\n2) In Figure.1, authors look at the angular velocity per iteration and in Figure.2, authors look at the angular velocity per epoch. Is there any reason, why one cannot look at the angular velocity per iteration in the Figure~2? Since your conclusion P2(iii) does not hold true in the non-convex DL setting, is it possible that one reason could be that you are looking at epoch level instead of the iteration level.\n\n3) How much training overhead does AutoDrop add in comparison to existing learning rate schedulers like fixed schedule or cosine scheduler etc.?\n\n\n4) Algorithm~1 seem to be somewhat confusing. Specifically, AutoDrop interacts with the network parameters with x and y variables. It also uses an exponential moving average of the parameters through z. Do you update the network parameters to the EMA parameters while dropping the learning rate?\n\n5) In Table~1, the variance for the AutoDrop method is consistently worse than the baselines. Is there any explanation for this behaviour?\n\n**Nit-Picks**:\n- In addition to being sensitive, Hyper-gradient based methods are also computationally expensive.\n- The phase transition between 90 degree and 120 degree in the experiments could be illustrated a bit more clearly. Currently, one has to go through the text and map the angle association with the learning rates.\n",
            "summary_of_the_review": "\nBased on the strengths and weakenss, I feel that the paper needs improvements. The empirical results do not convey the superiority of the proposed scheme, especially given that baselines such as cosine scheduler or EMA on Resnet/Densnet are missing. I really liked the idea of using a physical quantity such as angular velocity to help guide the learning rate and would really encourage the authors to address some of the issues being raised in the weaknesses. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a method to automatically decay the learning rate using the angular velocity of the network weights.\n\nIn section 3, they discuss some properties of the quadratic model which motivates their intuition. In section 4,5 they propose autodrop and study the convergence of learning rates with decays. In section 6, they do experiments and show that autodrop performs similarly to the standard learning rate schedules.",
            "main_review": "I think having an automatic way to decay the learning rate would be rather useful and autodrop might be a good step in that direction. \n\nHowever, I find the theoretical discussion is too detached  from the reason why we want autodrop (ie section 6). I will be willing to raise the score if the authors moved most of the theory (sections 3/5) to the appendix (maybe making a brief comment about the fact it converges) and had instead a more nuanced discussion about deep networks: for example can deep networks achieve the same error with smaller learning rates (and possibly more time) in principle, why are learning rate schedules important to have better generalization , ... \n\n\n- I find the discussion of section 3 rather misleading: it is well understood that a noisy quadratic model will have oscillations around the minimum but the landscape a quadratic model provides is rather different from what one expects in deep learning. It is interesting that autodrop improves convergence in this case, but the ultimate need for learning rate schedules seems to come from the benefit of large learning rates (see for example https://arxiv.org/abs/1907.04595).  Also can one plot fig 3 together with fig.1 runs and possibly y log scale?\n\n- NQM seems also a bad example for section 6 because cross-entropy loss has rather different dynamics. Could the authors comment on this? Maybe cross-entropy with weight decay is closer to NQM.\n\n- The discussion of section 5 of convergence with autodrop/dropping learning rates seems also rather detached from the practicality of these schedules (and this section only applies to convex losses). If one wants to converge to the closest local minumum one could do that via second order optimization methods. However, the reason why these more complicated learning rates is useful is not because of its converging properties but because they somehow manage to explore the landscape in a way that provides better generalization: the convergence discussion applies to the NQM too but there the generalization error does not depend on the initial learning rate as long as it is decayed to zero.\n\n- The authors should mention that ${\\cal G}$ is the gradient of the loss $f$, or define $f$ somewhere, otherwise it is confusing. \n\n- How are the recommended hyperparameters chosen? Could the authors minimize them somehow by stablishing more automatic thresholds? I might be worried that the recommended hyperparameters don't work as well in different settings. \n\n- The paper https://arxiv.org/abs/2103.12682 has a very similar learning rate with less hyperparameters. Could the authors comment on their similarities and differences?\nI think for scale invariant layers (which are most layers in networks with bn), $s_{t+1}.s_t \\propto ||s_t||^2$ for SGD (because the weights are perpendicular to the gradient) . Could the authors add one experiment without bn (like vgg) to make sure that it is not \"weight bouncing\" which is providing the effectiveness in matching the baselines?\n\n- lt would be nice if the authors had a better Imagenet example since 30% error is quite high.\n\n- It would be nice to explore autodrop for tasks different from image classification.",
            "summary_of_the_review": "While having an automatic way to the learning rate would be quite useful in practice, the most of the paper is focused around some theory and intuition which does not really apply to the experiments. I think the paper would benefit less theoretical results on convex settings and more discussion (either theoretical or empirical) about real networks.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}