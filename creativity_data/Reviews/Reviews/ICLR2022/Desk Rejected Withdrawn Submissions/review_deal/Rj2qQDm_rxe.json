{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper introduces KIMERA (Knowledge Injection via Mask Enforced Retraining of Attention), a framework to detect the importance of different attention heads in transformer language models and then retrain the attention heads according to their importance in a multi-task setting to inject/instil the domain-related information from different domain-specific knowledge graphs. The retrained model is then fine-tuned again for the downstream task. The authors have chosen clinical domain for multiple reasons: \n1.) A lot of specific domain knowledge can be injected via clinical/medical knowledge graphs such as UMLS \n2.) Sufficient downstream tasks.\n3.) Existing fine-tuned models such as BioBERT provides sufficient performance improvement over its BERT-base counterpart.\nKIMERA and its variants show performance improvemet across 8 different downstream tasks over BERT and BioBERT. The performance is more significant across passage retrieval tasks than clinical outcome predictive tasks. ",
            "main_review": "Strengths of the paper: \n1. It's a well-written paper and is easy to follow.\n2. The authors have proposed a possible solution for an important problem where the injection of knowledge graph information can boost the performance of the existing language models for a range of domain-specific tasks.\n3. Authors have created extensive variants of the proposed model/framework to compare across all 8 tasks. \n4. KIMERA utilizes a neat idea (adapted from Michel et al. 2019) of utilising the attention head importance to create hard and soft masks for the attention heads to finetune during retraining. \n\nWeakness of the paper (I've also included some questions below for the authors):\n1. BioBERT is a good baseline but would not be considered a really strong baseline when compared against models such as KIMERA which are retrained on a couple of downstream tasks from knowledge graphs. This also raises the question: Is the performance improvement because KIMERA was fine-tuned a couple of downstream tasks helping in aligning the model for further downstream tasks? This could be assessed by finetuning the model on some other classification clinical tasks which are not extracted via Knowledge graphs.\n2. A stronger baseline would be a model such as ERNIE: Enhanced Language Representation with Informative Entities (https://arxiv.org/abs/1905.07129) as knowledge graph information is explicitly introduced in the language models. Entity-Enriched Neural Models for Clinical Question Answering (https://aclanthology.org/2020.bionlp-1.12.pdf) have shown that explicitly introducing the knowledge graph information also helps the model in improving its overall performance. Authors have mentioned that ERNIE trains using UMLS but has quite a different training objective which is true but comparison against a model that uses knowledge graph to inject domain-specific information would provide a stronger and more comparable baseline.\n3. Though the results show that KIMERA performs better than the existing baselines (BERT and BioBERT), it's important to perform some error analysis and provide it in the discussion of the paper to show how KIMERA was able to better distinguish passages as compared to previous models. Some attention visualisations can help in identifying how/if the language model was able to change its focus to certain medical entities or relations which it is trying to learn from the previous knowledge graph-based downstream tasks. \n4. Though these are important tasks, NER tasks carry a lot of value in the clinical domain. I would recommend including some NER based tasks which are openly available such as i2b2 tasks built over the MIMIC dataset.\n\nA suggestion: \n1. I would change the Table to compare the performance between BERT, BioBERT vs KIMERA models. That would help in clearly showing the improvement of KIMERA over the baseline models.\n\n===================\nBERT\nBERT (pruned)\nBioBERT\n===================\nKIMERA scratch\n- \n-\n-\n-\n-\nKIMERA BioBERT\n\n===================\n\n",
            "summary_of_the_review": "After taking into account the novelty of the model, current experiments and analysis/discussion provided by the authors, I believe the paper can benefit a lot by just including a knowledge graph based LM while including some error analysis of the proposed model. Even though the core of the proposed model is based on existing work (Michel et al 2019), I believe KIMERA is a novel model that can be used to inject domain-specific information into existing language models. I would suggest adding a stronger baseline against KIMERA, include some error analysis and if possible, 1-2 NER tasks from the clinical domain (either i2b2 tasks or MADE 1.0 task). Though I would like to hear the authors' remarks and other reviewers remarks on if the current draft is sufficient and other work could be considered for future work.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The goal of this work is to develop an efficient approach for injecting knowledge into transformers by retraining redundant attention heads to capture information in knowledge bases. The method, KIMERA, is evaluated in the biomedical and clinical domains on information retrieval and clinical outcome prediction tasks. KIMERA outperforms BERT-base and KIMERA biobert outperforms biobert across all but 1 task. However, Biobert achieves much better prformance than KIMERA initialized with BERT-base on the biomedical QA tasks (all but MIMIC-III), and BioBERT performs a bit better on the outcome prediction tasks.  ",
            "main_review": "Strengths:\n-\tThe idea of injecting knowledge graph related knowledge into the redundant heads of BERT is a very interesting idea. The approach fuses two sources of knowledge (KG + text) without increasing model size or significantly changing the training setup. \n-\tThe paper is well written, and I appreciate how the work clearly articulates the motivation for the KIMERA variants and the baselines that are included in the paper.\n-\tThe authors state that they will release their models on HuggingFace’s model hub.\n\nWeaknesses:\n-\tThroughout the paper, the biomedical and clinical domains are often conflated. Three of the four information retrieval datasets are biomedical, but the MIMIC-derived dataset as well as the 4 clinical outcome prediction tasks are clinical tasks (i.e. derived from electronic health record notes). The paper itself notes that clinical text has idiosyncratic language (e.g. medical abbreviations, semi-structured text, etc.), yet on page 1, it states that BioBERT is trained on in-domain data. This is particularly important because BioBERT is well suited for the biomedical tasks, but not as well suited for the clinical tasks. A better in-domain model would be a clinicalBERT model. It seems a bit strange for the paper to emphasize that KIMERA can improve on an in-domain model when the BioBert model isn’t really “in-domain” for most of the tasks. Relatedly, the only task in which KIMERA outperforms BioBERT is on MIMIC CAPR task, which is likely more so due to the ill fit of BioBERT to clinical text compared to biomedical text. \n-\tWhile the KIMERA model does outperform BERT-base and the BioBERT + KIMERA model does provide benefit over BioBERT alone, KIMERA doesn’t outperform BioBERT. This begs the question of when a KIMERA model provides benefit. It appears that in-domain pretraining is better than general BERT+KIMERA, but it’s unclear whether in-domain pretraining + KIMERA is better than in-domain pretraining alone. While the paper does compare BioBERT to BioBERT + KIMERA, this doesn’t seem to be a fair comparison on the clinically oriented tasks. A more natural comparison might be to compare a ClinicalBERT model and ClinicalBERT + KIMERA. ClinicalBERT model performances are already reported in the van Aken et al. paper on the Clinical Outcome Prediction tasks, but are not included in this paper for some reason. Some discussion of this seems warranted, especially because these models and the new model in van Aken et al. outperform the models presented in this work. If the authors are trying to state that their  domain adaptation approach is more efficient, they’d benefit from more explicitly comparing the time to further pretrain BERT on in-domain data (beyond just train time of BioBERT) vs time to train KIMERA.\n-\tThere is no comparison to other approaches for incorporating domain knowledge into transformer models (e.g. UMLSBert or Kim et al. 2020) so it’s difficult to make any conclusions about the relative pros/cons of different strategies. \n\nOther Comments:\n-\tPlease specify which results are taken from other papers and which are rerun yourself. I noticed that the BERT & BioBERT results on the OP tasks were identical to those in the van Aken paper. \n-\tI’d be interested to see what the performance would be when you randomly select which attention heads to retrain compared to the soft/hard masking approaches. This would help evaluate how good these approaches are at identifying useful attention heads to retrain.\n-\tOverall, I like Figure 1, but it’d be helpful to explain what “r Index: 4” means in the table.\n-\tHow are cases handled where there are multiple correct answers (i.e. multiple entities that would suffice) for the entity prediction pretraining task? Do you mask out all word pieces corresponding to the entity, and if so, does the number of masked word pieces inherently give a clue as to the entity masked?\n-\tAdditional details about the filtering of UMLS should be included in the appendix. How were “relevant relation types” defined? Which sources in the Metathesaurus were used? Etc.\n-\t“the process halts once a threshold T of the overall performance is reached.” Can you clarify how the range of values for T were determined? Further clarification of the T parameter would be useful.\n-\tThe paper states that negative examples for triplet classification were generated by “replacing one of the three components with the same component from a different randomly selected triplet.” Instead of just replacing e.g. an “object” with an “object” from another triplet, did you consider selecting an “object” associated with the same relation as your positive example? This process will generate harder negative examples and could improve the pretraining performance.\n-\tIt’s hard to understand the format of the CAPR tasks from the description alone. An example query and passage(s) would help clarify this. In particular, further information is needed to specify how MIMIC-III was used as a CAPR task. Why were the discharge summaries truncated to 376 when the max length of BERT is 512?\n-\tClarify why the masks are generated from separate held out parts of the test sets of each dataset (pg 6) instead of the train set. \n-\tAre there any measures of variance in model performance in table 1?\n-\tThe appendix A1 includes results for the CAPR tasks. What does this analysis look like for the clinical outcome prediction tasks where the benefits of KIMERA were less prominent?\n-\tWhat is the standard deviation of model performance in the A4 figure? \n",
            "summary_of_the_review": "Overall, I think the idea of injecting domain knowledge into redundant attention heads is interesting. However, I think this paper in its current work is weakened by using a biomedical model for both biomedical and clinical downstream tasks, which makes it difficult to ascertain the benefit of combining KIMERA with in-domain pretraining. The lack of comparison to other methods for inserting domain knowledge into transformers also makes it difficult to determine their relative strengths and weaknesses.   ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed the Knowledge Injection via Mask Enforced Retraining of Attention (KIMERA) for detecting, retraining, and instilling attention heads with complementary structured domain knowledge. The proposed multi-task training method has the ability to identify and target individual attention heads that are least useful for a given downstream task and optimize their representation with information from structured data. The authors applied their proposed method in the medical domain, and the method achieves significant performance improvement on the seven datasets. Such a result proves the proposed method can introduce the domain knowledge effectively, and it is novel research.",
            "main_review": "1.\tFirst and the most important, the author mentioned that “We find in our experiments that it is usually most beneficial to keep all αn at 1 and leave the exploration of soft weightings to further research.”, but how to prove that the overall performance of the model is not dominated by one of the tasks in the existing research? It is better for the author to verify the impact of different αn in this research.\n2.\tCompared with the BERT-base and BioBERT models, how can the authors prove the advantages of their proposed method in terms of training time or other computing resources?\n3.\t“For this benchmark an attention mask is generated for each of the tasks individually.”->”For this benchmark, an attention mask is generated for each of the tasks individually.” ;”we found also in the Passage Retrieval tasks”->”we also found in the Passage Retrieval tasks” The author need to check the details of their English writing.\n",
            "summary_of_the_review": "This research has a good structure and complete experiments, but the author should verify the different αn on the model performance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a three stage approach to knowledge injection into transformer based language models with the aim of utilizing knowledge bases when a given domain has limited training data with which to pre-train a language model.  \nFirst, given a pre-trained LM (BERT/ BioBERT) they use the model compression technique in Michel et al. (2019) to learn an importance attention mask ( Layers X Attention heads ) and prune a percentage so that the performance of the final net stays close to the fully fine tuned net.\nSecond, they take the initial LM and the learned attention mask from step one and follow a multi-task training setup based on common Knowledge Graph Completion tasks ( entity prediction, relation prediction and triplet classification ) obtained from 3 knowledge graphs from the clinical domain ( UMLS, HSDN and Rotmensch et al. (2017) and update the weights during the backwards pass in inverse relation to the learned attention mask ( ie, unimportant heads weights are updated ).  Here they experiment with three strategies for incorporating the learned mask (1) using hard weights (0 or 1), (2) using soft weights  and (3) allowing for the forward/backward passes to be affected ( ie, only allowing nonmasked heads to be utilized during the forward pass)\nIn the third and final step, they take the LM that has been trained on KG completion tasks and weighted by the learned attention mask and fine tune it on the downstream task used in step 1 to learn the mask.\n\nThe authors run two experiments in the clinical domain, zero-shot Clinical Answer Passage Retrieval (CAPR) and Clinical Outcome Prediction ( COP ).  \n\nThey show greatly improved results over the BERT-base general domain model for the CAPR datasets ( MedQuad, HealthQA, MIMIC III and WikiSectionQA ) and smaller, but still significant improvement over the BioBERT model that has been pre-trained on the biomed/clinical domain.  \nFor the COP experiment, they obtain smaller improvement gains over the datasets ( MP, DIA and PRO ) and perform slightly worse on the Length of Stay LOS dataset.",
            "main_review": "Strengths\n- novel and interesting idea for injecting knowledge into transformer models leveraging model compression and a suite of Knowledge Graph completion tasks generated from a set of in-domain knowledge graphs.\n- in general they show substantial improved results on BERT and lesser, but still significant improvements on BioBERT ( which is a much more honest baseline to compare against for the clinical domain task ) \n- the paper is well written and clearly motivated\n\nWeaknesses\n\nOverall the motivation behind the approach was clear, but I feel some improvements to the experiments would strengthen the paper some.\n- The authors should have provided results using PubMedBERT ( https://arxiv.org/pdf/2007.15779.pdf ) for experiments as it has been shown to beat BioBERT in 10 out of 12 benchmark Clinical NLP tasks.  I imagine this will shrink the margin of improvement somewhat, but showing KIMERA would still improve accuracy would be impressive.\n- I like the idea of trying this technique on limited training data domains though the clinical domain has quite a bit of publicly available including all the data BioBERT is trained on and the knowledge graphs utilized in the paper.  Its still useful regardless, but it would be interesting to try and leverage this on a low resource language task that had an existing KG or possibly another additional domain.  In lieu of extra experiments ( which is probably future work ), the authors could allude to some possible domains/cirmunstances where this approach would be useful. Your improvements over BERT make it seems like this approach could be tried out in other areas with knowledge graphs and no already strong in-domain pre-trained LMs ( like BioBERT here ),  but showing this too be true would strengthen the paper.\n- How does this technique compare accuracy wise against other knowledge injection schemes such as those that you all mention in the Structured Knowledge and Knowledge Injection subsections of Section 5, particularly Ernie, K-adapters and Hao 2020?    \n- How does the head importance selection criteria affect things?  How would using DIFFMASK ( https://arxiv.org/abs/2004.14992 ) or something from the feature attribution literature affect performance?   What if you had just randomly selected 10% of heads/layers to drop and then do stage 2 and 3?  In the appendix, you show higher importance overall, but not necessarily that the unimportant heads are the source of the improvement ( ie, you don’t use post hoc feature attribution to show these heads led to specific improvements ). \n- What would happen if you had only allowed Fine Tuning to affect the unimportant heads without doing the KG completion step at all in Step 2?  So identify the mask in part 1 and then just finetune the BERT weights for the unimportant heads in the mask on the downstream task?\n- How do methods such as those from “Dense Passage Retrieval” ( EMNP 2020 ) and “A Deep Metric Learning Method for Biomedical Passage Retrieval“ ( Coling 2020 ) perform on the CAPR task? \n\n- Additionally: The anonymized code repo initially worked, but about a week before the review deadline ( and now )  the repository is showing as expired and the code is not available.",
            "summary_of_the_review": "Overall I think the novelty of the method opens a new avenue of possible research to follow and the improvements over BioBERT shows that its applicable even when strong pre-trained LMs exist.  I do think the experiments could have been strengthened by using PubMedBERT, including a set of experiments in another domain, some ablations and comparisons with existing work as outlined in the prior section.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}