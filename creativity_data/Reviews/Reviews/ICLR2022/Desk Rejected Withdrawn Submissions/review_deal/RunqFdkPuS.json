{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes to jointly learn features from different modalities including image, point cloud, and mesh in a self-supervised model. To this end, it learns two types of features: modality-invariant features and modality-specific features and enforces a soft orthogonality constraint between the modality-invariant features and modality-specific features. This high-level idea is novel and interesting to the best of my knowledge. The method is validated on ModelNet benchmarks for various retrieval problems.",
            "main_review": "My main concern with the paper is the lack of novelty in the contrastive learning pipeline. the contrastive losses are not really novel although I agree the idea of learning modality invariant and specific features on these 3 modalities is new and interesting. However, given that Jing et al. already does cross-modal learning between PC and images, this paper adds another modality of a mesh but it is not fully explored. e.g. Mesh structure brings 3d connectivity information that is absent from PC and images. So one way to incorporate and justify adding another modality would be by enforcing features to learn this extra connectivity information in some way. e.g. by exploiting the Laplacian operator on meshes. \n\nTo summarize, compared to the baseline Jing et al. 2020, the proposed pipeline aims to exploit one more modality of mesh. However, I do not see a direct connection either in loss function or in results. Modelnet is already a saturated benchmark where methods based on just pc already achieve comparable performance.",
            "summary_of_the_review": "To summarise, I like the high-level idea of learning modality invariant and modality-specific features over images, point cloud, and mesh. However, the methodology lacks novelty from a contrastive learning perspective (e.g. Jing et al. 2020) and empirical advancements do not seem significant as they are limited to only Modelnet with a comparable performance from baselines of 2020. If the authors can justify how/why mesh connectivity is beneficial in the current framework e.g. by providing additional results, I am happy to revisit my score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper attempts to address the self-supervised learning task for 3D feature learning by proposing to jointly learn features from multiple modalities, such as image, point cloud, and mesh. The proposed method aims to learn two types of features, including modality-invariant features and modality-specific features. The network is trained with the contrastive losses and orthogonality constraints. The learned modality-invariant features can be applied to cross-model retrieval, while the learned modality-specific features can be applied to 3d recognition.\n\nThe authors evaluate the performance on ModelNet40 and ModelNet10 dataset for classification and model retrieval. ",
            "main_review": "STRENGTHS:\n\n- Although there are some works studying the self-supervised problem for 3D feature representations, the proposed method considering the multiple modalities is interesting.\n- The method is introduced clearly and easy to understand, in general.\n- The provided qualitative visualization in Figure 3 is helpful for readers to understand the method.\n- In most cases, the performance improvements can demonstrate the effectiveness.\n\nWEAKNESS:\n\n- Why the cross-view invariant constraint, i.e., Eq. 6, can enforce the network to learn modality-specific features? I am not an expert in contrastive learning. It would be better if the authors can give an explanation.\n- For other self-supervised learning methods in Table 1, the SVM is used for classification? I find that several methods, like Orientation, ContextPred, and XMV, perform better than the proposed method only with Point (89.7\\%). The authors should give an analysis.\n- It would be better if the authors can conduct experiments on other datasets. For example, the authors can fine-tune the pre-trained network on ShapeNet for model retrieval or directly train the model on ShapeNet. Only conducting experiments on ModelNet40 / ModelNet10 is not enough.",
            "summary_of_the_review": "This paper is interesting. However, some claims are needed to be clarified, and more experiments are expected.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a multi-modal self-supervised learning (SSL) model to jointly learn both modality-invariant and modality-specific features with contrastive learning. The multi-modal features learned by the proposed paradigm is validated to be useful on different downstream tasks such as recognition and retrieval. Experiments show that the proposed method outperforms a good amount of SSL methods and is on-par with supervised learning methods.",
            "main_review": "Strengths:\n\n1. The paper is well-motivated and the objective is clear.\n2. Overall, the writting is easy to follow. (See abuse of notations below)  \n3. The proposed model is compared with a good amount of SOTA and achieves satisfying results.  \n4. Although further elaboration and analysis is definitely needed. This proposed multi-modal SSL model has the potential to be used on a lot more tasks and scenarios.\n\nMajor Weaknesses:\n\n1. Abuse of notations. \n    1. In Sec 3.1, modality-invariant features should either be expressed in I_i^1, I_i^2, ..., I_i^M, or I_i. Using superscript m is very confusing. Similarly, for modality-specific features, the last superscipt should be M. \n    2. In Sec 3.2, 6th line, last subscript of G should be M rather than m.\n    3. A good amount of other erronous mix between M and m in various other expressions.\n    4. A good amount of inconsistency b/w using m and M_i to denote each distinct modality \n2. Novelty. The whole ideas of modality-invarient and modality-specific feature learning is basically applying contrastive loss [1] in the inter- and intra-modality manner. The only novel element is the soft orthogonal feature constraint, yet this part is not given enough discussion and analysis. The overall novelty seems weak.\n3. I do agree with the design of constraints L_S and L_I. But I am not convinced by the design of L_O. I think the first term of L_O (inter-modal orthogonality) is a reasonable constraint, but the constraints of orthogonality between modal-specific features is not so obvious. \n    \n    In my opinion, the modal-specific features need not be orthogonal to each other — the structure of each modality's feature highly depends on the modality itself and the orthigonality seems to be too strict. This may hinder the learning of these features, because I feel that orthogonality with the modality-invariant conponent is enough to guarentee a good learning. Ablation experiments and further analysis is needed to to validate the usefulness of the second term.\n    \n4. Error in main figure. L_I denotes modality-invarient loss, which indicates that it should be connected with I features. Yet in Figure 2 it is connected with S features.\n5. The generalization concerns. Ideally, the learning paradigm should be independent with the choice of backbone feature encoder. I wonder what the results will be if the backbone is changed. For example, what if another commonly used point encoder PointNet [3] or PointNet++ [4] is used as backbones. \n6. Missing details on reproducibility. What is the actual number of hyperparameters alpha, beta, gamma, in the loss function? How and why is the number chosen in the experiments? How to choose the weight of different terms when the method is used in a new scenario where the modalities available are different from the ones used in the paper? Some ablation experiments or at least some discussion is needed.\n\nMinor Weaknesses:\n\n1. I do not understand why random rotation along one axis is an augmentation to point cloud. DGCNN [2] is used in the manuscript to learn the point feature, and the graph stucture should make the model rotation invariant. Please elaborate more.\n2. Discription of constraints in the introduction section is incomplete. Maximizing the similarity b/w same objects is not enough, it should also minimize the similarity b/w different objects in the same time. \n3. Inaccurate citations. E.g., SimCLR [1] is an ICML paper but is cited as arxiv paper. Please make sure your citations are all up-to-date.\n4. Visualization in Figure 3 is really good. But it's better if authors can include an additional row with the visualization of hidden feaure F. This helps to understand how the features are projected into two different spaces.\n\n\n\n\n[1] Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" International conference on machine learning. PMLR, 2020.\n\n[2] Wang, Yue, et al. \"Dynamic graph cnn for learning on point clouds.\" Acm Transactions On Graphics (tog) 38.5 (2019): 1-12.\n\n[3] Qi, Charles R., et al. \"Pointnet: Deep learning on point sets for 3d classification and segmentation.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n\n[4] Qi, Charles Ruizhongtai, et al. \"PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space.\" Advances in Neural Information Processing Systems 30 (2017).",
            "summary_of_the_review": "The pros and cons of the paper is discussed in the Main Review. I think this work has the potential to be accepted. However, the current version of the manuscipt is very incomplete. \n\nThus, I give it a 3 at this point.\n\nI suggest the authors revise, and add missing part of the paper. I will increase my score if my concerns are resolved later.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}