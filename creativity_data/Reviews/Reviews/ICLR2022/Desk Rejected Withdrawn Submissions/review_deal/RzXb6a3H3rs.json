{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents the idea of prompting in the field of continual learning. Specifically, the author introduces an L2P framework that learns to dynamically prompt the model to learn tasks sequentially under different task transitions. To evaluate the proposed L2P, the paper conducts experiments under various settings.\n\nOverall, the paper is well organized and clearly written. The proposed Learning to Prompt for Continual Learning (L2P) is quite straightforward and technically sound. ",
            "main_review": "Strengths:\n1. The paper is the first to address continual learning with prompting techniques.\n\n2. The proposed Learning to Prompt for Continual Learning (L2P) is technically sound. \n\nWeaknesses :\n1. The baselines considered in this paper are too weak. Many latest methods are missing. Such as LoRA, K-Adapter, and other parameter-expansion methods.\n\n2. The proposed Prompt Pool is similar to the idea of many memory-based continual learning methods. The paper should give more discussions and comparisons with these methods.\n\n3.  This paper only experimented with visual-based data, and the results of language-based data could make the evaluation more convincing.\n\n",
            "summary_of_the_review": "The paper is the first to address continual learning with prompting techniques, which makes sense to me.  The proposed L2P is also technically sound. However, the evaluation in this paper is a great concern to me. More strong baselines and more datasets can be included to make the evaluation more convincing. Besides, the paper should give more discussions and comparisons with the memory-based continual learning methods.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Learning to Prompt (L2P) prepends a subset of learnable parameters (called prompts) from a larger set (called pool) to the input embeddings to learn tasks sequentially given a pretrained backbone model. The model was applied to various continual learning settings demonstrating better results than baselines.",
            "main_review": "The related work section ignores numerous recent work in continual learning, especially closely related work such as:\n- Joan Serrà, Dídac Surís, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. arXiv preprint arXiv:1801.01423, 2018.\n- Loo, N., Swaroop, S., & Turner, R. E. (2020). Generalized variational continual learning. arXiv preprint arXiv:2011.12328.\n\nThe baselines in Tables 1-3 also ignore more recent stronger work.\n\nThe use of a fixed backbone is somewhat contentious because the main notion in continual learning is to continuously adapt the model. The 5-datasets experiments demonstrated that the model can adapt to different datasets. What about the heterogeneous case?\n",
            "summary_of_the_review": "Nice idea, missing related work, weak baselines, more experiments required to validate the fixed backbone in the model.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper focuses on class-incremental, task-agnostic, and domain incremental settings in continual learning. It applies the idea of prompting (previously introduces in NLP) to continual learning over image transformer architecture. The main novelty (if I got this correctly) is the extension of prompt tuning to prompt pull and application to continual learning.\n",
            "main_review": "The paper combines various mechanisms from previous works, but the details are not clear. The ideas sound similar to fast and slow learning weights (in few-show learning), including the key-value pair based query mechanism for dynamically selecting suitable prompts for different inputs. The paper says that keys are learnable, but I didn’t find how they are learned – eq. 3 performs selection using the cosine similarity between the query and keys, but it doesn’t say how the keys are obtained.\nIn experiments, the proposed method shows high accuracy compared to other tested methods. However, the paper claims that it compares to SOTA, while there are more recent works with higher performance. \n",
            "summary_of_the_review": "Due to incremental contribution, lack of details about the base methods and somewhat lacking benchmark, I cannot recommend acceptance.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}