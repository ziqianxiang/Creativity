{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The manuscript proposes a method for imitative modeling, where task-irrelevant subsequences in visual demonstrations do not negatively impact learning.",
            "main_review": "Section 2: Terminology. The manuscript states: \"... the vast majority of work are done in the state space, which is hard, if not impossible, to be transferred to high-dimensional observations.\" First, \"hard\" and \"impossible\" are subjective terms, which should be replaced with more precise characterizations of the challenge in transferring methods to higher-dimensional problems. Next, the manuscript conflates 'state' in the context of control (fully-defines the system; e.g., as in state-space equations, usually of low dimension) with state in the context of an MDP (need not fully define the system; can be of arbitrary dimension). I believe the manuscript means to draw a distinction between state spaces that happen to be of lower dimensions (could be 16x16-resolution RGB images, end-effector pose, joint positions) versus high-dimensional states (e.g., \"high\"-resolution images, joint positions of many agents, etc.).\n\nSection 2: The manuscript states: \"Recently, Chen et al., (2021) propose to learn policies from 'in-the-wild' videos. While achieving impressive results, the videos are still curated by human without much extraneous components.\" This sounds a little too subjective for my liking. If making a claim about a method’s ability to ignore the unimportant content from videos with more or less \"extraneous\" demonstration subsequences, there should be a way of *measuring* the extraneousness. I would wonder how the manuscript arrived at the conclusion that its curated dataset is better and more realistic than in Chen et al., (2021).\n\nSection 3.1: Why define an MDP? The MDP formulation is not used in the remainder of the manuscript. How are trajectories incorporated in the MDP?\n\nSection 3.1 ends with \"When there is one reference trajectory,\". Section 3.1 is incomplete/cutoff.\n\nSection 3.2: The manuscript states \"Assume more than one demonstration sequence are given, we can match their temporal embeddings to find the most task-relevant portions of the video.\" Beyond this, the manuscript implicitly assumes access to a *diverse* set of demonstration sequences. It is easy to find a set of maximally-noisy (i.e., 50%) sequences that will result in perfect alignment with each other and will teach the agent extraneous behavior. Accepting that what we *really* need here is indeed a diverse set of sequences, further discussion is needed for how detect/measure/ensure that diversity.\n\nSection 3.2: The manuscript states \"In the case where a perfect reference trajectory (no noise) is available, we can match frames in other sequences to that of the reference trajectory.\" Under the manuscript's assumptions (e.g., no annotations), how would the model know when there is a perfect reference trajectory? The approach would not be capable of this, without some form of supervision.\n\nSection 3.2: The manuscript should be more clear about the training paradigm: modular pre-training steps, imitation, refinement, etc. It is unclear where what losses are imposed, when.\n\nSection 3.2: The manuscript should be more careful about the use of the term ‘temporal’, as this implies that the model has, e.g., a time-recurrent inductive bias that attempts to model the environment's transition dynamics. Where this term is used in this manuscript, only an MLP is imposed, at each time-step, with no regard to other time-steps.\n\nSection 3.2: UVA: The UVA needs to be compared with other alignment strategies, to show its true value -- e.g., dynamic time warping, cross-attention, factorized self-attention, temporal energy-based similarity, etc.\n\nSection 4.3: More experiments and baselines are needed. As it stands, the manuscript was not even able to compare the baseline on all tasks. More time should be taken to reimplement the baselines and benchmark, as necessary.\n\nSection 4.3: Table 4 is an unfair comparison and provides no information, since the approach has access to a perfect reference.\n\nSection 4.4: Figure 5: No intuition is provided. What task was being performed — qualitatively, how should we judge that an arbitrary frame is *necessarily* extraneous or not?",
            "summary_of_the_review": "See main review above. The manuscript has several areas that are either incomplete, lacking description and support, and lacking in experimentation; this limits one's ability to assess the value of the methodology.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this paper, a special scenario of imitation learning (IL) from imperfect demonstrations, IL with extraneousness,  is studied. The extraneousness means that there could be \"consistent but irrelevant\" subsequences existing in the demonstrations. The main idea for the proposed extraneousness-aware IL (EIL) algorithm is to filter out these undesirable subsequences by self-supervised learning. Experiments over several continuous control tasks as well as a newly proposed discrete control task are conducted to verify the effectiveness of EIL.",
            "main_review": "Strengths of the paper:\n- The paper studies a new setting of IL from imperfect demonstrations. As discussed in the paper, the majority of the previous researches in this area focus on IL from noisy demonstrations, under which the demonstrations could be wrong, but are always relevant to the task. In my view, the IL with extraneousness setting indeed appears in real-world applications, and it is evidently necessary to propose new techniques under this setting.\n\nWeakness of the paper:\n- In my view, the main weakness of the paper lies in experimental evaluation:\n>- For the first paper to study IL with extraneousness, it is necessary to convincingly show its difference from other IL settings. For example, including state-of-the-art IL from noisy demonstrations algorithms and third-person IL algorithms as baselines. If these methods failed, the readers would notice that IL with extraneousness is a unique and challenging problem that requires new approaches to solve.\n>- The results lack showing how the portion of irrelevant subsequences, as well as the number of demonstrations, affect the performance of EIL. I guess that when the portion of irrelevant subsequences gets high, and the number of demonstrations gets smaller, the problem would become more challenging. It is necessary to verify the sensitivity of EIL under these factors to show its effectiveness as well as limitations.\n\nFurther questions and suggestions:\n- I wonder whether it is necessary to make the detection of irrelevant subsequences a fully self-supervised problem. I think the imitation training error may also be useful information for detecting these subsequences. While for EIL, the IL part and the self-supervised part are done separately. \n\n- I don't know why not share the feature-backbone part for the image encoder and the policy? Even though the paper suggests this possibility, it wasn't realized in the experiments.\n\n- The paper needs further proofreading to fix minor errors. To list a few,\n>- In Equation (1) and (2) there should be paratheses after $exp$.\n>- In the first line of Section 4.1, the word \"extraneous\" is not correctly spelled. \n>- Part of the numbering of the tables and figures in Section 4.3.2 seems to be incorrect.\n\n\n\n",
            "summary_of_the_review": "In summary, even though the paper studies an interesting and meaningful new problem, I think the experimental part needs significant improvement as described above. \n\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper motivates learning from demonstrations that include extraneous subsequences which are locally consistent but not relevant to the task. It introduces a method to filter out such subsequences using a temporally aligned representation. Experiments are run on simple robotics tasks (reach, push, and stir with distractor sub-trajectories) and a new “learning from slides” task (a discrete maze interspersed with irrelevant frame sequences). The proposed method effectively learns from demonstrations with extraneousness in these settings.",
            "main_review": "Strengths:\n* The idea of extraneousness in demonstrations is interesting.\n* The paper introduces experimental settings for testing this idea.\n* The proposed method makes sense and seems effective at filtering out extraneousness.\n\nWeaknesses:\n* I wonder if other (perhaps simpler) approaches could work in this setting. For example, if an observation from the environment is provided ahead of time, some simple clustering of observations from the demonstrations might work for filtering out extraneousness. Also, if a goal is specified, goal-conditioned RL or planning could work well.\n* The baselines (BC, RL, TCN) perform quite weakly and it seems like there might be something wrong in these implementations or they might need tuning.\n* There are several missing experimental results: there is no RL baseline in Table 3, and there are missing tasks in Table 4 and Table 5.\n\nBaselines:\n* Behavior cloning: It's unclear to me whether the behavior cloning baseline takes in the embedding or image. If it is using the embedding, I’d expect better performance. I'm surprised that its performance on the stir task in Table 4 (12%) is so much worse than it was in Table 2 (77%). The standard deviation of 51% in Table 2 seems high. I'm also not sure why behavior cloning fails at learning from slides in Table 2.\n* RL: This could use more information: which algorithm, how long was it trained, and training curve plots. Section 4.3.1 says RL algorithms struggle to accomplish the reach task due to high-dimensional input. I don’t believe this reasoning and there is no analysis to support this claim.\n\nOther experiments:\n* Section 4.5 (“Ablation Study”) is not useful; these hyperparameter experiments don’t provide meaningful insight.\n* The end of section 3 says we can substitute RestNet-18 with the frozen encoder. It could be nice to see this comparison.\n* Results ablating the use of action vs image in the encoder could be interesting.\n* Random policy performance on each task would give a better sense of difficulty. So would an additional table showing success rates when trained on demonstrations without extraneousness.\n\nWriting suggestions:\n* Figure 2 could use more explanation, e.g. description of the colors representing each video.\n* The experimental questions at the beginning of Section 4 aren’t interesting. As a rule of thumb, yes/no questions aren’t great for these.\n* A description of the oracle would be helpful. It’s not clear to me why it doesn’t achieve 100% success (for example in Table 2).\n\nMiscellaneous (extraneous?):\n* Figure 4: It would be nice to see this on a learning from slides video.\n* Section 3.1 ends mid-sentence.\n* Section 3.2: there is a typo in the nearest neighbor definition of v_j.",
            "summary_of_the_review": "The setting is interesting, but the baseline results are quite lacking. I think the RL baseline should do better; if it cannot do better, I’d like to see some analysis of why it fails. I’d like to see additional baselines (e.g. goal-conditioned RL, planning).",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The overall context is the one of imitation learning where the agent has to learn a policy that is similar to the provided examples. The authors propose a framework that allows learning from trajectories that contain “extraneous” parts, i.e., parts that are not relevant to the task at hand. What is more, they work with visual observations. In order to do so, they embed the observations and actions using a self-supervised technique called TCC. They then propose a “voting”-based procedure to filter-out the irrelevant parts of the demonstrations. They afterwards perform behavioral cloning on the remaining parts of the demonstrations.",
            "main_review": "**Strengths**\n\nThe setup is interesting because it is realistic. Humans are perfectly capable of filtering out irrelevant parts of the demonstrations while it may confuse an artificial agent.\n\n**Weaknesses**\n- Method\n   - There are two fundamental points I don’t understand with the method.\n       - Concerning TCC: Let’s imagine you have two demonstrations. One demonstration has 50% “garbage” at the beginning and then 50% task-related content. The other one has 50% task-related content first then 50% garbage. Given my experience with TCC, aligning these two trajectories will not work and the resulting embedding will be completely off. Do you agree? If no, can you explain why, and if yes, can you then develop on the hypothesis made about the demonstrations so that the readers understand the limitations of the method.\n       -Concerning UVA: Consider now you have N demonstrations that are 90% task related but all start with 10% of irrelevant content (realistic setting=> most “teaching” videos on Youtube start with a “useless” introduction/advertisement for example). If most demonstrations start with some garbage, then UVA, starting from the first frame and taking the closest neighbors will actually reject all the task-relevant content and select the garbage. Did I misunderstand? If yes, can you clarify how UVA works, if no, can you think of a trick that could help with this issue? E.g. repeating the procedure starting from different frames of the demonstrations and “averaging”? \nCan the authors explain when/how the UVA procedure ends? I think this should be in the paper.\n\n- Related Work\n  - PWIL (https://arxiv.org/abs/2006.04678) was the first paper to use TCC to align demonstrations in the context of imitation learning and I think the authors should 1) cite this paper 2) should not claim the usage of TCC as a main contribution, or otherwise explain how different it is from PWIL.\n\n\n- Experiments\n    - Please report the percentage of extraneous demonstrations in each setup.\n    - I don’t understand why BC fails on the learning from slides task. As the extraneous parts are within a completely different subset of the state space (the “slides” look very different). It should be able to perform correctly, by learning the correct behavior on the relevant part of the image-state-space. Maybe it is just because you affect random action to the extraneous frames? Can you try putting a single action (like zero vector) to all these extraneous frames? I feel like it is much easier to detect “irrelevant frames” if they are associated with pure noisy frames. I would say it is hard when “meaningful” actions are also performed in “irrelevant” parts of the demonstrations.\n    - Can you explain why BC performance drops so much when not provided why the reference trajectory. \n    - Why is there one environment missing in table 3 and two missing in table 4?\n    - As PWIL first used TCC, you could use PWIL+TCC as a baseline too. This would really show if ignoring extraneous parts helps or if a “classical” imitation learning method performs correctly.\n    - Why is EIL better without reference?\n    - When a reference trajectory is provided, EIL actually gets the information that this trajectory is the “good” one. I think one important missing baseline is then BC on this single trajectory. Other papers like ORIL (https://arxiv.org/pdf/2011.13885.pdf) present methods to learn from demonstrations and unlabeled experience (that could be your extraneous demonstrations). In the “one-reference trajectory” setup, having this baseline seems really important to me given how close the setups are.\n    - If the core contribution is UVA, why not testing on proprioception (without embedding) before extending to vision-based observations?\n    - I would encourage you to check out https://arxiv.org/abs/2105.12034 for HP selection in the context of Imitation Learning.\n- Writing\n    - 4.1: “extI raneous”\n    - 3.2: “loss described in …” ?\n    - 3.1: “When there is one reference trajectory, … “ ? A whole paragraph is missing here?\n",
            "summary_of_the_review": "This is an interesting paper that deals with an important and understudied problem, yet, in my opinion, it requires more work, notably concerning the evaluation and the writing, before being published. I would increase my score if the authors clarify the questions I asked in \"Method\", if they position better w.r.t related work and solve the major issues I raised in the \"Experiments\" part.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}