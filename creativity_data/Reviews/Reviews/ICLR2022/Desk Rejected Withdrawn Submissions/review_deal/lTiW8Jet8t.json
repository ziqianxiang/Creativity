{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper aims to make Ensemble Graph Neural Networks for node classification more efficient, wrt computational time, the number of arithmetic operations and memory usage. They show that their framework for creating diverse GNN ensembles with ENI (GEENI) is faster and more accurate than single model GNNs.\n \nThey achieve this through three main contributions. First, they propose an Ensemble creation technique that promotes accuracy and model diversity. They do this by iteratively adding models to an ensemble based on the model’s prediction accuracy and disagreement with the current ensemble’s predictions. \n \nNext, they argue that incorrectly classified nodes (error nodes) are harmful when their messages are sent to neighbouring nodes. Therefore, they propose the Error Node Isolation (ENI) technique, where error nodes are identified and isolated (i.e. they are not allowed to pass messages to other nodes).  They claim that ENI leads to better node representation and also decreases the number of operations, memory usage and the runtime during inference. \n \nFinally, to further increase efficiency they explore Neural Network and Edge pruning techniques. In the proposed edge pruning approach they iteratively find a set of unimportant edges that can be pruned while maintaining a certain accuracy constraint. They consider edges for pruning in an order defined by a proposed redundancy score and use an early stopping criteria to avoid traversing the entire graph. Furthermore, they state that it is hard to apply pruning to normal single model GNNs because nodes that become error nodes, as a result of the pruning, propagate their harmful messages in the graph. However, the GEENI is more resilient to pruning for two reasons. First, since they use an ensemble the impact from one model misclassifying a node is reduced. Second, since the ENI method isolates error nodes their impact on other nodes is reduced. \n",
            "main_review": "The authors seem to achieve good results. They obtain state of the art results for node classification on three citation network datasets. Also, they find that the GEENI is between 2.1 to 2.8 times faster than the current state of the art models, see Table 1. \n\nHowever, despite that the results seem promising, the experiment section of the paper is not presented clearly enough for me to be confident in my interpretations of the results. In the first section under: “GEENIs are faster and more accurate than traditional single models.”, you find that GEENIs outperform the best single models and the ensembles without ENI in all tasks when you present the results in Table 1. However, it is not entirely clear to me which metric you are using in your comparison? My suggestion is that you add which metric you are using to Table 1, as well as when you refer to Table 1 for the first time in the text. I also do not understand how you use these models for semi-supervised learning. Can you please add an explanation for how that problem was set up?\n\nAnother point of confusion with the section: “GEENIs are faster and more accurate than traditional single models.” is the speed comparison. First, I don’t understand whether the column “Speedup from ENI” in Table 1 is a speed comparison between GEENI and the “Best Single Model” or between GEENI and “Ensemble without ENI”. Also, from your abstract, it sounds like you were going to present results from both of these speed comparisons, since you state that: “ENI are /.../ faster (by up to 2.8x and 5.7x) when compared to the best-performing single models and ensembles without ENI, respectively.”. Yet, I can’t seem to find results for a speed comparison where the GEENI is 5.7x times faster than either of the mentioned models? Second, from the information in the abstract, it appears as if the speed column in Table 1 actually presents the speed comparison between the single model and GEENI. However, if you want to measure “Speedup from ENI”, shouldn’t you compare GEENI to “Ensemble without ENI”, such that the only difference between the two approaches is the addition of ENI? Please clarify which models “Speedup from ENI” are comparing and why this particular comparison measures the speed-up effects from ENI.\n\n\nAnother thing that could be improved in the experiment section is the fact that the authors only use two types of datasets to evaluate their model: citation networks and the Reddit dataset. It would be insightful to see whether the performance of the GEENI approach on more types of graph datasets with other types of challenges in order to investigate whether the approach generalizes.\n\nThe authors compare accuracy against inference time for three different approaches: the GEENI, The Best single model with Network and Edge pruning, and the Best single model with another inference efficiency technique called UGS, see Section: “GEENIs prune better than traditional single models, being significantly faster at iso-accuracy”. The authors find that The best single model with Edge and Network pruning performs better than the same model with UGS at the same inference time (see Figure 3). However, the results are again not presented clearly enough for me to trust my interpretation of them. Why do you report inference time as a percentage in Figure 3? My suggestion is that you add a more thorough description of this figure in the caption of Figure 3. I would also like to know why you only compare your approach against one other inference efficiency approach (UGS)?\n\nIn this experiment, they also find that their novel ensemble approach GEENIs is more accurate at a lower inference speed compared to the two single models, see Figure 3. My understanding is that they attribute GEENIs better accuracy to their error node isolation technique and to the fact that GEENI uses an ensemble of models. However, it is not apparent to me how they came to that conclusion based on this experiment. From my understanding, the difference between the Best single model with edge and network pruning and GEENI is that the latter uses both ENI and an ensemble of methods. Therefore, the improvements in performance could come from either ENI, the Ensemble or both. Given that my interpretation is correct, my suggestion is that you also run this experiment with an Ensemble model where you remove the error node isolation procedure. That way, you can see the individual impact of the ENI and the Ensemble. Could you please explain how you can tell that both the Ensemble and ENI contribute to the accuracy? \n\nIn section: “Discussion of overheads”, the authors evaluate the effects on Edge Pruning time from using their importance-based ordering score with early stopping. They find that their Edge pruning is 2-3 times faster compared to a random exhaustive search of all edges. However, I don’t understand which dataset you used for this comparison. \n\nAnother point of confusion is the importance score function itself. When you formulate the importance score function in section 5, you mention that you base the function on a number of observations regarding what makes an edge redundant. However, I think that you should mention from where you made these observations, e.g. Did you base them on an analysis of the Reddit and citation datasets? If so, it would be particularly interesting to see how the scoring function generalizes to other types of graph datasets.\n\nIn the section \"Model diversity is vital for effective ENI\", the authors find that an Ensemble model with ENI is better at detecting error nodes compared to a single model with ENI. They also show that adding more diversity to the ensemble by changing the individual model hyperparameters and by varying regularization increases the accuracy and the percentage of detected error nodes of the GEENI method, see Table 2. But again, I'm having difficulties with interpreting the presented results in this section. Are all of the regularization techniques in Table 2 applied to the GEENI model? Why do the three last row names begin with a \"+\" sign? Which regularization technique is the row name: \"+ Network Diversity\" referring to? It would also be nice if you could add an experiment where you evaluate the impact from diversity from your proposed Ensemble creation algorithm (Alg 1).\n\nMinor comments: \nGrammatical error page 4: “Then, each model is scored based the extent to which adding the...”\nSpelling mistake page 4 (predicitons): “by promoting models that make correct predicitons”\nExtra word page 4 (nodes): “predicitons for nodes correctly classified nodes by the ensemble”",
            "summary_of_the_review": "The authors in this paper seem to achieve good results. Mainly the GEENI method seems to result in increased accuracy and lower inference runtime. They also report that they have achieved state of the art results for node classification on three citation network datasets. However, the method is only evaluated on two types of datasets, and their pruning techniques are only compared against one previous approach. Also, the experiment section is not presented clearly enough for me to understand their results and how they arrived at certain conclusions.\n\n\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to build a mixture of GNNs for node classification. The method consists of three different elements. The first one is the ensemble construction algorithm: models are generated using various architectures and hyperparameters and added to the ensemble if they tend to classify correctly nodes that are already well classified and if the provide a different answer than the consensus when the latter is wrong. The second element is a *node isolation* method: in each model, nodes that do not agree with the consensus are prevented from sending messages. The third one is the application of pruning techniques to models of the ensemble for efficiency. The proposed approach is evaluated on three data sets. The results provided in the paper show that the proposed ensemble method increases accuracy by a few percent with respect to the baselines while being between 2 and 3 times faster. Experiments also study pruning efficiency and diversity of the model.",
            "main_review": "**Strong points:**\n- The proposed approach is more accurate than a single model, which is to be expected. \n- But the approach exploits the ensemble to also make it faster than a single model, which is really nice.\n- The paper is reasonably well written and clear. \n- Main points of the proposed approach are evaluated in the experiments.\n- As far as I can tell the experiments are reproducible, as a lot of hyperparameters are provided in the appendix B.\n\n**Weak points:**\n- Imho, the ensemble creation process is not well motivated or discussed. I understand that the models in the ensemble must be accurate and diverse. The proposed algorithm builds the ensemble incrementally. It tries to include models that make the same prediction as the ensemble when the ensemble is correct and a different prediction when the model is wrong. So the latter prediction might be incorrect. The paper states that this leads to \"ensembles composed of accurate and diverse models\". I think additional explanation are necessary. I can think of various strategies to build such an ensemble and it is not clear to me why the proposed strategy is the best. For example, why not add models that make correct predictions when the ensemble is wrong? Or models that do not change the prediction of the ensemble for nodes correctly classified? The former would increase diversity more, as nodes wrongly classified by the new models would be isolated, leading to different networks. The choice seems somewhat arbitrary to me. \n- As far as I understood correctly, the proposed approach is limited to inference tasks where the goal is to label nodes of a partially labeled graph. It does not handle tasks other than node classification or classification of nodes of a new graph.\n- Having figures on half of the page with main text on the side makes the paper awkward to read.  \n- I saw no statement that the code would be made available. \n\n**Questions:**\n- I would really like the authors' opinion on the first weak point listed above.\n- Would you have any results studying the diversity of the models obtained by using different strategies to create the ensemble? \n- I think the number of models in the ensemble is not well studied. Three different architectures are considered. So it makes sense that ensembles of three models are better. I suspect that these ensembles contain one model based on each architecture. Is this correct? If so, then then model creation (Algorithm 1) does not seem that relevant. Wouldn't it be better to directly create one model per architecture and then aggregate them? I also acknowledge that results are discussed in the appendix for 5 models with 5 different architectures, but unless I am mistaken they do not answer these questions. \n\n**Details**\n- Use subsection in the appendix for better linking.\n- Figure 3 is too small. I also think that putting figures on the side with the main paper text to the left makes the paper harder to read.\n- Algorithm 1: I think V is not defined",
            "summary_of_the_review": "I think the paper is quite good already. It beats the state of the art and makes clever use of ensembles to gain both on accuracy and efficiency. While simple, I believe the combination of ideas is not trivial. My main concerns are that it seems to be applicable to node inference only and that I am not convinced the ensemble building algorithm is necessary.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors proposed a new framework for efficient ensembles of graph neural networks with the aim to achieve both improved classification/prediction accuracy and reduced inference complexity. In my eyes, the limited contributions lie in an ensemble of various existing techniques, including error node identification through majority vote and graph pruning. Overall, this paper only provides some heuristic ideas without theoretical support or large-scale experimental verification. ",
            "main_review": "Strengths: \n1. An ensemble of graph neural network models that can potentially achieve both improved classification/prediction accuracy and reduced inference complexity. \n2. Organization of the paper is clear.\n\nWeaknesses: \n1. The core idea is quite heuristic and lacks theoretical support on the optimality and efficacy of the proposed framework.\n2. Classification performance gain is not quite obvious from the given experimental results, and it is not clear how the speedup ratio (see table-1) is computed. \n3. English usage can be improved, but this is minor compared to the technical contribution.\n",
            "summary_of_the_review": "This paper is neither theoretically strong nor impressive with interesting experimental insights, and I tend to reject it for ICLR-22. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Authors propose a framework for creating Graph neural network ensembles with ENI (GEENI) and claim they are are both faster and more accurate than traditional single models. Authors experimented the model on transductive and inductive node classification tasks to demonstrate the performance of the proposed methods.\n\n\n",
            "main_review": "\"For an edge connecting two nodes A and B, we compute its importance score\" I am curious how this formula of important score is designed? It would be great to provide more insight, as well as ablation study on the impact of the importance score to the model performance.\n\nIt would be great if authors can provide more details on the speed comparison. How is the speedup computed, given authors used parallel computing? Also is there any comparison of the training cost for the proposed method, as it seems requires multi round of training to identify error nodes? \n\nIt seems the efficient improvement is from the pruning. Therefore if without pruning, what is the speed/accuracy for the proposed ensemble with ENI? Also if we apply similar pruning to the regular ensemble method, would it also benefit from that without reducing accuracy?\n\nThere are many ensemble methods, and authors used one of the ensemble methods as benchmark (select models based on max ensemble quality score). Therefore authors should make it clear that the proposed method achieves improvement over one of ensemble method.",
            "summary_of_the_review": "My major concern is the focus of this paper. My feeling is that authors stacked multiple tricks (ensemble, node pruning, edge pruning) to improve the model performance. Most of the tricks are well known practice for GNN, and it is unclear which technique is the focus of this paper. I would encourage authors to highlight the major novelty of this paper, instead of proposing a combination of new/existed techniques.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not observed",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}