{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a video transformer architecture by introducing class-agnostic object modeling. In practice, two object-level streams are adopted. One is for appearance modeling by conducting self-attention over the image and the specific object regions. Another is for dynamic modeling by applying the object trajectory interactions. Extensive experimental results demonstrate the effectiveness of the proposed method.\n\n",
            "main_review": "Strengths\n+ The paper is well written and easy to follow.\n+ The proposed object-region video transformer seems straightforward but makes sense for the video recognition tasks.\n+ The experimental results on various video benchmarks demonstrate the effectiveness of the proposed method.\n\nWeaknesses\n1. Please give a more detailed description of the Mformer and Mformer-L, since I could not align their architectures to the original paper, especially by considering the GFLOPs and Param.\n2. How about the performance of Mformer + STRG + STIN by using the detected box in Table2-(a)? Could the proposed method still maintain the performance gain in such a case?\n3. Comparing Table2-(b) and (c), the performance gain drops seriously, could you give more discussion about the reasons?\n4. Is the performance of the proposed method sensitive to the average number of objects in each video?\nDoes the proposed method still achieve good performance with massive objects in the video?",
            "summary_of_the_review": "From my perspective, the paper does not have any major flaws, but I still have some questions about the technical details. Thus I rate the paper as weak accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces class-agnostic object-centric information into the video Transformer. The proposed ORViT block has two object-level streams: the appearance stream where self-attention applies over the patches and object regions, the dynamics stream, which models trajectory interactions. The experimental results on Something-Something V2, Epic-Kitchen100, Diving48, and AVA show the method's efficiency.",
            "main_review": "Strengths\n1. This paper firstly introduces object-centric information to Transformer architecture in videos.\n2. The experimental results show significant performance improvements.\n\nWeakness\n1. This paper uses class-agnostic object-centric information. However, when we have the object bounding boxes, the object category information is usually provided, no matter from ground truth annotation or off-the-shelf object detectors. Why only utilize object boxes and ignore categories? The motivation needs to be stated more clearly.\n2. The number of objects is dataset-specific. Different hyperparameters are adopted for Something-Something v2, Epic Kitchens 100 (EK100), AVA, and Diving48. Does this hyperparameter setting based on some prior knowledge, e.g., counting the objects in videos? What is the impact of this hyperparameter? Such experiments and analysis are missing in the paper and supplemental material. It'd be good to experiment with setting different objects on the same dataset and seeing the performance. Is it possible to set a general number?\n3. The inference time. This method needs bounding boxes provided. While in real-world applications, bounding boxes could only be generated by object detectors and trackers. Intuitively, the test time should be longer. How about the inference speed when including the boxes generation stage, compared to those who don't need boxes?\n4. There are works modeling object bounding boxes in video relation detection and activity prediction [a][b]. Specifically, Liang et al. [b] use LSTM to person-object boxes for activity prediction in videos. Such works are not mentioned. Discussion and analysis are needed.\n\n[a] Shang, X., Ren, T., Guo, J., Zhang, H. & Chua, T.-S. Video Visual Relation Detection. ACMMM 2017.\n\n[b] Liang, J., Jiang, L., Niebles, J. C., Hauptmann, A. & Fei-Fei, L. Peeking into the Future: Predicting Future Person Activities and Locations in Videos. CVPR 2019.",
            "summary_of_the_review": "In total, importing box coordinates seems to be promising for Transformer architecture to understand actions in videos. While incorporating object bounding boxes to recognize actions is not novel, this method is the first trial for Transformer architecture in videos. If the authors could clearly state the motivation and have additional experiments, I'd like to raise my rating.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper modifies the self-attention used in Transformer architectures for video understanding to incorporate object detection boxes. I can improve Something-Something-v2 accuracy by 1.4%, at the cost of using 36% more parameters in the transformer, as well as external object detectors [Faster R-CNN detector (Ren et al., 2015; He et al., 2017) with ResNet-50 backbone (He et al., 2016) and Feature Pyramid Network (FPN) (Lin et al., 2017) that is pre-trained on MS COCO (Lin et al., 2014)] and trackers [SORT (Bewley et al., 2016)]. ",
            "main_review": "The strengths of the paper are\n+ Integrating object-level information into video understanding models is an interesting research direction and can be useful; \n\n+ Using a tracker in addition to object boxes seems new. \n\nThere are several drawbacks that I see in the current work, as well information missing in the paper. For example \n\n- It is not clear wether improvements are from (i) external data (COCO, ground-truth) and models (Faster RCNN, Tracker), (ii) increased model complexity over baseline (+35% parameters), or (iii) ground-truth boxes used in all ablations.  \n\n- GT boxes *on the test set* should not be used for any ablation or SOTA comparison. These boxes can be used for training but not for validation. I suggest to redo ablation studies on Something-Something-v2 and without any ground truth boxes used in evaluation. \n\n-It is not clear if the improvements of ORViT come from the improved attention that uses the detected boxes, or from an improved training methodology (e.g. the current work uses a differently tuned training & hyperparameter setting than the MFormer baseline).\n\n- Which detectors are used to provide boxes? Simply referring to \"For SSv2 and EK100, we used the supplied detection boxes from the original codebase.\" is not sufficient. \n\n- Using external detectors and trackers might be ok to prove scientific value of the current approach; however, it induces extra external information into the method and therefore is not an apple-to-apple commparison to prior work. For example, I assume the detectors are trained on COCO (as mentioned above, it's not specificed); therefore enjoying more external pretraining than the compared methods.\n\n- Besides pretraining advantage, using external detectors and trackers have significant practical limitations (e.g. applying the current approach in any practical setting is very cumbersome and time consuming). This is a minor point among the others. \n\n- What is the \"learnable object-time position embedding\" concretely? To me that sounds like simple positional embeddings. It is not defined fully. Is it possible to relate this to prior work using simple positional embeddings? \n\nminor: \n- \"If fewer objects are presented, we set the object coordinates with a zero vector\" \nshould be \"set to\"\n- Why does ORViT [L:2, 7, 11] + Traj have 148M param, but ORViT [L:2] the same number of parameters as the baseilne (Mformer)? I think these are typos in the main paper (supplement has different #params).",
            "summary_of_the_review": "Object level information in video transformer models is an interesting research direction (and has been used in the past e.g. in STIN (Materzynska et al., 2020) and (STRG) (Wang & Gupta, 2018)). The current approach applies this to more recent vision transformer models and stronger baselines. The technical novelty over STRG is relatively limited. There are minor differences that benefit from recent trends in transformers (e.g. positional embeddings). Further the current work uses a tracker (even though it's impact is marginal as shown in the ablations).\n\nMy main critic of this work is that GT information is used for all ablations and the state-of-the art comparison. Further, technical novelty over prior works such as STRG are relatively minor. Empirically there is an improvement over such prior work; however, this is also related to the technical advancements in vision transformers that this work builds upon. \n\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents a video transformer that explicitly encodes objects and their locations in addition to standard non-overlapping patch token embeddings considered in the previous works.  Object descriptors are extracted with ROIAlign operation on patch tokens using either GT or predicted boxed obtained by off-the-shelf detector. Attention is then applied on a combined list of object and patch tokens. In a parallel branch, a separate attention is applied on object co-ordinate locations which are then shape expanded to match patch token embeddings. Both the features are then merged to obtain the refined patch tokens. Experiments conducted on four different datasets show improvement over baseline transformer models that does not use any object information. ",
            "main_review": "The idea of explicitly modelling object and their interactions although is well studied for action recognition, this paper sheds light on how such additional information can benefit transformer models. As the research on transformers is in its early days, this work provides another important insights into handling of objects for action recognition. The proposed idea to integrate objects with patch tokens seems reasonable, and obtains minor improvement over baseline models. \n\nMy concern is the lack of discussion around object box generation which is the main reason for performance improvement. In the paper, both ground truth and predicted boxes with  off-the-shelf detector are explored. Using a separate CNN network to generate these object bounding boxes kills the whole purpose of transformer models which are popular for being convolution-free. How do authors justify this? Have authors considered generating object boxes on a key frame [2] in a parallel transformer module similar to [1]. Also, the improvement in performance with predicted bounding boxes is small (only with GT boxes, large improvement is obtained) considering the cost of training a separate object detector and need for a two-step pipeline.\n\nAs authors compare extensively against recent video transformer models, it is not clear how does the proposed work with an additional object detector fit into this new line of work.\n\nIt is also claimed that detector considered in this work is class agnostic (Page 6: Box Input to ORViT). How is the detector trained on MS-COCO class agnostic? Clearly, object information is used for training the detector.\n\n[1]. Carion et al, End-to-End Object Detection with Transformers, ECCV2020.\n[2]. Giridhar et al, Video Action Transformer Network, CVPR19\n\nOther minor concerns:\n\nTable 1: Authors should also add ORViT Mformer with predicted boxes (62.3% given in the text). Results shown on GT boxes alone in Table 1 is misleading as the assumption of availability of GT boxes is too strong. In many cases, it is not possible to annotate object classes for large scale datasets.   \n\nTable 2-3: Why has the authors used different baseline model for different datasets? Mformer on Something-Something and Epic Kitchens, and TimeSformer on Diving. There is an improvement of 8% with TimeSformer on Diving? Does this mean that the gain with proposed approach is high with TimeSformer compared to MFormer? Comparing different baseline models for different dataset makes it hard to follow.\n\nSpatio-temporal action detection: The baseline for AVA seem to be much lower compared to best results reported in MViT. What is the reason for not comparing against best models from previous works? Also pre-training is done on K400 instead of K600. Will similar improvements be observed with best performing variants of MViT, MFormers? \n\nHas the authors tried using objects alone along with neighbouring context instead of considering all the non-overlapping patch tokens.\n\nAVA Table: Should the MViT 16x4 is 24.5 mAP or 25.5 mAP. In Table 8 of MViT paper, it is specified as 24.5. Is this a typo or models are trained again?\n\nMinor fixes:\n1. Sec 3.1 : Notations are not clearly defined. T X W X H X d == what is d??\n2. Sec 3.2  : Define O at Bounding boxes  B \\in R ^{TO X 4} .. assuming O is the number of objects. Are the number of objects O fixed??\n  3.  Figure 4 -> Table 4. ",
            "summary_of_the_review": "The paper proposes an incremental approach to modify existing video transformer models by incorporating object information. The proposed approach to combine patch tokens and object descriptors is reasonable. However, paper lacks discussion around object detector in the context of video transformers, their bottleneck in training with CNNs and the need for a two stage pipeline for a small improvement in performance is limiting. I am willing to reconsider the rating if my concerns are addressed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}