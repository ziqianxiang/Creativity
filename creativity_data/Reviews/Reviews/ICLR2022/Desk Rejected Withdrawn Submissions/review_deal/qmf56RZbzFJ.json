{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes to minimize the Kulback-Leibler (KL) divergence between the policy and expert state transition trajectories. First, their approach learns the normalizing flows-based state transition model offline using expert data. Second, their method also learns normalizing flows-based forward and inverse dynamics models, which are trained using interaction experiences to predict the next states for the given policy’s action. Finally, they compute a reward function based on learned models to minimize the distribution mismatch over the next transition states between the expert and imitating functions. The policy is trained to optimize the reward function through the maximum-entropy RL method. The results are shown on basic control tasks such as half cheetah, hopper, walker, ant, and cartpole, and compared against OPPLO and Behavior Cloning (BC). ",
            "main_review": "Strengths:\n- Their problem is well-defined, and methods are explained clearly. \n- Related work covers basic prior work.\n- Comparison is provided with one of the recent methods.\n\nWeaknesses:\n- Their contribution is minor in terms of their algorithm designs and policy objective function.\n- The results are not well discussed. The evaluation metric should be clearly described. First, in Fig.2, why baseline performance drops in the middle when demonstration trajectories are increased?  Second, in halfcheetah, their approach stays continuously above expert. Does that mean the expert was sub-optimal?   \n- No ablation study provided to support their design choices such as the use of normalizing flows.\n- A comparison with a few other approaches that use either forward or inverse dynamics models for state-based imitation learning (Sun et al., 2019; Edwards et al., 2018, Liu et al., 2020; Jiang et al., 2020) is necessary to highlight the importance of their inverse and forward dynamics based framework.\n",
            "summary_of_the_review": "Their contribution is slightly below the bar, the results are not adequately discussed, and no ablation study is provided to investigate their design choices. However, the paper can be improved and turned into a good publication, but it is not there yet. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a method to learn a policy that matches the state distribution of the expert distribution. They train three conditional normalizing flows to estimate the expert state conditional likelihood mu(s’|s), the forward dynamics p(s’|s,a), and the inverse action distribution p(a|s’,s). They evaluate the proposed method against OPOLO and BC on a set of Mujoco locomotion tasks.\n",
            "main_review": "(1) Lack of discussion & empirical comparisons to prior work\n\nEmpirically, the paper only compares to OPOLO and BC. Given the large number of prior work & relevant SOTA algorithms in the problem domain, I believe more comparisons are needed. For example, how does the proposed method compare to NDI [Kim ‘21] (which requires expert actions)? E.g., is the performance comparable if you learn with vs. without expert actions?\n\nThere are many relevant prior work that are not cited, but should be discussed. For example, non-adversarial imitation learning is closely related to learning a stationary reward function in inverse RL:\n* EBIL [1] optimizes the reverse KL divergence in the trajectory space by treating the expert state-action marginal as an energy-based model.\n* RED [2] uses support estimation on the expert data to extract a fixed reward.\n* f-IRL [3] directly learns a parameterized reward function by gradient descent on the KL objective between the policy & expert state distributions.\n* f-MAX [4] provides an understanding of IRL methods from the lens of state marginal matching.\n\n[1] Liu et al., Energy-based imitation learning, 2020\n[2] Want et al., Random expert distillation: Imitation learning via expert policy support estimation, 2019\n[3] Ni et al., f-IRL: Inverse Reinforcement Learning via State Marginal Matching, CoRL 2020 https://arxiv.org/abs/2011.04709\n[4] Ghasemipour et al., A divergence minimization perspective on imitation learning methods, CoRL 2020\n\n(2) Insufficient empirical analysis\n\nGiven that this is an empirically-driven paper, I believe the paper can also be strengthened with additional ablation experiments to show the strengths of the different components of the proposed algorithm. For example:\n* Can you evaluate the quality of the learned normalizing flow models (e.g., by varying the offline dataset size), and how does this affect the overall algorithm performance?\n* Can this method scale to more complex imitation learning benchmarks (high-dimensional observations, and/or longer-horizon tasks), such as visual manipulation, which have been used in many recent IL papers?\n",
            "summary_of_the_review": "My main concerns are (1) a lack of comparisons to existing methods, both in Related Work discussion & experiments, and (2) a lack of  empirical analyses that show interesting insights (e.g., about the different components of the algorithm; strengths of the proposed approach over prior work; etc.).\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a state-only imitation learning approach removing the need for adversarial optimization. To achieve this, their method SOIL-TDM decomposes the reverse KL divergence between state-trajectory distribution between agent and the expert into three terms - Expert state transition, Environment model, and Inverse action distribution, that are learned separately from the collected environment interaction and expert data,  using a conditional normalizing flow. Subsequently, the reverse KL divergence between trajectories is a MaxEntRL problem with reward function which is a combination of the three terms above. The authors use the fact that if trajectory distributions are exactly matched then state-marginals are matched to explain their method as performing state-next_state distribution matching common in previous LfO approaches. Their method shows state-of-the-art results on locomotion imitation tasks with a varying number of trajectories.",
            "main_review": " Pros:   \n1. Presents a decomposition for reverse KL state-trajectory matching which allows the terms to learn individually in a non-adversarial way and use any RL algorithm to train for the reward composed of those terms.\n\n\n2. The paper shows results outperforming a recent baseline on off-policy imitation with different numbers of available expert trajectories.\n\n\n3. Paper empirically shows that the policy loss from their method can be effectively used as a stopping criterion for training IL methods as opposed to using ground truth reward function which is usually not available.\n\n\nCons:\n1. The method SOIL-TDM bears a similarity to the work FORM[3], where instead of using Bayes rule to expand $\\mu^{\\pi_\\theta}(s,s')$ in SOIL-TDM and learning 3 separate models, they directly model the density $\\mu^{\\pi_\\theta}(s,s')$ from agents current visitation distribution using a single model. The authors should elucidate and clarify the algorithmic difference between SOIL-TDM and FORM since the methods seem to differ primarily on the way  $\\mu^{\\pi_\\theta}(s,s')$ is parameterized for learning. It would also be important to justify why and how the Bayes expansion helps in learning as well to show the novelty of their approach.\n2. Paper has insufficient baselines - The paper presents a non-adversarial method and presents a comparison to recent baseline OPOLO.  I would recommend the authors to include experiments to compare the sample efficiency in terms of environments interactions as that is also an important metric in imitation.  I also recommend the authors to add additional baselines to claim state-of-the-art results to other methods that rely on learning from observation([2],[3]) and methods that learn from observation in a non-adversarial way [1], many of which are not compared in OPOLO paper. OPOLO presents a comparison with fixed number of expert trajectories(4), and it is not clear if OPOLO outperforms previous methods in sample efficiency with a differing number of expert trajectories which necessitates the need of including previous baselines for comparison as well since SOIL-TDM presents a comparison with differing number of expert trajectories.\n3. The paper does not provide a discussion of the distribution-shift encountered by the expert state-transition term in the reward function. $\\mu^E(s_{i+1}|s_i)$ is trained only on the expert state $(s_i \\sim \\mu^E(s))$ while it is used for training the RL algorithm at all states visited by the agent.  This reward function is also used for testing the convergence of SOIL-TDM as well.  A further discussion on this issue and analysis on the correctness of the OOD estimate of $\\mu^E(s_{i+1}|s_i)$ particularly in the presence of very few expert trajectories might be important since it forms an important part of training as well as the convergence objective.\n\n\t\nClarification:\n1. “A conditional normalizing flow policy has been chosen for the expert to make the distribution matching problem for OPOLO and SOIL-TDM - which employ a conditional Gaussian policy - more challenging and more similar to real-world IL settings.” Can the authors clarify further how normalizing flows are used in OPOLO and the relation to real-world IL?\n\n[1] Ni, T., Sikchi, H., Wang, Y., Gupta, T., Lee, L., & Eysenbach, B. (2020). f-irl: Inverse reinforcement learning via state marginal matching. arXiv preprint arXiv:2011.04709.    \n[2]Yang, C., Ma, X., Huang, W., Sun, F., Liu, H., Huang, J., & Gan, C. (2019). Imitation learning from observations by minimizing inverse dynamics disagreement. arXiv preprint arXiv:1910.04417.    \n [3] Jaegle, A., Sulsky, Y., Ahuja, A., Bruce, J., Fergus, R., & Wayne, G. (2021, July). Imitation by Predicting Observations. In International Conference on Machine Learning (pp. 4665-4676). PMLR.    \n\n\n",
            "summary_of_the_review": "In the paper's present state, I don’t think the paper is quite ready for publication. The reasons for my recommendation are a. Similarity with a previous method[3], b.  The paper should present a discussion/analysis on the issue of distribution shift in the SOIL-TDM objective in low expert demonstration regime since it forms a central part of the method before the paper is accepted and, c. Lack of sufficient baselines used for comparison in converged performance experiments, and the paper also lacks a comparison on the sample efficiency metric of environment interactions with respect to baselines in order to claim state-of-the-art performance.\n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes an adversarial approach for learning from observation, where in difference to prior work (GAILfO), the conditional distribution $p_E(s'|s)$ rather than the joint $p_E(s',s)$ is to be matched. In typical fashion, the optimization involves alternately estimating the reward function (density-ratio) $\\log \\frac{p_{E}(s'|s)}{p_{\\pi}(s'|s)}$ and updating the policy using reinforcement learning. Instead of using a discriminator*, the paper estimates $$\\log \\frac{p_{E}(s'|s)}{p_{\\pi}(s'|s)} = \\log \\frac{p_{E}(s'|s) p_{\\pi}(a|s,s')}{p(s'|s,a) \\pi(a|s)}$$ using density estimation (with normalizing flows) to estimate $p_{E}(s'|s)$ (performed once), $p_{\\pi}(a|s,s')$ (performed every iteration, but using the replay distribution $p_{\\text{RB}}(a|s,s')$) and $p(s'|s,a)$ (performed every iteration). The known term $\\log \\pi(a|s)$ turns the RL problem (generator update) into a MaxEnt-RL problem.\n\n\n\\* For estimating the conditional density ratio, one would usually use two discriminators $D(s)$ and $D(s,s')$, see e.g. [Han 2020. \"Unbiased Learning with State-Conditioned Rewards in Adversarial Imitation Learning\"]",
            "main_review": "I don't have much to state about the paper. I spent little time on the paper, but I also do not see how I could spend more time on it. I'm very familiar with the topic and was able to read the paper without ever feeling the need to pause and think. Expressed positively, the paper is well-written and easy to follow. On the other hand, it just has little content. My paragraph above is basically a complete summary of the approach. Apart from that, you can only find some experiments, and the typical sections on related work, introduction, etc in the paper. I do not see any noteworthy novelty.\n\nStill, I have a few comments:\n* The paper states that the approach is non-adversarial. I object and argue that using density estimation instead of binary classification for estimating the log-ratio does not change the nature of the approach. It is still an adversarial game, but between the generator/policy and the density estimator. \n* I also think, that using three density-estimators instead of two discriminator (or one if the joint distribution is matched) is not well-motivated. Sugiyama et al. [2012] show that direct density-ratio estimation (e.g. with a discriminator) is a simpler problem than density estimation. Using density estimation within imitation learning is also not novel. For example, Liu et al. [2020] also used normalizing flows to construct a reward function (but in a non-adversarial setting).\n\n* According to the derivations, the reward function should directly depend on the policy via the term $p_{\\pi}(a|s,s')$, which highlights why the approach is still adversarial. This dependency is ignored by keeping the reward function fixed during reinforcement learning. Instead of using the inverse model of the policy, the paper estimates the density based on samples from the replay buffer, which is not theoretically justified. \n\n* Matching the conditional distribution $p(s'|s)$ instead of the joint distribution is not motivated. The paper correctly states, that both objectives would lead to the same solution if the target can be perfect matched, however, in practice I would expect the joint distribution to perform better, as it provides a stronger incentive to reach the demonstrated states. \n\n\nReferences\n\nM. Sugiyama, T. Suzuki, and T. Kanamori. Density-ratio matching under the bregman divergence: a unified framework of density-ratio estimation. Annals of the Institute of Statistical Mathematics, 64(5):1009–1044, 2012.\n\nLiu, M., He, T., Xu, M., & Zhang, W. (2020). Energy-based imitation learning. arXiv preprint arXiv:2004.09395.",
            "summary_of_the_review": "The paper is easy to follow, but I could not find a significant contribution.\nEstimating the inverse model of the current policy based on the replay buffer is not technically sound.\nThe deviations from existing work (matching the conditional distribution, using density estimation to estimate the log-ratio) are very minor and, furthermore, not motivated.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}