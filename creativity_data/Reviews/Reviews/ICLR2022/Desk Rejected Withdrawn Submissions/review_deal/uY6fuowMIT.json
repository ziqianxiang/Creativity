{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a weakly supervised representation learning method.  The supervision is provided via the splitting criteria between two sets. Some attributes, namely, inactive factors, remain the same on either set, and the feature of samples is learnt via finding the correspondence from another set with similar active factors.\n\nIn addition to supervised setting, the method can also be extended to partially labeled data, namely ABC-X.\n\nAccording to qualitative analysis, the resulted representation is disentangled with respect to active factors via PCA visualization. Quantitatively, the ABC-X method shows superior accuracy in rotation angle estimation for simple 3D objects compared to previous methods.\n\n",
            "main_review": "First of all, the reviewer is not very familiar with the lines of the work on set supervision and contrastive learning. The followed reviews are not highly confident and relatively half baked.\n\n## strengths\n### Good quantitive results on 3D pose estimation.\nAccording to Tab. 1, the resulted representation achieves better accuracy in rotation angle estimation than other methods by a large margin.\n\n\n## weakness\n### math formulation of the method\nWhile InfoNCE is not the contribution of this paper, since it is at the core of the training, it should be provided here in this paper. Otherwise, the method part is incomplete.\nAlso, it was not good experience in reading through this paper. For scientific papers, rigorous math formulation is better than sentences.\n\n\n### weak supervision set up\nIt remains problematic to extend this type of weak supervision to practical usage, where for the two given set, one sample from set A cannot find any meaningful correspondence from set B. It also remains unclear how the method handles the degenerated cases where correspondence collapse.\n\n\n### missing related works\nThe set supervision setup is pretty similar to the formulation of \"multiple instance learning\" (MIL) which can be applied to OCR, video/image co-segmentation and speech recognition without temporal alignment. Authors should discuss the link to related MIL works.\n\n### embedding representation\nIs there any specific reason in using PCA to visualize the embedding, other than TSNE, which does a better job in distance preserving dimension reduction.",
            "summary_of_the_review": "Since reviewer is not an expert in the related field, there is not a good understanding on how this work differs from previous works on set supervision.\n\nThis paper proposed a self-supervision method via set supervision, which is not a very new set up. The reason of marginal rejection is because the set up needs quite a lot supervision to split the dataset. Provided the same type of supervision, simpler method can be used, e.g., techniques for attribute learning, to learn the representation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes to isolate active factors of a set of elements by learning the approximate correspondence between it and another set of elements that also has these active factors. A factor is inactive if all elements in the set share the same value in terms of this factor.  The correspondence of one element is achieved by computing the attention of it to all elements in the other set. ",
            "main_review": "Major concerns:\n\n1. This paper is a little bit hard to follow. \n\n2. In the introduction, authors state \"A more readily available form of set supervision is where the desired factor is active in each set. Continuing the example, such supervision can easily be obtained by simply imaging each car from multiple viewpoints (set A in Figure 1). This does not require correspondence in viewpoints across object instances, nor any pose values attached to the images. However, this\nsupervision makes learning much harder, as it no longer provides images which correspond in the desired factor.\" and indicate using sets of elements where the desired factor is active is not a good choice, yet in the method authors use exactly such sets and supervisions to isolate active factors. This confuses me a lot.\n\n3. It looks like in all experiments active/inactive factors are known a priori. The double augmentation and construction of training mini-batches also involve factor-specific operations. I wonder whether this is a strong assumption when applied in practice. \n\n4. In the cases where there are multiple active factors to isolate, it's hard to disentangle these factors from each other using ABC. Although the PCA results in Fig.3 indicate multiple active factors may be disentangled, this is probably becasue during training ABC observes constructed data where not all active factors change simultaneously. However in practice, it's likely that we can only observe data where all active factors change simultaneously. I'd like to see authors' comments on this point.\n\n5. All experiments are conducted in very controlled settings. And I'd like to see more experiments on natural data. The pose estimation experiment uses training sets of CompCars, Cars196 for car category while using the test set of Pascal3D+ car category. Reason?\n\n6. Following above points, current experiments cannot fully support the claim in Section 5 \"It is perfectly suited for domain transfer and the common real-world scenario where information about an abundance of unannotated real data is desired and related synthetic data is available.\"\n",
            "summary_of_the_review": "This paper proposes an interesting and simple idea to isolate common active factors of different sets of elements. Experiments in controled settings demonstrate its effectiveness to some extent, but its effectiveness should be more comprehensively validated with natural datasets.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to use set supervision to locate the contrastive pairs, seemingly for feature disentanglement. However, its writing is so unclear that it is difficult to figure out what is going on.",
            "main_review": "The paper is written in an extremely unclear manner that makes understanding almost impossible. \n\nLet's start by the fact that the center theme is \"set-supervision\", but it never defined what it mean by \"set-supervision\" mathematically, and the casual descriptions in many places sometimes lead the readers to think that it sounds like a supervised classification problem, e.g. \"the data is grouped into sets such that all images in a set have the same object class\" -- which means you can directly assign a class label to each and every image in that set -- or a multi-label classification problem (however whether it is the former or the later is completely unclear because of the lack of definition). In the experiments, they seemed to indicate that they put class labels as \"set-supervision\" in the MNIST experiment (Fig.5) but I couldn't find any clear remark about it besides in the figure caption. The ShapeNet experiment in Sec. 4.3 also sounds like it's just a regular classification problem.\n\nNow even if I try to overlook that, the \"approach\" also is written in an extremely unclear manner. So you use the InfoNCE loss betweeen mathcal{U} and mathcal{V}, but only u_i appeared in the entire equation in Definition 2. So where does V come into play? Also, if you already have classification supervision (or multi-label) as in all your experiments, why are you using infoNCE loss? Why don't you directly use a classification loss such as cross-entropy? SimCLR is justified because it doesn't have labels. But of course once you have labels you can just run supervised classification. The ABC approach is equivalent of running a prototype network (Snell et al. 2017) on a supervised dataset, which doesn't seem to be anything novel or hold any advantage over regular supervised learning.\n\nDue to my understanding that they are just running regular supervised learning, the baseline selections were extremely inappropriate. In a supervised setting, it is not fair to compare with an unsupervised VAE, instead, the obvious baseline should be any supervised classifier (e.g. trained VGG-16 or ResNet networks). Previous work on supervised disentanglement such as:\n\n[Zhang et al. 2014] Ning Zhang et al.. Panda: Pose aligned networks for deep attribute modeling. CVPR 2014\n[Liu et al., 2018] Yu Liu et al.. Exploring disentangled feature representation beyond face identification. CVPR 2018\n\ncould be compared instead of unsupervised baselines.\n\nIf one talks about the semi-supervised case, then there also exist work such as \n[Siddarth et al. 2017] N. Siddarth et al. Learning Disentangled Representations with Semi-Supervised Deep Generative Models. NeuRIPS 2017\n\nBasically, the experiment setup of the paper is not fair, and the correct baselines were not being compared. The authors need to go back to do more work to prove their approach.",
            "summary_of_the_review": "The paper is badly written, confused about their own setup and compared against wrong baselines that utilize different setups as their approach. It needs a lot more work to be considered for ICLR.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Based on the set supervision, the authors propose a novel approach, *approximate bijective correspondence (ABC)*, to isolate the active factors of variation by finding correspondences between different sets. \\\nBy leveraging complete knowledge of generative factors in the artificial Shapes3D dataset, they measure the information content of the learned representations and quantitatively define the factor isolation through mutual information measurements.\n\nThey further demonstrate the effectiveness of the proposed approach in two applications:\n* Fast Digit Style Isolation, where the proposed approach yields style-correlated embeddings orders of magnitude faster than the related approaches.\n* Pose Transfer from Synthetic to Real Images, where the training is done on a combination of set-supervised synthetic data and unsupervised real images.",
            "main_review": "### Strengths\n\n* The idea of building bijective correspondences between the set-supervised data with controlled factor of variations is interesting. This differs to the standard contrastive learning approaches by obtaining positive pairs through matching across sets rather than explicit augmentations.\n\n* The approach is clearly described and well presented, together with empirical results studied in three arenas. In particular, the evaluation on the synthetic Shapes3D dataset is comprehensive.\n\n### Weaknesses\n\n* **[Pose Estimation Results]** \\\nIn Table 1, the authors present the experimental results of training without pose annotations and compare with a few baseline methods. Though the proposed approach achieves the best performance in this setting, it's unclear to me why we don't use the pose annotations of synthetic images since we can get them for free by rendering 3D models. Regarding the effectiveness of synthetic-to-real pose transfer, the performance of the proposed approach is lagged behind [1] that also uses only synthetic images for training. \\\n\n* **[Missed Related Works]** \\\nIn terms of learning task-aware representations in contrastive learning, [2] discusses how to learn a disentangled embedding space using specific data augmentations and multi-head networks. Unlike the proposed method relying on set supervision, [2] assumes that no prior knowledge about the down-stream tasks is provided, and attempts to learn coherent representations for various tasks. \\\nBesides, a pose-aware contrastive learning approach has been introduced in [3], where a thorough discussion of the data augmentations and the contrastive losses is included.\n\n* **[Digit Isolation]** \\\nWhile the authors demonstrate that the proposed approach can achieve fast style isolation on MNIST, an additional experiment of fast content isolation could be very interesting to see. For example, maybe we can build different sets of digits with each set corresponding to a specific style, e.g., font, and apply ABC to find the same digits across different styles.\n\n\n------\n\n[1] Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views. (2015) ICCV\n\n[2] What Should Not Be Contrastive in Contrastive Learning. (2021) ICLR\n\n[3] PoseContrast: Class-Agnostic Object Viewpoint Estimation in the Wild with Pose-Aware Contrastive Learning. (2021) 3DV",
            "summary_of_the_review": "Based on the set-supervised data where we can control the variation factor within each set, a novel representation learning approach is proposed by finding bijective correspondences between different sets.\n\nThis approach differs to previous contrastive learning approaches in the sense that the positive pairs are obtained through matching images across sets rather than augmented from the same image. \\\nBy considering different sets of data, the authors demonstrate the effectiveness of the proposed method in three arenas and show better performances than several related methods.\n\nAlthough achieving convincing results on synthetic data, the empirical results on the real data remain to be improved (see weaknesses)",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}