{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper is about robust Graph Neural Networks (GNNs). It aims to develop a novel type of GNNs that are not vulnerable to structural attacks and noises. This paper firstly indicates the limitation of vanilla GNNs and claim that they are generally defined on the input fixed graph, which may restrict the representation capacity and be vulnerable to structural attacks and noises. To address the problem, this paper proposes a new robust GNNs called Latent Graph Convolutional Representation (LatGCR). Specifically, given an input graph, LatGCR aims to generate a flexible latent (clear) graph to boost robustness w.r.t. graph noises and attacks. Experiments on three datasets show that the proposed LatGCR achieves better performance on two downstream applications, i.e., semi-supervised node classification and unsupervised clustering.",
            "main_review": "Strengths:\n1. The proposed method is clearly explained. In the method section, the authors firstly review the vanilla GNNs and then introduce the proposed method, which enables better comparison and understanding. On top of that, the authors clearly point out the difference between their work with existing works.\n\n2. Many experiments have been conducted to verify the effectiveness of the proposed method. Specifically, the adopted baselines include both vanilla GNNs and SOTA robust GNNs. Besides, the authors conduct many experimental analyses, such as running time analysis and representation visualization. \n\n3. This paper is in good format and has a coherent description. The proposed LatGCR is a general GNN framework that may provide a general basic block for many GNNs.\n\n\n\nWeaknesses:\n1. The contribution of this work may be marginal. As the authors mentioned in Section 4, many existing robust GNNs have been proposed and the essence of this work with the previous works (e.g., GeC) is not large. \n\n2. More experiments are needed to support the claims in this paper. This paper claims that the proposed LatGCR can serve as a general block and be widely used in many GNNs. To verify this claim and the effectiveness of the proposed method, more datasets (especially larger datasets) are needed, such as the ogbn datasets. Secondly, the results in Table 1 may not well support the claims. Specifically, since the AGC can achieve comparable performances with the proposed LatGCR, the authors should run the experiments and report the performance variance to clearly show the superiority. Third, it is better to compare with other robust GNNs in Table 1, if available.\n\n3. Some experiments may be unnecessary or unsuitable. First, it may not be very meaningful to compare the running time on small datasets. Second, in Figure 6, i.e., representation visualization, it is necessary to compare with other robust GNNs, not only vanilla GNNs.",
            "summary_of_the_review": "This paper proposes a robust GNN called LatGCR that aims to generate a flexible latent (clear) graph to boost robustness w.r.t graph noises and attacks. This paper is in good format and has a coherent description. But more experiments are needed to verify the effectiveness of the proposed method.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work aims to improve GNN-based graph learning by using Latent Graph Convolutional Representation (LatGCR). Compared with conventional graph convolution, the proposed method update the adjacency matrix together with node representations in each step, which can mitigate the noise of graph structure to some extent. LatGCR can be easily implemented as a recurrent block and serves as the basic building block in various GNN architectures. In the experiments of this paper, authors combine LatGCR with GCN and verify its effectiveness on semi-supervised node classification and unsupervised clustering tasks.",
            "main_review": "Generally, I admire the core idea of this work, i.e. alternatively refining graph structure and updating node representations. Also, I agree that the proposed LatGCR could potentially be a simple yet effective module plugged into various GNN architectures. However, I have some concerns on model design and ablation studies:\n\n1. In the implementation of LatGC Neural Network, authors use the original adjacency matrix as the input of each LatGCR block instead of the adjacency matrix output from the last update. This design choice seems weird to me, since, in this way, the model cannot leverage the refinement done by last layer to perform consecutive structure refinement. Why is such an independent refinement scheme employed? Are there some ablation studies that verifies the superiority of such a design?\n\n2. It is announced that the proposed LatGCR module can be combined with various standard GNN architectures. However, in experiments, only the combination with GCN is evaluated on benchmark tasks. In order to extensively explore the value of the proposed module, the combination with more GNN architecture, e.g. GraphSAGE, GAT, GIN, etc., should be evaluated on at least one or two representative benchmarks. \n",
            "summary_of_the_review": "In general, I believe it is an important research problem to purify the noise in graph structure, and I admire the exploration done by this work on that direction. However, from the perspective of finishing a comprehensive and convincing research project, I think the current version lags in ablation studies and complete experimental verification. I am open to upgrade my rating according to authors’ response.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a graph learning module for graph neural networks. The authors assume that the fixed graph may not be optimal for representation learning. Thus, the authors suggest learning a new graph and using it to replace the original graph for subsequent learning. Experiments on noise data are conducted to show its effectiveness and efficiency.",
            "main_review": "Strengths:\n1. This paper is well-written and easy to follow. \n2. The experimental results are extensive. Both semi-classification and clustering tasks are tested.\n\n\nWeaknesses：\n1. The technical contribution is not enough. The proposed Eq. (3) is not novel. It is a simple extension of (2).\n2. The experimental evaluation is not convincing. First, some closely related works are not compared. For example, \"Iterative deep graph learning for graph neural networks: Better and robust node embeddings\". Second, the clustering performance of the proposed method is not STOA. There are several graph clustering papers that have reported much better performance. The authors should compare with some recent methods.\n",
            "summary_of_the_review": "Based on the technical contribution and experimental evaluation, I think this paper is below the bar of ICLR.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel graph structure optimization-base method, namely LatGCR, to learn robust graph representation. The proposed method uses a “structure reconstruction + feature reconstruction” condition to restrain the optimal graph structure. With such condition, the graph structure and the graph convolutional parameter are optimized in an alternative manner, which is integrated into a LatGCR block. The block can be further used to form a GNN for downstream tasks. Experiments on three classic datasets demonstrate the effectiveness of the proposed method. ",
            "main_review": "Strengths:\n- The proposed method is simply and effective to apply. Specifically, the proposed LatGCR block can be applied to diverse downstream tasks.\n- Sufficient experiments are conducted to compare the performance of the proposed method and the baselines.\n- Most parts of this paper are written in a logical and clear way.\n\nWeaknesses:\n- A major concern is that the novelty of the proposed method is minor. Using the idea of neighbourhood reconstruction to generate an optimal graph is not a very novel solution. Previous works like SLAPS [*1] use a similar solution to learn the graph structure. Besides, Pro-GNN [Jin 2020b] apply a feature smoothness loss to restrain the learned structure, which has a similar inherit idea with that. Minimizing the distance between the optimal structure and the original structure is also considered by the loss of Pro-GNN. Meanwhile, the alternative training of optimal structure and GNNs is a trivial technique in graph structure optimization, which is also considered by IDGL [*2]. \n\n- The authors do not perform a thorough literature review about the domain of graph structure learning which aims to optimize the graph structure with GNNs. Although there are some recent works are reviewed in this paper (e.g., TO-GCN, Pro-GNN and GLCN), the majority of works in this domain are not mentioned in this paper. Missing these new but significant works leads to inappropriate claims of the contribution of this paper. For example, in the last paragraph of Section 4 (Comparison with related works), the authors said that other works use a joint loss function to train, while LatGCN learns with a block to learn without extra loss function. However, there exist works, such as GRCN [*3] and LDS [*4], that also use a single cross-entropy loss to learn the graph structure. The authors claim that LatGCR is derived based on reconstruction. As I mentioned before, SLAPS [*1] also uses similar technique to optimize graph structure. To sum up, the authors are recommended to reviews more works in this domain (related survey: [*5]).\n\n- The experiments do not show a significant performance gain by the proposed method. In Figure 4, there exist some scenarios where LatGCN has worse performance than the baselines. The authors should also consider more baselines about robustness graph learning with structure optimization (e.g., GUARDGNN [*6] and GIB [*7]) and large-scale datasets (e.g., ogb series) to better demonstrate the effectiveness of the proposed method.\n\n- More experiments are needed to analyse the property of the proposed method. For example, it is valuable to show what kind of connections are learned by the proposed method.\n\nComments and Problems:\n- The proposed method ensures the sparsity of the proposed method by letting ~A_{i,j} = 0 if A_{i,j}=0. Here is a question: if the graph structure is attacked by randomly dropping the original edges, how to revise these lose edges with the proposed method? Considering that dropping the edges is an effective way to attack the graph structure and LatGCN focuses on the robustness scenarios, it is valuable to deal with such problem.\n\n- In Eq. (2), how to produce the third part (argmin…) with the second part (1/d_i…)? Is them equal?\n\n- Could the authors please to provide the details of the deduction from the first equation in LGE-step to Eq. (5)? \n\n- In Figure 1 (B), the LGE block should be max{…,0} or max{…,\\epsilon}? The figure does not match the description in Eq. (7).\n\n- Eq. (7) indicates that most elements in the estimated adjacency matrix ~A is \\epsilon instead of 0. In this case, how to perform an efficient graph convolution that leverages the sparse property of adjacency matrix? \n\nRelated references:\n\n[*1] Fatemi, B., Asri, L. E., & Kazemi, S. M. (2021). SLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks. arXiv preprint arXiv:2102.05034.\n\n[*2] Chen, Y., Wu, L., & Zaki, M. (2020). Iterative deep graph learning for graph neural networks: Better and robust node embeddings. Advances in Neural Information Processing Systems, 33.\n\n[*3] Yu, D., Zhang, R., Jiang, Z., Wu, Y., & Yang, Y. (2020, September). Graph-revised convolutional network. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 378-393). Springer, Cham.\n\n[*4] Franceschi, L., Niepert, M., Pontil, M., & He, X. (2019, May). Learning discrete structures for graph neural networks. In International conference on machine learning (pp. 1972-1982). PMLR.\n\n[*5] Zhu, Y., Xu, W., Zhang, J., Liu, Q., Wu, S., & Wang, L. (2021). Deep Graph Structure Learning for Robust Representations: A Survey. arXiv preprint arXiv:2103.03036.\n\n[*6] Zhang, X., & Zitnik, M. (2020). GNNGuard: Defending Graph Neural Networks against Adversarial Attacks. Advances in Neural Information Processing Systems, 33.\n\n[*7] Wu, T., Ren, H., Li, P., & Leskovec, J. (2020). Graph Information Bottleneck. Advances in Neural Information Processing Systems, 33, 20437-20448.\n",
            "summary_of_the_review": "While the paper studies an important problem, the novelty looks minor. Literature review and experiments are not sufficient. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}