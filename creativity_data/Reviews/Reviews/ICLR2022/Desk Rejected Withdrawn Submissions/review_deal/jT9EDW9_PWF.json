{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper aims to address the challenges of using GNNs in disassortative graphs with the proposed GCN-SL model. Compared to the vanilla GCN, it builds a reconnected adjacency matrix based on graph learning, and also adds spectral clustering features as inputs, both based on the original node features. The experimental section illustrates the effect of these added components.",
            "main_review": "Strengths:\n1. The proposed model is easy to understand. \n2. The ablation study in the experiment demonstrates the effect of the proposed modifications to GCN. \n\nWeaknesses:\n1. Technical contribution:\n1.1 Reconnected adjacency matrix. \nThe process to learn the reconnected adjacency matrix is almost identical to the original IDGL paper (Zhu et al. Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings. 2020). Both construct an alternative adjacency matrix with weighted feature similarity and $\\epsilon$ thresholding. This becomes more of a concern since the paper does not have a dedicated related work section that contextualizes it with respect to graph learning literature. It is therefore unclear whether the proposed method adds technical contribution to the field. \n\nThe authors claim a “special preprocessing to X” guarantees “the construction of the similarity matrix M” and also solves the “over-fitting problem”, yet it is very confusing what this special preprocessing does exactly (randomly select a feature dimension and add 0.5?), and how it reduces over-fitting and improves the learned similarity matrix. \n\n1.2 Spectral clustering. While the authors spent efforts developing efficient methods to generate spectral clustering features, the entire process is equivalent to applying standard dimensionality reduction techniques to the high-dimensional node features. It looks more like a simple data preprocessing technique rather than technical contributions. \n\n1.3 Combination with GCN. The final model takes the input of the combination of the original node features and spectral clustering features, and then concatenate the aggregated embeddings via the original and reconnected adjacency matrix for prediction. Just as the authors have shown in the experiments, using both the original and reconnected adjacency matrix actually leads to poorer performance for assortative graphs. It would be nice if the authors can improve the proposed combination technique to adapt to graphs with different homophily. \n\n2. Empirical significance:\n2.1 The paper lacks comparison with other graph learning baselines (e.g., IDGL). \n2.2 The proposed model can only achieve better performance when the graph is disassortative, which limits its application to general node classification task when the homophily of the input graph is unknown. \n2.3 It would be nice if the authors can present comparative results under different experimental settings (e.g., train/valid/test ratios) to show that the current one is not cherry-picked.  \n2.4 The discussion on how the proposed method reduces over-fitting and over-smoothing is unclear. \n\n3. Quality of presentation:\n3.1 The writing of the paper needs significantly more work. There are frequent typos, grammar mistakes, and many confusing sentences. A few examples:\n•\t“… then make GCN-SL is capable of performing representation learning on both disassortative and assortative graphs” in the abstract. \n•\tIn the first paragraph of introduction, “GNNs have developed many artificial neural networks”. \n•\tIn the fourth paragraph introduction, the authors first listed Geom-GCN as “recent approaches to solve the above problem” (of disassortative graph learning), and then argued that Geom-GCN is often unsatisfactory “when the concerned datasets are disassortative graphs”. \n3.2 Figures 1, 3, and 4 are too blurry to even recognize the legends. Apart from better resolution, Figure 4 could also be improved by using more distinctive colors and line types for different methods. \n",
            "summary_of_the_review": "Based on the above-mentioned weaknesses in technical contribution, empirical significance and presentation quality, the paper needs much more work and is not ready to be published in ICLR.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "details_of_ethics_concerns": "In the introduction, the authors wrote “for example, most people tend to chat with people of the opposite gender in the dating website”. The sentence implicitly assumes heterosexuality and gender binary, which is not inclusive and raises concerns for discrimination and bias.",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes GCN-SL, which changes both edges and node features to capture long-range dependencies. In terms of edges, new re-connected edges are generated via similarity learning. New node features are constructed using the spectral clustering method. To improve the efficiency of the scheme, ESC and ESC-ANCH are also proposed. These special data preprocessing techniques help GCN-SL overcome the overfitting and capture long-range dependencies. Empirical studies are performed to show the performance gain on disassortative graphs.",
            "main_review": "**Strengths**\n\n(1) This paper addresses the limitation of long-range dependencies of graph neural networks (GNNs), which is one of the critical issues of most existing GNNs.\n\n(2) The implementation details are well explained. In particular, a pseudocode and hyper-parameters in the supplement give help to understand. \n\n**Weakness**\n\n(1) It is nicer if the paper would include a related work section. Even the related work section is included in the supplement, there are only explanations of GCN and spectral clustering. There are many GNNs to deal with a disassortative issue [1], [2].\n\n(2) The claim \"the nodes of the same class always possess similar features no matter whether the homophily of the concerned graph is high or low\" would confuse me. Does it mean that input node features are similar between nodes of the same class?\n\n(3) The novelty of the proposed method seems somewhat incremental. The process of constructing reconnected adjacency matrix is seemingly similar to the method in [3].\n\n(4) In Table 2, The addition of the reconnected graph $\\mathbf{A}_*$ often negatively affects the performance on homophily datasets. Why does it have a negative impact on the performance of homophilic graphs?\n\n*[1] Adaptive universal generalized pagerank graph neural network, Chien, Eli, et al., ICLR 2021.*\n\n*[2] Graph neural networks with heterophily, Zhu, Jiong, et al., AAAI 2021.*\n\n*[3] Iterative deep graph learning for graph neural networks: Better and robust node embeddings, Chen, Yu et al., NeurIPS 2020.*\n\n*[4] Non local Graph Neural Networks*",
            "summary_of_the_review": "Overall, I am leaning towards rejection. My major concern is about the novelty and contribution of reconnected graph. I will be happy to improve my score if concerns are addressed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "GNNs have shown promising results on semi-supervised classification tasks where a homophilous graph structure is available. Recent work aims at extending their reach to datasets where the available graph structure has a low degree of homophily (called disassortative in this work). This paper proposes a structure learning approach for the low homophily case, where, in addition to the original graph structure, a second structure is learned from data based on node similarities. Experiments have been provided on a range of small-sized datasets.\n\n",
            "main_review": "[Pro] Graph structure learning has recently gained attention and is becoming a mature research area. From what I have seen, previous work mostly studies structure learning under the assumption that either a graph structure is not available at all, or the available graph structure is noisy. This paper extends these studies to the cases where the provided graph structure is not homophilous which is a good contribution.\n\nThere are, however, several drawbacks as described below.\n\n[Novelty] One major concern is the novelty of the proposed architecture. Many of the proposed components have been already used in the graph structure learning literature. For example, the assumption of node feature similarity corresponding to label similarity is one of the key assumptions in SLAPS [1]. Building a graph using eq. (1) and \\eps-sparcification have been done in IDGL [2] . Using anchor points to reduce computations has also been done in IDGL [2] and several other works.\n\n[Baselines] While there are many existing graph structure learning approaches, the proposed approach has not been compared against any of them. There is even no discussion of the key differences between the current and the existing approaches. Therefore, it remains unclear what the advantage of the proposed approach is and why it should be used instead of, e.g., SLAPS, IDGL, LDS [3], and many other existing approaches.\n\n[Experimental analysis] I have several concerns regarding the experimental section. 1) If we take the numbers for \"GCN-SL without SC\" features from Figure 3 and compare them to the baselines in table 4, we can see that in many cases GCN-SL is outperformed by the baselines. This raises the question of whether the improved performance is only comping from the SC features. 2) In table 4, on the last 4 datasets where GCN-SL outperforms the other baselines, the standard deviation is so high that it raises the question of whether the improvements are statistically significant and whether a different split would have resulted in other baselines outperforming GCN-SL. 3) All the datasets are quite small in size raising concerns regarding the scalability of the approach. 4) It is claimed that GCN-SL is immune to over-fitting but I didn’t find a proper justification for that. Is this claim only based on the results from Fig. 4?\n\n[Readability] There are several grammatical errors in parts of the paper that make it hard to read and understand. For example, I couldn’t understand the paragraph on preprocessing. There are also several extreme claims with almost no justification. For example, it is mentioned that “the nodes of the same class always possess similar features” which is a very strong claim and the only justification for it is the reference to Zhu et al. (2020), but I don’t remember seeing such a claim in that reference. In fact, the setup in that reference is quite the opposite: “... where connected nodes may have different class labels and dissimilar features”.\n\n[1] SLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks\n[2] Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings\n[3] Learning discrete structures for graph neural networks",
            "summary_of_the_review": "Extending the reach of graph structure learning approaches to non-homophilous graphs is a good contribution. However, there are concerns regarding the novelty of the proposed architecture, lack of adequate comparison to existing work, inconclusiveness of the experimental analysis, and the readability of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}