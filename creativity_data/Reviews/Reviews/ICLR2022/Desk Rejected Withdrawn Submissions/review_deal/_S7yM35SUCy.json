{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The author proposes a new method for open set recognition (OSR) by extending the cross-entropy loss with beta proper composite loss. In addition, the author also proposes modifications to some of the previous losses for OSR  with beta loss versions. Finally, the author demonstrates the effectiveness of the proposed model in 3 OSR datasets.",
            "main_review": "Strengths:\n- The paper is well written.\n- The problem of open set recognition is interesting and has many applications.\n- The proposed technique is motivated by the proper loss theory. \n- The performance of the proposed model is competitive compared to the baselines.\n\nWeakness:\nI have a few comments, concerns, and questions regarding the paper:\n1) For most cases, the loss contains an integral operation, which is hard to compute. \n2) The objective also optimizes for the value of alpha and beta. Naively optimizes it will produce a meaningless loss function; hence regularization is needed. It's not clear what the resulting alpha and beta represent. The loss is also changing during the training as the value of alpha and beta change. It's a bit counterintuitive since the loss function is usually fixed in the standard training procedure.\n3) The author extends beta-type losses to previous OSR losses. But the paper does not explain clearly how to do that. The modification also lost the theoretical property of beta loss since it violates the assumption. \n4) The results show that CSI \\beta is the most performing one despite the fact that it violates the beta loss assumption. Detailed analysis on these results is suggested.\n5) The AUC is usually used in binary classification. In the experiment in the paper, does it corresponds with predicting if a sample belongs to inliner data or open set data? How about the original multi-class problem in the inliner data? Does the AUC capture the accuracy results on multi-class problems?",
            "summary_of_the_review": "Based on the issues and weaknesses above, I recommend a rejection.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a open set recognition (OSR) method by using the Beta loss, which generalizes cross-entropy loss. The majority of the open set recognition work explored this problem from the perspective of the model and used cross-entropy for the loss function, while this paper investigates changing the weight function of cross-entropy to improve OSR. The authors find that by easily deploying the Beta function (Buja et al., 2005) as the weight function (a concept in proper loss), the existing OSR algorithms can be advanced.",
            "main_review": "PROS:\n\n- The paper is well written, with clear and complete explanations.\n- The method is simple and easy to follow.\n- The results are competitive.\n\nCONS:\nThe novelty is incremental: Combining the Beta functions with scaled logistic links. It is not clear why this combination works, especially for the OSR problem. In other words, I think this paper is not well-motivated. I agree the loss function is a particularly important part of solving a learning problem, while it is not a good idea to change some different existing functions to see if it works for the target problem. Some evidence should be given from a theoretical point of view, or at least we need more detailed empirical understanding of the loss, for example, a more thorough analysis of hyperparameters. There is an understanding gap between the Beta loss and the goal of OSR. This paper has evidence to show better performance compared to others. But, however, to me personally, it doesn't meet a high standard, as the method is just a modification of what's already existing. \n",
            "summary_of_the_review": "The paper presents a simple approach and reproducible to OSR, but its motivation and effectiveness are not convincing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper addresses open-set recognition (OSR). It explores to generalize the cross entropy loss with a Beta loss. The motivation of this work is that, other than the cross entropy loss, other loss functions are seldom used to train deep learning models for OSR. The paper shows that the Beta loss produces consistent improvements over cross entropy loss for OSR, achieving the state-of-the-art accuracy.",
            "main_review": "The paper has good introduction and covers sufficient amount of OSR papers. However, it has several major weaknesses in terms of readability, motivation, and experiments. Below are some detailed comments.\n\nWhile the authors reasonably argue that cross-entropy might not place proper \"weight at the extreme ends of the P(Y|X) range\" for OSR, do authors have intuition what kind of weights should one place the weight? Should it be larger or smaller on the extreme cases? According to the paper, this is the motivation of the proposed Beta loss, but why the Beta loss can properly weight on the extreme cases? \n\nAs a reviewer, I am an expert of open-set recognition but do not have background in statistics. Frankly, I practiced my due diligence to read Section 3, but I cannot understand how this part serves OSR. To me, Section 3 is full of fancy terms and authors borrow too many results (e.g., concepts and theorem) from other papers without clearly explaining why they serve OSR.\n\nWhat do authors mean by \"posthoc grid search\"? Did the author really train networks using the Beta loss with different hyperparameters?\n\nIn the caption of Table 2, by \"posthoc sensitivity analysis\", what does \"posthoc\" mean? Did authors really train the models or not?\n\nThe authors use L2 norm of the feature representation to detect the open-set. This is atypical as others use the predicted class probability. Can authors explain why it works?\n\nThe authors mention in the paper that they \"present results as reported in Table 1 from Guo et al. (2021)\". But the authors use a DenseNet to train their models. Many of the listed methods in Guo et al. (2021) do not use the DenseNet backbone. Therefore, the experimental comparison to other methods is not fair and the conclusion is not reliable.\n\nThe authors have a misunderstanding of Open Set Recognition (OSR). First, the authors did not explain well the difference between OOD detection and OSR -- the authors think the difference is in the datasets used for the OOD / OSR study. However, the difference is in the task specified by OOD and OSR. Concretely, OOD is a part of OSR that cares about detecting the OOD example, but OSR further requires K-way classification if the testing example is from a known class. Second, while the paper positions itself as OSR, the experiments focus on only OOD detection because it does not report accuracy on the known classes. Typical metrics for OSR include macro F1-score, or Open-Set Classification Rate [R1].\n\n[R1] Reducing Network Agnostophobia, NeurIPS, 2018",
            "summary_of_the_review": "The paper has several major weaknesses. For example, (1) the readability of the paper is poor as it seems to use fancy terms on purpose, (2) its OSR position and experimental validation (that focuses on OOD detection using AUC) are not consistent, (3) the motivation is not strong as it simply emphasizes \"To our knowledge, none of the previous work has investigated changing the weight function of cross entropy to improve OSR.\" Therefore, I vote for rejecting this paper.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to enhance the cross-entropy loss by composing it with a beta loss. The authors claim that this procedure may be combined with an open set recognition (OSR) or out-of-distribution detection (ODD) baseline approaches to improving performance. They present some math and experiments to endorse their claims. The main strength of the paper is to propose what appears to be novel: Using the beta function to generalize the cross-entropy loss to increase OSR and ODD performance. We believe this is a decent novel research direction.",
            "main_review": "A brief comment not directly related to the paper: Open set recognition and out-of-distribution detection are essentially different names for the same task: we need to reject inference samples that do not represent an instance of a class used during training. Otherwise, the network needs to classify the inference sample. Unfortunately, we have those two names for historical reasons. Even worse, we have two different benchmarks that are constructed differently. While the OSR benchmark uses some classes of a dataset for train and tries to detect the others, the ODD benchmark use a different dataset to work as out-of-distribution.\n\nFirst, a limitation that needs to be better presented in the paper is the fact that the proposed approach produces classification accuracy drop of about 2% in some cases. This relevant information cannot be in the appendix. The authors should discuss this limitation openly.\n\nAnother weakness of the proposed approach is to have two hyperparameters (i.e., alpha and beta). Reducing those two to only one called lambda was a good idea. However, requiring hyperparameter tuning is particularly problematic when dealing with OSR and ODD, as we do not know in advance which out-of-distributions the system will deal with in the field. Therefore, when we tune hyperparameters in such cases, we usually make assumptions of the out-of-distributions and consequently overestimate our results. This is even worse when the distribution used to validate the hyperparameters is similar to those used for performance evaluation.\n\nFor example, Table 3 includes datasets similar to the CIFAR10, which was used to tune hyperparameters. This fact may be overestimating the results of the proposed approach because the hyperparameters tuning procedure somehow had contact with classes that are now considered out-of-distribution. We suggest performing hyperparameter tuning using an entirely different dataset not related to the ones used to evaluate the performance. If you wish to reuse the dataset to validate hyperparameters and to perform evaluation, some data should be removed from the training set, implicating an even higher classification accuracy drop.\n\nTable 3 should mention the model used. Moreover, more than one model should be considered for each dataset. The authors also need to show the standard deviation of the results in the main paper. Indeed, we usually observe very tiny gains (1% or less) that may not be truly significant from a statistical point of view. In Table 4, using the beta enhanced loss is not better than search for the best baseline approach.\n\nUnfortunately, despite proposing to enhance the cross-entropy loss to improve OSR and ODD performance, the paper does not mention that we already have two losses specially designed to improve ODD: the IsoMax loss and the IsoMax+ loss. These losses are seamless because they do neither produce classification accuracy drop nor require hyperparameter tuning.\n\nFor example, this sentence _\"The majority of recent deep learning approaches to open set recognition use a cross-entropy loss to train their networks. Surprisingly, other loss functions are seldom used.\"_ is a mistake, and it needs to be removed or reformulated. Again: _\"Almost all of the recent deep learning OSR methods use a cross-entropy loss combined with a function (e.g., softmax or sigmoid) to map unnormalized logits to probabilities. Surprisingly, other loss functions have not been extensively explored for OSR despite known issues with the use of cross-entropy on representation learning for OSR\"_ is also wrong. More one: _\"To our knowledge, none of the previous work has investigated changing the weight function of cross-entropy to improve OSR.\"_\n\nUsing the SoftMax loss (i.e., the combination of cross-entropy loss, the SoftMax activation, and the last linear layer) constitute a truly easy baseline to be compared against. Therefore, we recommend incorporating the IsoMax loss variants as an additional baseline to be improved by the proposed method in a revised version of the paper. The cross-entropy term (i.e., the log function) is a component of the mentioned losses. Hence, it would be pretty simple to enhance both IsoMax variants with the proposed approach. After that, the authors should evaluate if the overall solution produces some classification accuracy drop. Moreover, it needs to be mentioned that hyperparameter tuning was required.\n\nMinor: The first and second paragraphs of section 4.1 and Figure 1 somehow wrongly make us believe that the proposed loss is connected to a given architecture to work, rather them independent of it. We suggest rethinking those parts of the paper.\n\nMinor: We also do not like the one-vs-all scheme because this may not scale well when dealing with too many classes datasets.\n\nMinor: Table 2 shows that we really need to tune the lambda for each baseline approach we plan to combine with the proposed method with. This requires training the network many times to find a good lambda for a given baseline approach.\n\n[1] IsoMax loss (first preprint, 2019): https://arxiv.org/abs/1908.05569v1\n\n[2] IsoMax loss: https://arxiv.org/abs/1908.05569\n\n[3] IsoMax loss (journal version): https://arxiv.org/abs/2006.04005\n\n[4] IsoMax+ loss: https://arxiv.org/abs/2105.14399\n\nDavid Macêdo",
            "summary_of_the_review": "The initial idea of improving the cross-entropy loss using beta functions appears to be original. However, we consider that the current version of the paper currently has many drawbacks (e.g., it omits related works, problems with the experimental procedures, the significance of the gains obtained (mainly considering that classification accuracy drop is observed in some cases)).\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}