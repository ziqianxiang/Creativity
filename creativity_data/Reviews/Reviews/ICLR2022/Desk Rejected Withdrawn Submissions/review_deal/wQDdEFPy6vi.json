{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper considers the issue of non-iid data in a federated learning (FL) framework.   The authors first carry out an empirical study of state-of-the-art methods that deal with data heterogeneity.  They perform thorough experiments on CIFAR-100, implementing 6 methods.  They compare accuracy and in an attempt to identify the source of good performance, also track metrics such as the top Hessian eigenvalue and other Hessian metrics.  Based on the information they gather, they modify the top-performing method, GradAug, a regularization approach in a distillation-based framework.  The authors then propose FedAlign, a similar method where they modify the training loss to include a mean-squared error between the  approximated Lipschitz constant vectors for the reduced width and full width final block.",
            "main_review": "The paper carries out a thorough performance evaluation of current methods for mitigating impacts of non-iid data in a federated learning framework.  \n\nThe manner in which the experimental results are reported is quite pedagogic and helps the reader see which components of the various frameworks have most impact on accuracy when there is varying data heterogeneity across clients.  This leads to the authors’ proposed method and it’s straightforward to see how the authors arrive at their method, and thus reasoning for it.\n\nOne of the motivations for this study is the amount of memory and compute power necessary for the well-performing state-of-the-art methods.  Results are only shown for clock time.  For completeness the authors should show memory and compute configurations as well.\n\nThe authors mention that GradAug, FedAlign’s competing method, is primarily designed for vision data, stating that their’s isn’t specific to vision data and could be applied to other data.  There are however no experiments to show that their method might be better or more applicable than the other methods.  The experiments shown are all for vision data.  It’s unclear why such a claim was made.\n\nThe improvement in performance due to the proposed method is not spectacular, and GradAug performance better in all classes.  How significant is the reduction in clock time with respect to GradAug?  What about other aspects like memory and overall compute configuration?\n\nThe authors briefly mention the privacy-preserving nature of FL in the abstract and introduction, but never address it in the main paper and it is never an element of comparison between the various methods. \n",
            "summary_of_the_review": "I appreciated the thorough experiments and instructive nature of the empirical study.  However, there is no accuracy gain in the new proposed method, and any trade off with respect to memory and compute power is not sufficiently addressed and discussed.  \n\nFurthermore I believe there isn’t sufficient novelty for acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the data heterogeneity challenge of federated learning in terms of local learning generality. The authors tested different regularization methods to alleviate the problem. They found that GradAug is more effective than the other existing regularization methods, but the infra cost is high. They devised GradAug and proposed a new approach to achieve both accuracy and efficiency win.",
            "main_review": "Strengths\n- The paper is well written, easy to follow.\n- This paper has a comprehensive empirical study of the state-of-the-art methods.\n\nWeakness\n- Novelty: the proposed method is built on GradAug, I don't see much novelty from it.\n- Experiments are only limited to vision tasks\n  - Not sure if the FL setting changes, how the simulation will be. \n  - if more comments for how the proposed method can work for other tasks can be provided, it will be better.\n",
            "summary_of_the_review": "This paper tackles an important problem -  improve local learning generality in federated learning. To alleviate the problem, this paper did comprehensive experiments to see how existing regularization methods works in vision FL setting. The experiment results provide the strengths and weakness of different methods, which then motivates the authors to design a new approach to achieve both accuracy and efficiency win.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents a method to tackle data heterogeneity in federated learning (FL) from the perspective of generalization. The main idea is to use a regularization approach to improve generalization, which is empirically found to increase the model accuracy compared to several standard FL techniques. It is also found that GradAug provides the best accuracy compared to the other regularization approaches that were evaluated. Inspired by this, the paper proposes FedAlign which is an enhanced version of GradAug that gives a better resource efficiency. Further experiments show that FedAlign gives (a relatively marginal) performance compared to baseline FL techniques.",
            "main_review": "**Novelty:** It is not surprising that the generalization capability of local models in FL has an effect on the overall accuracy. Especially when generalization is associated with the Hessian and the \"smoothness\" of the loss landscape, it is quite straightforward that it is easier to merge models on smooth loss landscapes compared to those on \"bumpy\" landscapes. On the technical front, the proposed FedAlign seems to be largely inspired by GradAug. It only has some minor tweaks over GradAug so the novelty seems limited.\n\n**Relevance to FL & presentation issues:** The paper considers generalization in an FL setting, but the main new contribution seems to be improving GradAug and the connection to FL is weak. FL algorithms can generally support a wide variety of model architectures and loss functions (including regularization terms). It is not clear what is the specific challenge in incorporating GradAug or FedAlign in an FL framework. While it is true that efficiency is an issue in FL, it is an issue in general machine learning problems as well. So the goal of improving the efficiency of GradAug would also exist in a non-FL setting. More importantly, the presentation in this paper makes it very difficult to understand what are the specific FL-related steps. For example, Eq. (2) gives a loss function. Is it a local or global loss function? What information does the server and clients need to exchange in both GradAug and FedAlign? It would be really helpful if a pseudocode can be given that includes the steps of GradAug/FedAlign and information exchange between clients and the server.\n\n**Insufficient experiments:** As an empirical paper, I would expect a more thorough set of experiments. In this regard, I believe that SCAFFOLD and FedNova are closely relevant benchmarks that need to be compared against. The reason is that SCAFFOLD and FedNova attempt to compensate the client drifts, while the current benchmarks do not seem to specifically aim at compensating the drift, although their regularization techniques may alleviate the effect of data heterogeneity to a certain extent. Related to the experiments, I would also expect to see more complete results for the current experiments. For example, Table 7 should include both accuracy and time (or complexity in terms of FLOP counts) measurements. If time (or FLOP counts) is the primary concern, it would also make sense to fix one variable and measure the other, e.g., what is the time or FLOP counts required for reaching a certain target accuracy by the different methods, or what is the accuracy reached within the same amount of time or FLOP counts by the different methods. In addition, multiple experiments with different random seeds should be run and both mean and standard deviation numbers should be reported, to see whether the performance difference is statistically significant.",
            "summary_of_the_review": "The paper appears to have limited novelty, unclear presentation particularly in terms of relation to FL, and insufficient experiments. Therefore, I feel that it is not ready for publication.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}