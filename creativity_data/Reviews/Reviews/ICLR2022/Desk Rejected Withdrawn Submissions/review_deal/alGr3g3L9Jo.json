{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a new loss function that prevents class collapse in supervised contrastive learning, with light touch on theory, and supported by some numerical experiments.",
            "main_review": "Strength: \n1. A new loss function is proposed\n2. The proposed method performed well in numerical experiments\n\nWeakness:\n1. Section 3.1, the theoretical motivation is not clear to me. If the authors want to motivate a new loss function to prevent class collapse, they should show that minimizing the original loss function L_{SC} [Khosla et al, 2020] would lead to class collapse. However, in Section 3.1, the author shows that minimizing the generalization error leads to class collapse. This somehow suggests that class collapse is a blessing, as it minimizes the generalization error (the error we care about (arguably) the most)\n2. On top of page 4, p.hat(y | f(x)) is not defined.\n3. In the paragraph before Theorem 1, how the positive and negative samples are generated is unclear to me. At the end of page 4, it's said that \"positive distribution consists of augmentations and the negative distribution consists of i.i.d samples from the same class\". Does the formula in the paragraph before Theorem 1 generate the samples as claimed?\n4. Theorem 1 analyzes the asymptotic regime where n^+,n^-->infty by directly following the analysis in Wang and Isola (2020). Is it possible to generalize the result to the finite sample case?\n5. Lemmas 1 and 2(on page 6) gives an upper bound on the generalization error of \\hat f_1. The upper bound looks loose and doesn't even depend on the sample size N. Is it possible to refine this bound?",
            "summary_of_the_review": "A new loss function is proposed to prevent class collapse by adding a new term that encourages points within a class to be spread apart. The claim \"we introduce a theoretical framing to analyze this loss\" is a bit overselling. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors examine the use of contrastive loss based learning for capturing unlabeled data attributes/categories, termed \"strata\". The intention is to use their loss for downstream applications in which good generalization requires learning \"different\"/\"distant\" embeddings for strata despite belonging to same class, eg. coarse-to-fine transfer learning. \nTo achieve this goal, motivated by the class collapse behavior of supervised contrastive loss (L_SC), they propose L_spread. L_spread is a new loss formed as a convex combination of L_attract (a slight modification of L_SC) and L_repel (a class-conditional Info-NCE loss) aiming to better separate embeddings of different strata. Intuitive theoretical intuitions are given for why L_spread might succeed at fulfilling its purpose and its performance is evaluated empirically on three downstream applications: coarse-to-fine transfer learning, unsupervised unknown strata recovery, coreset construction.",
            "main_review": "Strengths:\n-----------------------\n1) The problem of guaranteeing good balanced / worst-group accuracy for data subcategories (unknown during training) beyond classes is an important one for applications and one that is gaining traction in the literature\n2) Contrastive-loss approaches have been recently shown to provide powerful alternatives to \"conventional\" supervised approaches such as CE minimization and have also shown improved performance in imbalanced classification tasks. Thus, it is rather natural to consider its use in the problem at hand.\n3) The performance of the proposed loss is evaluated on a range of three downstream applications\n\nWeaknesses:\n-----------------------\n1) I feel that the theoretical justifications given for L_spread are rather weak and in cases presented in a convoluted way that obscures the message. Here are two examples:\n    - It is unclear what the purpose of Theorem 1 is. To my understanding, it reads more as an unsuccessful attempt to say something conclusive about L_spread. As the authors acknowledge: the fact that the individual components (eg L_uniform, L_neg) have not compatible individual minima does not prevent the possibility of being able to characterize the global/joint minima. The authors continue by saying that even if one were able to do that, then it would not capture strata because strata are unknown to training time. And, this raises my concern: then what purpose does Theorem 1 serve? (Besides, sure the loss does not incorporate strata, but your data model does, so won't this affect the minimum?)\n     - I found the \"thought experiment\" of Section 4.1 confusing and inconclusive. Particularly so, the arguments made for case 2 top of page 6. Are these rigorous claims? And eventually what is their purpose? Is there a more concrete conclusion other than that \"the strata are separated based on the difficulty of the respective OOD problem\"?\n\n2) There seems to be lack of comparison to other state-of-the-art methods. For example in Fig. 5 L_spread is compared only against standard baselines L_CE and L_SC. The authors should compare their methods to Sohoni et al (2020) and Liu et al. (2021) (https://arxiv.org/abs/2107.09044)\n\n\nMinor points: \n* What is L_SS in Figure 4?\n* h(x) is defined in the appendix table, but not in the main body requiring the reader to guess to follow some of the statements\n* missing parenthesis: first sentence of Sec. 2, second to last sentence of Sec. 2.1\n* typo: Theorem 1: from->form\n* standard errors should be reported on the plots\n* Lemma 4 is essentially a corollary of Theorem 1 in Graf et al. (2021)\n* Lemma 1 L(x,z,f_1) is not generalization, but expected loss\n",
            "summary_of_the_review": "Overall, I found the theoretical justifications given for L_spread somewhat weak to support the claims and at places confusing. I suggest that the authors reconsider what needs to be included and the main conclusions from the theoretical analysis be made more transparent. Besides, while this would not be a concern if the conclusions of the theoretical analysis were satisfactory, the techniques themselves are not particularly innovative and mostly build on previous works, eg. Graf et al., Wang & Isola. \nWhen it comes to empirical evaluations, for a fair assessment of the quality of the proposed solution it should be compared to other sota methods such as those. in Sohoni et al (2020), Liu et al. (2021) etc.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper tackles the problem of _class collapse_ in supervised contrastive learning. The authors propose an additional class-conditional InfoNCE loss, which pull apart representations within each class. This method is shown to improve strata representations, which leads to improved performance in certain downstream tasks such as coarse-to-fine transfer learning, rare strata detection and minimal coreset construction.",
            "main_review": "Strengths:\n1. The $L_{spread}$ loss is simple both conceptually and to implement. It is also clearly motivated, as the InfoNCE loss is known to prevent collapse and to encourage entropy in representations.\n2. The paper is clearly written.\n3. The downstream tasks are cleverly chosen to show the advantage of the proposed algorithm, which is representations that are stratum-aware. Strata is defined as \"unlabeled categories such as breeds, poses or backgrounds\".\n    - coarse-to-fine transfer learning naturally benefits from representations that can cluster by finer categories.\n    - robustness against worst-group accuracy and noise: benefits from the fact that strata clustering helps identify outliers and rare strata.\n    - minimal coreset construction: by removing easy examples from large strata and retaining easy examples from small strata, the algorithm helps construct a balanced and efficient coreset.\n\nWeaknesses:\n1. **On novelty the proposed method**: $L_{spread}$ is essentially a combination of supervised contrastive loss and a within-class unsupervised contrastive loss. There is previous work on combining supervised and unsupervised contrastive losses and their effects on transfer learning [1]. Combination of alignment loss and repelling loss is also well known (InfoNCE as an example). It is still valuable to study the exact combination of supervised contrastive loss and within-class contrastive loss, but I think its novelty is medium-level.\n\n2. **On surprisal of the proposed method**: The method itself and its quality of \"strata clustering\" are not very surprising. Contrastive methods such as SimCLR etc has shown the capability of clustering representations by categories, and I don't think there are fundamental differences between category-clustering and sub-category (i.e. strata) clustering. \n\n3. **On significance of the proposed method**: I think the biggest significance of this work comes from the downstream tasks picked to show when this method is most beneficial. The method would be much more significant if it performs better on the vanilla image classification task or the more common transfer learning baselines (e.g. [2] or [3]). Therefore I think its significance is also medium-level.\n\n5. Potential improvements:\n    - I think the paper lacks some training details of the experiments for a fair comparison, e.g. number of training epochs/steps. In Figure 4, it may not be a fair comparison if self-supervised models are trained for the same epochs as the supervised models, because the former is known to need to train longer. Please add more training details, including optimization details, to Appendix E.\n    - Visualization of strata could be interesting. Figure 2 gives an example of t-SNE plot of embeddings and 2 example images. It would be interesting to see more example images that are clustered differently using SelfCon vs $L_{spread}$.\n    - Some expressions are never defined, e.g. $L_{SS}$ is used in Figure 4 but is never defined. Reading through the paper it is not hard to infer its meaning to be \"self-supervised\", but for readers who jump to figures it can be confusing.\n\n[1] https://arxiv.org/abs/2103.13517\n\n[2] https://arxiv.org/abs/1805.08974\n\n[3] https://arxiv.org/abs/1912.11370",
            "summary_of_the_review": "This work is well written with theoretical support and well-designed experiments. My main concern is the novelty of the idea, and significance of results, both explained above. For this reason I feel it is marginally below the threshold.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I do not have concerns relating to ethics on this work.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new loss that is the sum of two existing losses, with some tweaks. The first loss is a variant of neighborhood component analysis (Goldberger et al, 2004) known as “SupCon,” which introduces L2 normalization and a temperature. The second loss, a variant of InfoNCE, encourages points within a class to be “spread apart.” The overall loss is taken as a convex combination of the two losses by introducing a weighting hyperparameter. The rationale for the proposed loss is to combat feature “collapse,” where all embeddings for examples from the same class map to the same point. \nThe loss is evaluated indirectly by considering “end model” performance after fine-tuning the learned representations for a target classification task, where it is found to improve generalization performance in most cases.",
            "main_review": "I found the discussion of “strata” to be too imprecise to be useful.  A more general way to think about avoiding feature collapse without making generative assumptions is enforcing some minimum amount of input sensitivity in the resulting vector space, i.e. bi-Lipschitz constraints. Several methods have been proposed for this purpose, e.g. spectral normalization, and it would have been nice to see this connection made in the paper, and better yet to see an evaluation of these methods compared to the proposed loss.\n\nAnother missing connection is to variational information bottleneck methods (“VIB”) and their metric learning counterparts, “hedged instance embeddings.” Here, the amount of information encoded in the embeddings is explicit in the KL term of the loss, where the Lagrange multiplier term \\beta might serve a similar role as the \\alpha weighting term introduced in this paper. This gives a probabilistic framework with which to think about feature collapse, as well as practical training methods to mitigate it.\n\nFigure 1 is unconvincing because without knowing the true strata, lower average cosine similarity may or may not be a good thing. This presumably depends on the irreducible uncertainty associated with the images, which is unobserved and varies from class to class. That being said, one potential advantage to thinking in terms of latent partitions (“strata”) is to conduct direct evaluations to see if the embeddings correspond meaningfully with *observed* strata when using the proposed loss. For this purpose, it is not clear why datasets with a known class hierarchy, such as CIFAR100, were not used.\n\nTherefore, the benefits of the proposed loss may be entirely due to having a helpful regularization effect, rather than producing embeddings that are in any way aligned with latent strata. To account for this possible confound, it would be helpful to present results with baselines where other kinds of regularization are introduced, such as the spectral normalization mentioned above, or even simpler approaches such as L2 regularization, dropout, or early stopping.",
            "summary_of_the_review": "Two main concerns: (1) the motivation is disconnected from the approach and (2) the evaluation doesn't consider appropriate baselines. Other analytical frameworks (input sensitivity; information bottleneck) seem more aligned with the proposed method, yet are not discussed or evaluated.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}