{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work studies the impact of aggregation function on the performance of graph neural networks. The authors test the final performance on large-scale graph datasets from the Open Graph Benchmark (OGB) and propose the Generalized Aggregation Function which is differentiable. An end-to-end learning framework is built on generalized aggregation. Experimental results show their method can achieve state-of-the-art results.",
            "main_review": "Strengths:\n\n(1) This work proposes a generalized aggregation function, which covers commonly used functions.\n\n(2) The generalized function is differentiable and the authors propose an end-to-end learning framework.\n\nWeaknesses:\n\n(1) The novelty of techniques used in this work is somewhat trivial. Even though the proposed aggregation function is permutation invariant, I did see what is the benefit of this property.\n\n(2) It seems that the reason that Learning Dynamic Aggregators perform much better than searching hyper-parameters is not so clear. \n\n(3) In Table 3, the backbone used for the different datasets is different. I hope the authors could discuss the reason for the choice.",
            "summary_of_the_review": "This work is overall clearly written, and the experiments are adequate. However, the novelty of techniques is somewhat incremental, and in-depth analysis is needed to show the advantage of their methods. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose new and, in particular, parameterized aggregation functions for GNNs in order to especially support the construction of deeper GNNs. The rather extensive evaluation provides some \n\n",
            "main_review": "(-)\n\nI am somewhat stunned by this submission. I have reviewed a previous version of the paper, which - as far as I can see - does not differ much from the new one. In particular, it does not seem as if the authors incorporated many of the reviewer comments at all. In particular:\n\n- Approach: The authors define several parameterized aggregation functions but the motivation is left unclear. Indeed, it seems that the main motivation for splitting the definitions in this way seems to be to generalize as many of the existing functions as possible. The paper is missing an explanation why these different aggregation functions are supposed to specifically support deeper GNNs.\n\n- The authors now included experiments with reduced model capacity (for fair comparisons), but only compare to very basic GNNs.\n\n- The main table contains very many similar results but is missing standard deviation. This is especially strange given that the OGB encourages to use several folds.\n\n- Table 2 is supposed to show the comparison with SOTA, but that is lacking. The initial OGB leaderboard contains only the most basic GNNs. However, when proposing an approach for creating deeper GNNs, the paper has to compare to this kind of models too. The row in which the authors compare to GCNII, JKNet, DAGNN, etc. actually seems to show that these models are as good/better. \n\n- One very closely related work is not cited/included at all, although it is part of the leaderboard:\n\nLi et al. Training Graph Neural Networks with 1000 Layers, ICML 2021\n\n\n(+) The idea introduced in the paper is interesting and simple. In particular, the dynamic aggregation considered in Section 4.2 seems really interesting to me.\n",
            "summary_of_the_review": "Overall, the paper is fairly understandable and the \"deeper GNN\" topic has gained more attention recently. However, the paper is missing the theoretical justification of its proposals and/or a thorough comparison to the state of the art. Altogether, I therefore suggest to reject the paper. I am open to adjusting my score in case I missed critical aspects of the contribution, or if the authors provide more comparison to relevant SOTA models.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a generalized message aggregation function to train deep GNNs. This generalized aggregation function covers regular mean, max and sum aggregation functions. Experiments on several OGB benchmarks demonstrated positive results compared to other GNN algorithms.",
            "main_review": "Strength: \n\n(1) The paper is well written and easy to follow. (2) Experiments are conducted on 7 OGB benchmarks for evaluation. \n\nWeakness: \n\n(1) The main claim of the paper is questionable. There are actually two main modifications in the proposed algorithm “DeeperGCN”: (a) generalized aggregation function, and (b) skip connection, while the latter is the main contribution from DeepGCN (Li et al. 2019). However, the authors claim that generalized aggregation function leads to improved performance.\n\n(2) The proposed generalized aggregation function is a simplified version of attention-based aggregation of GAT. So it’s novelty is very limited. The observed gains (btw very small in most of cases) are likely due to skip connections proposed in DeepGCN.\n\n(3) The experimental comparison can be improved. Since the paper proposes “DeeperGCN”, it should compare rigorously with DeepGCN, while the paper mainly compared with other GNN algorithms. In addition, as generalized aggregation function is a simplified version of attention-based aggregation, another important baseline should be GAT (while the paper only provides one data point).\n\n(4) The name “DeeperGCN” is kind of misleading. Since a generalized aggregation function is proposed for the message passing based GNN algorithms, it has no reason to tie it to GCN. DeeperGNN may be a more suitable name.\n",
            "summary_of_the_review": "The main claim of the paper should be justified rigorously by comparing DeeperGCN with DeepGCN and GAT. Clearly, the current experiments can’t support the main claim of the paper.\n\nThe novelty of the paper is also limited. It’s a simplified GAT attention-based aggregation.\n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors study the aggregation functions in graph neural networks. The authors point out that the commonly used aggregation functions, such as mean, max or sum, are suboptimal. To overcome the issue, they propose the Generalized Aggregation Functions, which can unify the aforementioned aggregation functions. These functions are differentiable and trainable. The authors have also conducted numerous experiments on large-scale datasets to show the effectiveness of the proposed method.",
            "main_review": "1. Questions about the main contributions:\n(1) The title points out the conditions required to train a deeper GCN model, but what is the relationship between the proposed aggregation functions and the depth of the model? Does the method proposed in this paper help to deepen the GCN model? Why is it helpful to deepen the GCN model?\n(2) \"ResGCN+\" is not a contribution of this work. So why does \"ResGCN+\" improve performance?\n(3) Figure 2 does not reflect the main contribution of this paper.\n\n2. Questions about the method:\n(1) The message feature needs to be kept positive, will this reduce the generalization ability?\n(2) What are the advantages and differences of \"PowerMeanSum\" and \"SoftMaxSum\"? Can we unify them?\n(3) As shown in Figure 1, whether there exists an intersecting relationship between \"PowerMeanSum\" and \"SoftMaxSum\" (except the four black points)? Please give some proof?\n\n3. Questions about the experiments:\n(1) As shown in Figure 1, \"SoftMaxSum\" contains a larger search space than \"SoftMax\", but why the performance (shown in Table 1) is not good as \"SoftMax\".\n(2) This work mainly adopts GCN as backbone. Can the proposed aggregation functions improve the performance on other GNN architectures, such as GIN or GAT?\n(3) What about the sensitivity of \"DyResGEN\" to initialization?  Why the hyper-parameters in \"DyResGEN\" is initialized to 1 and 0.5 ?  what about the random initialization?",
            "summary_of_the_review": "In general, the paper is written well and easy to follow. However, there exist aforementioned concerns that the authors need to address.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}