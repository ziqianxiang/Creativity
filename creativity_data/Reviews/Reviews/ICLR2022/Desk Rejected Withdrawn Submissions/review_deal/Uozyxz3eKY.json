{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper investigates the data and model points of view for consistency training together focusing on supervised learning rather than semi-supervised learning. A wide range of application areas ranging from image classification to NLP to neural machine translation are considered for experiments. \n\n",
            "main_review": "Consistency training and its variant have been producing state-of-the-art results in semi-supervised learning over the last few years. Consistency training is in general implemented by penalizing differences between outputs (or representations) produced by a neural network for different augmentations of a sample, e.g. rotations for an image. This is data point of view for consistency training. However, the model point of view has also been investigated where consistency is enforced for the output of perturbations of a model, e.g. dropout, for the same sample point. This paper investigates the data and model points of view together focusing on supervised learning rather than semi-supervised learning. A wide range of application areas ranging from image classification to NLP to neural machine translation are considered for experiments. These experiments confirmed the combined benefit of using the data and model points of view together for consistency training as expected. The wide range of tasks considered is the strongest point of the paper. In fact, Sajjadi et al, 2016 (reference contained in the paper) appears to have used the data and model consistency together previously.  \n",
            "summary_of_the_review": "The straightforward combination of two previously existing ideas is not sufficiently novel in its own and appears to have been proposed before. Furthermore, the experiments only seem to serve the purpose of confirming an expectation rather than producing new knowledge. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies how to add consistency regularization into the network to improve the robust and better generalization ability. The key idea is to combine the widely adopted data-level consistency and model-level consistency (i.e., dropout). The authors conduct experiments on different datasets (like neural machine translation, natural language understand, and  image classification).",
            "main_review": "Strengths:\n1. This paper is easy to follow and conduct experiments on different datasets on different domains. \n\nWeaknesses:\n1. The technical novelty is limited, as combing the widely adopted data-level and model-level consistency is trivial. Moreover, this paper is based on the recent model-level consistency method R-Drop (Liang et al., 2021) and the improvement compared with this paper is incremental. \n2. Although the authors conduct experiments on different tasks, while the ablation study of different level consistency is only conducted on two datasets.\n3. Overall, the contribution (including technical novelty and empirical novelty) may not meet the standard of ICLR.",
            "summary_of_the_review": "This is a good empirical paper and studies a simple yet important problem about consistency training, while the contribution is limited from the technical, theoretical, and empirical aspects.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a unified framework (DM-CT) for consistency training, incorporating data-level consistency training via augmentation and model-level consistency training via model variance for improved robustness and generalizability in supervised deep learning models. DM-CT is validated on neural machine translation, natural language understanding, and image classification tasks. The authors also perform a comprehensive ablation study showing the impacts of different variants in data- and model-level consistency training.   ",
            "main_review": "PROS\n1. Exhaustive experiments across the tasks in NLP and vision showing consistent improvements with the proposed DM-CT over the baselines.\n2. Well-studied literature review covering regularization methods, and separate data-level and model-level consistency training.\n3. Good ablation experiments in supporting the optimal design or hyper-parameter choices.\n\nCONS\n1. The paper lacks novelty in terms of technical contribution. I don't see how it adds new knowledge except incorporating data-level and model-level consistency which some models could already do (data-level consistency with model randomness e.g., dropout). Furthermore, it is not experimentally justified if the proposed DM-CT improves model generalizability. \n2. It is claimed that DM-CT achieves significant performance improvement although the authors do not present any statistical evidence to support it. \n3. Only accuracy is not a true performance indicator, AUC or F1 scores could be reported. In most cases, the DM-CT achieves marginal improvement compared to the baseline methods. It is not clear if the models were run just once or multiple times. If the latter is true, the authors should report the standard deviation scores. \n4. Figure 1 doesn't help much as it is not illustrative enough. It should show how to obtain x' from x, \\tilde{P} from P_{s1} and P_{s2}, etc. The curly braces look like just copying x as x and x', and so on. Also, in the caption \"x and x_i are the original data and the noised data respectively\" should be x and x'.\n5. The paper presents experiments on fully-supervised learning. To me, it would be more practical to experiment with supervised learning from substantially reduced train data (only labeled) and evaluation on the same test set if at least competitive against the fully-supervised baselines without consistency training.  \n6. Two predictions for each data sample are averaged to obtain the output prediction, what's the intuition behind that? Will it work the same with e.g., geometric mean?\n7. How would the overhead training and testing time despite the double batched data passing effort justify the marginal performance gain? Is there any explanation why DM-CT's improvement is so low when applied to a pre-trained model vs a model from scratch?\n8.  None of the tables includes the units/scale of the reported metrics. Some of the scores are incorrectly bolded. Table captions could be more explanatory. \n9. The writing of the paper needs to be improved. Words like \"of\", \"to\", etc. are missing in several sentences.\n10. Fig. 3 is referred to in the texts in 4.2 while the description suggests Fig. 2 instead.\n11. It is not clear which model was used for the comparison of augmentation in Table 7. From the scores, it seems Resnet110, but it should be clarified in 4.1.",
            "summary_of_the_review": "I appreciate the effort of presenting exhaustive experimentation including the ablation study covering the different aspects in data-level and model-level consistency training. However, the presentation of the paper is not up to the mark, a lot of things need to be clarified. Additionally, the paper lacks enough technical novelty and claims some things (e.g., significant improvement, generalization) without supportive evidence, and the results are not compelling either. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a unified framework that integrates both data-level and model-level consistency training to improve the robustness and generalization of the trained models. Consistent model predictions for augmented data is used for data-level consistency training and matching output distribution of different sub-models is used for model-level consistency training. A comprehensive set of experiments including different learning tasks showcases performance improvements.",
            "main_review": "Strengths:\n\n- The paper presents an integrated perspective for consistency training that leverages both model and data to regularize training toward a better generalization and robustness.\n- The proposed method can be extended to a semi-supervised learning setting.\n- Effectiveness of the proposed framework is evaluated on different learning tasks.\n- A comprehensive evaluation including ablation experiments and assessing different augmentation methods.\n\nWeaknesses:\n\n- The novelty of the paper is limited to integrating existing ideas for consistent training in the same framework.\n- No discussion/justification on how the inference gap is reduced with forcing consistency on sub-models predictions.\n- Experimentation lacks assessing the impact of out-of-distribution and domain shift cases on the generalization of the models.\n- How does the proposed framework perform when training networks from scratch (i.e., not using pre-trained ones), specifically for image classification? \n\nTypos:\n- Caption of figure 1: x_i should be x'",
            "summary_of_the_review": "The idea of integrating model- and data-level consistency training has merit, yet the paper's novelty is limited by borrowing existing methods in consistency training. The paper presents rigorous experimentation on different learning tasks, but the empirical results are only limited to supervised learning settings where labeled data is abundant. It is worth investigating the impact of consistency training on limited data scenarios and training networks from scratch.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}