{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Paper proposed a regularisation method for defence against adversarial attacks. paper presented some mathematical lemma to support the motivation for using Hamming distance regularisation.",
            "main_review": "\n\nIt is important to lay foundations for discussions by defining all math symbols. Undefined symbols can lead to confusion and misunderstandings. e.g. Eq(1) x is not defined. Eq(7) y is not defined. I presume x is the input and y is the label. But this must be made clear. What is the space of x must also be made clear. x \\in math{R}^d? I also cannot find the definition of \\bar{X} in lemma 5.2. please also check other undefined symbols in the whole paper.\n\n\nSection 5.1: why f_H lives in the hypercube {0,1}^N? If f_H is the activation patterns under relu, then f_H must take continuous values and not binary values. For this confusion, I cannot understand how to take hamming distance of continuous variables d_H(f_H(x), f_H(y)). You can see that symbols are mixed up. y was defined earlier as the labels. Now y is mixed with x.\n\n\nSection 5.3: this section is not written in a way that be clear and understandable. How does it relate to the rest of the paper.\n\n\nThe key equations for the computation is really Eq (17), Eq (18). Authors should explain why we need the second term in Eq(18).\n\n\nSection 6.2: it is very difficult to understand this section. I suggest define the symbols and the plots more clearly.\n\n\nSome of these key papers on regularisation should be cited:\nS. M. Moosavi-Dezfooli, A. Fawzi, J. Uesato, P. Frossard, Robustness via curvature regularization, and vice versa, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 9078–9086.\n\nA. Ross, F. Doshi-Velez, Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients, in: Proceedings ofthe AAAI Conference on Artificial Intelligence, Vol. 32, 2018.\n\nPaknezhad, M., Ngo, C. P., Winarto, A. A., Cheong, A., Yang, B. C., Jiayang, W., & Kuan, L. H. Explaining Adversarial Vulnerability with a Data Sparsity Hypothesis. arXiv 2021. arXiv preprint arXiv:2103.00778.\n\nT.  Miyato,  S.  I.  Maeda,  M.  Koyama,  S.  Ishii,  Virtual  adversarial  training:  a regularization method for supervised and semi-supervised learning, IEEE transactions on pattern analysis and machine intelligence 41 (8) (2018) 1979–1993.doi:10.1109/TPAMI.2018.2858821.",
            "summary_of_the_review": "explanation of the paper is not clear in several parts. the linkage between the theory and algorithm development should be made stronger.\n\nmore data set other than CIFAR10 should be use for benchmarking.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studied the robustness of deep neural networks from the perspective of training regularization. Motivated by the notion of $\\epsilon$-$\\delta$ stability, it proposed two regularization terms, i.e., L_S stability regularizer and L_H Hamming regularizer. The two regularizers are added to the original loss function to train deep neural networks jointly. The effectiveness of robustness improvement of the proposed regularizers is evaluated on MNIST and CIFAR10 datasets.",
            "main_review": "This paper proposes two efficient regularizers to improve model robustness without adversarial training. In comparison to adversarial training, this reduces much computational overhead. Besides, encouraging the stability of feature extractor $f(\\cdot)$ is intuitively reasonable. \n\nHowever, I have the following concerns:\n1. As mentioned in section 5.2, \"Note that a simple corollary is that if the set X forms a hypercube, it is both necessary and sufficient to check the assumptions of either lemma on the opposing vertices to apply their conclusions.\" The truth is that this assumption is difficult to satisfy and also very hard to verify for realistic datasets. \n2. On the experimental results (table 3), I am very confused about clean accuracies. Why does the stability regularizer degrade clean accuracy to 34.7%? This number seems strange. The stability regularization only encourages the l_2 distance between outputs of two perturbed versions of each sample. \n3. In terms of the main results in Table 1, the proposed method degrades the clean accuracy to 72.1% for CIFAR10. It is not acceptable in practice. Efficiency is an important criterion for evaluating defense methods, but the premise is that the performance on clean data is guaranteed.",
            "summary_of_the_review": "I will update this after the review period...",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a regularization method for improving adversarial robustness of neural networks using ReLU functions. The proposed regularization method penalizes the difference between the activations of ReLUs of randomly perturbed inputs and the difference of outputs.  Experiments demonstrate that the proposed method efficiently improves the robustness against broad types of attacks, e.g., norm constrained attacks, JPEG compression, and Recolor. ",
            "main_review": "# Strength\n- This paper may have a moderate impact because there are few techniques that have succeeded in improving adversarial robustness by using the regularization term to the best of my knowledge. \n- The proposed method is effective for broad classes of adversarial attacks. Since previous methods for adversarial robustness are generally specialized to specific attacks, this effectiveness is a major strength of the proposed method. \n- The motivation of the proposed method is clearly written: the proposed method is based on the observation of the stability of the piecewise linear functions. \n- The proposed method is very efficient because it does not compute adversarial examples.  Thanks to this efficiency, the proposed method might be able to be used in large datasets while the existing adversarial training cannot be used due to the large computation cost. \n# Weakness \n- It seems to be difficult to tune hyper-parameters of the proposed method. The proposed method has two hyper-parameters of regularization coefficients. In the experimental setting, they are linearly increased according to training proceedings as shown in appendix A.  Hyper-parameter tuning is generally difficult when it requires scheduling. For example, scheduling of learning rate is one of research topics.  Thus, I would like to see the evaluation of sensitivity of the proposed method to hyperparameters.\n- I think some related methods should be compared with the proposed method. \nOne of the strong points of the proposed method is efficiency. To improve the efficiency of adversarial training, several methods have been presented [1,2,3], and I would like to see the comparison with these methods.  \nIn addition, since proposed method uses the regularization term instead of the adversarial training, I would like to see results of previous regularization-based methods, e.g., Jacobian regularization (Jakubovitz and Giryes, 2018), but it might be not necessary if the previous regularization methods are known to be weak against strong attacks like PGD.\n- The proposed method might impose large constraints that limit the representation capacity of models: the proposed method enlarges the input space around each data point where the model behave as a linear function. In fact, the proposed method tends to sacrifice clean accuracies for the robustness compared with adversarial training. Evaluation on large datasets, e.g., ImageNet, will alleviate this concern.\n\n    [1] Wong, Eric, Leslie Rice, and J. Zico Kolter. \"Fast is better than free: Revisiting adversarial training.\" ICLR 2020\n    [2] Shafahi, Ali, et al. \"Adversarial training for free!.\" NeurIPS 2019.  \n    [3] Zhang, Dinghuai, et al. \"You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle.\" NeurIPS 2019\n",
            "summary_of_the_review": "- I like this paper since I think that the proposed method seems to be the first successful simple regularization method for adversarial robustness.\n- The proposed method has  impressive efficiency and the effectiveness  against broad classes of attacks.\n- Experimental evaluations might be not sufficient to show the practical effectiveness of the proposed method.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a regularization method to produce robust models.  The regularization method encourages the model to be stable by penalizing difference in network output as well as activation patterns for small perturbations around the input.  Unlike adversarial training, the perturbations are randomly sampled so the training time is much lower in comparison.  The authors demonstrate that this approach leads to robustness on unseen adversaries and improve on verfied robust accuracy over prior works.",
            "main_review": "Strengths:\n- Strong empirical results - the authors demonstrate that without using adversarial training their regularization technique can generate models that are robust to unseen adversaries.  Additionally, they show that their model can achieve good PGD and AutoAttack robust accuracy across datasets (CIFAR-10, MNIST).  They also demonstrate that their regularization can also improve verified robustness.\n- Design of regularization is well-motivated - The authors motivate their regularization by connecting it to stability and smooth decision boundaries.  Additionally, design choices such as sampling perturbations are justified with theoretical results.\n- Paper is well-written - the paper is clear and well-organized.\n- Significant impact - The regularization can achieve good robustness without adversarial training and is more efficient than adversarial training.  Additionally, it is applicable to the realistic setting where the adversary is unknown.\n\nWeaknesses:\n- Datasets - the bulk of experiments are performed on CIFAR-10 and MNIST.  It would be good to include additional datasets such as CIFAR-100 or ImageNet since the MNIST task is not very complex.  I think ImageNet results would be especially nice since the regularization is much more efficient than adversarial training.\n- Comparisons to other regularization methods - The bulk of comparisons in the paper are to adversarial training, but the authors can also compare to other methods of regularization for robustness for example [1, 2] (which can also be added to related works as well)\n- Architecture consistency - In Table 1, the authors used adversarially trained models from Laidlaw et al. (2021) which I think uses ResNet-50, but the authors use PreActResNet-18.  It would be helpful if the authors could also include results on ResNet-50 so that the comparisons are more consistent.\n\nAdditional suggestions:\n- Measuring stability - the motivation behind the formulation of the regularization is improving stability to small perturbations.  It would be nice to include perturbation stability measurements on CIFAR-10-P [3].\n- I was wondering if compared to standard training adversarial training implicitly reduces hamming distance.  Does adversarial training with NTPM which generalizes better to unseen attacks reduce the hamming distance regularization term more in comparison with $\\ell_{\\infty}$?\n\n[1] Moosavi-Dezfooli, Seyed-Mohsen, et al. \"Robustness via curvature regularization, and vice versa.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n[2] Qin, Chongli, et al. \"Adversarial Robustness through Local Linearization.\" Advances in Neural Information Processing Systems 32 (2019): 13847-13856.\n[3] Hendrycks, Dan, and Thomas Dietterich. \"Benchmarking Neural Network Robustness to Common Corruptions and Perturbations.\" International Conference on Learning Representations. 2018.",
            "summary_of_the_review": "I recommend accepting the paper due to the significant results.  The authors introduce a new regularization method for achieving robustness that is theoretically justified and demonstrate that it is both more efficient than adversarial training, can achieve good certified robustness, and can achieve robustness against unseen attacks.  The paper could be improved in terms of scope of experiments though.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}