{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a novel framework named DSSR, which aims to decouple the strategy learning and surface realization. The core idea is to construct a latent content space for strategy optimization and disentangle the surface style from it. The Experimental results show that their proposed framework improves performance evaluated by several evaluation metrics on the multi-domain task-oriented dialogue datasets MultiWoz 2.0 and MultiWoz 2.1.",
            "main_review": "Strengths:\n1. I really like the idea of decoupling the strategy learning and surface realization in task-oriented dialogue systems, and their method is also well-designed. \n2. The paper is well organized and easy to understand\n\nWeaknesses:\n\n1. My main concern is the technical novelty of this work as there are many similar model architectures in the current academic community, which leverage latent variables to disentangle the style and content within the text.\n2. Usually there will be a human evaluation part in the works of the dialogue systems, this paper also needs to do this.",
            "summary_of_the_review": "In short, I am concerned about the technical novelty and experimental setup of this work.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper attempts to decouple strategy learning and surface realization in task-oriented dialogue, by using a latent content space for strategy optimization and disentangling the surface style from it. Additionally, a surface style variable is introduced to further parameterise the generative distribution of responses, which is trained in a new loss similar to cycle consistency loss. \n\nHowever, the decoupling idea is not new to the NLG community, and the inference algorithm is not well designed or problematic. Though the evaluation did show a slight improvement on the combined scores of multiWoz datasets, the experimental results are not convincing enough to demonstrate that the way this proposed model decouples the strategy (mixture Gaussian & REINFORCE e.g.) achieves better performance. More ablation study is required to improve the experiments. \n\nIt seems to be more incremental than original work, but it could be promising if the method section and experiment section can be further improved. ",
            "main_review": "Strength\n- It achieves slightly better performance than benchmark models\n- The overall writing of this paper is great. \n\nWeakness\n- The inference algorithm is not well designed.\n- The content latent variable z is confusing\n- The ablation study is not good enough (only RL is controlled).\n- The Lemma 1 and T-SNE projection seem to have no contribution to the paper",
            "summary_of_the_review": "Details\n- The intuition of decoupling the learning of strategy and surface realization is not well explained. I do not vote against the idea, but there seems to be no connection between this intuition and the model design. For example, a discrete latent variable is used in Latent Intention Dialogue Models (Wen et al. 2017) for strategy, and no style latent variable is applied. So why it is better to use mixture Gaussian and why style is required to be modelled separately? Given that the diversity of the responses is not in the evaluation metrics.\n\n- The content latent variable was introduced as a sample from a Gaussian mixture distribution, and later it is optimised by RL. Why REINFORCE is required to optimise a continuous sample? Or z is actually the categorical parameter of the Gaussian mixture?\n\n- Assume the z is drawn from Gaussian mixture. It is not the best way to carry out the inference for Gaussian mixture. Deep Unsupervised Clustering (Dilokthanakul et al 2017) can be the reference, and alternatively gumbel-softmax can be used to backpropagate the gradients to the category of the Gaussian mixture. At least, a control variate should be applied to reduce the high variance in REINFORCE algorithm.\n\n- What does it mean \"We do not have the exact definitions of the styles but can differentiate between them\"\n\n- There seems to be some discripencies between the reported scores and the ones from original papers (e.g. HDNO)?\n\n- There too many hyper-parameters α, β, λ, η. How do you choose the values for them?\n\n- The BY-PRODUCTS deviates from the title. It is better to have more experiments to demonstrate the core claim 'decoupling'\n\n- The loss that is similar to cycle consistency loss should be further explained and experimented to show the benefits.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposed an approach to decoupling the dialogue strategy and style generation of responses in task-oriented dialogue. \nThe content, style and dialogue acts are modeled by a latent variable z which is Gaussian mixed distribution. \nExperiments on MultiWoZ 2.0 and 2.1 verify the performance of the proposed model. \n",
            "main_review": "The motivation of this paper is not easily to understand and even a bit farfetched. \nWhy does a ToD system or model need to generate responses with different styles? \nThe authors \"observe a set of dialogue context c paired with responses with the same content distribution but in two different styles s1 and s2\"   and thus propose to jointly model the response generation and style disentanglement. \nAs the authors have the paired dialogue data with different styles, a natural way to model the proposed task is to separately modeling two dialogue models with different styled training data or using multi-task learning. \nIt's also an unclear point about the motivation. It seems the authors first have the proposed model p(r|z,s)  and then find a task which no one has done it, but it makes no sense. \n\nThe modeling details are also unclear. \n1. In lemma 1, the A and b are not defined before using. \n2. What is the latent variable z modeling? The content or actions?\n3. How to construct or collect the dataset of \"a set of dialogue context c paired with responses with the same content distribution but in two different styles s1 and s2\"? \n4. How to estimate the p(s) in different styles? \n5. The authors define the action as <domain, action, slot, value> and design the comparison of each aspects and the F1 score. However, in the setting of the proposed model, can the model see the gold response, which is corresponding to the predicted tuple?  As the traditional NLU models do not see the gold response and predicted response, is it an unfair comparison?\n6. The efficiency results are not reported in the paper. \n7. For a given response (gold or predicted), is the corresponding dialogue state, which means the <domain, action, slot, value> tuple, inheriting the context in the forms of text and tuple?\n\nMinor point, DialogGPT --> DialoGPT. ",
            "summary_of_the_review": "The proposed model is not well motivated in the current version. \nThere are also some main concerns about the proposed model. \nFor more details, please see the \"main review\"",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new architecture for decoupling strategy learning and surface realization. The architecture consists of three modules. \n\n(1) An inference network p(z|C) that maps the dialogue context C to the latent space.\n\n(2) An encoder E(r, s) that takes the response r and the surface style s to the latent space.\n\n(3) A generator p(r|z,s) that takes the latent vector z and the surface style s to generate the dialogue response r.\n\nThis paper combines supervised learning and reinforcement learning to train then finetune these modules.\n\n1. Supervised learning:\n\n(1) Maximizing ELBO: In eq. (3), the evidence lower bound of p(r|c) is maximized. \n\n(2) Replacing the prior distribution of z with E(r, s): To align p(z|C) and E(r, s), the paper minimizes the KL divergence between these two distributions.\n\n(3) Autoencoder loss for the response r: The encoder and the generator can be combined to obtain an autoencoder. Therefore, the paper also uses an autoencoder loss for optimization.\n\n2. Reinforcement learning:\n\nTo make the model more goal-oriented, the paper also uses REINFORCE to finetune the model. In this way, the model can directly maximize the automatic metrics.\n\n\nContributions:\n\n(1) A new model for decoupling strategy learning and surface generation. \n\n(2) This paper achieves better performance compared to previous methods on MultiWOZ (when oracle belief states and database states are given).",
            "main_review": "Strengths:\n\n1. The method proposed in this paper is sound. Also, this paper achieves better performance compared to previous methods on MultiWOZ (when oracle belief states and database states are given).  \n\n2. This paper extensively evaluates dialogue act prediction and response generation, although one or two experiments are missing (elaborated in weaknesses).\n\n3. The paper is clearly written and easy to follow.\n\nWeaknesses:\n\n1. A major confusing aspect is about the surface style s, which is used extensively in the paper. The surface style needs to be inferred, as MultiWOZ does not annotate the style of the response r. However, this paper only mentions the style inference in the statement, \"The surface style indicator s is implemented via one-layer linear model, only trained during pretraining and fixed during asynchronous RL.\" Where is this one-layer linear model applied (e.g. in encoder, generator or inference network)? As the surface style is a latent variable, how is it trained? Besides, how the training dataset (c, r1, s1, r2, s2) is constructed? I assume MultiWOZ only contains (c, r)? How to obtain (r1, s1) and (r2, s2)? Where the style s comes from in the encoder and the generator?\n\nFrom the code (model_dssr.py) in the supplement, it seems that the authors use an encoder to encode both the dialogue act and the response to obtain \"style\". This encoded vector is used to constrain the inference network p(z|c). Please correct me if I am wrong. If this is the case, the authors should make a major revision of Section 4 and Figure 2. In the current writeup, it seems that \"style\" is the input instead of the output of the encoder network. Also, there is a KL divergence between the latent vectors from the dialogue act and the response. The draft does not seem to mention this.\n\nThis results in some mismatches between the code and the draft. \n\n2. I think the surface style s is a continuous vector because it is generated by a one-layer linear model. How to ensure that this continuous vector represents the style of the response instead of the content of the response (similar to the latent vector z)? Is there any objective function to ensure this? \n\n3. I am also confused by the style transfer p(r2|r1; s1, s2) mentioned in Section 4.2. This style transfer function should be able to modify the style of the response r1 from s1 to s2 for obtaining r2. The paper shows some examples of dialogue acts and responses in Table 6 and Table 7. Could the authors also show some examples of the responses after switching the style from s1 to s2?\n\n4. Missing ablation studies. The paper does not provide enough evidence about where the improvements come from. How about ablating the encoder network? How about ablating the style vector? What if using a gaussian prior of z? How about ablating the autoencoder loss?\n\n5. The reward during reinforcement learning is unclear. The paper seems to use the three automatic metrics as the reward. But the equation of combining these metrics is not shown. Also, the paper claims that \"The lack of foreseeing the future is still a main shortcoming for such models. The model manages to foresee the future for more intelligent response in order to achieve better task completion.\" The paper may need to elaborate on how the future is foreseen via reinforcement learning. Besides, which type of rewards is used, e.g. turn-level or dialogue-level?\n\nSome minor points and typos:\n\n1. p(r|c,s) = p(z|c)p(r|z,s). Should add a sum over the latent variable z?\n\n2. Inspired by (Chen et al., 2019) -> Inspired by Chen et al. (2019) \n\n3. Confusing statement: \"The information in the initial state is assumed to be propagated to the hidden states at the future time steps, so we only feed it in the initial state in implementation.\" The authors want to express that z and s is used as the initial state of GRU?\n\n4. We adapt three automatic metrics -> We adopt three automatic metrics.\n",
            "summary_of_the_review": "This paper achieves state-of-the-art performance on MultiWOZ (when oracle belief states and database states are given). However, there are some flaws in (1) the descriptions about the surface style, (2) ablation studies, and (3) rewards used in reinforcement learning. Due to these weaknesses, it is hard to say where the improvements come from.\n\nI will consider raising my score if the comments raised in the main review section are properly addressed. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "\nThis paper proposes a general framework DSSR to decouple policy learning and natural language generation for dialog response modeling. Compared with previous work LarL, which also has latent space for policy/NLG learning, DSSR disentangles NLG from policy learning, and the latent space is optimized only for dialog policy. Experiments on MultiWOZ 2.0 and 2.1 show improvements over several baselines.  \n\n\n",
            "main_review": "Strengths :\n- The presented ablation studies on learned by-products are interesting. \n- The paper is generally understandable and well written.\n\nWeaknesses:\n- The lack of discussion on RL for text generation. For end-to-end dialog modeling, I agree that it is necessary to disentangle dialog policy and natural language generation modules. DSSR could be a nice contribution to dialog response generation. It is interesting to see the results of combining DSSR and SimpleTOD/UBAR. \n- There are several gaps in the story of the paper presented in the 2nd paragraph. The language modeling approach is after end-to-end RL approach... In addition, the criticism in the 2nd paragraph, \"all labels results in the middle are indispensable,\" is too strong. For example, SOLOIST by Peng et al., which is not cited in this paragraph, does not use dialogue act for response generation. Both SimpleTOD and SOLOIST require belief state annotation, but the experimental setup of this paper is context-to-response, which assumes the existence of ground truth belief states. \n- Another concern of this paper is the lack of human evaluation. \n- It is not clear whether the performance differences between DSSR and HDNO different or not. \n- It might be better to describe in table 1 that the task setup is context-to-response. ",
            "summary_of_the_review": "This paper proposes DSSR to decouple policy learning and natural language generation for dialog response generation modeling. I agree that it is necessary to disentangle policy learning and NLG. The presented ablation studies on learned by-products are also interesting.\n\nHowever, I have several concerns, including:\n1) the lack of related work on RL for text generation to assess the technical contribution.\n2) Some gaps in the storytelling.\n3) Experiments are not convincing due to the lack of statistical tests and human evaluation.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to decouple the strategy learning and surface realisation in a general framework, namely DSSR. They test DSSR on two dialogue dataset\\s MultiWoz 2.0 and MutliWoz 2.1 in comparison with methods missing strategy and surface realisation in different levels. ",
            "main_review": "The current paper marries deep latent models with RLs, which is quite a plausible solution to end2end RL dialogue models. I personally think the proposed method is analogous to the conditional VAE model (Zhao et al., 2017). Anyway, however, fundamentally, it is hard for me to believe that what the model does when decoding from a latent variable is only about surface realisation. There has been a bank of studies in NLG and dialogue systems to decouple planning from realisation (or you can say decouple realisation from planning), but most of them are done in an explicit way. More explanation is needed here or the authors need to use other terms. \n\nIn addition to the above issue,  I also have two major concerns about the evaluations in this paper, which hinder me from recommending an acceptance.\n\nOn the one hand, the experiment results cannot suggest the effectiveness of DSSR, because: (1) the authors *imply* that BLEU is *only* about the surface realisation. I do not think this is correct; (2) DSSR has lower performance on deciding the strategy compared to classic RL methods. This paper says although these RL models achieve better Inform and success scores, the responses they produced are ungrammatical.  However, no evidence has been provided to support this claim. If the authors mean they have lower BLEU scores, then such evidence does not reliable as lower BLUE does not lead to an ungrammatical text and BLEU itself has low validity for evaluating dialogue systems (see Liu et al. (2016) for more discussions); (3) it appears to me, for a task-oriented dialogue system having higher inform and success scores is much more important than higher a BLUE score, but the combined score values too much for BLEU. This makes the DSSR have a higher combined score than other RL baselines but this does not necessary mean DSSR can produce responses with higher quality. (ps. an evaluator that has been used in previous work does not mean it is correct and reliable).\n\nOn the other hand, when compared with language models, this paper argues that DSSR can defeat these pre-trained LMs. However, it has been evidenced that pre-trained LMs for language generation are good at surface realisation but do a worse job on planning. This is why when you evaluate the dialogue system as a whole, they did not perform good, but when you test them on the task of generating text from DA, they defeated DSSR.\n\n\nReferences:\n\nZhao, T., Zhao, R., & Eskenazi, M. (2017). Learning discourse-level diversity for neural dialog models using conditional variational autoencoders. arXiv preprint arXiv:1703.10960.\n\nLiu, C. W., Lowe, R., Serban, I. V., Noseworthy, M., Charlin, L., & Pineau, J. (2016). How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. arXiv preprint arXiv:1603.08023.\n",
            "summary_of_the_review": "The general idea of this paper looks interesting to some extent, but generally akin to CVAE (more explanation is needed here), definitions of some concepts do not clear in this paper (e.g., surface realisation), and the evaluation has major flaws.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}