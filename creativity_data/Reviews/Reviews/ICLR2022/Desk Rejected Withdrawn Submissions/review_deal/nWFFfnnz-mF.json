{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the optimization properties of conservative Q learning in offline reinforcement learning. In particular, the paper shows that the optimization objective of CQL has high condition number. This problem can only be mitigated by imposing strong geometrical assumptions on the dataset. Inspired by the theoretical findings, the paper proposes to use behaviour cloning regularization to improve the convergence property. Finally, the authors provide empirical studies of discrete and continuous control tasks to validate the theoretical findings.",
            "main_review": "The idea of conservatism (or pessimism) is widely used in offline RL. However, it still remains unclear when such approaches can and cannot work in practice. To this end, this paper studies a very important problem in offline RL literature. \nThe paper also provides some novel insights about the CQL objective and explains why it has a bad optimization behaviour in practice. \n\nHowever, I still have the following concerns and questions. Please address these in the rebuttal and correct me if I misunderstand some critical part of the work. \n\n1. It seems that the ill-conditioned value space of CQL (Explanation 1) is due to the usage of softmax in CQL, which is not necessarily an effect of conservatism. \n\n2. Following the first point, in fact there are a lot of implementations of conservatism in practice, such as the model-based approaches and behaviour-regularized approaches. It's hard to see how the explanations used in this paper can extend to those methods. Thus I think the title might be misleading. It's actually only about CQL, not conservatism in offline RL. \n\n3. I am also not very satisfied with the notations and definitions. The expected discounted reward objective defined at the end of page 2 is very weird. Why $s_t$ is sampled from $\\mathcal{D}$? In Eq 2, what is softmax(Q)? I guess it's the softmax policy defined by Q, but it should be a vector or matrix right? Then there is a vector transpose missing? The Hessian should be a matrix, so how $\\hat{\\pi}_\\beta$ is defined? It's also useful to exactly define the Bellman operator $\\hat{\\mathcal{B}}^\\pi$ in Eq 1 as a reader with no RL background probably won't know the meaning of that. \n\n4. I think maybe it is worthy showing the derivation of Eq 4, at least in the appendix. \n\n5. The idea of using behaviour regularization to improve the optimization objective in Section 4.4 is interesting. How is that different with behaviour regularization used in other offline RL algorithms, like BRAC (Wu et al., 2019)? ",
            "summary_of_the_review": "I recommend rejecting this paper as there are some technical flaws as explained in the above section. Also, I am not fully convinced that the theoretical insights can easily extend to other offline RL algorithms. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper attempts to examine the effect of using conservative estimates in offline RL. The authors argue that such estimates result in an ill-conditioned learning objective (the condition number may be large) by providing a lower bound of the condition number. They further argue that the issue becomes serious if the function space is not expressive enough (i.e., low-rank). Then the authors show that a naive approximate second-order method cannot address such a problem because there is a strong assumption on the Hessian matrix. Hence, to improve the precondition number, a regularization method is proposed to maximize the mutual information between the learned policy and behavior policy. ",
            "main_review": "Strength: \n\n1. CQL concerns an important problem in offline RL — distribution shift; the paper focuses on understanding the effect of using conservative estimates to deal with such a problem, so it is an important topic. \n\n2. The paper does suggest some new perspectives to understand the effect of learning conservative estimates on the optimization process. \n\nWeaknesses:\n\n1. The significance of the contributions is questionable. \n\nFirst, it points out the bad condition number induced by using CQL. However, it never has a solid theory. It also never empirically examines how this condition number can hurt. Existing work shows CQL learns faster, and it is popular. So it is unclear if the condition number is really an issue for CQL. \n\nSecond, it says the approximate 2nd-order method is not feasible because there is a strong requirement on the approximated Hessian, and hence regularization method should be used. There are several gaps here. The theory is unsound, it never explains how exactly the approximate 2nd-order method affects convergence rate, and it never clearly describes the assumptions. Furthermore, how about other types of approximate second-order methods? To reduce condition number, there are other approximation methods, other ways to design a preconditioning matrix. Why not consider them? \n\n2. The theoretical findings lack depth. \n\n(a). Section 4.1, the only interpretation of the conditioner number is “in the case of estimation errors at successive iterations being higher … ” However, there is no intuitive way to see whether that would happen and how much higher at each iteration. Such interpretations do not convey a clear message. A good way is to derive the bound for some baseline algorithm that does not use conservative estimates, and then the authors can compare different bounds. It could also be helpful if the authors design some experiments to verify the theoretical results. \n\nFurthermore, what would be the actual effect of such bound? From an optimization perspective, a worse condition number should imply that the algorithm learns slower. However, from the original CQL paper, it seems CQL learns a better policy within the same iterations as used by other competitor algorithms. \n\n(b) “eq (4) implies t\\prop 1/k.” I think it is hard to claim this. Note that the condition number is not independent of \\hat{\\pi} and |D|. Furthermore, there is a gap between eq (4) holds and explanation 2 — they are logically disconnected. First, small t can imply that capital K is small, not necessarily indicating the condition number is large. Second, the paper never clearly defines “under-parameterized approximation”. I guess it means the low rank of the value function space, so in the linear case, it means the rank of the representation space. However, then it is counter-intuitive to have explanation 2: lambda_min in the conditioner number is actually the smallest nonzero lambda. I do not see how it is related to the rank of the representation space.  \n\n(c) section 4.3. I don’t think it is intuitive to see the relation between eq (3) and (8). Also, “a loose lower bound … improved curvature.” This is not true and cannot be verified. \n\nAn additional note. I feel it is really confusing when you say “from value function to approximation.” I thought you intended to present some results in a linear/nonlinear function approximation setting. However, from eq (5), I believe you actually mean Taylor's approximation of LSE(Q). \n\n(d) explanation (3) is not justified, and the statement itself is vague. What is the meaning of an expressive dataset? Why can “expressive dataset leads to improved convergence” lead to the 3rd explanation? \n\nWhat does it mean by “accurate convergence”? \n\nIf I understand correctly, the assumptions are actually about the Hessian matrix. Saying “geometric assumptions” seems unnecessary jargon, and you never explain what these assumptions exactly are.\n\n(e) section 4.4. The proposed regularization lacks justification. There are two main problems: 1. adding an entropy regularization can improve the convergence rate. Why not do that? Why choose MI specifically? 2. Although adding such as MI term increases convergence rate, it changes the optimum in a potentially unreasonable manner. What if the behavior policy is bad? Adding such regularization could significantly hurt the optimality of the learned policy. \n\nGenerally, it does not make sense to improve the convergence rate when the optimum is changed significantly/undesirably.  \n\nOne more problem is, before proposing the regularization method, the authors never attempt to improve the condition number from other perspectives, such as the quasi-newton method or other approximate second-order methods. \n\n3. The paper contains numerous unnecessary jargon and unusual/awkward expressions, which significantly weaken the clarity of the work. I already mentioned some in the above comments. I list some under “minors”. (Note that usually, this should not be a major concern, but there are too many in this paper. I may have my own preference of reading. Hence I will also refer to other reviews and adjust my comments about this accordingly.) \n\n4. Relevant literature is not sufficiently reviewed. I already mentioned some.  \n\n\nMinors:\n\n1. Page 2: the second line from the bottom: d(s)pi(a|s) can only describe (s, a), not (s, a, s’)\n\n2. Page 3, before eq 1, rho, mu are not defined. I guess they are from the existing literature of CQL.\n\n3. Section 4.1: affect —> effect\n\n4. 1st paragraph of section 4.2. The authors should give more background about the following terms: implicit sparsification, rank degradation rounds, the drop in representational capacity (why sparsification leads to such drop). \n\n5. “An alternate viewpoint of evaluating optimization is the consideration of parameterization.” Awkward. \n\n6. “Inaccurate approximations pose a dire need … “: didn’t you mean strong assumptions on the H matrix poses a need … ? What is “inaccurate approximations”? \n\n7. Page 6: facilitate approximations which provably inform the agent of the dataset. Awkward. \n\nFollowing the “rationale of regularization”, what does this mean? \n\nPersistent rounds of rank …: awkward. \n\nExplicit penalization of Q values fulfills two obj: (1) … (2) … However, I cannot understand the two points you mentioned, I don’t think their meaning is clear. \n\nAlleviate undesirable search space. What does it mean? \n\n facilitate approximations which provably inform the agent of the dataset. This sentence is unclear. \n\n8. Page 1: are driven by the motivation of … -> are motivated by \n\nThe nature of lower bounder Q values … unclear \n\nP1 last paragraph:\n\n“Conservatism implicitly … ” is it a general conclusion from the cited paper? \n\n “this in turn cripples the policy … ” is there evidence? \n\n9. Improve search space: I guess you mean improve condition number ",
            "summary_of_the_review": "Please see the main review. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the impact of conservatism in offline RL, specifically analyzing the optimization pitfalls of current conservative learning objectives and proposing potential remedies. In a nutshell, the theory suggests that conservative offline RL methods suffer from a progressively degrading optimization landscape in part due to compounding approximation errors stemming from underparameterization of function approximators. To tackle these issues, the authors propose a new regularization objective for conservative offline RL and show empirically that the resulting method benefits from improved optimization stability and is less prone to over-pessimistic estimates.",
            "main_review": "There are a number of strengths with this paper:\n1. The paper is well-motivated: learning conservatively in the context of offline RL is a relatively new and understudied problem, and having a theoretical understanding on “conservatism” will offer new insights in developing better offline RL algorithms.\n2. To the best of my knowledge, this paper presents a novel insights on conservatism that has not been studied in prior literature, while at the same time referencing other related work when relevant, for example the work by Kumar et al. on implicit under-parameterization in deep RL.\n3. From some inspection, the theoretical analysis is sound. However, as a disclaimer, statistical learning theory is not my area of expertise and I did not rigorously examine all of the analysis in the supplementary section.\n4. The empirical evaluations support much of the theoretical analysis: (1) with CQL, there is ill-conditioning in the optimization process with resulting over-pessimism. (2) CQL(Bohn) results in an improvement but not consistently so, as the theory suggests. (3) CQL-IM on the other-hand appears to offset issues with optimization and over-pessimism.\n\nAlongside these strengths, there are also a number of concerns and subsequent areas for improvement:\n1. I had issues understanding some parts of the analysis. Most notably, Explanation 3 states that methods like CQL(Bohn) “impose strong geometric assumptions on the approximated Hessian,” however the paper does not expand on what these “geometric assumptions” exactly are. Perhaps the authors are relying on readers to infer what these assumptions are, but I had a difficult time understanding. There is a small paragraph proceeding the explanation that tries to elaborate, but it is a bit handwavy to me, and warrants significantly more discussion.\n2. The section that introduces CQL-IM (Section 4.4) is currently presented in a rather abrupt manner. The discussion for having regularizers and incorporating mutual information somehow feels disconnected from the previous sections — it does not quite delve into why incorporating these components allows CQL-IM to avoid the pitfalls of alternative approaches.\n3. (Building on the point above) The authors imply that CQL-IM can address some of the pitfalls of CQL and CQL(Bohn), however, unlike the analysis on CQL and CQL(Bohn), there is no corresponding analysis on the optimization landscape or lower bound analysis on CQL-IM. Therefore, it is difficult to understand why CQL-IM is theoretically justified over the alternatives. Perhaps the authors did make this justification, and even if this is the case, the writing in its current form does not appear to do so.   \n4. While there is a significant focus on theory, there are also some empirical evaluations, which helps to support the theoretical claims of the paper. However, there is no unified description of how these methods are implemented in practice (eg. pseudocode or a high-level text overview). This can help answer questions such as the following: how is Bohn(Q) computed in practice for CQL(Bohn), and how is the approximate posterior q trained in QL-IM? While the answer to some of these questions may be obvious to the authors, I think many readers would appreciate further details on the practical implementation of these algorithms.\n5. I’m curious as to why the authors chose a simulated Drone environment as their testbed for empirical evaluations, where there already exist more established domains for evaluating offline RL methods such as D4RL [1]. Certainly, it wouldn’t hurt to include the drone environment, but testing on a diverse set of environments (locomotion, adroit, kitchen, etc) that are well-studied by other papers is strictly beneficial, especially for other researchers who may want to benchmark the methods presented in this paper. In this sense, I would consider the empirical studies presented here as the weak link of the paper.\n\nAlso, this may be a minor point, and one that other reviewers may disagree on, but this paper can benefit from reducing flowery language. For example, the phrase “provision of ill-conditioned search spaces … pose a dire need for sounds convergence” can be rephrased to use more simple language. This does not affect my assessment of the paper in any way and reflects my personal tastes, but I wanted to share this comment merely as a suggestion.\n\n[1] Fu et al., D4RL: Datasets for Deep Data-Driven Reinforcement Learning, 2020",
            "summary_of_the_review": "There are a number of strengths with this paper: the paper studies an important problem, offers interesting theoretical insights, and verifies these insights with some empirical evaluations. On the other hand, there is notable room for improvement, namely elaborating more on the theoretical claims and incorporating a more diverse suite of environments in the empirical evaluation. In my view, the paper is currently marginally below the acceptance threshold, but with appropriate improvements I can anticipate raising my recommendation score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The manuscript presents two new variants of CQL(H). They are loosely motivated by a geometric argument in value-function space. They are compared to BC, SAC, and the original CQL.",
            "main_review": "Apologies for the shortness of my review but this manuscript is very poorly written. Much of the claimed novelty of the paper lies in theoretical insights and theoretically motivated algorithmic improvements, however most of the mathematical expressions in the paper lack rigour to the point of making them extremely difficult to assess---expressions like \"for all Q in S x A\" for instance are small but representative example of this. To be clear, there may very well be some worthwhile insights in this work but the exposition needs a lot of work before it can be reviewed, let alone published.\n\nI have a few concerns regarding the experimental section as well. As general comments: the fonts in the legend and axis labels are too small; Figure 4 and 5 lack error bars; two tasks are missing from Fig 5 and one from Fig 6. More detailed issues with Figures 4 and 5 concern the choice of y-axis limits, these make the differences between the algorithms seem much larger than they are, especially given the lack of error bars; for example, in Figure 5 top-right, the range of ranks is less than 2%. Finally Figure 7 and its explanation made little sense to me.",
            "summary_of_the_review": "Apologies again for the short review. The paper is very poorly written in its current form. It may have some merit but it requires too much work to reach the bar for publication at this venue in my opinion.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies how conservatism (in particular, the CQL method) effects the optimization process of the Q-function. Both theoretical and empirical results suggests that the optimization of CQL method is not stable due to a large condition number. This paper argues that changing the regularization term in CQL to its upper bound decreases the condition number, and therefore stabilize the training. It is also argued that combining with behavior cloning, the convergence of Q-function further improves. ",
            "main_review": "==== after rebuttal: Thanks for addressing my concerns promptly. I'll increase my score to 5 accordingly. However, I think this paper needs a major revision before acceptance: 1. resolve the ambiguity in both writing and proofs, and 2. improve the structure and clearly states the relation between CQL, CQL_Bohn, and CQL_IM =====\n\nThis paper considers the optimization stability of CQL for offline reinforcement learning, while existing results mostly focus on the statistical performances and ignore potential optimization issues. The question is well-motivated and can potentially help RL community understand conservatism methods.\n\nOn the plus side, this paper establishes a lower bound on the condition number, which implies that the optimization of CQL can be instable. The result is also supported by experiments. \n\nHowever, I find this paper hard to follow, and several claims lacking explanation:\n-\tUnder Explanation 1, why increasing estimation error leads to increasing condition number? If I understand correctly, in the case \\|softmax(\\hat{Q}^{k+1}-softmax(Q)\\|= \\|softmax(\\hat{Q}^{k}-softmax(Q)\\| (i.e., the estimation error grows exponentially), the LHS of (3) should be a constant.\n-\tWhy Eq (8) leads to a looser bound? It’s unclear to me whether the softmax always increases the condition number.\n-\tBelow Explanation 3, it’s unclear to me how the diagonal terms of \\tilde{H} affect the estimation. In particular, why is the unit ball considered as the boundary of over and under estimation?\n-\tAppendix C.2 is just a list of equations without explanation. It’s hard to follow what’s argued here.\n-\tWhat’s q_t in Eq. (12) and how it’s computed? q_t here depends on the behavior policy. Does the algorithm knows the behavior policy?\n\nI also have concerns regarding the methods:\n-\tAlthough the CQL_Bohn reduces the condition number, it significantly hurts the performance (from Table 1). Besides, it’s unclear whether CQL_Bohn always decreases the condition number because of the results in Figure 4, hover. So I’m not convinced that the large condition number of CQL is an significant concern – even if the condition number is large, the performance of CQL is still acceptable, and on the other hand, fixing the condition number doesn’t improve the performance.\n-\tIt’s mentioned that the behavior policy is a pretrained SAC behavior policy. In Table 1, the behavior policy outperforms (or at least comparable) with all the offline methods. So I’m skeptical about the claim that adding behavior cloning CQL in general. It seems that in this case, comparing CQL with behavior cloning with standard CQL is unfair because the behavior policy is near optimal. What if the behavior policy is a mixture of the SAC policy and a random policy? Will the behavior cloning hurt the performance?\n\nMinor issues:\n-\tIn Eq (2), what’s the dimension of each term? In particular, what’s the dimension of softmax(Q), \\hat{\\pi_\\beta}}, and the Hessian itself? \n-\tFigure 1 (left) is not very informative. What’s the meaning of the two manifold and the mapping?\n-\tIn Appendix A.1, it would be better to explain in detail why CQL(H) is convex. It’s not trivial to see the Hessian is PSD.\n-\tIn Eq. (15), what’s the dimension of the terms? It seems to me that the LHS is a symmetric matrix, but softmax(Q) is a vector.\n",
            "summary_of_the_review": "This paper is well motivated and studies an interesting question about conservatism in offline RL. My main concerns are (1) the clarity of the writing (mainly the technical part) and (2) the experiments. As a result, I recommend a reject at this point, but with low confidence.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}