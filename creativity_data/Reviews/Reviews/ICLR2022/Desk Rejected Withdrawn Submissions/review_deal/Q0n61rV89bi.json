{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "A well written paper that describes a neural network based training framework for semi-supervised learning of phoneme-level discrete speech representations using word level constraints. The primary contribution of the paper is a rigorous mathematical formulation and framework for the problem at hand. Within this framework, a suitable objective that encourages the discovery of phoneme-level units is designed. In conjunction with four components: a pre-trained segmentation network that segments the signal into phonetic units, a pre-trained embedding extractor, a trained classifier and an estimated quantizer, this objective function allows the model to learn phoneme inventories, with significantly better performance on several speech datasets. ",
            "main_review": "Strengths of the paper:\n1. Rigorous mathematical definition of the problem, solid problem formulation and theoretical guarantee of proposed approach.\n2. Development of a suitable objective function that can be use within a neural framework for phoneme inventory discovery.\n3. Improved results which demonstrate the efficacy of the proposed approach.\n\nWeaknesses of the paper:\n1. The efficacy of the proposed method is very much dependent on the quality of the input features, quality of segmentation and the use of a pre-trained embedding extractor. This is evident from the results in Table 3b and 4b. The performance of various models improve when different features are combined and a better segmentation is used. The model is not a full end-to-end neural model but is trained in parts.\n2. Given that the pre-segmentation network and embedding extractor are pre-trained off-the-shelf models, the novelty of the paper is limited to the training of the word classifier and the quantifier. Neither the architecture nor the training criteria of these components (minimizing cross entropy/minimizing KL divergence before and after quantization) is novel in isolation. The novelty is in the combination of these modules with a suitable joint training criteria as described in section 5.1.\n3. The notion of using word level constraints for learning a phonetic inventory is not novel. See Jansen et al (2013), \"Weak top-down constraints for unsupervised acoustic model training\". It will be useful to reference and compare with this approach.\n4. The usefulness of better feature representation, neural network features have been studied before. See Carlin et al (2011), \"Rapid evaluation of speech representations for spoken term discovery\".\n5. An evaluation of word-level phoneme inventory discovery can also be conducted on the TIMIT corpus. The TIMIT corpus is annotated at the word level with a closed vocabulary and balanced phone inventory. Given these ideal settings, can the model approach the theoretical guarantees outlined in Section 4.3 and recover back 61/48 phones used in TIMIT? If not, a discussion on the practical limitations and constraints of the proposed approach will be useful to better appreciate the efficacy of the proposed method.",
            "summary_of_the_review": "1. [Correctness, score of 3]: While paper is mathematically very rigorous, the paper needs to provide empirical evidence to some extend of its contributions for example along the lines of comment 5 under weaknesses of the paper. This will help validate the claim that the correct phoneme inventory can be discovered with exponentially low error rate (contribution 2 of the paper, Section 1). \n2. [Technical novelty and significance, score of 2]: Comments 1,2,3,4 above, under weaknesses of the paper. \n3. [Empirical Novelty And Significance, score of 3] Comments 1,2,3 above, under strengths of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper discusses how words can be used to probabilistically constrain the discovery of the phoneme inventory for a given corpus, which contains speech recordings and word level transcripts. The contributions of the paper include 1) deriving a statistical and computable definition of a phoneme that is related to the linguistic definition, 2) a loss function for learning a phoneme inventory given a finite corpus, with bounds on the error when minimized, 3) a computational system for optimizing the loss that takes speech samples, segments them into phoneme segments, generating embeddings which are fed into a word classifier; the segment embeddings are quantized via codebook to yield a discrete phoneme inventory. This computational system is called an \"information quantizer\". Experiments show they can generate good phonetic inventories on the TIMIT and MBoshi.",
            "main_review": "Strengths: The paper is written clearly and precisely. The formulation of the problem is clear and the description of the objective and information quantizer thorough. One gets the sense that they could replicate the work with the level of detail in the paper. The results presented are strong exceeding several good, reasonable baselines. \n\nWeaknesses: Demonstrating overall impact. While the results are strong for the metrics evaluated, it's less clear what the practical impact of their work is. F1 scores and t-SNE plots do show that the generated inventories are reasonable, but do the automatically derived phoneme inventories perform better than manual ones for tasks such as ASR? Do they show improved performance for classification tasks with far less data? These zero-shot scenarios, while well intentioned, yield systems that have high error rates and hence limited usefulness.\n\nMinor comments:\n- the use of semantics in the paper doesn't seem quite right. this work is an aspect of phonology, of how sounds can be categorized into phonemes which substitutions yields different words and different meanings. A definition: \"phonology is the study of the categorical organisation of speech sounds in languages; how speech sounds are organised in the mind and used to convey meaning.\" https://all-about-linguistics.group.shef.ac.uk/branches-of-linguistics/phonology/. it is not the meaning of words, rather how the words themselves that constrain the sequence of phonemes. this doesn't affect the premise of the paper though.\n- the use of \"type\" in the paper is odd, e.g. phoneme type or word type. to the reader, this sounds like a class of phonemes like nasals or word classes like nouns. it could simply be dropped.\n- It's a little confusing what parts of the system are trained on the whole of Librispeech (~460h clear or 960h all), sounds like the segment embedding model only, or the word-only subset of Librispeech. ",
            "summary_of_the_review": "Technically the paper appears sound with good results and analysis and is clearly written. The impact for practical systems is less clear. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this work, the authors study the problem of identifying a sub-word unit inventory given examples of words which have been pre-segmented into sub-word units (phonemes in this work). Thus the authors propose a technique which allows the identified phoneme segments to be assigned categorical labels in an unsupervised fashion, given word level labels. The authors also prove that the proposed techniques discover the optimal phoneme dictionary with exponentially low error rates.",
            "main_review": "**Strengths:**\nThere are aspects of this work that I found very interesting. For example, the “statistical” definition of phonemes which re-casts the standard linguistic definition based on minimal pairs, and its use in defining the objective function for the task of phoneme dictionary learning is an interesting idea. The fact that the proposed techniques are accompanied by theoretical guarantees is also a strength.\n\n**Weaknesses:**\n- One of my main criticisms of the work, is that the task -- as it is currently defined -- is of limited interest. In particular, the proposed techniques require knowledge of word identity and phoneme segments (in this case, obtained using the previously proposed work of [Kreuk et al., 20]). In the introduction, the proposed task is motivated by stating that the proposed system could be used to: “... repurpose algorithms developed for written languages so that they could be used for unwritten languages”. I think it would be interesting, for example, if the authors evaluated the proposed techniques on such a task in addition to showing results on the phoneme identification task. \n\n- In the introduction, the authors mention that: “... we formulate the problem of learning a phoneme inventory as a self-supervised learning problem, where a small amount of semantic supervision is available. Such supervision can be ... name of a visual object semantically related to the spoken word, ... (or) the translation of the spoken word into a written form in another language”. I found this formulation to be extremely broad, and more general than the problem setting that is actually evaluated in the experiments, since in all experiments, the authors assume that the word label for each phoneme segment is available. I think the introduction should be formulated to more accurately reflect the problem formulation in the work.\n\n- In section 3.2, the authors mention that “Further, instead of explicitly minimizing K as required by the definition, we can fix K either based on prior knowledge of the language or by simply gradually decreasing K until the distributional property of phonemes is no longer feasible.” I was not clear on how this might be done in practice. For example, in the results in Figure 2, it appears that using a larger value of K (256) improves the Token F1/NMI metrics on Librispeech, but this is clearly larger than the number of phonemes that one might expect for English. Could the authors comment on how the optimal size K might be determined without prior knowledge.\n\n- The authors mention that “Note that the amount of data we need is much lower than previous works (Yusuf et al. (2020): around 30 hours, Feng et al. (2021b): around 600 hours), and the vocabulary size used is much smaller than the total vocabulary size in the language.”. However, since the authors use a CPC model as the speech encoder trained on Librispeech for the task, it should also be more clearly acknowledged that the authors use a large amount of unsupervised speech in addition to the dataset used for the proposed method. Was the same English CPC model also used as the speech encoder e_(\\theta_1) for Mboshi?\n\n- Equation 5, defines the conditional distribution over the word tokens conditioned on isolated acoustic segments. This would seem to be a very hard task for the neural network to learn, since you would have to be able to determine the word identity for a single phoneme snippet. What are the accuracies of the model when trained on the various datasets for this task? \n\n- The description of how the datasets were constructed for the experiments in 5.2 isn’t very clear. The main text mentions that details are provided in Appendix B, but the appendix only contains statistics of the sets. Also, I noticed that the authors mention in Appendix B that “the whole datasets are used for both training and evaluation” following previous work. This would seem to be a problem given that the models are trained on relatively small datasets, so training and evaluating on the same data feels like it would lead to overfitting. Could the authors please clarify why this is an acceptable evaluation paradigm?\n\n- *Minor comment*: In Section 3.2, it would be usual to clarify what a “phoneme segment within the word means”  -- e.g., x_t is a sequence of acoustic frames etc. \n\n- *Minor comment*: In Section 4.3, the authors mention that one of the assumptions is that “the true prior of the phoneme posterior” is uniform, and that “The uniform prior is chosen as it approximates phoneme prior sufficiently well, though other prior can also be used.” I’m not sure if this is actually the case for English/Mboshi. Could the authors please provide statistics for the empirical prior of phonemes computed on their datasets, and/or some citations to studies which compute these quantities.\n",
            "summary_of_the_review": "Overall, while the work has some interesting ideas, I feel that the work is limited in its impact. I also have some concerns about the evaluation and the fact that the authors train and evaluate the models on the entire dataset. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The task is similar to a zero-resource task.  The task is defined in section 3.3.  Assume a phoneme inventory of K (where K is an input).\nLearn a model that will do well in the evaluation, minimizing TER (token error rate) as defined in (eq) 4.  \n\nThe contribution is a proposed model (IQ) based on an information quantizer.  The evaluations show that the proposed method works better than some baselines.",
            "main_review": "The proposed method looks like a reasonable way to approach the zero resource task, though the use of word level constraints seems to be a big innovation that is new to me.\n\nGiven recent successes with wav2vec, I'm less excited about zero resource methods than I used to be.  It appears that wav2vec works amazingly well, even on surprise languages, even without fine tuning.   We are seeing much better numbers on PER (phone error rate) on these benchmarks with wav2vec, though of course, wav2vec has been trained for English and these benchmarks are in English.\n\nI found the paper hard to read.  Consider the definition of TER in eq (4).  This requires understanding what z means.  The text refers to definition 2, but I don't see a mention of z near definition 2.  Definition 2 starts out with a definition of X as a set of speech segments.  Normally, in phonetics, a segment is a phone, but I'm not sure you mean that.  Do you mean an interval of time?\n\nDefinition 2 then starts talking about Z, but again, I don't know what that is.  Is that something like a spectrogram or a ceptrum.  That is, it is a sequence of frames, where each frame is a vector of floats?  And then Z_i is a sequence of frames that should hopefully correspond to a phone?  This paper would be a lot easier to read if you provided an example of how this notation could map into standard units in speech processing.  You do some of this in section 5.1, but it would help if you connected the dots between frame rate and the notation earlier in the paper\n\nI found myself having to read the references such as https://arxiv.org/pdf/2011.03115.pdf to figure out the evaluation metric (NMI).  That paper is much more clear than this paper on notation and definitions of metrics such as NMI.  I really should not have to read the references for simple things like this.\n\n",
            "summary_of_the_review": "This paper looks like it will be appreciated as an improvement over the references, but I suspect that the audience will be limited because it looks like alternative methods such as wav2vec are likely to beat zero-resource methods.\n\nThe paper will also have a somewhat limited audience since it is very difficult to understand this paper unless one is familiar with the references for reasons mentioned above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}