{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes to prune networks by optimizing a cross-correlation objective between the original network outputs and the pruned network outputs. This is similar to distillation objectives often used to reduce model size (not necessarily by pruning e.g. as in DistillBERT that this paper references), however the authors show that their approach reaches better accuracy at similar pruned weights.\n\nThe authors state that their method recovers performance faster, and obtains superior results on GLUE and XGLUE benchmarks when compared to other distillation and pruning methods.",
            "main_review": "Overall this is an interesting paper that proposes to prune the network while distilling to the outputs of the same network pre-finetuned on the target task, using a few different objectives and showing that cross-correlation objective works best.\n\nStrengths:\n- While individual components of the method are not novel, the combination of them seems to be (i.e. magnitude based pruning combined with cross-correlation based self-distillation)\n- The results look fairly convincing, especially when compared to other pruning methods\n\nWeaknesses:\n- It might be interesting to see how this kind of pruning would work if applied before the fine-tuning stage. Cross-correlation objective does not require to have any specific labels to distill onto (as shown in the Barlow Twins paper, one of the inspirations for this work), so why limit to fine-tuning to specific tasks?\n- It is not too clear what the authors intend to show when discussing the connection to 'frobenius distortion minimization'. Could you motivate this section?\n- Why is there such a large drop in MI for SDP-CC-MBP as compared to other SDP methods as well as regular MBP in the 50-80% space?\n- figure 5 is quite difficult to read",
            "summary_of_the_review": "Overall this is an interesting paper that takes a particular approach to show that self-distillation based pruning with various objectives (especially the cross-correlation objective) works well and outperforms other pruning methods that do not rely on self-distillation (which in itself relies on having a fine-tuned model to distill against, for each task).\n\nI feel the paper can be improved by exploring whether the same distillation approach works on models before fine-tuning, motivating the analysis a bit more and attempting to explain some gaps in the performance across various approaches (e.g. MI for SDP-CC-MBP, better performance on some of the tasks by pruning methods with no self-distillation step)",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper propose a self-distilled based pruning approach showing it outperforms many baselines in NLP task. Below please find my detailed comments.",
            "main_review": "Major:\n1. Overall, I think the writing is poorly organized. The motivation of the approach is not clearly explained. For example, in the second paragraph of introduction, it starts with discussing ‘the mask based pruning algorithm add extra parameters’ (which I actually don’t think is a big deal as it will be absorbed at deployment) and then suddenly switch to ‘minimizing the performance degeneration due to pruning’. And then, without any reasoning, the authors start to make hypothesis on self-distillation might be able to make improvement. In the later paragraph, there suddenly comes a ‘maximize the cross-correlation’ …\n\nThe presentation of the paper is far below a basic requirement.\n\n2. Too many unsupported claims in the paper. In section 3.3, authors discuss the potential gain of the algorithm without any theoretical and empirical support.\n\n3. The method seems to be quite trivial. Simply adding some self-distill loss without and in depth observation.\n",
            "summary_of_the_review": "Overall, I feel this paper is below the acceptance bar.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studied the problem of model pruning in a pre-training fine-tuning setup, i.e., prune model for down-stream tasks from a pre-trained model. The authors proposed a self-distillation based framework. Specifically, the self-distillation framework leveraged soft labels from a fine-tuned teacher model as well as a cross-correlation objective. Then an MBP based algorithm was used to prune the model. The authors provided insights on why self-distillation + pruning could work better than other baseline methods, and conducted experiments on GLUE and XGLUE.",
            "main_review": "Strengths:\nI think the distillation + pruning idea makes sense and could be potentially better than the typical distillation only (larger -> small) or pruning only methods. The authors also provided several key insights explaining why self-distillation improved pruned model generalization. \n\nThe authors also conducted extensive experiments on large-scale NLP benchmark datasets and tasks. Experimental results are quite encouraging where proposed methods with (10% or 20%) of parameters out-performed other pruning baselines. \n\nWeaknesses:\nI think the efficiency of the proposed method needs to be discussed. In the introduction, the authors mentioned that pruning based methods with continuous masks are more costly since it requires twice the number of gradient updates. However, the proposed framework needs to have a fine-tuned teacher model, then fine-tune student model along with the teacher model. This seems to be even more costly, if not similar to the pruning methods with continuous masks.\n\nAlso, I think the presentation of this paper can be further improved. For example, self-distillation might be done in different ways, it might be better for the authors to be clear in the introduction on how exactly self-distillation used in the context (e.g., with a fine-tuned teacher, then apply MBP based pruning method). Another example is in the experiments, in Table 1, it might make sense to compare knowledge distillation baseline in a  similar way to the pruning methods, i.e., using the 20% for CoLA, and 10% for others. It seems like in this case,  the proposed method performed similarly to distillation baselines.\n",
            "summary_of_the_review": "Overall, I think the proposed framework makes sense and like the insights the authors provided. However, I see this paper can be further improved on its presentation, while including more discussion on efficiency. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The proposes to combine self-distillation and magnitude pruning for training and sparsifying NLP models, such as BERT. The main novelty lies in combining these techniques in an effective manner to improve over simple magnitude pruning.",
            "main_review": "Overall, I was not particularly impressed by the paper. I think the main problem is that the paper does not propose any new techniques and instead combines existing techniques (self-distillation and magnitude pruning) to improve upon state of the art. \n\nThis in itself would also not be a problem since I believe such papers can bring high value to the community. However, if you do that, I think the setup of the paper should be different. The paper should discuss their particular design choices, why these design choices are the best, and validate their choices with clear experimental settings and interesting ablation studies. \n\nRight now, the paper is cluttered with lots of formalism (especially Section 3), makes fairly bold claims about the theoretical contributions, and tries to motivate their choices through superficially-motivated theoretical metrics such as mutual information as a proxy for \"generalization power\". \n\n\n## Strengths\n\n* Overall, the paper is well-written and can be easily followed. \n* The experiments are well designed and already provide some interesting and valuable insights. \n* Judging from the experimental results, there seems to be merit in the method.\n\n## Weaknesses \n\n1. **Theoretical discussion vs. ablation study**: The paper could be much more accessible with a clearly designed ablation study with justification for the proposed design choices instead of relying on some proxy metric inspired by some loose theoretical connections.\n\n2. **Comparison methods**: Why does the paper not compare to movement pruning (Sanh et al. 2020) in the experimental section? It takes a very prominent role in the introduction and so I am wondering why it seems to be excluded from the experiments. For example, Sanh et al., 2020, report 90.2/86.8 f1/acc for QQP on BERT with soft movement pruning and 10% remaining weights according to Table 3. In Table 1 of your manuscript, you report 83.79/86.37 on the same task with 10% remaining weights. I don't want to judge your performance on a single number but it seems this requires some more in-depth discussion.\n\n3. **Tone down introduction**: I have several issues with the abstract and the motivation around the paper. It is misleading to me.\n    1. In the abstract you compare yourself to _(6 times) larger distilled networks._ This is unfair and misleading since I assume those distilled networks are still dense and not sparse as your resulting pruned networks. \n    2. Your claims _that self-distillation (1) maximizes class separability, (2) increases the signal-to-noise ratio, and (3) converges faster after pruning steps_ help explain your performance. I would argue these metrics provide valuable insights but there are still proxy metrics. So I would tone down the claims surrounding the effectiveness of such metrics.\n    3. You claim that it is undesirable to have double the number of parameters to update as is the case for movement pruning. Your method requires self-distillation, i.e., maintaining a second network and running inference on it. Updating the scores in movement pruning is fairly cheap if you already have the weight gradients. So I really don't see any kind of difference in the complexity of your approach compared to other approaches. \n_Self-distilled pruned models also outperform smaller Transformers with\nan equal number of parameters and are competitive against (6 times) larger distilled\nnetworks. We also observe that self-distillation (1) maximizes class separability,\n(2) increases the signal-to-noise ratio, and (3) converges faster after pruning steps,\nproviding further insights into why self-distilled pruning improves generalization._\n\n## Ways to Improve My Score\n\nI would start with comparing to movement pruning to highlight that the method is indeed state of the art. Then I would try to go into more depth about the effectiveness of your method and how each aspect contributes to it instead of focusing too much on the theory. We can then continue the discussion during the rebuttal period as well. ",
            "summary_of_the_review": "The paper is interesting but does not provide a new method but rather combines existing methods. Consequently, I would like to see more clearly motivated design choices and helpful ablation studies in order to understand why this particular combination of techniques is superior to prior work or simpler approaches. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to use self-distillation to improve model pruning. They show that the proposed cross-correlation objective encourages model sparsity and naturally benefit magnitude-based pruning. The experiments show that the proposed self-distilled pruning improves generalization of the pruned language models. ",
            "main_review": "Strengths:\n1. The idea of combing pruning and self-distillation is straight-forward and interesting. \n2. The proposed cross-correlation objective is interesting.\n2. The experimental results support the claim of this paper and show some improvement over basic pruning baselines. \n\nWeaknesses:\n1. Table 1. Consider removing the KD baselines, where the comparison is not fair and delivers little information. \n2. Why the self-distillation can improve the Signal-to-Noise Ratio? SNR seems only related to the last layer output.\n3. The notation in Figure 3 needs to be improved. E.g., Only use solid line / star marker for the proposed method. & Why SDP-CC has no MBP?\n4. In figure 4, left, MBP has a lower MI compared with SDP-KLD-MBP/SDP-COS-MBP. However, the performance of MBP is worse. How to explain that?\n5. SDP-COS-MBP/SDP-KLD-MBP/SDP-CC-MBP all have similar SNR. But the performance has big difference (Table 1/2). How to explain that?  \n\nMinor:\n1. related reference: 'Prune Your Model Before Distill It', 'Knapsack Pruning with Inner Distillation'",
            "summary_of_the_review": "This paper proposed an interesting approach to combine self-distillation and pruning. The proposed method obtains a reasonably good improvement. However, the motivation of the proposed cross-correlation objective is not well explained. I also don't think the analysis of SNR and fidelity is well explained. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}