{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The given paper proposes a decision tree model which incorporates kernel density estimation (KDE) and the corresponding learning algorithm. Specifically, KDE is used to assign a weight (probability) for each root-to-leaf path and the final prediction is obtained by the weighted sum of all (or a subset of) leaves. Although authors emphasize that their method is a generalization of \"fuzzy decision trees\", the term \"fuzzy\" here is a bit confusing as the most commonly accepted name for such trees is soft decision tree.",
            "main_review": "I will break my score justification to the following categories:\n\n1. Novelty\nIncorporating KDE into decision trees by itself is not a novel idea. As it was mentioned by authors, such a model has been studied in Smyth (1995), Itani (2020), etc. In general, the method conceptually reminds soft decision trees: each root-to-leaf path is assigned a non-negative weight (probability), each leaf's output is multiplied by that leaf and averaged. Here, the only modification is that not all leaves might be employed (depending on bandwidth). In terms of training, the approach is the same as in CART (top-down tree induction based on purity criterion) but with kernel function taken into account. The proposed efficient optimal splitting algorithm is based on sorting and clever traversal over intervals but a similar approach is already used in the original CART book by Breiman.\n\n2. Experiments\nAuthors provide a comparison against CART trees (both in single tree or ensemble setting) which is clearly very simple baseline. Even in that case, the differences in RF and ExtraTrees columns are not statistically significant. This might be due to simplicity of the given datasets. In that case, I'd suggest authors to perform evaluations on somewhat larger benchmarks (e.g. original XGBoost paper uses Higgs dataset with ~10M instances). Next, the most important missing baseline is the soft DTs as they are closely related to the proposed model. I encourage to use one or several soft tree based approaches (e.g. [1]) as related baselines. Moreover, DT research is not restricted with CART or \"fuzzy\" decision trees. You may want to compare against more advanced tree learning methods that are \"non-greedy\" and generally outperform CART-style algorithms: optimal decision trees [2,3], alternating optimization of decision trees [4], etc. [5]. By the way, some of them can be included into the related work section.\n\n[1] Nicholas Frosst, Geoffrey Hinton. Distilling a Neural Network Into a Soft Decision Tree. https://arxiv.org/abs/1711.09784\n\n[2] Dimitris Bertsimas, Jack Dunn. Optimal classification trees. https://dspace.mit.edu/handle/1721.1/110328\n\n[3] Jimmy Lin, Chudi Zhong, Diane Hu, Cynthia Rudin, Margo Seltzer. Generalized and Scalable Optimal Sparse Decision Trees. https://arxiv.org/abs/2006.08690\n\n[4] Arman Zharmagambetov, Miguel Carreira-Perpinan. Smaller, more accurate regression forests using tree alternating optimization. http://proceedings.mlr.press/v119/zharmagambetov20a.html\n\n[5] Norouzi et al. Efficient non-greedy optimization of decision trees. https://arxiv.org/abs/1511.04056\n\nOther comments/questions:\n- Equations are not numbered which makes them hard to refer during review. The 2nd set of equations in p.4 starts from equality sign which is confusing. Does it equal to the above equation (which cannot be true since that eq. is defined for 1 attribute only) or just u^l? The relation of that equation with the 3rd equation in the same page (upper and lower bounds) are not well described. \n\n- Interpretability. Abstract and conclusion emphasize interpretability of the proposed model. However, this claim was not supported by experiments or by any other evidences. I'd encourage authors to re-consider this claim (or provide evidences) as I believe that the obtained trees are not trivially interpretable. Because, first of all, traditional CART trees by itself may be hard to interpret if the resulting trees are big (and deep). The proposed trees are also axis-aligned (one feature at a node) which tend to grow deep, especially with modern large-scale datasets. On top of that, there are multiple paths are possible which requires inspecting all of them. Second, according to the experiments, a true improvement comes by ensembling trees into a forest in which case the story about interpretability becomes more tricky.\n\n- I didn't quite get how bandwidth is used with traditional trees (i.e., no kernels). From my understanding, applying kernel there is equivalent to feature/threshold rescaling?\n\n- Did you report somewhere the runtime of the efficient optimal splitting vs simple enumeration? What was the avg speedup?",
            "summary_of_the_review": "The paper definitely carries some interesting ideas: more powerful tree model which leverages KDE; fast learning algorithm to train them which asymptotically improve over enumeration; using a subset of paths which is controlled by bandwidth, etc. However, I am leaning towards rejection of this paper due to shortcomings in experiments and methodological novelty (as described in the main review).",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposed a weighted approach to improve the performance of a decision tree predictor by smoothing over the tree via a kernel approach. The idea is as follows. Given a decision tree, a usual predictor conduct prediction based on the average of response $Y$ of those observation whose covariates belong to the same leaf/node/region of the tree. The proposed approach replace this local average by a weighted average where the weights are constructed by the smoothing kernel. And the final prediction is another layer of weighted average of the new local average. The authors provide an analysis of the computation complexity and describe the a simple way to compute the final estimate. Empirical analysis on benchmark datasets show the improvement of the proposed approach.",
            "main_review": "While this paper introduced a novel approach for predictions, it seems to me that some key analysis was not conducted so it is hard to gain insight from the proposed method. Here are my main comments on this paper.\n\n1. Improving the description of the method.\nThis paper introduced the tree predictor from a graph perspective and apply a smoothing. If I understand it correctly, the procedure can be equivalently written as follows: \nGiven a decision tree, the leaves form a partition of the feature space $R_1,\\cdots, R_K$, where $K$ is the total number of leaves. \nFor each observation $(Y_i,X_i)$, in the classical method, the value of response $Y_i$ only contribute to a region $R_j$ based on which region/leaf that $X_i$ belongs to. \nThe proposed method 'smooth' out this procedure by applying a smoothing kernel at $X_i$ so that there are some chances that $X_i'$ (after smoothed) may belong to other leaves (generally nearby leaves). \nThe conditional mean of leave/region $R_j$, $v_j= E(Y|X\\in R_j)$, is then replaced by a weighted average of $Y_i$ according to the above chance. \nThe final predictor is a weighted average of the conditional mean and the weight is again derived from the above probability.\nI would recommend adding alternative description of the method to the paper. This may draw new insights in to the method and may help the readers to understand the method better. \n\n2. Re-structure Section 3.1 and 3.2. \nI think some contents in Section 3.1 and 3.2 should be restructured. \n(i) At the beginning of Section 3.1, the authors should emphasized that we are assuming the tree is given and the paths refer to the root-leave path.\n(ii) The prediction $\\hat y_{\\hat f}(x)$ in Sec 3.1 involves the concepts of $v^{(\\ell)}$ but $v^{(\\ell)}$ was later introduced in Sec 3.2. \n\n3. Justification of the smoothing. \nThe key novelty of the proposed method is the use of a smoothing kernel to the tree predictor. But the authors did not provide any argument on why this is a preferred approach. Some argument or analysis should be done to justify this procedure.\nIn particular, why is the weighted estimator of $\\hat y_{\\hat f}(x)$ in Sect. 3.1 is preferred over the usual tree predictor?\nThe usual conditional mean has a simple justification that it is the minimizer of the MSE. But when we add weights to it, the resulting predictor will not be optimal under MSE. So it was not clear why this weighted approach is preferred.\n\n\n4. Justification of the weighted estimator $v^{(\\ell)}.$\nIn Sec. 3.2, the authors recommend using a weighted estimator of the conditional mean. But why is this weighted conditional mean a better estimator than the usual conditional mean estimator (without smoothing, it is just simple sample average within the leave/region)?\nThe usual tree method has already done a smoothing based on the region of leaves (so any point $x$ in the same leave will contribute the same to the prediction). Adding an additional layer of smoothing seems to be redundant and could introduce additional bias.\n\n\n5. The power of product kernel. \nTo me, the idea of using a product kernel shows a benefits of computation in the current paper. The authors may want to highlight this contribution. There are other smoothing kernels (such as radial basis kernel) for multivariate features but they may not have the same computational benefits as the product kernel.\n",
            "summary_of_the_review": "Overall, while this paper has some novelty in the combining of a smoothing kernel and tree predictor, a couple of key analysis is missing in the paper. It was not very clear to me why the weighted approach should be a preferred approach. The use of tree itself has already introduced the concept of smoothing, so adding another layer of kernel smoothing may introduce more bias in the output.",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a kernel density decision tree that learn a fuzzy tree with kernel density estimation-based decision on the leave. ",
            "main_review": "- The paper is hard to follow. It is written in a way that is suitable for implementation rather than showing main ideas. I'd suggest to describe it in a top-down manner and specify where the ideas are and how the problems are solved. \n- I could not see how the threshold candidates are generated in the paper. I guess that the paper requires a detailed knowledge of decision tree implementation to follow.\n- I specifically looked for the form of the KDE on each leaf node and could not find it. \n- As the paper is about DT construction, it is better not to talk about how to predict (section 3.1) before constructing the trees.",
            "summary_of_the_review": "I find the paper is hard to follow and could not verify its claim in a few reads of the paper.",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies a supervised learning algorithm for trees based on the kernel density estimator. The proposed method is formulated as the fuzzy decision tree (FDT). An efficient algorithm of finding the splitting point is theoretically justified. Numerical experiments using standard datasets are conducted to show the proposed method's effectiveness compared to existing tree-based learning algorithms. \n",
            "main_review": "This paper is clearly written. A theoretical justification provided in Theorem 1 is interesting. However, the authors did not provide theoretical insight on statistical properties, such as the robustness of the proposed method. In Section 3.3.1, KDDT is formulated as an FDT with the specified splitting function. Based on such a result, The authors could investigate more detailed statistical properties of the KDDT.\n\nIn numerical experiments, the standard datasets for multiclass classification problems were used. In addition to that experiment, showing the robustness properties against contaminated data would be nice. ",
            "summary_of_the_review": "Overall, This paper is clearly written. However, theoretical insight and numerical experiments were not enough to justify the usefulness of the proposed method. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}