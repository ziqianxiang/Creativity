{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work focuses on the uncertainty estimation to help training with the most/least uncertain samples in the limited supervision cases. The supervision cases considered in the paper are semi-supervised learning, active learning, and one-bit supervision. To better estimate the sample uncertainty, the work proposes a novel method that measures the vibration of the sequential data. The sequential data comes from a model with invariant structure but different parameters from different converged epochs by Fourier Transformation. The experiments are conducted on three datasets and three tasks under the limited supervision setting.",
            "main_review": "The stengths and weaknesses are concluded as follows:\n\nStrengths:\n1.\tThe proposed method is novel using Fourier Transformation to measure the sample uncertainty in the limited supervision.\n2.\tMost of the paper is easy to follow in terms of writing.\n3.\tThe experiments are comprehensively designed in three datasets and three tasks. Plus, the parameter sensitivity analysis of $alpha$ in Eq. 6, $mu$ in Eq. 5 and start epoch is compared.\n\nWeaknesses:\n1.\tSome assumptions should be justified or analyzed. For example, “the instantaneous probabilities often provide inaccurate estimation to uncertainty” in P.2 of Sec.1  is a very strong assumption and cannot be true. Besides, the four examples shown in the Figure 1 are specific cases and it is better to give statistics about the numbers of the true cases and false cases in a general case, such as average probability sequence of all training samples in a small dataset.\n2.\tSome introduction of the previous works need suitable citations, such as “consistency-based approaches” in the P.3 of Sec. 1 and “one-bit” supervision in the P.3 of Sec. 1.\n3.\tSome details need more clarification. For example, the Eq. 1 should have “q” in the first item or write the two items separately, as only showing the “q” in the second item misleads that the “q” will not help the training of “f”. Besides, “the iterations of stochastic gradient descent equal to a stochastic process ” in the paragraph between Eq. 2 and Eq. 3 is confusing by what “stochastic process” is. Moreover, the “predictive distribution” in the paragraph below Eq.3 is not clear in terms of the owner of distribution. Moreover, the “more accurate” has no comparison in the paragraph under Eq. 5. Plus, it is better to clarify the “consistency-based methods”. Though the work has its explanation the P.1 of Sec. 3.4, it is still unclear about the concept. Finally, the “By checking the ground-truth” … is unclear to me in the last paragraph of Sec. 3.\n4.\tThere are little typos, such as “we we” in the Sec. 3.3, “be be” in the Sec. 5.\n5.\tIt is better to answer how to choose a range of converged epochs and how to sample the some epochs from the range.\n",
            "summary_of_the_review": "The work focuses on the uncertainty estimation to help training with the most/least uncertain samples in the limited supervision cases. The proposed method is novel by measuring the uncertainty that is the vibration of the sequential data. And comprehensive experiments are conducted. But the work has some assumptions that are too strong to believe. And several details need to be clarified for better understanding the significance of the work. If the work revises, verifies, or analyzes its assumptions listed in the “weaknesses” part and gives the clarifications of the details listed in the “weaknesses”, I will improve my score.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors proposed to use the snapshots of models captured at the latter training epochs for uncertainty estimation. Specifically, the maximum predicted probability over these training epochs forms a sequential data per input sample. The authors proposed to use Fourier transform to analyse the dynamics of the maximum predicated probability sequence. The ones with high fluctuation and/or low baseline are identified as high uncertain samples. This information is then further exploited for semi-supervised learning, active learning, and one bit supervision.",
            "main_review": "It is interesting and novel to use the Fourier transform to analyse the maximum predicted probability over the last training epochs. However, the analytical and empirical results in this work would require further consolidation. \n\nOn the algorithm side, the motivation is clear, but the formulation (5) is rather heuristic. For instance, beside subtraction, one could alternatively use the ratio between high frequencies and DC. For the bit flip case, an interesting comparison would be against the count of bit flips. Further empirical analysis/ablation on the form can make the proposal more convincing. Also, it would be beneficial to have some visual plots of the Fourier spectrum for representative cases, e.g., Fig. 1. \n\nDespite using Fourier transform seems to be novel, tracking the training dynamics is a known technique, which has different use cases. The authors should also compare with this line of work. For instance, in the work https://proceedings.neurips.cc/paper/2020/file/c6102b3727b2a7d8b1bb6981147081ef-Paper.pdf, the authors tried to monitor the training dynamics for noisy label identification. Noisy labeled data is essentially one type of out-of-distribution data.\n\nOn the experiment side, the baseline methods are not SotAs. For instance, the methods in Tab. 1 for semi-supervised learning are quite old. For active learning, K-center and confidence based ones are definitely not strong. paperwithcode leaderboards can be a good source to look for strong baselines. Comparison with strong baselines is important to promote the proposed method.\n\nOne important aspect in the proposed method is the choice of the starting epochs. However, from Fig. 4-a) and -d), there is no consistent observation for different datasets. In Sec 3.2, the authors motivated the use of latter epochs, which however is not reflected in Fig. 4-d), where the starting epoch equal to 0 is nearly as good as the one equal to 80. Also, the authors only showed the performance till 80 epochs, while the total epochs is 180. Furthermore, it is interesting to see how the learning rate adaptation correlates with the choice of the starting epochs. \n\n",
            "summary_of_the_review": "Overall, I like the idea of using Fourier transform. However, my decision is reject as I believe this work requires improvement.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper is about an uncertainty quantification method for semi-supervised and active learning. The overall motivation of this paper is to learn from less data and with limited supervision.\n\nThe authors propose a method to estimate uncertainty from \"vibrations\" in the predicted probabilities (maximum one) in a sequence of epochs, then fourier transform is applied to this sequence, in order to extract frequency information, from where an uncertainty metric can be derived.\n\nAdditionally this uncertainty metric is combined with vibration of the sequence indicating changes in the predicted class label, as a way to improve performance.\n\nThe main contribution of this paper is the vibration method to estimate uncertainty/difficulty/etc from a sequence of probabilities during training.",
            "main_review": "Strengths\n\n- The idea seems to be novel, using training sequence information to infer uncertainty makes sense, and I believe that it is novel. I see some parallels to Arriaga and Valdenegro's work (\"Unsupervised Difficulty Estimation with Action Scores, 2020\") about action scores where training loss is combined to produce a difficulty score, but this work aims to estimate uncertainty.\n- There seems to be an improvement in semi-supervised and active learning on the evaluated datasets (CIFAR10/100, Mini ImageNet) in comparison to the selected baselines, but I comment in weaknesses that the baselines are not robust.\n- For semi-supervised learning, the technique does scale with an improvement in larger datasets like ImageNet.\n- The evaluation that was performed for semi-supervised and active learning seems to be correct.\n- The authors provide ablations (Figure 4) to justify the selection of hyper-parameters for the proposed method.\n\nWeaknesses\n\n- The paper is difficult to understand at some points, specially in the introduction. There are some incorrect statements or unfounded claims, like:\n    * \"predictive probabilities (Lewis & Gale, 1994) and the entropy (Wang et al., 2016), usually fail in estimating the uncertainty for the out-of-distribution data\" (not true generally, depends on model, Bayesian NNs can do OOD detection using probability and entropy).\n    * \"We develop a Gaussian approximation to connect the model optimization to the Bayesian procedure\" (I argue below that there is no such connection to a Bayesian model).\n    * \"To satisfy the requirement of Bayesian approximation\" (What requirement? This is not clear to me).\n    * Overall there is an excessive use of the article (\"the\") where it is not needed, for example:  measuring the uncertainty -> measuring uncertainty, the maximum predictive probabilities, the entropy -> maximum predictive probabilities, entropy, Supposing we have the sequence -> Supposing we have a sequence, etc.\n\n- The authors claim that this method is Bayesian, with an argument that SGD iterations are a Ornstein-Uhlenbeck process, but this applies to the weights, not to the predictions which are the ones used to compute vibration. I believe this is a very weak argument that the proposed method follow some Bayesian computation.\nThere is no relation between equations (2) and (3), where the authors argue that a Gaussian approximation to the posterior is made, but this does not follow from the math. I would recommend to completely remove Sec 3.2 from the paper (except for the last paragraph).\n- No comparison with other uncertainty estimation methods. The only baselines used for comparison are other semi-supervised learning methods, and while the paper mentions common uncertainty methods like MC-Dropout, these are not used as additional baselines. I would have expected a comparison with MC-Dropout and Deep Ensembles at least, specially in the case of active learning.\n- No metrics that measure uncertainty. The authors only consider the case of semi-supervised and active learning, and do not analyze what their proposed method actually produces, is it uncertainty or something else? I would have also expected to produce calibration plots and expected calibration error, to show that the probabilities are proportional to incorrect pseudo labels being produced. Part of this is detailed in Fig 2 with a accuracy vs confidence curve.\n- No details on out of distribution performance. In the introduction it is argued that predictive probabilities and entropy do not work for out of distribution data (which in general is not true), but there is no further proof or experiment that deals with this issue in the proposed method. This significantly weakens the Introduction and Conclusions sections that mention OOD data.\n- I believe that Eq (5) is weakly motivated, for example the authors say \" We believe that utilizing equation 5 to estimate the uncertainty will be more accurate\", but why is there a need for belief here, and not use an experiment to show that is indeed more accurate in terms of uncertainty? What alternatives are there to extract an uncertainty metric that is based on vibration?\n- For the active learning case, I believe that the baselines are too weak. Since active learning requires precise estimations of epistemic uncertainty, robust baselines that are able to estimate epistemic uncertainty should be used, like MC-Dropout/DropConnect, Ensembles, Bayesian NNs, etc. For a small example on baselines for active learning please consult Sec 4.5 of \"SDE-net: Equipping deep neural networks with uncertainty estimates\" by Kong et al. ICML 2020.\nOutside of uncertainty methods, the authors should compare to state of the art active learning methods from the last years, for references please consult \"A Survey of Deep Active Learning\" by Ren et al. 2020. For active learning, I think comparison with multiple standard epistemic uncertainty quantification methods should be enough.\n- I believe that the baselines for semi-supervised learning are also too weak. There is no comparison in this work with Cascante-Bonilla et al. 2020 which is part of the references, and some comparison baselines in Cascante-Bonilla's work obtain better performance, for example Sohn et al. 2020 in CIFAR-10 with 4000 labeled data obtains 4.26 error rate, and curriculum learning obtains 5.92 error rate in the same setting with a different network. These comparison are important to determine progress with respect to the SOTA.\n\nMinor Issues\n\n- In Sec 3.3, I would recommend to include plots to show how the fourier coefficients look like for a selection of probability patterns (this could be done in the supplementary) for the reader to understand how they represent low and high frequency components. It could also help motivate Eq (5).\n- The paper does not have a supplementary, it is good practice to include additional results or ablations in a supplementary, in order to free space in the paper for additional analysis and argumentation. For example I would suggest further analysis of uncertainty quantification using standard metrics like expected calibration error or out of distribution detection performance.\n- Which dataset is used to produce the results in Figure 2?\n- Overall this method seems to be specific to classification (this should be mentioned in the paper), and the authors do not have an alternative formulation for regression.\n- It is recommended that the end of the introduction should include a clear contributions statement, which is missing.",
            "summary_of_the_review": "I believe that the paper is not ready for publication at this time, due to the many issues I have mentioned in weaknesses, specially considering the unproven claims, weak baselines, and evaluation of uncertainty estimation.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}