{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the memorization overfitting problem in meta-learning from a causal perspective. It provides explanations to current solutions and proposes to update the meta parameters by maximizing the interventional distribution $p(\\theta | do(\\theta'), S, Q)$. It provides an identification strategy for the interventional distribution by the front door criterion.",
            "main_review": "Strength:\n\n- The view of this paper is new. It is interesting to study meta-learning and memorization overfitting from a causal perspective, which is novel. \n\n- The proposed solution is simple and easy to implement.\n\n- The empirical studies cover four representative examples.\n\nWeakness:\n\n- The connections with existing literature are not explained clearly enough. The memorization overfitting is known as a problem when a single model can solve all the training tasks without using the task support data. This paper describes the memorization as a problem that the ideal target  $\\max p(\\theta | \\theta', S, Q)$ differs from the practical target $\\max p(\\theta | \\theta', S, Q)$. What is the translation between the two descriptions of the same problem?\n\n- The causal graph Figure 1(b) is not very correct with some missing pieces. The current graph implies the query set Q and the support set S are independent, and $\\theta'$ and $\\theta$ are independent given $Y$, which are not correct.\n\n- In a usual causal graph, the variables are probabilistic. But in the graph of MAML, many relationships are deterministic. Can the author provide a discussion on the probabilistic and deterministic causal graph and also the references for the frontdoor criterion to be applied in the deterministic case?\n\n- Eq (1) and (2) imply that $p(\\theta | do(\\theta'), Q) = p(\\theta | do(\\theta'), S, Q)$. By the do-calculus, does it mean $\\theta$ independent of S given $\\theta'$, Q in the graph $G_{\\bar{\\theta'}}$? But Figure 1(b) does not imply this conditional independence.\n\n- Some simulation results are not aligned with the previous works. In sec. 5.1 and sec 5.4., the performance of standard MAML is very good. This contradicts with the results in (Farid & Majumdar, 2021) (Rajendran. et al. 2020) (Yin. et al. 2019) that the performance of standard MAML will deteriorate significantly when the tasks are not mutually-exclusive.  \n\n- The sinusoid regression in sec. 5.1 seems not a non-mutually-exclusive version that will induce memorization overfitting. \n\n\n\n\n\n\n\n\n",
            "summary_of_the_review": "This paper provides a novel perspective on the meta-learning and task-level overfitting. However, the reviewer think the motivations need further explanation, the causal graph and identification strategy need scrutiny, and the experimental settings need to be double checked. I would like to hear author's feedback.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the problem of memorization overfitting in meta-learning and proposes to construct a causal graph for gradient-based meta-learning. From a perspective of causal graph, this paper demonstrates how existing methods solve the memorization problem. Further, it proposes a novel causal intervention principle to debias the spurious correlation. Specifically, two implementations of the proposed principle are conducted, i.e., by sampling multiple versions of the meta-knowledge via Dropout and grouping the meta-knowledge into multiple bins. Experiments on four benchmark datasets demonstrates the effectiveness and compatibility of proposed algorithm.",
            "main_review": "Pros:\n1.\tThe paper is well written and the intuition is demonstrated well;\n2.\tThe experiments are conducted in four benchmark datasets, which validates the proposed approach sufficiently;\n3.\tThe causal perspective in meta-learning opens a new door to improving meta-learning.\n\nCons:\n1.\tThe description of the proposed algorithm is not clear in the main body. I suggest that the pseudo-codes in the appendix is placed to the Section 3;\n2.\tAlthough the intuition of proposed approaches is clear and demonstrated well, the improvements are incremental in contrast to MetaMix, which weakens its effectiveness;\n3.\tPlease indicate the reference of claims “In meta-learning, there are two types of overfitting problems:” In section 2.1.\n",
            "summary_of_the_review": "To conclude, the proposed perspective of causal graph in meta-learning seems interesting and worth to exploring. I believe if the performance of the method could be improved further, this would initiate more researchers’ interest.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper addresses the undesirable memorization problem in the context of gradient-based meta-learning, where the meta-learning knowledge obtained as a result of memorizing all meta-training tasks yields poor generalization and is detrimental to task specific adaptation. While previous work has focused on regularization-based and augmentation-based solutions to prevent the memorization of all meta-training tasks, the authors argue these methods and their benefits are still not well understood and propose a causal perspective of meta-learning in a unified causal framework. In this context the authors identify the universal label space of the base model as the confounding factor of memorization causing spurious correlations between initializations learnt in different meta-training steps. Informed by this analysis, the authors demonstrate why existing meta-learning overfitting solutions work, and propose two deconfounder approaches to address the issue of memorization, namely i) sampling multiple versions of the meta-knowledge via dropout (MAML-Dropout), and ii) grouping the meta-knowledge into bins (MAML-Bins).   ",
            "main_review": "Strengths:\n\nThe paper addresses an important issue (poor generalization due to undesirable memorization overfitting in meta-learning) and aims to clarify why existing solutions work, which is an important gap lacking solid theoretical understanding in the literature.\n\nThe novelty of the paper lies in the causal perspective the authors adopt for understanding gradient-based meta-learning, bridging together the separate research areas of causal inference and meta-learning. \n\nThe proposed causal graph framework allows authors to identify the universal label space as the confounder behind undesirable memorization. In the context of their proposed causal framework, it is possible to explain from the perspective of deconfounder approaches why existing regularizer-based and augmentation-based solutions work. \n\nThe argument that the universal label space Y leads to spurious correlations is well-argumented and convincingly explained in the context of the causal framework. The change in semantic meaning of task labels that vary from task to task is indeed problematic and additionally affects labels Y. \n\nThe authors break the path from \\theta’ (confounded past meta-knowledge) and the task adapted model \\phi through frontdoor adjustment deconfounding in the causal graph and propose two simple methods to implement this approach (MAML-Dropout and MAML-Bins)\n\nTheir proposed deconfounding methods are compatible with and work on top of existing meta-learning algorithms, displaying improved performance\n\nThe paper is well-motivated, well-framed in the context of the literature, informative and well-written; figures serve to illustrate the content and arguments of the paper.\n\n\nWeaknesses:\n\nNot clear from the paper itself to what extent the current work is different from past works of Bengio et al (2020) and Yue et al (2020) that also combine meta-learning with causality, the authors should aim to better clarify the differences. In addition, the authors do not clearly describe the difference between MAML-Dropout and DropGrad (Tseng et al, 2020) - is it just adding dropout layers in the forward network in the meta-training phase? \n\nThe paper lacks important details about the choice of hyper-parameters, in particular:\na) How do you determine the dropout rate for MAML-Dropout in your experiments?\nb) How do you determine the ideal number of bins to split the value range of training data into for MAML-Bins?\n\nWhat is the additional computational overload caused by the introduction of clustering in the MAML-Bins inner step? \n\nIn their experiments the authors select which method to report results for (whether MAML-Dropout, MAML-Bins, or their combination) rather arbitrarily without clear motivation and comparison among these methods. For example, in Sections 5.1, 5.3 and 5.4 the authors claim that  both methods have positive effects and that using them together (MAML-Dropout + MAML-Bins) achieves the best performance. However they contradict this statement in Section 5.2 where they only report results for MAML-Dropout claiming MAML-Bins brings additional noise.\n\nFigure 3: how do authors explain that MAML-Dropout is in general outperforming MAML-Bins? There is no discussion included in the paper about this.\n\nLong-range question: What insights can be drawn from this study that could be useful in combating learner overfitting, and how to leverage those?\n\nTypos: \nSection 2.1: “​​In outer-loop optimizaiton,”\nSection 3.2: “We details the auxiliary task in Appendix C.”\n\n\n",
            "summary_of_the_review": "The paper is generally sound and convincing, and addresses an important problem in the literature. Combining meta-learning with causal inference to combat memorization overfitting seems a promising approach that can alleviate shortcomings of existing meta-learning methods and opens new research directions. The claims are generally well-motivated both theoretically and empirically. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper aims to battle memorization overfitting problem in meta learning using causality. The authors view the process of meta-learning through the lens of causality which produces several causal graphs i.e. Fig. 1(b) for the general one and Fig. 2 (a-d) for special cases. Based on the causal graphs, the authors use 'front-door' adjustment and propose two algorithms MAML Dropout and MAML Bins. ",
            "main_review": "The memorization overfitting problem is worthy investigating, and using causality for meta-learning makes sense. The experiment is good. \n\nThe meta-learning procedure is a dynamic process, and thus it might be better to use a temporal model instead of a static model. \n\nThe authors stated that 'directly cutting the causal relationship between $Y$ and $\\theta'$ via backdoor adjustment is impossible', and then 'Despite this, we still propose two kinds of deconfounded methods applying to MAML'. That sounds self-contradictory. \n\n---Post-rebuttal---\nI am happy with the responses from the authors and have increased the score.",
            "summary_of_the_review": "I think this direction is worthy exploring. The paper's idea is interesting and the experiment results are promising. The two algorithms can be better motivated and explained, which can perhaps be addressed in the camera-ready version. I am leaning towards accepting the paper. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}