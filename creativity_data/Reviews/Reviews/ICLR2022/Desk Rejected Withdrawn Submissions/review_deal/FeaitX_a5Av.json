{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a fairly general approach to generating from language models: words are sampled from a renormalized distribution consisting of the smallest set of words in the vocabulary which together have probability at least $\\rho$, where $\\rho$ is a function of the entropy of the distribution. The authors show that several standard decoding algorithms arise from particular choices of $\\rho$. The authors then propose to use a particular $\\rho$, essentially $\\tanh(\\text{entropy}/\\text{average batch entropy})$, which they argue is attractive because it has no hyperparameters. The authors evaluate the generations obtained from decoding with their approach by comparing them to baseline generations in terms of perplexity assigned to generations by GPT-2, self-BLEU, and other recent automatic metrics.",
            "main_review": "Strengths:\n- The paper is an engaging read.\n- The core idea of being more conservative when the entropy of the word distribution is low and less otherwise seems reasonable.\n\nWeaknesses:\n- It's rather strange to have a decoding algorithm depend on the batch and batch size. (Indeed, this is especially strange given the authors' emphasis on constructing a human-like decoding algorithm, and on not having any hyperparameters).\n- The experimental procedure and results are rather unclear. First, despite the decoding algorithm depending on the batch, I don't think the authors mention what batch size they use, or what the results look like for different batch sizes. It's also unclear how the authors chose which  results should go in Table 1. In particular, although the authors apparently tried using p=0.9 for nucleus sampling (Section 4.1) these results are not in the table. Furthermore, the authors appear to neither try nor report the results for the best setting of p in nucleus sampling (p=0.95)  according to the original paper (Holtzmann et al., 2020).\n- The paper is missing a human evaluation of generation quality, which did, for instance, appear in the Holtzmann et al. paper.",
            "summary_of_the_review": "The paper proposes an interesting idea, but the experimental evaluation is incomplete and not well described.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper concerns decoding from language models. The authors give a formulation that contains several decoding as special cases, and a sampling algorithm which uses entropy averaged over a batch of sequences.",
            "main_review": "**Clarity**. The paper's clarity is below the bar for ICLR. It contains many:\n- Unsubstantiated claims (e.g. \"virtually all decoding methods currently developed are pragmatic to address the text degeneration problem\").\n- Verbose presentations (e.g. sections 2.1 and 2.2 take up 3 pages but don't introduce anything new).\n- Hand-wavy, imprecise explanations (e.g. \"In communications, people prefer to sample words from this subset rather than searching from the whole dictionary. This personalized sub-vocabulary is essential to make up distinct oral and writing styles of human communications\").\n\nIf needed, I can provide more examples.\n\n**Theoretical significance of method**. While it is indeed possible to view existing algorithms as special cases of equations (8-10),  we do not get much insight from doing so (or at least the paper has not articulated these insights). This aspect is important since there are many possible generalized equations that yield existing algorithms as special cases, so we need to know why this one is notable.\n\n**Method**. The use of entropy is not well-motivated by the authors (and this choice impacts the resulting formulation). The statement \"we prefer to use entropy since variance is relatively harder to control because of its unlimited value range\" does not provide enough justification. Second, the dependence on the contents of the batch and the batch size is puzzling (equation 15). Does the proposed method not work with batch size 1? How does the sampling change as the properties of the batch change (e.g. a batch of \"similar\" sequences versus \"different\" sequences)?\n\n**Experimental quality and empirical analysis**. The empirical analysis and experimental quality are below the bar for ICLR. ",
            "summary_of_the_review": "While the idea of a generalized stochastic decoding algorithm that does not require hyperparameters is interesting, the presentation, contributions, and empirical analysis in this paper would need substantial improvements for me to argue for acceptance.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper deals with decoding algorithms for neural text generation. The paper proposes a way to connect greedy decoding, beam search, (ancestral) sampling, top-k sampling, top-p (nucleus) sampling. The proposed GSD is on page 6 (specifically, equations 8, 9, 10). The visualization for how equations 8-10 tie the decoding algorithms together is in Figure 1. The authors claim that this generalization can provide an “infinite space” for designing more decoding algorithms. Empirical results show that open-domain text generation from GPT-2 using GSD results in a perplexity that’s closer to the human perplexity, and a self-BLEU that’s closer to the human self-BLEU. \n",
            "main_review": "I need to acknowledge that Equations 8-10 which tie the decoding algorithms are clever. Yes, using different $\\tau$ in GSD might give us more decoding algorithms. \n\nThe background introduction (although long) is great, so that readers who don’t have an in-depth knowledge in text generation decoding algorithms will be on the same page. \n\nIt’s not clear why we need a hyperparameter-free decoding algorithm (and even if the authors' motivation convince us to use hyperparemter-free decoding algorithm, related works are not mentioned). Moreover, the authors show that the perplexity for generated texts is close to the human perplexity. But we can manipulate the temperature in ancestral sampling, top-k, top-p, or the k in top-k, the p in top-p, or we can even create some hand-crafted decoding algorithm on our own to generate a perplexity that’s close to human texts’. This doesn’t mean that the decoded texts are “high-quality.”\n\nOn page 4, the motivation is that “everyone has a preferred way to speak and has a unique subset dictionary to realize this personalization” but this is not necessarily true in all text generation tasks. We want diversity in some tasks (dialogue, open-domain text generation) but for some other tasks like machine translation, summarization, and table-to-text summarization, one good output is enough. Related work for “generating one good output is enough”: https://openreview.net/pdf?id=RovX-uQ1Hua. So it’s not clear if GSD will be useful in those tasks. \n\nRelatively minor:\n- On page 5, the definitions of top-p decoding could be incorrect in certain corner cases. For example, let p=0.1, and suppose the token probabilities for the top 6 tokens are 0.15, 0.14, 0.13, 0.12, 0.11, 0.10. The authors’ definition allows us to choose any of the 6 words, but we should instead choose the top-probability token. To be fair, this scenario probably doesn’t appear too often in practice. \n\nThe writing could be more precise and clearer. For example…\n- In the abstract, “why text generated from these algorithms are divergent” -- it’s unclear what “divergent” means.\n- In the abstract, “novel implementation with a distinctive core” -- unclear what “core” means.\n- On page 2, section 2.1, what’s $\\delta$? It’ll be great if the authors can immediately explain what this new symbol refers to after mentioning it. \n- For Figure 2, it’s not clear what the context is. Is it a cherry-picked example? Is there ever <eos>? More explanations of what’s going on will be great. \n- In Eq. (2), it’ll be best if the authors can immediately describe what kind of object $\\phi$ is. \n- In Eq. (8), it’s helpful to write what’s under the summation sign, and expand P, given that it’s the few most important equations in the entire paper. \n- On page 8, do you mean Figure 3 instead of Figure 4.2?\n\nSlightly confusing: on top of page 2, the authors say “we rarely ask God to configure us with a hyperparameter before uttering individual words.” But then the authors also claim that “everyone has a preferred way to speak” and “it usually depends on personal habits of language use.” So this sounds like a hyperparameter to me? Why do we need a hyperparameter-free decoding algorithm?\n\nThe logic on page 3, in the “another deterministic decoding algorithm is beam search” paragraph is a bit incorrect in my opinion. The authors claim that given that we cannot find the optimal path (i.e., search error), beam search leads to degenerations. But the issue might be modeling errors as well. For example, see Murray and Chiang (2018): https://aclanthology.org/W18-6322.pdf, and a more recent paper: https://aclanthology.org/D19-1331/. Beam search with very large beam size could generate empty translations. As an aside, Welleck et al. (EMNLP 2020), Consistency of a Recurrent Language Model With Respect to Incomplete Decoding, is also a related paper addressing the repetition issue; it shows that greedy, beam search, top-k, top-p can all generate repetitions and proposed decoding algorithms to avoid the repetition issue.\n\nRelatively minor:\n- Page 1, line 4-5 of the introduction, the authors forgot to mention nucleus/top-p sampling before citing the paper\n- When the authors are citing multiple papers consecutively, it’s best to do (xx et al.; yy et al.; zz et al.) instead of  (xx et al.) (yy et al.) (zz et al.)\n",
            "summary_of_the_review": "Equations 8-10 which tie the decoding algorithms together are clever. However, it's not clear why we need a parameter-free decoding algorithm from the authors' discussion (and to be fair, $\\tau$ is still like a parameter), and it's not clear how more algorithms created from GSD are useful/good. The result that we can generate texts with a similar perplexity as humans' does not prove that the generations are \"high-quality\" and can be adapted to other tasks. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work introduces the generalized stochastic decoding (GSD) framework and the subsequent Intrinsic Decoding algorithm. They formally show how several existing stochastic decoding algorithms can be viewed as special cases of GSD. Further they perform empirical comparison of sequence completions between their Intrinsic Decoding and other algorithms paired with GPT-2 using automatic metrics. They suggest that the GSD can be seen as a platform for future research around stochastic based decoding strategies.",
            "main_review": "Here I will concentrate on the weaknesses as I believe they outweigh all strong points which exist too.\n\n1. Important related work is ignored. This paper discusses both generalization of stochastic decoding strategies and a new 'hyper-parameter free algorithm'. Both of these directions have been investigated here: https://arxiv.org/pdf/2009.07243.pdf, https://arxiv.org/pdf/2007.14966.pdf. This observation alone weakens the strong points of this work as I believe authors indeed may have been unaware of this existing work. This work may be significantly improved if detailed comparison with existing work would be provided. Frankly speaking, I was surprised to see that this paper does not have related work section / paragraph.\n\n2. The evaluation setup does not look strong enough in order to make claims about the efficiency of Intrinsic Decoding. First, no human evaluation was done. As you can see in my detailed feedback part, authors had *many* statements about 'how humans communicate' etc. but they have not done any evaluation involving human judgement. In addition to this, existing algorithms like Mirostat which controls the uncertainty of token-level distribution need to be included in the experiments. Second, authors checked only one single model in one task which was sequence completion. There are many other options which have well settled automatic metrics: machine translation, question answering and so on.\n\nThe strong point of this work is the GSD framework itself. I believe authors are able to create better-motivated task conditioned algorithms based on it. Unfortunately, the framework itself can not be directly applied to any specific task / goal so more efforts are certainly required there.\n\nBelow I provide more detailed feedback about specific parts of the paper, I hope it can be helpful to authors in order to refine this work.\n\npage 1, \"Neither diversity nor fluency is a pre-specified\": If we go in this area, then I could imagine a person to condition the style based on the listener at least on the level of vocabulary and diversity.\n\npage 2, \"we rarely ask God to configure us with a hyperparameter before uttering individual words.\": It is not clear what is the purpose of this statement. When it comes to the hyper-parameters of the decoding algorithms -- they are introduced as constraints on the sequence/token-level distribution of the model.\n\npage 3, \"Thus, it has been widely used in many generation tasks until now.\": As far as I know, greedy decoding as choice of the final decoding strategy is not used as much beyond low uncertainty tasks e.g. grammar error correction. Provide references to make this statement strong.\n\npage 3, \"Theoretically, only when its search width hyper parameter w is equal to the size of the dictionary can it be ensured that the paths sought by Beam Search include the globally optimized solution.\": This looks wrong: beam size converging to infinity would correspond to the global exhaustive search.\n\npage 3, \"Empirical results in previous works (Welleck et al., 2020) have shown that text generated by Beam Search is prone to text degradation.\": I believe the major argument from this work was about the issue of modeling error which may lead to degeneracy with beam search. That is why in the end they showed how better modeling improves beam search predictions in terms of repetitions.\n\npage 3, \"both Greedy Decoding and Beam Search are under the same paradigm\": Afaik sometimes it is called approximation of the MAP decoding.\n\npage 3, \"probabilities assigned by Greedy Decoding and Beam Search are pretty different from the distributions of human language\": Example with one specific sentence doesn't help, but may be deceptive. Is there a way to quantify such 'trivial peaks' on the corpus level? I guess what you meant is not 'the distribution of human language', but 'from model probabilities of the samples obtained from data distribution'? \n\npage 3, \"it seems deterministic strategy may not be the best answer to the question of how to get x\": Why? It only shows a single example without any details on how the model was trained and so on. Perhaps it is possible to support such statement with more details.\n\npage 4, \"Given these observations of how humans communicate\": I believe one of main reasons behind truncated sampling based algorithm was to suggest an interpolation between MAP based algorithm and the ancestral sampling. ''How humans communicate' and so on does not sound as verifiable statements. This sounds specifically speculative here given that this work does not do any human-involved evaluation.\n\npage 5, \"A benefit of defining ∆ in such a way\": about eq5: should delta/k be part of the \\phi arguments since it is used as part of the definition? In addition, what is the value of numerator if x is not in delta? Usually in top-k it is 0, this definition would leave it as is.\n\npage 5, \"Now, it seems that all problems have been settled.\": What problems? Such statements do not help me to understand what problems are solved.\n\npage 5, \"Except for Sampling Decoding and Greedy Search,\": Greedy search is a special case of beam search so it has the same hyper-parameter. Ancestral sampling (as you write it sampling) is indeed different because it provides unbiased sample from the underlying *sequence-level* distribution defined by the generator.\n\npage 5, \"we first propose a more generalized decoding framework referred to as GSD\": Were you aware about this work? https://arxiv.org/pdf/2009.07243.pdf. They study properties of existing sampling algorithm and unify them under the common conditions. I believe it is not only worth citing, but more importantly comparing with it.\n\npage 7, \"While different from all the other methods, Intrinsic Decoding is hyperparameter-free.\": Were you aware about Mirostat algorithm (https://arxiv.org/pdf/2007.14966.pdf)? They use similar uncertainty based computation at each step to compute k for top k sampling. It reminds me a lot your intrinsic decoding method.\n\npage 7: \"By setting up different conditions, our framework provides infinite space for new decoding approaches.\": Formally there are always infinitely many ways to construct the decoding algorithm, but the question is why the given algorithm makes sense conditioned on task or performance measure. I believe in your case the choice of tau may deviate the generator towards different properties on sequence level. This should be discussed in detail.This again reminds me the Mirostat paper.\n\npage 9: \"To the best of our knowledge, this is the first work developing a unified method to bridge existing decoding algorithms using formal theorems and concrete implementations.\": I am afraid this is not true as I mentioned before: https://arxiv.org/pdf/2007.14966.pdf describes the sampling algorithm based on topk where k is computed based on 'surprisal' (similar to uncertainty in your case), https://arxiv.org/pdf/2009.07243.pdf unifies existing sampling algorithms rigorously using specific set of constraints.\n\n",
            "summary_of_the_review": "Given the major strong weaknesses of this work I do not recommend to accept it in the current state. Nonetheless, I believe authors did decent work assuming they were not aware about multiple highly relevant existing work. I think following major parts are missing in this work which makes it hard be accepted: (1) detailed comparison with existing work, (2) experimental setup checking multiple tasks and targeting specific research questions.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}