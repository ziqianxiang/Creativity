{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents batch-softmax contrastive loss which selects hard negatives as in-batch negatives and comparing question to answer only rather than question to question and answer to answer.\nResults show that the proposed objective could achieve better performance than baselines in datasets that require semantic similarity.",
            "main_review": "Strengths:\n- S1 - The idea of batch contrastive learning is interesting. Unlike SimCLR (Chen et al., 2020) and SimCSE (Gao et al., 2021), the loss is more focused on sequence classification tasks.\n- S2 - The paper is well-written and easy-to-follow.\n\nWeakness and Questions:\n- W1 - I think the important baseline is missing. Unsupervised SimCSE can be applied to these datasets since most dataset is based on semantic similarity, not question answering tasks. Since the paper is mentioning that the difference from SimCSE-style loss is not comparing the (q1,q2) and (a1,a2), think the paper can present the performance difference as well. Also, SupCL-seq (Sedghamiz et al., 2021) can be the baseline as well.\n- W2 - Such contrastive learning is usually enforcing the semantic similarity of the model. If we use hard negative in terms of semantic similarity (by SBERT), doesn't it potentially harm the performance since it considers true negative examples in the batch as negative instances ? The use of labeled negative examples improve the performance ? What if we sample negatives which are semantically distant ?\n",
            "summary_of_the_review": "The paper's claim is interesting but still needs more analysis to support the claim.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work proposed a loss function called batch-softmax contrastive (BSC) loss for fine-tuning large-scale pre-trained Transformers for pairwise sentence scoring tasks. In detail, a number of variations for the calculation of the loss were studied, including symmetrization, incorporating labeled negatives, aligning scores on the similarity matrix diagonal, normalizing over the batch axis, etc. Experiments were based on various traditional NLP tasks, i.e., classification, regression, and ranking, which verified the superior performance of the BSE loss.",
            "main_review": "Overall, this paper provides a clear overview of the variants of contrastive loss. The method is sound, and the paper is easy to read. \n\nThe major concerns are listed as follows.\n\n[Introduction]\n\n1. Many claims are handwaving, lacking necessary supports. For example, there is no reference in the Introduction part, though it is full of strong claims without any evidence. \n\n “While a lot of attention has been paid to the architectures, especially for deep learning, there has been less focus on studying loss functions.” \n\n- This comment is trivial. There are many studies on loss functions, especially in the era of pre-trained language models, where one of the most important research lines is designing effective loss functions.\n\n“loss functions based on similar or on the same ideas were reinvented multiple times under different names.” \n\n- This is a strong accusation. I would suggest polishing it to be more accurate or providing adequate details. \n\n“An example of such universal loss function is the batch-softmax contrastive (BSC) loss, which we will discuss below.”\n\n- No detail is provided in the introduction part. It would be better to use a few sentences to indicate why we need BSC\n\n2. The motivation for the BSC loss is not clear. According to the introduction part, would BSC just unify a “universal” name of previous loss functions?\n\n3. “We study the use of a batch-softmax contrastive loss for fine-tuning large-scale transformers to learn better task-specific sentence embeddings for pairwise sentence scoring tasks.”\n\n- How could the BSC loss learn better task-specific sentence embeddings? What kind of knowledge is captured after training with the new method? It would be beneficial to provide examples to demonstrate the motivation and the effects clearly.\n\n[Related Work]\n\n4. The innovation is limited. In the related work part, the discussion of differences with previous studies is too abstract. \n\n“we will use the ‘modern’ variant of the loss”, “a number of novel and important modifications”. \n\n- This work reads like a technical report that combines the findings in previous studies. The techniques presented in Section 3 are existing methods, though there is no reference nor discussion of difference with previous work. It is not clear what is new in this work after reading the whole paper. \n\n5. “While the above-described loss functions have different names, they are all based on similar ideas.”\n\n- As discussed in the related work part, the existing studies would solve different problems with different motivations. If they are really based on similar ideas, why would the authors choose to combine those components, which would result in redundancy?\n\n[Method]\n\n6. “Pointwise approaches for training models for pairwise sentence scoring tasks, such as mean squared error (MSE), are problematic as the loss does not take the relative order into account. For instance, for two pairs with correct target scores (0.4, 0.5), the loss function would equally penalize answers like (0.3, 0.6) and (0.5, 0.4).”\n\n- This is called the negative divergence issue, which has been studied in text generation. I would suggest adding the reference to help readers have a picture of related studies.\n\n[1] LI, Zuchao, et al. Data-dependent Gaussian Prior Objective for Language Generation. In: International Conference on Learning Representations (ICLR). 2019. \n\n[2] WIETING, John, et al. Beyond BLEU: Training Neural Machine Translation with Semantic Similarity. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL). 2019. p. 4344-4355.\n\n7. “We achieve this by fixing the content of the batches at each epoch of the training process.” “To this end, we apply the current model to encode all pairs at each epoch.”\n\n- This method would cause more memory consumption and computation. How about the training efficiency? \n\n[Experiments]\n\n8. The observations are intuitive and doubtful regarding the generality. The experiments are only conducted on simple NLP tasks, with only a simple backbone model. Besides, the baselines for comparison are either simple or old. If this work aims to survey the loss choices and find the best combination, it would be necessary to use different backbone baselines and compare them with more recent models.\n\n",
            "summary_of_the_review": "This work attempts to combine the variants of contrastive loss for NLP. I am inclined toward rejecting this paper mainly because of unclear motivation, limited innovation, and primitive experiments.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper explores using the InfoNCE contrastive loss or the batch-softmax contrastive loss for sentence pair scoring tasks in NLP. Towards this, the paper uses the BSC loss in a straightforward manner using embeddings obtained from an underlying SBERT model. 2 methods of constructing the batches: one using example-based shuffling via k-means and one using fast-shuffling using superficial attributes are presented. The empirical evaluation is performed on 4 ranking, 2 classification and 1 regression sentence-similarity scoring tasks, and compares the performance of using the BSC loss (and the BSC + MSE loss) compared to only using the MSE loss.",
            "main_review": "Strengths:\n1) The BSC loss (ranking loss) which accounts for the entire ranking of the example seems like a natural choice for pairwise sentence scoring tasks (especially ranking tasks like QA, retrieval, etc). This loss has manifested itself in several forms already (Eg: in Dense Passage Retrieval, etc), and thus the empirical improvements presented in some hand-selected datasets are natural and unsurprising.\n\n2) The paper presents related work on the batch-contrastive loss along with different viewpoints such as InfoNCE (via Mutual Information maximization) well by the related work section.\n\nWeakness:\n\n1) The problem and motivation for the paper is weak. The introduction does not provide any reason as to why this is an important problem or approach that needs to be considered important by the NLP research community.\n\n2) There is no technical novelty in the approach presented in the paper. It is a straightforward application of an existing famous loss (InfoNCE) for a pairwise sentence scoring task in NLP. Previously, SimCSE (Gao et al, 2021) have explored using the InfoNCE loss for sentence-pair tasks like NLI, etc.\n\n3) The described approach does not explain how to deal with multiple positives per rank (in case a dataset has more than one correct answers for a single question). This is crucial for Question Answering tasks and is a shortcoming of the proposed BSC loss approach in it's current form.\n\n4) The empirical evaluation of the paper is weak and unconvincing:\n\t- Missing baseline of SimCE (Gao et al, 2021) from the empirical results tables which also uses the Info-NCE loss.\n\t\n\t- The datasets considered in this paper are not well indicative, and sometimes not the original splits: for ranking/ answer-sentence selection there are several more challenging and popular datasets like WikiQA, TREC-QA, ASNQ, etc. , the QQP dataset is randomly sub-sampled and the results cannot be directly compared with previous baselines.\n\t\n\t-There are no ablations or studies highlighting the empirical benefits of the specific shuffling approaches proposed in the paper w.r.t a random shuffling baseline. This is crucial for justifying the design choice of the batching strategies.\n\n\nMissing References:\n\n1) COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining, Meng et al, 2021\n2) Supervised contrastive learning for pre-trained language model fine-tuning, Gunel et al, 2021",
            "summary_of_the_review": "The paper presents a straightforward extension of using the batch contrastive loss (InfoNCE) for sentence-pair scoring tasks in NLP. There is very limited technical novelty to the proposed approach. The experiments are performed on hand-selected datasets, with several important ranking QA datasets missing from the evaluation. The results are presented without strong baselines, and without any ablations highlighting the importance of the presented batching strategies. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}