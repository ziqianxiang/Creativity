{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a probability contrastive learning (PCL) for semi-supervised learning and unsupervised domain adaptation. PCL replaces the normalized deep embedding in InfoNCE loss by the softmax probability, and hence it can be applied to classification-based tasks with a final linear classifier. Experimental results on tiny datasets (i.e., CIFAR, OfﬁceHome and VisDA-2017) are provided.",
            "main_review": "Pros:\n1.\tThe proposed method seems easy to implement.\n\nCons:\n1.\tThe motivation of PCL is quite confusing for me. The authors state that ‘the optimization of FCL does not take into account the class semantics due to directly operating on the features before class weights’. The ‘class semantics’ seems to be the softmax probability as mentioned in the paper. However, it is unclear why using this softmax probability is better than deep features. The positive-negative sample pairs are still obtained based on each individual sample.\n2.\tThe novelty of PCL is limited. Eq. (5) seems almost the same as the formulation of CE-based consistency regularization. Similar approaches have been widely explored at least in the context of semi-supervised learning (e.g., MixMatch, FixMatch). \n3.\tFor me, the experimental improvements are not significant enough to ignore the aforementioned drawbacks. For example, on CIFAR, PCL on top of FixMatch only marginally outperforms CoMatch.\n4.\tThe writing of this paper requires improving. \n",
            "summary_of_the_review": "Confusing motivation, limited novelty.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes probability contrastive learning (PCL) which enforces features to be distributed around the class centers. Instead of applying contrastive learning on the feature level, the proposed method performs contrastive learning on the output probabilities. The paper also provides experimental results on semi-supervised learning, unsupervised domain adaptation, and semi-supervised learning.",
            "main_review": "Strengths:\n1. Extensive experiments demonstrate the benefit of applying PCL to existing methods.\n\n2. Unlike previous self-supervised contrastive learning methods, PCL can enforce unlabeled data to have class-wise clustered features.\n\nWeaknesses:\n1. The technical novelty of the proposed method is not significant. In previous contrastive learning methods, a contrastive head above the encoder is commonly used (Chen et al., 2020a; Khosla et al., 2020). Chen et al. (2020a) demonstrated that using a contrastive head performs better than not using the contrastive head. The proposed method in this paper can be considered as a specific form of the contrastive head; using a linear layer + softmax as a contrastive head.\n\n2. An advantage over supervised contrastive learning is not clear. The benefit of using the classifier as a contrastive head is that PCL can be applied to unlabeled data. However, as the authors pointed out in section 3.3, PCL enforces unlabeled data to cluster around some class weight. In other words, unlabeled data may have some pseudo label. If so, we can also consider giving pseudo labels iteratively to unlabeled data and apply supervised contrastive learning. The alternative method using supervised contrastive learning even enforces unlabeled data with the same pseudo label to get closer. In PCL, samples with a similar probability are still considered as negative pairs and they are forced to increase the distance between them.\n\n3. Concern on hyperparameter. The proposed PCL is applied to existing methods and it adds one more hyperparameter. Since existing methods may have hyperparameters, it would be better if the performance is insensitive to the additional hyperparameter lambda. However, according to section A.1, various hyperparameter lambda values are used for different tasks. This may hamper the applicability of PCL to other methods since it will complicate hyperparameter search.\n\n4. There are some minor issues with the writing of the paper. For example, there is a redundant space on top of page 7, below Table 2. Typos such as Talble 7 on page 8, section 5.2.",
            "summary_of_the_review": "The proposed method has some merit over previous self-supervised contrastive learning. However, technical novelty is limited. The proposed method can be considered as a specific version of the previously introduced method. Using the classifier as a contrastive head is somewhat novel, but its efficacy over using supervised contrastive learning needs more clarification.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces a very simple component named PCL (probabilistic contrastive learning) that can be added to semi-supervised learning, unsupervised, and semi-supervised domain adaptation. The main idea is to, instead of doing contrastive learning at the feature level, simply do contrastive learning after the softmax. The authors argue that this simple change encourages representations to cluster around classifier weights, and thus is more effective for SSL. \n\nThe method is evaluated on semi-supervised learning showing benefits with very small numbers of labeled data. It is also evaluated on un-/semi-supervised domain adaptation, where it shows good results.",
            "main_review": "The title refers to the paper as a representation learning approach, but this is not accurate. Representation learning often refers to methods that learn embeddings via a pretext task (which can be self-supervised, fully supervised, or semi-supervised), and then deploys the representations on other downstream tasks. However, this paper does not do that. All experiments are done by adding the PCL component to the optimization for each dataset/task. It is interesting to see that the same component can help with several tasks, but I wouldn't call it a representation learning approach.\n\nThe writing could be improved substantially. The motivation provided for the method is vague. Section 3.3 tries to shed some light into the inner workings of PCL, but it is confusing and under-developed. For example, what are the gradients for PCL? How does it encourage clustering around classifier weights? etc.\n\nResults on STL and SVHN are missing on SSL experiments (table 1). These are often used, in conjunction with CIFAR, to evaluate SSL approaches, especially when using only small-scale datasets.\n\nIn table 1, some of the bolded values are within the margin of error. PCL only seems to be effective at 4 labels per class. When using more than 4 labels per class, PCL is no better than FixMatch.\n\nThis method seems to be especially effective for domain adaptation. This is probably because it is more likely that, in domain adaptation, unlabeled samples will deviate from the class prototype constructed in the source domain due to the domain gap. Thus, encouraging representations that cluster around these class prototypes is likely beneficial. However, this is what is already done by several domain adaptation methods. In sum, I'd rather see this paper to written as a domain adaptation approach, with more extensive comparisons to domain adaptation methods. For example, sim2real semantic segmentation is one of the main challenges in this area, with many relevant prior work done in that domain. Showing improvement in this task would benefit this paper substantially.",
            "summary_of_the_review": "The paper presents a very simple component (PCL) that seems to benefit domain adaptation problems. The writing of the paper and the focus of the paper could be improved substantially to reflect that. A more focused evaluation on domain adaptation benchmarks would also provide experimental support for PCL. As is, the paper argues that PCL is better than FixMatch (and other contrastive-like methods for SSL) for domain adaptation problems. However, FixMatch was not introduced for domain adaptation, so these improvements are not surprising.\n\nIn sum, the paper presents some interesting results on domain adaptation, but I believe that it has lots of room to improve. In my opinion, the paper is borderline, and for the above reasons, I'm leaning towards rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose probability contrastive learning which helps the features to be distributed around the class prototypes, as a replacement for the widely used feature contrastive learning. In addition, instead of L2 normalization in feature contrastive learning, they propose to use L1 normalized probability. The paper provides experimental results on three different closed-set tasks, namely, unsupervised domain adaptation, semi-supervised learning and semi-supervised domain adaptation.",
            "main_review": "Paper Strengths:\n+  The idea is simple and works effectively for a variety of tasks.\n+ The idea is intuitive.\n+ The paper is reasonably well written and is easy to understand.\n\nPaper Weaknesses:\n- There are several typographical errors throughout the paper. It should be carefully proofread and corrected.\n- Though for SSL task, when the number of labels is less, the performance improves, it actually decreases compared to FixMatch for CIFAR 10 with more labels, which is counter-intuitive. \n- The evaluation is not exhaustive. For example, for UDA, results on only one DA task (Synthetic-> Real) are given in Table 2. It will be good to have results for the other DA tasks as well. \n- For DomainNet data results (Table 4), results with MME+PCL and MME+PCL+FixMatch are given. Results with MME+FixMatch should also be provided to ensure that the proposed approach is improving upon only FixMatch.\n- In Table 1, the performance of the proposed method + FixMatch is given. And FixMatch alone performs better than most of the other compared approaches.\n- The contribution of this work is incremental. ",
            "summary_of_the_review": "The paper proposes an intuitive approach, namely probability contrastive learning as a replacement for the widely used feature contrastive learning. The results on the three tasks are promising, but more experiments and analysis are required to justify the effectiveness of the proposed approach. Also, the contribution is incremental.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces a variation of contrastive learning called “probability contrastive learning” (PCL). Instead of contrasting features, the authors propose to use class probabilities as inputs to the contrastive loss. These probabilities are produced using a classifier (linear layer) and a softmax layer. Supposedly, PCL enforces the features to be distributed around the class prototypes. PCL is evaluated on three close-set image classification tasks: unsupervised domain adaptation, semi-supervised learning, and semi-supervised domain adaptation.",
            "main_review": "Strengths:\n- The proposed method (PCL) is simple to understand\n- The experiments are quite thorough and interesting\n\nWeaknesses:\n- Section 2.1 is missing recent contrastive-based self-supervised learning papers (e.g.: [MoCo v3, NNCLR, Debiased Contrastive Learning, paper1, paper2]) but most importantly the authors do not cite, discuss and compare with Prototypical Contrastive Learning which can be adapted for the semi-supervised case with minimal changes.\n- In addition, the authors do not discuss the similarities of PCL with NCL, a contrastive-based method for novel class discovery (a similar setting to semi-supervised learning). Although NCL tackles the open-set setting, it can be easily used in semi-supervised learning as well, by using the labels to find positives and the nearest neighbor strategy to retrieve pseudo-positives from the unlabeled set.  \n- Moreover, in NCL, they find that the presence of false negatives at the denominator of the contrastive loss is likely to be problematic in semi-supervised settings. To me, this sounds like a reasonable explanation for the poor performance of vanilla contrastive learning. However, this phenomenon is not mentioned or tackled in PCL. I think PCL might actually exacerbate it by backpropagating the contrastive loss to the prototypes. The prototypes are learned with supervision, and therefore should be quite well trained and stable, so updating them using an unsupervised objective might not be a good idea, especially considering the problem of the false negative. Have you tried detaching them (i.e. optimizing them only with the supervised loss)? Does it improve?\n- I wasn’t able to understand why PCL softmax is applied twice (first in the logit dimension, and then in the batch dimension). After the first softmax, the authors propose to compute the inner product of the probabilities of the samples in the batch in a pair-wise fashion. It turns out that this is equivalent to the probability of two samples belonging to the same class. At this point, a simple Binary Cross Entropy can be used to encourage the network to output similar distributions over classes for positive samples, and dissimilar outputs for negative samples. This simple baseline, which is very well-known in the literature [LSD-C, RS, NCL, OpenMix], is not reported in the paper. With this said, I was not able to find a simple intuition for the need to apply softmax a second time on the batch dimension. For vanilla contrastive learning, this makes sense because the output of the inner products are cosine similarities, which cannot be immediately interpreted as probabilities, and are in range [-1, 1].\n- Moreover, the simple baseline with BCE that I mentioned has the nice property that it encourages the network outputs to be one-hot. Does PCL have similar properties? Consider analyzing the properties of PCL in more depth.\n- Consider not showing the gradient of the loss function with respect to the features in the main paper (equation 2). The point you want to make is already well-known and clearly understandable from equation 1.\n- The notation is not ideal. Consider removing the superscripts n and W from the features f and the probabilities p. They do not seem to provide additional information and make the equations harder to read.\n- Missing comparison with [PAWS].\n- Please export figure 3 with higher DPI, it is quite blurry.\n- Although I appreciate that the code is provided in the submission, its quality could be improved. For example, the files are full of very long functions with too little structure or modularity. Sometimes blocks of code are commented, hurting readability.\n\n[MoCo v3] https://arxiv.org/abs/2104.02057\n\n[NNCLR] https://arxiv.org/abs/2104.14548\n\n[Debiased Contrastive Learning] https://arxiv.org/abs/2007.00224\n\n[paper1] http://proceedings.mlr.press/v119/wang20k.html\n\n[paper2] https://arxiv.org/abs/2005.10243\n\n[NCL] https://arxiv.org/abs/2106.10731\n\n[RS] https://arxiv.org/abs/2002.05714\n\n[OpenMix] https://arxiv.org/abs/2004.05551\n\n[LSD-C] https://arxiv.org/abs/2006.10039\n\n[PAWS] https://arxiv.org/abs/2104.13963",
            "summary_of_the_review": "The method is simple and the experimental evaluation is thorough, but the method does not seem motivated well enough. An ablation study with simple baselines (e.g. simple BCE (see full review)) is missing. The code is difficult to inspect. I would like to see a more in-depth analysis of the properties of PCL and a comparison with PAWS.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}