{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper uses inverse reinforcement learning framework to infer the reward function, which is then used as the basis for mapping neural activity to actions. The authors find that the resulting neural decoder outperforms decoders that directly learn a mapping from neurons to behavior in a vibration detection task. The same model is then used to predict behavior when the neurons are inactivated, and these predictions are experimentally confirmed.",
            "main_review": "\nStrengths:\nThe use of inverse reinforcement learning framework for grounding the link between neural activity and behavior is conceptually new to my knowledge. In contrast to blindly learning a mapping from neurons to behavior, the proposed approach has a strong theoretical foundation which makes the results interpretable.\n\nWeaknesses:\nI found the description needlessly abstract in many places, so I couldn't follow some of the key methodological details. Here are some questions:\n- Was the vibration cue always presented 1.6s after lever press? If yes, then to the extent that the rat can precisely count time, the cue has no relevance to behavior? In that case, it could be viewed as a memory task so is RL still a meaningful framework to understand behavior in this task.\n- Figure 4: In what sense is this a release distribution? The percentages do not add to 100. On a related note, how to interpret the learned reward being highest at 2.4 seconds? The true reward should be zero in this state, no?\n- Section 4.4: \"we use the neural spikings per time-step and trial as features\". This is unclear. Did you mean a vector of instantaneous firing rates of all neurons? Please clarify what \"trial\" has to do with it.\n- Section 4.4: \"Since the resulting features are very sparse, we further append the time spent since trial initiation to the feature space\". Clarify why it is sparse -- isn't it a vector of firing rates? Or was it spike counts? Were all models appended with t? Although this is not a shortcoming of the framework, this strikes me as potentially problematic for interpreting the results. Correct me if I am wrong but it seems to me that the key advantage of learning to represent the reward function is that state information (time, in this experiment) can be abstracted away. But if the state has to be explicitly fed in order for the model to predict actions, then the model loses some of its appeal.\n- Section 4.4: How robust were the results to the choice of threshold ($\\epsilon$=0.6)?\n- Please clarify the meaning of \"normalized cumulative embedding\"\n- Section 4.4: \"The latent representation of the neural features grounded in the learned immediate reward preserves the temporal coherence which stands in contrast to the latent embedding of the classifier.\" Isn't the release probability also a smooth function of time and therefore temporally coherent? So why is the NNC not able to exploit it?\n- Would be worth considering an alternative NNC model with the Before-cue or After-cue as an additional predictor. This would correspond to a model where signals from brain areas that encode vibration are (nonlinearly) integrated with RFA signals to predict release probability. \n- \"We calculate the feature matrices accumulating the neural spikings by using an incremental mean over all trials for each rat (to aggregate all available information) and compute the weights via least squares, assuming a linear combination of the state features as described in Section 3.2. Then, as described above, we map the features to intrinsic rewards and compute the Q-values and corresponding stochastic Boltzmann policies\". Sorry, I was confused by this description. Is this a different model now? If it is the same model as section 4.4, then why not just set a fraction of inputs to the neuRL network to zero and predict the release probability? What exactly was minimized for fitting $\\theta_\\rho$ here?\n- Would the NNC model with an explicit cue input (Before-Cue or After-cue) explain inactivation results?\n- Limitations of the approach are not discussed. What kinds of tasks would the proposed approach be best suited for?\n\nMinor:\n- Section 4.2: S = {0.0 s, 0.2 s, . . . , 1.2 s} --> shouldn't it go up to 1.6s?\n- Section 4.2: \"{Before Cue, Cue, After Cue, After Cue_1, After Cue_2, . . . , Time to Release, Late Release}\". What do the subscripts 1, 2, in After Cue mean?",
            "summary_of_the_review": "The framework is promising but the demonstration was not compelling due to the complexity of the modeling approach. The latter could be addressed by improving the clarity in describing the methods, considering alternative models that have access to cue information, and a clear discussion of the limitations.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes to use inverse reinforcement learning for inferring an intrinsic reward function for understanding the connection between neural signals and behavior, also known as neural decoding. The author proposes a three-step approach: 1. propose and compute a closed-form solution of an intrinsic reward function on observed rat trajectory. 2. Connect the recorded neural signal to the intrinsic reward previously obtained. 3. Make predictions of simulated behaviors. ",
            "main_review": "This paper is well-written and organized. It is very well suited for neuroscientists trying to understand the connection between neural decoding and intrinsic reward. \n\nHowever, it seems to resemble more the thought process of the authors rather than presenting the really impactful part of the paper and focusing on the novelty. It would have been better to start from the end instead of building towards it, particularly when the authors mainly used well-known concepts in IRL. This paper is probably more suited for a neuroscience venue when the core of the novelty is arguable in the experiments and their interpretation from a neuroscience perspective rather than on the method itself. \n\nThis paper has no particular weaknesses nor strengths, and this work certainly has the potential of being more broadly used than only in neural decoding. We would suggest the authors try and broaden the spectrum of experiments of this work to be able to touch a broader audience. ",
            "summary_of_the_review": "This is an interesting piece of work that would be better suited for a computational neuroscience journal than ICLR. This work does not offer a substantial improvement on existing methods from a reinforcement learning point of view and the experiments are only clear for a neuroscience expert but will be hard for attendees of ICRL. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Here, the authors present a previously-published algorithm for inverse reinforcement learning (IRL), which allows them to estimate an intrinsic reward function simply from observing behavior. In this case, the authors analyze behaviors performed by a rat in a simple lever pressing task – the rat presses a lever and holds until a vibration cue is presented, then they must rapidly let go. First, they show that they can estimate an intrinsic reward function that allows them to faithfully reproduce the set of responses from the rats. Next, they use the estimated intrinsic reward functions as an intermediate representation for decoding release times from spiking activity of neurons in RFA. Finally, they simulate the effect of inhibiting neurons that project from RFA to CFA and confirm their simulations by performing in vivo optogenetic inhibition of the same cells.\n\nThe main new contribution of the paper is to propose the use of IRL in generating a representation that is useful for neural decoding, and for generating predictions about the function of specific neurons.",
            "main_review": "Strengths:\n\n1. The use of IRL as an intermediate representation for neural decoding is an interesting and potentially useful one.\n\nMajor issues:\n\n1. The per-trial behavior predictions from neural data. A central claim here is that NeuRL is a superior decoder for predicting behavior from neural activity. What is unclear to me is, assuming I have interpreted Table 2 and Figure 5 correctly, what extra information is being extracted *from the neurons*. That is, what mean prediction accuracy would the authors achieve using just the learned reward function and time in trial itself? Is the gain in performance here due to NeuRL *extracting more information* from the neurons?  This is an essential question to resolve to be able to interpret these results. Moreover, does NeuRL have access to more information than NNC, LR and random?  Since the intrinsic reward function contains information about the animal's behavior in the training set that the other methods do not have access to, the authors should provide an argument for why their comparison is apples to apples since it appears not to be.\n\n2. Figure 5 is an interesting visualization, but the authors must provide more systematic analysis to back up the point they are trying to make.  That is, if they are arguing that NeuRL contains a more temporally coherent representation, then they must show that the temporal distance between two points maps better onto the NeuRL latent space than the NNC latent space.\n\n3. Figure 6B unfortunately is very hard to interpret.  I could not find any formal analysis of the data.  The authors mention that 60% of the neurons increase their activity during the response period.  How was this determined exactly?  By eye it is difficult to see any systematic clustering of neural activity over time, which is concerning since these neurons are used for analysis throughout the manuscript.\n\n4. I am not sure how to interpret Figure 8. The trend shown for simulated inhibition is obviously very strong, but it is hard to know what to make of the experimental results shown for \"Rat Batch 2\".  There *maybe* is a trend towards increased reaction times, but the difference looks extremely small in magnitude and not approaching significance. Moreover, I do not see any experimental controls, e.g. no mice that received the same stimulation protocol without any opsin (light controls), so we do not know if the small change in behavior is due to mice responding to the light itself, vs. the effect of the opsin.\n\n5. A larger conceptual question: I'm not entirely sure why the authors endeavored to look at RFA neurons through a reinforcement learning framework.  Did they hypothesize that reward representation would be relevant in RFA? I could not find a justification for these experiments, and why this analytical approach is relevant.\n\nMinor issues:\n\n1. I found figure 4 unnecessarily difficult to parse. Is the left y-axis accurate? If so, I do not understand why the numbers do not add to 100%. Additionally, why is the learned reward pinned to -5 in timestep 0? Showing the learned reward from -5 to +5 makes it difficult to see how the fluctuations in learned reward map onto the predicted releases. Additionally, I am assuming that the orange corresponds to the Boltzmann distribution, which is exactly overlapping with the real distribution.  The authors should provide better intuition for why the predicted releases at times diverge from the learned reward (e.g. top second to last point the learned reward increases yet the predicted release sharply decreases from its previous point).\n2. Figure 7 what are the shaded regions?\n3. typos throughout, please fix\n4. What neurons are used in the per-trial decoding, the 10 neurons shown in figure 6?\n5. Table 2, the authors should indicate statistical significance in the shown differences.  Since I do not know what +/- correspond to in the manuscript it is hard to suss this out.\n6. Figure 8 panel B, how does this show processing of neural recordings and extraction of neural spiking?\n7. What is the source of the error bars for the modeling results shown in Figure 8?  Different random restarts of the model?  ",
            "summary_of_the_review": "I like the idea of using IRL as an intermediate representation for understanding the activity of neurons generally.  However, I found that the manuscript is conceptually underdeveloped (why ought we use an RL framework to understand the activity of neurons in RFA?). More seriously, the paper contained a number of major technical flaws that make the results extremely difficult to interpret.  ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}