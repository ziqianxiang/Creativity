{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a structural pruning method with latency-aware regularization for vision transformer model to achieve latency reduction. A parameter redistribution strategy is introduced to enable the control of latency-accuracy tradeoff. ",
            "main_review": "Strength:\n\n-\tBesides model pruning, a new architecture inspired by the parameter redistribution is achieved. \n-\tThe proposed pruning method achieves better run-time speedup compared to the DEIT model. \n\nWeakness:\n\n-\tIt’s difficult to identify the real novelty/contribution of the proposed method. The core techniques such as importance score-based pruning and full model distillation (essentially self-distillation) have been explored in the network pruning literature. \n-\tThe rules designed for parameter redistribution are draw from the observations in Fig. 4. However, there is no ablation study to validate its effectiveness as compared to other design rules. \n-\tThe proposed reshaped attention block (i.e., explicit head alignment) for pruning has only minor improvement over the strategy of no explicit head alignment (which achieves a higher model compression rate) in terms of latency reduction.\n-\tCompared to the DEIT model family (Table 3), the performance gains are very small for the proposed NViT on base and small models. Although about 1% gain is achieved on the tiny model, it may attribute to the larger model of NViT-T (0.8M more parameters).\n-\tThere is a lack of comparison with other pruning methods.\n-\tAblation should be conducted to verify the effectiveness of latency-aware regularization (Eq. 6). \n",
            "summary_of_the_review": "Please see the weakness part. I would like to see the author's reponse to my questions.\n\n=========== Post rebuttal =============\n\nThanks for the author's efforts in addressing my concerns. Most of my concerns are addressed. I would like to raise my score to weak accept.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the latency reduction of Vision Transformer model. The proposed pruning method w.r.t importance score is trained with the full pre-trained model using knowledge distillation, with latency aware regularization. In addition, the author designed a new architecture NViT with a parameter redistribution. The experiments evaluated the proposed methods with respect to the accuracy, FLOP reduction, and parameter reduction.",
            "main_review": "- Did the authors consider any baseline for comparison? As shown in Chen et al. 2021 (https://arxiv.org/pdf/2106.04533.pdf), the authors could use simple baselines to perform structured pruning. Yet, in the paper, there seems to be no comparison to any baseline. \n- Also, I would encourage an ablation study. As the authors pointed out, the paper considers extra modules of ViT in comparison to Chen et al. 2020. It would be interesting to see the results when considering the same modules of ViT with Chen et al. 2020 and show comparison w.r.t Chen et al. For example, another ablation study could be done by removing knowledge distillation with the full model and using cross-entropy with ground truth labels instead. Knowledge distillation can be applied to any model and should increase the performance of any baseline or recent methods as well.",
            "summary_of_the_review": "Please see the above comment.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper applies latency-aware global structural pruning to vision transformers (ViTs), which results in redistribution of the model parameters and better a speed-accuracy tradeoff. Compared to Deit-B, the pruned vision transformer model (NVP) is 1.85x faster with almost no performance loss. Based on the insights discovered in the pruning process, the paper also presents the novel vision transformer (NViT) architectures. NViT outperforms Deit on ImageNet, CIFAR, and iNat benchmarks with similar running times.",
            "main_review": "Strengths:\n+ The proposed vision transformer architectures (NVP and NViT) can achieve a better speed-accuracy tradeoff than Deit.\n+ The paper presents interesting and informative insights about the model design of vision transformers in Sec. 4.1.\n\nWeaknesses:\n- The novelty of the paper is not enough. To me, this paper is only an application of pruning algorithms on vision transformers. I acknowledged that it may need many tunings to make things work well on vision transformers, but these tunings cannot be considered as something novel.\n- The paper is only based on Deit, which is too simple and can only be used for image classification. Recently, there are many new vision transformers such as PVT [1] and Swin [2], which are designed for dense prediction tasks (e.g., object detection) and have wider applications. I think the paper should conduct experiments on these transformers, which will be more significant.\n\n[1] A Versatile Backbone for Dense Prediction without Convolutions.\n[2] Hierarchical Vision Transformer using Shifted Windows.\n",
            "summary_of_the_review": "The paper presents a better speed-accuracy tradeoff than Deit but the novelty is limited. In addition, the resulting models in this paper can only be used for image classification, which makes them less significant.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The authors have addressed my concerns. I admit the paper does have a positive impact on transformer-based research works since pruning transformer-based models should be different from pruning CNN-based ones in some aspects. Therefore, I lean to accepting the paper.",
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes to prune Vision Transformer models with global and structural pruning with latency-aware regularization. The proposed NVP model achieves similar performance compared with DeiT-Base with much fewer parameters and faster speed, especially on the recent Ampere GPU architectures which support the acceleration of sparse matrix multiplication, i.e, comparing the speed of V100 and RTX3080.",
            "main_review": "Pros:\n\n1. The proposed pruning algorithms jointly take latency and structural into consideration, as well as the properties of the hardware.\n2. The experimental results confirm the effectiveness of the proposed pruning method. For example, the pruned models are efficient in speed with a little performance drop compared with their original one. The model can also be pruned to different model sizes while outperforming others with similar model sizes.\n3. The paper is easy to follow and well-written. For example, Figure 4 is really helpful in comparing the distribution of the parameters of the proposed redistribution method with the previous one.\n\nCons:\n\n1. All pruned models in this paper are based on a single model, DeiT-B. This cannot validate the generalization ability of the proposed pruning method on other models, but the authors claim the pruning method as a contribution in the paper. I am also wondering why the authors choose DeiT-B as the base model considering that there are already many better vision transformers such as PVT, Swin, and ViTAE. Can the proposed pruning algorithms be applied to these stage-wise models? And can the systematic analysis from the pruning process help to scale the stage-wise transformers considering that these models are largely different from DeiT?\n2. In Table 1, the performance of Swin-B is 83.3 accuracy but the official performance in its paper is 83.5. I am wondering how this performance gap comes.\n3. The pruned models with various sizes are compared with others in Table 1. However, the superiority of the NVP variants comes from several aspects, which makes the comparison unfair. The first is the larger and better model (DeiT-B) which NVP are pruned from. The second is the distilling loss from DeiT-B. The last but most important is that the training settings of these models are different. According to Appendix A1, the models for pruning are trained with 32 V100 GPUs and a batch size of 144 per GPU (a batch size of 4608 in total) but other models are trained with the batch size of 1024 in total. Besides, if we consider DeiT-B (where NVP variants are pruned from) as a pretraining process, NVP variants need to finetune another 300 epochs on ImageNet-1K, which sums up to 600 epochs totally not to mention the iterations in the pruning process. But the performance of other models like DeiT-B and Swin-B is reported upon only 300 epochs, while more training epochs have been proved helpful for a performance gain in both ViT and DeiT. For example, DeiT-B with 1000 epochs can reach 84.2 Top-1 accuracy while 300 epochs lead to 83.5 accuracy. I am not challenging the classification performance and acceleration of NVP variants, but wish to see a sound comparison, especially under a fair training setting.\n4. The authors also propose a new architecture scaling rule in this paper and claim it as a contribution. However, several concerns are raised on it according to Table 3. For example, NViT-T outperforms Deit-T by around 1 accuracy but NViT-T has much more model parameters (6.4M v.s. 5.6M) which generally leads to better modeling ability. Besides, with the model parameters to 86M, the performance gain of NViT-B vanishes to only 0.1 Top-1 accuracy. This trend may demonstrate that the new design rule does not fit large models. Therefore, I am not convinced by the experiments in Table 3 and consider that the contribution of the new architecture scaling rule is somewhat overclaimed.\n5. The authors claim that the efficiency demonstrated on ImageNet can be preserved on downstream classification tasks according to Table 4. However, I have some concerns about the experiments in Table 3. First, the model parameters of NViT-T and NVP-T are 6.4M and 6.9M respectively according to Table 1 and Table 3, while DeiT-T only has 5.6M parameters. Second, NVP-T is pruned from a well-trained DeiT-B model, thus inherently seeing more images (more training epochs) than DeiT-B. These two aspects make the experiment less convincing. Third, the authors should consider validating the transferring ability of a larger model NViT-B to support the conclusion.\n",
            "summary_of_the_review": "The paper makes some interesting points and observations, but as noted in the main review, some of the paper's claims are not well supported. The different settings for training and the marginal improvement in performance raise concerns for me about the soundness of the findings.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}