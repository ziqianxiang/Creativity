{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a method based on Surrogate Mutual Information Maximization for unsupervised domain adaptation. The main idea is to maximize the mutual information between the features from the same class on the target and source mixture distribution. To encourage the clustering structure, the authors also employ a Laplacian regularization. ",
            "main_review": "Pros:\n\n1.\tThe theoretical analysis based on mutual information is novel to me\n2.\tThe overall structure of the paper is clear and easy to follow\n\nCons:\n\n1.\tWhile the theoretical framework is novel to me, my main concern is that this paper does not provide any new message to me. Eventually, the idea is still to perform semantic conditional distribution alignment (i.e., aligning P(G(x)|y)). I understand that the authors aim to highlight the role of mutual information maximization to enhance the discriminative power for the feature, but this is also not new in the literature (e.g., using contrastive learning).\n2.\tWhen reviewing the related work in the literature, several important references are missing (e.g., [1], [2]), which also study the theoretical guarantees of “class-level” works. In fact, they considered a more challenging scenario where label shifts also exist. Given these previous works, the contribution of this work is quite limited, from both theoretical and algorithmic aspects. \n3.\tI am a little confused about the claim “source distribution may suffer from the class imbalance problem, which will harm the performance on classes with fewer data” – class imbalance problem is another issue in classification problems, not specifically for domain adaptation. In addition, even though we can do class-balanced sampling in the source domain, how can we guarantee the target domain is also balanced?\n4.\tThe experimental results are also not quite convincing – compared with other SOTAs, the improvement is incremental.  \n\n[1] Rakotomamonjy, Alain, Rémi Flamary, Gilles Gasso, Mokhtar Z. Alaya, Maxime Berar, and Nicolas Courty. \"Optimal Transport for Conditional Domain Matching and Label Shift.\" arXiv preprint arXiv:2006.08161 (2020). (recently accepted by MLJ)\n\n[2] Tachet des Combes, Remi, Han Zhao, Yu-Xiang Wang, and Geoffrey J. Gordon. \"Domain adaptation with conditional distribution matching and generalized label shift.\" Advances in Neural Information Processing Systems 33 (2020). \n\n\n",
            "summary_of_the_review": "While the theoretical framework proposed in this paper is new to me. The practical contributions to the field of UDA are quite limited given existing literature. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a framework called Surrogate Mutual Information Maximization Domain Adaptation (SIDA) with fruitful theoretical analysis. In the framework, along with the cross-entropy loss, and surrogate mutual information loss, auxiliary loss, and laplacian regularization are applied. Experimental results demonstrate comparable performance with state-of-the-art methods.",
            "main_review": "Strengths:\n1. The strong theoretical analysis supports the intuition of applying mutual information maximization.\n\n2. The paper is well written and supplied with detailed explanations.\n\nWeaknesses:\n1. In section 4.2, the mixture distribution is the average distribution of the source and target domain. The ratio between the source and target is fixed to 1 for the whole training. However, we can give more portion to the source at the beginning of the training since unlabeled target data may have noisy representation at the beginning. As the training evolves, the portion of the target can be increased, because the main purpose is to classify the unlabeled target domain. Have authors considered this?\n\n2. The proposed Theorem 4.1 emphasizes that minimizing the MI term may benefit in minimizing the risk on the target domain. However, does the proposed theorem have a tighter bound compared to previously introduced bounds? According to the proof of the theorem, it seems that the proposed bound is not tighter than the theorem proposed in (Ben-David et al., 2007).\n\n3. The proposed upper bound still can be arbitrarily significant even though the MI loss term is minimized because the target distribution is approximated to the surrogate distribution Q.\n\n4. There are many hyperparameters to optimize. The coefficients of MI loss and auxiliary loss, and the hyperparameters of surrogate distribution optimization (k, t, learning rates).\n\n5. Hyperparameter values are quite different among different tasks ((0.3, 0.1) for Office-31, (1.3, 1.0) for Office-Home, and (3.0, 1.0) for VisDA-2017). What was the hyperparameter search space and is it scalable to various tasks? Are performances sensitive to hyperparameter values?\n\n6. More baselines are recommended to be compared. For example, Kang et al. (2019) achieve 87.2% (SIDA: 84.0%) on VisDA-2017, and 90.6% (SIDA: 90.4%) on Office-31 and it is not included in the baselines. Compared to state-of-the-art methods, the empirical significance of the paper is marginal.",
            "summary_of_the_review": "The paper provides theoretical analysis behind the mutual information maximization, but it still has a pitfall of the mis-approximating surrogate conditional distribution. Many hyperparameters along with the four loss terms hamper the generalizability of the proposed method to various tasks. Experimental results are not significant compared to state-of-the-art methods.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new method for domain adaptation by maximizing surrogate mutual information (MI). The motivation is that MI of features within the same class (regardless of which domain they come from) should be maximized. To achieve this goal, this paper introduces a surrogate distribution to estimate the MI of mixed source and target distribution. The proposed method is evaluated on commonly used benchmarks and demonstrates significant improvements.",
            "main_review": "Strength:\n\nThe idea of maximizing the MI of mixed source and target domain is novel, compared with previous UDA methods that maximize MI of input and label within each domain [1][2]. The authors also provide theoretical analyses and complete experiments of the proposed method.\n\nWeakness:\n\nMy major concern is the surrogate distribution construction in Section 4.3.2. What if simply using pseudo-labels to get the conditional distribution, as mentioned in [3]? Although the author claims three advantages compared with pseudo-labeling, it is still not clear quantitively or supported by experiments. \n\n[1] Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation (2020)\n[2] TVT: Transferable Vision Transformer for Unsupervised Domain Adaptation (2021)\n[3] Deep transfer learning with joint adaptation networks\n",
            "summary_of_the_review": "This paper provides a new view of transferring knowledge across domains by maximizing MI of features within the same class. The paper is novel but needs more ablation studies.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}