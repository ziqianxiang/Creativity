{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a machine learning technique that includes a predictor for whether a function is vulnerable or no from its code. In addition to returning this label, the method also highlights the lines in the code that are the root cause of the predicted vulnerability label of a function.\n\nThe paper is not based on a technique that sees the actual data flow in the program, but guesses these lines in an unsupervised fashion from the vulnerability predictor.\n",
            "main_review": "The main selling points of the paper are that the technique can highlight the lines that participate in a vulnerability. The idea is a combination of learning techniques, applied in a specific combination useful for analysing vulnerabilities in code.\n\nTechnically, the paper doesn’t differentiate well from work of Nguyen et al, 2021 (cited in the paper). Other than some technical differences, it is not clear if there is any other advantage of the method. For example, the overview claims that the approach enables to “...find and highlight code statements, in functions or programs, truly relevant to the presence of significant source code vulnerabilities...”. However, the same can be achieved also with the previous approach. Furthermore, the previous work performs significantly better if extended to semi-supervised mode where a small number of functions have their vulnerable statements labelled.\n\nIn terms of claimed precision, the work compares to a few works, best of which is again Nguyen et al, 2021. The comparison table sometimes matches the precision numbers for the baseline of that previous paper and sometimes is suspiciously lower (making the baseline look worse than it is). The dataset itself is taken from the Code Gadget Database, which is introduced in the VulDeePecker paper. This paper chooses to split the dataset in a different way and does not compare to this baseline. The explanation for why this baseline is omitted in the evaluation is also unclear - the need of the baseline to compile the code is fine, but this dataset should have no problems to compile as it was done in a prior work.\n\nThe work on Nguyen et al, 2021 also has flaws that are shared by this paper as well. One of the problems is that there is no real ground truth, but this data was collected based on heuristics observing changes in code. Based on these heuristics, the paper is evaluated on how well it points to the lines responsible for a vulnerability, but this data wasn’t labelled in the original dataset and the authors do not claim to have manually checked that all (most) of it is correct.\n",
            "summary_of_the_review": "The paper explores an area that is of narrow interest and applies existing techniques to a vulnerability detection problem. The choice of technique and the empirical results are inconclusive. There are significant unexplained differences in the numbers and good reasons to suspect incorrect values in the evaluation.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes an end-to-end learning model to detect code statements related to software vulnerabilities. The model learns a set of latent variables to represent the relevance of the code statements to the detected vulnerability using mutual information. Based on the observation that vulnerable codes have some identifiable patterns, the authors propose clustering of the statements and applying supervised contrastive learning on the clustered statements to improve the reliability of detected vulnerable statements.",
            "main_review": "Strength:\n\n+ This work tackles a practical problem of fine-grained vulnerability detection. Detection of actual code statements that cause the vulnerability reduces the manual verification of vulnerabilities detected by an automated method. \n+ Leverages the presence of vulnerability patterns to propose an improvement over the existing works.\n\nConcerns:\n\nAlthough the motivation of using k-means clustering  to group vulnerable source code statements with the same vulnerability patterns sounds reasonable, clustering adds some extra complexities. It is not clear how to choose an appropriate value of k for the k-means clustering algorithm. The ablation study suggests that the value of k should be the number of patterns for a specific type of vulnerability. However, there could be different types of vulnerabilities and each one could have a different number of patterns. Non-vulnerable statements can generate some unexpected patterns too.\n\nThe effectiveness of the added complexity is not evident from the ablation study. Figure-6 shows the reduction in performance with an increased number of clusters but the improvement over 1 cluster (or no clustering) is not very significant.\n\nIn Table 1, with a higher number of selected statements (K=10) the VCP and VCA of the proposed measures increases compared to K=5. However, improvements over the baseline models ICVH seems to be reduced in all metrics with K=10. It seems the benefit from the proposed approach may become  insignificant with a slightly higher value of K (e.g., K=15).\n \nThe approach considers source function/program with fixed length of L statements. How to choose L? Does this impact the performance of the proposed method? L=100 used in this model seems smaller considering the motivation states that the goal is to help developers find vulnerable code from hundreds or thousands lines of code. Also, it seems the experimental dataset contains on average ~50 lines of code, which should not be considered as a sufficiently large number to motivate the overall approach.  \t\n\nAdditional questions:\n1. The k-means clustering is performed on a mini-batch. What is the mini-batch size? Does the size of the mini-batch impact the overall performance? \n2. Does the explanation of figure-3 consider that the dataset contains more than one type of vulnerability?\n3. How is label classification accuracy (ACC) in Table-1 computed?\n4. How are the inputs processed? Although, Section-2 refers to the experiment section for this discussion, it seems there is no discussion on input embedding in the experiment section/appendix. \n\n\n\n\n",
            "summary_of_the_review": "The proposed end-to-end approach adds novelty over existing works by introducing clustering with the supervised contrastive learning. However, the effectiveness of the added complexity is not well-justified. Although the motivations seems reasonable, the proposed design and experimental results do not sufficiently justify the changes.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper introduces a new method for training neural networks to label individual lines of source code as contributors to vulnerabilities at the function or program level. The authors accomplish this by adapting a contrastive learning approach from the literature, adding in clustering to allow for detection of different vulnerability patterns. Their method outperforms other existing baselines, achieving better recall and accuracy.\n\nNote: see disclaimer in main review\n",
            "main_review": "DISCLAIMER: During my review, I did not reference the cited Nyugen 2021 paper and so did not pick up on the overlaps in text pointed out by reviewer QaHV in a comment. The review below is strictly based on the paper itself, but after having seen the flagged overlaps I share the concern of the other reviewer in terms of the paper needing additional ethics review to determine if this is a case of the authors themselves reusing text from another of their papers or pulling text from a paper they did not write.\n\n=======\n\nThis work’s main strength is that it contributes a new approach for statement level vulnerability detection that does not require labels at the individual statement level for training. It synthesizes multiple previous information theoretic and contrastive learning approaches from the literature in a novel way to allow for this to happen, and it does achieve good results on the dataset which it is evaluated on. The method seems mathematically sound and is well-motivated.\n\nThe main limitation of this paper is the dataset on which it is trained and evaluated. According to the cited data source, the dataset contains a mixture of production, synthetic, and academic programs with software vulnerabilities. If possible, it is important for this work to specify what proportion of the programs being evaluated comes from each source. Since the method is built around detecting shared patterns of vulnerable lines across programs, it is crucial to understand how much of the algorithm’s measured performance is coming from being able to detect the patterns present in synthetic vulnerability examples. Please add a breakdown of performance by example type. \n\nAdditionally, the paper’s method is built around the assumption that, typically, only a “few core statements” are the source of a vulnerability. There is no work cited to support this claim or any evidence presented in the paper itself (aside from the performance of the approach). Please add information about the average number of vulnerable lines in the training and evaluation datasets.\n\nThese limitations do not mean that the method is not valid, or novel, but depending on the number of synthetic programs in the dataset, some of the bigger claims in the paper about working on “real-world” code may need to be qualified or moderated. It is well known that there are not large-scale real-world vulnerability datasets available, so this is not a suggestion for the authors to evaluate their method on a “better dataset,” but rather to be clearer about the performance claims in the paper given the limitations of the data. \n\nIn summary, I believe the paper would benefit greatly from the addition of:\n- More detailed statistics on the distribution of program types in the dataset (synthetic vs. real-world vs. academic)\n- Performance statistics for each program type individually\n- Additional data on the typical number of vulnerable lines in the dataset\n\nBelow, I give some more specific in-line suggestions for improvements:\n\nSpecific comments\n\nPart of the sentence at the beginning of the first full paragraph of page 2 seems to be missing\n\nAt the beginning of section 2, it would be good to clarify, when defining the label Y, that models are only being trained on a single vulnerability type at a time, rather than combining many types of vulnerabilities into one dataset\n\nWhen it is stated that “there is usually a small subset with K code statements that actually lead to F being vulnerable,” can the authors provide some information about previously measured values of K if they are available?\n\nI had a hard time understanding the sentence after equation 3, perhaps the relationship to epsilon could be described in words?\n\nAt the end of section 2.1, it was difficult to understand how omega and g interact with one another during training - a diagram of the training pipeline might be helpful here\n\nWhat does the index on omega_i mean in the selection process section at the end of section 2.1?\n\nIn section 3.2, the numbers of vulnerable and non-vulnerable functions for the two CWEs do not match the numbers shown in the cited source for the dataset (Li et. al. 2018a). Was there some additional selection process applied?\n\nThere could be more detail given on how the individual line labels are derived for evaluation purposes\n\nThe VCP measure given corresponds to recall, and it would be good to state this (or even just call the metric recall). One thing I found myself wondering is what the false positive rate is for the different methods (# of lines flagged that were not vulnerable / total # of non-vulnerable lines). You show that as you increase the K, you get better VCP, but this must be as a trade-off for worse false positive rate. It would be good to discuss this.\n\nFigure 6: In the caption, please specify what # of lines (K) was used for these plots (from comparing to the table, I believe it is 10, but it would be good to state explicitly)\n\nFigure 6 seems to suggest that the method, without clustering (# of clusters = 1), would still perform better than existing methods. If this is the case, why include the clustering? Is the idea that real-world vulnerabilities will have much larger numbers of patterns? If so, please clarify in the text.\n\nAppendix: please clarify what kinds of programs are included in the dataset; are they all single functions? Additionally, what tool is used for the tokenization step?\n\nAppendix: it is stated that each method is run 5 times and the results are averaged. Is a different seed used each time?\n",
            "summary_of_the_review": "\nThe paper introduces a new method for source vulnerability detection at the single-line level that performs well on the dataset it is evaluated on. While the limitations of the method, and in particular the dataset, need to be made clearer, overall the work is novel in that it is one of the only approaches that does not require labels at the line level, and it outperforms existing methods significantly. \n\n======\n\nNOTE: see disclaimer in main review\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
            ],
            "details_of_ethics_concerns": "Seeing the previous comment from reviewer QaHV about overlap with one of the citations, I believe the paper should undergo an additional review to investigate the overlap.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes to localize vulnerabilities of source code at the statement level. The paper approaches the problem by first leveraging the mutual information theory to attribute the importance of each statement to the prediction of the vulnerability. It then designs a contrastive learning objective to further improve the prediction results. Empirical results demonstrate that the proposed technique outperforms state-of-the-arts.",
            "main_review": "Software engineering and security community has been studying automated vulnerability detection for many years. This paper approaches this problem by learning to localize the vulnerable statements in an unsupervised fashion. While the studied problem is critical, this paper has the following issues that need to be addressed before acceptance.\n\nNovelty. The authors mentioned their design of mutual information theory is inspired by Chen et al. (2018); Nguyen et al. (2021). Is there any new design in this paper as compared to these related works? If yes, could the authors highlight what they are and how they contribute to the outperformance of this paper over Nguyen et al. (2021)? If no, it seems the only new design is clustered contrastive learning. In the ablation study, the authors should clarify whether their improvement over Nguyen et al. (2021) only comes from such a design or any other potential designs not described. However, the current ablation studies only investigate hyperparameters (e.g., number of clusters).\n\nIt seems that learning pi (for Bernoulli distribution) based on the input statements and then relaxing it to make it end-to-end trainable shares a similar spirit of the attention mechanism in neural networks, i.e., learning an input-dependent soft attention score and applying to the input. Could the authors discuss their similarities and differences and (if possible) compare them by experiments?\n\nIt is also unclear how clustered spatial contrastive learning can help localize vulnerabilities. It relies on K-means to first cluster the close statement embeddings across multiple functions in a mini-batch, and then train the model to maximize the similarity between these samples. This looks like the model is trained by its own prediction. Why does encouraging those samples already in the cluster to be closer to each other can help the model learn a good representation? Also, the authors claim that such a design can improve model robustness. However, there is no experiment to back this claim up.\n\nCould the authors add more ablation studies to show how each training objective, i.e., optimizing classifier, selection process, and clustered contrastive losses, contributes to the model's performance? This is important to justify the design choices made in this paper.\n\nThere are several deep-learning-based vulnerability/fault localization approaches [1,2,3]. The authors should at least discuss how the proposed technique compares to them by including a related work section.\n\nReferences: \n\n[1] Hellendoorn, Vincent J., Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. \"Global relational models of source code.\" In International conference on learning representations. 2019.\n\n[2] Dinella, Elizabeth, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang. \"Hoppity: Learning graph transformations to detect and fix bugs in programs.\" In International Conference on Learning Representations (ICLR). 2020.\n\n[3] Vasic, Marko, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh Singh. \"Neural program repair by jointly learning to localize and repair.\" arXiv preprint arXiv:1904.01720 (2019).",
            "summary_of_the_review": "This paper needs more experiments to back its main claims up. (1) how novel are the designs as compared to previous works? (2) how do they contribute to the performance? (3) how does its design improve the model's robustness. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
            ],
            "details_of_ethics_concerns": "I noticed several copy-pasted texts in this paper from its citation Nguyen et al. (2021).\n\nExamples:\n\n(1) On page 6, the first paragraph in Section 3.1: \"From machine learning and data mining perspectives, it seems that the existing methods in interpretable machine learning\" is the same with Nguyen et al. (2021) Section IV (2nd paragraph).\n\n(2) On page 6, the second paragraph in Section 3.1: \"We did not compare with VulDeeLocator.....\" is almost the same as the sentences in Nguyen et al. (2021) Section IV (3rd paragraph).\n\n(3) On page 6, the second paragraph in Section 3.2 \"Labeling core vulnerable statements for evaluation,\" has the same section title as Nguyen et al. (2021) Section IV-A(2) (with the same sentences like: \"we used the description of vulnerability information (e.g., the comments and annotations) in the original source code).\"\n\n(4) On page 7, in subsection \"Measures and evaluation,\" the last sentence: \"In addition to VCP and VCA measures, we also reported the label (i.e., Y)\" is the same as the last paragraph of Nguyen et al. (2021) Section IV-A(5).\n\n(5) On page 8, Section 3.4, \"In order to demonstrate the ability of our proposed method in detecting and highlighting the vulnerable code statements in the vulnerable functions to support security auditors and code developers,\" is almost the same (with minimal changes) with Nguyen et al. (2021) Section IV-B(4)\n\n(6) On page 8, Appendix A first paragraph, \"removing comments and non-ASCII characters, mapping user-defined variables to symbolic names...\" is the same as the sentence in Nguyen et al. (2021) Section IV-A(3).",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a clustered spatial contrastive learning approach (FinVulD-IC) to cluster the vulnerability-relevant code statements that were produced by leveraging the mutual information theory in learning the latent variables which can represent the relevance of the source code statements to the corresponding vulnerability. By comparing with some baselines e.g., RSM, L2X, ICVH,  FinVulD-IC proves its effectiveness in detecting real-world two types of vulnerabilities i.e., CWE -399 and CWE-119. Furthermore, it also provides two examples to show the capacity of their approach in finding the vulnerable statements that trigger this vulnerability. ",
            "main_review": "Compared with current mainstream DL-based techniques such as Devign[1], BugLab[2],  this paper proposes a brand new approach by combining with mutual information theory for vulnerability detection, which is very interesting and meaningful. However, because of the innovation of the approach, a large number of experiments are needed to verify the feasibility of the proposed approach. There are some questions that need to address:\n\n(1). The comparison with some state-of-the-art techniques is missed to show the superiority of the proposed approach. Although these state-of-the-art techniques may not target locating the vulnerable statements such as Devign, there are some other shared evaluation metrics for example classification accuracy that can be used for comparison. \n\n(2). This paper mentioned that the selection process is robust, however how to prove its robustness is not clear. Why $\\alpha$ higher than $10^{-1}$ can robust the selection process. In Figure 6, it seems that such a conclusion cannot get.\n\n(3). In the evaluation part, this paper compares with some baselines, for RSM, why need to choose it to compare, since it randomly chose the suspicious statements and the performance is expected to be low; for ICVH, I found the numbers of ICVH in Table 1 are not consistent with the original paper and there is a large discrepancy,  for example, under the setting of ICVH CWE-119 K=10: VCP 89.13 (original) VS 93.5 (yours) VCA 86.27 (original) VS 91.1 (yours),  ICVH CWE-339 K=10: VCP 86.82 (original) VS 84.5 (yours) VCA 80.46 (original) VS 77.0 (yours), however in the model configuration section, this paper claims that the structures are followed in the corresponding original paper. Hence, it is very strange and it is best to provide more details. \n\n(4). The introduction section is not well-organized, the contributions need to summarize and highlight for readers to capture the emphasis of this paper.\n\n\n\n[1]. Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks. Zhou, Yaqin et al. Neurips 2019. \n\n[2]. Self-Supervised Bug Detection and Repair. M. Allamanis, et al. Neurips 2021. ",
            "summary_of_the_review": "The proposed technique seems to be interesting for vulnerability detection but needs more experiments compared with some state-of-the-art vulnerability detection works to prove the effectiveness.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}