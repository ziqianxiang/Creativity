{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors propose a unified framework on the priority of learning from easy or hard samples. Based on this framework, a unified weighted loss that combines the characteristics of existing weighting scheme and its theoretical analysis is proposed. Based on the analysis, a new weighting scheme is proposed and then its performance is experimentally evaluated.",
            "main_review": "Strengths:\n1. This work is comprehensive and can be used in different domains and scenarios, as shown in the experiments.\n2. Both theoretical and experimental analyses are provided to support the claims in this work.\n3. This method covers the three modes of learning and can be switched flexibly in the proposed scheme.\n4. The introduction of model complexity and its connection between bias and variance is simple but effective. Proposition 1 is quite intuitive and reveals the essence of the weighting scheme based on the previous assumptions.  \n\nWeaknesses:\n1. To make this work more intuitive, the authors have to further clarify their claims in the abstract, e.g., a one-sentence summary of the 'preliminary conclusions' in line 8 should be added.\n2. The authors should give rigorous definitions of 'easy', 'medium', and 'hard'. In my opinion, Assumption 2 can be used for this purpose with minor modification.",
            "summary_of_the_review": "This work gives a high-level analysis and solution on the priority of learning from easy or hard samples. The theoretical analysis of the relationship between model complexity and weighting scheme is novel.  Experimental results also show its superiority against existing methods. I support acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper discusses the sample weighting problem in learning tasks and proposes a weighting scheme that can be adjusted based on the data characteristics and task prior knowledge. The proposed weighting scheme unifies previous work from easy-first mode (self-paced learning) to hard-first mode (focal loss). Experimental results show that the proposed weighting scheme achieved better results when the learning mode is properly set based on the data properties and task settings. ",
            "main_review": "Strengths:\n1. This paper presents a unified way to look at different weighting schemes.\n\n\nConcerns:\n1. All conclusions in this paper can be found from previous studies. Although the proposed method achieved better results, it doesn't change the fact that the optimal weighting scheme still follows the same heuristics (e.g., lower weights for noisy samples, higher weights for minority samples, etc.).  \n2. The theoretical analysis conducted in the paper doesn't add much insight about how to choose the weighting scheme. It is not clear to me how the Proposition 1 helps the discussion.\n\n\nAfter reading the rebuttal, I decided to increase the score by 2.",
            "summary_of_the_review": "This paper presents a unified framework for sample weighting. The contribution of this paper is not significant enough for the ICLR acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In the paper, the authors try to answer the question of whether easy or hard data should be learned first for a task in the sample weighting approach. They propose a high-level unified weighted loss to explore this question theoretically and a flexible weighting scheme to overcome the defects of existing weighting schemes. The effectiveness of the proposed method is verified through experiments on image classification, graph classification and object detection. \n",
            "main_review": "Strength:\n1. Figure 1 in the paper shows a comprehensive summary of existing weighting schemes in different domains, which is helpful for readers to get an overview of existing sample weighting-based methods. \n2. Experiments on various settings show the proposed method is better than the baselines.\n\nWeakness:\n1. The core concepts of the paper, easy or hard samples, are not well defined. It's better to have a definition of the meaning of easy/medium/hard because the words are quite relative (via comparisons). Does it depend on to what extent the data can be correctly classified?\n2. Prior knowledge of \"hard\"-first or \"easy\"-first is still needed through a way of assigning values to two hyperparameters of the proposed method. It is unknown that how these hyperparameters are chosen. Also, if such prior knowledge is known, some other corresponding weighting methods can be applied accordingly. \n\nComments:\n1. The authors claim that \"an ideal weighting strategy should let the weights for noisy samples to be reduced\". How does the proposed method distinguish whether a \"hard\" sample is a noisy sample (should downweigh it) or it is a valuable sample (should upweigh it, such as in class imbalance setting)?\n2. How to integrate the prior knowledge for the cases where we have both class imbalance and label noise in the data? Is it possible to handle this complex case for the proposed method?\n3. I have some doubts about the results of label noise experiments, i.e., Table 1. In label noise setting, overfitting label noise/applying hard-first mode at the beginning of the training would be very harmful. How could FlexW (hard-first) largely outperforms some easy-first baselines in label noise learning, such as S-model and Co-teaching?\n4. Importance sampling is a classic and important weighting method. I suggest adding it in Figure 1 and having some discussions about it. ",
            "summary_of_the_review": "The research problem is important, but some key parts of the proposed method are not well defined/explained, such as how to choose the hyperparameters related to prior knowledge, how to define hard/easy-first. I also have some doubts about how the proposed method could achieve such a good performance on label noise experiments. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper claims to propose a unified method for weighting examples while training. It first gives some survey on recent baseline methods and argue that there are disagreements on which samples, easy or hard, to learn first. Their method, dubbed as FlexW, is shown to contain some baselines methods, like Focal loss or SPL, and by tuning some hyperparameters, FlexW is shown to achieve slightly better performance than others. ",
            "main_review": "Strength:\n\n1. The paper gives an extensive survey on the existing weighting scheme (Figure 1) and propose a general one (Eq.(11)) that can contain others. \n2. Depending on different hyperparameters used, FlexW is shown to achieve better results than the baselines. \n\nWeakness:\n\n1. It is not clear how to choose the hyperparameters of (11) and determine which weighting schemes, i.e., among easy-first, medium-first, hard-first, to use for given data. The paper does not describe the details on this process, which significantly undermines the practicality of their method. With this reason alone, I do not think the work is enough for a publication at ICLR.",
            "summary_of_the_review": "Please see above main review. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}