{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Leveraging invariances in the underlying causal data generating mechanism to improve out of distribution generalization of prediction models is fundamental to wider-ranging artificial intelligence. The authors provide one iteration on this problem, defining a set of \"causal invariant transformations\" of the training inputs that do not perturb the causal relationship between inputs and outputs, and using those as a data augmentation tool to bias the learning algorithm to be \"invariant\" to those transformations. The authors give a number of results in terms of transformations required to be available to guarantee out-of-sample generalization. However, knowing this information is comparable to knowing the true input-output relationship, detracting from the practical utility of the proposed approach.",
            "main_review": "The problem formulation would benefit from more precision. It is not clear for instance how domains vary, is it in the distribution of $\\eta$? in the distribution of $X$? Are interventions that change the causal mechanism allowed? Does the set $\\mathcal H_\\star$ mean that there are multiple optimal functions? Why would the optimal function not be unique, is it reasonable to think that if two optimal functions give different predictions for some input-output pair $(X,Y)$, and say function 1 is further from the truth, that we can construct a distribution with higher probability on these examples resulting in function 1 have different expected loss than say function 2?\n\n\nI am not overly familiar with the literature on data augmentation, but the general idea proposed in this paper is to my knowledge a common strategy to train robust prediction models. The surveyed literature does not describe data augmentation techniques in any detail, I wonder how much overlap there is there?\n\nMinor:\n- Remark 1 states that $g(X)$ is independent of $\\eta$ but that $X$ may be correlated with $\\eta$, this is not true unless some additional restriction is imposed on $g$. For instance, if $g$ is invertible $X$ would be independent of $\\eta$ also.\n- Definition 2 should have $\\mathcal I_g \\subseteq \\mathcal T_g$.\n",
            "summary_of_the_review": "I find defining causal invariant transformations an interesting approach to OOD generalization but I worry that in practice this kind of domain knowledge is largely unavailable and that as a consequence none of the corresponding performance guarantees hold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors study the problem of OOD generalization using the lens of invariance. The authors show that if we have the knowledge of the causal invariant transforms, any transformation to the original data X that does not change the causal feature. For e.g., the authors use rotation of the object as a causal invariant transformation.  The authors propose a procedure that relies on the knowledge of these transforms and learns a predictor that is invariant under such transformations. The authors show that the solution to this minimization generalizes OOD. Since such transforms are not available in real data experiments such as the ones based on PACS, VLCS, the authors rely on using cycleGAN to learn such transformations. ",
            "main_review": "**Strengths:** The paper addresses an important and challenging problem, which is to achieve provable guarantees for OOD generalization when the data generation process has non-lineartiies. The paper is easy to read.\n\n**Weaknesses** The paper has several problems that I highlight below:\n**Major technical problem:** The authors claim that if the the set of transformations $T_g$ is known then that can allow OOD generalization with one domain. The knowledge of $T_g$ is hiding the whole problem that needs to be solved. I will provide a simple example and then generalize it to show that knowledge of $T_g$ leaves nothing to be solved: \n\nConsider the example 1 from IRM [Arjovsky et al.] \n\n$Y \\leftarrow X_1 + N$ \n\n$X_2 \\leftarrow Y + N^e$\n\nIn the above example, it can be shown that to generalize OOD one needs to rely on the causal parent $X_1$ of the label $Y$ only. One way to interpret IRM and ICP [Peters et al.] is that you want to use data from multiple environments to figure out what the causal parents of $Y$ are and only use those. Note that these methods do not assume any knowledge of the parents and children of $Y$ and the whole point is to learn to distinguish them.\n\n\n\nIn the language of the current paper $g(X_1,X_2)  = X_1$. \nNow let us try to see what is the set of transformations $T_g$ for the above function. Any transformation $T$ that leaves the cause $X_1$ unchanged and impacts the anti-causal variable $X_2$ is in the set $T_g$. If the transformation impacts $X_1$ it cannot be in the set $T_g$. If we know the set $T_g$, we know which features are causal in the above example and which features are non-causal. This knowledge is precisely what IRM and other methods set out to learn from the data. However, the current paper just assumes that we know the causes of $Y$  and chidren of $Y$ by knowing $T_g$. This is simply wrong. \n\nIf we generalize the above example to d dimensions, then there what we would have the following \n\n\n$Y \\leftarrow \\gamma^{T}X_s + N$\n$X_s' \\leftarrow Y + N^e$ \n\nIn the above example $X_s $ and $X_{s^{'}}$ together form the Markov blanket of the label $Y$. The set $T_g$ will contain transformations that do not impact $X_s$ but impact $X_{s'}$. Therefore, $T_g$ contains the knowledge of what variables are causal parents and what variables are not in the Markov blanket of the label $Y$. \nNow if the authors were to say that we can learn the transformation set $T_g$ from the data, the way to do it is using data from multiple domains. This makes the whole claim circular and shifts the entire problem to learning $T_g$, which is no easier. Hence, the paper does not make progress on the real central part of the whole problem. To learn $T_g$ one has to still rely on IRM or other approaches before it such as ICP.\n\n**Missing discussions on Lu et al. and Robey et al.** The authors seem to ignore works that precisely try to address the same problem of non-linear OOD generalization. In Lu et al. the authors study non-linear invariant risk minimization and provide OOD generalization guarantees. In Robey et al. the authors also consider a non-linear domain transformation model that is learned from the data. This model has similarities to causal invariant transformations that the authors rely on. A discussion and comparison with these works would be useful.\n\n**Claim on generalization in Theorem 1 not completely true** The work of [Yamaguchi et al.] already has some generalizations. \n\n**MNIST experiments** Why are you using grayscale images in MNIST experiments? This is not correct and unfair to existing works that do not contrast colored image against its uncolored counterpart. The whole point is to learn from colored MNIST images. \n\n \n\n\n[Lu et al.] Non-linear invariant risk minimization: a causal approach \n[Robey et al.] Model based domain generalization  \n[Yamaguchi et al.] When is invariance useful in an Out-of-Distribution Generalization problem ?\n\n\n\n\n \n\n",
            "summary_of_the_review": "Overall, the paper is trying to address an important problem. However, the paper has major technical problems, the authors assume the knowledge of invariant transformations and that leads to the misleading theory result that says without multiple domains you can do OOD generalization. The whole problem is now shifted to that of learning transformations, which is no easier. There are other major gaps in discussion and comparisons to related works as well.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This work proposes a practical algorithm for OOD generalization based on a theoretical connection between invariant causal mechanisms and data augmentation. The OOD problem is posed as a minimax problem and shown to be equivalent (under some conditions) to another minimax problem where the max is taken over the set of \"causal invariant transformations\". The authors show how this objective can be simplified by restricting the max to a smaller \"causal essential set\" of transformations. The practical algorithm is demonstrated on standard benchmarks for OOD generalization. The causally invariant transformation are learned using CycleGAN.",
            "main_review": "I enjoyed reading this paper. The idea is simple, well motivated and presented nicely. I am not following the literature closely on OOD generalization, but the contribution seems novel and significant. \n\nPros:\n- Paper is easy to follow.\n- The theoretical results are sound and well presented (although I did not read the proofs).\n- The connection between data augmentation and OOD generalization is interesting and well presented.\n- I thought the use of CycleGAN to learn the causal invariant transformations was a good idea (I never saw that before, as a non-expert).\n- I found the experiments sufficient.\n\nCons:\n- In Remark 1, the way the spurious correlations are modelled in Equation (1) should be detailed a bit more, given this is the central problem the paper tackles. A suggestion would be to take a classic OOD generalization problem (like the horse/camel example) and explain precisely how each variable maps to this problem and \"where\" exactly does the spurious correlation come from in the model.\n- I think an important point should be emphasized more: Theorem 2 makes the central connection between the OOD problem as formalized by (2) and the \"data augmentation formulation\" with $\\mathcal{T}\\_g$. Knowing exactly what are the causal invariant transformations for a specific application can be tricky. For instance, the operation \"changing the background\" is probably a causally invariant transformation, but is probably hard to formalize or learn. Similarly, for lighting or weather ... etc. I had this question early (around equation (4)) and I think referring to CycleGAN as a way around this problem earlier would comfort readers like me. This potential difficulty was raised in the paragraph just before Section 5, but I'd like to see a more extended discussion.\n\nMinor:\n- Mention what is the \"invariant causal mechanism\" explicitly in (1). I think it is said only quickly that g and m are fixed at the beginning of Section 3.2, without using the \"invariant causal mechanism\" terminology. \n- In the paragraph just before Section 3.2, what does \"$\\eta$ can be non-separable with $g(X)$\" means?\n- In (3), might help giving an example of $\\phi(w)$ for a specific loss function $\\mathcal{L}$, like the L2 or the L1 loss.\n- In the second remark after (5), it is claimed that \"The set $P_\\text{aug}$ is likely to contain distributions that do not have spurious correlations or even entail opposite correlation\". That was not obvious to me.  ",
            "summary_of_the_review": "I recommend acceptance since the paper is well written and presents interesting new ideas. The contribution is well justified by theory and appears to be practical. (My assessment of novelty should be taken with a grain of salt, given I am not an expert in the field.) ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors show that if we could perfectly augment only the non-causal features when training or learn a predictor which is perfectly invariant to these features, then we could learn the causal invariant predictor. They then show that when we know a priori the non-causal “spurious” features and are able to modify them without perturbing anything else, we can augment these features (such as with Cycle-GAN) to learn a predictor which is more invariant to these perturbations.",
            "main_review": "Strengths: The paper confirms empirically the general consensus in the OOD community, which is that if we could identify the non-invariant features and if we could modify them in the observation space without changing anything else too much, then we could improve a predictor's OOD generalization.\n\nWeaknesses:\n\nThe following is a one-line description of each of the theorems:\n\n* Theorem 1: If the label depends on X only through the causal features (i.e., any non-causal predictor is not minimax), then the causal predictor is minimax.\n* Theorem 2: If we consider all possible transformations which *only* modify the non-causal features, then the causal predictor is the minimizer of the minimax risk under all such transformations.\n* Theorem 3: The causal invariant predictor which uses all causal features achieves minimum risk among all predictors which are invariant to a \"representative/complete\" set of non-causal transformations.\n\nUnfortunately, I don't think any of these statements say anything new or interesting---these results are obvious both intuitively and mathematically. The existing literature on data augmentation is precisely trying to move *beyond* such claims. Here's just a few papers which attempt to give theoretical results beyond this: [1, 2, 3]. It's concerning that this paper has no review on the theory of data augmentations, since that's what this paper is actually about.\n\nFurthermore, I believe the theory has mistakes, Theorem 1 in particular. Observe that the model requires $g(X) \\perp \\eta$, but allows for correlation between $X$ and $\\eta$. Suppose I add an additional element $x'$ to $X$ which is precisely equal to $m(g(X), \\eta)$, and that $g(X)$ does not depend on $x'$. Then clearly we still have $g(X) \\perp \\eta$, and yet the minimizer $h\\in\\mathcal{H}_\\*$ would be to simply output $h(X) = x'$. Thus the distribution we've defined would be in $\\mathcal{P}$, yet it would not hold that $\\mathcal{H}_s \\subset \\mathcal{H}$*. This is almost certainly fixable with some additional restrictions on the model, such as a temporal ordering of the variables...**but these are exactly the additional restrictions that the paper claims to obviate compared to existing work**. Perhaps this is fixable in some other way but this very basic error is not promising for the remaining theory.\n\nEven just considering the experimental results, this paper is fundamentally answering the wrong question which is not relevant to the literature in which the authors have contextualized this work. In the field of data augmentation, it is of course desirable to identify a priori the \"spurious\" features and, if possible, provide augmentations at train time to induce invariance to such features. This is an entirely separate field of research and it has led to great successes in improving generalization (Cutout, Mixup, Augmix, Gaussian blur, etc.).\n\nThe **entire point** of this line of OOD generalization is when we do not a priori know such a partition. Using Cycle-GAN to modify the background of an image requires that we explicitly identify the background as a possible spurious feature, and furthermore requires that we have access to datasets with different \"styles\" to *train the GAN in the first place*. It is immediately clear why this cannot be assumed when we are trying to generalize under major distribution shift. Randomly flipping the color in C-MNIST is cheating for exactly the same reason.\n\n**Additional notes:** I've found that the authors sometimes conflate low expected risk with low minimax risk---these are two very different concepts and it's essential that the distinction is maintained. For example, at the end of the second paragraph: \"It has been shown that a model would perform well across different domains if such a causal mechanism is indeed captured.\" No, it has been shown under these models that the causal invariant predictor would be minimax optimal (under varying threat models of distribution shift)\n\nI also feel that the intro/related work repeatedly points to the restrictive assumptions made by previous works. This heavily implies that this work will provide genuine theoretical guarantees under weaker assumptions, which is not the case. I would caution the authors to refrain from pointing to these formal models as weaknesses when the contribution of this paper does not include theoretical guarantees under weaker assumptions.\n\n[1] Fabio Anselmi, Joel Z. Leibo, Lorenzo Rosasco, Jim Mutch, Andrea Tacchetti, and Tomaso Poggio. Unsupervised Learning of Invariant Representations in Hierarchical Architectures\n\n[2] Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel R. D. Rodrigues. Generalization Error of Invariant Classifiers.\n\n[3] Akiyoshi Sannai and Masaaki Imaizumi. Improved Generalization Bound of Group Invariant/Equivariant Deep Networks via Quotient Feature Space. ",
            "summary_of_the_review": "The theory is known and doesn't say anything interesting (and also I think there are mistakes). The experiments are valid, but fundamentally answer the wrong question. The whole point of this line of research is to improve OOD generalization when we *don't* know a priori which transformations are \"causal\" and which are not, and the point is to learn it from data (when and where possible) or learn to be invariant to it in some other way. Having access to an oracle which can induce invariance in the predictor by providing the \"correct\" data augmentations is unrealistic and doesn't give us anything in practice.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}