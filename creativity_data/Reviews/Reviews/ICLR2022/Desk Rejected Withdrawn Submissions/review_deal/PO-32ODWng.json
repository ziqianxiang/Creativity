{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes \"probe scaling\", which attaches classifier heads to intermediate feature representations, and learns a post-hoc recalibration model with the logits from the intermediate layers. The method is tested on a wide range of datasets and models.",
            "main_review": "Strengths:\n\n- This paper is generally well-written and easy to follow.\n- Further, the topic of better post-hoc methods for calibration is a relevant and important topic to the general uncertainty quantification community.\n- The experiments section explores a wide range of datasets and model choices.\n\nWeakensses:\n\nGenerally, this paper lacks depth and is mostly centered around empirical evaluations, which may have some issues (discussed in more detail below) and does not provide much insight.\n\n- The wrong reference was given for the definition of calibration (Equation 1). Guo et al., 2017 defines *confidence* calibration, whereas the Equation (1) is a different notion of calibration (sometimes referred to as \"multiclass\" calibration). Please check Definitions 1, 2, 3 in Kull et al. 2019.\n- $f_y$ in Equation (2) is not defined. Also note that classwise calibration is not weaker compared to confidence calibration (which is the core metric for ECE used in this paper). Equation (2) *is* a weaker notion than Equation (1) (a.k.a. multiclass calibration).\n- For all of the experiments, pre-trained models should have been used instead of training the models from scratch (pre-trained models are only used for ImageNet). In fact, many of the trained models don't even achieve accuracy that would be expected of the model: e.g. Resnet50 should achieve ~78% accuracy on CIFAR100, ~95% on CIFAR10.\n- It is hard to understand what the point of extracting logits from intermediate layers are — essentially, you are ignoring the rest of the network for the prediction task. In fact, the final layer logits should produce the best accuracy, but the fact that probe scaling (i.e. a linear combination of intermediate and final layer logits) improves accuracy in the experiments raises concerns about whether the base model training even finished.\n- Continuing from the above points, have the authors checked the accuracy of each intermediate layer's logits?\n- While probe scaling is proposed as a post-hoc recalibration method, it does not consistently improve ECE over baseline methods. Given this is an empirical paper, the lack of strong calibration results is quite concering.\n- How is \"contribution\" defined in Section 4.3?",
            "summary_of_the_review": "This is an empirical paper with weak results. Further, the method is not principled and lacks depth and insights.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a calibration method known as “probe scaling.” After training, linear classification layers are trained on the intermediate neural network representation, using the training data. At test time, the final softmax logits are a linear combination of these intermediate logits, where the parameters of the linear combination are learned using validation data. The authors demonstrate that this approach results in good calibration (as measured by ECE and proper scoring rules), as well as better accuracy than the original model.",
            "main_review": "**Significance.** While technically this method is “a generalization of temperature scaling,” I don’t think that this method should be described in this way. Part of the appeal of temperature scaling is 1) it is simpler than other calibration methods, 2) it has almost no training cost, and 3) it does not affect accuracy, which decouples calibration and accuracy. This method is significantly more complicated, requires many parameters that need training, and does not decouple accuracy and calibration. (I’m not saying that extra accuracy is a bad thing; but I don’t think that this method can be claimed as a “calibration method.” See below.)\n\nThis method learns a significant number of parameters on the validation set, and these parameters have the capacity to change the predictions of the neural network. In other words, both the training and validation data are used to learn parameters that impact accuracy. I imagine that I could also get high accuracy if I used a standard training procedure, but where I trained on both training + validation data. These experiments should probably compare against such a baseline, to determine if this is actually an efficient allocation of validation data.\n\nSo would a practitioner use this method in practice? I’m not sure, and the reason I’m not sure is because I don’t think that the set of experiments sufficiently proves its superiority. Would you get better results if - for example, you did temperature scaling, but trained the network on 49,500 data points (instead of 45,000) and only used 500 to optimize the temperature parameter? I imagine that this is a strong baseline, as learning a single temperature parameter probably doesn’t require that much data. These should be the baselines that this method compares to, in order to draw a reliable scientific conclusion about what the probe scaling method is actually doing.\n\n**Imprecise/unscientific language and claims.** The paper uses imprecise and unscientific terminology, and some of the key ideas behind the method are only offered as vague intuitions.\n- Throughout the paper, the authors claim that “earlier layers tend to learn general-purpose representations whereas later layers specialize.” The authors should be more rigorous about what they mean by “general-purpose” and “specialize.” Furthermore, it is not obvious from reading this paper why “general-purpose” features should be better for calibration.\n- In Figure 2 (reliability diagrams), the authors claim that temp. Scaling and probe scaling perform better than the other post-hoc methods, “with probe scaling performing best overall.” How is performance measured from reliability diagrams? There does not seem to be any statistically significant difference in the reliability diagrams.\n- The term “probe” is very vague and nonstandard, especially since what you are referring to as a “probe” goes by the more common name “linear projection.” I think it’s okay to call your method “probe scaling,” but it should not take me until page 5 to get a concrete definition of what a probe is.\n- Why do the authors use a 1D convolution for computing $R(\\mathbf x) \\beta$? It seems that standard matrix-vector multiplication works in this case, and using a 1D convolution only makes things more confusing to me as a reader.\n\n**Missing citations**: There are lots of works that apply intermediate classifiers after various neural network layers. One of the earliest works that I’m aware of is Deeply-Supervised Nets (Lee et al., 2015). This idea is also heavily used in anytime classification networks (e.g. Bolukbasi et al., 2017, Huang et al., 2017). Of course, these works are not using these intermediate classifiers for calibration, but it would be good to cite them.\n\n**Other notes.** I really appreciate that the authors include a significance test for their results. I hope that this is a practice that can be used more widely throughout the community.\n\n**Small notes**:\n- In proposition 1 - $\\lambda$ can be set to anything that is proportional to $1/\\sqrt{N}$ - it does not have to be set exactly to $1/\\sqrt{N}$\n- The notation in the paper is a bit inconsistent and confusing. Do bold lowercase letters represent vectors (the common notation in ML literature)? If so, then $\\beta$ should be bolded. Furthermore, the use of non-bold $y$ and bold $\\mathbf y$ in Eq. 6 is very confusing, since the former is an index and the latter represents a (non-vector) label.\n- Section 3.1: “Nevertheless, we remark that the latter is by no means necessary…” I am confused by this sentence - what are the probe vectors trained on?\n- Algorithm 1: $R(\\mathbf x)$ is a $| \\mathcal Y | \\times d$ matrix, not $d \\times | \\mathcal Y |$\n\n\n**Refs**:\n- Deeply-Supervised Nets (Lee et al., 2015)\n- Adaptive neural networks for fast test-time prediction (Bolukbasi et al., 2017)\n- Multi-Scale Dense Networks for Resource Efficient Image Classification (Huang et al., 2017)\n",
            "summary_of_the_review": "Overall, the experiments do not convince me that probe scaling is a useful method. This is not to say that the experiments aren’t thorough (I especially appreciate the significance tests), but rather that I don’t believe that the experiments are comparing against the appropriate baselines to draw a scientific conclusion. In addition, I believe that this paper could be improved by using more precise scientific language. As it stands, I would vote to reject the paper.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a method for post-hoc calibration in multi-class classification. The so-called \"probe scaling\" method inserts linear probes after intermediate layers in the original network. After this insertion, the training process trains all these inserted linear probes. Finally, all outputs from these linear probes are concatenated together and the aggregated representation is used to train another calibrator on a separate calibration data set.\n",
            "main_review": "strengths:\n1. This paper is well-written and easy to follow.\n2. The presented empirical evaluation is organized and clear. \n3. The empirical study on contributions of linear probes is very interesting. However this paper fails to describe or interpret this behavior. What are the possible reasons? Why do contributions of probes from the middle layer to the 80% depth vanish in VGG-like architecture? \n\nweaknesses:\n1. The part which concerns me most is the empirical experiment section. Firstly, to be frank, I do not think the comparisons with other competing algorithms are fair. In Section 3.1, the linear probes are trained on the training datasets as well while other competing methods only use the calibration data set and are \"truly\" post-hoc. Although this paper claims this step is unnecessary and can be applied to pre-trained architectures on larger datasets, this paper does not present experimental results along this line. Secondly, A lot of recent works on post-hoc calibration for multi-class classification are missed, e.g. [1,2,3,4]. Comparisons with these algorithms should be done. Otherwise, the reasons why these comparisons are unnecessary should be described. \n2. The inserted probes are very high-dimensional since the intermediate representations are flatten before the probes are trained.  This can be impractical since the overhead is too high given the improvements over temperature scaling are not proportional to the increased overhead.\n3. The additional time spent on training linear probes and additional parameters are not described in this paper. \n4. This paper presents experimental results on ImageNet-C to test the performance in the case of distribution shift. This is a good thing. However, this paper fails to describe why the proposed method outperforms other competing algorithms in this case.\n\nminor issues:\n1. Conventionally, the $k$-dim simplex has $k+1$ vertices, and the simplex used in this paper is $\\Delta_{K-1}$\n\n\n[1] Zhang, Jize, Bhavya Kailkhura, and T. Yong-Jin Han. 2020. “Mix-n-Match: Ensemble and Compositional Methods for Uncertainty Calibration in Deep Learning.” ArXiv:2003.07329 [Cs, Stat], June. http://arxiv.org/abs/2003.07329.\n\n[2] Wenger, Jonathan, Hedvig Kjellström, and Rudolph Triebel). 2020. “Non-Parametric Calibration for Classification.” In International Conference on Artificial Intelligence and Statistics, 178–90. http://proceedings.mlr.press/v108/wenger20a.html.\n\n[3] Ma, Xingchen, and Matthew B. Blaschko. 2021. “Meta-Cal: Well-Controlled Post-Hoc Calibration by Ranking.” In International Conference on Machine Learning, 7235–45. PMLR. https://proceedings.mlr.press/v139/ma21a.html.\n\n[4] Patel, Kanil, William H. Beluch, Bin Yang, Michael Pfeiffer, and Dan Zhang. 2020. “Multi-Class Uncertainty Calibration via Mutual Information Maximization-Based Binning.” In . https://openreview.net/forum?id=AICNpd8ke-m.\n\n\n\n\n",
            "summary_of_the_review": "Given the theoretical results present in this work are very minimal, I would have expected a much more extensive empirical study on the proposed method. As already described in the above section, the presented empirical experiments are far from extensive and many existing works are missed. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose probe scaling, a post-hoc calibration method that utilizes the learned representations of intermediate layers in the neural network to improve the uncertainty estimation capability of DNNs. The authors also demonstrated that probe scaling outperformed temperature scaling on multiple benchmark datasets.",
            "main_review": "The proposed probe scaling is an interesting approach to improve post-calibration techniques. Although the results might look promising, my primary concern is that this is not a **post-hoc** method as you need access to the training dataset to train the linear probes, is that right? So this belongs more to a hybrid of trainable calibration and post-hoc calibration approaches, and the comparison to post-hoc approaches seems a bit unfair. Please let me know if I overlooked something with regard to my concerns.",
            "summary_of_the_review": "Since all the experiments are based on the claim that the proposed method is post-hoc, the experiments do not highlight anything meaningful.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}