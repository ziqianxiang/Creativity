{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Authors work towards lightweight neural networks. They split the output channels to generate data sequences with length T as the input to the recurrent layers with shared weights. They show experiments on various network architectures.",
            "main_review": "Strengths: \nAuthors compared with many architectures to demonstrate the effectiveness of the proposed method.\n\nWeaknesses:\n1) One major concern is that using recurrent networks may increase computation complexity. Authors should include FLOPs and inference time in all tables. Computation is a very important factor in networks - one can easily have a much stronger network with fewer #parameters but more computation. On the other hand, having FLOPs is not enough, as low FLOPs does not mean low inference time. Therefore, including both FLOPs and inference time can be a fair comparison.\n2) Authors should compare to more and latest network compression works, not just vanilla version. For example, Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Stabilizing the lottery ticket hypothesis.arXiv preprint arXiv:1903.01611, 2019 and Yonathan Aflalo, Asaf Noy, Ming Lin, Itamar Friedman, and Lihi Zelnik. Knapsack pruning with inner distillation.arXiv preprint arXiv:2002.08258, 2020.\n3) Similar works such as Tied Block Convolution: Leaner and Better CNNs with Shared Thinner Filters (AAAI 2021) need to be discussed and compared.\n4) Following comment 1) - if authors did not find improvement in FLOPs or inference time, I suggest looking at if there is any improvement on the accuracy or specific properties. For example, with the recurrent model, maybe the sequential relationship is easier to mode?",
            "summary_of_the_review": "Although using recurrent models for modeling split channels is new and may have interesting properties, authors are encouraged to provide computation comparisons. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents to use recurrent deep models to train lightweight CNN, which seems interesting to me. The results show some improvement. Overall the paper is well organized and easy to follow. However, there are also some points that need to be clarified.",
            "main_review": "Strength: applying CSRConv to lightweight network seems to be new.\nThe results show some improvement.\nWeaness:\nSome of the results are not quite convincing. \n\n",
            "summary_of_the_review": "1) It is not clear how the CSRConv recuces the parameters?\nI suggest the authors to give an example how a network changes by adopting CSRConv.\n2) The authors mentiond that they grid search to determine which convolutional layers in the backbone network should be replaced by\nCSR-Conv layer. I wonder if such search might lead to some overfitting?\n3) Since CSR-Conv might lead some performance loss, how is it compared with network compression? The comparsion in Table 3 seems not fair. I would see if we start from the same baseline network, how CSR comapres with network compression?",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this manuscript, authors propose the channel-split recurrent convolutional (CSR-Conv) layer as a light-weighted replacement for standard convolutional layers. Features maps are split into a sequence along the channel dimension, and then fed into recurrent neural networks (RNNs, GRUs, or LSTMs) to produce feature maps to be concatenated as outputs. The per-layer compression hyper-parameter (sequence length) are determined through grid search due to the  relatively small search space. Empirical evaluations are conducted on various backbone network architectures, which indicate that the proposed method can indeed produce highly accurate models with fewer number of parameters.",
            "main_review": "Strengths:\n1. The proposed method is simple yet effective, and can be used on top of various convolutional neural network architectures, including standard ones (ResNet and DenseNet) and light-weighted ones (MobileNet and ShuffleNet).\n2. The comparison with light-weight networks (in Table 3) indicate that the proposed method can achieve a better trade-off between model complexity and prediction accuracy.\n\nWeaknesses:\n1. For most application scenarios, in my opinion, the model size (or the number of model parameters) is often not major concern. In fact, the run-time speed is usually more important, but the recurrent computation mechanism is not so GPU-friendly, especially when compared with standard convolutional layers.\n2. In Section 3.1, authors state that the per-layer sequence length is determined by grid search. For each candidate per-layer sequence length combination, is the corresponding network trained from scratch? If so, could you provide more details on the overall training efficiency?\n3. Is it possible to adopt the weight sharing scheme widely used in NAS into the proposed method to improve the training efficiency (rather than training from scratch)?\n4. In Section 3.2, both inputs and outputs as treated as vectors for simplicity, and each recurrent unit only involves matrix multiplication. It is somehow misunderstanding since convolutional layers are actually used in the recurrent unit, based on the attached source code.\n5. In Section 3.3, Equation (3), the (1 + d / D) part should be replaced by (1 + D / d).\n6. It would be better to include the real-time speed-up comparison with other light-weighted network architectures and/or network compression techniques.",
            "summary_of_the_review": "My major concerns on this manuscript include:\n1. The run-time speed remains unclear and comparison with baseline methods is missing.\n2. The training efficiency could be a issue due to the grid search and training from scratch scheme.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a method to adapt an arbitrary CNN backbone model to a lightweight model using recurrent convolutions on split channels. With the recurrent convolutions, the original heavy convolution weights can be reduced by a factor of T^2, where T is the number of timesteps in the recurrent module. Experiments are conducted with VGG-16, Resnet, Mobilenet, efficientNet etc on CIFAR10 and ImageNet.",
            "main_review": "Strength:\n-\tThe proposed approach can greatly reduce #param while keeping a reasonable performance across different models. This method achieves comparable with state-of-the-art lightweight models under similar number of params (Table 3).\n\nWeaknesses:\n-\tAlthough the designed recurrent module can effectively reduce number of parameters, it is not so effective to reduce FLOPs. The comparisons in Table 2 and 3 only show #param reductions. Comparisons on FLOPs is necessary to show how much computation this method can reduce and is more practical for real applications.\n\n-\tIn section 3.3, the authors give some analysis on the theoretical model size and flops reduction with the proposed method. However, the analysis is only based on full convolutions, it does not apply to models with depthwise convolutions (EfficientNet, MobiletNetV2 etc). What are the corresponding model size and flops reductions for these networks? And what kind of model adaptations are done for these networks? In order to match the depthwise convolutions, will the recurrent convolution become depthwise recurrent convolution? \n\n-\tThe recurrent structure is not friendly for efficient inference on modern hardwares. Traditional split-and-merge structures such as group convolution can fully utilize the parallel computational capacity of hardwares. However, the recurrent structure can only be inferenced in a sequential manner, thus hampering its potential for real scenarios. Some evaluations on the actual inference speed of the proposed model will be beneficial to resolve this concern.\n",
            "summary_of_the_review": "Overall, this work presents a recurrent-convolution-based adaption method to reduce number of parameters in a network while keep comparable performance. The comparisons are mostly done on #params while FLOPs are preferred to validate the effectiveness of the method, which is the greatest weakness of this paper. Some technical details are also not clear. Lean to reject.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}