{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes Adaptive Unbiased Teacher for cross-domain object detection. The proposed method integrates teacher-student learning with gradient reverse layer as well as adversarial learning, and obtains good improvement on adapting object detector on Clipart1K, Watercolor2K and Foggy Cityscape datasets. ",
            "main_review": "It is not very easy to read and understand this paper. First, a number of terms were given without explanation; Second, the proposed system has multiple components and I am not sure about the overall novelty. In addition, it is not clear how easy it is to adjust the weights across multiple components. \n\nIn more details:\n- Figure 1: what does \"mean teacher\" mean?\n- Section 3.1, para 2: what is \"weakly augmented\" and what is \"strongly augmented\"? Note that Figure 1 also mentioned \"Weak-Strong\" augmentation, and I am not very sure about these meanings.\n- Section 3.2, eq(1): better explain what \"L_cls\" and \"L_reg\" mean. \n- Section 3.2, eq(2): could you explain why this equation can reduce the amount of false alarms?\n- Section 3.2, eq(3): does this update happen in every SGD step after every batch? Or update after every epoch?\n- Section 4: could you show the evolvement of curves during training (different loss, detection rate on source & target dataset)?",
            "summary_of_the_review": "This paper proposes a reasonable method with good performance improvement.  It is not clear whether the proposed method is easy to use or not, and the presentation of the paper could be improved. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "All the dataset/metrics have been extensively used in previous works. I don't find any new concerns related to ethics. ",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a method for cross-domain object detection inspired by unbiased teacher and discriminator adversarial learning. The idea is simple yet effective. Just follow a discriminator after the student encoder. ",
            "main_review": "It looks like one target domain for one discriminator? Is it possible to use a conditional discriminator for model-compact purposes?\n\nAny ablation study for the choice of confidence threshold?\n\nSome questions about the implementation details. Any reason to set \\lambda_unsup as 1, equivalent for labeled and unlabeled data. Because for unbiased teacher paper, they have larger weight for unsupervised loss. Why using different backbones for two domain adaptations. Real to Artistic Adaptation for resnet-101 but Weather Adaptation or VGG16?\n\nAny insights to explain why for some specific classes, the performances of the proposed methods are worse than other methods? For example, aero, bicycle, bus, dogs, etc. Although the averaged mAP is the best, 7/20 are worse, not a small ratio.\n\nI think the results of the original unbiased teacher should be added in table 1 and 3. Because this is a strong baseline.\n\nSimilar to the ablation study on weak augmentation, one curiosity is if the strong augmentation does matter. For example, will the performance changes a lot when adding or eliminating more strong augmentation for the current sets, like those in AutoAugment?",
            "summary_of_the_review": "Overall, I think the paper is good. Just some detailed questions listed above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors tackle the problem of cross-domain object detection using a teacher-student mechanism with adversarial learning and weak-strong data augmentation. The teacher network is trained on the target domain, while the student learns from both domains. In this way, the teacher improves without suffering domain shift. The adversarial setup ensures that the distributions from the source and target domains are aligned in the student network.",
            "main_review": "The paper addresses an interesting, yet difficult problem for which the current approaches do not perform very well. \n\nThe paper is pretty well written, with the method clearly explained, although I feel that more details about the detailed related work should have been added.\n\nThe results look promising, with tables showing that the proposed method achieves the best performance in multiple scenarios. Still, the evaluation seems to be lacking a key element: even if multiple papers have the same base model, they can learn better or worse depending on multiple elements. The absolute gain is a more reliable method than mAP evaluation, and results should be reported considering the base model of each paper. \n\nFurthermore, I believe that the ablation study should be extended, providing the results of multiple combinations of the existing components. \n\nIt would also be interesting to see what happens when using a different object detector?\n",
            "summary_of_the_review": "I believe that the paper tackles an interesting problem and shows good results. Still, I think the evaluation section and the ablation study should be enhanced. The paper is not highly novel, most works in this area being based on the same ideas. As the authors note, teacher-student networks and pseudo-labels have already been successfully employed in cross-domain object detection tasks. Still, some relevant papers have not been added at all in the related work section, e.g. [A].\n\nI think these problems should be addressed before the paper can be accepted at such a high quality venue.\n\n[A] Soviany et al. Curriculum self-paced learning for cross-domain object detection. Computer Vision and Image Understanding, 204, p.103166, 2021.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}