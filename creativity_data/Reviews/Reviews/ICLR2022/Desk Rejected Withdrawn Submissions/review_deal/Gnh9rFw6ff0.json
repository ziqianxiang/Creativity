{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper provides analytical explanations for why retaining more information in representations is useful for downstream tasks; provides two concrete losses to achieve this; and shows convincing results for transfer to other tasks.",
            "main_review": "Strengths:\n\n1. Nice analysis on minimal sufficient mutual information, and generalization bounds for retaining more information than the minimum. While these ideas are intuitively sensible, providing a mathematical footing for it makes it solid and relevant.\n\n2. Analytical work is backed up by a number of experiments. \n\nWeaknesses:\n\n1. I could not understand the InfoNCE based Implementation II: If there is no reconstruction of the input at all, how does this not result in a suboptimal or trivial solution where both the parameters of f and the resulting representation z, move towards the minimal representation?\n\n2. It would have been nice to try larger pre-training datasets such as ImageNet...the improvements for CIFAR-10 are quite minimal (although still interesting).\n\n3. The idea of using reconstruction-based loss to regularize contrastive learning is not novel. But the analysis is nevertheless interesting. E.g. A relevant missing reference is:\n\n@article{li2020making,\n  title={Making Contrastive Learning Robust to Shortcuts},\n  author={Li, Tianhong and Fan, Lijie and Yuan, Yuan and He, Hao and Tian, Yonglong and Feris, Rogerio and Indyk, Piotr and Katabi, Dina},\n  journal={arXiv preprint arXiv:2012.09962},\n  year={2020}\n}",
            "summary_of_the_review": "This paper shows that retaining more than the minimal information in a contrastive loss setup, allows for better downstream task transfer. Interesting analytical results are given.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The submission studies the problem of \"missing\" information in contrastive learning. While previous work (such as Tian et al., 2020b) implicitly assumes that the shared information between multiple views in contrastive learning contains the task relevant information (e.g. object categories for the classification task), the submission points out that assumption might not hold as each view might contain task-relevant information that are not shared in other views. The submission proposes a new training objective to augment existing contrastive learning frameworks (e.g. SimCLR, BYOL) with weighted sum of mutual information I(z_i, v_i). They performed empirical evaluations on CIFAR and other datasets.",
            "main_review": "Strengths:\n+ The observation that it can be important to preserve non-shared information between views is interesting and could be useful for future work on self-supervised learning.\n+ The transfer learning experiments are interesting.\n\nWeaknesses:\n- Although the observation on non-shared information is insightful, the proposed solution is a bit underwhelming. First, the title asks for representation learning under the contrastive learning framework, while the proposed additional objective is not contrastive learning but an auto-encoding style objective. Second, it is known empirically that existing autoencoding objectives cannot match the performance of contrastive objectives for self-supervised representation learning, since the submission attempts to make a theoretical analysis, it would be important to analysis this empirical observation, and further share some insights on how the weights (\\lambda_1, \\lambda_2) should be selected.\n- Although one could argue it is unfair to compare the proposed method with a SoTA contrastive learning method with heavily engineered augmentations, it is still important to conduct larger-scale experiments on ImageNet (or at least a subset, such as ImageNet-100) to put different methods into perspective under the \"standard\" setup. This could be particularly important for the transfer learning experiments in Table 3.\n- As already mentioned by the authors, the assumption that task-relevant information is not shared between views might be more realistic for multimodal learning (vision, language, audio, optical flow, etc.). It would be really interesting if the authors could conduct experiments on these types of data, as the results might be more convincing.",
            "summary_of_the_review": "The submission proposes an interesting perspective on the choice of information to keep for contrastive learning. They claimed that it is important to retain task-relevant information that are not shared among views. However due to the lack of \"natural\" supervision as in the contrastive learning scenario, the authors resort to the autoencoding objective and combine it directly with the contrastive learning objective. More analysis on the proposed solution, especially its limitations would be greatly appreciated. On the experiment side, the empirical results are interesting but in their current states not very convincing, given that only smaller datasets have been used. Due to the reasons above, I could not recommend acceptance right now.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper makes the observation that contrastive representation learning may discard task-relevant information that is not shared between the data views. To address this, the paper introduce infomax losses that aim to increase the information the network encodes about each view. It's a very simple idea that in practice yields suprisingly large gains on downstream transfer tasks.",
            "main_review": "I really like the simplicity of this paper. The rationale is reasonable and obvious in retrospect. The method is straightforward (which is a good thing) and yields suprisingly large performance gains. I'm surprised this hasn't been tried before (at least to my knowledge).\n\nHowever, there are also some weaknesses to the paper:\n\nThe first 4 pages of the paper feel rather long-winded to me, and I'm not sure the theory adds much. The same motivation and basic theory could be stated much more succinctly. \n\nEspecially Theorems 2 and 3 don't add much: these are obvious consequences of failing to encode all task-relevant information, and the theory here seems unrelated to practice. The point of learning minimal representations is to improve sample efficiency and generalization performance, which is somewhat orthogonal to Bayes' error on the training set (some good discussion of the value of minimality is in Soatto & Chiuso 2014 and Tian et al. 2020b, or in papers on the Information Bottleneck). Trivially, Bayes' error, and other training error measures, can be minimized if you use the complete information -- just using the raw views, with no learned representation on top, will achieve this. I don't think this tells us much about representation learning beyond a simple statement like \"discarding task-relevant information necessarily comes at the cost of reducing the highest achievable performance on the task\" (and isn't that just definitional?).\n\nFigure 2 visualizes increasing I(z_1, v_1) as only increasing task-relevant information. This is misleading as increasing I(z_1,v_1) could also increase task-irrelevant information.\n\nThe paper should at least mention that adding information may come at the cost of increasing noise and increasing sample complexity. Even better, I think an experiment testing sample efficiency would be great to see. Does the increased information come at the cost of slower training or worse sample efficiency (especially at transfer time)?\n\nOn the other hand, Figure 3 is a nice demonstration that encouraging too much information can hurt performance. I think this point -- that you can go too far -- would be good to preview in the theory section.\n\nThe experiments are well designed but I would like to see one that more directly tests the key hypothesis: that increasing I(z,v) helps. One option is to plot I(z,v) vs transfer learning performance for a variety of different learned z's. I(z,v) could be estimated using I_NCE like in Eqn 15 or a different MI estimator.\n\nThe actual method appears sound and reasonable to me. It is a bit confusing why Eqn 14 is not just the usual contrastive loss. I see that p(z|v) is being modeled as a distribution rather than treating z as a deterministic function of v. Why is this necessary? Why not simply model I(z,v) the same way as I(z1,z2) is modeled, i.e. using an off the shelf contrastive learning method (with the architecture modified to fit the dimensionality of v on one branch)?\n\nMinor comments:\n1. \"can not\" --> \"cannot\"\n2. “_tend_ to be minimal” (emphasis mine) is imprecise. Can a more precise statement be made? One option is to use the result that the contrastive loss can be decomposed into an alignment and uniformity term. When alignment is minimized, the representation is minimal (because the encodings of the two views are identical). See Wang & Isola ICML 2020 for more details on this argument.\n3. Theorem 1: “contains less” —> “contains less or equal”?\n4. Related work seems kind of out of place and redundant by the time we get to it.\n5. “LBE” — both methods are lower-bounds, right? If so, would be clearer to rename this.\n6. The title is intriguing but I think it's overly broad and would be better to highlight the more specific contributions of the paper\n",
            "summary_of_the_review": "Overall I like the core of this paper but I also think there is much to improve in terms of the presentation, theory, and experiments (I would like to see more direct tests of the hypothesis that increasing I(z,v) helps). Therefore I think the paper is slightly below the acceptance threshold in its current state.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper claims that contrastive learning models have the risk of over-fitting the shared information between views and proposes increasing the mutual information between the representation and input to introduce more task-related information in the representation. Experiments on CIFAR-10, STL-10 other small datasets are delivered. ",
            "main_review": "--- \nPros: \n* The idea that contrastive learning overfits shared information between views is interesting. \n* Overall, the paper is well written. In particular, the Venn diagrams (Fig. 3, 4) are illustrative. \n* The results show the effectiveness of the proposed method, both RC and LBE, although experiments are mainly conducted on small vision datasets. \n ---\nConcerns: \n \n* The key concern about the paper is that its main story does not make sense. The paper has the following logic: shared information between two views is not enough of the downstream task -> increase the mutual information between input and representation to increase the task-related information. Such information-theoretical reasoning is not valid to me. Because purely from an information perspective, doing auto-encoding perverse the most information in the input thus should provide the best performance to any downstream tasks. Apparently, it is not true. It is common sense that contrastive learning is way more effective than self-supervised with auto-encoding. \n* Although the theoretical part of this paper is invalid, the experiment results still stand which means increasing mutual information between representation and input does improve the performance of the learned feature. However, to the best of my knowledge, this paper is not the first to find this phenomenon. There is an existing work from MIT [1] that has already shown that combining reconstruction loss and contrastive learning loss improves the downstream performance. They also provide an explanation about why adding reconstruction loss helps (removing shortcuts) which I find more convincing. I am surprised that this submission does not cite [1] in the related work nor discuss its relationship to this submission. \n* The paper’s experiments are weak. Although it shows the trend that RC and LBE improve the performance, the improvement is marginal. Further, I find the baseline is weak. The author is reporting that SimCLR on CIFAR-10 has an accuracy of 85.76%. While according to the open-source GitHub repo [2], SimCLR can achieve 93.6% accuracy on CIFAR-10. Such a large gap make me skeptical about the author’s experiments result. I suggest the author do experiments on top of the state-of-the-art. Otherwise, we cannot clearly know whether the proposed method can improve the state-of-the-art. \n\n[1] Making Contrastive Learning Robust to Shortcuts (https://arxiv.org/pdf/2012.09962.pdf)\n\n[2] https://github.com/HobbitLong/SupContrast\n\n---\nMinor comments: \n* I am not sure I understand the title of the paper. After reading the paper, I do not have a clue about what makes for good representations for contrastive learning. ",
            "summary_of_the_review": "Due to the above concerns, I vote for rejecting the paper. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}