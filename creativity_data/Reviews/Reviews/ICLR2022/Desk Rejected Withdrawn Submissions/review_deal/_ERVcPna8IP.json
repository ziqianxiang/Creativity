{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work presents empirical observations that pruning with medium sparsity can degrade model performance whereas at very high sparsity it achieves robustness in the presence of label noise. Referred to as a double descent phenomenon, the authors use this basis to claim a benefit of pruning neural networks. The authors conducted various experiments across different networks, data sets and pruning methods to demonstrate such a phenomenon. The authors further analyze the loss landscape of training and the final solution found by both pruning and re-dense and show that the distance to the initialization may provide a better clue than sharpness/flatness of solution as to explaining generalization of sparse networks.",
            "main_review": "This paper is written well and presents interesting findings including that pruning for very high sparsity could prevent memorization of label noise indicating a kind of robustness. But this result is already known as mentioned in the paper, and there are several remaining concerns and questions. I hope the authors could clarify.\n\nThe described double descent phenomenon (“at low sparsities model performance might degrade as pruning, while at non-trivial sparsities, the behaviors of sparse networks resemble that of under-parameterized dense models, exhibiting a U-like curve of bias-variance tradeoff”) does not look evident. While the double descent phenomenon (Belkin et al. 2019) refers to the trend that the test risk decreases twice as the complexity of the model increases, this trend is not obviously or consistently seen in Figure 1. For example, the best test accuracy does not show a clear double descent trend; can you explain why we would care more about the last test accuracy in which “the last” sounds quite arbitrary and can change depending on how much to perform the retraining? Also, it’s only visible in CIFAR-100 ResNet-18 with epsilon=20% whereas there is hardly any agreeable sign of double descent for \\epsilon=80%. Here, of course I understand that higher sparsity refers to less model complexity, and higher test accuracy to lower test risk, and thus that I’m supposed to see the graphs in (left-right and up-down) flipped manner to resemble the conventional double descent curves.\n\nThe authors suggest interpreting seemingly all arbitrarily high sparsity levels differently depending on their empirical results obtained rather than a priori, meaning that the definition of “high” sparsity is vague and it seems that it can always change however they want to interpret the results. Hence “critical sparsities”, “high sparsities”, and “extreme sparsities” all seem arbitrary and may not apply to other potential experimental settings including data sets and neural network models.\n\nIt’s not clear how much conclusion one could draw out of re-dense (by re-enabling pruned parameters) training (basically why is meaningful to check “whether the robustness could still be maintained if we bring back the pruned connections to networks”?) and its 1D interpolation results, as this experiment results seem quite preliminary. This doesn’t show any strong evidence or provable guarantee about the generalization properties of sparse neural networks.\n\n“sparse neural networks still classify clean labels correctly and neglect noisy labels” Can authors explain where this claim is supported in the paper? I’d be really intrigued to see this.\n\nHow realistic/meaningful is the result with large permutation for the label noise, say \\epsilon > 50%?\n\nOther questions\n- LTR used to re-initialize before training the sparse network; what happens without it?\n- “we consider all weights from linear layers except for the last layer as prunable parameters” Why did you not prune the final layer?\n",
            "summary_of_the_review": "The paper presents empirical findings and is overall written well. However, the evidence to support the idea is not very clear, and results don’t seem to add new significant insights. In contrast to the prior work by Chang et al. (2021) where they derive the generalization characteristics with respect to the double descent phenomenon for sparse networks, this work relies on empirical evidence that doesn’t look strong or consistent. It is difficult to draw any concrete conclusion yet. The technical novelty is limited since the tools used in this work are all borrowed (this may be okay though).\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper demonstrates that under the presence of label noise, network pruning results in a double descent phenomenon in which test accuracy decreases, increases, then again decreases as the network is pruned more aggressively. The paper presents three observations about this phenomenon: that high sparsities are more robust to label noise than medium sparsities, that more label noise requires denser networks to interpolate and sparser networks to generalize, and that train and test accuracy aren't necessarily correlated when pruning. \n\nThe paper then evaluates several conjectures about the loss landscape of pruned neural networks, showing that continuing to train the sparse network as if it were dense reverts the generalization gains from pruning in the label noise settings, that at higher sparsities the distance between the pruned network and the initialization is smaller than at low sparsities, and that such smaller distances result in higher test accuracy.",
            "main_review": "## Novelty and Significance\n\nThe paper presents a novel set of insights about the performance of pruned models in settings with label noise. Though the double descent phenomenon exists in other settings, I have not previously seen it studied in the context of network pruning. Though the observations are not immediately actionable, they could lay the groundwork for an improved understanding of pruning in contexts with noise and potentially more broadly.\n\n## Correctness and Clarity\n\n- The paper is missing explicit testable hypotheses throughout. Specifically, it would be helpful to have an explicit criteria to be able to analyze the curves and decide whether there is or isn't double descent (or other phenomena) happening.\n- The introduction claims that there is label noise in datasets by default. If so, why do we not see this behavior with epsilon = 0? If not, what is the motivation behind these experiments?\n- The paper overall is somewhat scattershot, in that lots of different observations and experiments that aren't obviously related to each other. The body of the paper also has a mix of descriptions of plots, analysis, and speculation, and would be more clear if the factual analysis of the plots were separated from the speculation.\n\n\n### Section 4\n\nThe paper presents a correct evaluation and analysis of the claim that pruned models exhibit a double decent phenomenon as a function of the sparsity. I do have some qualms about missing experiments and analysis, though none of these are fundamentally disqualifying:\n- The scope of evaluation is fairly narrow, consisting of a single MNIST network and a single ImageNet network evaluated on CIFAR. The paper does not evaluate contexts in which the original network does not interpolate the dataset (e.g., with a smaller CIFAR network or on ImageNet). \n- The paper does also not evaluate other settings, including other classes of models, datasets, and other retraining methods (note: all of these experiments are to validate the generalizability of the observations, rather than to validate the correctness of the existing observations)\n- The paper is missing an evaluation of the performance of a dense network with the same number of parameters as the pruned network (though the randomly pruned network is a sufficiently similar baseline that this is not a disqualifying concern)\n- It would help to see these plots with epsilon=0 to line this up with the phenomenon of pruning resulting in slightly higher test accuracy in the standard setting\n- Fig 1: the distinction between the final and best test accuracy is interesting:\n  1. The double descent phenomenon does not show up in the best test accuracy; is there a reason we should or should not expect it to?\n  2. Is this best test accuracy observed during training, or is this test accuracy corresponding to the epoch with the best validation accuracy? The first could be overfitting.\n\n### Section 5\n\nThe first set of experiments in this section (those of Figure 4 and 5) correctly evaluate the claim that reinitializing weights to zero and continuing to train them results in deeper minima with worse test error than those of the sparse solutions.\n\nThe methodology for computing Figure 6 is not explained in the paper.\n\nThe methodology for Figure 7 are also not clear. Specifically, it's not clear whether the presented distances are to the original initialization of the full model or whether they are to weights of the pre-trained network (for sparse) or the re-trained sparse network (for re-dense). It's also not clear precisely how the L2 distance is defined -- the conventional definition (the sum of squared distances) applied to trainable parameters means its not at all surprising that l2 distance decreases as sparsity increases, because there are fewer trainable parameters. I assume that the authors are using a slightly different definition which should be defined. I'm also not very convinced by a couple of claims in this section:\n- \"Note that when increasing sparsity, ... decline continuously.\": this does not seem true for MNIST LeNet\n- \"However, if we focus on the relative difference ... we can confirm an similar conclusion\". Though this claim is true on the data, it was not the original hypothesis and seems plausible that it is coincidence.\n",
            "summary_of_the_review": "The paper contains a core novel observation, explained with sufficient clarity and with a technically correct evaluation, and without obvious missing baselines. Weak accept, conditioned on the authors adding an explicit statement of the limitations of the experiments presented in the paper (that the paper only considers two vision networks on MNIST and CIFAR which interpolate at full density) and more clarity on the methodological details in Section 5.\n\nI would be willing to raise my score if:\n- The authors refine the set of experiments to specific, testable hypotheses that can be explicitly re-validated or refuted in future work\n- The authors repeat the analysis in more settings (e.g., ImageNet, other networks, other re-training methods) and include a dense network baseline\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, they present empirical studies on pruning models under label noise. The core results demonstrate that appropriate sparsity can help the deep model be robust to symmetric label noise.",
            "main_review": "The strength of this paper can be summarized into the following points:\n1. The authors provide comprehensive results regarding pruning under label noise. Representative pruning methods are considered like magnitude pruning, gradient guided pruning, and random pruning. The results are presented across a wide range of sparsity levels. The core empirical result is that pruning networks to certain sparsity can make them robust to label noise. \n2. They also present some possible explanations regarding why a sparse neural network is robust to label noise. These explanations include flatness of the local minimal, $l_2$ distance to the initialization, and so on.\n\nThe weakness of this paper:\n1. The novelty of the results of this empirical study is limited. Similar results have been presented in SNIP and other related works. Although authors argue that their empirical study is more thorough compared to previous works, the core observation is pretty similar. The investigation of the reasons behind the robust sparse network is a plus, but this finding alone is not enough.\n2. This paper focuses more on the pruning side and less on the perspective of label noise. Specifically, they only consider symmetric label noise and other types of label noise like class flip label noise are not included. It's crucial for the readers to know whether this empirical study can be extended to other types of label noise or even real-world label noise.\n3. The scalability of this empirical study is limited. Larger datasets like Mini/Tiny-ImageNet are not considered. On small datasets, the phenomenon is obvious, maybe because the sparse network can find a good trade-off between capacity and robustness. For example, in Figure 2 with 80% label noise on CIFAR-100, 99.53% sparsity tends to overfit the data indicating there is still capacity to memorize noisy samples. On the other hand, 99.85% of sparsity is not affected by label noise, but the learning capacity is limited. When applying weight pruning to larger datasets, the limitation of the learning capacity will be more severe, and the core observation of small datasets may not be obvious anymore. \n4. Another problem is that what if we use a smaller model? For example, what if we use the CIFAR-10 version of the ResNet-20? Will the core observation still hold or become trivial?\n5. The connection between robustness and $l_2$ distance to the initialization seems not very convincing. The most convincing results are on MNIST, where the $l_2$ distance of re-dense is much larger than the sparse model. On CIFAR-10 and CIFAR-100, the $l_2$ distance of re-dense and sparse models are very close when the sparse model performs much better than the re-dense model. As a result, the connection between robustness and $l_2$ distance either depends on the complexity of the dataset or could not be the main reason for the robustness of the sparse network.",
            "summary_of_the_review": "Overall, I feel this paper is below the acceptance threshold. Because the core observation is similar to previous results, and the investigation of robustness and $l_2$ distance may have problems. In addition, this paper does not consider different types of label noise. The scalability of this empirical study is limited and models with diverse sizes should also be considered.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors investigate pruning as a means for regularizing overfitting in classification models that are trained on data with noisy labels. Preventing overfitting in this case may improve robustness against label noise and therefore enhance generalization performance.\nThe authors observe that higher sparsity levels may improve generalization on unseen test data compared to medium sparsity levels. The authors pose that a common belief exists that sparsified models end up in a flat minima, which in turn would explain the higher robustness, compared to the dense counterparts. In constrast, the authors experimentally observe that this is not necesarilly true and hypothesize that rather than the loss surface, the distance from initializaiton plays a larger role in robustness against label noise.\n",
            "main_review": "Strengths:\nThis paper sheds new light and provides a new hypothesis about the regularizing effect of pruning. The authors use different analysis techniques to investigate the dynamics of pruning, which is also applied in three different ways (magnitude-based, gradient-based, and random). \n\nWeaknesses:\nNot all drawn conclusions seem extremely well supported by the provided graphs. In some cases the conclusions are drawn more firmly than the graphs seem to suggest. The experiments are only performed on very simple datasets, while label noise is actually a real-life problem, so it would be highly encouraged to additionally add an experiment on real noisy data, and investigate whether high sparsity levels indeed improve test accuracy, compared to medium sparsity levels. \n\nSome specific feedbacks:\n\n- related work: What do you mean with \"static sparsity patterns\" (last sentence)?\n- The authors use SGD as optimizer, but a much more common optimizer nowadays is Adam, which includes momentum. Adding momentum might heavily change the conclusions of the experiments, as it might faciliate weights to move to a further distance from the initialization. So why did the authors specifically choose SGD, and did the authors also test the influence of momentum during training?\n- From section 3.1, if I understand it correctly, if epsilon is set, then epsilon% of the training samples is permuted, and 100-epsilon% of the training data is unpermuted. Then, a set of restored samples is additionally added to the training set as well, so effectively when epsilon grows, the size of the training data set grows as well?\n- 3.2 (LTR explanation): The authors mention that 20% of the weights will be pruned in every iteration, but doesn't this percentage depend on the sparsity level? Where does the 20% come from?\n- The term \"interpolation threshold\" could use some more explanation, such that the reader does not have to read the cited paper as well. As it is not explained in the paper, Observation 2 is difficult to grasp.\n- 3.3: What is the dimension of theta, theta_s and theta_r, on which weight is this technique applied? Also, in that sentence you mention an increment of 0.01, is alpha the increment you refer to?\n- The sentence: \"We further plot 1-D loss function over a center minimizer theta using filter-wise normalized directions as introduced by Li et al. (2018)\" is difficult to understand if the reader does not know the work from Li et al.  What do you mean by center mnimizer and filter-wise normalized directions? And the loss function is in general 1-D right?\n- section 4, observation 1: Medium sparsity is mainly hurting generalization on the last test result, but on the best test result this effect is much less visible. Why do the authors find the last test result a suitable metric to draw this conclusion on? Because in practice, one would use a validation set, and select the model with the best validation performance (of course assuming generalization to the test set). \n- Observation 3: The authors mention that gradient-based pruning consistently surpasses magnitude-based and random pruning.  This claim seems a bit excessive, as this is for example not the case for the highest sparsity levels, the difference between magnitude- and gradient-based pruning seems negligible.\n- What do the authors mean by : \"the smoother the training curve is wrt sparsity ... \" ?\n- The last paragraph of section 4 is rather vague. The authors mention a coincidence between their analysis and experimental results, but I'm not too sure which analyses and experimental results are referred to now. Also the last sentence mentions \"two methods\", which two methods are the authors referring to here? In general I'm not to convinced about the analysis regarding pruning methods in page 7. The authors mention themselves as well that the analysis might be too simple for the case where e.g. LTR is used. So what's the real conclusion from this section? \n- Section 5: It is mentioned that solutions at high sparsities are not-stable in dense subspace. This conclusion feels again a bit to heavy, as the graphs in Fig. 4 do not seem to differ that considerably among sparsity levels. \n- Section 5: It is mentioned that the linter interpolation of the loss function in Fig. 5 and 14 monotonically decrease. While this is indeed the case for the training data, for the test data this is true in a lower extend; the curves see more like convex/concave curves. So is it valid to only draw this conclusion based on the curves of the training set?\n- Section 5: It is remarked that the re-dense distance declines continuously, but this is not the case for the MNIST LeNet-300 experiment. How can the authors explain this, and can the claim still be made? How are we sure it's not related to the ResNet-18 structure of the other experiments?\n- Fig. 6: I'm not too sure how to interpret this figure. What is the x-axis label for example?\n- The paper could be extended with an additional paragrah that explains the implications for future application of machine learning models on noisy real-life data. Because now after reading the paper, I'm not too sure what these recommendations are. Is it that everyone that is using noisy data should now start pruning their models at very high sparsity levels to increase robustness against label noise? And how should the reader deal with observation 3? Are there recommendations regarding types of pruning, because I can't find them back in the conclusion section.\n\nMinor things / typos:\n- Figure 1 is mentioned for the first time already in the introduction. But reading the caption of Fig. 1, it mentions the permuted fraction epsilon, which is not introduced by then yet. In general I had trouble understanding the graphs already in the introduction. So I would not refer to it already there, and also place the figure closer to the actual discussion, so that the reader does not have to turn pages all the time when reading the discussions and checking the graphs.\n- 2nd paragraph introduction: \"... which introduces noise ....\" instead of \"... which introduce noise ...\"\n- last line page 1: \"... degras as pruning, while ... \". Should \"as\" be \"at\" here?\n- related work: explain the \"double descent phenomenon\" the first time you mention it, for the readers unfamiliar with it.\n- 3.1: \"... by randomly permuting the labels ... \" instead of  \"... by randomly permuted the labels ... \"\n- 3.2: What do the authors mean by: \"we sample five sparsities\"? How are sparsities sampled? Only in the random case sampling from a Uniform distribution takes place right?\n- Fig. 2: What happened around epoch 80 for all trainings?\n- Fig. 3: Figure 3 splits performance for the thee differen trainin-subsets, which is really nice. On which of these sub-sets are the reported training accuracies and training losses reported in the other graphs? Is it the combination of the three subsets?\n- Fig 4: \"sweet-spot\" instead of \"swet-spot\" in the caption.\n- Section 5 has a lot of subsections, and from the bold sentences I found it initially a bit hard to see the relation between all the paragraph. I would advice to combine the first two since they also have the same conclusion (both conclude that sparse solutions do not necesarilly stick around flat minima), and the second two (both deal with distance from inialization in loss landscapes).\n",
            "summary_of_the_review": "This work provides interesting insights into the dynamics of pruning for regularization with regards to label noise. \nThe main doubts concern the interpretation of the provided graphs. Some conclusions seem to strong, given the current empirical evidence. So a re-evaluation of the conclusions is recommended, in combination with an additional paragrah that explains the implications for future application of machine learning models on noisy real-life data (see earlier remark). An additional experiment on real noisy-labeled data might therefore be of additional value as well. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}