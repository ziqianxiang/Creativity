{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors discover feature overcorrelation as a new issue for training deep GNNs and propose the DeCorr framework with explicit feature dimension decorrelation and mutual information maximization to reduce the effect of feature overcorrelation. Experiments show that the proposed framework alleviates the performance drop in deep GNNs.",
            "main_review": "The paper is well-written and presents an interesting study on feature overcorrelation which may be an issue for training deep GNNs. However, there are some concerns to need to be addressed.\n\n1. Important related works [1] [2] were not mentioned and discussed in the introduction and related work.\n\n2. What is $p$? in equation (4)?\n\n3. How do the overcorrelation relate to the rank of the feature matrix? If the correlation is high, can someone use dimensionality reduction to reduce the feature correlation by reducing dimension?\n\n4. The Corr(X) metric seems to be merely a correlation metric instead of a ** overcorrelation ** metric. Would it be better to measure the correlation between inter-classes and intra-classes to show the overcorrelation?\n\n5. All the experiments in Table 1 show that shallow GNNs outperform deep GNNs significantly (L2 is always better than L15 and L30). This raises the question that whether do we really need deep GNNs and makes the contribution of this paper much weaker.\n\n6. I conjecture that the performance drop may be related to the dataset size. [2] [3] [4] show that deeper GNNs improve performance on larger datasets such as ogbn-proteins [5] and Open Catalyst [6]. I suggest the authors validate the proposed techniques on larger datasets to make the proposed techniques more convincing.\n\n7. All deep GNN methods such as [2] [3] [7] do not impose any constraints on feature correlations. They show deep models perform better. Do they also suffer from overcorrelation?\n\n8. Duplicate references:\n\n* Micha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in neural information processing systems, pp. 3844–3852, 2016b.\n\n* Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017b.\n\n* Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACMSIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 974– 983, 2018b.\n\n\n\n[1] Li, G., et al. \"Deepgcns: Can gcns go as deep as cnns?.\" ICCV 2019.\n\n[2] Li, G., et al. \"Deepergcn: All you need to train deeper gcns.\" arXiv 2020.\n\n[3] Li, G., et al. \"Training Graph Neural Networks with 1000 Layers.\" ICML 2021.\n\n[4] Godwin, J., et al. \"Very Deep Graph Neural Networks Via Noise Regularisation.\" arXiv 2021.\n\n[5] Hu, W., et al. \"Open graph benchmark: Datasets for machine learning on graphs.\" arXiv preprint arXiv:2005.00687 (2020).\n\n[6] Chanussot, L., et al. \"Open Catalyst 2020 (OC20) Dataset and Community Challenges.\" ACS Catalysis 11.10 (2021): 6059-6072.\n\n[7] Chen, M., et al. \"Simple and deep graph convolutional networks.\" ICML, 2020.",
            "summary_of_the_review": "The paper systematically performs an analysis on feature overcorrelation of deep GNNs. I enjoy reading the paper. However, I believe the concerns in the main review need to be addressed before a recommendation for acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper identifies and studies a potential issue in GNNs: overcorrelation. The authors argue that this phenomenon is related to the well-known oversmoothing problem and propose DeCorr --- a new method to mitigate the overcorrelation issue. The experiments assess the effectiveness of the proposal on node classification tasks.",
            "main_review": "\n**Strengths**\n- The simplicity of the proposed method. \n- Overall, the paper is easy to follow. \n\n**Weaknesses**\n- Motivation for addressing overcorrelation (W1);\n- Weak experimental setup and significance of the results (W2);\n- Important ablation studies are missing (W3).\n\n**Regarding motivation (W1)**\n\n- The notion of extreme oversmoothing given as \"node representations... proportional to each other\" is imprecise --- This is not what is proved in the mentioned reference (Li et al., 2018). Instead, extreme oversmoothing would make the features of connected components indistinguishable (i.e., constant vectors at the columns of X). In this case, Corr is not even defined. \n\n- Propositions 3.1 and 3.2 basically say that oversmoothing implies overcorrelation but not vice-versa. While this shows the two concepts are different, it does not motivate the proposal toward tackling overcorrelation. One could argue that we should pursue avoiding oversmoothing (the cause) rather than overcorrelation. Thus, I believe it is important to evaluate how much the existing models for addressing oversmoothing also tackle overcorrelation.\n \n- Maximizing the mutual information between the hidden representations and the inputs resembles the idea of long-range residual connections, a well-known way to alleviate oversmoothing. Is this a valid intuition behind the choice? \n\n**Regarding experimental setup (W2)** \n- Overall, Table 1 shows that the best-performing models are shallow networks (2 layers). I believe that alleviating oversmoothing/overcorrelation without enabling more accurate GNNs is not a relevant goal. \n\n- Tables only contain average results. Therefore, it is difficult to assess the significance of these results --- no standard deviations are reported. \n\n- Drawing conclusions mainly based on Cora, Citeseer, and Pubmed has been proved problematic (e.g., see [1, 2]).\n\n- Relying on node classification looks rather limiting here. These benchmarks have strong homophily and benefit from low-pass filter designs that essentially smooth features over the graph. The authors should consider graph classification benchmarks.\n\n- The authors mention that \"the proposed framework can also boost the performance of 2-layer GNNs\". I believe the gains are marginal (see GCN on Cora, PubMed, and CoauthorCS). \n\n**Regarding ablation studies (W3)** \n\n- It would be crucial to observe how much the models proposed for tackling oversmoothing also alleviate overcorrelation. There is only one plot (Figure 3b) to show that, and it looks that DGN already can deal with overcorrelation. In this regard, it is important to consider other methods and other datasets.\n\n- The need for the  $\\mathcal{L}_M$ term seems to enforce the idea that dealing with overcorrelation is not enough. Is using $\\mathcal{L}_M$ less efficient than combining DeCorr with models that handle oversmoothing? This would be an interesting experiment.\n\n**Typos and minor issues** \n- I found Propositions 3.1 and 3.2 rather trivial. I would not put them as propositions.\n- Not all GNNs can be cast in the formulation of Eq. (1). For instance, we can not denote higher-order ChebyNets via message-passing in the original graph.\n- What is the variable $k$ in Eq. (2)?\n- Eq. (4) has $p$ instead of $\\rho$.\n- Typos: \"more smoothing\" (page 4),  \"for the first a few layers\" (page 4), \"the propose framework\" (page 8).\n\n[1] Shchur et al. Pitfalls of Graph Neural Network Evaluation. Arxiv, 2018\n\n[2] Dwivedi et al. Benchmarking Graph Neural Networks. Arxiv, 2020.",
            "summary_of_the_review": "Although the paper is well-written and proposes a simple idea, I believe it can benefit from additional ablation studies, a better experimental setup, and stronger motivation.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a new observation in deep GNNs -- feature overcorrelation. From empirical and theoretical analysis, the authors find out that the graph signals corresponding to different feature dimensions are more and more correlated with the increase of GNN depth. As overcorrelation implies redundant information in the learnt representation, the authors further present regularization terms in the loss functions to alleviate overcorrelation. Experiments shows that overcorrelation improves the GNN learning accuracy and can be used in addition to existing techniques addressing oversmoothing. ",
            "main_review": "Strengths\n\n+ Relevant topic. The training of deep GNNs still remains an open problem and it is good to see the authors discussing a new perspective other than the well-known oversmoothing. \n+ Clearly written. The new observation of overcorrelation is well explained and backed-up by sufficient experiments. \n+ Experiments show that the proposed technique to address overcorrelation can improve accuracy significantly in some cases. \n\nWeakness\n\n- While the authors present some analysis in Section 3.2, the theoretical results are straightforward and lack insight. The proofs of Propositions 3.1 and 3.2 are very straightforward. In addition, the conclusion from Proposition 3.2 sounds a bit misleading -- the existence of vectors with high correlation and low smoothness does not tell anything on how / why such vectors can be generated by GNNs. Thus, Proposition 3.2 does not prove oversmoothing and overcorrelation are two different GNN phenomenon. \n- It is not clear what is the specific role of GNN in overcorrelation. According to Section 3, both message passing and weight transformation of GNNs contribute to overcorrelation. For message passing, the mechanism leading to overcorrelation is exactly the same as that leading to oversmoothing -- we can just take the \"extreme oversmoothed features\" and conclude that their correlation is 1. So overcorrelation seems to be a trivial consequence of oversmoothing. For the weight transformation part, overcorrelation would happen to a deep MLP, not just a deep GNN. Then it is not clear why overcorrelation is not an issue for MLPs but an issue for GNNs. \n    -  In addition, I would suggest the authors re-plot Figure 2b by either using a well-trained GNN, or adding reference curves of randomly initialized MLPs. \n- The proposed technique to alleviate overcorrelation lacks some originality. For example, the mutual information maximization part directly use a well-known technique and ignores the graph structure completely. \n- For the experiments: \n    - For Table 1, although the proposed DeCorr significantly improves baseline accuracies for deep GNNs, the accuracy gain on 2-layer models are not significant. However, 2-layer models actually achieve much higher accuracy than deeper models. In this case, there is no motivation to use deep GNNs. \n    - The evaluated datasets are of small scale and are relatively old. ",
            "summary_of_the_review": "I am leaning towards rejection. I think while overcorrelation exists, it is not clear how the mechanism behind overcorrelation and oversmoothing are different, or how overcorrelation is not an issue for MLPs (see details above). The proposed techniques are largely based on existing literature and the experiments are based on outdated and small datasets. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the over-correlation problem is observed and then studied in the deeper GCNs in experimental and theoretical aspects. It gives a detailed comparison between the over-smoothing and over-correlation and designs the DeCorr method to address this problem. The extensive experiments demonstrate the effectiveness of this method.",
            "main_review": "### Strengths:\n\n1. Two metrics on the column and row-wise are given to evaluate the feature matrix. The experiments in Figure 1 indicate the difference between over-smoothing and over-correlation problems.\n\n2. The empirical and theoretical results on propagation and transformation gave a deeper insight into the over-correlation problem.\n\n3. DeCorr is provided to alleviate this problem, and the results demonstrate the effectiveness of the proposed method.\n\n\n### Weakness:\n\n1. In section 3.1, CORR and SMV functions are proposed to evaluate the feature matrix X. They have naturally different aspects and the different performance in Figure 1 is foreseeable. Therefore, it is not a strong existence proof of the over-correlation problem.\n\n2. In section 3.2, the higher correlation score Corr indicates the less encoded information. However, the random variables will have a low Corr score and it is not equivalent to the high encoded information at all.\n\n3. According to Proposition 3.2, the decrease of CORR does not lead to the decrease of SMV. In this paper, the DeCorr is claimed to alleviate the over-smoothing problem, and it seems achieved with the design of mutual information. Therefore, it would be better to show the comparisons of SMV comparisons in Figure 4.\n\n4. In section 3.3, the differences between these two problems are given on top of the definitions, but the definition of CORR is designed by the author. It lacks a deeper insight into the difference between these two problems. It is not clearly mentioned in this paper. To be specific, how do the propagation and aggregation affects the over-smoothing and over-correlation problems, respectively. It seems that the analysis in Proposition 3.1 and section 3.2.2 are also suitable for the over-smoothing problem. What is the key point for the difference between these two problems?\n\n5. In Table 1, the accuracy of DropEdge is extremely lower than its paper. In its paper, the performance gain in deeper GCNs is obvious. It would be better to show the performance comparisons under the same data split in its paper. In Table 1, the solutions on over-smoothing cannot address the over-correlation problem. What is the key reason for the failure of these methods?\n",
            "summary_of_the_review": "Please see the strengths.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper aims to claim that feature overcorrelation happens when the depth of GNNs increases and it significantly deteriorates the performance of deep GNNs. The paper first proves the existence of feature overcorrelation. Then it validates the differences between oversmoothing and overcorrelation. Finally the authors propose a framework and design a decorrelation loss and a MI maximization loss to tackle the overcorrelation issue.",
            "main_review": "Strength:\n1. The paper introduces a new over-correlation issue that may harm the deep GNNs.\n2. The paper introduces a new method DeCorr to reduce the feature over-correlation by designing a new loss function.\n\nWeaknesses/Concerns:\n1. The motivation is not strong. The authors prove that over-smoothing can cause over-correlation but over-correlation may not cause over-smoothing, which means decorrelating the features can reduce the risk of over-smoothing. And by this statement, We don’t know whether decorrelating helps improve the generalization of model or the decorrelating reduces over-smoothing and improves the performance. These are two very different situations. It looks like that the authors want to prove the first situation but they did not clarify it. More specific, three observations in section 3.1 do not make sense to me. And the over-correlation issue may just be a “side effect” of over-smoothing.\n2. For the experiment setting, I find that the hyper-parameters for DeCorr(the proposed framework) is not the same as DGN which makes the result unreasonable. I want to note that all performance for BN, PairNorm and DGN for GCN and GAT in this paper are retrieved from DGN paper. And in DGN paper, all the results are implemented under the same setting and hyper-parameters, so I think if the authors want to directly use these performance, they also need to use the same setting and hyper-parameters. If authors want to fine tune their framework, they also need to fine tune other baselines or use the original performance in baselines’ papers.\n3. Missing the study of hyper-parameters Alpha and Beta in the loss function.",
            "summary_of_the_review": "Overall, this paper claims a new issue that may need to be considered in deep GNNs and introduces a new method to reduce the feature overcorrelation, but the paper’s motivation is not strong and experiment setting need to be fixed when comparing.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}