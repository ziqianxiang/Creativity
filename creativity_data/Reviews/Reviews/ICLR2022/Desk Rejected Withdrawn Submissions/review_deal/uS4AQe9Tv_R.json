{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors have proposed GEEP (Gender Equality Prompt) to improve the gender fairness of existing pre-trained language models while avoiding catastrophic forgetting. GEEP is pre-trained on a new augmented dataset which is ~0.7 times the size of the actual pre-training dataset and only fine-tunes the embeddings related to gender-biased professions while freezing the rest of the model. Whereas previous techniques would fine-tune all the model parameters on the new gender debiased augmented data. This results in catastrophic forgetting and leads to worse performance on downstream tasks of GLUE. ",
            "main_review": "Strengths of the paper: \n1. The paper is well-written and is easy to follow. The paper clearly explains the problem with the existing models and what is different between the proposed GEEP and existing techniques.\n2. The authors not only compared the gender debiasing performance of the proposed technique but also compared them on the downstream tasks provided as GLUE dataset.\n3. GEEP performs better than SPPA (second-phase pre-training) model on all three gender-debiasing tasks: Winogender, WSC and DPR while also performing better on GLUE dataset. \n4. GEEP is more efficient than the existing SPPA techniques as it does not finetune all the model parameters but conditions W_emb over the existing W_base parameters. I have a question here: Could you please report the comparison between the time taken to train SPPA model vs GEEP? That would also make a case for GEEP to be considered over the existing techniques.\n\nWeakness of the paper: \n1. The authors have mentioned that the previous SPPA methods result in catastrophic forgetting while putting forward the evidence using the reduced downstream performance. And the remedy would be to finetune the small set of embeddings (W_p). Though empirically the performance of GEEP shows better performance of the downstream tasks but the intuition behind the proposed methodology is not quite clear as to why this would prevent catastrophic forgetting? Is this just because some weights are frozen (meaning infinite regularization on W_base)?\n2. The authors have also mentioned that according to their knowledge recent works haven't compared the performance of their proposed methods on different downstream tasks. Though, authors in the past (https://aclanthology.org/2020.acl-main.488.pdf) have compared the performance of their proposed models on different downstream NLI tasks.\n\nSuggestions: \n- I would suggest changing the positioning of Table 1 to better coordinate with the experiments.\n- Remove the description of BERT and rather introduce the analysis of time complexity or total training time between GEEP and SPPA models.\n\n\n",
            "summary_of_the_review": "This is a good paper that provides a solution to the gender biasing problem and performs both sets of experiments to show the improvement in gender-debiasing and downstream (GLUE) tasks. My main reservation is around the intuition of why the methodology works over the SPPA models. My current recommendation is according to the strengths and weaknesses provided in the earlier section but I would like to discuss this further with the authors and other reviewers during the rebuttal period.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper focuses on the problem of mitigating gender bias in pre-trained language models. The authors argue that existing mainstream debiasing strategies, i.e., second-phase pre-training, suffer from catastrophic forgetting issues. To solve this problem, this paper proposes a new method called GEEP to debias pre-trained language models. Specifically, after the first-phase pre-training, it fixes all parameters of the model and trains the embeddings of a set of profession names on recollected gender-neutral data, as the second-phase training. Finally, the authors conduct experiments on downstream tasks, including GLUE tasks, pronoun prediction, and coreference resolution, and show that the proposed method can effectively avoid the catastrophic forgetting issue and alleviate gender bias.",
            "main_review": "Strengths:\n1. This paper investigates an interesting and important problem. Pre-trained language models are widely used in various NLP applications. But due to the large-scale biased training data captured from the Internet, such models inherit human stereotypes towards certain groups. It is of great significance to effective strategies to solve the unfairness issues in such models while not damaging their performances.\n\n2. Experiment design is clear and reasonable. The authors propose experiments on three downstream tasks (GLUE, pronoun prediction, coreference resolution) to verify their three claims: (1) traditional second-phase pre-training method SPPA damages the performances of the original pre-trained model; (2) the proposed method GEEP can alleviate gender bias; (3) GEEP doesn't damage the performances of the original pre-trained model. Also, the authors conduct experiments on public available pre-trained language models, which make the experiments more reproducible.\n\n3. The presentation of the paper is clear and easy to follow.\n\nWeaknesses:\n1. The technical contributions provided by this paper are limited. The proposed debiasing method is based on the traditional second-phase pre-training strategy and only makes a simple adjustment that just updates a part of word embeddings in the second-phase pre-training, which doesn't introduce enough novel technologies to the community.\n\n2. The bias in natural languages is complex and diverse. The proposed framework only focuses on debiasing the embeddings of profession words but cannot handle the bias of other forms, e.g., bias encoded in verbs, other nouns, and the syntactic structures, etc.\n\n3. Some unnecessary contents are included in the paper, e.g., Section 3.1 where BERT is elaborated. This part can be omitted since readers can refer to the references. Also, there are some typos and citations in the wrong format in this paper.",
            "summary_of_the_review": "This paper proposes an interesting method to debias pre-trained language models while avoiding catastrophic forgetting issues. But due to the limitation of its technical contributions, I think this paper doesn’t meet the standard of ICLR. I suggest a weak rejection for the current version.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the gender bias problem in pretrained language models. Some previous methods do a second pre-training step on gender neutral data to alleviate the bias. Authors demonstrated in their empirical evaluation that such an approach shows catastrophic forgetting, i.e., the new models forget what has been learned in the first place while training on the smaller gender neutral data. They proposed their gender equality prompt (GEEP) to address such a problem. In their method, they only update parameters newly introduced related to gender bias (e.g., profession in this study) while keeping the pretrained models frozen. They showed on different datasets that this method achieves good performance on gender debiasing, and on GLUE tasks that there is less forgetting and their method has comparable results to the initial pretrained models.   ",
            "main_review": "Strength \n- The proposed method is intuitive and is well motivated. Focusing on gender bias related words during the second step training makes sense. \n- Experimental results support some of the motivation and the proposed method (see below for some issues).  \n\nWeakness:\n- Some experimental results are not very convincing.  The authors said and also show that training the pretrained model on gender neutral data has the catastrophic forgetting problem. This is easy to understand if as the authors said the gender neutral data is very small, especially in comparison to the original data.  However, in the experiments the authors conducted, the size is quite comparable, e.g., 78% of the Wikipedia and book corpus trained for BERT training.  The reduction of the model performance after this second step training is not well understood. Is it because the test set or the tasks in GLUE have gender bias?  because the data created in second step pretraining is not natural language?  because 78% is still small (if so, a learning curve would be good to include)?  More analysis is needed. \n\n- Similarly for the gender debiasing experiments, in addition to the overall performance, more detailed analyses (for system errors or strength) and discussions are needed for readers to gain some insights about the model behavior. \n\n- Gender-related prompting can be more clearly explained.  What does prompting mean exactly in the proposed method?  and more generally how GEEP work lacks some clarify.  Authors add additional embeddings (called gender equality prompts) as shown in Fig 2, these are trained using the gender neural data, but it's not clear about the old embeddings (in the frozen model), those are never used in inference time?     \n\n- More detailed information on the model size is also needed. In the proposed method, how many new parameters are added?  we all know model size can affect performance significantly, is the new added parameter one of the reasons for the improved debiasing performance?   \n\n- In introduction, authors talked about one benefit of the proposed method is that it doesn't need to fix bias from scratch making debiasing faster. This is indeed the case intuitively, but there's no experimental results supporting this.  It would be good if authors can add results related to training time. \n\n- Table 1: the results for SPPA are questionable, why are they similar to BERT baseline? training on the gender neutral data should be able to improve its debiasing performance. Why is it not the case? \n\n\n ",
            "summary_of_the_review": "This paper addresses an important research problem, and is generally clearly written and easy to follow for readers. The proposed method is well motivated, and experimental results support some of the claims.  \nA main weakness of the paper is the experimental results. Detailed analyses are lacking, and some results are questionable.\nIn addition, the contributions of the paper are a bit thin in its current form, e.g., Sec 4.2 is the method part, and experimental results also left some questions unanswered.  \n ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO.",
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "details_of_ethics_concerns": "I'm not sure if this is a concern, am flagging this for the committee or chairs to take a look. \nThe paper is about gender bias, there's no ethical statement, which I don't know if it's required. \nOverall the description of the work and the data used in the paper seem fine to me. ",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to use additional embeddings to encode profession-related tokens to combat gender bias in the pre-trained BERT model. The paper provides convincing experimental results on gender debiasing and general glue tasks.\n",
            "main_review": "Strengths:\n- The paper is well written. The motivation is clearly explained. \n- The method is simple and easy to follow. And its performances are superior to the BERT base model and BERT model with second phase pre-training.\n\nWeaknesses:\n- The issue of catastrophic forgetting is not as severe as claimed according to the results in Table1 and Table2. The authors should provide a more detailed analysis of these results.\n- Current experiments can't fully support the claim \"catastrophic forgetting still occurs if the size of gender-neutral data is smaller than that of original training data\" in the abstract. The author could make it more solid by providing results with different second-phase data.\n- The cost of second phase training of SPPA and GEEP should better be displayed to show the advantage of GEEP.\n- The usage of citation in the 7th line in section 4.1 should be corrected.\n\nSome questions:\n1)  Is there any reference or statistical result that help prove that gender bias is most prominent on profession names?\n2)  When encountering a profession name in the downstream task, which embedding will GEEP use? Is that the one from $W_p$?\n3)  Are there any gender bias in downstream tasks on the GLUE benchmark?\n4) Could you compare your method with [1] that mitigates political bias in language models?\n5) Could you compare your method with [2] that editing wrong knowledge stored in language models?\n\n[1] Mitigating Political Bias in Language Models Through Reinforced Calibration\n[2] Knowledge Neurons in Pretrained Transformers",
            "summary_of_the_review": "\nThe proposed method is easy to follow and achieves good results on gender debiasing and general language understanding tasks.\nThe paper should add more experiments and descriptions to the superiority of GEEP over SPPA.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The pre-trained language models, such as BERT, achieve state-of-art performance in many language understanding tasks, they have been demonstrated to demonstrate strong gender bias from its training data. Therefore, this paper aims to address such bias and make the text representation more gender-fair. The previous method uses second-phase pre-training to balance the gender-related corpus and prevent the model from biasing towards certain gender. However, such approach is expensive as it needs to update all the model parameters and could cause catastrophic forgetting. The goal of this paper is to propose a cheaper approach to reduce gender bias. Then GEEP trains new embeddings of profession names as gender equality prompts conditioned on the frozen model. Its empirical results demonstrate the model can achieve SoTA performance on debiasing while having little negative impact on GLUE datasets.",
            "main_review": "First of all, I think the paper touches an important bias problem in current NLP literature. Its efforts to reduce catastrophic forgetting is worth praising. However, there are some potential flaws in the paper. \n\nFirst of all, the pronoun prediction bias score used evaluate the fairness of a model seems a bit narrow. The arbitrarily picked templates and restricted professions are only providing the model make the evaluation not very trustworthy. What if we have things regarding about some other aspects, like hobbies (it also has strong bias) rather than professions? Would the metric bring totally different landscape? \n\nSecondly, the way to collect gender-neutral data seems also very heuristic-based or rule-based. There could be much more implicit gender bias connotated by our daily text. I don't think the current collection approach will be able to handle these more complex cases. It seems to me the whole pipeline to quite overfit to \"profession\", which is only a very tiny slice in the general scope of gender bias.\n\nThirdly, from table 2, I don't see much degradation after second-phase pre-training. A 0.7% drop can't support the claim of \"catastrophic forgetting\". I was expecting something like 20, 30% drop, but was surprised to see such a small drop. I have reservation for the claim.",
            "summary_of_the_review": "The paper is easy to follow. However, it seems too narrow and the way to do evaluation and data collection seems quite overfitted to \"profession\". It's hard to evaluate how much the approach could contribute to the realistic gender bias problem in our real life. I would suggest the authors think deeper and apply their approach to a broader domains to validate its effectiveness.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}