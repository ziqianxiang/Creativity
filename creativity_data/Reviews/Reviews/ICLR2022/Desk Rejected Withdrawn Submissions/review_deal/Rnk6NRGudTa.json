{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the impact of having parametric activation functions on adversarial robustness. They analyze the interplay between shape/curvature of activation functions and robustness, and find that activation shapes of positive outtputs on negative inputs and high curvature increase robustness. Based on these observations, they propose a new activation function called PSSiLU, and show that their proposed methods outperform ReLU for robustness.",
            "main_review": "- Strengths:\n\t- Incorporating parameters into act function to improve robustness is a novel idea.\n- Weaknesses:\n\t- In Table 1, the significant improvement is only for Softplus and SiLU, and for ReLU it's a marginal improvement. I'd attribute the improvement more to the fact that Softplus and SiLU are just bad, and it happens to have a large room of improvement for these two act functions. On the other hand, ReLU seems to be optimal from the beginning, and from this point of view, the contribution of PAF seems marginal. Even though the fact that PAF improves Sofplus and SiLU is interesting, there is not much practical utility if PAF+ReLU's improvement is marginal.\n\t- To improve over ReLU, it seems that PAF requires significantly more training data (6M or more?). I think it undermines the benefits of using PAFs. To strength this paper, I believe it should explore developing better regularization methods in the architecture side to mitigate the need of additional training data. ",
            "summary_of_the_review": "While the idea of parameterizing activation functions to improve robustness is novel, I don't think the community gets much out of this paper with the current formulation, mainly due to the fact that their method still requires significantly more additional training data to outperform the existing methods. Because of this practical limitation, I'd recommend weak reject, and encourage authors to develop better regularization techniques to mitigate overfitting.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors investigate the role activation functions play in the robustness of deep neural networks to adversarial attacks. Based on a parameter search, the authors claim that having a positive output for a negative input and a high curvature are useful for defending against adversarial attacks. The authors then design a parameterized activation function with two parameters that allow the networks to tune the aforementioned properties.\n\nWhen testing their activation function on CIFAR-10 with and without adversarial attacks, the proposed activation functions sometimes performs slightly better when compared to existing activation functions such as ReLU and SiLU.",
            "main_review": "STRENGTHS:\n\nThe authors investigate what aspects of activation functions are responsible for adversarial robustness. While there is existing work on this topic, the approach that the authors took regarding curvature is somewhat unique. There needs to be more investigation into the properties of activation functions and how they relate to adversarial robustness, in general.\n\nWEAKNESSES:\n\nWhile the experiments show having a positive output for a negative input and high curvature are correlated with better robustness, I do not think the authors have good evidence that this is the cause. While the authors mention work by [1], their results could possibly by explained by this work as they investigate how symmetry in activation functions is related to adversarial robustness. Though the activation functions that the authors use are not perfectly symmetric, allowing there to be a positive output for a negative input makes them closer to a symmetric function.\n\nWork done in [2] investigates much of what the authors are doing in this paper. Though the authors do cite this paper, I do not think they mention the similarities. [2] uses learns a parameterized activation function that is shown to both improve accuracy and adversarial robustness. The activation functions learned in this work also appear symmetric and have a positive output for a negative input and the effect this shape has on accuracy is investigated. Given this related work, I am not sure what the novel contributions of this work are.\n\nThe accuracy on the clean CIFAR-10 images seemed rather low. Typically, Resnet models achieve around 90% accuracy with ReLU [2,3], however, the authors achieve around 83% accuracy with ReLU. Is there a reason for this?\n\nREFERENCES:\n[1] Zhao, Qiyang, and Lewis D. Griffin. \"Suppressing the unusual: towards robust cnns using symmetric activation functions.\" arXiv preprint arXiv:1603.05145 (2016).\n[2] Tavakoli, Mohammadamin, Forest Agostinelli, and Pierre Baldi. \"Splash: Learnable activation functions for improving accuracy and adversarial robustness.\" Neural Networks 140 (2021): 1-12.\n[3] He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.",
            "summary_of_the_review": "The paper investigates some potentially interesting directions when it comes to activation functions and adversarial robustness. While the authors find correlation between certain properties of activation functions and adversarial robustness, I do not see evidence for a cause relationship. Related work brings this causal relationship further in to question by offering alternative explanations. Furthermore, the authors do not properly take related work into account. Given the related work, the authors' contributions are unclear.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a parametric activation function where we can control the shape of the activation function using two additional parameters, more specifically the first parameter alpha controls the curvature of the activation function and the second parameter beta controls the behaviour on negative inputs.  Firstly using a weak square attack on a standard trained model the authors empirically show the high influence on robustness on changing these parameters. Based on this hypothesis the authors propose an activation function PSSiLU which belongs to the SiLU class but consists of two additional parameters alpha and beta. Using this activation function instead of commonly used ReLU the authors show extensive gains in adversarial robustness on the CIFAR-10 dataset when using additional data from DDPM-6M. However, the gains are insignificant when not using any additional data. Overall the paper is well written, easy to follow and the experiments are well organized and clear.",
            "main_review": "Strengths:\n\n* The paper is well written and easy to understand. The experiments are properly thought off. For instance, variance is shown in figure-3 plots without which a clear understanding of the robustness is not possible. Similarly, the strong Auto-Attack is used for all the robust evaluations.\n* The paper achieves good improvements in robustness on CIFAR10 with additional data (DDPM-6M) as compared to the existing art.\n* Some of the limitations are also shown and addressed properly.\n\nWeaknesses:\n\nThe major concerns are as follows:\n* Though [1] has been referenced but I think a more detailed discussion on [1] is required in the paper. Some of the claims of both these papers are aligned. Both the works discuss how changing the curvature of activation functions can impact robustness. Though overall I think there are differences in both the papers but still novelty as compared to [1] needs to be discussed in detail.\n* The results for SiLU reported in Table-2 seem to be suboptimal. These are lower than what is reported in figure-5 of [2]. In general, PReLU seems to degrade the results, is there any explanation for this?\n* It's a little difficult to understand why at many places SiLU and PSiLU are better than PSSiLU. Though the former two are kind of a subset of PSSiLU. This makes me feel if the optimization on using the two additional parameters is quite tough? Can the authors share the results of PSSiLU on ResNet18 and WideResNet-28-10(if possible) for the CIFAR-10 dataset and the L-infinity threat model for 3-5 runs with different random restarts? This will certainly give a better understanding of the results. \n* It seems like more fine-tuning is required for the results reported where additional data is not used since these seem very suboptimal as compared to the existing methods. Could the authors clarify why they are low, figure-5 does not seem very convincing since even when using the additional data training accuracy is very high, thus overfitting is still present? Could the authors share their ResNet18 and WRN-28-10(if possible) with PSiLU and PSSiLU activations for AWP[3]? The authors can share for CIFAR10 with and without additional DDPM-6M dataset. This will help in understanding the approach better since AWP[3] is the current state of the art when extra data is not used and also helps in preventing overfitting. \n* The significant boost is reported only for the CIFAR10 dataset with additional data, while CIFAR100 or any other datasets have not been addressed properly. If possible could the authors show the results on using additional data for CIFAR-100 for WRN-28-10?\n\nSome minor concerns are as follows:\n* It seems like the results reported in table-8 are highly suboptimal. Since it is known that the results on using TI-500K additional data are generally better than on using DDPM-6M, however, these results show the opposite trend. Could the authors share the details on how the used TI-500K dataset for additional images?\n* Generally it is well known that AWP[3] and TRADES are much better than PGD. Could the authors clarify why they have not used TRADES/AWP as their base method?\n* In Figure-7 and Figure-6(c and d parts) could the authors clarify why the grey lines are not visible? Do all the grey lines coincide with a red line in these figures? Could the authors clarify what models and datasets they have used for the grey lines?\n* It's quite surprising that for the main activation function proposed in the paper, PSSiLU the value of beta comes out to be zero at the end of training. Could the authors share a plot on how the values of alpha and beta of PSSiLU change during the course of training? If possible could the authors share the graph on how the values of alpha change during the course of training for PSiLU activation as well?\n* Could the authors share the accuracy after each of the attacks present in AA attack that is APGD-CE, APGD-CE+DLR, APGD-CE+DLR+FAB and APGD-CE+DLR+FAB+Sqaure for the reported WRN-28-10 model on CIFAR-10 for ReLU, SiLU, PSiLU and PSSiLU activations.\n\nI am willing to raise my score if the concerns are addressed properly.\n\n[1] Singla, V., Singla, S., Jacobs, D., & Feizi, S. (2021). Low Curvature Activations Reduce Overfitting in Adversarial Training. ArXiv, abs/2102.07861.\n\n[2] Gowal, S., Qin, C., Uesato, J., Mann, T.A., & Kohli, P. (2020). Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples. ArXiv, abs/2010.03593.\n\n[3] Wu, D., Wang, Y., & Xia, S. (2020). Revisiting Loss Landscape for Adversarial Robustness. ArXiv, abs/2004.05884.",
            "summary_of_the_review": "Overall the paper is well written and shows good improvements as compared to existing art on the CIFAR10 dataset when additional data is used. However, there are some concerns related to the variance in the results on different random restarts and the results reported without using additional data. Further, the novelty of the paper as compared to [1] needs to be properly addressed since both approaches have some similarities.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose a new activation function called Parametric Shifted Sigmoidal Linear Unit (or PSSiLU). This activation function requires 2 learnable parameters (as a single set of parameters is learned for the whole network). With this activation, the authors improve upon the state-of-the-art on CIFAR-10 against l-infinity perturbations of size 8/255 with a WRN-28-10 by achieving 61.96% robust accuracy.",
            "main_review": "Pros:\n* Thorough analysis of various parametric activation functions (to the best of my knowledge, this has not been done before).\n* Improves upon SOTA on CIFAR-10 for identical model size (WRN-28-10) with 61.96% (vs 60.97% at the time of writing).\n\nCons:\n* The activation only helps in the high-data regime (with the use of additional generated data). Without this extra regularization coming from the extra data, the proposed activation function hurts performance.\n* Results vary a lot for different settings (CIFAR-100, ImageNette) and more often than enough the proposed activation does worse than PSiLU (proposed by Ramachandran et al., 2018).\n* As such, one of the key claims in the abstract (\"activation shapes with positive outputs on negative inputs\") is not thoroughly validated in practice.\n\nSpecific comments:\n\n1) Section 3 uses weak adversaries to determine which activation functions perform better. I'd rather see a strong adversary applied at smaller radii. While the Square and PGD results seem to correlate well on CIFAR-10, they do not correlate on ImageNette.\n\n2) Is there an equivalent of Fig 4 for ImageNette?\n\n3) While the jump from 54.07% to 58.21% when using PSSiLU instead of SiLU looks impressive, I'm questioning how the baseline networks were trained.  Sehwag et al. obtain 55.6% (on a ResNet-18 with ReLU) and Rebuffi et al. obtain 60.73% (on a WRN-28-10 with Swish/SiLU). These numbers are quite different from the 53.67% and 55.10% obtained in Table 2. Could the authors explain the difference in setup?\n\n4) PSSiLU needs to be regularized so that $\\beta$ remains small. That seems contradictory to the conclusion of Section 3 that \"activation shapes with positive outputs on negative inputs\". Similarly, PReLU which has infinite curvature is not more robust, which is contradictory to the high curvature hypothesis.\n\n5) It would be good to see the full breakdown of AutoAttack (for the different attacks) for the best models. I don't expect any surprises, but since the authors have the AA logs to would be good to share them.\n\n6) Axes labels are missing on almost all plots.\n\n7) The paper uses image extracted from Tiny Images in the appendix and makes reference to it in the main text. This dataset should not be used (see https://groups.csail.mit.edu/vision/TinyImages/). Since it is not important to the message of the paper, I'd suggest removing it completely.",
            "summary_of_the_review": "Overall, the paper is well written. It contains a significant number of experiments. However, the synthesis of the results is quite simplistic and conclusions are drawn prematurely. In particular: (i) the proposed activation is not always beneficial; and (ii) it is unclear how parametric activations help robustness. For (ii), the authors claim that it allows the optimization to find a better minimum (but there is no comparison on the training set performance). In other words, it remains unclear how parametric activation functions improve robust generalization.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}