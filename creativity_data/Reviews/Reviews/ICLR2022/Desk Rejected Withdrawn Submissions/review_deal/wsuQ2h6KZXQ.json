{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper investigated MLP-mixer in image denosing and compressive sensing tasks.\nExperiments shows that MLP-mixer outperforms U-Net models.",
            "main_review": "Strengths\nThis is the very first paper investigating MLP-mixer for image reconstruction.\n\nWeaknesses\n1. There is no direct comparison between proposed method and SOTA on benchmark dataset. e.g. BSD68 or Urban100 with Gaussian noise.\n2. We need more analysis to show the advantages of MLP-mixer over U-Net. e.g. comparison of model structure, receptive field; experimental compassion with different noise level, computation efficiency",
            "summary_of_the_review": "The idea to apply MLP-mixer for image reconstruction is great, and the experiments to compare convergence of Unet and ViT in section 3.4 is good.\nBut the writing of the paper should be improved, e.g. the figures takes too much space from the experiments; section 3.1 could be shorter.\nI suggest resubmission.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces an MLP-mixer based solution for the image inverse problem. The architecture is mostly inspired by the MLP-mixer and expanded its usage to the image to image domain. The paper applied the architecture to the image denoising problem and compared it with U-net, ViT, and BM3D.",
            "main_review": "Overall this paper is well constructed and well reasoned from the technical side. From the MLP based classification application to expand the usage to the image-to-image domain is somewhat a straightforward path of investigation. My main concerns lais in the novelty of the paper and whether it is above the technical significance required by ICLR. Other than the novelty concerns, I have the following comments:\n- 3.3 section is not well-argued, the author should spend more ink here to explain the purpose of this experiment and why it shows the advantage of the proposed architecture. \n- 3.4 contributes greatly to the understanding of the architecture's effect, but besides listing out the results, please make sure conclusions are drawn for each experiment. \n- if efficiency is the main selling point of the proposed architecture, please give more evidence such as run-time, memory requirements, embedding test, etc. ",
            "summary_of_the_review": "This paper's novelty is the main concern from my point of view, as well as the lack of support to some of the arguments/statements, such as the efficiency of the architecture. I hope the author could provide more evidence of the merit of this proposed architecture and reorganize the experimental section to demonstrate the advantage of the image-to-image MLP-mixer.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes the use of a modified MLP-mixer for image restoration. It largely follows the original MLP-mixer architecture, except that the token mixing layers (which go from all tokens to all tokens) are replaced with a pair of separable height-wise and width-wise mixing layers. The final token/patch embeddings are then reshaped back to a pixel grid, and used to reconstruct intensities with a 1x1 conv.\n\nThe paper compares the performance of using such an architecture for denoising (and compressive reconstruction) vs ViT and U-Nets. ",
            "main_review": "### On the modified mixer architecture:\n\n- The motivation for using the separable token mixing (i.e., height x width) is not very well explained. The paper says \"This modification retains the relative location of the patches in the 3D volume which induces an inductive bias enabling the image-to-image MLP-mixer to perform very well when trained on relatively few images.\" I'm not sure what that means. \n\n  The original token mixing also retains relative locations of patches --- except that it learns a mapping from all locations to all locations. The proposed change factorizes this map into row and column-wise operations.  While of course factorization is good (fewer parameters, compute, etc.), but I'm not sure what the inductive bias being talked about here is.\n\n- While I can see the factorization as being a win (fewer parameters and FLOPs as mentioned above), why would it be a win only for image reconstruction tasks? I would argue that if this is indeed a useful architecture innovation, it should also be evaluated on classification tasks (as the original MLP mixer paper).\n\n- But even in the context of denoising/restoration, while the paper does do a few \"inductive bias\" experiments in the form of trying to overfit to an image (in the style of deep image priors),  in the main performance evaluations in Figs 2 and 4, it compares the proposed \"Img2Img-Mixer\" to U-Nets and ViT, but not to the original mixer.\n\n### On the impact to denoising/restoration\n\nIt is unclear what the message of the paper is for denoising/restoration. The paper says: \"This experiment shows that the image-to-image MLP-mixer can obtain state-of-the-art denoising performance on real-world noise as well.\" But it is important to point out that U-Nets are NOT the state-of-the-art for denoising. (Indeed in general, statements/claims of that nature should be backed by comparisons and citations to a specific state-of-the-art method or paper.)\n\nRather,  the state-of-the-art for denoising (at least until recently) was the  DnCNN architecture (and variants thereof). These networks just contain a series of convolution layers with different dilation factors, and not multi-scale encoder-decoder layers like in a UNet.\n  \nThe most insight I could gather from the evaluation is that given enough parameters/data, the mixer does as well as U-Nets (perhaps very slightly better ...), but that U-Net has a better inductive bias but is better in the presence of limited training data.  But again, U-Nets are not the right networks to be comparing to ....",
            "summary_of_the_review": "Overall, I don't believe this paper has sufficient technical novelty or empirical insight to be accepted to ICLR in its current state.\n\n### Post-rebuttal comments\n\nI've read the authors' response and am electing to stick with my original score.\n\n- There are a number of new experiments in the revision. I've tried to go through them all, but really, the original paper should have had a lot more (and careful) evaluation in the first place.\n\n- Wrt SOTA: Firstly, the Brooks et al. paper doesn't really compare U-Nets to the DnCNN method per-se. Rather, it compares the method proposed in the paper (where they do use a U-Net architecture, but also new ways of generating training data with existing DnCNN models). The authors state that in their own experiments DnCNN-type architectures performed similarly to U-Nets. I can believe that---but given that fact, it follows that the paper should include the same scaling experiments for both DnCNNs and UNets (wrt training data and number of parameters). DnCNNs may scale differently (and better) than U-Nets.\n\n  But my original point was about what a \"comparison to the state-of-the-art\" should look like. The paper should identify a few commonly used denoising benchmarks (in addition to SIDD for real camera noise, it is common to evaluate on Kodak-24 / CBSD-68 / McMaster / Urban-100 for AWGN), find a few published methods that currently perform the best on those benchmarks, and compare to the published numbers of those methods.\n\n  Such a comparison is important given the significant effect of hyper-parameter tuning and such --- presumably the authors of these SOTA papers would have optimized the hyperparameters of their method as best as they could, and the authors of this paper correspondingly of their own method. The authors' own comparisons to U-Net are no-doubt useful, but to establish that a proposed method \"beats the state of the art\", the above kind of comparison is necessary.\n\n- I'm still not sure that the paper establishes what situation the proposed method provides an advantage. The mixer performs poorly with fewer training images, but the comparison for larger training sets is between UNets and Mixers with the same number of parameters. But the ablation also shows that a UNet with the same number of parameters is much faster ... shouldn't the comparison be between networks of equivalent speed? (Note the original mixer paper for classification shows comparisons of accuracy vs flops for different training regimes).\n\n- Finally, I still think the novelty in the paper is relatively limited. Even if the experimental results were complete, the paper would say that mixers are useful for restoration in addition to classification (the latter having been established by the original mixer paper). The paper still does not establish the importance of the new \"height-width\" separable mixing layers. In the ablation study, the paper compares the proposed variation to the original mixer at only the low-data regime --- but this is the regime where a UNet has an advantage because of its inductive bias. If the goal is to scale better with more training data, the right comparison is on large training sets ... where the original mixer may infact scale better.\n\n  It is also worth noting that this kind of separability has  been explored in other contexts (for separable convolutions instead of separable mixers, e.g., in Niklaus et al., \"Video Frame Interpolation via Adaptive Separable Convolution\", ICCV 2017). So the fact that separability is one way of reducing parameters / complexity is by itself not new. Also, the language about the benefit of such separable mixing as being \"to retain the position of the image patches\" doesn't make sense. The original mixer also retains positions of all patches (since they are mapped to the same dimensions in the flattened vectors ...).\n\nIn summary, the rebuttal hasn't changed my mind about the paper being somewhat limited in novelty, and the experimental evaluation being weak.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper adopts the recently popular MLP-mixer architecture to the task of image reconstruction. The paper claims that the proposed approach therein(I2I-mixer) gives better or comparable performance as other SOTA approaches(UNet and ViT) but provides two important advantages. It requires much fewer parameters and also learns with fewer training examples. To demonstrate this, experiments have been performed on three image reconstruction tasks- Gaussian denoising, Real-world camera noise denoising and MRI reconstruction.",
            "main_review": "Strengths\n* The paper identifies an interesting intersection between the recently proposed MLP-mixer architecture and the task of Image Reconstruction in memory/compute-constrained applications.\n* The range of architectural choices which have been experimented with (e.g. multi-resolution, linear MLP mixer layers etc.) give a good empirical insight into the behaviour and properties of different variations of the MLP-mixer architecture. Also, the ablation studies in Sec 3.4 answer a lot of the questions that arise while reading the paper.\n* The analysis of measuring the inductive bias of different architectures towards natural images seems useful and can serve as a starting point for further analysis into the fundamental properties of these architectures.\n* The paper is easy to follow. Although the writing can be made more organized (detailed later).\n\n\nConcerns:\n- First concern is the significance of the results. \nThe paper mentions in multiple places that the proposed architecture outperforms the SOTA approaches such as UNet. Whereas the experimental results show that the performance of the MLP-mixer is comparable or only marginally better than U-net. This marginal improvement does not seem to be significant.\n\nThe abstract claims that the proposed architecture requires fewer parameters to achieve the\nsame denoising performance as compared to UNet. While this is reflected in the experiments(Section 3.1, figure 2), the difference in dB is quite insignificant (less than 0.5 dB). The curves for I2I-mixer and UNet almost coincide.\nAlso, this claim has been experimentally demonstrated only for one case (Gaussian Denoising) and not for other discussed cases of real-world camera noise (Sec. 3.2) or compressive sensing (Sec. 3.5)\n\nThe abstract also claims that the modification which is proposed in the paper(retaining the relative positions of the image patches) imposes an \"inductive bias towards natural images\".\nWhile the experiments in Sec. 3.3 show that the I2I mixer does have an inductive bias in the same range as UNet and ViT, the original MLP-mixer is also shown to have a significant inductive bias, only slightly lesser than I2I-mixer.\n\n-Novelty\nThe changes proposed by the current paper to the original mlp-mixer are quite straightforward. Most of the benefits discussed here come from the MLP-mixer architecture itself and not the modifications proposed in this paper. Therefore this work is a simple adaptation of the MLP-mixer idea to the task of image reconstruction.\n\n-Some claims not supported by experiments.\nIn addition to the claims discussed under the first bullet point above, the abstract claims that the I2I-mixer learns with relatively fewer training samples. Whereas the results of the compressed sensing experiments(Sec.3.5 Fig.4) show that the I2I mixer performs much worse than UNet when the number of training samples is small and only comparable to UNet for moderate number of training samples. This can also be observed in the Gaussian Denoising experiment(Sec.3.1 Fig.2). \nTherefore, it is not clear why the paper claims that I2I-mixer learns with fewer training samples. For the task of Real-world camera noise denoising(Sec.3.2), this analysis has not been performed.\n\n-Paper lacks a clear motivation\nThe motivation for using MLP-mixer architecture for Image Reconstruction is unclear. It is unclear how the field of Image Reconstruction is benefited by incorporating MLP-mixer architecture.\nThe introduction mentions that this architecture gives lower computation/memory cost at inference. But the related works section does not discuss what are the existing techniques to improve the speed-accuracy tradeoff  of Image reconstruction algorithms, or other approaches to adopt Image Reconstruction algorithms to memory-limited or compute-limited applications. Therefore, a clear research question is missing.\nThe related works section should also discuss other methods which have been employed in other works to attain the two main claimed benefits of this work- training with reduced number of samples and achieving good accuracy with smaller models.\n\n\nMinor suggestions:\n- In Sec. 3.3, it can be better explained how the inductive bias is measured from the experiment results shown in Fig.3. This is a gap in explanation from paragraph 2 to paragraph 3 in Sec. 3.3.\n- The overall writing of the paper can be made more organized. E.g. The term “non-linear mixing” has been defined for the first time in the conclusion section.",
            "summary_of_the_review": "The paper is a straightforward adaptation of already existing MLP-mixer architecture to the task of image reconstruction. And the benefits such as fewer network parameters are coming from the MLP-mixer architecture itself and not from the modifications made in this paper. Additionally, the benefits of the proposed method are only marginally higher than existing methods (UNet) and hence not a significant contribution.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}