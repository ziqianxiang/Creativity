{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper introduces Softmax Gradient Tampering, a technique for modifying the gradient of the softmax loss to make the loss more smooth. On standard benchmarks the authors demonstrate improved training and test accuracy.\n\nThe reviewers are unanimous in their recommendation to not accept the paper. They identify the following problems:\n* a lack of theoretical understanding and rigor, and a lack of support for the claims that are made\n* insufficient experimental results to convince the reviewers of the merit of the proposed technique in the absence of theoretical understanding\n\nThe authors did not provide a rebuttal, and I see no special reasons to question the assessment made by the reviewers. I therefore recommend to not accept this paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors propose a technique called Softmax Gradient Tampering, which transforms the predicted output class probabilities to improve training performance of neural networks. The authors show that the proposed technique results in a smoother output probability distribution for lower values of the hyperparameter $\\alpha$. On standard benchmarks on the ImageNet and CIFAR-10 datasets, the authors show that the proposed method results in better training accuracy performance as well as improved generalization performance.",
            "main_review": "Strengths\n-\n\n- Overall I think this is a fairly well-written and easy-to-follow paper, although there are a couple of typos/grammatical errors which need to be fixed.\n- The proposed method does introduce a hyperparameter $\\alpha$, but tuning this hyperparameter seems to result in reliable, albeit small, performance improvements on the training set across the 3 benchmark tasks considered.\n\nWeaknesses\n-\n\n- One of the claims in the paper is that the proposed softmax gradient tampering method smoothens the gradients. This indicates that the authors are implying that the proposed method improves the Lipshitzness of the gradients. However, this claim is never made concrete, and it is not clear to me whether Theorem 1 implies this. It would be great if the authors can clarify this claim with a bit more detail, either through a theoretical result or through an experiment which shows smoothened gradients.\n- The paper will be improved with a much more thorough experimental section. While the baselines in the benchmarks considered have been tuned well, it would have been good to also present results across multiple independent seeds. It would also have been good to check how well the $\\alpha$ values of around 0.25 or 0.3 transfers to another task, particularly a transformer. \n\n",
            "summary_of_the_review": "While the proposed method looks promising in terms of results, the paper would benefit a lot from a more thorough experimental section, and from clarifying the claim of how softmax gradient tampering smoothens the gradients.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Other reasons (please specify below)"
            ],
            "details_of_ethics_concerns": "The authors provide a github link in the paper containing pretrained checkpoints and training logs, but this is not an anonymized link, and violates double-blindness.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces gradient tampering, a method whereby the gradient of of the cross-entropy loss function at the end is overwritten by using a smoothed p' output instead of the original output. ",
            "main_review": "The reasoning for why this method would or should work is unclear to me. There is a lot of grandstanding in the paper like \"the networks are just training better and arriving to better optimas at convergence\" and \"that improves the learning capacity of neural networks\" that just don't follow directly from any logical argument in the paper.\n\nAlso the paper claims in several spots that \"We theoretically analyze how softmax gradient tampering works from the perspective of\ngradient smoothness.\" But section 5 only shows that (1) indeed smoothes out p, but that is already fairly obvious from the definition itself. It really isn't necessary to devote 1.5 pages of math to this, this could easily go in the appendix as it seems trivial. The question remains on why gradient tampering is a smart idea. I could not glean this from the paper. \n\nTo me, it's also not clear why all gradients in the neural network would be 'smoothed' by having p redefined as p'. First of all, what does smoothing mean. I can see that a probability distribution is smoother as it tends more towards low entropy, but what does this exactly mean for the gradients themselves. This is too hand-wavy. Furthermore, having a flat probability distribution for the output, does not necessarily mean the gradients in the rest of the network show less peaks in their behavior. If anything, once my model is converged properly to p = q, then there is only a 0 gradient everywhere. That's as smooth as could be. \n\nIn the end, the method is really doing something similar to adding a temperature parameter to the softmax function. This is not much of a reach in terms of the idea. See e.g. https://www.sciencedirect.com/science/article/abs/pii/S1568494618302758?via%3Dihub , https://arxiv.org/pdf/1612.02695.pdf , or in deep metric learning:  https://arxiv.org/abs/1808.04699 , https://arxiv.org/pdf/1811.12649.pdf,  or for other methods that change the outputs that are e.g. too confident: https://arxiv.org/pdf/1701.06548.pdf . It would be good if the authors did a better search on similar methods and explorations of the temperature parameter. \n\nThe experimental results in this paper are far from sufficient. There is no comparison to other types of methods that are cited, such as label smoothing and other things you could do to your network for improvement. I understand your method does something slightly different, but it's similar enough to warrant a comparison. The alpha values are picked on the test set, which is not good practice. Results on the ResNets show only marginal improvements, and would benefit significantly from confidence bounds and reran experiments. From Fig. 2 it also looks like the network exhibits quite some noisy behavior when changing this alpha parameter. I'm not convinced the results show that training with a smaller alpha works better. \n\nSome smaller comments:\n\nLemma 1 - It would be better to define the threshold before or inside of the lemma for clarity. \n\nI don't think it's OK to give a non-anonimous link to your code. This goes against the anonimization idea of double blind reviewing. \n\nEditorial Notes:\n\npage 2 | I assume an exp is missing in your softmax definition?\npage 5 | Why does Softmax Gradient Tampering Works -> Work\npage 5 | why softmax gradient tampering work -> works\npage 6 | accuriacies -> accuracies",
            "summary_of_the_review": "The paper seems to amount to changing the temperature parameter of the softmax function during training. this has been investigated and done before, although not a very common practice. The paper does not show sufficiently why this would help during training, and resorts to unproven comments about smoothing of gradients. The theoretical section is insufficient to show why the method works, and the experimental section is not elaborate enough to prove beyond a doubt that the method works well. The paper also misses references to other works that have tuned the temperature parameter of the neural network.\nAll-in-all, the quality of this work is not high enough to warrant an accept. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a simple technique that manipulates the predicted values by the exponentiation at the backward step. The authors search best performing hyper-parameters to carry out the experiments to show the accuracy improvements over the baselines.",
            "main_review": "Pros)\n1. The idea looks simple and interesting.\n2. The accuracy improvement by ResNet18 without BN is quite intriguing.\n\nCons)\n1. The paper has a clear message, but the novelty is limited, and the any intuitions behind why the method works are not clearly presented.\n2. Experiments are insufficient to support the claim; provided training setups are very limited to support whether this works really well on a broad scale; some improvements are negligible such as in the case of ResNet-18. \n3. The experimental results are not convincing; all the accuracy gains should be validated by the multiple experiments with error bars, thus the authors should repeat the experiments against the randomness.\n3. Some figures such as Figure 1 and Figure 3b are not referred to in the manuscript.\n4. The manuscript should be refined: many typos (IN contains 50 images per class for the validation and the test sets both); need to refine all the citations for clarity. \n5. The works related to involving gradient noise are not cited such as the papers below. The authors should compare with such methods in the manuscript. \n[1] Wu, Jingfeng, et al. \"On the noisy gradient descent that generalizes as sgd\", ICML 2020;\n[2] Neelakantan, Arvind, et al. \"Adding gradient noise improves learning for very deep networks.\" arXiv preprint arXiv:1511.06807 (2015).\n\nComments)\n1. Searching for alpha depicted in FIgure 2 and Section 3.1 is not clearly stated. What is the rule of thumb for picking alphas in the searches? In Figure 2b, alpha=0.2 has better test accuracy compared with others but was not picked; due to the large train accuracy? Why did the authors experiment only with a single experimental setup to determine the parameter? Overall, It is hardly agreeable for the entire practice of searching the parameter, and because of this, the following results that could have come out better became worse.\n\n2. In the training setup on the ImageNet training, why did the authors use the 10-epochs cooling phase at the end of the training? Does the high weight decay (5e-4, the widely-used standard for ResNets is 1e-4) affect the results?\n\n3. Please clarify why the authors use the half or mixed-precision training in the paper?\n\n4. The following \"arxiv\" works do not need to be cited, but I recommend the authors to do more studies with such softmax manipulation methods below comparing with the proposed method:\n[1] Oland, Anders, et al. \"Be careful what you backpropagate: A case for linear output activations & gradient boosting.\" arXiv preprint arXiv:1707.04199 (2017).\n[2] Agarwala, Atish, et al. \"Temperature check: theory and practice for training models with softmax-cross-entropy losses.\" arXiv preprint arXiv:2010.07344 (2020).\n\n5. Can this method work well with stronger data augmentations such as dropout, stochastic depth, rand/autoaug, or mixup?",
            "summary_of_the_review": "Please see the main review.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes the softmax gradient tampering to modify the gradients in the backward pass to enhance the accuracy. The predicted probability value is transformed using a power-based probability transformation, and the gradient profile is more smooth. The experimental results show the slight accuracy increase.",
            "main_review": "Cons:\n1. The writing needs to be improved a lot to enhance the readability. The motivation is very vague, what is the problem of standard BP? Meanwhile, the benefits of the presented methods are not well explained. Why the softmax gradient tampering can better approximate the true gradient?\n\n2. The experiments are not convincing. For example, the hyperparamters $\\alpha$ in Table 1 are very important to the final performance. However, only two results with $\\alpha=1$ and $\\alpha=0.25$ are reported, remaining a large blank to evaluate the effectiveness of softmax gradient tampering.\n\n3. She authors argue the generalization gap between the training and test dataset can be controlled by softmax gradient tampering which influences the gradient accuracy. However, no clear conclusions are drawn from the empirical study to show their correlation or rules.\n\n4. There are a lot of blanks in Page 6 and 7.\n\n5. The discussion does not convey any useful information.\n\n6. The connection between the proof in Page 6-7 and the motivation is weak.\n ",
            "summary_of_the_review": "Please see the main review.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}