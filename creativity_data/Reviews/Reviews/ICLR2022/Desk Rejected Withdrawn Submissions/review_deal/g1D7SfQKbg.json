{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a novel two-head architecture to efficiently learn the label transition matrix and correct samples during training. Different from most meta-learning-based models which require at least three back-propagation in each training step, the proposed framework has much lower computation complexity and only needs a single back-propagation.",
            "main_review": "**Strengths:**\n\n1. The proposed two-head architecture makes the model learn both the underlying clean distribution and the direct noisy distribution at the same time during training. This structure is critical to a successive label correction since it makes the matrix estimate closely follows the shifting distribution induced by label\ncorrection.\n\n2. The computation complexity of the training with the proposed framework is close to standard training with cross-entropy, which makes this method useful in practice.\n\n3. Theoretical analyses and numerical experiments consolidate the results.\n\n**Weaknesses:**\n\n1. It is not fully convincing whether the proposed method is really useful in the challenging instance-dependent label noise settings. Although Tabel 2 shows the proposed LT2L achieves state-of-the-art performance on Clothing1M and beats the baselines by a large margin, it is still not convincing enough since the original loss-correction paper (Patrini et al., 2017) has shown training ResNet-50 following a simple pipeline: 1) train ResNet50 (initialized with ImageNet pre-trained weights) with 1M noisy samples by forward loss correction; 2) train ResNet50 (initialized with the model from Step-1) with 50k clean samples by cross-entropy, can achieve an accuracy of 80.38\\%. Compared with this approach, LT2L may require less training time but an accuracy of 77.83\\% could not show its effectiveness. Besides, it is not a good idea to claim state-of-the-art based on this result.\n\n2. To address the concerns in Weakness-1, the authors may need to test more noise settings, e.g., the instance-dependent label noise settings (Xia et al. 2020 cited in this paper).\n\n3. When a clean training set is available (cleaned by model [R1] or human [R2]), semi-supervised learning is powerful but time-consuming. There clearly exists a tradeoff between training complexity and test accuracy. The author should mention this line of works and highlight why the proposed method is necessary.\n\n4. Recent estimators for the noise transition matrix may benefit this paper, e.g., an end-to-end approach [R3], using clusterability of nearby representations [R4]. Besides, when the noise transition matrix is estimated with representations as [R4], the two-head architecture may not be necessary. \n\n[R1] Cheng, Hao, et al. \"Learning with instance-dependent label noise: A sample sieve approach.\" ICLR 2021.\n\n[R2] Zhang, Zizhao, et al. \"Distilling effective supervision from severe label noise.\" CVPR 2020.\n\n[R3] Li, Xuefeng, et al. \"Provably end-to-end label-noise learning without anchor points.\" ICML 2021.\n\n[R4] Zhu, Zhaowei,  et al. \"Clusterability as an alternative to anchor points when learning with noisy labels.\" ICML 2021.\n",
            "summary_of_the_review": "The proposed framework may contribute to an efficient solution with low training complexity. However, current experiments on the special symmetric noise and asymmetric noise cannot clearly demonstrate how it really works in the real-world scenario, where the label noise is often instance-dependent. The experiments on Clothing1M are also not convincing enough since an accuracy of 80.38\\% can be achieved by loss-correction (Patrini et al., 2017). I'd like to raise my score if the authors could better demonstrate the effectiveness of the proposed method when dealing with instance-dependent label noise.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the problem of learning with noisy labels. To solve this problem, this paper proposes to estimate a label transition matrix and uses this matrix to correct the given labels on the fly. This paper also introduces a two-head architecture to efficiently learn the label transition matrix and optimize the objective function with both clean labels and noisy labels. Experiments are conducted to demonstrate the effectiveness of the proposed method.",
            "main_review": "Pros:\n1. A new method for learning with noisy labels is proposed.\n2. Experiments show that the proposed method outperforms other robust meta learning methods.\n\nCons:\n1. The writing of this paper needs to be improved. Many descriptions and explanations are ambiguous. Actually, the proposed method is simple, while there are some exaggerated expressions. Besides, there are some typos. For example, in Section 3.1, “Note tha,t unlike our method, other approaches....”. The authors are encouraged to carefully re-check and revise the manuscript.\n2. The proposed method is not novel enough. First, the estimation of the transition matrix has been proposed by previous methods. Second, the training objective contains several terms while all of these terms have been used by previous methods. I consider that the novelty brought by this paper is the two-head architecture.\n3. Experimental results cannot show that the proposed method achieves state-of-the-art performance, because this paper did not compare with some state-of-the-art methods (like Dividemix [1])\n\n[1] DivideMix: Learning with Noisy Labels as Semi-supervised Learning. ICLR 2020.",
            "summary_of_the_review": "This paper proposes a new method to solve the problem of learning with noisy labels. However, the proposed method is not novel enough, and the writing of this paper needs to be improved. Experiments validate the effectiveness of the proposed method to some degree, while this paper did not compare with some important baselines so that the performance cannot be fully validated.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes Learning Transition Matrix to Learn (LT2L), an label transition matrix estimation method for training deep neural networks with noisy labels. To speed up training speed and improve the , the proposed method introduces a two-head architecture to efficiently learn the label transition matrix every iteration within a single back-propagation. The main contribution of this paper is the introduction of improving the label correction with the transition matrix estimation, which is continuously corrected on the fly. Empirical studies are performed to show the superiority of proposed towards miscorrected labels.",
            "main_review": "Strengths\n+ Learn the transition matrix throughout the training process on the fly.\n+ Attempt to improve the label correction with the transition matrix estimation.\n+ Some SOTA results on the noisy labels benchmark, e.g., Clothing1M.\n\nConcerns: \n- The key concern about the paper is that whether or not the proposed method can be considered as a meta learning method. The formulation of meta learning process and meta loss of the method is unapparent. Though the method optimizes Eq (4)(5) alternately, it lacks a proper explanation why this learning manner behaves in a meta learning way for the Eq.(6).\n-  Due to the common feature extractor $\\phi$, Eq.(5) can easily learn the feature extractor $\\phi$ fall into noisy data. Though Eq.(6) has additional Eq.(4) to prevent the feature extractor $\\phi$ from overfitting noisy data,  Eq.(6) can easily overfit the noisy labels and makes the transition matrix wrongly estimate, due to the ratio between clean dataset and noisy dataset is small. Different from [A] treats the performance on clean dataset as meta-level guidance, the proposed method tasks a same level supervision employing both clean and noisy dataset. Some form of theoretical/analytical reasoning behind the effectiveness of the proposed method need to facilitate. \n- The label correction strategy is increamental and not closely related to transition matrix. Such separated strategy seems to provide fewer insights to the community.\n- The paper lacks of more competitive experiments to study the usefulness of the proposed method, e.g., harder setting and stronger baselines. It only presents the results of asymmetric noise level 20% and 40%, but prior works have also strong performance on 60% and 80% setting. Meanwhile, it lacks the SOTA baselines on noisy labels, e.g., DivideMix [B].\n- Some wrongs for the important equations. E.g., the first equality in Eq.(1) does not hold, since $x$ is integrated in the second term, while first term contains $x$. The second equality in Eq.(29) does not hold, since $p(\\tilde{y}=j|y=i,x)$ is the noisy classifier. The definition of $d_i$ in Appendix A.3 is not true.  And many many wrongs need to check. \n\n\nMinor comments: \n- It lacks some related works, e.g., [C].\n\n\n\n\n\n\n\n\n\nRefs:\n[A] Zhen Wang, Guosheng Hu, and Qinghua Hu. Training noise-robust deep neural networks via meta-learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4524–4533, 2020.\n\n[B] Li J, Socher R, Hoi S C H. Dividemix: Learning with noisy labels as semi-supervised learning. In ICLR, 2020.\n\n[C] Shu J, Zhao Q, Xu Z, et al. Meta transition adaptation for robust deep learning with noisy labels. arXiv preprint arXiv:2006.05697, 2020.",
            "summary_of_the_review": "Overall, I vote for rejecting the paper. I think the formulation, the theoretical/analytical reasoning behind the effectiveness of the proposed method, and the experimental results of the paper is not well prepared for publishing on ICLR. There exists many wrong derivations throughtout the paper, which seems to the proposed method is not rigorous.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this paper, a robust and efficient method that learns a transition matrix to learn with noisy labels while continuously correcting them on the fly is proposed. To reduce the computational cost, a two-head architecture is adopted which guarantees that the label transition matrix can be learned with a single back-propagation. The experimental results show that the proposed method is promising both in accuracy and time cost.\n",
            "main_review": "strengths:\nIn this paper, a robust and efficient method that learns a transition matrix to learn with noisy labels while continuously correcting them on the fly is proposed. To reduce the computational cost, a two-head architecture is adopted which guarantees that the label transition matrix can be learned with a single back-propagation. The experimental results show that the proposed method is promising both in accuracy and time cost. \nweakness:\n1)\tHow does the proposed method reduce the influence of the noisy labels is not clearly presented. It is better to further explain it.\n2)\tThe details of the experiments should be further improved, which may be helpful to repeat the experiments in the paper.\n3)\tI wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n",
            "summary_of_the_review": "The transition matrix is very effective to improve the performance. Although the inverse of the matirx is used, the propsoed method is very efficient and the time cost is much less that some state-of-the-art methods.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}