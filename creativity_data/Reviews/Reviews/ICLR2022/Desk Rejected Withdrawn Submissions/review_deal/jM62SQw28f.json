{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposed a self-supervised learning technique to deal with time series anomaly detection problem. Different masign strategies are implemented to deal with point or continuous outliers.",
            "main_review": "Strengths:\n\n- The paper proposed a self-supervised learning technique to deal with time series anomaly detection problem. Different mask strategies are implemented to deal with point or continuous outliers. Efforts are made to detect the location of the anomaly.\n- The paper is overall well written and easy to follow. In addition to performance comparison with baselines, several ablation studies are also been carried out.\n\nWeaknesses:\n\n- The proposed method is very straightforward. Yet some parts are lack of explanation. E.g., after obtaining a set of non-overlapped imputed data, how does the integration work?\n- The motivation of the usage of DTW to calculate the difference between the original data and the inputed time series is questionable. Shouldn't the unmasked part of the input be close to the imputed data in the corresponding position? The input and imputed time series are not out of sync.\n- The performances from the main paper are not consistent with those in the Appendix. E.g., the F1 score of DeepFIB on EGG (A) dataset as shown in Table 3 of the main paper is much higher that those plotted in the Figure 2(b) in the Appendix. I could understand there could be small difference considering multiple rounds of experiments running under different settings. But 80.90% vs 60%+ doesn't make sense to me.\n- Some small issues\n  - The Figure 4 is not referred in the paper.\n  - In Table 2, the second best recall result for 2d-gesture is wrongly marked.\n",
            "summary_of_the_review": "There are some key questions to be resolved in the paper as mentioned in the respond to previous questions. Therefore, I would not recommend this paper to be accepted for publish at this time.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "·\tThe author proposes a self-imputation framework for time series outlier detection.\n\n·\tThe author adapts the time series forecasting model, SCINet to point-wise and sequence-wise imputation toward detecting outliers from time series data. \n\n·\tThe author propose a novel anomaly localization algorithm to identify the precise locations of sequence anomalies.\n",
            "main_review": "·\tStrength:\n\no\tThe paper is well-organized and easy to follow.\n\no\tThe proposed anomaly localization algorithm with DTW as anomaly score is novel and very well fit into the attribute of sequence-anomaly.\n\no\tThe empirical experiment seems promising on the adopted detests.\n\n·\tWeakness:\n\no\tThere are many existing time series imputation algorithms [1]. However, the author adopts an forecasting model (i.e., SCINet) without further justification, which makes the whole framework seems arbitrary. More justification on this may clarify the motivation behind the adoption.\n\no\tAlthough the author includes state-of-the-art anomaly detection algorithms as baselines, one of the most important classical discord analysis baseline (i.e., MatrixProfile) is missing. Comparing with MatrixProfile will certainly increase the credibility of the model performance. In addition, it is also interesting to see the comparison between the proposed model to existing imputation models for detecting outliers.\n\n[1] Fang, Chenguang, and Chen Wang. \"Time Series Data Imputation: A Survey on Deep Learning Approaches.\" arXiv preprint arXiv:2011.11347 (2020).\n",
            "summary_of_the_review": "·\tTo summarize, this paper proposed a self-imputation framework for time series anomaly detection. The proposed method is technically sound and the anomaly localization algorithm is novel. However, the motivation behind adopting forecasting algorithm rather than imputation algorithm is not clearly justified and the experiment is lacking an important baseline. Therefore, I believe this work is marginally below the acceptance threshold but addressing the above concerns will certainly improve the paper quality.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper investigates the anomaly detection problem in time series, and introduces a masking and reconstruction based framework for model training. The method has two masking strategies for dealing with two types of anomaly detection problems. With some specific designs in techniques, the experimental results demonstrate the effectiveness of the proposed method.",
            "main_review": "In this paper, the authors developed a masking and reconstruction based framework for training models on times series anomaly detection. The key challenge is to address the scarcity of the training data. The proposed framework uses masking methods to augment the training data with corrupted time series, where the masked values are reconstructed during training so that the model learns the temporal structure in the augmented datasets. To accommodate two type of anomaly detections, i.e., point-wise anomaly and subsequence-wise anomaly, random masking and sliding window based masking are respectively used as the strategy to augment the data. Accordingly, two methods of reconstruction were proposed, one is regular prediction, another is bidirectional prediction. The framework was built upon an existing method that encodes time series. Several other encoders were also tested in the ablation analysis. An anomaly localization approach was proposed to facilitate locating the window of the subsequence anomaly. The experiments were performed on several datasets involve the targeted two types of anomalies. The results demonstrate the proposed method is effective to detect those anomalies in time series. It is good to see many figures that helps illustrating some concepts.\n\nThe following are concerns of the paper.\n1. The technical novelty of the paper is limited. Using masking to enable training models with reconstruction capability is sort of straightforward in time series anomaly detection. For the two types of anomaly detection problems, the corresponding strategies, random masking and subsequence masking, are straightforward as well. The time series encoding was mostly built upon existing methods. The anomaly localization method for detecting subsequence anomaly seems to be a kind of trial-and-error method that iteratively checking the threshold, which is an engineering approach with limited novelty.\n\n2. Some technical details were not well justified. First, in random masking, (d * T / M) values were masked, but it is unclear why to set this number in this way. It seems to be an even division of the total number of values for M augmented samples, but this may relate the challenge of reconstruction to the choice of M. If M is small, the reconstruction may be challenging. The paper doesn't discuss the impact of M and how to select M. Second, the subsequence masking method only considers non-overlapping subsequences, it is unclear whether it neglects some temporal structures that could be learned from some overlapping subsequences. Third, in Fig 5(b), the reconstruction of the masked subsequences were performed in a bidirectional manner. Since time series have a temporal trend that is unidirectional, it is obscure why a backward prediction is useful from an intuitive perspective. An ablation analysis on this may help better understanding.\n\n3. The proposed method has many hyperparameters, such as mask size, window size, window stride, and its effectiveness may depend on domain knowledge such as how to set the hyperparameters and the threshold of anomaly detection. In particular, the correct detection of subsequence window requires iterative comparison between the prediction error and the threshold, thus the results may be sensitive to the choice of the threshold.\n\n4. In the experiments, to understand how precise the located subsequences are, it is better to visualize some comparison between the detected windows and the ground truth subsequence anomalies.\n\n5. Since the compared methods for point-wise and subsequence-wise anomaly detection in the experiments are two different sets, a elaborate discussion on the different choices of the compared methods is expected.\n\n6. Also, since the choices of datasets for different ablation analysis are different, it is better to provide some justification that the choices are not arbitrary.",
            "summary_of_the_review": "The paper provides a generally reasonable framework to handle the two studied types of anomaly detection in time series, but the proposed method is incremental upon existing techniques, and some technical design details were not well discussed. It could be seen as a good exploration of applications of those techniques with proper integration. The paper also remains to be improved in its experiments.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents a new method to tackle the time series anomaly detection task in a setting where the training data is limited. They propose to use the commonly used reconstruction approach to time series anomaly detection by training an auto encoder and detecting anomalies by putting a threshold on the reconstruction error. The novelty of this paper lies in proposing to use a masking of part of the input time series at training time as it should allow the model to learn more from the limited data. The experiments present the overall performance of the approach but do not highlight how it handles the low data regim. The method is not evaluated on a number of common datasets without justification.",
            "main_review": "\nStrengths:\n1. The proposed approach is simple, interesting, and can be applied to any anomaly detection model using the reconstruction approach to time series anomaly detection.\n2. The proposed approach seems to allow better anomaly detection results on the datasets that were tested on.\n \nConcerns:\n1. It would have been good to contrast and compare this approach to other work that use self supervised learning or general data augmentation for time series anomaly detection, see [1,2,3], as well as the many data augmentation techniques proposed for anomaly detection techniques in computer vision.\n2. The experimental results are insufficient to demonstrate the effectiveness of the proposed approach. The experiments would be more convincing if the following items were addressed:\n- It would be helpful to evaluate the more commonly used time series anomaly detection datasets, like SMD, SMAP and MSL, potentially also Numenta, KPI and Yahoo. See [4,5]\n- It would be important to compare the approach to non-deep baselines on these datasets. The main argument of the paper is that it can deal with the low data regime, this is a regime in which non-deep baselines should also perform well. There are some non-deep learning baselines shown for the credit card dataset, but I would also want to see the comparison on the other datasets.\n- The main proposed use case of the paper is the low data regime, while this may or may not be a common scenario, it is an important scenario to nail. This is a scenario where non-deep methods should perform better, making it even more important to compare them.\nI would like to see a plot showing that the performance of DeepFIB does not degrade significantly as the available data is reduced. It could be done as follows: pick a reasonably sized dataset, and measure the performance of the different algorithms as you reduce the size of the training set.\nThe plot could be structured as follows: on the y-axis the F1 score, on the x-axis the percent of the training set used, starting at 100% and going close to 0%. There we would want to see that the performance of DeepFIB stays good as the size of the training set decreases, whereas the performance of other models would drop quicker.\nIf this plot was to show that the F1 score of DeepFIB decreases less as the size of the training set is reduced, it would prove that it is in deed a solution to the problem presented.\n3. The proposed anomaly localization method is fairly common among window based approaches. Would your training procedure not allow you to propose a different and more accurate localization method? Maybe using different sizes of masking to find out when the error may start arising?\n4. While the number of hyperparameters is limited, it would be good to have a discussion on how one would pick the number of masked points and the length of the masked sequence.\n5. It is interesting to see the results of the model trained without the masking method in table 4, it would be helpful to see this for all datasets to better understand the impact of the masking on the performance.\n6. Finally, while it would not be a problem if the evaluation was thorough enough, one has to admit that the proposed method is not very novel, as both the architecture used and the masking methods have been proposed and the novelty lies in combining them and applying it to the time series anomaly detection setting. \n \nQuestions:\n1. It is not clear to me that data in time series anomaly detection would generally be scarce. Labeled examples are very expensive to obtain and often noisy, but in most practical settings one can have access to a larger set of unlabeled data. In the case where you would have access to labeled data, how would you incorporate it in the training?\n2. The two masking methods do not seem that different from the other, could you train a single anomaly detection model using both the sequence masking and the point masking?\nHow can you decide which model to pick without knowing in advance the kind of anomalies that you are looking for?\nCould you report the results of both models on the different datasets?\n\n\n\n\nMinor comments:\nSome minor form comments:\nsection 4:\n- “Whether DeepFIB outperforms state-of-the-art AD methods?” → “Does DeepFIB outperforms state-of-the-art AD methods?”\n- You could point to [6] for the F1 score computation method since it is proposed there.\n4.1:\n- It is very good to present the mean and standard deviation of your runs, it would be good to show over how many runs this was.\n4.2:\n- “Next, we replace...” → “Then, we replace...”\n- Table 5, I can guess that this is the F1 score but could you please mention it in the caption.\n\n\n[1] : Opprentice: Towards practical and automatic anomaly detection through machine learning. Liu et al. 2015\n\n[2] : RobustTAD: Robust Time Series Anomaly Detection via Decomposition and Convolutional Neural Networks. Gao et al. 2020\n\n[3] : Neural Contextual Anomaly Detection for Time Series. Carmona et al. 2020\n\n[4] : Robust anomaly detection for multivariate time series through stochastic recurrent neural network. Su et al. 2019\n\n[5] : Timeseries Anomaly Detection using Temporal Hierarchical One-Class Network. Shen et al. 2020\n\n[6] : Unsupervised anomaly detection via variational auto-encoder for seasonal kpis in web applications. Xu et al. 2018\n",
            "summary_of_the_review": "While the idea is simple and quite general, the empirical study presented is not enough to support the claim that this method helps in the low data regime. Moreover, the empirical study is not thorough enough to prove the general usability of the method.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I do not foresee any ethics concerns with this work.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a deep anomaly detection method for multivariate time series. The anomaly detection task does not seem to be clearly defined in the paper, but it would be either in-sample outlier detection (aka data cleansing) or out-of-sample outliner/sequential anomaly detection. \n \nAssuming that the latter is the case, the key idea seems to be a data augmentation method using masking. The authors propose two masking schemes, namely random point-wise and sequence-wise.\n \nThe underlying neural network architecture is the same as the one called SCINet, which is described in an archive paper that has not yet been accepted in any peer-reviewed venues.  The training is made by minimizing the reconstruction error of the masked entries.\n \nThe authors report on a few experimental results to claim superior performance.",
            "main_review": "The proposed data augmentation/training approach might be useful as a general method to boost the performance of neural sequential autoencoders. If it is the main claim, the authors will need to apply the same/similar approach to other models to compare the performance.\n\nIf the capability in data imputation is a selling point, the authors will need to clarify the relationship with other imputation method theoretically and empirically. \n \nThe authors claim that they modified SCINet by changing supervised signals. But it is unclear how SCINet, which is designed for time-ahead prediction, was used for their task. For example, the input is obviously different from the original setting. There does not seem to be a clear explanation in the paper.\n \nOverall, the description of the paper is high-level and qualitative. The model lacks a solid justification. The cited paper (SCINet) and the supplemental do not help understand the detail.\n \nThe experimental evaluation does not look comprehensive. Table 4 shows that the AR model gave a competitive performance with the Credit Card data set. This means that the time series has an extremely simple spectral structure (or a very simple periodic nature). In that case, many classical time-series analysis methods should work as well as the AR model does. There does not seem to be a clear explanation why those classical methods were not compared in other datasets.",
            "summary_of_the_review": "- High-level, qualitative description of the model that lacks scientifically compelling justifications\n- Weak experiments under an unclear setting ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}