{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper study the complexity of exacting an approximate critical point with stochastic momentum and also Adam for smooth optimization. The main contribution is that the momentum method (an Adam) enjoy a better complexity. ",
            "main_review": "I have the following concerns regarding this result: \n\n1. The title is about deep neural networks, but the problem set is a classical setting of stochastic smooth non-convex optimization which is very well studied in the literature. Since the authors are not using the structure of training loss of neural networks in this work, using deep learning terms in the title and abstract is very misleading (there is no neural network and there is no intuition related to deep learning). \n2. If I replace the optimal batchsize in the $K_\\epsilon$, then I get the minimum steps $O(1/\\epsilon^4)$ for the momentum and Adam-type algorithm. I am not sure that momentum obtains any sort of improvement in the stochastic non-convex settings without variance reduction. Even in a convex setting, stochastic momentum methods are not proven to outperform SGD. \n3. The optimality of the batchsize is with respect to the established upperbound. This has to be clearly stated in the claims. \n4. In the definition of Problem 2.1, the definition of local minima is consistent with the literature. Problem 2.1 targets a critical point where the norm of gradient is small. \n5. Theorem 3.1 states that with $\\beta=0$ we get the best bound on the norm of the gradient (so the momentum is not useful). \n6. What is $x$ in the definition of $D$? \n7. Experiments are not consistent with the theory as authors switch to the convergence in terms of function value not the norm of gradient. \n",
            "summary_of_the_review": "There are inconsistencies in the statements of the main results. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper shows theoretically the relationship between the minimum number of steps with specific optimizers and training batch size for non-convex optimization.  Based on the relationship, authors propose two facts. The first fact is that there exists an optimal batch size such that the number of steps needed for nonconvex optimization is minimised. The second is that the optimal batch size depends on the optimizer. Additionally, numerical results are provided to support the theoretical results in this paper.",
            "main_review": "Weaknesses:\nThis paper lacks a clear illustration of objective or motivation of the work before introducing the contributions. Besides, in the introduction, authors only list the related works and fail to put the proposed method into context.\nCan you give a more thorough and convincing discussion to demonstrate that under the given conditions (G1, G2) how would the Assumption (A3) holds?  \nAccording to the proof in the Appendix A.1 SUFFICIENT CONDITIONS FOR ASSUMPTION(A3), can you show more details about how you derive ||∇f_i (x)||^2  ≤ ||∇f_i (x* )||^2+ L_i ||x-x* || ≤ L||x-x*||? Based on ||∇f_i (x)||^2  ≤ ||∇f_i (x* )||^2+ L_i ||x-x* || and the assumption, it can be derived that ||∇f_i (x)||^2 - ||∇f_i (x* )||^2 ≤ L_i ||x-x* || ≤ L||x-x*||, which cannot support the rest derivations in the proof of Proposition A.1.\nAdditionally, the proof of Proposition A.1 derives ∑_(i∈S_k) ||∇f_i (x)|| ≤ G. Assume this conclusion makes sense, can you give more illustrations to sufficiently obtain Eq. (2) In Sec. 2.1? E[||∇f_(S_k ) (x)||^2]  is not equal to  E[||∇f_(S_k ) (x)||]^2 unless there are any assumptions.   \nWhat is the purpose of Sec. 2.2 NONCONVEX OPTIMIZATION IN DEEP LEARNING ? It is confused that the problem stated here is regarding convex optimization.\nIt is suggested to conduct empirically evaluation that using larger batch size over the ‘optimal batch size’ proposed in the paper does not decrease the number of steps needed to support the first proposed fact from theoretical results. Authors provide some numerical results to support the theoretical outcome, while the provided results can only sufficiently demonstrate the second proposed fact that momentum and adaptive methods can exploit larger optimal batches than SGD do under specific setting. \nIn Sec. 4, authors state they checked SGD and N-Momentum with s ≥ 2^10 do not satisfy f(x_k) ≤ 10^(-1). How about the experiment on Adam? From Fig. 3, it shows that the performance of Adam is getting well with increasing batch size s. Hence, it is not very sufficient to demonstrate the proposed ‘optimal batch size’ can achieve the optimal performance.  Can you provide a discussion or more experimental results here?\nWhat is (1) for Algorithm 1 in Sec. 3.1 refer to?\nIt would be better if this paper can be well written and well organized because some expressions are not very clear and some parts are not very easy to read.",
            "summary_of_the_review": "N/A",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper builds upon the body of literature in convergence analysis and shows that the needed number of steps in nonconvex optimization for SGD, Nesterov Momentum, and Adam-type algorithms can be expressed as a function of the batch size. It constructs theory that proves the existence of an optimal batch size for these algorithm classes, where the main takeaway is that Nesterov Momentum and Adam-type algorithms are able to exploit larger batch sizes than SGD, and increasing the batch size beyond this optimal batch size does not further improve the convergence rate. This is shown for, both, a constant learning rate as well as a decaying learning rate schedule and accompanied by an experiment on a ResNet-20 architecture for CIFAR-10 to support the theoretical results. ",
            "main_review": "**Strengths:**\n\n- Novel insights into the relationship between the required number of steps and the batch size considering, both, constant and a sqrt decreasing learning rate schedule of different optimizers.\n\n- Clear listing of assumptions for the theory with justifications and an extensive appendix that contains accompanying proofs for the relationship between the number of steps and batch size.\n\n- Great effort of putting theory behind some of the empirical observations found in the field over recent years regarding the batch size. Theory papers like this are essential for effectively moving forward in the empirical-driven field of deep learning optimization.\n\n- Connects the empirical observation that large betas work well in practice for adaptive methods with the theory produced in the paper.\n\n---\n\n**Weaknesses:**\n\n- The notation should be introduced better. I think, in particular, moving Section 1.2 before 1.1 and adding the part below Table 1 into that section along with better descriptions about $K_\\epsilon(\\cdot)$, $H$, $A_\\alpha$, $B_\\alpha$, $h_0^*$, $s$, and $s^*$ would improve the entry point into the paper. It would also enable the reader to quickly comprehend Tables 1 and 2.\n\n- What’s the justification behind choosing $H$ and $L$ as it was done in the experiments of Section 4? This feels a bit like cherry-picking to fit the presented theorems, even though some heuristics are given in the footnotes. I’d find it hard to pick these a priori for my experiments. \n\n- As _Shallue et al, 2019_ was often mentioned as one of the empirical works for the presented theory, it would be nice to see if the presented rational functions for the optimal batch size verify the observations found in the referenced paper. This would again strengthen the presented theory by a large margin.\n\n- Alternatively, Section 4 could be extended by experiments on another architecture and/or dataset besides CIFAR-10. For example, given that Transformers have seen much success in recent years across many tasks and these come with their own set of challenges when training, I believe adding them would be of significant interest to many readers.\n\n- _“Don't Decay the Learning Rate, Increase the Batch Size” (Smith et al., ICLR 2018)_ seems to be relevant here and probably should be cited as well as connected to the presented work.\n\n---\n\n**Clarifying Questions:**\n\n- Although I couldn’t find any immediate flaws with the math, I still find it hard to understand why using a larger batch size than the optimal one i.e. $s > s^*$ should result in a monotonically increasing number of steps $K_\\epsilon(s)$. Also, _(Shallue et al., 2019, Figure 4)_ and _(Zhang et al., 2019, Figure 8)_ show, as stated in the presented paper, “diminishing returns” for the number of steps but no monotone increase. This simply suggests that increasing the batch size beyond the optimal batch size does not reap any additional benefits in terms of convergence rate but in theory it should never hurt either. It may simply not be needed. The statements about diminishing returns are contradicting the previous statement for larger batch sizes than the optimal one. After all, a larger batch size better approximates the true gradient, so I see no point why it should hurt convergence w.r.t. to the training objective. Also, on page 7 it is mentioned that “ [...] there exists an optimal batch size ($s = s^*$) minimizing $K_\\epsilon(s)$; thus increasing the batch size does not always decrease the number of steps $K_\\epsilon(s)$.” This is also very contradicting with the definition of monotonically increasing as it implies that sometimes it CAN reduce it? Any thoughts on this? \n- Why was $\\beta$ in Section 4 chosen to be so small? Wouldn’t something closer to $1$ make more sense given the previous argument at the end of Section 3?\n- How was the batch size swept in Section 4? Why are $2^5 – 2^9$ not mentioned for SGD and N-Momentum? Similar for Adam? Since the table makes it look like the optimal batch size is always the largest one it might help to add a few rows for the larger ones you tried as well to avoid misleading a skimming reader.\n- What motivated the choice of Nesterov Momentum over Polyak Momentum and how easy is it to extend the presented work to this case as well?\n\n---\n\n**Minor Improvement Suggestions:**\n\n\n- I really like Figures 1 and 2 and they should probably be moved to the top of page 2 as they summarize the key findings of the paper pretty neatly. I’d suggest removing some of the notation and describing it in layman terms in the image description for easier comprehension since this will be before the notation section and serve as an “eye-catcher”. Again: Why does $K_\\epsilon$ increase after the optimal batch size and doesn't go flat? \n- The optimal batch size is a little vague w.r.t to the properties of $K_\\epsilon$ for $s > s^*$ and the previously mentioned monotonically increasing problem, a clear definition of that upfront would probably help the reader understand it better. \n- Each section should start with a short overview of the goals/content of the section. Starting a section immediately with a subsection is not recommended, and reduces clarity.\n- Table 1 and Table 2 would benefit from more spacing to all of the horizontal lines to make them look less cramped.\n- Remove ugly page break at the bottom of p.2. since there is only a part of one sentence.\n- Remove itemize environment on p.3 top since it’s only one element.\n- In the significant fact (II) on p.3 move the two usages of ‘can’ after SGD for better sounding sentences. Similarly, in the last sentence of the abstract.\n- References: Consistency in citing arXiv from a formatting and mentioned elements standpoint. Simply comparing _Arjovsky et al., 2017_ and _Schmidt et al., 2021_ there are quite some differences. Also, _Schmidt et al., 2021_ was published at ICML 2021. Further, be consistent about the citations from conferences as well as the same venue e.g. _Chen et al., 2020_ mentions the page numbers for NeurIPS but _Fang et al., 2018_ doesn’t. Also, be consistent about the usage of the conference and journal abbreviations.\n\n---\n\nApart from the mentioned points, the paper is well written. The presentation, structure, and general layout of thoughts are up to ICLR’s standard. Related work is thoroughly mentioned and cited accordingly. I’d still like to mention here, though, that the appendix is crucial to verify the claims in the paper, putting it way above the 9-page limit and as such it might be a better fit for a journal.",
            "summary_of_the_review": "Overall, I vote for acceptance. Grounded theory behind some of the empirically found best practices in the field is crucially missing and this paper presents a good effort to mitigate this problem to some extent. It provides novel theoretical insights for nonconvex optimization in deep learning that are significant and interesting for practitioners and researchers alike. My main concerns are regarding the introduction of the notation as well as the accompanying experiments that are supposed to verify the theory. If my concerns are properly addressed and discussed during the rebuttal period and some of the minor improvement suggestions are incorporated, I’d also be happy to further improve the score! ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper studies the relation between batch size and the number of required steps needed to train a deep neural network. For various optimizers including standard SGD, N-Momentum as well as several Adam-type optimizers, a rational function is derived describing the number of steps until convergence as a function of the batch size. This leads to the computation of an optimal batch size for each of the optimizers. The analysis is done for the case of a constant step size as well as diminishing step size. It is then experimentally verified on the CIFAR-10 dataset that the lowest number of steps is obtained when the batch size is close to the optimal value of the batch size predicted by the theory.",
            "main_review": "The main contribution of the paper is to construct a theory that describes the relation between batch size and runtime of various optimizers used in deep learning, and leads to a derivation of the optimal batch sizes. This theoretically confirms the numerical observations obtained in previous works. Since the choice of optimizers and their hyperparameters plays a crucial role in the practical performance of deep learning training, gaining a better understanding of the impact of hyperparameter choices such as the batch size is important. Moreover, the paper is technically sound and I could not find any technical flaws but did not check the proofs in the appendix. Moreover, the paper is well-written and quite easy to follow.\n\nOn the other hand, the practical impact may be limited as there is limited empirical novelty. Nevertheless, overall I believe that this work poses a solid theoretical contribution to the field of optimization in deep learning which may be of interest to a large part of the community. Therefore I am leaning towards acceptance of the paper.\n\nMinor Remarks:\n- The results in the table in Figure 3 are a bit misleading since the best result is obtained for the largest batch size which makes it look as if increasing the batch size more would lead to a further decrease in the number of steps, which is contradictory to their theory. Only in the description it is mentioned that increasing the batch size further would lead to the training not to converge within 200 epochs.\n- In Table 1 and Table 2 the paper mentions Adam-type approaches and specifically lists AMSGrad, AMSBound and AdaBelief in the text. In Figure 3 it just mentions Adam. Which of the Adam-type algorithms was used in the experiments?\n- In the conclusion it is shown that Momentum and Adam can exploit larger batch sizes than SGD, but that does not correspond to the results for diminishing stepsize in Section 3.2, where the optimal batch size for Momentum and SGD are in the same order of magnitude.\n",
            "summary_of_the_review": "While the practical impact may be limited, the work poses a solid theoretical contribution to the field of optimization for deep learning which may be of interest to a large part of the community.\n\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes an explicit dependence of (constant, diminishing) step size and batch size with momentum term also being critical. This analysis is executed on different algorithms including SGD, SGD with momentum, Adam-type, etc. This paper obtains the optimal match size for each algorithm respectively. ",
            "main_review": "Overall, the ideas and the contributions are clearly presented, and there is thorough analysis in the appendix. I can clearly see from the table the rate of each algorithm and how they depend on every parameter. The numerical experiment verifies the theorem. \n\nQuestions:\n1. What is the essense of the batch size? Before reading the paper, I imagined that a larger batch size means more averaging, thus the variance of the gradient is smaller. However, I find it might not be the case. (15) shows that the expected value of the stochastic gradient depends on $s$. First, why is it true? Assumption (A1) suggests that $f$ is an average of $n$ functions, and if I take the average of $s$ of them randomly chosen from the set, the expectation should not scale by $s^{-1}$. Seems it's not a typo since the next equations also have $s^{-1}$ scaling. Second, if it's true, then is choosing batch size equivalent to choosing step size? Is it better to multiply $s$ to the gradient and study the convergence? \nI feel this comes from the way of counting. A step with batch size $s$ is counted as an oracle of complexity $s$, thus the average (sub-)oracle gradient is equivalent to $s^{-1}$ times the actual stochastic gradient? I think this should be clear when writing the proof. \n2. Does the proof use the fact that more averaging means smaller variance in estimating $\\nabla f$? If so could you point me to the eq/step/lemma?\n3. Is $K$ both upper and lower bound orderwise, or it is only an upper bound? If it's the exact value subject to constant factor, then clearly minimizing over $K$ gives the optimal $s$, otherwise we only know this upper bound is minimized, and we don't know if it is tight. I think the experiment still verifies it, but it would be better if it can be covered theoretically.\n4. Does Adam-type method (i.e., choosing $H$) reduce to the case when Assumption 2.2 is true, and different $H$ choices under the assumption lead to the same result? I don't think this necessarily weakens the result, but just need to clarify.\n5. (Minor) What if $s=n$?",
            "summary_of_the_review": "I think overall the presentation is good and the contribution is meaningful, but I have some confusion above. I hope the author(s) can clarify about my confusion, and I'll raise the score. \n\n====================================================\n\nAfter discussion:\n\nI appreciate the author(s)' prompt response to my questions and the continuouse effort, which makes my understanding more clear. I regret to say that, I feel there is fundamental issue in the theoretical derivation, for which reason I have to change my \"Correctness\" score for now. It's still subject to change with further discussions. In particular:\n\n1. If the target is to minimize $K_\\epsilon$, I believe we should solve \n$$\\arg\\min_{s,\\alpha_k}\\ K_\\epsilon(s,\\alpha_k)$$\nor \n$$\\arg\\min_{s,\\alpha_k}\\ K_\\epsilon(s,\\alpha_k)\\ \\text{subject to}\\ \\alpha_k=\\alpha$$ \nrather than \n$$\\arg\\min_{s,\\alpha_k}\\ K_\\epsilon(s,\\alpha_k)\\ \\text{subject to}\\ \\alpha_k=\\alpha/s.$$\nFor the first two cases, the conclusion is that $s^* = n$.\n\n2. The author(s) mentioned that the better objective should be $K_\\epsilon s$ to address the total computational complexity, whereas I believe it should be something in the middle of $K_\\epsilon$ and $K_\\epsilon s$ with the consideration of difficulty of parallization at different kind of steps. In this case I believe $s^*<n$ can be true.\n\n3. Make sure that $K_\\epsilon$ is a tight upper bound. This can be verified by experiments showing the gap between theoretical and practical number of steps. \n\nThe details are in the following thread.\n\nI do believe that the formulation in Point 2 leads to a nontrivial result, and I suggest the author(s) try solving this problem and give appropriate justification about the \"oracle complexity\", \"number of steps\" and \"computational/time complexity\" (which is determined by the prior two quantities). \n\n====================================================\n\nAfter further discussion: In the thread with Reviewer 4PcN, the author(s) keep making false statements about the mistake, and at the end, the author(s) clearly add additional assumptions about $m_1, m_2$. It cannot be verified case-by-case, and it's even not true for my simple example. The author(s) also refuse to run the experiment to show that gap between theory and practice although I kept suggesting that and provided the starting code. Based on the above information I got, I believe the tightness is another fundamental issue that the approach is even far from \"Point 2 leads to a nontrivial result\", so I have to lower the overall score as well.\n\nI suggest the author(s) be prepared with more math maturity while responding to reviewer's comment and writing the proof.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}