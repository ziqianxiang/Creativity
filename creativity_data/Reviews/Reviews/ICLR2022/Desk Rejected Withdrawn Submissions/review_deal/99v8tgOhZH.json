{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents HW-aware NAS as a multi-objective optimization problem. ",
            "main_review": "While the problem is important and interesting, It is not immediately clear to me how this paper advances the state of the art. Additionally, there are no comparisons to prior work to contextualize the presented results.",
            "summary_of_the_review": "This paper is investigating an important problem but still requires further work to be ready for publication.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper presents a multi-objective optimization method for neural architecture search. Both model accuracy and hardware metrics (latency is used in this paper) are considered to search for models suitable for resource constrained devices.",
            "main_review": "There are a number of major issues with this paper:\n1. **Lack of novelty**: Several techniques have been used in this paper but all of them have been well studied in previous works. The authors did not differentiate their work from prior arts, and did not even cite them. The authors should do a literature search and review the papers that have been published recently. For example, a) Multi-objective optimization and constrained NAS have been studied in many literatures [1,2]. b) Accuracy and latency predictor have been proposed and used in lots of previous work [3,4,5,6]. c) There are many NAS methods, one of which is evolutionary algorithm that this paper used. It is quite outdated and the authors should take the latest work into account, d) Applying accuracy and latency predictors in constrained NAS setting using evolutionary algorithm is also presented in previous works [3,4].\n2. **Lack of reference and comparison**: Even the methodology is not novel, the authors should compare with prior arts as mentioned in the point above. The paper mentioned predicting performance without training, which is related to another line of work about zero-cost NAS [7,8].\n3. **Lack of technical details**: The novel part of this work is the 'upfront' approach, however it is only mentioned in the last paragraph of section 3. More technical details is needed - How is the architecure space being divided? Are the performance of architecture known upfront? Section 4 does not compare the proposed approach. The paper says existing method is not 'optimal', but the proposed method is also not backed up by experiment to show that it is better than existing methods.\n\nOther comments:\n- Figure 5: It says the green points are 'better trade off', which is not clear to me what it means. Why the Pareto-optimal points are not highlighted?\n- The paper says 'Pareto-optimal solution is not suitable for our case, ... randomly distributed'. Please explain.\n- Figure 6: What is it comparing against? If it is about multi-objective optimization, why is mono-objective also plotted here? It is also unclear if the proposed method can find model closer to the Pareto-optimal solutions.\n\nReferences:\n\n[1] M. S. Abdelfattah et al., \"Best of Both Worlds: AutoML Codesign of a CNN and its Hardware Accelerator\"\n\n[2] M. Tan et al., “MnasNet: Platform-Aware Neural Architecture Search for Mobile” \n\n[3] L.Dudziak et al., \"BRP-NAS: Prediction-based NAS using GCNs\"\n\n[4] H. Cai et al., \"ProxylessNAS: Direct neural architecture search on target task and hardware\"\n\n[5] C. Wei et al., \"NPENAS: Neural predictor guided evolution for neural architecture search\"\n\n[6] W. Wen et al., \"Neural predictor for neural architecture search\"\n\n[7] J.Turner et al., \"Blockswap: Fisher-guided block substitution for network compression on a budget\"\n\n[8] M. S. Abdelfattah et al., \"Zero-Cost Proxies for Lightweight NAS\"",
            "summary_of_the_review": "The work presented in this paper is incomplete. There are missing references and comparisons with prior arts. The methods described are not novel and it is unclear if the results are convincing and better.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper presents an approach for performing NAS within a latency-restricted environment for inference. An evolutionary algorithm is used.",
            "main_review": "Strengths:\n- NAS under constraints is an important area and it is good to see people working in this area.\n\nWeaknesses:\n- The work presented is rather outdated and has pretty much all been done before.\n\nMore comments:\n- The paper fails to take into account the research in NAS which has happened in the last five years. The research has moved away from evolutionary or reinforcement learning approaches towards differentiable, few-shot and zero-shot NAS approaches. The paper fails to take this into account. \n\n- The Related work section is more of a background section. A quick google scholar search will reveal many papers in this area. However, the related work section does not discuss or compare this work to those papers but instead introduces the ideas behind the authors work.\n\n- Given the wealth of material already out there on HW-NAS it is very surprising that the authors have not compared their work with the existing literature in the results section. This makes any claims about better performance unsubstantiable.\n\n- There is a lot of material at the start of the results section which is not results but more methodology. This should be moved to the right section. \n\n- There is not enough material presented in the work to reproduce the results or for a full understanding of what was done.\n",
            "summary_of_the_review": "The authors fail to take into account the existing work done in HW-NAS or the fact that NAS has moved on significantly from the approaches they propose. There is little, if any novelty in the approach proposed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents an approach to multi-objective NAS that tries to find Pareto-optimal models (e.g., latency vs. accuracy) by searching for the most accurate models for different latency thresholds.\nExperiments consider a custom (i.e., non-standard) search space based on VGG which includes the following: number of layers in each conv block, input image resolution, per-layer width scaling factor - notably, kernel size is fixed to be 3x3 and connectivity between layers is fixed to be a simple feedforward network (no skip connections?).\nSearching is performed using latency and accuracy predictors (a simple feedforward network taking arch encoding as input) together with an evolutionary algorithm - networks are trained on a subset of ImageNet consisting only of the 20 least frequent classes.\nHardware-awareness is achieved by considering (predicted) latency on a CPU and a Xillinx FPGA.",
            "main_review": "Unfortunately, the paper is badly written - leaving a lot of things unspoken - making it extremely hard to fully understand what the authors wanted to say.\nConsequently, I have serious doubts if the authors even understand the problem they are trying to solve, which is my main concern and the reason behind the extremely low score - this is explained in details in point 1 below (I'm happy to be proven wrong during the rebuttal).\nHowever, even if the thinking behind the paper is actually correct, there are still some serious problems with presentation, writing, overall clarity and experimental setting - pretty much the entire paper; these are covered by the points following the first one.\nConsidering the amount of shortcomings and the overall low quality of the submitted paper, I decided to skip the usual \"strengths\" part - in my opinion, the paper should first be improved in all aspects before it can be assessed in a usual way.\n\n1. As explained above, I find a lot of things presented in the paper to be plain wrong, or at least presented so imprecisely that they appear to be wrong.\n\n The main comment here would be related to the notion of optimality in multi-objective optimization.\n The paper contains numerous statements about how different methods do not guarantee \"an optimal solution\" (e.g., last paragraph on page 2).\n However, it is never explained what the authors mean by \"optimal\" - the usual approach is to either find the most accurate model for a predetermined latency threshold or to find Pareto-optimal models (which can be achieved, e.g., by maximizing accuracy for many different latency thresholds).\n The paper does contain some references to all these, without clearly distinguishing what the actual goal is.\n\n For example, equation (2) suggests that we simply want to find the most accurate model for a fixed threshold, in which case the comment on page 2 about a constrained accuracy search being unable to find the optimal solution is wrong as this setting directly optimizes for what is shown in Eq. (2).\n\n Further, Eq. (3) suggests that we want to maximize accuracy and minimize latency at the same time. However, this equation is completely wrong - an intersection of models that maximize accuracy with the models that minimize latency is most likely an empty set... in general, we can't optimize conflicting metrics at the same time, that's why concepts like Pareto-optimality were introduced in the first place (by the way, if I understand the equation correctly, it should be argmax not just max...).\n\n Which brings as the last point - the authors mention that Pareto-front \"can be found when the two parameters are negatively correlated or distributed\". Again, this is wrong. Pareto-optimality is well-defined for any set of objective functions (maybe except for some edge-cases when there are domain issues, but then the optimization problem itself could probably be deemed not well-defined) and in general Pareto-front can be found regardless of their nature (a different question is whether that makes sense or how hard it is).\n What is more, neither negative correlation nor negative distribution (negatively skewed distribution???) has anything to do with multi-objective optimization - I suppose the authors tried to refer to the fact that objectives should be conflicting, but negative correlation is something else! This is clearly visible even in the exact setting considered by the authors: accuracy and latency are conflicting and (usually) positively correlated! What is more, as explained before, there is nothing stopping us from finding a Pareto-front even when objectives are not conflicting (it might be just a single model, though)...\n\n Following up on the Pareto-optimality, we can further read that \"However, Pareto-optimization is not suited for our case, where accuracy and latency are randomly distributed.\"\nFirst of all, it's unclear what \"Pareto-optimization\" means - do the authors refer to a specific algorithm that tries to find Pareto-optimal models (in which case it would be desired to refer to the algorithm explicitly) or a generic family of methods that share this goal? If it's the second (my current understanding) then this sentence is again wrong.\n Per my previous point, Pareto-optimality can be decided regardless of the distribution of objective functions.\n\n This is even more serious mistake considering that what the authors later propose is actually a method that finds Pareto-optimal models! It's basically the epsilon-constraint method run for different constraints (possibly with minor modifications, but since I find it hard to fully understand what the authors propose I can't precisely say to what extend the proposed method is novel)!\nIn other words, the authors claim that they can't be searching for Pareto-optimal models and then propose a well-known method that finds Pareto-optimal points - I think this summarizes my concerns regarding correctness quite well.\n I'm happy to hear an explanation from the authors that would clarify all the details and provide a common, **solid** background for their work in the context of multi-objective optimization.\nHowever, in the current form the submitted text seems to me to be mathematically incoherent.\n\n2. The structure of the paper needs some extra work. Currently different parts are interleaved without much consistency - e.g., Eq. 1-5 are under \"Experimental results\" whereas they clearly should have been put under \"Method\" or \"Background\"/\"Related work\".\nSection 4 also repeats some information from Section 1, and so on.\n\n3. In general, writing of the paper is rather poor. Apart from the concerns about math outlined in Pt. 1, there are similar problems regarding clarity in other aspects.\nFor example, we can read things like: \"We collected thousands of (architecture, accuracy) pairs (...)\" -> the actual number of not given, there is a big difference between 1500 and e.g. 15000; or \"Our predictor-based approach accelerates the NAS search process by significantly reducing heavy search cost.\" -> both the cost of the baseline search and the proposed methods seem to have not been reported anywhere (apart maybe from hard-to-read plots, see 5).\n\n4. The actual algorithm is never presented clearly in any form (including purely textual). Instead, pieces of information seem to be added \"ad-hoc\" in different parts of the paper without any visible organization. As a result, after reading the paper a few times now, I still don't have a completely clear picture of what the method looks like end-to-end. The section in the Appendix does not help much.\n\n5. Results are also not presented clearly, all we have is a few plots that are rather hard to read when it comes to assessing performance of individual models - no summary of how well the proposed method (or any other) does quantitatively, mostly just some vague statements like \"We also were able to significantly reduce computing cost [never mentioned] of the architecture search process by building both accuracy predictor and latency estimator [no ablations with/without predictors], which makes NAS more reproducible [how???] and accessible [how???]\".\nAlso, following my comments in Pt. 1, it is unclear how the green dots in, e.g., Figure 5 were obtained. According to the caption \"green points are the architectures with the optimal trade-offs between accuracy and inference latency.\" but we can see many points that achieve the same/worse accuracy at a much higher inference time being highlighted in green - how are those points optimal in the context of latency-accuracy trade-off???\n\n6. The authors propose a completely new search space based on a very old VGG without a clear reason. This is not a problem per-se, but at the same time the authors do not present results of any of the existing algorithms on it. We only have results presented in Figures 1 and 2 which refer to some generic \"mono-objective\" and \"multi-objective\" searches, respectively, without providing necessary details to precisely connect those results to any existing algorithm. This is not enough to present a compelling case of why a proposed method is better (especially in light of rather poor motivation regarding multi-objective optimization).\nThis is especially relevant since there is plenty of different search spaces, including NAS benchmarks designed specifically for HW-NAS that could be used instead and provide more opportunities for meaningful comparisons with existing methods.\nAlso, from what I understand, the architectures have never been assessed on the full dataset, meaning that the results presented in the paper are not comparable to anything in the current literature that I'm aware of.\n\n7. The authors make some claims regarding existing algorithms that I find unjustified. For example, they claim that one method to perform HW-NAS is to: \"search candidate architectures with accuracy by repeating for many iterations. Then, to screen [is that a correct usage of this word?] only architectures satisfying the constraint of latency and to find the architecture with the highest accuracy within the constraint.\" However, I'm not aware of any published work that would propose an approach like that - it seems very inefficient and quite rightly is unlikely to produce good results.\nOn the other hand, there exist works that try to discover a Pareto-front in a single process (e.g., [1,2,3]), similar to what is proposed in the submitted work but the authors do not provide any information about comparison of their method to those.\n\n8. There are numerous errors, small contradictions and badly written (in a stylistic sense) places throughout the paper - some selected ones include:\n - \"Y verse X\" -> \"Y versus X\", \"verse\" is a line in a song\n - \"An architecture population is divided to\" -> \"divided into\"\n - \"The selected ones play knobs to guide the direction of evolution in NAS.\" - play knobs?????\n - \"(..) the optimized architecture extended its top accuracy to a much lower inference latency regime.\" -> this sentence suggests that an architecture now runs faster with the same accuracy (it was compressed/accelerated), but - as far as I understand - the authors wanted to say that a different architecture was found that runs faster and achieves similar accuracy\n - \"reduce computing cost of search\" -> \"reduce computational cost of a search\", or just \"reduce cost of a search\"\n - \"Applying NAS algorithms based on the proxy metrics, however, can be only suboptimal since the proxy metrics do not always correlate well with measured hardware-metrics\" -> logic error, it's either \"can only be suboptimal\" or \"the proxy metrics do not always correlate\" - the first one suggests that the metrics never correlate, the second suggests that they can correlate (so it can be optimal)...\n - \"the architectures of top accuracy are selected\" -> \"the most accurate architectures are selected\"? or \"the architectures achieving top accuracy are selected\"\n - \"This gap could mis-guild HW-NAS\" -> \"misguide\"\n - \"hardware-cost\" -> \"hardware cost\"\n - \"real-time execution for\" -> \"real-time execution of\"\n \n9. It would be good to compare to some published evolutionary algorithms that try to find Pareto-optimal solutions, e.g. [4] seems to be very relevant for the submitted paper.\n\n\n**References**:\n\n[1] B. Paria et al. \"A Flexible Framework for Multi-Objective Bayesian Optimization using Random Scalarizations\". 2020\n\n[2] E. Liberis et al. \"uNAS: Constrained Neural Architecture Search for Microcontrollers\". 2021\n\n[3] B. Moons et al. \"Distilling Optimal Neural Networks: Rapid Search in Diverse Spaces\". 2021\n\n[4] K. Deb et al. \"A fast and elitist multiobjective genetic algorithm: NSGA-II\". 2002\n",
            "summary_of_the_review": "Very poorly written, making it hard to fully understand what the authors did.\nThe background behind multi-objective optimization seems completely wrong at worst, at incoherent at best (could be attributed to poor writing).\nExperimental results are not comparable to anything and not even presented clearly (what is the best model found for different latency thresholds?).\nMissing comments about comparison to some relevant existing works.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}