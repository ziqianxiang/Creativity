{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposed a method to train the discrete voltage parameters for optical neural networks. The proposed method incorporate Gumbel-softmax to make discrete voltage optimization differentiable so that the voltage parameters can be updated via conventional SGD techniques. The authors also proposed to include the scale factor (lambda) for compensating the attenuated intensity of input light through the diffractive layers. The authors provided basic explorations of the proposed method, such as the temperature schedule for the Gumbel-softmax, comparison with conventional quantization, and the ablation study about the lambda factor. ",
            "main_review": "This paper's adaptation of the DNN training algorithm for emerging computing devices is very interesting. However, the proposed method seems to be preliminary for the following reasons:\n\n- The idea of employing Gumbel-Softmax sounds straightforward. If the discrete manner of the voltage parameters is the core issue, there could be other workarounds such as reinforcement learning or straight-through estimator. It is not clear why Gumbel-Softmax is particularly useful for DONN; would there be any specific aspects of DONN well suited for Gumbel-Softmax?\n\n- Comparison with conventional quantization looks appropriate, but the depth of analysis is a little disappointing. In fact, it seems that QAT with a straight-through estimator should also work pretty well for discretizing the voltage parameters since it is reported to compensate quantization error well. More in-depth theoretical analysis on the characteristics of voltage discretization of DONN and the comparison with STE would be desirable.\n\n- Motivation of introducing a scale factor for complex-domain data processing. But the choice of lambda seems to be empirical. Would there be an automatic way to find out the best lambda? Could lambda be different for different layers?\n\n\nQuestions\n- Many physical characteristics of DONN seem to be missing. For example, what are the physical constraints on the values of voltage parameters? What are their dynamic ranges? Are the voltage values uniformly distributed?\n\n- Since the training framework is based on Pytorch, how does it reflect the non-ideality of computation originated from the physical aspects of the devices?\n",
            "summary_of_the_review": "\nThis paper seems to open up an interesting research topic on improving the accuracy of optical neural networks. But the current manuscript might be in the preliminary stage towards a great discovery. Therefore, I am inclined toward rejection for now, but I look forward to the upcoming breakthrough.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes techniques to improve the training of diffractive optical neural networks (DONNs). Specifically, the paper uses Gumbel_Softmax to map the discrete level inputs of the optical system, and a regularization method to balance the weights gradients from amplitude and phase in complex-domain. The approach is verified on five diffractive layer networks and on MNIST and FMNIST dataset.",
            "main_review": "Overall, the paper is well written and technical details are well explained. The idea of using Gumbel Softmax to map discrete levels for training is straightforward. The complex-domain regularization technique is intuitive, and the results are supportive. The algorithms used in this paper are not new for classical neural networks but seems helping for DONNs.\n \nThere are a few concerns/questions that I would like clarification.\n\n1). The paper claims that Gumbel Softmax is better than other quantization methods to map the low precision discrete levels. First of all, Gumbel Softmax has been used as one of the approaches for low precision quantization (e.g. https://arxiv.org/abs/1810.01875). The authors should do a thorough comparison with related works. Secondly, to claim Gumbel Softmax approach better than PTQ and specially QAT, the author should provide more evidence. For example, on Fig. 3, what quantizers are used in PTQ and QAT? Are the QAT training conditions optimized? Numerically, what causes the better performance from Gumbel Softmax?\n\n2). Another question on Fig. 3: are the experiments list on Fig. 3 using the regularization as well. Wondering how much accuracy improvement from Gumbel Softmax and how much from the regularization.\n\n3). It would have been helpful if the author could provide more details on the experiments setup, for example, how the model trained in Pytorch on GPU, then deployed on the optical system?\n\n4). Another general concern is the small model and dataset that being used for the algorithm verification. I understand that the experiment setup is limited by the optical system. However, it is hard to predict if the same algorithm will be working on large models. The optical accelerator is clearly too far away from useful albeit its potentials. It seems to have some fundamental physical limitation. At this stage, I am skeptical on the algorithm side optimization since it cannot be verified on large models.\n\n",
            "summary_of_the_review": "In general, I think the paper is well written. The work may be useful for the research in DONN area.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors present an algorithm for training diffractive optical neural networks (DONN). The authors propose the Gumbel-Softmax with a complex-domain regularization to enable a differentiable mapping from discrete device parameters. The authors test their system on MNIST and FashionMNIST",
            "main_review": "Strengths\n-The paper is for the most part well written. However there is still room for improvement\n-An interesting application, and I find the algorithm presented interesting\n\nWeaknesses\n-Make in the introduction a better connection to recent work on low bit precision neural networks, and how this work differs. A comparison table may help\n-Near the start give a clear indication of weights & activations and how DONNs are discretized. eg: the number of bits per weight, per activation etc. This would make the entry path for someone who is not very familiar with DONNs but is familiar with the low bit precision literature more easy.\n-Eq 1: Define precisely what one_hot is. My understanding is that it is a binary vector, but be more explicit, don't use functions that were not defined previously\n-Define what Gumbel(0,1) and why it is needed. How would performance be affected for example by a simple Bernoulli distribution instead?\n-Above Eq.3: w^{i,j} is a real number representing voltage but in Eq.3 it is one_hot. I find this confusing, clarify\n-Where is f(W) defined?\n-Near Eq.4: Where is G(\\theta, g) defined?\n-Below Eq4: typo: \"by multiply\" should be \"by multiplying\"\n-Testing the algorithm on more challenging datasets (like CIFAR10/CIFAR100) would make the paper better, however I am not sure how easy this would be for DONNs\n-Unless I missed it, will source code be provided?\n-There is a lot of prior work on complex neural networks. Indicate more the relation of this prior work to your work\n-Section 4: was this tested on an actual physical DONN or was everything simulated? It was not clear to me. Make this more explicit",
            "summary_of_the_review": "Overall I find the algorithm interesting and the potential application to DONNs novel. As I indicate above there are a number of issues that should be addressed before publication.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper makes connection between Gumbel-Softmax with discrete states of training regime of optical neural network. They propose training regime based on this Gumbel-Softmax (together with temperature scheduling, complex-domain regularization) and show good result on MNIST and FMNIST dataset.\n\n\n\n\n",
            "main_review": "[Strengths]: \nUsing Gumbel-Softmax, authors make insightful connection of the discrete state to continuous state, enabling the effective training/learning of the network.\n\n[Weakness]: \nI have concern with the result part. The comparison is made with a 2018 paper. If we compare result of the same group in 2019 (also in this paper's references) Mengu et al (2019), their updated results may be a stronger candidate for making comparison. ",
            "summary_of_the_review": "This paper proposes a new way to handle poor training condition due to discrete state characteristic of the optical neural network. In their proposed solution, they make use of Gumbel-Softmax (together with temperature scheduling, complex-domain regularization) to enabling effective training. They show good result as compared to previous method. \n\nMy concern is mainly on the result part as mentioned in the main review. \nPlease makes some comparison with some of the more recent papers to highlight difference in approach. What will be the limitation and strengths of current method?  Is method in this paper could possibly be complimentary to other existing methods, to further boost the performance?",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}