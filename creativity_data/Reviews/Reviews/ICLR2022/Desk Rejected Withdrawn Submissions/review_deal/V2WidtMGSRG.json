{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proves a identifiability result for 1-hidden layer ReLU network, where the true data is generated from the same architecture. Following the result, authors proposed performing NN-based variable selection by inspecting the weight norm of LASSO-regularized network. Experiments on multiple benchmarks and under a variety of data generation mechanisms are conducted, where authors show competitive or improved over performance over existing methods.",
            "main_review": "Strength: \nAs far as I can tell, this is the first identifiability result for LASSO-regularized ReLU neural network models. However, given that the variable selection consistency has been proved for other types of networks (e.g., [1-2] and a list of related work in introduction of [2]), the novelty of the result is slightly compromised (although still interesting).\n\nWeakness:\n* A related work section is lacking: applying LASSO/Group-LASSO to neural network is not a new practice [1-5], and there does exist a body of work considering the parameter identifiability [9], the theoretical properties of neural network models in variable selection [1-2] and even in Bayesian context [7, 8].  Please consider including a section discussing how the current work is related to these previous works.\n\n* Experiment design:\n   * While the theoretic results is centered around parameter identifiability of the hidden weights, the experiment section seems to focus only on variable selection. For a theoretical work like this, I feel it is important to empirically verify theoretical claims by studying the convergence of hidden weights under different sample size / dimension / signal-to-noise level. This is likely the key missing piece of this article.\n  * As mentioned previously, there exists many methods applying LASSO to neural networks. How does the authors' algorithm compare to theirs, and if there exists a LASSO algorithm that is meaningfully different, please include them as one of the baselines.\n*  I noticed authors' variable selection results is reported using TP and FP, which is threshold dependent. For a more comprehensive evaluation, please consider also reporting AUROC for variable selection. Also please report the standard deviation of the results.\n\n\n\n\n[1] Vu Dinh and Lam Ho. Consistent feature selection for neural networks via Adaptive Group Lasso. arXiv preprint arXiv:2006.00334, 2020.\n\n[2] Vu Dinh and Lam Ho. Consistent Feature Selection for Analytic Deep Neural Networks. [NeurIPS 2020](https://papers.nips.cc/paper/2020/file/1959eb9d5a0f7ebc58ebde81d5df400d-Paper.pdf)\n\n[3] Jean Feng and Noah Simon. Sparse-Input Neural Networks for High-dimensional Nonparametric Regression and Classification\n\n[4] Ismael Lemhadri, Feng Ruan, Rob Tibshirani. LassoNet: Neural Networks with Feature Sparsity\n\n[5] Simone Scardapane, Danilo Comminiello, Amir Hussain, Aurelio Uncini. Group Sparse Regularization for Deep Neural Networks\n\n[6] Jian Wang, Huaqing Zhang, Junze Wang, Yifei Pu, Nikhil R Pal. Feature Selection Using a Neural Network With Group Lasso Regularization and Controlled Redundancy\n\n[7] Yuexi Wang, Veronika Rockova [Uncertainty Quantification for Sparse Deep Learning](https://proceedings.mlr.press/v108/wang20b.html)\n\n[8] Jeremiah Zhe Liu. [Variable Selection with Rigorous Uncertainty Quantification using Deep Bayesian Neural Networks](https://proceedings.mlr.press/v130/liu21g)\n\n[9] Verner Vlačić, Helmut Bölcskei. Neural network identifiability for a family of sigmoidal nonlinearities\n",
            "summary_of_the_review": "An interesting theoretical work that proves the identifiability of LASSO-regularized ReLU network weights. Author proposed a LASSO-algorithm which appears to be similar to previously works, and conducted variable selection experiment on a collection of data benchmarks. The related work section and a direct empirical evaluation of the theoretical result  appears to be missing.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work proposes a Lasso-type algorithm for two-layer neural networks. They show that if the neural network is sparse then their estimator can reconstruct the neural network and identify the non-zero coefficients when the number of samples scales logarithmically with the input dimension. The authors also propose a neural network-based variable selection method. Their experimental results show that their method is efficient for variable selection for nonlinear, large-dimensional, and noisy systems.",
            "main_review": " The paper is well-written and well organized. The paper sounds technical. I have checked some proofs and they seem correct to me. \nHowever, I have two concerns. First, it is the motivation behind this work. It seems not practical that we can access the input and output of a neural network in many samples.  Second, it is the reasonability of the assumptions. The assumption that w_i and w_j are near orthogonal is strong. In addition, the constants c_1, c_2, c_3  as well as the dependence of |W|_0 in Theorem 1 are not clearly discussed.  ",
            "summary_of_the_review": "I think that the paper is on borderline. However, I am leaning towards rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies an interesting problem that applies the popular Lasso regularization to 2-layers ReLU networks and analyzes convergence rate in the sample size and problem dimensions. Under the restricted isometry property, the authors show the logarithmic dependence in problem dimension p in Theorem 1. Furthermore, the variable selection consistency and unique parsimonious representation are discussed. Finally, in numerical experiments, the authors verify the efficiency of lasso type regularization on various different datasets. ",
            "main_review": "Strengths:\n\nThis paper discusses an interesting problem combining lasso with the 2-layers ReLU network and shows the convergence rate in theory. The proof techniques are novel and solid. I check the detailed proof and no major flaws are found.\n\nWeaknesses:\n\nSince the major contribution of this paper is from a theoretical perspective, it is ok to use small or synthetic datasets in numerical experiments. It would be better to include a relatively large dataset (e.g., MNIST or CIFAR10) to test the efficiency of the proposed Lasso type regularization. In classic deep learning, it is common to add L1 decay in the optimizer to attain sparsity. However, as the optimization problem (1) is different from adding L1 decay in the optimizer,  it may yield different numerical performance. ",
            "summary_of_the_review": "This paper considers an interesting problem applying lasso to the 2-layer ReLU network and shows the logarithmic dependence in total dimension. The proof techniques are novel and solid. Although I'm not fully convinced by the current numerical experiments, I think it is a well-written paper over all. At the current stage, I tend to accept it.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper analyses the identifiability of (shallow) neural networks by solving a Lasso problem. Its main contribution is theorem 1 where it is proved that under Gaussian iid assumptions on the data and assuming that the underlying network weights are sparse, one can guarantee recovery of the weights when the data scales polynomially in sparsity and logarithmically in dimension.",
            "main_review": "The main issue I see with this work is that the problem setting seems contrived, and the studied setting does not correspond to how neural networks are trained and used in practice.\nMain issues:\n- The iid Gaussian on the data $x_i$,$y_i$, is unrealistic.\n- This is not how neural networks are trained in practice, and it is unclear from the analysis why making an l1 penalty on the inner weights is beneficial.\n- The authors mention that most of the existing works focus on the study of generalisation properties instead of identifiability properties of network weights, but I think this is because identifiability in this case has little meaning in itself -- the neural network is an artificial construct with little meaning in itself (especially in the setting of Gaussian data studied), it is more the generalisation and classification properties that it is able to perform once trained that is interesting.\n- The numerics again only show identifiability, and not generalisation. So, there is no insight as to whether the Lasso formulation (of sparsity on the inner weights) is meaningful.\n\nOther comments: It is unclear how you perform the numerical experiments, you mention that ADAM is used, but how is the nonsmooth l1 term deal with? Is this basically subgradient descent?\n\n\nFinally, there are some important references missing: the reformulation of training neural networks as a sparse convex regularisation problem is not new. See for example\nBach, Francis. \"Breaking the curse of dimensionality with convex neural networks.\" The Journal of Machine Learning Research 18.1 (2017): 629-681.\n\n",
            "summary_of_the_review": "Overall, I find the setting unrealistic and does not correspond to how neural networks are used in machine learning.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}