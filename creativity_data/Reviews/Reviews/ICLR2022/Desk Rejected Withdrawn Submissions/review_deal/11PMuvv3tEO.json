{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper takes the min-max GAIL formulation and adds a third Lagrangian \"player\" to their game to enforce an auxiliary cost constraint. That safety aspect is expressed via the expected sum of cost criterion, as used in CMDPs. The authors then apply a form of stochastic gradient descent ascent ascent  (SGDAA ?)  to approximaly solve the resulting game. ",
            "main_review": "While the authors have some empirical results to show that their approach can work, the proposed approach is built on a recipe of unstable methods which I doubt would be easy to work with in practice. Let's zoom back: deterministic constrained optimization with equality constraints only and throw gradient ascent-descent at it. The resulting \"first-order Lagrangian\" (Bertsekas nonlinear programming textbook) method (Arrow-Hurwicz-Uzawa style) can only be shown to converge locally per Ostrowski's theorem; the case where we have noisy gradients doesn't make GDA any easier, per the recent wealth of results in game-theoretic optimization in the GAN literature. The proposed paper now goes beyond the typical model-free RL CMDP setup and also involves a min-max problem in the \"outer\" objective of their problem. We are now essentially dealing with a stochastic three-player game where each player gets updated at different time scales (a three-timescale stochastic approximation analysis would probably be appropriate here). And on top of that, of the player (the \"RL\" one in GAIL\") itself is inherently running on two-timescale ideas (the actor-critic architecture)\n\nThe way that I see this paper is that it should spend most of its time explaining why the \"obvious\" idea of adding one player within any other algorithmic consideration (appart from the PID damping) is apparently sufficient. The main contribution would be to convince the reader that, despite what they know, \"that one simple trick\" is enough and they should look no futher. \n\nActionable feedback: \n\n- Study and run ablation studies to convince the reader that your algorithm is stable across hyperparameters/timescales. You could have a plot showing the effect of changing (on the x-axis) the timescale ratios (number of GAIL updates) vs Lagrangian updates vs actor-critic updates and the performance on the y-axis\n\n- Explain clearly why working with GAIL, from the very begining in the intro. Why, in the specific context of safe IRL in the CMDP framework, choosing to work with GAIL is the right choice and not some other \"outer\" objective. \n\n- Try to find a way to write what you do in the form of a standard nonlinear constrained optimization problem. There are glimpses of it in the paper, but never a clear statement of the mathematical program. This is important because if you talk about Lagrange multipliers, then you enter the world of constrained optimization and the Karush–Kuhn–Tucker conditions. You need to be able to write your problem in standard form to use these concepts in the first place. \n",
            "summary_of_the_review": "I don't see a strong enough technical contribution in this paper. It may be a \"first to do x with y\"-type paper, but not one which addresses a previously challenging technical problem with new insights. However, it may be the case (but the authors would have to reframe the paper accordingly) that this paper demonstrate that the \"simple\" strategy is enough to get good performance, and that the community should look no futher. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper considers the problem of safe imitation learning (IL), wherein the goal is to learn a controller than satisfies a safety specification given some expert data.  The authors consider a setting in which the learner only has access to rollouts from an expert controller collected offline.  Moreover, it is assumed that some of this data is unsafe.  The authors then propose a primal-dual style algorithm for tackling this problem, and provide a set of experiments using this approach.",
            "main_review": "### Strengths\n\n**Safe learning.**  The paper studies the critical topic of learning under safety requirements.  As the authors note, this topic is important because it is often essential to ensure that a safety constraint is satisfied in real-world applications.  To this end, I think that proposing new methods for improving safety in learning applications is an important direction.\n\n**No access to the expert controller.**  It also seems reasonable to assume that we only have access to the expert through samples collected offline.  This makes the problem harder, as one cannot query the expert whenever one wants.  This is more similar to practical problems in which a good controller may be unknown, although safe/expert behavior may be easy to obtain.\n\n**Primal-dual approach.**  If our ultimate goal is to satisfy a hard safety constraint, then I think that the primal-dual style approach is a good tool for this problem.  Indeed, it is known that while regularization can work in practice, it is often not able to find a point which satisfies a safety criterion.  Moreover, regularization requires tuning weights that balance multiple objectives, which can be challenging in practice and can require expert knowledge.  So to sum up, I like the authors' use of the primal-dual scheme here as a principled yet practical method for thinking about safety.\n\n**Experiments.**  The experiments seem to show that (a) their method can achieve better satisfaction of the safety requirements and (b) there is a trade-off between reward and safety constraint satisfaction.  Both are interesting results, and in my opinion constitute the main contribution of this paper.\n\n\n### Weaknesses\n\n---\n\n**Reference and comparison to past work.**  I think that this paper could do a much better job of carefully citing past work.  In the abstract, the authors claim that the problem fo safe IL has been \"long neglected,\" and later on, they claim that \"little attention has been paid to guarantee the safety of agents in IL.\"  This seems untrue, as the authors have an entire paragraph in their related work titled \"Safe Imitation Learning\" with references to a dozen or so papers that study this topic.  I think this amounts to over-claiming, as the problem has seen some very recent progress.  Perhaps what the authors mean is that the particular instantiation of safe IL that they propose -- in which the safety information is available and the expert trajectories may contain unsafe data -- has not been studied.  While this seems more accurate, the claim of novelty seems more circular here, as it amounts to saying, \"The problem that we came up with has not been studied before,\" which is true for all papers that solve a new problem.\n\nThis relates closely to some comments I have regarding the experiment section.  There seems to be a lack of comparison to any of the baselines mentioned in the related work.  I understand that the setting is slightly different in that one cannot query the expert controller and therefore instead has to rely on pre-collected rollouts, but surely there are other baselines that could work here.  For instance, why does behavior cloning not work as a baseline.  \n\n---\n\n**Theory and guarantees.**  The authors claim that (a) previous methods cannot guarantee safety:\n\n> \"The above methods cannot guarantee the safety of the agents during training.\"\n\nand (b) that their method can make sure guarantees:\n\n> \"the proposed LGAIL can be guaranteed to learn a safe policy from unsafe expert data.\"\n\nHowever, I don't think that the authors ever offer such a guarantee, which means that from my perspective, they have both undersold previous work and have over-claimed with respect to their own work.  The authors do offer a \"theoretical interpretation\" of their work, but this really just involves stating the optimality conditions.  Indeed, the problem that they seek to solve is highly nonconvex, and even if it were not, the past work that they cite regarding the convergence of primal-dual methods relies on being able to find an $\\epsilon$-optimal point for the regularized primal problem.  Since the authors advocate for a method that uses a stochastic gradient based step in the primal problem, it's all but assured that they will not reach anything close to a close global minimizer of the objective.  So to summarize, I do not see how the authors guarantee anything about constraint satisfaction, especially given that presumably they want to guarantee safety of a statistical constraint for which, rather than having access to the full distribution, they only have samples.  \n\nThere are results that could be used to get a better idea of when one can make claims about guaranteeing constraint satisfaction or approximate constraint satisfaction (see e.g. https://proceedings.neurips.cc/paper/2020/file/c291b01517f3e6797c774c306591cc32-Paper.pdf), but without further details, I think that these claims are not validated.\n\n---\n\n**Incrementalism & lack of ablations.**  As a method, these seems somewhat incremental with respect to GAIL.  The only difference in Algorithm 1 seems to be the dual update step.  And while this yields a better empirical safety cost, it's unclear whether the proposed method is a significant enough contribution.  Indeed, one could imagine using this primal-dual perspective on any existing algorithm.  So from my perspective, the fact that the only real algorithmic contribution is to advocate for this dual update step doesn't amount to enough of a contribution to stand on its own.\n\nOne way to make a stronger case for why this dual step amounts to enough of a contribution would be to add ablation studies.  One could imagine running the algorithm with a fixed dual variable rather than dynamically updating it.  If Algorithm 1 does significantly better than this regularized approach, then it would indicate the the dual step is a fundamental difference maker in terms of safety.  Another ablation could be to sweep over $d_0$ and to show that this allows the user to control a trade-off between cumulative reward and safety.  It may be interesting to theoretically study this trade-off as well, which could provide additional insight into the proposed method.\n\n---\n\n**Practicality of the problem setting.**  The authors claim that their algorithm addresses a \"more practical Safe IL task\" (page 1).  I think that many (myself included) would push back on this claim.  The authors argue that the setting is more practical than the standard setting -- in which the learning has access to a safe policy -- because they do not require the full policy and because they can tolerate having some unsafe data.  However, is this actually more realistic?  In practice, my impression is that it's actually **very difficult** to obtain unsafe data.  Let's consider the example used by the authors or self-driving cars:\n\n> \"In autonomous driving, dangerous conditions such as collisions with pedestrians or cars can be easily identified.\"\n\nWhile this seems true, we should ask ourselves whether it's *practical* for practitioners to have access to data of crashes.  In general, I would tend to think that it's actually extremely difficult to obtain any meaningful amount of unsafe data corresponding to collisions with pedestrians.  The same could be said for robot crashes.  I think the real question to ask here is: Can I guarantee safety without having seen many examples of unsafe behavior?  In this way, I think it would be more insightful to look (experimentally or theoretically) specifically at settings in which very little safe data is available.\n\nThis relates to a sentence in the introduction:\n\n> \"Conducting Safe IL with unsafe expert dat his more realistic because... it is costly and laborious to obtain purely safe expert data because experts could take dangerous actions.\"\n\nAs the above self-driving example illustrates, I think it would actually be far more costly to obtain unsafe data, given that part of the cost should account for the considerable harm that could be done to humans if a car acts unsafely.  And on a separate note, if the expert takes many bad actions, is it really an \"expert\"?  Ultimately, do we really want to imitate an expert that frequently displays unsafe behavior?  All in all, I think that this paper could benefit from rethinking exactly how their setting is *more practical*.\n\nAnother point with regard to practicality: Doesn't behavior cloning also satisfy the criteria of not needing an expert policy?  One can just collect rollouts from the expert policy online and then use it to train in a supervised way.  \n\n---\n\n**The presentation of the optimization problem.**  There seems to be a mismatch between the proposed optimization problem in (4) and the algorithm used to solve it in Algorithm 1.  The main point of confusion is that $\\lambda$ does not appear as an optimization variable in (4), so the reader can only conclude that it is a fixed value chosen as a hyperparameter.  However, the algorithm advocates for a primal-dual style algorithm which performs dual ascent on the dual variable.  Yet there is no reason to update this variable in the formulation of (4).  \n\nIf the authors do want to use something like Algorithm 1, a more technically sound derivation of the optimization problem they are trying to solve is needed.  I think it would go something like this:\n\nStarting from (3), we can form the Lagrangian $L(\\theta, \\omega, \\lambda)$ as follows:\n\n$$ L(\\theta, \\omega, \\lambda) = \\mathbb{E}_{\\pi_\\theta} \\log D_\\omega (s,a)  + \\lambda \\left[ J_C(\\pi_\\theta) - d_0\\right] -  \\beta\\mathcal{H}(\\pi_\\theta) + \\mathbb{E}\\_{\\pi_E} \\log(1 - D_\\omega(s,a)).$$\n\nThen the optimization problem in (3) should be equivalent to something like:\n\n$$ \\min_\\theta \\max_{\\lambda\\geq0, \\hspace{3pt} \\omega} L(\\theta, \\omega, \\lambda).$$\n\nNotice that we have a maximization over the dual variable $\\lambda\\geq 0$.  Thus, if the constraint is not satisfied, i.e. $J_C(\\pi_\\theta) > d_0$, then we can take $\\lambda\\to\\infty$.  Now the minmax inequality tells us that\n\n$$ \\max_{\\lambda\\geq 0} \\min_\\theta \\max_{\\omega} L(\\theta, \\omega, \\lambda) \\leq  \\min_\\theta \\max_{\\lambda\\geq0, \\hspace{3pt} \\omega} L(\\theta, \\omega, \\lambda).$$\n\nThis problem on the left-hand-side is the **dual problem** to the problem in (3), and I think I'm correct in saying that this is the problem that Algorithm 1 is being used to solve, rather than the problem in (4).  The dual update in the final step of Algorithm 1 can then be thought of as computing the derivative of the dual function $\\min_\\theta \\max_{\\omega} L(\\theta, \\omega, \\lambda)$ with respect to $\\lambda$.\n\nI think that these details should be included in the \"Theoretical Interpretation\" paragraph on pages 5-6, because as written, it's unclear why the dual variable should be updates.  Furthermore, I don't understand why the authors have stated the optimality conditions, as this information is not used in the paper at any point (from what I can tell).  So unless there is some theoretical value to these results, I would remove them to avoid confusing the reader.\n\n---\n\n**Writing.**  The last point that I want to focus on is the writing.  I will not include this point in my final assessment, since I think that a paper should be judged on its ideas rather than on its presentation (within reason).  However, I think this paper would be much stronger if the writing were to be significantly improved.  There are numerous places where the word choice and grammar creates confusion.  For instance, the first sentence of the abstract:\n\n> \"Imitation Learning (IL) merely concentrates on reproducing expert behaviors and could take dangerous actions, which is unbearable in safety-critical scenarios.\"\n\nWhat does \"merely\" mean here?  The word \"merely\" needs a point of comparison, i.e. \"X does Y, while Z merely does W.\"  Is \"merely\" in reference to RL, in that IL can be thought of as a supervised counterpart of RL?  Also, the reader doesn't know what \"safety\" means in the context of RL.  Further, what does it mean for dangerous actions to be \"unbearable?\"  \n\nThis kind of thing persists throughout the whole paper, and I think that it makes the paper much harder to understand. Another example involves the use of the words \"strictly\" and \"absolutely\" (page 1).  The authors repeatedly say that they do not assume that they expert data are \"totally safe\" or that \"the expert policy is [not] absolutely safe.\"  What do these adverbs actually mean in a technical sense.  Is it that there are particular state-action pairs which violate a safety constraint?  Is it that full trajectories are unsafe?  Is it that we have multiple experts, and only some of them are safe?  Up until this point in the paper, the reader has no way of knowing what \"strictly\" means, and this make the paper more confusing than it needs to be.",
            "summary_of_the_review": "To summarize, I think there were several positive aspects of this paper.  The problem is important, the algorithm seems natural, and the experiments validate some of the hypotheses.  However, there is insufficient comparison to baselines and past work is disregarded in several places, there are not theoretical guarantees despite claims of such guarantees in the introduction, the algorithm feels rather incremental on top of GAIL, there is a lack of ablation studies, the problem setting seems somewhat impractical, and the derivation of the optimization problem corresponding to Algorithm 1 seems incorrect.  So while I think that this paper can be a great contribution, my opinion is that it needs significant revisions before it can be accepted.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper considers the problem of safe generative imitation learning for cases where the expert may behave unsafely. Building on GAIL, they use the Lagrangian formalism (and, to some extent, build on constraint MDPs) to introduce safety constraints. They present an algorithm that builds on a  notion of simultaneously updating the Lagrangian, and the Lagrange multiplier (to weigh the safety penalty), to arrive at a safe imitator.",
            "main_review": "I'll stay brief. Besides the point in the Summary, here is just one: I appreciate the more theoretical view in sec 4.3.2. However, I think that should be made more specific, in particular under which assumptions the eventual constraint satisfaction it actually holds. Maybe state this as some form of Theorem?\n\n",
            "summary_of_the_review": "I agree with the relevance of safe IL. Furthermore, using Lagrange formalisms, and using powerful methods based on them (such as the dynamically adaptation of the Lagrange factor in sec. 4.3.2.) intuitively makes sense. \n\nHowever, I think the paper is too premature. I feel that too much time is spend on motivation, and on introducing and reformulating the Lagrange-augmented loss function (eq4), compared to time spend on developing the actual contribution in depth. More importantly, safe generative IL methods where the GAIL loss is augmented by a safety penalty already exist (e.g., Raunak Bhattacharyya et al.: \"Modeling Human Driving Behavior through Generative Adversarial Imitation Learning\"), and the difference to them is not discussed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a safe imitation learning (IL) problem, where, in addition to imitating expert, the agent policy needs to satisfy a pre-defined safety constraint. The paper formulates this problem as a constrained divergence minimization problem and solves it using GAIL and Lagrangian relaxation. The proposed method, named LGAIL, is empirically evaluated on safety Gym tasks. The results indicate that LGAIL can learn close-to-expert policies while satisfying safety constraints. \n\n### Contributions:\nThe main contributions are a new problem setting of safe IL and a new method the solve it. \n",
            "main_review": "### Strengths \n\n1) Interesting problem setting:\n\nAn interesting and practical problem setting is introduced. The paper gives a clear and convincing explanation to why this problem should be of interested and studied further. \n\n2) Simple and sound method:\n\nThe paper proposes a simple and sound method to solve the problem based on GAIL and Lagrangian relaxation. Nonetheless, I also think that the method is too simple, since it is a direct  combination of GAIL and TRPO-Lagrangian. The theoretical justification partially supporting the claim that LGAIL learns a safe policy, although the result is not particularly surprising since it follows from the standard constrained optimization. \n\n3) Clarity:\n\nThe paper is overall well written and clear. I have only minor comments that some mathematical notations are not consistent: $R$ is overloaded to denote both the reward function and the cumulative rewards, and $w$ and $\\omega$ are both used to denote the discriminator's parameters.\n\n### Weaknesses\n1) Too few baselines: \n\n  I find that the experiments is somewhat unfair since LGAIL is the only method that uses the cost function during training. To have a fair baseline method, I suggest evaluating a naive-extension of GAIL that uses the cost function as a negative reward together with the discriminator reward (similarly to a weighted reward combination from POfD (Kang et al., 2018)). This naive baseline would be similar to LGAIL except that LGAIL adapts the cost weight $\\lambda$ during training. If LGAIL can outperform this baseline, it would strongly indicate that the Lagrangian method is important to realize safe IL. The same extension could be applied to 2IWIL/IC-GAIL as well to make the results stronger. \n\n  Other baselines that should be considered are inverse RL with other safe RL besides Lagrangian-based methods. For example, one may use GAIL's discriminator reward with CPO (Achiam et al., 2017) to learn a safe policy. Comparing this baseline against LGAIL would make the claim to use the Lagrangian method more convincing. \n\n2) Theoretical result is lacking: \n\n  In my understanding, the safety constraint biases the solution of divergence minimization so that the learned policy does not induce unsafe behavior. From this, an important question is, does this bias negatively affect the optimality of the original divergence minimization problem? I.e., does the learned policy equal to the safe expert policy in Assumption 1?\n\n  Based on the experimental results, this seems to be the case since LGAIL achieves good returns. However, the paper would be more convincing if it is guaranteed in Section 4.3.2 that LGAIL indeed learns the safe expert policy and not just \"a policy that satisfies the safety criterion\". \n\n### Suggestions/questions \n\n1) The entropy regularizer may be a reason to why GAIL with purely safe expert data learns unsafe policies in Section 5.3. This is because the entropy may bias the agent data to be outside of the expert data distribution. It will be fruitful to investigate and discuss the effect of entropy coefficient $\\beta$ on the safety criterion. \n\n2) It is mentioned that LGAIL fails to strictly maintain the safety during training. This could be because the Lagrange multiplier $\\lambda$ is updated with only a single gradient step in each training iteration. Have the authors tried update $\\lambda$ repeatedly until convergence in each training iteration?\n\n",
            "summary_of_the_review": "Overall, the paper proposes a new method to solve an interesting IL problem setting. However, since the proposed method is relatively simple, the paper requires stronger empirical and theoretical results to strengthen the overall contributions. I rate the paper as below the borderline. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}