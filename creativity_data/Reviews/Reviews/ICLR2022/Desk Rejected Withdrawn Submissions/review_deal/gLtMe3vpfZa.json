{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper provides some novel ideas combining neural processes, active learning, and deep sequence models toward accelerating the computationally intensive task of stochastic simulations for epidemic models. The authors incorporate aspects such as spatiotemporal dependence and age structure in the models they consider, and propose an active learning framework to leverage a neural process that can serve as a proxy for direct simulation of the stochastic dynamics. Most of the reviewers agreed that some of these ideas are novel, but the assessments together present a borderline case, and one reviewer remains convinced that the paper needs refinement and a more focused exposition to make clear the contributions and relative efficacy compared to existing state of the art. I tend to agree with some of the concerns raised by that reviewer, who responded favorably to the author response but remained not fully convinced.I also agree that important parts of the paper feel rushed, if I am to interpret \"feeling rushed\" as a statement on the ideas being spread thin in exposition due to the amount of ground the authors try to cover (rather than a statement on the quality of the presentation). There are many moving parts that combine good intuitions toward accelerating simulation-based methods for stochastic epidemic models. Several reviewers mentioned improving the empirical comparisons with recent existing methods, including likelihood-based methods, and though I agree with the authors too that it is not possible to exhaustively explore outcomes in stochastic process approaches, the largely empirically supported contributions can be better motivated with a more complete comparison and streamlined exposition. I encourage the authors to continue to revise and refine this manuscript to maximize the potential behind the ideas they propose here."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This study proposes a new method for learning surrogate models of stochastic simulators. The new method, Interactive Neural Process (INP), builds on Neural processes and leverages the spatiotemporal structure of the problems at hand to reduce the complexity of the inference task. On a few problems in epidemiology -- a low dimensional SEIR problem (2 parameters, 100-dimensional output), and a complex spatiotemporal LEAM-US problem --, the approach is shown to appropriately learn the surrogate in few iterations.",
            "main_review": "Some previous work is cited and discussed, and it is clear how this work differs from these previous contributions. However, I believe it would be important to revise the section on related work. In particular: \n\n-the authors write \"Kleinegesse & Gutmann (2020) [...] require a explicit likelihood model, conditional independence in experiments, and are limited to low (1-2) dimensional design\". I would urge the authors to clarify this statement. In particular, Kleinegesse & Gutmann (2020) deal with simulators for which no likelihood is available (also called \"implicit models\"), which is contrary to what is stated in this manuscript;\n\n-related to the point above, there is a large body of work in machine learning on surrogate models for stochastic simulators (e.g. Meeds and Welling 2014 \"GPS-ABC: Gaussian Process Surrogate Approximate Bayesian Computation\"; Gutmann and Corander 2016 \"Bayesian Optimization for Likelihood-Free Inference of Simulator-Based Statistical Models\"; Lueckmann et al. 2018 \"Likelihood-free inference with emulator networks\"; Järvenpää et al. 2019 \"Efficient acquisition rules for model-based approximate bayesian computation\"; Papamakarios et al. 2019 \"Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows\"), some of which use active learning schemes to adaptively choose model parameters for acquiring new simulations (e.g. Meeds and Welling 2014; Gutmann and Corander 2016; Lueckmann et al. 2018; Järvenpää et al. 2019). It would be important to appropriately discuss this work.\n\n\n\nThe paper is technically sound, and claims are for the most part well supported by empirical evaluation. However, I have a concern I would appreciate the authors to address:\n\n-I believe the study would strengthen substantially if the method would be compared both in terms of inference quality and computational efficiency with other state-of-the-art likelihood-free inference methods, such as the ones based on neural density estimation (e.g. Papamakarios et al. 2019, Lueckmann et al. 2018). Indeed, if the simulators are not very expensive computationally, then maybe neural density approaches for learning the likelihood (Papamakarios et al. 2019; Lueckmann et al. 2018), which tend to need a large(r) number of simulations, will be competitive with the proposed approach in learning a surrogate of the simulator.\n\n\n\nThe paper is generally clear, and I would just like to point a few typos and imprecisions:\n\n-capital letter missing in \"we prove the equivalence between\";\n\n-in \"The total number of dimension\", it should be \"dimensions\". There are a few other instances of \"dimension\" where this should also be corrected;\n\n-in \"which is scales linearly in computation\", it should be \"which scales linearly in computation\";\n\n-\"where it first randomly selects a set of groups and then use(s)\";\n\n-one can guess the dimensionality of the problem for the SEIR problem (if I am not mistaken, 2 parameters and 100-dimensional output). However, it would be important to explicitly state the dimensionality of the complex spatiotemporal LEAM-US problem (number of parameters of the simulator and dimensionality of the output of the simulator).",
            "summary_of_the_review": "Overall, the paper is technically sound and has a novel methodological contribution with convincing empirical results. This will inspire further method development. Some of the discussion of previous work needs to be revised and augmented, and it would be good to provide empirical comparisons with state-of-the-art simulation-based inference methods (in particular, ones that learn the likelihood function).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper tackles learning a flexible distribution approximator to approximate the output of a high-dimensional and computationally intensive stochastic simulator.  The contributions of this paper then reduce into three main components: \n\n1 - Definition of a spatiotemporal neural process that can succinctly model a more richly structured latent process.\n\n2 - Definition of a new acquisition function and using a neural process in an active learning setting.\n\n3 - Application to epidemiological simulators to “compile” estimation.\n\nThe method appears to work, validated on a toy-ish SEIR model, and a more sophisticated pre-existing epidemiological model.  \n",
            "main_review": "### Summary++:\n\nThis is a rangy paper that presents a lot of material.  The primary application domain the authors have in mind (expediting estimation in epidemiological simulators) is well defined and has great utility to the field.  \n\nHowever, the paper itself is cluttered/jumbled, and feels very rushed.  The clarity of communication is generally well below what I would expect for publication.  There are also a lot of separate ideas in this paper that are intermingled.  In reality, I think there is probably two and a half papers worth of material (if it were to be properly and fully explored and evaluated) that is only very briefly touched on here.  As a result, it feels like a bit of a jumbled mess with a lot of content that isn’t very well explained or analysed.  It is difficult to tease apart whether the performance gains come from the STNP, from the active learning, from the acquisition function, or simply as a result of hyperparameter tuning or particular applicability to this domain.  Beyond this, I feel like the lack of analysis of the fail-cases of this method leave it vulnerable to expected failures when deployed.  \n\nAs a machine learning paper, these components are insufficiently empirically analysed and contrasted to other SotA methods.  On the more application-specific flipside, if you are suggesting this as a method for expediting analysis of epidemiological models, then the implications and downstream utility of this method are under-explored to be a viable applications paper.  \n\n\n### Major Comments:\n\n1 - As intimated above, I think there are too many disparate elements in this paper.  Crucially, these elements are untested independently.  I would like to see ablation experiments on simpler and more widely studied problems to help understand the limitations of each component (i.e. apply STNP in isolation on some meteorological data; i.e. Vaughan et al., [2021]).  The epidemiological traces (especially in SEIR) are quite simple, and so I am not convinced that the STNP has been sufficiently validated.  The ablation studies in Figure 4 are very task-specific and I have very little intuition for how well the active component is _actually_ working.  \n\n2 - Beyond this, I think at some point in this paper the core message gets lost a little bit.  At its core, you are making a function approximator mapping from parameters to outcomes in an expensive simulator.  That is the “acceleration” discussed, and indeed the utility of the method -- being able to remove an expensive simulator from the loop.  Ultimately, it is the performance of the learned function approximator that is paramount.  The specifics of how you learn this approximator are somewhat tangential to this.  You could just learn a neural network mapping from inputs to outputs.  There is therefore, in my opinion, too little analysis of how accurate the predictions are and the limitations of this approach, especially given that the examples are quite simple.  On more complex simulators, does the NP just fail to capture complexity or become too difficult to learn?  At which point the predictions become unreliable, even spurious.  There is no evaluation or discussion of this, which I find troubling, as there is no mention that the method could silently fail and produce poor predictions (or worse, asymptotically biased predictions, unlike in, i.e., Baydin et al., [2019]).  \n\n2.1 - The active learning element gets far too much attention.  We sort-of-know that well-tuned active learning methodologies outperform non-active methods in terms of sample complexity, especially where the “true” model is within the support of the model being studied.  \n\n2.2 - I would also like some comparison of runtimes.  How long does the NP take to train?  How long does inference in the NP take?  How long does the optimisation in the active learning part take?  How long does running the simulator itself take?  These are important quantities to know if the main selling point is “accelerating”. \n\n3 - Following on from this, I find the “interactive” part of the title challenging (and further evidence that this paper is a bit jumbled, both on paper and in the mind of the authors).  There is nothing interactive about the neural process -- it is just a (novelly structured, yes) neural process.  The training method has a degree of interaction, but that is not to do with the neural process.  Just as many models can be learned _using_ interactive methods, that does not make each of those models interactive.  \n\n4 - The theory section is just dropped in with no real justification or use.  I feel the authors have just dropped this in to make the paper feel more legitimate, while adding very little to the paper.  \n \n\n### Minor comments:\n\nA - You should also reference these very relevant papers: \n- _Deep probabilistic surrogate networks for universal simulator approximation_ [Munk et al., 2019].\n\n- _Likelihood-free inference with emulator networks_ [Lueckmann et al., 2019].\n\n- _Planning as inference in epidemiological models_ [Wood et al., 2020].\n\n- _Simulation-Based Inference for Global Health Decisions_ [de Witt et al., 2020].\n\n- _Etalumis: Bringing Probabilistic Programming to Scientific Simulators at Scale_ [Baydin et al., 2019].\n\n- _Convolutional conditional neural processes for local climate downscaling_ [Vaughan et al., 2021].\n\n- I would also like to see some (qualitative) comparison to work like _Semi-supervised Sequential Generative Models_ [Teng et al., 2020]; or more generally the wealth of VRNN work (see Chung et al., [2015] and follow-ups) for alternatives for compiled time-series modelling.  \n\nB - Only capitalise proper nouns: i.e. “Interactive Neural Processes” -> “interactive neural processes”.\n\nC - The text on the figures is _tiny_.\n\nD - The last two sentences of the paper really concern me: “We plan to leverage Monte Carlo sampling and Bayesian optimization techniques to parameterize the reward function. Then we would also be able to directly optimize for the target parameters with auto-differentiation.”  These sentences make such little sense to me, that they make me lose a little bit of confidence in what is written in the rest of the document. \n\nE - The first two paragraphs of Section 3 are incomprehensible.\n",
            "summary_of_the_review": "This work is interesting, but is ultimately not ready for publication yet.  The raw quality of the manuscript is well below publication quality.  The components are presented in a bit of a jumbled way.  The methods presented seem to work, but it is impossible to identify where the performance gains come from, and the wider applicability of this method, or each of the components in isolation.  I implore the authors to tease apart the different threads presented in this work and explore them more thoroughly, completely and independently across several intermediate papers*.  \n\nGood luck!   \n\n(*and _then_ you can write a huge journal paper combining them all and get a big, sexy Nature paper!!)\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors proposed a novel active learning framework integrated with the neural process. The neural process is used to mimic the simulator dynamics which is later used for Bayesian active learning. They also proposed a new acquisition function utilizing the latent from the neural process. The experimental results show that INP works better to accelerate stochastic simulation than GP, and the proposed LIG leads to faster convergence compared with alternatives. \n",
            "main_review": "I think the model is novel with a spatial and temporal extension of NP, an NP-enabled active learning mechanism, and a new latent-based acquisition function. \n\nThere needs some significant improvement in the illustration of the model:\n1. Figure 2 is confusing. Some elements in the figure are not defined in the text, e.g., h_t, H_t. Basically, the graphs in all these figures don't have their matching text. The spatial component is not shown in the third column, no A. Basically, I got lost reading the modeling part and the figure 2. \n2. z_{1:T} is used to capture the dynamics, but how? Is there any dynamics relationship used to define z_{1:T}?\n3. In the first paragraph of 3.1, what is \"future states\"? Not defined. If they refer to \"{\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_T}\", there is no \"future\". \n4. The definition of context {theta, x} and target {theta, x} is unclear. \"Augmented parameter theta\" is mentioned a couple of times. Is it target parameter? What is \"augmented\"?\n5. In the ablation study, it's claimed that the temporal latent process is the key innovation. But SNP also has the latent dynamics z. So based on the analysis, the spatial part should be more significant. Unless the temporal component is encoded in different ways, which might require a bit more extensive ablation studies to analyze. \n\nMinor:\n1. In the second paragraph of 4.1, R is not defined. \n2. In page 9, \"which is scales linearly in computation\" -> \"which scales linearly in computation\"",
            "summary_of_the_review": "I think the model is novel and useful for disease forecasting. But I think the paper (especially the methodology part) is poorly written. It doesn't reach the bar for publication. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The manuscript entitled, \"Accelerating Stochastic Simulation with Interactive Neural Processes\", presents a novel approach to the problem of statistical emulation for mechanistic models of epidemic disease transmission.  To this end, structured neural processes are developed to exploit and respect the temporal and spatio-temporal character of these models.  An active learning strategy is developed to train these neural processes to minimise the computational costs of generating training instances of disease simulator outputs.  The methodology developed is applied to two SEIR compartmental model examples: a minimal one with a single homogenous population and a maximal one with many age and space delimited cohorts.",
            "main_review": "A strength of this paper is that the method is developed carefully with regard to implementation and calibration strategy (i.e., design of the model architecture, identification of objective function and construction of a suitable acquisition function).  These choices are supported by appropriate theoretical analyses of bounds and convergence rates.  Considering the high benchmark for papers submitted to ICLR, it is my opinion that (while highly commendable) reaching this level of quality is only a necessary but not sufficient condition to strongly recommend a paper for the conference.  The additional condition that must be met for a strong recommendation is that either the method itself is highly novel across other competing machine learning methods or the contribution to solving a problem in the applied area is highly substantive.  In this case I believe the paper is presenting an argument for itself in the latter category, but I personally do not see its merits here as I will explain below.\n\nMechanistic models of disease transmission can be incredibly computationally expensive to run with high overheads for both RAM and cpu time.  Even in the best case scenario that many of the parameters can be fixed leaving only one or a few free parameters to be learned against data, the calibration problem is computationally expensive.  In many cases an additional problem will be the lack of a tractable likelihood function and/or lack of control access to latent system random variables, such that the simulator needs to be treated as a black box.  Once calibrated, the analyst will typical want to generate model outputs under a range of 'tweaked' parameterisations representing scenarios such as possible interventions or future trajectories under a range of possible changes to human behaviour parameters or so on.\n\nMachine learning methods have been used to speed up both these aspects of disease modelling in the past and broadly speaking all have followed the approach of Bayesian optimisation.  Namely, iteratively learning a mapping between the model parameter space and a univariate statistic being either the goodness-of-fit to observed data (Andrianakis et al., PLoS Comp Bio, 2015; Reiker et al., 2021 preprint: https://doi.org/10.1101/2021.01.27.21250484 ) or a utility function for policy impact (Bent et al. 2017: https://arxiv.org/abs/1712.00428 ).  Rarely has the problem been tackled from the perspective of building an emulator as a surrogate for the entire mechanistic simulator itself (though see for example Cameron et al. Nat Comms 2015 in which a functional regression model is used to predict new simulator outputs from a training library).  The reason this latter approach is less explored is because the general input to output space is vastly larger than the sub-domain required for calibration to a specific dataset or specific set of policy objectives.  If the application of the surrogate model is to address one of these problems then the Bayesian optimisation methods focussed on that single objective will inevitably out-perform a general surrogate model.\n\nSince the method proposed here falls into the latter category of producing a general surrogate I am not convinced that the applications to which it has been explored here are sufficient to demonstrate its utility.  Application areas in which a general surrogate model might yield a particular advantage could be e.g. in transfer learning scenarios in which the calibration exercise must be performed over many different observation datasets, or perhaps as an intermediate model for a multi-resolution Bayesian optimisation (Kandasamy et al. ICML 2017).",
            "summary_of_the_review": "In brief, my opinion is that, while the methods are well developed, the utility of this method for solving problems in the application domain is not sufficient for a strong recommendation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}