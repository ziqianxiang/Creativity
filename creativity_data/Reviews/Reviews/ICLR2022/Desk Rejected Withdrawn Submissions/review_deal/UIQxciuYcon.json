{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an *integrated framework* with a suite of contrastive learning (CL) losses that are inspired by the idea of Neighborhood Component Analysis (NCA). This framework combines various prior ideas like debiased CL, hard negative sampling, mixup in augmentations, adversarial samples, and the paper makes extensive evaluations of various combinations of these objectives on CIFAR-10 and CIFAR-100 objectives. The paper also makes a case for evaluating such methods not just on the clean accuracy on downstream benchmarks, but also on robustness to adversarial examples and also transfer performance. The results in this paper can provide a useful benchmark for future work.",
            "main_review": "**Strengths**\n\nGiven the extensive experimental evaluations, the results in this paper can provide a useful benchmarks for future work on contrastive learning and representation learning in general. To my knowledge, some results in this paper push the boundaries of downstream performance on CIFAR-100 benchmark. The advocacy of evaluating robustness and transfer performance of learned representations also seems like a good idea. The idea of viewing contrastive learning objectives through the lens of NCA can potentially be useful to develop new algorithms, as alluded to in the conclusion section.\n\n\n\n**Weaknesses**\n\nI found the role and significance of NCA to be weaker than how it is presented in the paper, perhaps some more convincing is needed in this regard. The main part where NCA is really used is the proposed objective function $L_{Bias}$. This objective seems to perform worse than the simpler variance reduction strategy $L_{Var}$ (that takes an average loss of $M>1$ views per image instead of just 1 view), as evident from Table 1 and also stated in the paper on page 8. Although $L_{Bias}$ with $M>1$ does provide some benefit over the baseline of $L_{SimCLR}$, if a simpler strategy of $L_{Var}$ (that does not need the NCA framework) does better than $L_{Bias}$, then it is not clear how important NCA really is. In fact for the case of $M>1$, if one treats the total number of views per image throughout training as a fixed budget, it is not clear whether the best strategy is to use $L_{Var}$, $L_{Bias}$ or vanilla $L_{SimCLR}$ run for $M$ times more iterations. From Table 1 and Table S5, I am inclined to conclude that increasing iterations is the best strategy; in case there is also a wall-clock budget, perhaps $L_{Var}$ is the winner. Regardless, all of this seems to dilute the idea that the NCA framework is a strong motivation for the successful methods in this paper.\n\nThe other proposed objective $L_{Mixup}$ seems only weakly inspired by the NCA framework, and the connection seems a bit artificial on first sight; more on this below. In the grand scheme of things, this is not a big deal since it is evident from the results in this paper (and some prior work) that incorporating mixup in some form during contrastive learning can improve performance. The main point of contention here is about how significant the idea of NCA is in the successful methods presented in this paper. \n\n*Additional questions/comments* \n\n- It would be good to have a discussion about the trade-offs when it comes to picking different objectives, in relation to runtime and amount of data used (e.g. number of views per image in total)\n\n- For $L_{Mixup}$, why does it make sense to introduce a new class $c_{\\lambda, i}$ and only try to match just the probability of that class? Why does the following assumption make sense? \"Assume the probability $x_i$'s label is $c_{\\lambda, i}$ is $q_{\\lambda,i} = \\lambda + (1 - \\lambda)[c_y = c_i]$\"",
            "summary_of_the_review": "The ideas and results presented in this paper can provide a useful checkpoint and benchmark for future work on contrastive learning and representation learning. The main concern is about the relevance of NCA for the proposed methods as described in the main review.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work tries to reformulate neighbour component analysis with other existing contrastive learning methods. At the same time the authors propose a framework to integrate different losses (like mixup).",
            "main_review": "### Strength\n1. I appreciate the idea of connecting NCA and contrastive learning.\n\n### Weakness\n1. This paper is a little bit difficult to follow because they reformulate simCLR as a function K and a function g. I can understand the necessity of defining such a function g. But I have no idea why the log function is also defined? And that makes me confused about the following loss functions.\n2. In the abstract, the authors said they propose a series of contrastive losses that outperform the existing ones. They did not compare existing methods under the usual setting. (Tiny-imagenet and imagenet)\n3. This framework looks more like a combination of a bunch of existing loss functions rather than a unique framework.\n",
            "summary_of_the_review": "As a reviewer familiar with metric learning and contrastive learning, firstly I had a bit of difficulty reading it, and secondly, I had a hard time getting to what exactly we got from NCA. The final framework seems to me to be a combination of several different loss functions together. Based on these, I tend to reject this paper. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposed several conventional contrastive loss variants based on the idea of neighborhood component analysis (NCA). Independently of the correspondence with NCA, they also provided a framework to unify the methods proposed in the existing contrastive learning (CL) literature. Using the proposed loss variant and unifying framework, they found a combination that showed high performance in both accuracy and adversarial accuracy on the CIFAR-10/100 dataset.",
            "main_review": "## Strength\n- This paper proposed several loss variants for contrastive loss through the lens of NCA. In particular, they provided a way to properly incorporate mixups in the framework of contrastive learning under certain assumptions.\n- This paper provided a framework that allows for the unified use of several techniques that have been proposed to improve contrastive loss so far.\n- This paper empirically evaluated a particular loss function/technique combination that simultaneously achieved high performance in both accuracy and adversarial accuracy on the CIFAR-10/100 dataset.\n\n## Weakness\nDespite the contributions outlined above, I believe all of the following concerns need to be addressed in order for this research to be accepted.\n\n### A) A mismatch between motivation and contribution\nThe contributions provided by this study do not address the motivation as stated in the introduction. In the introduction, the authors raise the problems related to constructing positive and negative pairs in CL as the motivation for this study. Specifically, the following three points are mentioned as challenges in CL:\n1. definition of semantically similar/dissimilar pair is contingent on downstream tasks,\n2. practically data augmentation is used for a positive pair, but it still has a problem; although positive samples are valid, negative samples contain a non-negligible portion of invalid samples (class collision),\n3. and the computation of positive and negative pairs grows quadratically with the size of the dataset.\n\nEven though the authors state that they propose their method to examine these challenges, none of these challenges are addressed in Method/Results/Conclusion. Instead, they continue to use existing approaches that have the issues raised in the introduction. The authors should clearly state their contribution to 1) the definition of similar/dissimilar pairs independent of downstream tasks, 2) validity (especially for class collision) in negative samples, and 3) computation time issues mentioned in the motivation of this paper.\n\n### B) Derivation of the correspondence between NCA loss and contrastive loss\nThe derivation of Eq. 9 from Eq. 7, which is the main technical contribution of this study that leads to the correspondence between NCA loss and contrastive loss, has several concerns. The NCA uses all dataset except its own $\\\\{x_k \\colon k \\neq i\\\\}$ to evaluate the loss for the $i$-th data. This study makes an assumption of $\\\\{x_k \\colon k \\neq i\\\\} = \\\\{x_j^+ \\colon 1 \\le j \\le M\\\\} \\cup \\\\{x_i^- \\colon 1 \\le i \\le N\\\\}$ on this set to derive the contrastive loss from the NCA loss. I have two concerns about the validity of this assumption.\n1. This assumption implies that the entire dataset can be divided into $M$ “positive” samples and $N$ “negative\" samples. However, experimentally the positive/negative samples are subsampled from the entire dataset during the loss evaluation. In fact, in the numerical experiments in the second half of the paper, the authors treat $M$ as a hyperparameter, which is a significantly smaller number than the dataset size, so this assumption does not hold.\n2. The assumption implicitly assumes that positive and negative samples are mutually exclusive: $\\\\{x_j^+\\\\} \\cap \\\\{x_i^-\\\\} = \\varnothing$. However, in reality, negative samples are typically taken from random samples in the same minibatch for computational simplicity, which leads to collision with positive samples. The numerical experiments in this study treat the same, and again the assumptions are not supported also from this perspective.\n\nSince the correspondence between NCA loss and contrastive loss is the main technical contribution of this study, the validity of the assumptions for its derivation directly relates to the validity of the contribution of this study itself. The authors should address the above two concerns and clarify the validity of the assumptions made in this study.\n\n### C) Concerns about experiments\n**C-1)** There is a gap between the technical contribution of this study (proposal of loss variant using NCA) and the experimental contribution (performance improvement including unified use of existing methods). Although this study claims to have given a unified framework for CL from the perspective of NCA and to have shown improved performance in numerical experiments, the loss variants from the perspective of NCA and the unified framework that brings together existing methods and loss variants are mutually independent. The following three effects are thought to contribute to the experimental performance improvement separately:\n1. The effect of NCA-derived loss variants (choice of $\\mathcal{L}_\\mathrm{Na}$),\n2. the effect of combining existing methods (e.g., use of adversarial examples),\n3. and the effect of heuristics with low relevance to the main technical contribution (choice of $w(x)$).\n\nBecause these three effects were examined at once without clearly separating them, it is not clear how large the effect of the loss variant derived through the lens of NCA is. For example, the proposed method $\\mathcal{L}_\\mathrm{IntNaCL}$ has parameters $\\alpha$, choice of $\\mathcal{L}_\\mathrm{Na}$, $M$, $\\lambda$, $g^1$, $g^2$, and $w$. By setting these parameters, loss variants, including existing methods, can be treated in a unified manner. Since there is no description of the parameter settings of the proposed algorithm in Figure 1, it is not clear which of the three effects this performance improvement depends on. The authors should examine each of these three effects independently and separate the contributions in the text.\n\n**C-2)** Concern that the effect of the number of observed samples has not been removed. The authors claimed the contribution of $\\mathcal{L}_\\mathrm{VAR}$ when $M>1$ as the effect of NCA-derived loss variant. The authors stated that the $\\mathcal{L}_\\mathrm{VAR}$ has the property of being able to lower the variance of the estimator. However, it is not fair to naively compare the contrastive loss corresponding to $M=1$ and the $\\mathcal{L}_\\mathrm{VAR}$ for $M>1$ in the same fixed epoch, because experimentally, the model can observe different positive samples in the amount proportional to $M$. Suppose one would like to highlight the effect of the difference in the functional form of the losses purely. In that case, the authors should use long epochs where the optimization converges sufficiently or make adjustments so that the number of positive sample types observed does not change for different $M$ for a fair comparison. In fact, the authors have also trained the existing method with a longer epoch, and the results show that the existing method is comparable to the proposed method when trained with a longer epoch.\n\n## Minor comments\n- $x^\\mathrm{adv}$ in Eq. 6 is not defined explicitly.\n- “$g = g_0$” in the left-hand side for the definition of $\\mathcal{L}_\\mathrm{VAR}$, $\\mathcal{L}_\\mathrm{BIAS}$, and $\\mathcal{L}_\\mathrm{MIXUP}$ (p.6) is not clear since “$g$” does not appear on the right-hand side of these equations.\n- The undefined variable $x_{i_2 j}^-$ is in the right-hand side of the definition of $\\mathcal{L}_\\mathrm{MIXUP}$ in p.6.\n",
            "summary_of_the_review": "Although this study demonstrates an interesting connection between NCA and contrastive learning and proposes some loss variants, its derivation remains several concerns (B). As an empirical study, it is valuable to focus on the internal representation’s robustness and experimentally showed that a high-quality representation can be obtained by combining existing research and this study. However, this study is not comprehensive enough as an empirical study since it is unclear which factors contributed to the improvement (C). Also, most importantly, the proposed method does not address the problem that motivated this study (A). Despite that the introduction claims to address the problem of positive/negative sample construction in contrastive learning, this study only evaluates the performance of the proposed algorithm in terms of accuracy and robustness. Based on these reasons, I recommend rejecting the paper but am willing to increase the score if concerns A, B, and C are addressed.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper examines contrastive learning through the lens of neighborhood component analysis (NCA), and introduces Neighborhood analysis Contrastive Loss (NaCL) and integrated contrastive learning (IntCL/IntNaCl) as a broad framework for integrating existing approaches. They show that NaCl and its variants outperform existing baselines and also demonstrate improved adversarial robustness on CIFAR10 and CIFAR100.",
            "main_review": "Strengths:\n- I thought it was interesting how the connection to NCA could be leveraged to unify a wide range of existing methods. The overall story of the paper is nice to read, although there are many parts throughout the paper that are confusing and need to be fixed.\n\nWeaknesses:\n- The empirical results on CIFAR10/CIFAR100 are nice, but should be benchmarked on larger datasets (e.g. ImageNet) to prove the actual utility of the method.\n- While overall the paper is nice to read, there are several places throughout the draft that are pretty confusing (especially for the method in Section 4.1).  For example, I wasn’t sure how to interpret g^1 and g^2 in Eq. 10 -- can we just select any 2 g’s from {g_0, g_1, g_2}? Why does L_IntNaCl take as arguments additional loss functions? What is w(x) (is it just some generic weighting function)? Is w_hat(x) in Section 4.2 supposed to be the empirical estimate of w(x)?\n- There are a lot of different losses/expressions that are hard to keep track of throughout the paper -- it might be a good idea to collect them in a table somewhere so that it’s easier for the reader to keep track of them.\n- Technical exposition and notation can be cleaned up. For example, 1/t in the g_1 estimator right after Eq. (4) is not introduced, and there is an error in that expression (e.g. as defined in the paper, \\tau_+ should actually indicate the probability of a positive pair and 1/(1-\\tau_+) should be 1/(1+tau_-), where \\tau_- is what the paper defines as \\tau_+). I believe the same mistake holds in Eq. 5\n- Section 3 should be cleaned up, as it’s pretty hard to understand. While it is true that kNN selection is non-differentiable, the reason why NCA is non-differentiable is because it relies on the leave-one-out classification error (also, what we care about in NCA is differentiability with respect to the transformation A, not the inputs). Additionally, the transformation matrix A which maps x_i to the feature space in NCA is undefined in Section 3.1. The metric is also incorrect (it should be A^TA for NCA, not Ax which is just the transformed input). Also for the statement “The probability x_i’s label is c_i is given as q_i,” did the authors mean “the probability of x_j’s label…”? Since in NCA, x_j directly inherits the label c_i if it x_i is its selected neighbor. This section would greatly benefit from another pass.\n\nA subset of miscellaneous/minor comments and typos:\n- Couple awkward phrases (e.g. “presented representations”) that need to be fixed throughout the paper\n- “as [belonging to its] own class” in Section 2\n- “Then q_lambda,i should match the probability…” in Section 3.2\n- “...can be [chosen] from…” in Section 4.1\n\n",
            "summary_of_the_review": "The paper presents a nice unifying framework for a variety of existing approaches. However, it requires quite a bit of reorganization in terms of notation/collecting terms (it’s pretty hard to keep track of everything and there are quite a lot of errors) and could benefit from additional experiments on larger datasets (e.g. ImageNet).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper uses neighborhood component analysis (NCA) for understanding contrastive learning. From this connection, proposes an integrated framework that optimizes both standard and robust contrastive learning objectives. The proposed framework shows consistent improvements in CIFAR experiments in terms of both standard and robust accuracies.\n",
            "main_review": "### Strengths\n\nS1. This paper explains various contrastive learning objectives using one unified framework, neighborhood component analysis (NCA). \\\nS2. The proposed method shows consistent improvements in the tested scenarios.\n\n### Weaknesses\n\nW1. Unclear benefits of NCA\n- I agree that NCA is related to contrastive learning. However, I am not convinced that the NCA framework has an advantage when understanding contrastive learning. I think the challenges (that comes from the limited numbers of positive and negative pairs) have been known in the contrastive learning literature. How is the NCA framework helpful in solving the challenges? Furthermore, how can it help improve robustness?\n- Moreover, all the objectives are not new and can be obtained by minor modifications. To be specific, I think $L_\\text{VAR}$ and $L_\\text{BIAS}$ can be derived by just sampling more positive samples from the existing SimCLR objective. Also, $L_\\text{MIXUP}$ and $L_\\text{Robust}$ were already proposed. (I think that $L_\\text{MIXUP}$ and [1,2] have the same idea/concept, so the difference is a minor modification.) The final objective $L_\\text{IntNaCl}$ is the summation of existing objectives, so I cannot find novel/interesting parts of the proposed method.\n\nW2. Weak experiments.\n- The models are trained under too small-scale setups; for example, they are trained on CIFAR100 with a small batch size (i.e., 256). Under this setup, the experiments cannot verify the effectiveness of the proposed method because the gains could be marginal when the scale becomes larger. More extensive experiments, including large-scale datasets (e.g., ImageNet) and a larger batch size (e.g., 2048), should be conducted to verify the effectiveness.\n- Why Mixup degrades the performance in SimCLR? This result is somewhat suspicious because Mixup usually improves SimCLR's performance, as shown [1, 2].\n- Why M=2 is the best in the Debiased+HardNeg + Mixup setup?\n- This paper compares the proposed methods with a few baselines while many contrastive learning methods outperform SimCLR, for example, NNCLR [3].\n- I am not sure that the experimental setup (e.g., Adam with no cosine learning schedule) is the standard. I recommend following the standard setup as suggested in the original SimCLR paper.\n- What are the pros and cons of the proposed objectives, $L_\\text{VAR}$, $L_\\text{BIAS}$, and $L_\\text{MIXUP}$?\n\nW3. In overall, this paper is hard to read.\n- What is the optimization objective of NCA?\n- The definition of $q_i$ is somewhat awkward. If it is just constant 1, why is it required?\n- What is the support of $p_i$ and $q_i$? The authors stated $D_{KL}(q\\Vert p)=\\sum_i -q_i\\log\\frac{p_i}{q_i}$. This means, the authors consider $p_i$ (or $q_i$) as the probability of selecting the index $i$ from the set of all indices $\\{1,\\ldots,N\\}$. If it is right, $\\sum_i p_i$ and $\\sum_i q_i$ should be 1, but they are not equal to 1.\n- I suggest using mathematical notations more concretely and rigorously. For example, use $P(j|i)$ instead of $p(x_i,x_j)$ because the latter term can be misunderstood as a joint probability on the data distribution. I think that is the conditional distribution on the index set.\n- The \"Related Work\" section uses too much space and is hard to distinguish whether the part is essential or not. I recommend separating this section into \"Related Work\" and \"Preliminaries\" (or \"Background\") sections. Then, the former explains related works at a high level, but the latter contains only essential parts.\n\nMinor comments\n- BYOL (Grill et al., 2020) is not a contrastive method. Contrastive methods should use \"negative\" pairs. The citation in the first paragraph should be removed.\n\n[1] i-Mix: A Domain-Agnostic Strategy for Contrastive Representation Learning, ICLR 2021 \\\n[2] Towards Domain-Agnostic Contrastive Learning, ICML 2021 \\\n[3] With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations, ICCV 2021\n",
            "summary_of_the_review": "Although the concept of NCA is somewhat interesting, I cannot find the benefits of NCA for understanding/improving contrastive learning. Also, I feel the lack of novelty of the proposed methods. Furthermore, the experiments are conducted on only small-scale datasets, and I think the writing is not well-organized. Hence, I recommend rejecting this paper.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}