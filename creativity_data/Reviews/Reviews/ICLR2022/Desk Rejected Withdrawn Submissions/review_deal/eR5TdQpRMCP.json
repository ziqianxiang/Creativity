{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a more realistic incremental setting including the intra-class domain shift problem, compared to typical settings of CIL. For the bi-level nature of this new setting, it proposes a bi-level method to explicitly model the generation process of class and domain labels. Domain labels are not accessible, so it uses an EM algorithm to optimize the domain prediction part. Additional component reduction operation is used to reduce the model redundancy.",
            "main_review": "1. More details about “expansion-and-reduction” and “bi-level memory” should be provided in the method section. In my understanding, there are three important components in the proposed method: “mixture model”, “expansion-and-reduction” and “bi-level memory”. In the method section, the authors mainly introduced the “mixture model”, and used a few sentences for the other two components. I think how to do “expansion-and-reduction” is a very important question in this paper, while I cannot find enough details. \n\n2. The design of the component reduction step (described in section 3.3) and L_reg (described by eq 9) is paradoxical, since the former wants the components to be heterogeneous with each other (large linkage distances), while the latter wants the components to be closed with each other (small inner-product distance).\n\n3. The method requires additional computational costs in the E-step. If my understanding is correct, it needs to forward all the training images through the encoder during the E-step. It requires similar computational costs compared to the M-step. So it is not fair to directly compare the method with the baselines. The authors need to include the computational costs in the main table and analyze how much additional computational costs the method requires. \n\n4. In Table 1, there is no comparison to iCaRL [A]. I think it is worth doing this comparison because of the following reasons. (1) In our experience, if we observe the same number of classes in each session, the performance of iCaRL should be better than PODNet and UCIR in the NC setting. (2) iCaRL also aims for representation learning, and it is a common baseline for many class-incremental learning methods. In addition, it is not intuitive to me why Replay overperforms PODNet in settings of <iCIFAR-20, NC> and <iDigits, NC>? More details of baseline implementation are needed, I think. It is also helpful if there is a detailed comparison of the memory budget used in those methods.\n\n5. It would be helpful to provide the test results on non-uniform distributions. In Section 3.2, it is said that “the class distribution P (y|t) and the class-specific domain distribution P (z|x, t) takes the uniform distribution during the test.” However, in many real-world applications, the class-specific domain distribution P (z|x, t) is not the uniform distribution. For example, we often observe cars in the city, but we rarely see cars in the countryside. Thus it is important to evaluate the method on non-uniform distributions to validate the robustness of your method.\n\n6. In terms of the idea of including domain shift in incremental learning, this is not of high novelty. While it is interesting to see the method of automated learning of domain labels using the EM algorithm. I am wondering how far is this learning results from the case when the ground truth domain labels are given.\n\n[A] Rebuffi, Sylvestre-Alvise, et al. \"icarl: Incremental classifier and representation learning.\" Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 2017.\n",
            "summary_of_the_review": "I have a few concerns about the clarity of the method, the unfairness of the comparison to baselines, and the missing of some important comparisons to related methods.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper discussed an incremental learning scenario where both the class distribution and class-specific domain distribution vary across learning sessions. To deal with the case, they focused on the intra-class domain re-balancing via a domain-aware learning framework: in each learning stage, it constructed the intra-class structure by clustering image features to several components for each class under the current domain, and expanded the structure with new clusters when new classes come; the distillation and replay strategy are also designed for maintaining the structure. For experiments, they applied their method to CIFAR-100, DomainNet and a fusion dataset for digit recognition to simulate different scenarios in which the class and domain distributions shift and get consistent improvement. ",
            "main_review": "Strength:\n1. Distinguished from conventional IL works, this paper specifically discussed the intra-class structure maintained in IL and addressed the intra-class domain imbalance, which is necessary when the network needs to continuously learn new domains. \n2. The proposed domain-aware framework learns to cluster the images in each domain so that the resulting sub-class clusters can represent the intra-class structure intuitively. Besides, the bi-level balanced memory helps to select replay data equally from each cluster, which maintains the balance within each domain.\n3. The method shows improvement under the proposed New Domain setting without hurting the performance of the traditional New Class setting.\n\nWeakness:\n1.\tThe choice of von Mises-Fisher(vMF) distribution is too empirical and lacks explanation nor ablation. As the final loss function in equation (6) can be implemented with many options and even traditional cross-entropy, I found the use of vMF not so intuitive.\n2.\tIn my opinion, more details and analysis of the experiments need to be given. For example, the NCD setting is not clearly stated, the protocols of the domain sequence like MNIST->SVHN->SYN is not given, the ablation of the components number m is lacked, and experiments on more challenging incremental learning dataset like ImageNet in UCIR/DER are expected to be shown. Besides, as in Table 1, the performance of NC/ND/NCD is not consistent as it seems that ND is easy for CIFAR100 but hard for DomainNet and Digit Recognition, which deserves more discussion. \n3.\tThe P(z=k) and P(z=l) in equation (4) should be conditioned on y.\n4.\tMore related works on domain generalization would be better as this is not a traditional IL work.\n\n",
            "summary_of_the_review": "I recognize the value of the framework and studies made by this paper. I would be more than willing to accept the paper once my concerns are properly addressed.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n.a.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a more general incremental learning problem in which both the class distribution and class-specific\ndomain distribution vary over sequential sessions. This paper develops a von Mises-Fisher mixture model to capture the intra-class structure while introduces a bi-level balanced memory to deal with data imbalances within and between classes. Experiments show that the proposed model outperforms SOTA methods on three benchmarks datasets.",
            "main_review": "This paper has the following limitations:\n1. The problem proposed by this paper is similar to the class-incremental domain adaptation [1], which has been researched over many years. What is the difference between them? I cannot agree with the authors' claim \"We formulate a novel general offline incremental learning problem\".  \n2. This paper should consider SOTA class-incremental domain adaptation methods [1][2] to validate the effectiveness on benchmark datasets.\n3.  The memory selection strategy can be applied to other SOTA incremental learning methods to validate its effectiveness. It could be better for other SOTA incremental learning methods to achieve performance improvements via this strategy.\n4. The complexity of the proposed model is a bit high. Please add the comparison experiments about the training time and testing time.\n5. For the experimental settings, why do you sample 100 classes from 345 categories of common objects in iDomainNet dataset. Why not conduct experiments on original dataset. \n\n[1] Jogendra Nath Kundu et al., Class-Incremental Domain Adaptation, ECCV 2020.\n[2] Mengya Xu et al., Class-Incremental Domain Adaptation with Smoothing and Calibration for Surgical Report Generation, 2021",
            "summary_of_the_review": "Overall, this paper is marginally below the acceptance threshold of ICLR. If the authors address my concerns, I will consider improve my score in the post-rebuttal discussion.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes the general incremental learning problems, where both class distribution and intra-class domain distribution continuously change. In addition,  a vMF mixture model is proposed to obtain the domain-aware representation, and a bi-level balanced memory strategy is introduced to mitigate the imbalance problem.",
            "main_review": "Strengths:\n1, This paper proposes a novel incremental problem.\n2, The writing of this paper is clear.\n\nWeakness:\n1, The experiments are not sufficient to prove the effectiveness of the proposed methods. Many recent methods are missed in this paper, such as [1][2][3].\n\n[1]Semantic Drift Compensation for Class-Incremental Learning.\n\n[2]Prototype Augmentation and Self-Supervision for Incremental Learning.\n\n[3] Efficient Feature Transformations for Discriminative and Generative Continual Learning.\n\n2, I want to see more evaluation metrics, such as average forgetting.\n\n3, Can the proposed method be applied with incremental methods without memory?\n\n4, Do the numbers of instances from different domains are same? I cannot see the related description. I think this should be analyzed in the experiment parts.\n",
            "summary_of_the_review": "This paper proposes a novel incremental setting, which is more suitable for real-world situations. The writing of this paper is clear and easy to understand. In addition, the experiments cannot prove the effectiveness of this paper, which is described in the weakness.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}