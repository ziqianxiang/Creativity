{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "< Summary > \n\nMany machine learning models depend on empirical risk minimization (ERM), vicinity risk minimization (VRM), and ERM with data augmentation. Original ERM and VRM show a good performance in many tasks, but they fail to generalize the out-of-distribution dataset. This paper proposes a new regularization method, data augmented invariant regularization (DAIR) methods. DAIR is a kind of regularizer that penalizes loss inconsistency. This paper validates the superiority of DAIR on drug discovery, visual question answering, and CIFAR-10\n",
            "main_review": "< Strength >\n\nThe proposed method, DAIR is simple, and it is easy to use.\n\nThere were many works for feature inconsistency, but DAIR suggests the importance of loss inconsistency. It is one of the novelty points of this paper\n\nThe proposed method is model-agnostic, and it can be widely utilized in many models and tasks. Actually, DAIR is validated in diverse tasks\n\n< Weakness and Questions >\n\nThe importance of loss inconsistency is not well described. What is the advantage of loss inconsistency compared with feature inconsistency loss in self-supervised learning?\n\nIn 3p, there are SQ regularize, square root of loss inconsistency, but the intuition of square is not well discussed.\n\nIn section 3.4, this paper adopts feature regularization, instead of loss inconsistency. It is hard to understand the reason.\n\nFor section 3.4, What is the advantage of DAIR compared with [1].\n\nThere are many related works for learning invariant feature representation [2,3,4], and some of the works can be interpreted as loss inconsistency regularization research. What is the advantage of DAIR compared with [2,3,4]?\n\n[1] Sinha, Samarth, and Adji B. Dieng. \"Consistency Regularization for Variational Auto-Encoders.\" arXiv preprint arXiv:2105.14859 (2021). \n[2] Arjovsky, Martin, et al. \"Invariant risk minimization.\" arXiv preprint arXiv:1907.02893 (2019). \n[3] Sagawa, Shiori, et al. \"Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization.\" arXiv preprint arXiv:1911.08731 (2019). \n[4] Krueger, David, et al. \"Out-of-distribution generalization via risk extrapolation (rex).\" International Conference on Machine Learning. PMLR, 2021.",
            "summary_of_the_review": "Please see the weakness and questions section.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose a conceptually simple regularizer which forces consistency of loss between the true samples and their corresponding augmentations, termed DAIR-SQ. The authors demonstrate the qualitative characteristics of DAIR-SQ with a clean toy example, and a supporting theoretical result of asymptotic consistency on the same. For real-world datasets, the authors demonstrate the robustness induced by DAIR-SQ under domain shift, label noise, and adversarial attacks.",
            "main_review": "### Strengths\n\n- The proposed regularizer is conceptually simple and a natural consequence of what we expect from the training.\n- The authors present a very clean and simple toy problem to ease the reader into qualitative characteristics of DAIR-SQ.\n- The authors have made a conscious effort in framing the flow of the paper while writing, which is much appreciated. It is well-written.\n\n### Weaknesses\n\n- While the toy example is well-scoped, I am a little concerned about the setup here, making it unfair for (DA-)ERM. It is a little stretch to consider unregularized ERM, an unreasonably weakened baseline. In practice too, I presume one would typically apply regularization via weight decay. There is no reason for ERM to do anything better because the space of possible functions hasn't been constrained in line with our expectations anyways, effectively causing overfitting.\n- The usual parameter regularization also generally appears to be absent for any description in the paper. Is that just assumed to be obviously a part of the system?\n- Does Figure 2 contradict Proposition 1? Figure 2 says that there is a sweet spot for $\\lambda$? Or is it the fact that we only have a finite sample?\n- What explains the characteristic bump in Figure 2 for DAIR-SQ and some other variants? I think these graphs show there is a setting of $\\lambda$ for which DAIR-SQ would outperform others, but I am still unsure the conditions under which this happens.\n- Perhaps it is implicit in the existing results, but is it possible to visualize whether DAIR-SQ *really* adds invariance? For instance, it seems unreasonable to believe that a \"6\" changing into \"9\" should remain invariant. To that end, I am not sure why Strong Rotations are even meaningful to be checked. It it possible to visualize a sequence of rotations of say \"6\" and see where (DA-)ERM breaks versus DAIR-SQ?\n- For the unsupervised learning setup, I am still unsure why we couldn't add a regularizer for $g$ such that we force $g(f(x;\\theta_1);\\theta_2) \\approx g(f(\\widetilde{x};\\theta_1);\\theta_2)$ via $\\mathcal{R}_2$? (We could potentially freeze $\\theta_1$ in this case since we already have another regularization term that handles that). Could the authors expand on this?\n- I am not sure how much to trust Figure 7 to validate the fact that $\\mathcal{R}_2$ forces the same distributions. There could be more high variance directions just not covered in the first two dimensions. How does the eigenspectrum look like?\n- Would it be correct to say that Proposition 1 remains limited to the very specific 2-D case? Or can the proof be extended to more general versions? If not, then it is unclear to me what to take away from the theoretical justification.\n\n### Minor Comments\n\n- A labeled Figure 1 would make things easier for the reader. The caption is also not informative, perhaps the parameters don't matter for the intuitive story being told here.\n- The authors mentioned the regularizer $\\mathcal{R}$ to be a proper divergence in passing but most of the description of considering unnormalized NLL seems unnecessary since the actual regularizer used does not have an immediate direct likelihood interpretation. Please let me know if I am missing something obvious. (could have been a Gaussian centered at either the sample or its augmented version if not for the square root factor)",
            "summary_of_the_review": "The method obviously seems to work and the authors do a good job of taking a varied set of problems to demonstrate effectiveness. The proposed method is conceptually simple, and therefore most of my concerns lie with the experimental justifications as highlighted in the main review.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies a new learning method that makes a model be invariant under a data augmentation (implicitly, to achieve high test performance when the test data distribution is different from the training data distribution). The authors propose a new objective function, called DAIR, characterized by a regularized function where a regularizer maintains the performance of the model on data augmentations. Some numerical experiments by varying regularizers and datasets are provided (in Sections 2 and 3), and some theoretical results are presented. The authors claim that DAIR consistently outperforms the baseline methods including ERM and DA-ERM.",
            "main_review": "Main comments:\n- The form of a new regularization function in DAIR is reasonable but requires prior knowledge of data augmentation functions. This can be a critical drawback of this method, especially when a distribution of augmented data is quite different from a test data distribution. As far as the reviewer understands, most of the theoretical and experimental results presented in this paper implicitly assume that data augmentation functions are designed to cover the test data distribution or the like. For instance, in the rotated MNIST experiment in Section 2.2, ERM (without data augmentation) is compared with DAIR (with weak rotation) when the test set consists of 'strong rotation' samples. In that sense, the good performance of DAIR is not surprising. \n- In applications, it might be challenging (or even impossible) to consider a 'good' data augmentation technique for future test datasets. Maybe the authors need to perform sensitivity analyses to see the effect of misspecification of data augmentations on test performance. For example, what if 'weak rotation samples' are augmented but the test dataset consists of location-shifted images? \n\nTheoretical results:\n- All the results are too restrictive. Proposition 1 considers a very specific setup (a linear model $y=x+\\epsilon$ and a data augmentation $A(z)=(x, ay+n)$). I do not think the theoretical result is wrong, but it is difficult to imagine an extension to more general settings (e.g. non-linear models or different augmentations). Also, the current result considers the infinity lambda case, which is basically designed for the test distribution. \n- Lemma 1 is rather straightforward and only compares DAIR-SQ and DAIR-L1. What if we consider the other regularization functions?\n\nExperiment results:\n- As the authors mentioned in the paper, the comparison with IRM might not be fair as IRM does not use augmentation samples. Is there a fair baseline for this experiment?\n\nMinors:\n- In the first paragraph of section 3.1: 'by by' -> 'by'\n- In the first paragraph of appendix D: 'is is' -> 'is'\n",
            "summary_of_the_review": "This paper proposes a new objective function to train a model that is invariant under data augmentations. It seems the goal of this paper makes sense, but the theoretical and experimental results are too restrictive to consider various settings.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "There is no ethics concern on this submission.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors proposed a regularizer (DAIR) that penalize the difference between the model performance on original data and augmented data. The authors argued that the new regularizer imposes invariance and effectively prevented the model from overfitting to spurious features. By applying the regularizer on various types of machine learning tasks, the authors analyzed the experiments results and explained the empirical novelty.",
            "main_review": "The authors proposed a regularizer to further improve the effectiveness of utilizing augmented data. Great amount of experiments are conducted to demonstrate the impact of DAIR. Overall the idea is simple and clean.\n\nHere are some major concerns:\n1. The authors proposed the new regularizer to reduce the effect of spurious features. There should be some works have discussed upon this topic other than data augmentation. For example, self-training work [1]. Dropout could be another method that can help upon this purpose. The author should involve such work for analysis or comparison.\n2. The regularizer proposed requires a one to one corresponding between original samples and augmented samples. However, many data augmentation method could not provide such augmented data, e.g. mixup. This requires strong prior knowledge about the spurious feature and the distribution shift, thus limited the usage of the proposed method.\n3. The idea of penalizing the performance difference on original data and augmented data is not quite original. For example, some works in multi-view tasks have proposed to penalize the difference between logits of multiple views for the same entity. Though not proposed for exactly the same purpose, and similar ideas could be encouraged as long as it is reasonable and works. However, this could still weaken the originality and novelty of the proposed idea in this paper.\n4. This might be a minor point. The demo experiment in section 2.1 is not really convincing. The author proposed a rare scenario that one feature will only available at training time.  The setting looks like being specifically manipulated to benefit the proposed method. \n\n[1] Chen, Y., Wei, C., Kumar, A. and Ma, T. Self-training avoids using spurious features under domain shift. NeurIPS 2020",
            "summary_of_the_review": "Overall, the paper is easy to follow and the idea is clear. However, some related works should be discussed and analyzed. Moreover, the originality and novelty of the idea is not strong. These lead to my current conclusion: The is paper is marginally below the acceptance threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}