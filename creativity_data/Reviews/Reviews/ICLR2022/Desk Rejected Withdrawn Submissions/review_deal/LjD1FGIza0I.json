{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the problem of verifying ownership of a deep neural network. The premise is that the model architecture itself has intrinsic value and so should be watermarked, which is found through NAS. Side-channel methods are developed to perform watermark detection.",
            "main_review": "The idea that the model architecture itself has intrinsic value is interesting, but I do not find the arguments contained within this work convincing. Please find a detailed reasoning below:\n\n1. The motivation is unclear to me. What ultimately matters and practitioners likely care about protecting is the learned parameters of a model, this is where valuable information lies. NAS is a second order citizen; we may care to use it because we expect that by employing NAS we can find an architecture that has better utility on a particular task than an off-the-shelf model, but ultimately the value lies in the learned parameters of that model, not the architecture itself. Perhaps the architecture is valuable in and of itself if one can show no off-the-shelf model can come close to a NAS found model on a particular task with respect to some utility metric, but the authors have failed to find and delineate such a compelling case. \n\n2. This is a criticism of most paper's on model watermarking, but why can we ignore distillation in the threat model (given that the author's accept that an adversary replicating the functionality of the protected model is a violation of the scheme)? Previous works (e.g. [1]), have shown it is possible to extract a model using unlabelled data, and as such does not strike me as an onerous assumption to make about a motivated adversary. Furthermore, previous model watermarking works have evaluated their defenses under such attacks [2]. The oft used response, and one that appears in this work is that model extraction methods will require a relatively large amount of unlabelled data and computing resources, but this suggests the defense is being evaluated under an artificially weak attacker -- if a model architecture is valuable enough to watermark, one should probably assume a motivated / powerful adversary does exist.\n\n3. Unless I misread, the watermark extraction experiments were not performed on a TEE. Although there have been many side-channel attacks proposed on secure enclaves, it is a stretch to state that watermark extract is efficient and always realizable on these environments. Some empirical evidence would be valuable here.\n\n4. The \"proof\" for Theorem 1 is not a proof because the terms: effectiveness, usability, robustness, and uniqueness are not well defined. I see the value in explanation of why the authors think their scheme is useful with respect to such notions, but calling it a proof is a misnomer.\n\n5. Scalability: there are no experiments that suggest this scheme can scale to models with a larger number of trainable parameters (e.g. GPT-like models with billions of parameters). Language models are a natural area where one may want to watermark a proprietary model, showing this scheme works in such a setting would highlight that this method could be used in practice.\n\n6. I don't understand why it's not reasonable to expect a comparison with parameter based watermarking approaches. For example, we take the NAS architecture denoting the watermarked model and train (a) normally, and (b) using a parameter based watermarking scheme. We then compare the robustness of (a) (your scheme) and (b) (parameter based watermarking) against different attacks, and measure the ability to extract the watermark after the attack. Why is this an unfair comparison?\n\n[1] Jagielski et al. High Accuracy and High Fidelity Extraction of Neural Networks (2020)\n\n[2] Jia et al. Entangled watermarks as a defense against model extraction (2021)\n\n",
            "summary_of_the_review": "Ideas contained in this work are interesting and has value, but I find the motivation and empirical evidence to be confusing at best.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a NAS based DNN model watermarking by using specific connections as the watermark.",
            "main_review": "This paper proposes a NAS based DNN model watermarking by using specific connections as the watermark. I believe the idea is interesting, though it only be suitable for NAS models rather than the general DNN models. On the other hand, I have several concerns as below.\n\nDetailed comments:\n1. The paper may benefit from revising its grammars.\n2. In Sec 3.2, I am afraid I cannot intuitively understand why the path selection process has not considered the performance of NAS. If the paths are fixed randomly, why the suboptimal performance of NAS can still be achieved? Is there any theoretical bound to prove this without the assumption 1 in the appendix? \n3. For Sec. 3.2, I am also wondering how to determine the size of mk?\n4. For the function \"GetSubPath\" in ALgorithm 4, I am wondering how specifically it is achieved?\n5. In Sec. 3.3, I am wondering whether the generated architecture needs to be trained to fit the specific tasks?\n6. According to Secs. 1-3, since the propose method will fix certain connections, I am wondering why the proposed method can intuitively achieve robustness against pruning? I am afraid the proof in the appendix has not resolved this issue.\n7. I actually suggest the authors to use ImageNet or other large datasets to carry out the experiments in Sec. 5.1. Cifar-10 is too easy.\n8. For the results on ImageNet in Sec. 5.2, since the performance of the original model is too poor to assume convergence, I am afraid this comparison between the original model and the watermarked model cannot prove that the proposed method can guarantee to find the NAS structure with suboptimal performance.\n\n\n",
            "summary_of_the_review": "I believe the idea is interesting, though it only be suitable for NAS models rather than the general DNN models. On the other hand, I have several concerns as above. In general, I believe the paper still needs certain experimental results and/or theoretical proofs to support their claims.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose a watermarking scheme to achieve the intellectual property (IP) protection and ownership verification of DNN architectures.",
            "main_review": "#Strengths:\nThe paper is well organized and moreover, the research pursued is very important in the cyber security field.\n\n#Weaknesses:\nThe paper can be improved based on the following points:\n1) There are several grammatical errors and the authors should revise the paper accordingly. For example it should be …”even if the model is encrypted or isolated.”\n2) In section 1, it would be better to provide a motivational case study to keep the readers motivated and understand the importance of the research being pursued. How changing parameters slightly can plagiarism a model?! What is the actual threat here? Would be good to have a model/case study showing this.\n3) The justification behind leveraging NAS isn’t provided. Why use NAS over other methodology? Please mention this. What are the main benefits?\n4) One of the reasons watermarking is used such that in terms of copyright/IP infringement, it can be proven that the IP belongs to the said owner/author. However, the biggest challenge is that often times its difficult to prove the infringement since the AI/ML model isn’t directly visible to the outsider while inference is taking place, especially if the training is on the cloud. How do the authors propose to deal with this? This hasn’t been mentioned in the paper.\n5) In section 3, the proposed method lacks novelty in general. It seems like an upgrade to existing watermarking method for NAS. The authors need to specify why such method is chosen? What are the design constraints and choices being made as part of it? Only explaining the methodology doesn’t make it a novel method nor it contributes to the scientific knowledge.\n6)  in section5, the experimental/evaluation section lacked comparative study in general. Wold be great to have that as well to prove the efficacy of the proposed method.",
            "summary_of_the_review": "Though the paper is working on an important topic of research and has proposed an interesting method, the paper still requires a lot of work to be done before it can be presented at ICLR. Please refer to the points mentioned in the weaknesses part of the main review section to improve the quality of the work.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable for this one.",
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a defense scheme that aims to protect the intellectual property of unique neural network architectures. The defense has two components: (1) the NAS-based mechanism that embeds a watermark into the resulting architecture topology, and (2) the side-channel attack that extracts the watermark from the execution trace. In evaluation, the paper demonstrates its effectiveness as an ownership defense and usability. The paper also shows some resilience to simple, adaptive adversaries and the uniqueness of an individual watermark. The paper further evaluates the defense with RNN models.",
            "main_review": "**Strengths:**\n1. The paper is well-written, easy to follow.\n2. The paper studies the feasibility of using side-channel attacks as a defense for protecting ownership of DNN architectures.\n\n**Weaknesses:**\n1. The paper overclaims its contributions.\n2. The proposed defense could lead to a false sense of security.\n3. The threat model assumes an impractical defender.\n4. The evaluation with adaptive adversaries is weak.\n\n\n**Detailed comments:**\n\nThe paper makes an interesting proposal---let's use side-channel attacks that extract neural network architectures for protecting their ownership. It's one of many kinds of \"attack as a defense\" approach.\n\nHowever, turning the attack into a defense requires careful examination from an adversarial perspective. It's different from an offensive study---oh well, there is this case where my attack works. Indeed, this paper has many flaws we can observe \"attack as a defense\" approach. Thus, I am leaning towards rejecting this paper.\n\nHere, I provide my detailed comments on the weaknesses.\n\n[Overclaiming Contributions]\n\nIn many places, the paper overclaims its contributions. I listed them below. If we understand the actual contributions, I don't think this paper advances our knowledge of the architecture stealing and defenses.\n\n(In the abstract) The paper claims, \"we are the first to claim model architectures as an important IP...\" Prior work [1, 2], however, already acknowledged this problem and proposed architecture stealing attacks via cache-based side-channels. So, I don't think the paper is the first.\n\n[1] Yan et al., Cache Telepathy, USENIX 2020.\n[2] Hong et al., How to 0wn NAS in Your Spare Time, ICLR 2020\n\n(In the intro.) The paper claims, \"prior solutions [1, 2] ... fail to recover new operations in NAS.\" I carefully looked at the two papers: [1] indeed fails, but [2] extracts a NAS-generated architecture, ProxylessNAS, using the same side-channel attack, Flush+Reload. I don't think the authors carefully read the prior work and scientifically compare it with theirs.\n\nMore importantly, since prior work was able to extract NAS-based architectures, I cannot see any technical contributions for their techniques in Sec 4.\n\n(In the threat model) Again, the knowledge distillation (KD) is not proposed for re-design the model architecture completely. As one counter-example, in the literature, they use KD for distilling knowledge from ResNet50 to ResNet18. Thus, I don't think this paper makes fair comparisons with its related work, which makes me concerned about the validity of their claimed contributions.\n\n\n[Defenses That Foster A False Sense of Security]\n\nAnother major problem is that this work can foster a false sense of security and put people who do not run their models in the settings vulnerable to side-channel attacks in danger. Suppose that I have a unique architecture for a task and, to protect it, I don't run my model on any vulnerable environment (only run in-house environment). I then figure out this watermarking solution and start running my model on the public cloud (vulnerable to side-channel attacks). The moment I put my model in public, it can become the victim of side-channel attacks.\n\nSecond, once an adversary steals it, there is no more arms race. In security, we rely on the assumption that an attacker and defender continuously improve so that we can become safe than before. But, here, if we share the architecture with an adversary, then it's done. The attacker from there can use whatever means to hide it from the victim.\n\nMoreover, the authors did not clarify that the side-channel attacks are only possible when the model owner and the adversary run in the same host (physical hardware). There are many scenarios this assumption can't be met; thus, I don't believe this defense makes us safer than before.\n\n\n[Weak Evaluation]\n\nIn some cases, NAS has been used to find an architecture that improves the classification accuracy by 1-2%. It is really important for the task where we already see a saturated accuracy. But, this paper at first claims that the accuracy difference < 1% is acceptable. This claim does not convince me unless the authors run this defense with a much complex dataset, e.g., ImageNet. CIFAR-10 is too easy and shows a saturated\naccuracy, so any modification of NAS may result in a similar accuracy.\n\nMoreover, I suggest evaluating the defense when the adversary runs two architectures in parallel (one is the stolen architecture, and the other is just a dummy) and identifying the ownership. Running two processes simultaneously on a CPU significantly makes the trace noisy; thus, I don't think the defense can identify the architecture robustly. Prior work [2] already discussed this as a defense and showed some results. Compared to this, the others shown in the paper (model obfuscation and model transformation) are too easy adaptive attacks.",
            "summary_of_the_review": "The paper studies the ownership protection problem for unique neural network architectures. However, the paper has many flaws:\n\n1. The paper overclaims its contributions.\n2. The proposed defense could lead to a false sense of security.\n3. The threat model assumes an impractical defender.\n4. The evaluation with adaptive adversaries is weak.\n\nThus, I am leaning towards rejecting this paper for this ICLR.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "This work can foster a false sense of security and put people who do not run their models in the settings vulnerable to side-channel attacks in danger. People who even haven't thought about running their models in public or a vulnerable environment can start running them believing the defense will verify the ownership. However, the moment I put my model in public, I lost the battle---it can become the victim of side-channel attacks.\n\nAgain, if an adversary steals it, there is no arms race. In security, we rely on the assumption that an attacker and defender continuously improve so that we can become safe than before. But, here, if we share the architecture with an adversary, then it's done. The attacker from there can use whatever means to hide it from the victim.\n\nMoreover, the authors did not clarify that the side-channel attacks are only possible in a specific setting---where the model owner and the side-channel attacker run in the same host (physical hardware). There are many scenarios this assumption can be met; thus, I don't believe this defense makes us safer than before.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a method to watermark a deep learning model by embedding the watermark information into the model architecture. They encode the watermark information by restricting the search space of NAS, such that the model’s performance is retained and there is sufficient flexibility for embedding the watermark information in. Since the architectural information is encoded inside of the model architecture, to decipher the watermark, one needs to access the trace during model inference. They propose a way to decipher the cached side channel of the NAS architecture by monitoring the use of GEMM in the low level BLAS library. They modified the previous decoding scheme to a NAS architecture, which has more moving components.",
            "main_review": "Disclaimer: I have previously reviewed the paper, and since the paper hasn't changed very much since the last iteration. My review has mostly stayed the same\n\n**Pros**\n- The idea of encoding the watermark information into the model architecture is interesting and is definitely more robust to removal compared to some of the other methods that rely on embedding the watermark in the input/output pair or parameters.\n- The paper is well written and is very informative. It reviews the necessary concepts for me to follow the paper.\n\n**Cons**\n- From a practical standpoint, when the model is served on the cloud, getting access to the trace of the operations is a strong assumption. Previous watermarking techniques often rely on only the input and output pairs to verify the ownership. Also, the monitoring of side channels may violate user agreement of some softwares.\n- The main motivation of embedding the watermark into the model architecture is that parameter-based watermark is easy to remove, but that is not always the case. For example, [1] shows that one can find a set of conferrable adversarial examples, such that it only transfers to the owner’s model, and the degree of transferability marks the ownership. The approach does not modify the behavior of the model itself, so it is undetectable by the adversary. It is also difficult to remove because the conferrable adversarial examples transfer even in the face of distillation [1]. Given the presence of this prior work, the motivation and utility of the approach is diminished. However, the proposed approach is interesting enough, and the approach by [1] is very expensive. Ideally, some comparison should be done between the two approaches. However, given that they differ sufficiently, a direct comparison may not be needed. I would at least expect some discussion of this prior work.\n\n[1]Lukas, Nils, Yuxuan Zhang, and Florian Kerschbaum. \"Deep neural network fingerprinting by conferrable adversarial examples.\" arXiv preprint arXiv:1912.00888 (2019).\n\n",
            "summary_of_the_review": "Overall, I thought the approach is sufficiently interesting and very informative. The motivation is relatively weak in the presence of the mentioned prior work. I expect at least some discussion and ideally some comparison with the prior work to make it a better paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}