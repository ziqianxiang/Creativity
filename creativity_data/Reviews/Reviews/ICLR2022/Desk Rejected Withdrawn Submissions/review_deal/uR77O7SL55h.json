{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Authors leverage matrix-calculus identity in order to provide a more memory-efficient backpropagation version of the Sinkhorn algorithm. Some theoretical results are provided, and improvements over vanilla automatic differentiation are shown on a number of baselines",
            "main_review": "I believe this is a potentially relevant paper. And overall it is a nice contribution; addresses an actual need in the community and provides solutions. I do have some concerns (that overall justify my judgement)\n\n\n1) Empirical evaluation is OK, but not compelling. Results would be much more appealing if they were applied to a setup in which otherwise results would not be possible to get. The examples considered here are still quite standard. Authors may consider more involved applications.\n\n\n2)The findings on superior performance of the proposed method is interesting, but puzzling and even raises doubt: Why would we have better accuracy if \\tau is the same? the proposed method (to my understanding) only has to do with algebraic manipulations a matrix calculus, but the result should be the same. Authors should clarify why we get differences in practices, and why these differences are such that the proposed method does better.\n\n3)The main theoretical result, Theorem 5, is actually quite weak. Bounds are expressed in bounds for ||P^t-P^\\ast||_F. But bounding this quantity is an open problem by itself, and only a few stability results are known https://arxiv.org/abs/2108.08129. \n\n\n4)Some of the formulae here seems familiar, for example in Luise et al, 2018 https://arxiv.org/pdf/1805.11897.pdf. Actors should clarify what is new",
            "summary_of_the_review": "Potentially relevant, but requires stronger experimental validation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper considers the gradient computation of the optimal transport plan in the entropy regularized optimal transport problem with respect to the input marginal distributions as well as the ground cost matrix. The KKT condition of the entropy regularized optimal transport problem defines an implicit function that maps the input marginal distributions and the ground cost matrix to the optimal transport plan. The implicit function theorem is used to derive the target gradient.",
            "main_review": "This paper is well written and easy to follow. The derivation of the Sinkhorn backward operator is well explained. The error bound of the gradient computation is also analyzed.\nMy major concern of this paper mainly lies on its novelty: How does this research differ from the previous works that defined layers of neural network based on implicit differentiation? In previous research like [Amos & Kolter, 2017], the implicit gradient is derived when a quadratic programming with linear constraints is used as a layer. While there is indeed an extra entropy regularization term in the Sinkhorn backward operator, I do not see this to be a significant contribution as all the derivations follow similarly to the previous works.\n\nAdditionally, to improve the paper, I think the authors need to better motivate the development of the Sinkhorn backward operator, e.g. provide applications where the differentiation of the optimal transport plan with respect to the input marginal distributions and the ground cost matrix is required. The authors has listed a lot of potentially application, but most of them lacks detailed discussion, at least in the main body of the paper. \n\nThe only example that is concretely discussed in the experiment section is the (entropy regularized) Wasserstein barycenter problem. However, in such a problem, the only required differentiation is the gradient of the value of the entropy regularized OT problem (not the optimal plan) w.r.t. one of input marginal distributions, which is different from the inkhorn backward operator under study. Besides, such gradient has already been studied and used in previous works like [1] and [2]. \n\n\n[1] Feydy, Jean, Thibault Séjourné, François-Xavier Vialard, Shun-ichi Amari, Alain Trouvé, and Gabriel Peyré. \"Interpolating between optimal transport and MMD using Sinkhorn divergences.\" In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 2681-2690. PMLR, 2019.\n\n[2] Shen, Zebang, Zhenfu Wang, Alejandro Ribeiro, and Hamed Hassani. \"Sinkhorn Barycenter via Functional Gradient Descent.\" Advances in Neural Information Processing Systems 33 (2020). ",
            "summary_of_the_review": "This work provides a clear derivation of the implicit gradient of the optimal transport plan w.r.t. the input marginal distributions in the entropy regularized optimal transport problem, but the novelty is limited.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to use implicit differentiation to compute the backpropagation step for the Sinkhorn algorithm.",
            "main_review": "## Method and Novelty\n\nUsing implicit differentiation for the backpropagation for the Sinkhorn algorithm is already a known technique in the community.\nCuturi et al. already discussed that one can use the implicit function theorem to differentiate the Sinkhorn algorithm (https://arxiv.org/pdf/1905.11885.pdf).\nSoon after, the implicit differentiation for the Sinkhorn algorithm was (independently) explored in (https://arxiv.org/pdf/2002.03229.pdf) and (https://arxiv.org/pdf/2002.06504.pdf).\n\nMoreover, the accompanying source code published by Cuturi et al. (https://github.com/google-research/google-research/tree/master/soft_sort) has implicit differentiation as the default parameter (https://github.com/google-research/google-research/blob/3337726f470c640b0f96a288736d7c8b73ba5b89/soft_sort/sinkhorn.py#L291) and implement the implicit differentiation of the Sinkhorn algorithm (https://github.com/google-research/google-research/blob/3337726f470c640b0f96a288736d7c8b73ba5b89/soft_sort/sinkhorn.py#L154).\nThe OTT toolbox also contains a respective implementation of this (https://ott-jax.readthedocs.io/en/latest/_autosummary/ott.core.sinkhorn.sinkhorn.html#ott.core.sinkhorn.sinkhorn).\n\nI am not aware of a paper discussing the derivation of the closed form solution in the specific way and in as much detail as in the presented draft.\nHowever, the derivation is not novel, considering that implicit differentiation is well-known and has been applied to the Sinkhorn algorithm many times.\nI assume the error bounds in Theorem 5 are novel. \nIf there is any difference between the way the implicit differentiation is done and the way you do it, I strongly suggest that you focus on these differences. (I do not think this is the case, but just in case.) Maybe there are some numerical advantages to either method?\n\n## Experiments\n\nI have reason to assume that experiment 5.1 is very misrepresentative.\nIn the experiment, you use a very small batch size (probably 1) and therefore what you measure is almost entirely the overhead of communication between Python and GPU. This would manifest itself, e.g., by the ability to multiply the batch size by 10 and obtaining the same runtime.\nAnd this is the case.\nGoing from n=10 to n=100 makes the computation of the backward by a factor of $10^3$ more expensive (assuming matrix inversion requires $n^3$ which is fair for empirical considerations). \nHowever, the backward still requires effectively the same time although being $1000$ times more expensive.\nGoing further form 100 to 1000 would be yet another factor of $1000$. Here, the runtime increases by a factor of around $\\sqrt{10}$.\nThis means that, while the computational cost increases by a factor of $10^6$, you measure it such that you only observe an increment by $\\sqrt{10}$. This severely misrepresents the actual computation cost and also explains your \"superiority\" to automatic differentiation.\nAs the (by far) leading time-consuming operation is the communication between CPU and GPU as well as Pythons overhead, scaling a matrix by a scalar is effectively as time-consuming as inverting the matrix. This is because the GPU is not utilized properly.\nAs the auto differentiation requires more steps in the backward, it is more time-consuming. Not because it requires more computationally expensive (because it is not, at least not in the demonstrated setting).\n\nThe point about OOM is valid, however, also known.\n\nAs for the evaluation of how well the method's gradients perform, I would have liked to see a method that goes beyond Wasserstein barycenters, number sorting, and point cloud registration.\nOne well-established task for this is the 4-digit MNIST sorting benchmark, which has been applied in an array of recent works:\n\nhttps://arxiv.org/abs/1903.08850\n\nhttps://arxiv.org/abs/1905.11885  \n\nhttps://arxiv.org/abs/2006.16038\n\nhttps://arxiv.org/abs/2105.04019\n\nThis could benchmark the quality of the gradients by testing their utility for learning the CNN, and would make it comparable.\n\n### Remarks\n\nFigure 1 seems familiar; however, I cannot recollect where I have seen it or a very similar Figure before. (Does not influence my score, obviously, just as a remark.)",
            "summary_of_the_review": "I suggest rejection of the paper for the following reasons: \nWhile the derivation is interesting and well written, it is not a novel result.\nThe experimental evaluation is flawed.\n\nThe error bounds are (to my knowledge) novel.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to compute the gradient of the Sinkhorn operator (with respect to the input cost matrix and the input marginals) via implicit differentiation. This is done in a standard way: viewing the resulting KKT system as an implicit function (mapping the input cost and marginals to the output transportation map and dual variables), and then applying the implicit function theorem to compute the Jacobian. The authors also provide an error bound on the computed gradient (in terms of the error on the returned solution). The authors performed experiments on synthetic data and a real-world application (point cloud registration), showing that the proposed method achieved competitive accuracy compared to automatic differentiation, while being more GPU-efficient, especially for a large number of Sinkhorn iterations.\n",
            "main_review": "The paper is well written, well structured, and easy to follow. The proposed method is simple to implement and has potentially great practical values.\n\nUnfortunately, this paper also has a major (and somehow unfixable) limitation, which lies in its incremental technical novelty. I was quite excited at the beginning of my reading (given the catchy title), but then quickly got disappointed, because the paper presents virtually nothing really new to me. \n\nI believe that this would be the impression of any reader who is familiar with, e.g., Gould et al. (2019) and Campbell et al. (2020). Gould et al. already present in detail the principles of using implicit differentiation to compute the derivatives of optimization layers, and the current paper sounds like a mere application (it could have been presented compactly as a section in Gould et al. (2019)). Certainly technical details could greatly differ from case to case, but in this case, the application to the Sinkhorn operator was already considered by Campbell et al. (oral presentation at ECCV 2020) (see their Section 3.3 and supplementary material). It should be noted that the current paper considers the most general form, in the sense that it also shows how to compute the gradients with respect to the marginals, in addition to the gradient with respect to the cost matrix (as done by Campbell et al. (2020)). However, this minor addition is rather straightforward. In conclusion, the technical contributions of this paper are **very** incremental, in view of Campbell et al. (2020).\n\n\nI was thinking what I would have done to improve this paper and make it a solid contribution, if I were an author. This is not an easy task, but I hope the following ideas could be useful to the authors (even though they are not necessarily doable for this submission):\n\n1. Consider more optimal transport (OT) related tasks, especially those that are more difficult than vanilla OT. An interesting example could be the task of computing the Gromov Wasserstein distance.\n\n2. More extensive experiments, not only in terms of tasks but also in terms of competing methods. Note that comparing to naive automatic differentiation is not always fair. For example, in the point cloud registration experiments, naive automatic differentiation should not be used in practice, because for the given cost matrix it is not necessary to backpropagate through all the Sinkhorn iterations but only through the last one. This is known in the literature (see for example Feydy et al. (2019), Section 3.2). Backpropagating through only the last iteration would probably result in better performance than the proposed method, in terms of both speed and memory footprint.\n\nA good set of experiments should cover two cases:\n\na. Special cases where specialized methods can be used: show that the proposed method could achieve comparable (or not much worse) performance than those specialized methods.\n\nb. General case: show that the proposed method could have certain advantages compared to other methods: naive automatic differentiation, black-box differentiation (Pogan et al., 2019).",
            "summary_of_the_review": "The technical novelty of this paper is very incremental, in view of Gould et al. (2019) and Campbell et al. (2020).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}