{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies multi-agent reinforcement learning (MARL) in a setting where all agents work cooperatively to maximize their average reward. Specifically, the global state $s$ is observable to all agents, and each agent has its own local action $a_i$. The agents can communicate over a time-varying network. The authors allow each agent to have a partially personalized policy $\\theta_i$. For each agent $i$, a subset of parameters $\\theta_i^s$ are shared among all agents, while the remaining part $\\theta_i^p$ is optimized locally. The authors propose an actor-critic algorithm to optimize the policies, and analyze the number of samples needed to converge to a stationary point of the objective. Their results provide the first finite-time convergence bound that works for partially personalized policies in this MARL setting.",
            "main_review": "The main contribution of this paper is the proposed Coordinated Actor-Critic (CAC) algorithm and its finite-time convergence bounds. However, since the framework of the proposed algorithm is similar with [Zhang et al., 2018], and many finite-time bounds exist in similar MARL settings, I think this contribution is not significant enough. Although the authors emphasize a major difference is that they allow a part of local policy parameters to be shared among all agents, I am not fully convinced about the importance of this generalization in policy class compared with fully personalized policies or fully shared policies. The advantage of CAC with partially shared parameters is only weakly demonstrated in the experiment part, and there are no theoretical results to guarantee that sharing a subset of parameters helps in homogeneous settings.\n\nI might overlook some contributions of this work due to lack of familiarity. I hope the authors can help me understand the following questions better in the rebuttal:\n1.\tWhen the policy parameters in the proposed CAC algorithm is fully personalized or fully shared, are there any existing finite-time results for the same algorithm? Does the $O(\\epsilon^{-\\frac{5}{2}})$ sample complexity bound matches the best existing bounds in this MARL setting?\n2.\tHow well does CAC compare with other benchmarks (excepts its variants) in numerical experiments? \n3.\tThis paper uses a linear function approximation for the global reward function. I don’t think this is common in previous works. Is there any justification for this approach?\n\nBased on my current evaluation of this work, I will vote for weak reject with a low confidence score. I’m happy to adjust the score based on the authors’ response.\n",
            "summary_of_the_review": "I think the major contribution of this paper is not significant enough, but I might overlook some contributions due to lack of familiarity. Thus, I vote for weak reject with a low confidence score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a coordinated actor-critic (CAC) algorithm for MARL with a shared part and an individual part. The shared part takes care of the homogeneity and is jointly optimized among all agents and a personalized part targets the heterogeneity among agents and is only locally optimized. Sample complexity of $\\mathcal{O}(\\epsilon^{-5/2})$ is proved for the proposed CAC algorithm.",
            "main_review": "The paper is well written and easy to follow. I checked the proof and I believe the main results are correct.\n\nHowever, I have some major concerns regarding the motivation for separated joint policy and local policy, as well as how an agent could know which parts of his/her policy parameters are for the joint policy parameters or for the individual policy parameters. I am happy to raise my score if the following questions could be addressed.\n(1) It is not clear to me what problem formulation (i.e., reward design and transition kernel) will lead to an optimal policy with a shared part and an individual part.\n\n(2) At the beginning of Section 3.1, the authors assume directly the policy is decomposed into $\\theta_i:=(\\theta_i^s,\\theta_i^p)$. This assumption is too strong from a practical perspective. Suppose a learning agent is optimizing over a 4-dimensional parameter space $\\theta=(\\theta_1,\\theta_2,\\theta_3,\\theta_4)\\in \\mathbb{R}^4$. Why is it innocent to assume that the agent *knows a priori* that  (for example) $(\\theta_1,\\theta_2,\\theta_3)$ is for the shared policy and $\\theta_4$ is for the local policy?\n\n(3) In formulation (5), the authors restrict $\\theta_i^s=\\theta_j^s$ if (i,j) are neighbors. Does this imply that there are k global policies if there are k clusters on the graph? I don't feel the constraint is well motivated.\n\n(4) It is stated on page 5 (under paragraph \"Policy Optimization\") that \"trying to make sure that the shared parameters are not too fr from their neighbors\". Again, if there exists strong heterogeneous behavior between two connected agents, then there is no incentive for them to take similar policy parameters.\n\n\nSome other comments:\n\n(a) The agents are assumed to have a joint state. Is it possible to have individual state space? If yes, does the complexity grow exponentially with respect to the number of agents?\n\n(b) The authors consider linear function approximation for the reward. Can the authors say anything about the approximation error? See \n(Provably Efficient Reinforcement Learning with Linear Function Approximation, Chi et. al, 2019)\n\n(c) Some references that might be relevant: \n\nNetworked agents:\n\nScalable Reinforcement Learning for Multi-Agent Networked Systems (Qu et. al, 2019)\n\nMean-field regime:\n\nA Principled Permutation Invariant Approach to Mean-Field Multi-Agent Reinforcement Learning (Li et. al, 2021)\n\nModel-Free Mean-Field Reinforcement Learning: Mean-Field MDP and Mean-Field Q-Learning (Carmona et. al, 2019)\n\nMean-Field Controls with Q-learning for Cooperative MARL: Convergence and Complexity Analysis (Gu et. al, 2020)\n\nMARL with shared policy:\n\nCalibration of Shared Equilibria in General Sum Partially Observable Markov Games (Vadori et al, 2020)\n\n",
            "summary_of_the_review": "The paper is well written and easy to follow. I checked the proof and I believe the main results are correct. However, I am not fully convinced why it is interesting or important to consider separated joint policy and local policy.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces a new class of multi-agent reinforcement learning algorithms in which the agent's policy parameters are decomposed into two parts; one part which is shared across (neighbouring) agents and another which is optimised only by the agent. The motivation of the paper is to leverage the benefit of the existing extremes (full sharing among homogenous agents and totally localised updates). The paper then proves some results on finite-time bounds as well as performing a simple empirical analysis.  ",
            "main_review": "Parameter sharing and communication serve as useful tools to speed up and stabilise learning in multi-agent systems and even improve overall outcomes. However, in some situations forcing the agents to share all parameters and use some common updating procedure such as consensus updates on their learning parameters may be inappropriate; therefore allowing the agents to have some individual component of their learning update would seemingly be beneficial in some circumstances. This is somewhat achieved by allowing the agents to jointly estimate a global value functions while individually optimising their local policies. The idea of this paper seems novel in that it pushes this question one step further allowing the agents to have both an individual and a shared component of their policy parameter. \n\nThough I found the idea quite novel, the paper seems to have several gaps which make it i) very difficult to understand the idea in its entirety ii) hard to evaluate the benefits relative to existing approaches iii) difficult to know whether the idea has practical benefit.\n\nSpecifically:\n\nThe paper has some nice theoretical results but omits any discussion about how the results improve the existing state of affairs. For example, for Theorem 1 it would have been nice to have a discussion about how this result compares to similar results for which there is not decoupling between shared and not-shared policy parameters. \n\nProposition 1 seems to relate purely to the analysis of using linear approximations for the value functions and the global reward function. Given that linear approximation is a fairly common technique across RL in general, I think this needs a discussion to place these results in context with known results.\n\nAlso, I find the experiments to be quite weak. Though there are comparisons between the variants introduce in this paper (and some nice insights), no comparison to existing methods is performed and so one can’t evaluate the practical advantages over existing methods (this is also omitted in the theoretical analyses of the paper too).\n\nLastly, I didn't get a full sense of how the each decomposition formed i.e. learning what is best for the agents to share and what is best for them to keep localised. In other words, I couldn't see how each of the agent's two parameters affects the policy given that now the agent’s policy is determined by both parameters. The variants cover the two other extremes but I didn't get a feel of how the two parameters determine the agent's policy.\n\nLastly, there are a few restrictive assumptions in the paper. Though the references are convincing enough of the idea that these assumptions are common in some other works, it would be nice to include a brief discussion on the implications of these assumptions i.e. when will they hold true or be violated. \n\n\n",
            "summary_of_the_review": "Overall I found the idea novel and the paper has some useful theoretical analysis. However, there were some gaps (in the analyses and the empirical studies) which make it hard for me to understand how practicable the method is in its current form. Additionally, I think the paper is lacking discussions on how the results fit within the context of existing results. Since the benefits of the method haven't been backed up by empirical evaluation either the extent of the advantages of the method remain mainly unknown.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}