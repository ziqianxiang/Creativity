{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This manuscript studies the dimensionality reduction problem. The presented method consists two paths to reduce dimentionality by an encoder, then increase the deminsionality by a projector. Finally, correlation is used as a loss to learn the model parameters. Experiments have been done to verify the correctness of the presented method.",
            "main_review": "## Strength\n1. The experiments have been done in a very comprehensive way, which is good!\n\n\n## Weakness\n1. The idea presented in this manuscript, as illustrated in Figure.1, is very close to the following paper:\n[Zbontar et. al, \"Barlow Twins: Self-Supervised Learning via Redundancy Reduction\", in Proceedings of the 38th International Conference on Machine Learning, PMLR 139:12310-12320, 2021. ](https://proceedings.mlr.press/v139/zbontar21a.html)\nComparing Figure.1 of this manuscript and Figure.1 of above mentioned paper, we can see that there is almost no difference at all. Considering that the above mentioned paper was just published in a few months ago, the contribution of this manuscript is rather limited.\n\n2. Even though there was not the above mentioned paper published in ICML 2021, the idea of this paper is rather simple: (1) an encoder to (nonlinearly) reduce dimentionality, followed by a projector to raise the dimensionality again, (2) CCA, as show in Eq.(1), is used to utilize data correlations. So the novelty of this manuscript, as well as the ICML paper mentioned above, indeed is questionable in terms of methodology. The only possible contribution of this manuscript, as well as the ICML paper mentioned above, may be its simplicity, which may lead to better computational efficiency for dealing with big data. Yet, when encoder is used, it is hard to see it is possible to have a fast solution, compared to traditional PCA (or kernel PCA), ICA, CCA, in terms of computational speed. Most importantly, it seems computational speed is not verified in experiments.\n\n3. Although the experiments have been done pretty comprehensive (which is a plus), some important aspects are not evaluated.\n    1. Considering an encoder can learn nonlinear dimenionality reduction, it is important to compare the presented method against kernel PCA.\n    2. It may be good to compare encoder-decoder as well.\n",
            "summary_of_the_review": "This manuscript studies the dimensionality reduction problem by presenting a two-path strategy to use CCA loss, in which each path consists of an encoder to reduce dimentionality and a projector to increase the deminsionality. A similar idea has been published in ICML 2021 and some important aspects are not evaluated in experiments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper introduces a dimensionality reduction technique based on the Barlow Twins loss. The emphasis is on linear dimensionality reduction, with the primary goal of maintaining scalability and serving as a replacement for PCA in a variety of tasks. The proposed method improves on the state-of-the-art for the tasks considered, and some connections are made between self-supervised representation learning and dimensionality reduction.",
            "main_review": "+ The proposed method is simple, scalable, and fairly robust to hyperparameter selection.\n+ Because of the above, the proposed method could realistically serve as a replacement for PCA as a first step in the considered tasks.\n+ The authors present a thorough ablation study and a strong comparison to existing methods.\n\n- The main weakness of the paper is that, while potentially quite beneficial, the proposed method is very heavily based on the work of Zbontar et al, 2021. I appreciate that the authors address this directly in Sec. 4, but it unfortunately still results in a limited contribution.",
            "summary_of_the_review": "The proposed method has significant benefits over other existing linear (and non-linear) dimensionality reduction techniques, but the technique is highly similar to existing work.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this work, the authors consider the problem of large-scale linear\ndimensionality reduction as a replacement for other linear\ndimensionality reduction methods such as PCA. To achieve this, the\nauthors learn an encoder function, via a redundancy reduction loss on\n\"similar\" examples over the training set. The similar examples are\npre-computed offline using simple nearest neighbors on an input\nembedding vector from another neural network. Extensive experiments\nshow that the proposed method is able to outperform PCA and its\nvariants in two different retrieval tasks.\n",
            "main_review": "In my view the main contribution of this work is to improve upon\nprevious manifold learning methods, especially Hessel et al., to\nachieve dimensionality reduction via a non-contrastive\nloss. Specifically, the authors choose the Barlow twins loss (and the\nnecessary architecture tweaks such as the use of the projector) to\npreserve the assumed similarity signal in the input space. The authors\nprovide enough empirical evidence to show that TLDR is able to do what\nis claimed. However, the technical novelty of this work is limited to\njust that, offering just a few minor insights on top of prior\nwork. For instance, the authors point out discussions from prior work\nregarding the benefits of non-contrastive learning approaches over\ncontrastive learning. However, there isn't sufficient\nmotivation/discussion on the choice of Barlow-Twins loss over other\napproaches for this task. The statement \"having a highly informative\noutput space is more important than a highly discriminative one\"\nprovided as a justification has very little basis. Having said that\nthe experimental results are strong and the authors have conducted\nextensive ablation studies to validate the significance of the major\ncomponents of their proposed method such as non-contrastive learning,\nencoder etc. ",
            "summary_of_the_review": "The idea of using self-supervised non-contrastive losses for\nsupervising a dimensionality reduction task is interesting, but the\npaper can benefit from a more rigorous analytical treatment. The\nexperimental results are promising.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors proposed a form of dimensionality reduction algorithm that uses an encoder-decoder structure along with the loss from the Barlow-twin work (Cross-correlation between samples) to learn a lower-dimensional representation of input data where distances are to some degree reflected.",
            "main_review": "My review will be short here.\n\nThe TLDR is an encoder-decoder structure for dimensionality reduction. The contribution of the authors is to employ the cross-correlation as a loss similar to what devised in the Barlow-twins (as acknowledged by the authors as well). \n\nI may be very ignorant here but in general I do not think DR in this form is a very interesting problem in the field now.\n\nI also contributions very incremental (although showing that the overall algorithm works is not known to me but on the other hand it was not surprising). \n\nThe way the positive pairs is constructed is the Achilles heel of the algorithm (and similar ones). One can never feed TLDR pure raw data unless Euclidean distances in the input space is meaningful. Authors may argue, it is task-dependent and given another metric, the algorithm may work (which I agree) but as a meta-algorithm, when I think about this construction, I am not 100% satisfied (this is a bit different in self-supervised learning)\n\n ",
            "summary_of_the_review": "Again, I do appreciate the efforts put into this work by the authors but unfortunately I do not find the work novel and I also do not think DR in the form considered in this work is a very important task any more.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}