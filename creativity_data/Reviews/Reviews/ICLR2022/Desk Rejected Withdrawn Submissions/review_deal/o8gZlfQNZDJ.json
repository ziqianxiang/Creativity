{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper describes a model for sound generation (SMF) which is intended to be used in an interactive or live performance setting. The generation model operates on a predetermined range of sounds that the model should be able to synthesise and specified parameters in the form of sounds which determine the controllable aspects of the sound during synthesis. The model comprises of a GAN or Conditional-GAN, a self-organizing map (SOM) which learns a linear subspace by utilizing user-selected points from the GAN latent space. Finally, an RNN is trained using the GAN generated audio along with the low-level parameters from the SOM, and the parameter manager (these are the params used in C-GAN).\n\nThe SMF is evaluated for three things: smoothness of the latent space with and without the SOM adaptation, and response time to parameter changes, and sound quality. The authors find that SMF receives better ratings from listeners in terms of both directness, as well as evenness of steps between sounds in the space. They also find the RNN to have a very quick response time with almost immediate change in pitch when provided with an updated pitch parameter. They utilize an audio classification network to determine whether the sound generated by SMF is similar to sounds generated by a GAN without the SOM and RNN and find a similar distribution of logits for both concluding that the sound quality is similar.",
            "main_review": "Strengths:\n- The paper is well written and clear for the most part. I especially would like to mention the section 1.1. It does a good job describing existing work and the drawbacks and challenges in the space. One line of work that I find missing is some work by Esling et al.[1, 2] which is related to this work.\n- The problem is well motivated. Oftentimes methods for music generation are proposed but rarely do they attempt to utilize the generation system in a real-time interactive setting.\n- The results sound fine. The audio example shows the quality of the sounds generated by SMF as well as the responsiveness to parameter changes. The sound quality is not great (some artifacts, low sample rate), but the parameter response is pretty impressive.\n- The method presented is very creative. They use a GAN for audio generation knowing that GANs are capable of synthesizing short clips of sound well. The challenge of manipulating or controlling the GAN latent space is addressed using a SOM to linearize a subspace. The problem of synthesizing continuous sound is solved by the RNN. Additionally, while it may seem counterintuitive that the model is only capable of synthesizing a limited range of sound at a time, in my opinion it is ingenious to limit the sound generation capacity and provide very specific controls to the end-user. The fact that this model is capable of achieving that makes a strong case for it.\n\nWeaknesses:\n\n- While I am impressed with this work, there is an issue of novelty. The main technical novelty presented in this paper is the use of the SOM to navigate the GAN latent space. Apart from that the GAN is adopted and so too is the RNN. I am not sure if the SOM to meshify the GAN space is enough.\n- The fact that the authors remove the attack, decay, and release of the sounds to train the GAN really simplify the problem but the ADSR is an important aspect which a model should also be generating. In a synth, these are also characteristics that one could control, for instance. Perhaps a sentence or two to justify the removal of these parts of the signal is a good idea.\n-  The method used to curate the unpitched sound dataset is unclear. The paper just mentions that the sound should be noisy and unpitched. Did the authors use a pitch tracker to perform this filtering? Perhaps a description in an appendix might be useful for collecting the BOReilly dataset will be useful for reproducibility.\n- The RNN training is a little confusing to me. First, the input sequence is 16-bit floating point, while the output is 8-bit mu-law. Is there a reason the authors didn't use 8-bit mu-law at the input? Also, is the RNN trained fully teacher forced or is there some kind of scheduling to make sure it is also trained autoregressively.\n- In the first paragraph of section 3, the authors mention that 100-200 randomly generated sounds are sufficient to find sounds that are similar to the training data. However, it is not clear how the 4 points are chosen after this step. Does the designer listen to all 100-200 samples and pick the 4 they want to set as the corners? If so, please make this explicit in the paper. The authors mention that the choice of the 4 data points is crucial. This process seems a little hacky to me, but since this is a subjective problem it doesn't bother too much. Also please explain the meaning of \"pinned corners with constrained edges\".\n- One of the main issues I have with the evaluation is the sound quality eval. I have no problem with the statement claiming that if the generated audio from SMF is similar to a GAN whose perceptual quality is deemed to be good, then we can say that the SMF-generated audio also has good perceptual quality. My issue is with the similarity metric used. The metric used here is the closeness of the logits from an audio classification model. I am not sure this is a valid method to determine similarity. First, we don't know what features of audio the classifier is using to discriminate. Second, it's also not clear how one can decide at what stage we can say the sound is similar. 88% of the sounds are classified similarly, but is that enough to conclude that the perceptual quality is similar? Also, the logits distribution is somewhat matching, but I'm not sure we can conclude that the perceptual quality is the same. I would recommend a listening experiment for this.\n\nTypos, etc:\n- some inverterd commas are incorrect (\"smoothing\" in abstract)\n- types reed -> types to reed\n- sometimes train conditionally: what do you mean by sometimes? For NSynth, I believe the GAN is always trained conditionally, no?\n- as well how -> as well as how\n- Fig ure 2-> Figure 2\n\n[1] Esling, P., Chemla-Romeu-Santos, A., & Bitton, A. (2018, September). Bridging Audio Analysis, Perception and Synthesis with Perceptually-regularized Variational Timbre Spaces. In ISMIR (pp. 175-181).\n\n[2] Esling, P., Masuda, N., Bardet, A., Despres, R., & Chemla-Romeu-Santos, A. (2020). Flow synthesizer: Universal audio synthesizer control with normalizing flows. Applied Sciences, 10(1), 302.\n",
            "summary_of_the_review": "The proposed method is a very well-engineered method that takes advantages of two well-known methods for audio synthesis: GANs and RNNs, and makes them controllable and responsive. There are some issues that I point out in the weaknesses above. The two main criticisms I have are concerning the novelty and evaluation. There is no doubt that this work has some degree of novelty, but I'm not sure if it is enough. Second, the sound quality evaluation should really just be a listening test. Due to these concerns I rate this paper as only marginally above the acceptance threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a neural net-based model for sound generation by integrating a generative adversarial network (GAN) and a recurrent neural network (RNN). For easier and more intuitive control over synthesis parameters, the authors select a subspace from the latent space of GAN and use resulting low-dimensional parameters to condition RNN for sound generation. A user study was conducted to evaluate the smooth interpolation ability of the learned low-dimensional parameter space, and the results support the proposed method successfully interpolates between the sounds in perceptual dimension. Quantitative evaluation with audio classification was also performed to show the proposed method captures sound-specific characteristics.",
            "main_review": "Strengths:\n- the paper deals with an interesting topic of sound modeling/synthesis\n- demo page is nicely designed and executed\n\nWeaknesses:\n- the paper is poorly organized and hard to follow\n- integration of RNN and GNN is not new; there are many prior works that take advantage of both architectures in the audio domain\n- detailed explanations are missing as to how to train the model, objective functions, etc.\n- the title is misleading in that it is too broad and ambiguous: the main contribution is to find a latent space for sound modeling/generation where the distance in the space well matches that in the perceptual space \n- the experiments are very limited to support the main arguments made in the paper: smoothness in interpolation is qualitatively evaluated though a user study, but I don't understand how the results of audio classification and logit value distribution are related to sound quality",
            "summary_of_the_review": "Although it touches on an interesting topic of sound modeling/synthesis, the paper is missing many components to fully address the key contributions claimed by the authors. Overall, I believe it falls short of the standards set by the ICLR.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents an approach for generative audio modelling, where the desired system need to have an expressive parametric control over its output, which is constrained on a limited range. The proposed approach is composed of (1) a GAN, which is able to generate fake audio representation, (2) a SOM over the GAN space, which linearize the parameter space and (2) an RNN which produce the desired audio output. The model is then evaluated using subjective and objective evaluations. ",
            "main_review": "Strengths:\n- The target task is very well defined and the proposed approach clearly delivers in terms of functionality.\n- The proposed approach is thoroughly evaluated and the studies show the viability of the approach. \n\nWeaknesses:\n- clarity: Section 2.2 is quite confusing, as it is more about the experimental setup than the architecture of the model. I think it would be better to move it the Section 4, or at least after Section 3, to clearly distinguish between the model and the experiments.\n- The novelty is limited, as the paper mostly consists of assembling well-known techniques together. But it is novel for the target application, as it seems to be the first system to be able to generate pitched or noisy sounds and to have parameters to morph between sounds. \n- The significance is also limited, as the system was designed for a very niche application, hence it's significance to the entire ICLR community is limited. That being said, the proposed system is probably significant in the field of sound generation, but I am not familiair enough with this field. \n- Minor improvement: in Secton 1, i would like to read a bit about the real-world applications of the model, is it use for music production? sound effect? sound design?",
            "summary_of_the_review": "The paper presents an approach for a very specific task, and the proposed system clearly delivers on the requirements. However the task being so niche, the novely and significance of the paper are limited. Hence I'll recommend accepting the paper, but just above the decision threshold.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a sound modeling method called Sound Model Factory (SMF), which combines the advantages of two generative models: GAN and RNN. GAN has an advantage in that it learns time-collapsed latent space having fixed dimensions for a whole audio sequence. Therefore, it is easier for GAN to interpolate between two audio sequences in the latent space. However, the length of the output is fixed so there is a limitation for GAN to control the time resolution. On the contrary, RNN is good for immediate response parameter control, but not good for latent interpolation. Furthermore, this paper proposes a self organizing map (SOM) technique for smoothing the latent space of GAN that results in perceptually smooth interpolation between audio timbres. In experiments, it shows that SMF achieves to smoothly control the timbre along the time axis, and also shows that the audio samples generated by the RNN in SMF and the generator of GAN based on a latent vector ‘$z$’ are actually similar.",
            "main_review": "I think this paper is novel and contributive to designing a sound modeling method. It first suggests criteria for a sound modeling method to be “playable\" (immediate parameter control and smooth interpolation). Then, it shows how it meets the criteria by combining two generative frameworks, GAN and RNN. Plus, I think the SOM technique for smoothing the latent space can give many insights to future works.\n\nHowever, there are several concerns and questions about this paper:\n1. Although it is mentioned that the sound models have a more targeted range of sounds that they can produce, I feel it is difficult to search the range of the sounds that a user wants to use, and even impossible if mode collapse happens. (I think this concern can be resolved to some extent if there are audio samples that are randomly sampled by GAN on the demo page.)\n2. The experiments seem a little weak to support the system. For example, I think it would be better if there are more objective and subjective evaluations conducted based on the previous sound modeling methods, even if it seems obvious considering their architectures. (e.g. what if a wavenet encoder is trained to learn time-collapsed representations?)\n3. Also, I have a slight concern about the method for measuring sound quality based on binary instrument classification. I think it would be better if there is a pair-wise subject evaluation between audios generated by SMF and GAN both in terms of quality and timbre-similarity.\n4. Personally, I have some slight concerns about the clarity of the presentation that made it more difficult to understand the approach and its motivation than I’d expect from an ICLR paper.\n5. There are questions about the way of training the RNN: (1) how the pitch condition (noted as p1 in Figure 1.) is used when training the RNN using an audio sample in the 21x21 grid?; (2) I think I do not understand it well, but is it right that the RNN is trained using the 21x21 audio samples? I thought it is a little questionable because the number seems small and not that much amount of data to take 3 days for training.",
            "summary_of_the_review": "Overall, I think this paper is novel and contributive to designing a sound modeling method. However, it seems that various experiments could be added to support their system. Moreover, it is needed to clarify the writing more for the readers to easily follow. Therefore, I give a score of 5 for this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}