{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Important: this paper has acknowledgment that breaks Anonymity. It may be ground for rejection.\n\nThis paper proposes TransTCN that introduces the attention mechanism to TCN to model long-term dependency. They claim that TCN is not good at modeling long-term dependency due to the convolution module.  They add the attention module as a path parallel to the original TCN.\nEmpirical studies are performed to show the superiority of TransTCN over TCN baseline. Their method outperforms TCN significantly.\nResults are shown on the task of Sequence modeling, including language modeling and music prediction.\n\nTheir results are unbelievable. Their simple module not only outperforms TCN but also outperforms extremely large pre-trained language models. The gap is dozens of times larger than the improvement achieved by the large pretraining model compared to TCN. Taking results in Table 3 as an example, on word-level Penn Treebank, TCN gets 87.90 ppl, GPT-3 (with the Transformer model and 150B parameters) gets 20ppl. Still, the method in this paper gets 1.33 BPC. The results are similar to other language modeling tasks. The author of this article did not notice that the improvement made in this article has greatly shaken off the pretraining mechanism based on large model big data, the Transformer model. However, they still only said to improve TCN.",
            "main_review": "Strengths:  \nThe results on language modeling are groundbreaking.\n \nWeaknesses: \n\nThe improvements are inconvincible. The proposed model is a simple combination of TCN and Self-Attention. Still, the improvements are dozens of times larger than the improvement achieved by the large pretraining transformer models compared to TCN. According to the previous progress, the improvement brought by this article may take decades to reach. But the author didn't even discuss such incredible progress, just as an improved version of the TCN model.  \nThe authors do not give theoretical evidence or understanding to support this groundbreaking result.  Their idea is not comparable to the improvement. The idea is conventional and does not bring new knowledge.\n\n\nThe idea of combining attention and TCN is not new. It was proposed in Hao et al. [1].\n[1] Hao et al., Temporal Convolutional Attention-based Network For Sequence Modeling\n\n\nAlthough the idea is simple, the writing is not clear. They do not state which module is proposed by them, why it is challenging or  of great significance,  \n\n\n\nQuestions: \n1. Since the improvements are dozens of times larger than the improvements achieved by Transformer compared to LSTM, why not apply this model to pretraining or machine translation?\n2. Do you forget to shift right the input to the decoder in language modeling?\n",
            "summary_of_the_review": "I recommend rejection since the paper is not novel, and the results are not convincing.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "1: strong reject",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new TransTCN architecture, which adds attention modules and other modifications to the original TCN architecture. Experiments on standard benchmark datasets are reported and ablation experiments were conducted to investigate different structural variations.",
            "main_review": "[Strengths]\n- The proposed architecture was designed delicately.\n- Ablation experiments are extensive.\n\n[Weaknesses]\n1. Readability is poor and heavy revisions might be needed. \nAspects of revision range from grammar, wording & typos to content incompleteness and adjustment of the organization of this paper. Some more detailed examples are given below.\n\nGrammar, wording, typos & citations\n- Improper usage of conjunctions, e.g., \"*Although,* recurrent networks extract this information via a recurrent connection, the training step also considers the temporal connection, which reduces the efficiency.\"\n- Grammar and wording, e.g., \"*However*, *a neural network* is used to extract the long-term dependency in sequential series such as BERT(Devlin et al., 2018), which is a giant network having *several* parameters, and the long-term dependency information can be memorized owing to the capability of the model.\"\n- Ambiguity, e.g., \"In addition to *the sequential issues*, Transformer can also deal with problems in the computer vision field, which was first introduced by (Dosovitskiy et al., 2020). \"\n- typos, e.g., in abstract, \"temporal connection network (TCN)\" --> \"temporal convolutional network (TCN)\"\n- Duplicated citations, e.g., \"Klaus Greff, Rupesh K Srivastava, Jan Koutn´ık, Bas R Steunebrink, and Jurgen Schmidhuber. Lstm: A search space odyssey. IEEE transactions on neural networks and learning systems, 28(10):2222–2232, 2016a.\"\n- Improper citations, e.g., a review paper is cited for TCN instead of the original paper.\n- Residual connections commonly refer to skip connections, which seems different from the usage by the authors.\n\nContent and organization\n- The first three sections in this version are a bit long and lack clarity. Authors may consider giving more space to the experiment section.\nFor example, what is the purpose of reviewing Performer or high-order ResNet? What are the connections with this paper?\n- In the related work section, readers might expect a review of recently proposed variations of TCN and a highlight of the difference between existing architecture and the proposed architecture. Examples of possibly related works are given below and the authors may judge the relevance.\n\nShi, Z., Lin, H., Liu, L., Liu, R., Han, J., & Shi, A. (2019). Deep Attention Gated Dilated Temporal Convolutional Networks with Intra-Parallel Convolutional Modules for End-to-End Monaural Speech Separation. In Interspeech (pp. 3183-3187).\n\nZhao, Y., Wang, D., Xu, B., & Zhang, T. (2020). Monaural speech dereverberation using temporal convolutional networks with self attention. IEEE/ACM transactions on audio, speech, and language processing, 28, 1598-1607.\n\nDai, R., Minciullo, L., Garattoni, L., Francesca, G., & Bremond, F. (2019). Self-attention temporal convolutional network for long-term daily living activity detection. In 2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS) (pp. 1-7). IEEE.\n\nHao, H., Wang, Y., Xia, Y., Zhao, J., & Shen, F. (2020). Temporal convolutional attention-based network for sequence modeling. arXiv preprint arXiv:2002.12530.\n\n\n\n2. Experiment results might not be credible and lack baselines and SOTA results other than TCN.\n- I looked at https://paperswithcode.com/sota/language-modelling-on-penn-treebank-word, and found their best test perplexity or validation perplexity around ~20 or ~30, even for GPT-3. Thus, I highly doubt the 1.33 perplexity reported for TransTCN. Authors please double-check this number or explain what is the difference in perplexity calculation compared with the benchmark website.\n- More comparisons with baseline networks and state-of-the-art networks are necessary. \nFor example, can the authors show that TransTCN achieves similar performance compared with Transformers while having fewer parameters? Can TransTCN outperform other newly proposed variations of TCN?",
            "summary_of_the_review": "- Current presentation does not reach publication quality.\n- Experiment results need to be further validated and comparisons with state-of-the-art networks other than TCN should be included.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new model architecture, TransTCN, to better modeling the sequential data. The model consists of three branches: the temporal branch, causal-dilated residual connection branch, and global attention branch. The authors conducted experiments on six tasks to verify the model architecture.",
            "main_review": "Strength: \n\n- The authors conducted experiments on various domains and tasks.\n\n- In general, the paper is well organized and easy to follow.\n\nWeakness:\n- The model design motivation is not clear. In the abstract, the author claims that they want to combine attention and TCN for better \nmodeling. But in the network, in addition to TCN and attention branch, a complex causal-dilated residual connection branch is also used. What is the performance if the model only uses TCN and attention?\n\n- The network is inspired by TCN and Transformer. However, the Transformer baselines are missing in the experiments. Therefore, it’s hard to decide the effective of model\n\n- In Table 3, only vanilla TCN (without causal-dilated conv) is compared, which makes the comparison unfair.\n\n- The total number of parameters is larger than TCN. What will be the results if we control the total number of TransTCN to be similar as TCN (e.g., reduce the embedding size or depth)?",
            "summary_of_the_review": "In summary, I recommend rejecting this draft because:\n-\tMore explanations about the design motivation of architecture are required.\n-\tMissing Transformer baseline.\n-\tOnly compared with vanilla TCN.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}