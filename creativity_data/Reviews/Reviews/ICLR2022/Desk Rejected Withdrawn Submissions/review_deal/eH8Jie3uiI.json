{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a method to meta learn pruning neural networks to smaller models based on the previous structures produced by pruning for similar tasks. Specifically, given a pretrained model and assuming access to a dataset of pruned models corresponding to tasks, the method first takes all the pruned models for tasks that are at a fixed similarity level to the current task. Then, each layer in the pretrained model is included with probability scaled to how many of the pruned models contain that layer.  \nThe paper performs its studies on two different models (Resnet-18/Resnet-50), two different pretraining schemes (supervised Imagenet and SSL), two datasets (subsampled classes of CIFAR-100 and Tiered-Imagenet).",
            "main_review": "**Positives**\n- The idea of using the pruning results of previous tasks to accelerate/guide the pruning of new tasks is interesting\n- The paper covered several different pretraining settings, architectures, datasets, and training regimes. \n- I thought the experiments in section 3.2 were interesting. I didn't expect such a clear, neat separation as you went deeper into the model.\n**Concerns**\n- My first concern is how the overall experiment was structured. Specifically, a dataset of pruned models was created, and then this dataset was used to find the masks for future tasks. The creation of this dataset was not included in the computational cost of using this approach, and only the cost when performing the pruning for the individual task was compared. Granted, this dataset creation cost is amortized the more tasks you need to prune the model for, but that original cost should still be included in calculations. One other interesting experiment to think about is how to perform metapruning as this dataset is being created.\n- There's an important baseline that seems to be missing to support the thesis of this work. Specifically, the paper claims that allowing models to vote *based on similarity level* is important to their results. It's unclear how true that is, however, as the layers being voted for could just be the ideal layers for all models, and maybe filtering tasks/models by similarity isn't required before voting. \n- The approach used to measure similarity for most of the experiments seems limited. In many situations where a network is being transferred/pruned, the set of classes will likely not match those in the dataset already. The similarity metric introduced for Group-III experiments is potentially a good solution, but additional experiments are probably needed to show how well it works. Specifically, how well does it match the other class based metric? How well does it work when there are actually shared classes between tasks? In general, do all the other analysis results hold with that metric?\n- The conclusions in section 3.1 needs more support. The fact that the self-supervised network did better doesn't necessarily mean that it is better in general, and for the reason claimed. For example, the results could be explained by the quantity and quality of data used to pretrain the models.  \n**Minor:** \n- The notation in the paper is hard to follow. layers, indices, and numbers are all represented with alphabet characters in the same format. Some variables were not defined ($n_l$) or defined after significantly after usage.",
            "summary_of_the_review": "I think the paper is a good potential start in terms of the exploring metapruning. I like that experiments are done in a variety of settings with respect to the pretraining, model architecture, datasets, and amount of data available. Overall, however, I think there are a few concerns that need to be addressed with the setup before the paper can claim that they are a more efficient way of doing pruning, and that all parts of their method are actually necessary to do the meta pruning.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies how pruned models of similar tasks can be best used to inform pruning for the current task. Then, it proposes a Meta-Vote Pruning (MVP) method that reduces data and time required for pruning, by initializing a sub-network from the pruned models for similar tasks. ",
            "main_review": "Strength:\n* It is an interesting direction to use pruned models for similar tasks to improve the efficiency of pruning.\n* The paper shows some promising results on how pruned models of similar tasks can reduce the time and data required for pruning, which can be practically useful.\n\nWeakness:\n* Given the first (1/2) part of the contribution is an empirical study, the analysis can be conducted more extensively. Currently, it only considers two basic ResNet-18 and ResNet-50 networks, not the recent networks, and only considers two basic datasets CIFAR-10 and Tiered-ImageNet, not a more diverse selection of datasets.\n* Experimental settings are rather limited. For one, it is currently only compared with a basic IFP -- it will also be good to compare with some reported performance of state-of-the-art pruned models. For another, it will be insightful to see more experiments on tasks with no shared classes and different domains. Currently, the tasks are quite similar -- not sure how practical it is, if we just assume all the tasks only differ in some classes but are fundamentally very similar. Examples include transferring from animal classification, age classification to gender classification, or even to semantic segmentation. Alternatively, demonstrating similar strategies work on language classification tasks.\n\n\n",
            "summary_of_the_review": "This paper studies an interesting problem and can be potentially quite practically useful. However, given that the analysis is not quite suprising (e.g. more shared filters between models for two tasks if there are more shared classes), and the method (MVP) has limited technical novelty, I think it is especially important to have an extensive analysis, and demonstrate good results that are practically useful/generalizable to a wide range of tasks. Currently, to me, the analysis and experiments are not sufficiently extensive. I'm happy to bump up the score if authors have better and more explicit justifications on how the analysis and method can be practically useful (e.g. in what scenarios).\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigates how to initialize the network to be pruned in order to reduce the pruning iterations. Through empirical study, the authors find two effective ways to initialize the network to be pruned on target task: 1) using self-supervised pre-trained network weights; 2) sampling a sub-network by voting from pruned networks of similar tasks. Experiment results demonstrate that pruning efficiency can be largely improved compared to the original pruning algorithm. ",
            "main_review": "Pros:\n1) Generally, the paper is easy to understand and follow. \n2) The authors provide valuable empirical studies, e.g. investigating the effect of supervised pre-trained weights and self-supervised pre-trained weights on the pruning efficiency. \n3) The idea of sampling subnetworks based on networks pruned on similar tasks is interesting. \n\nCons:\n1) The authors proposed using IoU to evaluate the overlap of two sets of filters. Does such overlap only mean the original filter index overlap? Even if different pruned networks share the same sets of remained filters, the weights of those filters can be different because those networks are also fine-tuned. So is it suitable to only use index overlap to measure the similarity of two pruned networks? \n\n2) For comparisons in the experiment part, task similarity is measured by the number of shared classes between two tasks (or the mean cosine similarity among matched pairs of classes from two tasks). Suppose the target task has 10 classes, task A  also aims to classify 10 classes while task B aims to classify 100 classes. If both task A and task B share 5 classes with target task, task A and task B has the same similarity with target task. But the complexities of task A and B are quite different. In such cases, should task A and task B be treated equally during the voting process? As such details are missing in the experiment part, I don't know if such cases are taken into consideration and what the results are. \n\n3) For Gourp III comparisons, can the authors provide more details about the similarity values? I wonder at what level of task similarity, the proposed voting framework can bring benefits. \n\n4) The comparisons of computation cost seem not fair enough. Usually we don't have any pruned networks on similar tasks at hand. To use the proposed method, we need to firstly prune networks on several similar tasks. Such computation costs should also be taken into consideration. If so, the proposed method may bring more computation cost. So in practice, the proposed method may not improve pruning efficiency as claimed. ",
            "summary_of_the_review": "The perspective of this paper is somewhat new and interesting. I have concerns about some technical details and the practical meaning. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a network pruning method. Firstly empirical study is conducted to compare the supervised and unsupervised pre-trained models in pruning, and to find the connection between pruned models for different pruning tasks. Then a majority voting based meta pruning algorithm is proposed. ",
            "main_review": "The proposed method looks technically correct. The empirical study well supports the algorithm motivation and the proposed algorithm is meaningful to bring performance benefit.\n\nThe meaning of Eqn 1, which is critical to the proposed algorithm is not clearly explained. I assume it is to compute a weight for each task according to the normalized class overlap with the target task. Such a definition is not general enough because it tends to ignore similar yet not exactly the same classes.\n\nOverall, the technical contribution is incremental. \n",
            "summary_of_the_review": "The paper idea is interesting. However, the proposed majority voting weight computation method has limitation, which can be improved to be more generalizable.\nThe technical novelty is incremental.\nSome important technical details are not clearly described, such as Eqn. (1).\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}