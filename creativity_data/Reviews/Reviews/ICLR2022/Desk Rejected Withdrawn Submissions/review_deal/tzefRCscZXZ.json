{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "\nThis paper uses a causal view to study adversarial robustness. There are several significant strengths and weaknesses in this paper. Specifically, this paper is quite novel, and the motivation is good. However, the proposed method is not an actual causal inference. It would be good to see my detailed comments below.\n\n",
            "main_review": "\n(Negative) The authors think that the cause is the spurious correlation ubiquitously existing in learning, i.e., the confounding effect, where attackers exploit these effects precisely. Whether this assumption is correct or not needs more strict mathematical proof. Actually, to some extent, most trainable machine learning systems are attackable due to the dualism between training a model and attacking a model. Hence, it would be good if the authors did not hold an absolute statement.\n\n(Positive) I am happy to see that the codes are available in supplementary materials.\n\n(Positive) I agree that adversarial training is currently the most promising defense against adversarial examples, and I believe there exists a more elegant method than AT.\n\n(Positive) I agree that, in few-/zeroshot scenarios, it is even impossible to collect enough adversarial training samples based on the out-of-distribution/unseen samples.\n\n(Positive) I agree that we should rethink the investigate the essential difference between machines and humans, and this should inspire the indispensable solution for adversarial robustness.\n\n(Positive) The following observations are pretty interesting: \"as a result, a model trained by associating samples with labels will recklessly use the counting of vertical edges—the confounding effect—as the indicator of digit one without learning the overall causal structure.\" The corresponding explanation is also reasonable: \"Once tampered edges are constructed, which is much easier than editing the entire digit directly, the confounding effect will mislead the model prediction as shown in Figure 1 (b).\"\n\n(Positive) The following assumptions are reasonable: \"In general, any pattern co-occurred with certain labels can constitute confounders. Most of them are even imperceptible, like local textures, small edges, and faint shadows.\" \n\n(Positive) I am happy that the authors designed a Confounded-Toy dataset to demonstrate how an adversarial attacker fools the model by exploiting the confounding effect. But there is also some weakness in the dataset. Please refer to other comments below.\n\n(Negative) The following assumption is interesting but lacks a strict theoretical guarantee: \"Intuitively, non-robust confounder patterns are local impulses that won't perform consistently across different retinotopic centers. They are either captured or not by a retinotopic observation.\" As we know, in the centers of the retinotopic observation, there are still local impulses and details. The situation is even worse when the authors use retinotopic sampling to approximate the human retinotopic observation.\n\n(Negative) I have a feeling that the robustness performance gain is from the randomness in sampling and the smoothness in sampling. My rich experience told me that randomness and smoothness do help robustness. I assume that the performance gain is not from causal inference but from randomness and smoothness. This assumption could be supported by the fact that \"forcing a model to learn features that linearly vary with the change of R can suppress unstable confounding effects,\" by the fact that \"the method combines a spatial data augmentation through retinotopic sampling with a consistency regularization loss,\" and by the fact \"the reaction of the biological visual system to different light intensities is also important, which controls the amount of light absorbed by the retina.\" The assumption is also supported by an experimental result in the ablation study section that I will comment on in the following.\n\n(Negative) The paper uses \"retinotopic sampling\" to approximate instrumental variable estimation. But I don't think this fits instrumental variable estimation perfectly. This is one of my main concerns. Traditionally, the instrumental variable should be placed in \"concept-level\" factors. But, in this paper, retinotopic sampling is imposed on the input images. This makes instrumental variable estimation an unexplainable black box, pushing it away from the goal of explainable AI. This is away from the goal of applying causal inference into deep learning. Rather than considering this a causal intervention, I tend to regard it as randomness and smoothness.\n\n(Negative) For the picture on ImageNet, the object are large, so performing retinotopic sampling is useful for removing the confounding effects. But for other datasets (e.g., COCO), some of the objects are too small. In those cases, retinotopic sampling might be useless.\n\n(Positive) I agree that a universal remedy that can proactively defend against all the known and unknown attackers is still absent.\n\n(Positive) The proposed method is indeed superior to existing methods that need pixel-wise masking supervision.\n\n(Negative) The authors use a causal view to explain adversarial attack, saying that \"Equation 1 essentially equals to maximize a tampered confounding effect through perturbations: maxδ P(Y = ˆy|X = x + δ) P(Y = ˆy|do(X = x + δ)), subject to P(Y = ˆy|do(X = x + δ)) = P(Y = ˆy|do(X = x)).\" I am not sure whether this explanation is correct. In the first term, the goal is to maximize the confounding effect. I agree with the first term. But the second term means that adversarial attack is to minimize the causal effect. I can't entirely agree with the second term. Actually, if the author read the papers of AT, attacking an AT-trained model is to encourage the causal effects but  NOT to discourage the causal effect.\n\n(Negative) The authors are suggested to show the results of AT in Figure 3.\n\n(Negative) In Figure 3, the results of the authors are different from the baseline. But it is very difficult to say that the authors' results are better because the authors' results are also unexplainable. Hence, I think the proposed dataset is not good enough.\n\n\n(Neutral) I am happy to see that mixup (Zhang et al., 2018) has some positive effects. Actually, I think appropriate randomness could benefit the adversarial robustness.\n\n(Neutral) Equation (2) is reasonable. Equation (3) is a standard instrumental variable formulation.\n\n(Negative) When talking about retina sampling satisfying the requirements of a valid instrumental variable, one assumption is that the non-robust confounders won't manifest stable patterns under different R. This lacks strict theory. To some extend, non-robust confounders might manifest stable patterns under random sampling, e.g., (dropout).\n\n\n(Negative) Another of my main concerns is that when these two losses are used for white-box attacks, could the proposed method defend against the attack? As we know, the white-box attack can utilize all possible conditions. I assume that the white-box attack in the paper only uses the cross-entropy loss. I doubt whether the proposed method is robust when adding the L1 loss into the white-box attack.\n\n\n(Negative) The visualization in Figure 8 is better than that in Figure 3. But unfortunately, it is still unexplainable.\n\n\n(Negative) It can be seen from Table 4 that the performances of retinotopic augmentation had higher clean performance but worse adversarial robustness than CiiV. Note that\nRetiAug itself can also be treated as an approximation of CiiV by assigning all α to 1.0. Besides, cross-entropy losses under different r also forced the model to ignore the non-robust confounding patterns. I understand using cross-entropy loss with this kind of augmentation implicitly approximates Equation (5). But this indicates Equation (5), one of the core contributions of this paper, only obtains a slight gain.\n\n\n(Positive) I am very happy to see the complete evaluation checklist.\n\n\n(Neutral) The following assumption seems ok but lacks strict theory: \"causal features are global structures that change consistently across different retinotopic masks while the adversarial patterns are local impulses (Duet al., 2020) that simply collapse after applying different retinotopic sampling.\"\n",
            "summary_of_the_review": "\nBalancing the strengths and weaknesses of the proposed method, I would like to recommend a rating of weak acceptance for this paper.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "\nNone.\n\n",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "# Overview\n\nThis work introduces a causal formation to improve adversarial robustness for image classification. \nThe basic idea is to incorporate instrumental variables into the causal graph model (CGM). \n\nAdvanced from the CGM in (Yang et al. 2019), the authors build upon an ontological view of semantic information and label prediction (y: outcome). The authors later provide some investigation into different features generated from their causal regularization frameworks.\nThe results showed that, virtually, the vulnerable features with the CiiV are much obvious. CiiV relies on a hypothesis that a pre-defined retinotopic mask is independent of the confounder (or confounding variable). \n\nThe authors later use the CiiV to improve the model performance against adaptive attacks and included some benchmark attacks, such as auto-attack variants. \n\n",
            "main_review": "## Pros\n1. The proposed causal format is interesting; although some parts of CGM refutation are missing in the current submission. \n\n2. The CiiV regulation shows some good empirical performance boosting for both with and without augmented adversarial training.\n\n***\n\n## Cons\n\nAs one of the preliminary attempts to causal learning applications, I think this work could extend to many directions.\n- Here I only list some crucial missings related to the technical correctness and their claim. \n\n1. The CGM is not evaluated. \n\nWhen the major technical contributions are based on the CGM with additive instrumental variable formation. \nThe author should conduct some random-placebo tests, such as used in [1, 2], to report its direct causal effects from the deployed CGM. When the authors have a causal effect estimation section in Sect. 5.1. and Eq. (3), it is uncommon to witness these insufficient refutation tests. \n\nNote this proposed CGM is not strictly Rubin's causal model; treatment effect estimation is not required. \n\n2. Does the adversarial robustness is really improved? \n\nWhen the author claims highly focus on their interests in model robustness against additive information. It is necessary to at least provide some measurement for the image classification model. That is, a CiiV regulated image classifier has a large tolerance against additive information before changing its predictions. A Cross Lipschitz Extreme Value score [3] could be one easily attained measurement to support this claim. \n\n***\n\n### Additional Comments \n\n4. I am not sure my understanding of the retinotopic mask is correct. How does the mask is created and formatted correctly as retinotopic sampling? I have read over the suggested Appendix C. \n\n- But does the nine retinotopic have some formal scientific supports and to see it is truly independent of the confounding variable?\n\nFor example, the camera and screen are still manipulated by photographers associated with some prior viewpoints. \n\n5. How does some classification activation mapping looks like for the proposed learning scheme. \n\n- How does the proposed CGM is related to the perturbed-based saliency map [4] at the intervention level in Pearl's causal hierarchy?\n\n6. It is good to have some sensitivity analysis [5,6] as LIME to understand the proposed CiiV. \n\n***\n\n**References**\n1. Causal Inference Q-Network: Toward Resilient Reinforcement Learning, 2021, ICLR SSL-RL\n2. DoWhy: An end-to-end library for causal inference, Microsoft \n3. Evaluating the robustness of neural networks: An extreme value theory approach, 2018, ICLR \n4. Interpretable Explanations of Black Boxes by Meaningful Perturbation, ICCV 17\n5. LIME, ACM 2016\n6. Model Agnostic Supervised Local Explanations, NeurIPS 2018",
            "summary_of_the_review": "Overall, I think this paper is interesting to provide an interesting direction to combine causal learning and computer vision. \nMost of the adversarial robustness and causal learning works have been well covered when some recent works are still missing. \n\n- However, to make the insufficient refutation of the CGM and independence between the retinotopic mask and confounding variables stand, the authors will need some necessary statistical refutation. \n\n- I think this is a borderline paper between 5 and 6 considering its empirical contributions. The causal formation could be less sufficient and - it is not formal. \n\nI will consider modifying my recommendation scores depending on how the weakness and additive clarification would be revised. \n\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposed an adversarial training method, namely CiiV, from the perspective of causal intervention, in order to boost the robustness to adversarial attacks. Specifically, the authors claimed that the confounding effect were easily to be exploited by attackers to attack a model. Therefore, the authors proposed to use the instrumental variable to encourage the model to learn causal features, instead of local confounding patterns. Experimental results indicated the effectiveness of CiiV.",
            "main_review": "[Strengths]\n1. This paper focused on an important topic, i.e., boosting the robustness of deep models.\n2. Various experiments were conducted to show the effectiveness of the proposed method.\n3. This paper is well written. It is easy to follow the authors’ idea.\n\n[Weaknesses]\n1. The modeling of adversarial attack using the causal framework was problematic. In this paper, the authors claimed that the underlying reason behind adversarial examples was tampering (increasing) the confounding patterns. There were two problems.\n- First, it was unclear about how to quantify the “confounding patterns”, i.e., the entity of reasons towards adversarial examples. \n- Second, using the causal framework, people could not quantify the increase of new reasons and the reduction of old reasons.\nTherefore, it was unable to interpret the adversarial robustness from the perspective of causal learning up to now.\n\n2. Equations of the proposed causal model were lack of mathematical foundations.\n- In Section 5.1, authors presented $X$ as $x = w_{cx}c + w_{rx}r + b_x$ (the last line in Page 5). There is no evidence to show that $X$ can be computed as the weighted sum of the confounder $C$, the instrumental variable $R$ and the independent component of $X$. For example, a pixel $i$ in $x$ is irrelevant to the inference. In this case, $i$ does not belong to $c$ and $r$, since $c$ and $r$ have effects to the output. Besides, $i$ does not belong to $b_x$, since $b_x$ is the independent component of $X$ (which means $b_x$ is a bias constant). Therefore, I do not agree with decomposing $x$ into the above three components. This requires a clear proof, but not a simple assumption.\n- The assumption behind Eq. (3) is totally unrealistic. In Eq. (3), the output $Y$ of the network is linear w.r.t. the input $x$. The linearity indicated that each pixel in the image is independent towards the inference. However, this conflicts with the real situation, because deep neural networks are highly non-linear systems, in which input variables does not independently contribute to the output. Therefore, I do not agree with Eq. (3) in modeling the neural networks as linear systems.\n- Eq. (4) was definitely wrong. It was claimed that the network output $Y[X=x_r]$ was linear w.r.t. the spatial coverage $\\alpha_r$ of the proposed mask. However, since the neural network is a highly non-linear system, I do not accept this linear assumption. Authors are suggested to provide a more solid mathematical foundation for this assumption.\n\n3. The retinotopic sampling mask proposed in this paper was lack of biological foundations. Please provide the evidence on the relationship between the retinotopic sampling mask and humans’ retina. Otherwise, I disagree with simulating the mechanism of humans’ retina via such a simple image mask.\n\n4. The basis of the proposed framework is wrong. Take the following mental experiment as an example. In Figure 3, if the task is not to classify the geometry in the foreground, but to classify the background, then causal features and confounding features will be totally different. However, the proposed framework CiiV does not consider this issue. Please provide details about how to determine causal features and confounding features.\n\n5. In Section 5.2, authors claimed that they used the retinotopic sampling as the instrumental variable. However, in Figure 6, authors only use the attention mask with Gaussian distribution to manipulate the image $x$ in CiiV. Authors are suggested to provide detailed proof between the human vision and the framework CiiV.\n\nMinor.\n-\tIn the line below Eq. (1), the notation $\\hat{y}=-y$ was not rigorous. How would the label be a negative number? I guess the authors would like to write $\\hat{y}=\\neg y$.\n-\tIn Figure 8, authors only provided the adversarial perturbations of models w/o CiiV. Are these adversarial perturbations still imperceptible on adversarial samples? Authors are suggested to visualize adversarial samples as well.\n-\tTraining details of this paper seem to be trick. For example, why did authors choose retinotopic centers using the 1/6, 1/2, 5/6 of width and height for each image, instead of 1/4, 1/2, 3/4 of width and height? Authors are suggested to provide more discussions on this issue.\n",
            "summary_of_the_review": "The topic this paper focused on is important. However, I do not think this study provides a good solution. Besides, I do not accept assumptions proposed in this paper, which were lack of mathematical foundations. Therefore, I think this paper should be rejected.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}