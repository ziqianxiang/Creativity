{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper addresses the problem of how to deal with stochasticity of the agents when they work with other agents. Stochasticity is assumed to happen over time, as result of degradation of the agents (e.g., robots that fail). The approach proposed learns a representation of the uncertainty and assigns to the stochastic agents tasks that are simpler and closer by while assigning to the deterministic agents more complex tasks.  A step function is used for the stochastic agents to change their behavior, which simplifies the retraining.  The proposed method (DSDF) computes a discount factor for all the agents simultaneously. The paper proposes also another method, iterative penalization method, which does not perform as well as DSDF, but is simple. The methods proposed are compared to other approaches in two different benchmark environments, StarCraft and a Foraging environments,  and show improved results.",
            "main_review": "The method proposed is described well, with an outline of the DSDF algorithm used and with details in Appendix on the parameters used.  The method is relatively simple and appears to perform well. In the StarCraft experiments, the degree of stochasticity varies from 0 to 0.6. Since the number of agents is limited to 6, and only 3 agents out of 6  are stochastic, the experiments are not fully convincing. Larger problems would be appropriate to increase the quality of the experimental results presented.  \nIt is not clear how important the problem is in real applications and if the solution proposed will work as well in real applications.  How important is to address the proposed problem is also not clear. In the robotics example shown the proposed approach is useful, but still it remain to be seen if the approach scales up to real environments and problems,. The choice of a step function to compute the discount factor makes the approach less realistic. It would make sense to look at how robots tend to degrade and apply them, to see if they provide any new insight .\nFirst line of Section 3: \"more optimal joint policy\" is not quite right, since nothing can be better than the optimal policy!",
            "summary_of_the_review": "A method is proposed to automate the computation of the discount factors for robots executing a task, in cases when some robots show stochasticity. The discount factor is related to how well the agent is performing the task, which depends on the proper functioning of the robot's hardware.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No issues",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the problem of centralized multi-agent training. The paper claims that a problem that arises in multi-agent training is when agents slowly become more stochastic over time (aging robots). To address this issue, the paper proposes to learn separate discount factors for each agent in a way that considers the discount factors for other agents.",
            "main_review": "The problem motivation is rather unconvincing. Sure, there can be drift along many axes over time in multi-agent training (non-stationarity, swapping of roles...), so I'm confused as to why the paper focuses on this very narrow problem of increased stochasticity. A lot of multi-agent domains are AI agents that don't decay over time. Even in robotics settings, is there any support for whether deteriorating components will affect *collaboration* (I can see it affecting control mostly, but not so much collaboration)?\n\nOn a related note, it seems excessively narrow that the paper is only focusing on learning separate discount factors for each agent. There seems to be many possible ways to address the proposed problem, the most natural of which seems to be modeling the stochasticity of the other agents directly!\n\nAnother question I have is, Section 3.1 title is \"...Calculate Optimal \\gamma\", but the proposed solutions all seem to be heuristics (\"with every mismatch we will decrease the discount factor\"). Why does the paper claim to be calculating the optimal \\gamma? Is it even feasible (computationally) to compute the optimal \\gamma?\n\nLastly, the writing is really, really poor, and is littered with grammar mistakes, and incomplete sentences. I find the writing unacceptable for a scientific publication.",
            "summary_of_the_review": "The problem motivation is unsatisfactory, since the problem seems too narrow (only focusing on decay in stochasticity) and unrealistic, and the proposed solution is also unnecessarily narrow (only looks at tweaking discount factors). None of these choices are justified particularly well, so the overall contribution is really difficult to pinpoint. Lastly, the writing needs to be greatly improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose a method to handle stochastic (noisy) agents by tuning the discount factor during training. Agents that are stochastic learn to use a lower value of the discount factor which in turn changes their RL objective to one of a shorter horizon.",
            "main_review": "At its current version, the paper has major issues which are detailed below.\n\n- The overview of the existing literature is lacking.\n\t- Issues with the literature description begin in the first paragraph: e.g. (Foerster et al, 2018a) is not learning a centralised policy (and it is the same citation to Foerster et al, 2018b)\n\t- Paragraph #3 discussing CDTE is unfinished\n\t- Literature discussion focuses only on CTDE methods. CTDE is an established paradigm and there should not be a need to motivate it. I would expect to see at least a discussion on stochasticity, and on changing the discount factor during training.\n\t- I believe the paper would benefit from a \"Related Work\" which can help place it in the existing literature.\n\n\n- The use of the term \"stochastic/deterministic\" agent is confusing.\n\t- Stochastic policies have a different meaning in RL. There are even cases where a stochastic policy can be an optimal policy (e.g. rock-paper-scissors). I believe the authors mean to introduce noise to the actions of some agents, but the term deterministic/stochastic is not correct. Even in the case of the paper's experiments, the policies followed by the \"deterministic\" agents are actually stochastic since they use epsilon-greedy.\n\n- The method tunes the discount factor\n\t- In the background section (and in the literature) the discount factor is treated as part of the Dec-POMDP definition. Changing this factor directly changes the nature of the problem and influences the learning objective. I would be open in discussing if this is a valid approach in the first place, but I would at least expect a discussion of the literature on the subject.\n\n- The experiments \n\t- SMAC/2s3z is a relatively simple environment. The original unmodified version is solved by most algorithms including QMIX/IQL (as shown in Papoudakis et al, 2021). By adding noise to the actions of the agent the environment becomes harder: but how does, in practice, the optimal policy change? I would expect the agents still need to focus their attacks on specific enemies, (and the noisy agents should do their best to also focus their attacks). However, in QMIX/IQL the agents do not seem to learn to solve this environment. Does DSDF learn a different behaviour than the one I described?\n\t- I disagree that LBF \"does not require much collaboration...\" (caption Fig4). LBF requires agents to move together towards a food item and simultaneously try to load it. There is no reward if the agents do not manage to do this. In contrast, SMAC (especially easier tasks such as 2s3z) is much more lenient in agents not collaborating for a certain amount of time (an agent moving further away, an agent trying to independently kill an opponent, etc).\n\t- I would expect multiple seeds and plots that include std across seeds. The argument \"In this case study, we skipped doing multiple experiments as we thought they are redundant\" (page 6/first sentence) is not convincing. Especially in RL/MARL where different seeds can lead to significantly different results.\n\n",
            "summary_of_the_review": "There are a few major issues with the motivation of the method which leads me to recommend the rejection of this paper.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper tunes the discounted factor for all agents by using a learning representation of uncertainty to update the utility networks of individual agents. Results on two environments shows that the performance of the proposed method is better than SOTA methods.",
            "main_review": "The paper proposes an interesting idea that different agents should use different discounted factors. Although this idea is already proposed in the single agent setting, varying discounted factors in multi-agent setting is still interesting and novel in my opinion. I have some concerns listed as follows.\n\n1. In the introduction part, authors use a soccer match example to show that agents would change their policies when their abilities change, which is attractive and reasonable. However, in the method and experiment parts, the problem is not mentioned or addressed. More specifically, the randomness of agents is fixed in the experiments and the method does not mention when to retrain the network that outputs discounted factors.\n2. I do not understand why you assume that the policies are deterministic. For the stochastic policy, the problem that some agents would change their policies when their abilities change also exists. Is it possible to use the same methods for stochastic policies?\n3. For the experiments, some details are ignored. How many runs are used to generate figures is unknown. The variance of the performance is not given. The seeds used in the experiment is unknown. The discounted factors of baselines are not provided.\nThe lack of details and codes make the paper hard to be reproduced.\n4. Stochastic level of agents is fixed. It is not convincing by conducting experiments in one setting. I suggest that more settings should be considered or randomly generated.\n",
            "summary_of_the_review": "In summary, \n\n1. The setting of the method and the experiment is not the same as the authors claim in the introduction.\n2. The details of experiment are not provided. The results are not convincing.\n3. The Stochastic level of agents is fixed and designed by authors, which is not convincing.\n\nOverall, the idea is interesting and novel. However, the experiment and method still need to be improved.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}