{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper has developed a framework for contrastive learning through time (CLTT). Instead of using augmentation operations to create positive pairs for contrastive learning, this paper creates several datasets with a near-photorealistic training environment by changing different view directions. The temporally close video frames of the new data are thus similar and should align in representation, which can naturally be considered as positive data. \n\n",
            "main_review": "Pros: Overall, I think the idea is interesting and worth exploring. \n1.\tThe problem description and aim of the paper are very clear. \n2.\tMimic the human infant that keeps gaze on the same object for a certain number of fixations and then redirecting to other objects in the learning process could learn good representations linearly separable that approaches that of the fully supervised learning.  \n3.\tThe authors demonstrate the performance of their approach on three separate datasets including a simulated dataset, a biological-related dataset, and a real-world dataset. \n\n\nCons: \n1.\tThe baseline method compared in this paper only contains the supervised learning method. However, it is necessary to compare with the conventional contrastive learning counterpart, i.e., SimCLR, RELIC, BYOL. For example, take each frame’s several augmented views as the positive samples, and frames in other videos or temporally far from the current frame within the same video as negative samples. I’m curious about whether the augmentation operations are better, or the simulated views are better.\n2.\tThe sliding window contains N_fix – 1 intra-class pair and one inter-class pair, which means in the process of constructing the sliding window, the image object labels are required. In this circumstance, the supervised contrastive learning method should also be considered as a baseline.\n3.\tIn Figure 3B, the N_fix equals 30 performs the best with SimCLR-TT and BYOL-TT. However, the SimCLR-TT and BYOL-TT curves still look increasing. What if N_fix > 30? Will the performance continuously increase or will decrease at some point? What is the best number of N_fix? Also, are the SimCLR-TT and BYOL-TT curves increasing due to the reason that the number of N_fix in a sliding window increase (i.e., have more positive samples) or the fraction of inter-class pair in a sliding window decrease (i.e., 1/N_fix decrease when N_fix increase)? To explain the behavior, I think other experiments should be conducted, keeping the number of N_fix but various the fraction of inter-class pair in a sliding window, and keeping the fraction of inter-class pair but various the number of N_fix in a sliding window.\n4.\tWhat if the sliding window does not contain any inter-class pair? i.e., all the N_fix samples are the intra-class pairs.\n5.\tAs for the experiment in Section 4.1.2, what is the sequence shift strategy? Is it the random sequence or fixed sequence method? Why is the N_fix equal 5 the best in the COIL-100 dataset while N_fix equals 30 the best in the ThreeWorld dataset? What do you think is the reason?\n6.\tIn Figure 5 A and B, the fixed sequence experiments on Fractals and TDW. Can this generalize to the real dataset COIL-100? Do these results depend on the pre-fixed order? How about changing to other different sequences? For example, the next object can be spatially close to the previous one in a room (book and desk, the book is on the desk), or semantically close to the previous one (dog and cat, both dog and cat are animals), or just randomly selected then fixed. Is there a better order than the others?\n7.\tThe Miyashita-style dataset is constructed with transformation, which looks the same as data augmentation in conventional contrastive learning. What is the difference or advance compared with conventional contrastive learning counterpart?  \n8.\tIt is worth exploring with the first-person video or ego-centric video to do the experiments.\n",
            "summary_of_the_review": "In general, I think the idea of mimicking the human gaze on different views of objects and then transit to the other object is interesting. Conceptually, these kinds of different 3D views are more natural than the augmented views in conventional contrastive learning. However, the current paper lacks comparison experiments with conventional contrastive learning counterparts. It is possible that the augmented samples of conventional contrastive learning are more diverse, and perhaps its learned model is more robust. Also, the ego-centric or first-person video datasets may be more suitable for the motivation of this paper.\n\n\n++++++++++++++++++++++++++++++++++++++++++++++++++\n\nUpdated review after the rebuttal\n\n++++++++++++++++++++++++++++++++++++++++++++++++++\n\nI would like to thank the authors for their response and revised paper. The revised paper is clearer than the original submission in some respects. For example, I agreed that it clarifies the method is unsupervised without any implicit supervision, where exactly all adjacent samples form positive pairs, no matter if they show the same object or not.\n\nHowever, I still disagree with the authors regarding point 3. I think for the current submission, comparisons with the standard augmentation operations are necessary. Moreover, the experiments show that uniform sampling is better than random walk sampling since it creates more diverse views than the random walk sampling strategy. For uniform sampling, the two adjacent images can be the same object, but the augmentations are more random rather than a continuous change (corresponds to the random walk sampling). Then, as for my understanding, the uniform sampling can be considered as a kind of augmentation without considering the temporal structure. That is, the positive pairs are two samples from the same object with random augmentations, whereas the negative samples contain N_fix – 2 augmentations from the sample object and (N_obj – 1) * N_fix samples from other objects. Of course, there are some probabilities (1/N_fix) that the positive pairs are from two objects. But if we just consider the intra-class case with uniform sampling strategy, it is very much like standard augmentation methods, except that the negative samples contain N_fix – 2 augmentations from the sample object. Therefore, the comparisons with standard augmentation are necessary, otherwise, the contribution of this paper with considering the temporal structure is not clear.\n\nIn summary, I increase my score a little based on the clarification of the unsupervised setting, but I’m still concerned about the lack of important baseline comparison.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, author introduce an image-based contrastive learning framework which substitutes traditional data argumentation by using successive images along with the temporal dimension. A new dataset is also generated in this paper for better studying the topic. Extensive experiments demonstrate that the proposed method is able to learn as good feature representation as supervised learning  in some datasets.",
            "main_review": "Strength:\n1. the paper is well written and the core idea is clearly presented.\n2. extensive experimental results show the effectiveness of the proposed method.\n3. biological findings are interesting\n\nWeaknesses:\n1. As a computer vision paper, the technical novelty is limited. The self-supervised learning pipelines are all from previous works. The idea of CLTT may be novel in the image domain, but it has already been explored in the video domain. Similar work as [1,2]. \n2. 'Specifically, we divide the sequence of views into windows where the object identity stays the same'. In the SimCLR-TT, I am not sure how to produce such window that could keep the object identity stays the same. Subsequent images are likely having the same identity, but there would be 'gap areas' that produce mixed label. \n3.The proposed method is only evaluated in some small scaled datasets. It would be interesting to extend it to a large scale benchmark.\n\nSuggestion:\nIt would be great to include vanilla SimCLR, BYOL and RELIC in the experiment part to show the effectiveness of proposed method.\n\n[1] A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning, CVPR 21\n[2] Broaden your views for self-supervised video learning, ICCV 21",
            "summary_of_the_review": "Considering the weakness mentioned above, I would vote reject.\nThe technical novelty is the major concern of mine.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new method to select positive pairs for contrastive learning: using adjacent frames in a sequence, referred to as Contrastive Learning Through Time (CLTT). This is relevant since CL approaches have been shown to depend a lot on what kind of augmentation is used. The method is integrated with a number of well-known CL approaches, and evaluated on three different datasets, two of which are novel --the method is shown to approach fully supervised results. The paper also provides a valuable discussion on how the notion of time can help representation learning. The method is also tested for latent representation similarity when one object or pattern is systematically shown after another, something which has been studied in the field of biological learning, and a similar trend is found here as well. ",
            "main_review": "Strengths:\n\nS1: The main idea of CLTT has not been evaluated before, it is well-motivated and therefore should be explored -- it will be of value to the vision community.\n\nS2: The approach is evaluated on three different datasets.\n\nS3. The paper constitutes a nice first step in the direction laid out, and although the datasets are somewhat artificial, this has the advantage that more could be controlled for in the experiments -- as stated by the authors, future work can build on this and try with noisier types of data.\n\nS4: Repeated runs and their average and variation are included throughout the paper.\n\nS5: Limitations of the approach are listed in the discussion.\n\n\nWeaknesses, major:\n\nW1: The contribution relative to Orhan et al. is not sufficiently discussed. In Sect 2 it is stated that their objective can be considered a specific instance of CLTT, but how much broader is CLTT, then?\n\nW2: In Figs 3,4,5 the test accuracy is (presumably --this relates to Q2) evaluated after each epoch. This seems very much to me like using the test set for model selection. It would be good if a final, completely held out set was used for any results presented.\n\nWeaknesses, minor and should be amenable:\n\nW3. I am missing discussions of some things (listed here below), which makes the article, in its current state, a bit more lightweight than desirable.\n\nW3. 1: I’m missing a motivation for the “mixing view” in SimCLR-TT. Why is that a good thing? To me, it just sounds like adding noise -- maybe that is the purpose?\n\nW3. 2: Missing a discussion of why Nfix=5 is the best setting in Fig4B.\n\nW3. 3: From 4.1.1 “Interestingly, while for RELIC-TT and SimCLR-TT greater Nfix improves performance, this is not the case for BYOL-TT.” this is never discussed -- would be useful for the reader.\n\nW4: Some things can not really be understood from the paper right now. If I understood it correctly, the construction of windows is also the same for RELIC-TT and BYOL-TT? This seems very important and is not clear. This should be easily amenable. Also, see questions further down.\n\nW5. Top of page 8, “This experiment illustrates a fundamental property of CLTT that can be directly related to biological findings”, is a very strong claim and should be toned down. It does evoke the thought of the biological findings, but in my view the paper has not shown a fundamental property of CLTT that is *directly related* to bio.findings, but it might be associated with those. We should be careful with these types of claims in ML.\n\nW6. Parts of the top paragraph of page 9 are a bit vague, notably “or purely internal “thoughts” into more abstract representations, thereby laying a foundation for (more) abstract thought.”, this gives less confidence in the text, because it sounds a bit speculative. Make more clear.\n\n\nDetailed suggestions:\n\nD1. It would be nice to see Fig 3C and 4C for the supervised case, as a comparison.\n\nD2. Section 4.2.1, “fractals that were”  >> “fractals that are”\n\nD3. N_{FIX} has not been introduced in the main text (only vaguely in the caption of Fig 1 when it is mentioned in 3.1.1, this could be made more clear. The real introduction only comes in 3.2.1.\n\nD4. Sect 3.1.3 “minimizes the similarity”, should it be “maximize” instead? Since they are augmented versions of the same data? Or minimize the distance (aka maximizing similarity)? \n\nD5. Maybe section 4 should be called experiments instead, since it contains some method parts? For example 4.1.1 is quite heavy on methodology rather than results.\n\nD6. 4.1.2: rather th*a*n computer rendered\n\nD7. There are no error bars in Fig 5A, but the caption talks about error bars. Move that part of the caption to 5B?\n\nD8. I found it difficult to know upon the first read when you are really referring to frames that are *temporally* close, in the video frame sense, and when you mean it in a general sense, for any sequence (which includes the Miyashita case). These are both related, and principally different, in my perspective, and I think the paper would benefit from making this more clear, for example reflected in (sub)section headings.\n\n\nQuestions:\n\nQ1. What is the relation between the window size of W(i) compared to N_{FIX}? We never hear how many frames are in a window.\n\nQ2. Fig 3A,B,4A,B, it is never stated whether this is accuracy on held-out validation data. Is it? Quite an important point\n\nQ3. Does a lower Nfix mean less training data? Or are there just more smaller windows then?\n\nQ4. 4.2.1, 10^6 stimuli presentations is ambiguous, what does this mean? 10^4 repetitions of the 100 unique fractals? Where each repeated fractal is differently augmented each time?\n\nQ5. Why is BYOL-TT chosen for 4.2.2? it seems a bit arbitrary, should be motivated.\n",
            "summary_of_the_review": "The paper will be of value to the vision community, and to the field of contrastive learning, since this type of augmentation is such a natural choice. Strengths S1-S3 shows that this paper does what it sets out to do, and raises an interesting point in that temporally close frames should be aligned in the latent space. If the authors can take the detailed suggestions into account to clarify the paper, and address W1/W2, in my view, the paper will be ready for publication. The major weaknesses listed are amenable, so the positive outweighs the negative here.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose to perform self-supervised learning (SSL) by enforcing temporal consistency (as opposed to augmentation consistency in most recent efforts). They do this by adapting existing SSL approaches (SimCLR, RELIC and BYOL) to leverage video frames that are close in time as opposed to different augmentations of a given image (akin to a temporal augmentation). In addition, the authors introduce two new datasets which they use to train and study the learnt representations in detail, showing promising results.",
            "main_review": "This paper is well-written, easy to follow, and has a reasonable experiments/analysis section. The main idea explored here (temporal consistency) is intuitive, although not particularly novel.\n\nHowever, I have significant concerns regarding the precise way in which it is implemented. As I understand it, going off of the SimCLR-TT description in page 3, the authors divide a sequence of images into \"windows\" where the object identity is constant. They then form positive parts within each window and negative pairs across windows. To me, this seems like a clear form of supervision, in that one needs object identity labels to define the relevant windows. In other words, given an unlabelled sequence of images, it is not clear how one should form the windows required to train SimCLR-TT and RELIC-TT. If my interpretation is correct, then the approach proposed here is not self-supervised learning at all, but rather a form of supervised contrastive learning, akin to what is frequently used to train embeddings for object tracking.\n\nThe BYOL-TT variant should suffer less from this issue, since it does not require explicit negative pairs. In page 4, the authors state that positive pairs are formed by taking adjacent images in the timeline. What is less clear is whether the authors explicitly avoid false positive pairs, i.e., whether they make sure that these adjacent frames always belong to the same object. Looking at Figure 3 and Figure A.1, it seems like BYOL-TT works best when the number of frames per object is small (performance is better for 5-10 frames compared to 30 frames). Given that a smaller number of frames leads to a higher ratio of false positives I wonder if the authors might indeed be avoiding the inclusion the false positives, but this is speculation on my part.\n\nWorryingly, the results section suggest that SimCLR-TT and RELIC-TT, which seem to implicitly use supervision, perform better than BYOL-TT. (In fact, the authors don't report BYOL-TT results for a few experiments.)\n\nStill on the issue of how positive and negative pairs are formed, in the last paragraph of section 3.2.1 the authors seems to suggest a different strategy for forming these pairs but only positive pairs are discussed. It would be helpful to bridge this paragraph with what is described earlier in the context of each variant. For example, in SimCLR-TT, all frames within a window constitute positive pair but in this paragraph only adjacent frames can constitute positive pairs. The caption of Figure 1 also assures the reader that the approach is fully unsupervised and seems to suggest a similar strategy as section 3.2.1 but again doesn't address the negative pairs required for SimCLR-TT and RELIC-TT.\n\nFinally, while I generally enjoyed the experiments section, I think it would be helpful to include comparisons with the standard variants on the methods proposed (SimCLR, RELIC and BYOL). How much does one gain by explicitly incorporating temporal structure vs using more naive augmentations?\n\nI also found the results in Figure 4 a bit puzzling. When using the random walk sampling strategy (which produces sequences more akin to a video stream), performance at the end of training was similar (maybe even worse) than it was at the very beginning. It would be interesting to see what the LLS Accuracy would be on representations extracted by a randomly initialized network. Performance does improve substantially using the uniform sampling approach, but this approach seems to also implicitly include supervision in the sense that it uses collections of completely different views of each object grouped together. It seems hard to image how one would collect such a dataset without having access to object labels.\n\nAll this said, it is entirely possible that I misinterpreted something about this approach, in which case I think the manuscript would greatly benefit from explicitly addressing the concerns above as I suspect other readers might raise similar questions.",
            "summary_of_the_review": "++++++++++++++++++++++++++++++++++++++++++++++++++\n\nThe score has been reviewed based on the author's response.\n\n++++++++++++++++++++++++++++++++++++++++++++++++++\n\nWhile the main idea explored here is interesting and intuitive, it is not clear to me that the approach described here represents a form of self-supervised learning (or more generally unsupervised learning), as it implicitly seems to leverage object labels. Regardless of whether or not this is the case, I believe the manuscript would greatly benefit from more clearly discussing this issue.\n\nIf the main claim is not that the proposed approach allows one to learn without labels but rather that using contrastive-style losses is a good idea, the results section also seems to negate this, as the supervised baselines reach better performance that the contrastive-style approaches.\n\nAll this said, this is otherwise a nice piece of work. If the authors can make a convincing case that SimCLR-TT and RELIC-TT training is fully unsupervised I would be comfortable raising my recommendation score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}