{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Authors study the generalization error through the notion of weight expansion. It is shown that a weight expansion reduces the generalization error and that dropout is an efficient method that increases the weight volume. Authors also argue that weight expansion should be regarded as the cause of better generalization.",
            "main_review": "Strengths\n* The manuscript is mostly well written. The arguments made by the authors are stated clearly.\n* Introduce a new concept (weight expansion) to characterize generalization error.\n* Provide experiments to support the technical claims.\n* The weight expansion concept could open new questions worth studying.\n\nWeaknesses\n* Minor: Proofs are based on well-known techniques so there is not much novelty in this case.\n* Major: Authors claim that \"weight expansion should be regarded as the cause for the increased generalization capacity\" but there is no formal proof of this in the paper.\n* Moderate: Lack of discussion about the effect of weight expansion on the error upper bound.\n\n",
            "summary_of_the_review": "After my first read of the paper, I am recommending weak acceptance. It would help if the authors can comment on my following concerns:\n\n1. Authors have shown \"relationships\" between weight expansion and generalization error. However, what is been shown is not a formal proof of causality, which is an overstating of the contributions. Related to this I have two questions:\n    * In the equation right before eq.(5), we have in the numerator of $\\ln$ the product of the diagonal entries of the covariance matrix from the prior $P$, not $Q$! Thus, that is not precisely the weight volume according to the definition of weight expansion in Def 3.1. Either this is a mistake, which would badly hurt the paper, or there is an implicit assumption that could also hurt the contributions depending on how strong that is.\n    * Authors disregard other terms regarding the weights. In eq.(5), the volume is not the only quantity that changes. Why is the norm not being considered? How do we know that perhaps that plays a more critical role?\n\n2. Can authors comment on the numerical effect of a larger weight volume? For instance, it looks like if another method could double the weight volume the numerical value in the error upper bound could be not really significant.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes the \"weight volume\" as a new metric for neural network model selection. To compute the weight volume, one should first consider a vector of random variables that take values \"around\" the weights of the neural network without changing its outputs. Then the weight volume is measured from the normalized determinant of the covariance matrix. The paper shows mathematically that increasing the weight volume decreases a PAC-Bayesian generalization bound, and (2) the dropout method naturally endeavors weight volume expansion. Then, the paper presents two methods for estimating the weight volume of a neural network (which is mandatory as neural networks commonly have a large number of parameters). Finally, extensive experiments show empirically a clear correlation between the introduced metric and the generalization gap of neural networks,\n",
            "main_review": "I enjoyed reading the paper as it is based on a clever idea, and contains several interesting insights (listed in the summary of the paper above). However, the current manuscript has several shortcomings. \n\n(1) First of all, despite being intuitive, the definition of weight volume in Section 3 is unclear about the very nature of the considered random variable. I understood reading the paragraph \"Sampling method' of Section 4 that, given a neural network parametrized by weight W, the random variable takes values \"around\" W since that the neural network outputs are almost unchanged. The precise random variable considered by the \"Laplace approximation\" is unclear to me.\n\n(2) The analysis of the weight volume through the PAC-Bayes framework is interesting. However, the authors present the PAC-Bayes literature as there was nothing done in that field between the early results of McAllester (1999) and the neural network approaches of Dziugate & Roy (2017) and Neyshabur et al. (2017). This is problematic for at least two reasons:\n\nA. This disregards important early works that develop ideas reused in the current paper. Importantly, the parametrization of the PAC-Bayes bound using Gaussian distribution has been introduced by Langford and Caruana (2001), Seeger (2002), Langford and Shawe-Taylor (2002) and promoted by many others (e.g., Germain et al., 2009, Parrado-Hernández et al. 2012, Alquier et al. 2016). In this regard, giving the credits to 2017-2021 papers for the classic PAC-Bayes bound of McAllester (Eq 3) and the use of Gaussian distributions (Eq 4) is inadequate.\n\nB.  This prevents the authors from exhibiting relations between previously explored idea and their own. For instance, the idea of \"expending the volume\" while preserving the predictor output is reminiscent of a strategy used in the simplest linear classification setting; that is, one can multiply the weight vector norm by a constant to obtain tight PAC-Bayes generalization bounds, while preserving the output of a (potentially kernelized) linear classifiers (Langford and Shawe-Taylor 2002, Germain et al., 2009, Parrado-Hernández et al. 2012). Also, Alquier et al. (2016) optimize a PAC-Bayesian bound using a Gaussian posterior with a full covariance matrix.\n\n(3) Beginning with the abstract, the text mentions several times that \"weight expansion should be regarded as the cause for the increased generalization capacity\". This seems an (enthusiastic) overstatement, as the paper shows a correlation more than causality, and the supplementary material shows that other common regularizers - dropout and batch normalization - do not tend to increase the weight expansion (this is elusively mentioned in the paper). While the paper presents an extensive empirical study exhibiting a link between weight expansion and generalization, it does not explain the mechanisms linking both phenomena. Intuitively, I would guess that the weight expansion metric captures a notion of margin or predictive stability, the latter being the cause of generalization. Similarly, explaining the SVM, I would not say that promoting a small weight vector norm is the cause of its generalization ability; however, a small vector norm translates into a large classification margin, which improves generalization.\n\n(4) While the empirical results are promising, a more fine-grained analysis would help understand better the role of \"weight volume\". One unanswered question is the effectiveness of the metric depending on the architecture choice: Is it as informative for relu or tanh activations? Is the weight volume capturing the adequacy of CNN layers? Another relevant experiment would be to check if the existing PAC-Bayesian bound optimization procedures for neural networks (e.g., Dziugate & Roy, 2017, Letarte et al., 2019, Pérez-Ortiz 2021) naturally promotes large weight volumes.\n\n Other comments:\n- Figure 1. The procedure to generate plots (b) and (d) needs to be explained.\n- Section 4: The sentence \"For CIFAR-10, we let $\\epsilon=0.05$...\" should be deferred in the experiments section.\n- Section 4.1: Does the notion of \"disentanglement noise\" come from the literature? If so, please provide references.\n- Table 2 results: The generalization gap (GG) metric needs to be properly defined in the main paper, notably the purpose of $\\omega$. It is crucial to interpret the results.\n\n### References\n- Alquier, Ridgway, Chopin. On the properties of variational approximations of Gibbs posteriors. JMLR 2016.\n- Germain, Lacasse, Laviolette, Marchand. PAC-Bayesian learning of linear classifiers. ICML 2009.\n- Langford and Caruana. (Not) Bounding the True Error. NIPS 2001.\n-Langford and Shawe-Taylor. PAC-Bayes & Margin. NIPS 2002.\n- Letarte, Germain, Guedj, Laviolette. Dichotomize and Generalize: PAC-Bayesian Binary Activated Deep Neural Networks. NeurIPS 2019.\n- Parrado-Hernández, Ambroladze, Shawe-Taylor, Sun. PAC-Bayes Bounds with Data Dependent Priors. JMLR 2012.\n- Pérez-Ortiz, Rivasplata, Shawe-Taylor, Szepesvári. Tighter Risk Certificates for Neural Networks. JMLR 2021.\n- Seeger. PAC-Bayesian Generalisation Error Bounds for Gaussian Process Classification. JMLR 2002.\n",
            "summary_of_the_review": "The paper is based on a clever idea, and contains several interesting insights. However, the current manuscript has several shortcomings.\nNotably, the literature review on PAC-Bayes learning is clearly inadequate.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the important problem of answering the question \"Why does drop-out help with generalization\"? The answer presented in this paper comes in two stages by relating drop-out to weight expansion and weight-expansion to generalization. They show both their claims are theoretically supported and then experimentally validate the findings on various applications that are of interest. Their findings are not entirely theoretical however, as they lead to a new method via weight expansion.",
            "main_review": "As mentioned in the summary, the main strength of this paper stems from the importance of studying drop-out theoretically. A theoretical understanding of dropout would serve as a substantial contribution to the machine learning community at large since it expands our impression of how deep learning methods are generalising and can be further improved in practice.\n\nThis is evidenced by the comprehensive contribution in this work where a new method for regularization is presented, referred to as weight expansion. While the method is not entirely actionable as a new method, the connection at least theoretically is sufficient to warrant as a novel discovery.\n\nThis work discusses all related work that is relevant to the best of my knowledge and compares to the right level of depth against any competing methods or possible explanations of drop-out. \n\nThe level of clarity of this paper should also be commended as they use bold text thematically throughout the paper that makes it clear of the two layers of contributions (weight expansion and generalization) which makes the experiments much easier to read. \n\nI only had one question that I would like to be clarified regarding the further developments coming from this work that involve the study of drop-out or weight expansion in other areas of machine learning. It seems the paper does not explain completely what kind of future developments would benefit and while it is implied, I believe the paper would benefit if written down and expanded upon in the conclusion section. For example, they may mention that this work paves the way to explore regularization in other areas such as in generative models.",
            "summary_of_the_review": "I believe the paper should be accepted for presentation due to the importance of the problem tackled and the exposition between weight expansion and generalization. Therefore I vote for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed to understand dropout by introducing a new concept, weight volume. The authors show that: 1. weight volume can effectively capture the generalization ability; 2. dropout expands the weight volume and thus helps generalization.",
            "main_review": "Pros:\n- This paper introduces a novel and interesting quantity, weight volume, to capture the generalization ability.\n- The  PAC-Bayesian analysis is well suitable in this scenario which gives an insightful characterization of why weight volume can indicate generalization ability.\n- The empirical results of replacing dropout with disentangled noise appropriately verify that weight expansion can help generalization.\n\nConcerns:\n- The major concern is about the experimental results. The authors showed results on several vision datasets with VGG and AlexNet-like models but didn't report some commonly-used metrics like accuracy. This makes it unclear if the results are reliable. Also, more experiments on other domains, e.g., NLP, are encouraged as dropout is more frequently used in NLP. I'll increase my score if this concern is well addressed.",
            "summary_of_the_review": "The idea of using weight volume to capture generalization and explain the effect of dropout is novel and interesting, however, more justification on experimental results is needed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}