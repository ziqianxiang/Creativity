{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents an empirical study to analyze the importance of architectural components for norm ratio (Raghu et al., 2021) and resulting FID. Importance of feature locality, residual connection, token extraction method are analyzed. Due to these bug fixes, the proposed method is comparable to state-of-the-art GAN architectures on a number of datasets.",
            "main_review": "[Strengths]\n- Using norm ratio (Raghu et al., 2021) can be a useful way of understanding the instabilities of Transformer-based GANs. It was never used in analyzing Transformer GANs in prior literature.\n- The proposed method seems to be very strong in low-resolution datasets (based on the results on CelebA 64x64 and CIFAR10 32x32).\n- Experiment analysis and ablations are quite extensive.\n\n[Weaknesses]\n- Insufficient discussion to prior work:\n1. In \"Empirical Strategies for STrans-D\": the ideas of overlapping patches and equalized learning rate were first proposed in ViTGAN (Lee et al. 2021). However, it was not mentioned.\n2. In \"Skip-Projection\": The idea of changing the normalization position was first tried in StyleFormer (Park & Kim, 2021). However, it was not mentioned.\n- Problem with CelebA comparison: There are 2 versions of CelebA, namely aligned and cropped. This paper and TransGAN used cropped, whereas ViTGAN used aligned. Aligned and cropped are essentially 2 different datasets and cannot be put together in one table.\n- Transformer based discriminator is still underperforming compared to StyleGAN2-D according to Figure 7. But this is a minor problem as I believe solving making Transformer-D match the performance of StyleGAN2-D is still very challenging.\n- Parameter count to compare architectures is not that meaningful. How does FLOPs compare between different method?",
            "summary_of_the_review": "On one hand this paper is not particularly exciting. Technical components of the method were existing from the literature and it is hard to the method outperforms StyleGAN2-ADA in terms of its FID score. However, I believe existing Transformer-based GAN methods have the same issues. Transformer-based GANs failed to demonstrate that they outperform CNN-based counterparts. This shows the difficulty of using Transformers for image GANs. And existing literature is really limited in terms of understanding why Transformer GANs fail. Although the proposed method does not bring a practical significance in its current form, I believe the analysis techniques used in this paper may inspire future work on Transformer based GANs. Hence, I hope to see this paper in this conference.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper describes transformer-based generator and discriminator architectures for GANs, with global attention modules in low resolutions combined with local attention Swin blocks for higher resolutions. Conditional ImageNet generation results are presented. Some state-of-the-art results are presented for the transformer generator combined with CNN discriminator. ",
            "main_review": "Strengths\n\nThe paper describes an architecture that achieves SOTA FIDs and ISs on some generation tasks. Conditional generation is described as a first in the transformer GAN nice. I found the analysis of signal magnitudes during training relatively insightful as a means of guiding architecture exploration.\n\nWeaknesses\n\nI feel most of the argumentation behind the architecture is based on vague intuition that is presented as something more than it is. For instance, the attention distance results presented in Fig.3d: why would larger distances in themselves be an issue? The claim that the \"unpleasant artifacts\" are caused by *this* is not backed up by apples-to-apples studies.\n\nFurthermore, the CelebA 64^2 results – that also serve as the basis for the abovementioned distance study – are so horribly aliased that I suspect the dataset has been generated by incorrect image processing (perhaps pure point sampling, a surprisingly common problem). It is difficult to say for sure, as no samples from the reals appear to be shown in the submission. *This is a major issue, because the only significant reduction in FID compared to the competition is reported for this dataset.* If the dataset is indeed processed so that it aliases as much as the results seem to indicate, it is no surprise that CNN-based techniques have a great difficulty with it; clearly, producing pixels that have a large random component is clearly difficult when you only build the result as nonlinear combinations of nearby feature maps. On the other hand, even a global attention mechanism will clearly have no difficulty with such data because spatial proximity plays little or no role. The other trained models, likely trained on datasets obtained from other authors, appear to show no aliasing artifacts like this, but also FID and IS results reported on them are relatively much worse.\n\nOverall, I feel I have not learned much that is useful about the properties of the architecture. The paper would benefit from much more detailed and careful analysis of the emerging attention patterns.\n\nI'm wondering about the wild layer by layer fluctuations in Figs. 4c and 5d. I suspect this is sign of some pathology.\n\nIt would help to explicitly write out the definition of the Swin layer in the paper. This would help the paper to be more self-contained.",
            "summary_of_the_review": "The paper describes some interesting observations about transformer GAN generators, but I feel studies are left half way so that not much is learned in the end. The main good result, good performance on CelebA 64^2, is in serious doubt due to potential misprocessing of the data.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper conducted an empirical study on using Transformers for high-resolution image generation in GAN. The authors perform analysis on the importance of feature locality in image generation, residual connections in self-attention layers, and conditional generators by Transformers. The studies result in new CNN-free architectures for both the generator and discriminator. Experimental results show that the proposed method can reduce the gap between CNN-based and Transformer-based GANs.",
            "main_review": "Pros. \n\n1. This paper is easy to follow and overall well-structured.\n\n2. I think the problem this paper tried to tackle is interesting for the community. Designing Transformer-based GANs is definitely an important problem that has been seldomly studied.\n\n3. I acknowledge the significance and importance of the empirical observations the authors made in this paper, which should benefit further research in Transformer-based GANs.\n\nCons.\n\n1. My biggest concern about this paper is its limited technical novelty/contribution. Although the empirical findings are promising, the final solution they came up with is either simple (projected layer and one additional AdaNorm layer) or directly brought from the existing one (Swin Transformer). Additional results such as theoretical bounds or analysis should strengthen the contribution.\n\n2. This paper targets high-resolution image generation, but the proposed method generates images with a resolution of at most 256, which has a notable gap between current state-of-the-art methods with the same focus (e.g., StyleGAN2 and HiT can produce at most 1024 resolution). Thus, results with higher resolutions (e.g., 512 and 1024) should be added if possible. More importantly, in order to work on the 1024 resolution, HiT completely removes self-attention modules at high-resolution stages, I wonder if the proposed Swin-based architecture can handle this case.\n\n3. The proposed approach has more components than TransGAN and HiT. It is also important to see detailed comparisons on runtime performance (e.g., FLOPs or Throughputs) at the resolution of 256 and 1024 if possible.\n\n",
            "summary_of_the_review": "The paper studied a challenging and important problem in Transformer-based GANs. Although the empirical findings are interesting, I have concerns about the technical novelty of the proposed approaches. Additionally, some important experiments are still missing, which leads me to the current rating. I hope the authors can address my concerns in the rebuttal.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a new transformer-based architecture for the GAN framework. Both the generator and discriminator consist of several Swin transformer blocks. This paper provides the following contributions in the network structure design.\n1. Reduce the window size of the Swin layer in the generator.\n2. Replacing the residual connection with a skip-projection layer in the discriminator.\n3. Present a viable way of injecting condition information in the generator.\n\nExperiments are conducted on the CelebA, FFHQ, CIFAR10, and ImageNet datasets. Although not surpassing the previous CNN-based methods in most experiments, the authors claim that they mitigate the gaps between transformer-based and CNN-based GANs.",
            "main_review": "Strengths:\n1. This paper is well written and easy to follow.\n2. The authors conduct various experiments to show the effectiveness of each proposed module, both quantitatively and qualitatively.\n\nWeakness:\n1. The novelty of the overall approach is very limited since it just applies the Swin transformer in the generator and discriminator. Although the authors propose several structure modules to try to stable the GAN training. But for the generator, reducing the window size is more like an implementation trick, not to mention the result of Swin's default window size 8 is already good enough (the gap is less than 0.8% as shown in Table 4).  For the discriminator, all the best results are still achieved by using StyleGAN2's CNN-based D (in Table 2 and Figure 7a). The author seems to have no need to raise STrans-D.\n\n2. I am not fully convinced that the claim \"the global attention module always attends to pixels with a medium distance (10-20 pixels) that cannot offer direct guidance in synthesizing local textures\" is well-founded. The baseline model Trans-G is worse than expected. TransGAN [Jiang et al., 2021] shares a similar setting and applies full attention for stage resolution lower than 64x64 and achieves FID 5.01 on the CelebA 64 dataset, which is significantly better than the author's baseline model (FID 9.8). So I don't buy the results and conclusions in this section. Furthermore, the benefit of the transformer is modeling long-range relations. If the author thinks that the local relation is sufficient for image generation, what is the motivation for the author to use a transformer to replace CNN?\n \n3. The proposed skip-projection seems to work well. However, its ability to balance the norm ratio seems weak. In Figure 4, the two curves are overlapped in most cases, even at 16x16 resolution, the ratio has reached 40+. Therefore, the empirical analysis of why skip projection is good is not very valid.\n\n4. The experimental result section is weak. I wonder if the authors experiment with higher-resolution images. e.g., 1024 x 1024\n\n5. Will the result be better if the number of the parameters is increased?\n\n",
            "summary_of_the_review": "Overall, the proposed method is not novel and the experimental results are not particularly strong. I believe the claims in the paper are not at all well justified.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}