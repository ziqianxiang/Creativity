{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper aims to address the trade-off between the robustness and the generalization of a classification model, where the standard accuracy tends to decrease for a robust model and a model without adversarially trained tends to non-robust. First, the paper empirically demonstrates the trade-off for models under the standard training (ST), adversarial training (AT) and joint training (ST+AT). In addition, the paper shows the Grad-Cam attention of models trained under various setting and demonstrates that ST model places more attention on the outline of the MNIST digits. Motivated by such observation, this work proposed two approaches (AELF and AELFs) to encourage the feature of an adversarial example to be similar to that of a natural example. Overall, the main contribution of this submission is unclear because the observation of such trade-off has already been presented in the literature. The proposed method also does not compare to many existing works and does not demonstrate better performance.",
            "main_review": "Strength\n1. The writing is simple to follow, and the overall manuscript is well organized.\n2. The visualization and plots in Figure 2 and Figure 4 are interesting.\n\nWeakness\n\n1. The author is suggested to provide a highlight of the contribution in this work. First, the trade-off between robustness and accuracy have been known in the literature (for example Yang et al. 2020 and many others) and this paper does not add any new insight for this existing trade-off. Second, the proposed AELF does not achieve surprising improvement (also mentioned by the author in the end of sec 4.2 and the last sentence before Sec.5). \n2. As mentioned by the author (See last 2 paragraph of Sec2.2), no data augmentation is used during training and test. Since most training are performed with data augmentation, it is unclear whether the proposed framework still works under the standard training with data augmentation.\n3. Missing baselines for Mnist. As shown in Table 2, the proposed AELF is only compared with standard training (ST), adversarial training (AT) and joint training (JT). More baselines should be compared.\n4. It is unclear why the L1 loss in Eq(1) is defined as one label loss rather than cross entropy loss\n5. Eq 10. \\bar{w} is not optimized according to Algorithm 2 and need not appear under “argmin”\n6. Typo : data `` argumentation” (such as random cropping)\n",
            "summary_of_the_review": "As mentioned in the weakness section, the core contribution of the paper is unclear and the performance of the proposed method is not competitive. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this paper, the authors investigate the trade-off between the robustness and generalization from the aspects of model attention. The authors further pointed out that adversarial training may make the model ignore the inherent feature of the input which would weaken the generalization on the unseen inputs. The authors then propose a novel framework of attention-extended learning which can encourage a broad spatial attention on the input. Extensive experiments demonstrate that the algorithm have superior performance with good trade-off on adversarial robustness and generalization. ",
            "main_review": "In general, the paper is easy to follow by addressing an interesting problem but I still several concerns:\n1.\tThe authors argue that the “AT model usually concentrates on more sparse spatial regions” which may sacrifice the generalization. However, the argument is heuristic without sufficient evidence. The authors try to justify the argument but somewhat empirically, and it would be better if the authors could provide more theoretical supports. \n2.\tIt would be better if the authors could use a metric and provide a quantitative evaluation on the “region sparseness” of adversarial training. \n3.\tIn AELF, there exist several hyperparameters for the optimization in Eq. 10, and the authors are expected to analyze how these parameters influence the performance. \n4.\tIt would be better if the authors could provide more comprehensive comparison with more attacks/defense algorithms to demonstrate the superiority of the algorithm. \n5.\tThe authors are expected to make more comprehensive evaluation on the OOD data to demonstrate the improvement on the generalization such as ImageNet-C, ImageNet-O, etc.  \n",
            "summary_of_the_review": "The paper is easy to follow by addressing an interesting problem but the main argument is somewhat empirically without sufficient support. The authors are expected to provide more theoretical analysis to explain the inner mechanism and more comprehensive evaluations.  \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper studies adversarial training as a common solution to adversarial model inputs. The key \"trade-off\" issue is that, while training a model on adversarially-constructed examples can cause it to be more robust to those examples at test time, performance suffers on natural examples. ",
            "main_review": "Strengths:\n* This paper offers the following analyses that might be interesting to those studying adversarial examples (but I can't verify whether they are novel or not)\n    * At least on MNIST, adversarial training \"tends to produce a model which uses few inherent features.\" (page 5; \"inference\"). \n    * In the vision setting, this might have the effect of reducing performance when images are perturbed naturally (e.g. shifting an MNIST image by 8/32 pixels). This motivates training the model to \"attend\" to all regions of an input image.\n\n* This model introduces a new algorithm for adversarial training. The idea seems to be to use two losses: one is the standard cross-entropy loss from a natural image, and the other is a regularization term penalizing the drift in a feature map, from the network applied to a natural image, versus the network applied to a perturbed image. \n* The algorithm seems to work well on MNIST.\n\nWeaknesses:\n* To this reviewer, the experimental section seems a bit lacking. \n    * The experiments are only done on MNIST and CIFAR, and to this reviewer, it's not clear whether these results are meaningful at this scale. One worry in particular is that the view of \"standard training only discovers a few pixels as being important\" only applies on mnist. This does seem reasonable for MNIST because most of an input image is \"background\" anyways, but it would surprise this reviewer if the same exact trend held for deeper networks on CIFAR, since presumably the background context of an image helps contextualize the foreground.\n    * Summarizing the results on CIFAR, it is still not clear to this reviewer whether the proposed AELF approach is \"better\" than past methods like Song et al 2020's. The key issue seems to me is that there are two numbers that you are trying to compare. Could it work to show, e.g. a pareto-efficiency like curve where over different hyperparameters of AELF, it can outperform all of the earlier methods on either natural accuracy or perturbed adversarial accuracy?\n* It is not very clear whether Table 4 and Table 5 are truly fair comparisons between different models, or whether the experimental setup is changing significantly between each experiment (with different perturbation networks WRN-40/WRN-32, CNNs studied, etc.).\n* To this reviewer, the paper could be presented better and more clearly. It took a while for me to understand what the main argument of this paper was, and how the algorithm works; even then I might have missed something.\n    ",
            "summary_of_the_review": "To this reviewer, this paper seems somewhat promising, but a worry that I have is that many of the key claims and intuitions seem to be specific to MNIST (like implicitly assuming a black-pixel background). It is not clear to me that such intuition applies to other visual domains, and it is not obvious that this method would work better on CIFAR (or on Imagenet).\n\nFor these reasons, I don't accept acceptance right now, however I think this paper could be greatly improved by studying spatial dependence on other datasets, and by improving the writing and presentation.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}