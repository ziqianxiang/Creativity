{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a method to derive explanations of black-box ML models. The primary contributions of the paper are algorithmic. The proposed method, MCXAI, identifies small feature subsets which best explain the transformation of a correctly classified example to an incorrect one and vice versa. The formulation involves inducing a pair of deterministic MDPs with goal rewards and use MCTS to explore the search space efficiently. MCXAI is compared with LIME and SHAP and demonstrates improvement on the shortest path length metric on well-known datasets as well as correlation with the outputs of LIME and SHAP. The experiments also demonstrate the possibility of further improving the black box by removing bad features.",
            "main_review": "- The paper tackles the important problem of generating explanations of black-box ML model predictions. The key idea is find a sequence of feature transformations (masks) which can convert an example from correctly labeled to incorrectly labeled (by the black box ML model). This is also done in the opposite direction (incorrect to correct). Formally, this seems equivalent to inducing deterministic MDPs with goal rewards and use MCTS to find optimal trajectories. The reward function incorporates both the number of features (as path length) as well as the change in prob. estimate of the black box of the new state (example with some masked features). The final output is a minimal subset of features which cause the predicted label to flip (correct <--> incorrect)\n- While the approach to generate the minimal feature subset seems sound, the motivation for preferring MCXAI over, say, SHAP is unclear to me. The theoretical justification for MCXAI is not currently convincing, especially compared to SHAP. The paper makes a number of claims without much supporting theoretical or empirical evidence. For example, \"we interpret the prediction of the black-box model in terms of tree structures that humans can naturally grasp\". I did not find any evidence showing humans prefer this over SHAP or MUSE. I think it's important to verify that the MCXAI outputs are actually preferred by humans over those produced by SHAP or MUSE.\n- I'd be curious to understand how the hyper-parameters (\\tau, \\eta) affect final performance. Also, MCTS is a very large algorithmic family and one can easily imagine variance in tree structures affected by node initialization, arm selection policy, rollout policy, leaf evaluation heuristics, etc. The problem is exacerbated in larger search spaces, when only a fraction of the full search tree can be explored. This would be less of a concern if only the principal variation (i.e., ranked list of features) was being presented to end users. Since tree structure is claimed to be important, a sensitivity analysis would be good to include.\n- Some minor points\n  - Possible typo on Page 3. \\sum_i i = 1 -> \\sum_i a_i = 1 ?\n  - The action semantics are inconsistent. In Fig 1, a_i=0 masks x_i compared to the formalism in Sec 3 where a_i = 1 masks x_i.\n  - Is there any reason why the \"reverse\" search problem (incorrect -> correct) is not a mirror image of the \"forward\" search problem (correct -> incorrect -> correct)?\n  - The description of the full MCTS search is a bit confusing. How exactly is the surrogate model obtained and used? Initially, I thought it might be used as a leaf evaluation function or as a rollout policy (\\pi_e) but the paper claims random rollouts are used instead.\n- The absence of human evaluation data means that it's very difficult to evaluate the performance of MCXAI. The choice of selected metric (average number of steps) seems to favor MCXAI since it's part of the learning objective via the reward function. I don't see how this is a fair evaluation. What am I missing? Ranking metrics might have alleviated these concerns somewhat but only a single example is analyzed in Figure 2 and is very hard to read. An evaluation using ranking metrics across the dataset may be worth considering.\n- Overall, the paper proposes an interesting XAI algorithm but needs more theoretical justification and careful empirical evaluations. It is currently difficult to evaluate if there is an improvement over existing XAI methods.",
            "summary_of_the_review": "Interesting approach to XAI but unclear if it's an improvement over existing methods. Needs more theoretical and empirical rigor.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes to use monte-carlo tree search for computing\nexplanations, using a two game approach.\n",
            "main_review": "The use of explanations in high-risk situations, as claimed in the\npaper, must offer guarantees of correctness, i.e. one ought not\nprovide explanations that are logically incorrect. Unfortunately, this\nis what tools like LIME or SHAP do, to name a few. There is extensive\nevidence regarding tools like LIME or SHAP computing logically\nincorrect explanations. The references below include some of those\nexamples.\n\nI can understand that informal explanations can be important in a\nnumber of settings, and that is where I could see tools like SHAP or\nLIME being used. However, I would expect that explainers for high-risk\nsituations to offer far stronger guarantees of rigor, and that\nrequires far more than being more informative, as argued in the\npaper.\n\nGiven the way the paper is positioned, it should acknowledge the\nlimitations of existing informal explainers, including LIME and SHAP,\nand be clear about its own limitations. Alternatively, the paper\nshould not overlook the work on computing logically rigorous\nexplanations, and validate its own results against tools that offer\nsuch guarantees of rigor.\n\nA few minor details. I was unable to understand the complexity of the\nalgorithm on papge 6. What is \"A\"? The reference for MCTS is a 2 page\npaper from 2008, namely (Chaslot et al., 2008); a different reference\nshould be used.\n\nThe list of below includes a sample of references on computing\nlogically rigorous explanations, their complexity and tractability\nresults, their use in assessing informal explainers, and some of   \ntheir applications. The references are in chronological order.\n\nAndy Shih, Arthur Choi, Adnan Darwiche: A Symbolic Approach to\nExplaining Bayesian Network Classifiers. IJCAI 2018: 5103-5111\n\nAlexey Ignatiev, Nina Narodytska, João Marques-Silva: Abduction-Based\nExplanations for Machine Learning Models. AAAI 2019: 1511-1519\n\nAndy Shih, Arthur Choi, Adnan Darwiche: Compiling Bayesian Network\nClassifiers into Decision Graphs. AAAI 2019: 7966-7974\n\nNina Narodytska, Aditya A. Shrotri, Kuldeep S. Meel, Alexey Ignatiev,\nJoão Marques-Silva: Assessing Heuristic Machine Learning Explanations\nwith Model Counting. SAT 2019: 267-278\n\nAlexey Ignatiev, Nina Narodytska, João Marques-Silva: On Relating\nExplanations and Adversarial Examples. NeurIPS 2019: 15857-15867\n\nAlexey Ignatiev, Nina Narodytska, João Marques-Silva: On Validating,\nRepairing and Refining Heuristic ML Explanations. CoRR abs/1907.02509\n(2019)\n\nOana-Maria Camburu, Eleonora Giunchiglia, Jakob Foerster, Thomas\nLukasiewicz, Phil Blunsom: Can I Trust the Explainer? Verifying\nPost-hoc Explanatory Methods. CoRR abs/1910.02065 (2019)\n\nAlexey Ignatiev: Towards Trustable Explainable AI. IJCAI 2020:\n5154-5158\n\nAdnan Darwiche, Auguste Hirth: On the Reasons Behind Decisions. ECAI\n2020: 712-720\n\nBotty Dimanov, Umang Bhatt, Mateja Jamnik, Adrian Weller: You\nShouldn't Trust Me: Learning Models Which Conceal Unfairness from\nMultiple Explanation Methods. ECAI 2020: 2473-2480\n\nAlexey Ignatiev, Martin C. Cooper, Mohamed Siala, Emmanuel Hebrard,\nJoão Marques-Silva: Towards Formal Fairness in Machine Learning. CP\n2020: 846-867\n\nWeijia Shi, Andy Shih, Adnan Darwiche, Arthur Choi: On Tractable\nRepresentations of Binary Neural Networks. KR 2020: 882-892\n\nGilles Audemard, Frédéric Koriche, Pierre Marquis: On Tractable XAI\nQueries based on Compiled Representations. KR 2020: 838-849\n\nJoão Marques-Silva, Thomas Gerspacher, Martin C. Cooper, Alexey\nIgnatiev, Nina Narodytska: Explaining Naive Bayes and Other Linear\nClassifiers with Polynomial Time and Delay. NeurIPS 2020\n\nAlexey Ignatiev, Nina Narodytska, Nicholas Asher, João Marques-Silva:\nFrom Contrastive to Abductive Explanations and Back Again. AI*IA 2020:\n335-355\n\nStephan Wäldchen, Jan MacDonald, Sascha Hauch, Gitta Kutyniok: The\nComputational Complexity of Understanding Binary Classifier Decisions.\nJ. Artif. Intell. Res. 70: 351-387 (2021) \n\nJoão Marques-Silva, Thomas Gerspacher, Martin C. Cooper, Alexey\nIgnatiev, Nina Narodytska: Explanations for Monotonic Classifiers.\nICML 2021: 7469-7479\n\nAlexey Ignatiev, João Marques-Silva: SAT-Based Rigorous Explanations\nfor Decision Lists. SAT 2021: 251-269\n\nEmanuele La Malfa, Rhiannon Michelmore, Agnieszka M. Zbrzezny, Nicola\nPaoletti, Marta Kwiatkowska: On Guaranteed Optimal Robust Explanations\nfor NLP Models. IJCAI 2021: 2658-2665\n\nYacine Izza, João Marques-Silva: On Explaining Random Forests with\nSAT. IJCAI 2021: 2584-2591\n",
            "summary_of_the_review": "The paper proposes the use of MCTS for computing explanations. The\npaper does not address the limitations of the proposed approach, and\ndoes not relate with logically rigorous explanations. This is critical\nis high-risk situations, which the paper uses as motivation.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Goal: an algorithm that explains the predictions of a black box classifier while displaying: 1) feature importances, 2) feature dependence, 3) features supporting the prediction and 4) features with negative correlations to prediction\n\nApproach:\n\nDepending on whether the instance is correctly or incorrectly classified, the authors learn a series of actions where each action consists of setting a feature to a constant value to make the instance incorrectly or correctly classified respectively.  To obtain the set of actions they use Monte carlo tree search and return the tree as a way to display the explanation\n\nEvaluation:\n\nHow many steps (actions) are needed to change prediction of model using their approach and LIME/SHAP. Shows that their approach generally outperforms the baselines\nUsing their method as a way to remove features that improve model performance\n\n\n",
            "main_review": "Strengths:\n\nNovel approach to obtain feature importance that are ranked\n\nWeaknesses:\n\n- Approach relies on the label y not available at test time, I don’t believe this to be necessary and one can restructure the paper by only having a single game which is to flip the prediction. \n\n- Removing feature by setting it to a constant: this seems to me like a heuristic for removing a feature that is not well argued for. LIME has to generate multiple alternatives by sampling randomly to be able to estimate feature importance. Setting a feature to a constant may not be as robust as exploring the space to estimate the importance of the feature.\n\n- Exposition of the paper is at times unclear and hard to parse: sections 3.1 to 3.4 need to be restructured in a more reader friendly way, namely a figure to explain all the components of the approach is necessary and how to draw the final tree with the attached labels of win-rates. Furthermore, it is unclear in the writing that the actions act on single features as it is written that each action $a \\in \\{0,1\\}^n$ meaning actions can act on multiple features at the same time. Additionally, details of MCTS should be summarized more succinctly as they do not describe contributions of the paper and only the high level details of the algorithm should be written.\n\n- Number of steps is not sufficiently argued for as a metric for an explanation method: NoS is a good metric to judge if MCTS is doing it’s job well, but it’s not a good metric to check if an explanation is interpretable for human users to act on. Namely, what if the algorithm was operating on features that make the data point out of the distribution but flip it’s prediction? MCTS is not constrained to stay in the data manifold.  It seems that Figure 2 in fact illustrates that SHAP is able to turn the 7 into a 1 but McXai replicates LIME into turning into 3 vertical dots. \n\n- Run-time has no reference: when comparing run-time of algorithm it is important to first establish the complexity of each (which authors provide for MCTS but don’t state for LIME and SHAP) and then running times should include the machine information and other information.\n\n- Lack of discussion on why in section 4.2 the approach works for some models and not others, and why it works for some even.\n",
            "summary_of_the_review": "The paper presents a novel approach to obtain feature attribution for classification using the notion of two games that derive actions (feature set to constant) to change the prediction. There are many concerns with the evaluation as it does not display whether the explanations are possible (Actions are valid with respect to distribution) or human readable. Furthermore, the approach depends on the label y which may not be available at test time. My recommendation is a rejection that can be changed if authors show that the actions obtained on the datasets are reasonable (can we see the trees obtained on a few examples ?)  and how to handle the fact that the label y may not be available at test time.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper the authors propose a model-agnostic reinforcement learning framework involving two games to assess and explain the feature importance of a given black box classifier. The two agents apply Monte Carlo tree search to explore the space of feature sets in order to alternatively approve and disapprove the classifier's decision. The authors empirical show that the important features and the misleading features found by the proposed method are more informative with respect to the classification task in contrast to other benchmark methods.  ",
            "main_review": "The McXai method proposed in the paper is a relatively novel approach to understand the feature importance. The authors have clearly motivated the research question and elaborated the proposed method. The paper is easy to follow.\n\nThe reviewer has the following major concerns regarding the weaknesses of the paper.\n\n1. The proposed McXai needs more theoretical justification. Though its MCTS part is well established by previous results, its other design and analytic aspects might be subject to the following questions, for example:\n- The classification game and the misclassification game seem to be forming some duality. A natural question to ask is that to what extent these two games report similar explanations. The authors argue that the classification game picks the more supporting features towards the correct label, and the misclassification game chooses the sensitive ones towards the wrong labels. This claim does not have theoretical supports. On the other hand,  the empirical study is done on a binary classification dataset, whereas this binary setup makes it harder and more ambiguous to understand the difference between contributing to the correct label and being sensitive towards the wrong label. \n- On evaluating the importance of the feature set and the interaction of different features, the concept of \"feature set\" is not respected by the corresponding paths which would imply a **sequential** selection of features. Does McXai promises any commutative property, or what would be the correct way to merge different paths corresponding to the same feature set?\n- More comparison with SHAP, since both methods are focusing on subset performance.\n- Certain design choices can be better explained, e.g. the constant choice of *\\tau* might still leave correlation with other features on the table in contrast to, e.g. random permutation. \n\n2. Regarding the empirical study:\n- The classification game study is self-contained in its own scope of \"number of steps\". What's missing here is the justification of NoS as an intended way of giving explanations, versus for example, any oracle properties guarantee that only true important features would be selected. Maybe an experiment with known data generating distributions would be useful.\n- The misclassification game could be, additionally, analyzed similarly as the classification game as an empirical comparison between them two.\n\n\nThe reviewer also finds that the paper has a few minor editorial issues which, though not blocking reading through, might cause confusion. A partial list:\n- Introduction: \"...has a negative influence of the prediction\", needs more elaboration.\n- Methodology, \"\\sum_{i=0}^c g(x)[i]\" -> \"\\sum_{i=1}^c g(x)[i]\". Or specify whether c or c+1 is the dimension of the output space. \n- Methodology, \"y \\in \\mathbb{N}\" -> y \\in {1,\\dots, c}.\n- 3.1 \"... with \\sum_{i=0}_n i = 1\", seems incorrect.\n- Figure 1. a_1 and a_2 has more than one \"1\" in it, potentially contradicting with the definition. \n- 3.3. Unify A and \\mathcal{A}, and maybe define the action space earlier.\n- 4.1 Table 2 -> Table 1\n- Figure 2. LIME and SHAP flipped?\n\n\n\n",
            "summary_of_the_review": "The McXai method proposed in the paper is intuitive and relatively novel. However its effectiveness is insufficiently supported theoretically and empirically by the paper in its current shape. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}