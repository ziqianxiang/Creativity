{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces a goal-based architecture which relies on a partitioning of the replay buffer to produce, sample, and learn from meaningful goals.\nThe proposed method has many moving parts:\n- A goal-conditioned policy (trained with SAC)\n- A clustering method for the replay buffer, which includes refinement and deletion of clusters and nodes within the buffer.\n- A feature learning method (based on InfoNCE) which informs clustering, and makes the clustering reflect the topology of the MDP.\n- A sampling method that induces uniform sampling from clusters in the absence of reward. Clusters, then states within clusters, are sampled as goals.\n\nExperiments show that:\n- The learned goal space is strong enough that in the absence of reward, one can plug in the right goals a posteriori and obtain meaningful performance\n- In dense reward environments, meaningful reward-maximization is achieved\n- In sparse reward environments, the method obtains rewards when other baselines do not (presumably due to its ability to explore)\n- Individual parts of the proposed architecture are meaningful\n\nBeyond higher rewards and an ablation, the main evidence of _why_ this method works is that it manages to learn clustering-friendly latent representations of the state observations. The authors also do a good job (in the appendix) of figuring out what values of the new hyperparameters make sense.\n",
            "main_review": "\nStrengths:\n- The paper is well written, although because of the many components of the method it relies on prior knowledge of many different papers, as well as frequent trips to the appendix\n- The method is interesting. Discretizing MDPs is an important challenge in RL and this paper shows that with relatively few priors, meaningful structures can be captured and useful skills learned.\n\nWeaknesses:\n- The novelty of this method is relatively limited. Ingenious clustering and regularizations of the latent space are nothing new (and the authors have a well written Related Work section on this). The authors have discovered an interesting combination with cool properties, but what does this teach us about the field, and where we should be doing research next?\n- If I understand it correctly, the method itself relies heavily on sampling previously visited states. This seems like something which may not scale as well as other methods relying on models/\"imagination\". See [A].\n- The choice of baselines seems lacking. See [B].\n\n\n\n[A] One thing I'm unsure about is how strongly this method is capable of doing \"extended horizon\" exploration. If I understand correctly, goals (which push the agent to both aquire reward and explore) are sampled from an replay buffer, which means they need to have been sampled at least once. This means if the agent is really good at reaching goals, it will explore at best neighbouring states of already visited states.\nThere doesn't seem to be a mechanism for visiting _imagined_ far-away rewarding states, as opposed to, say, model-based uncertainty/sampled random goals. The advantage of relying on a function approximator (for e.g. a model or a pseudo-count) is that if we (as a field) get really good at generalization, then these methods will naturally work really well at visiting \"structurally predictable\" novel/unvisited regions of the state-space.\n\n\n[B]\n- In Figure 1, I understand the comparison to Skew-Fit, it is a close method. What about other goal-based methods? There are many other contenders here that it would make sense to compare to, if just a naive Hindsight ER model.\n- The results of Figure 2 are a bit surprising. There are methods that are able to solve sparse ant maze, see e.g. [1]. Either there is a bug somewhere or this is an odd choice of baseline.\n\n[1] D4rl: Datasets for deep data-driven reinforcement learning, J Fu, A Kumar, O Nachum, G Tucker, S Levine, 2020\n\nComments on text:\n- \"An efficient way for a deep reinforcement learning agent to explore can be to learn a set of skills that achieves a uniform distribution of terminal states.\" That is a fairly bold statement to start a paper with, and one I'd personally disagree with. In combinatorially large state spaces, uniform sampling is very inefficient and not particularly desirable. Imagine visiting terminal states uniformly in Go, virtually all of them are losses, implausible or uninteresting.\n- for `argmin` you can use e.g. `\\mbox{argmin}_{...}`, $\\mbox{argmin}_{...}$\n- It would be good to explain a bit more technically what \"Growing When Required\" is in the main text.\n",
            "summary_of_the_review": "The paper combines existing methods in interesting ways while adding a few novel components that make the resulting method work well. There's a decent empirical evaluation of the method, although it could probably be extended so as to understand the mechanisms involved. I think it has the potential to be applied to interesting problem, but it's not obvious what are the long term impacts of this paper and how it should affect future research on latent representations in RL.\n\nFor these reasons I recommend acceptance, although I do believe the paper could be made stronger by increasing its scientific scope.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a goal-conditioned skill discovery method which can be combined with extrinsic rewards. Its key contribution is a novel method to propose goal-states. ",
            "main_review": "Strengths: \nThe paper tackles a challenging problem: How to propose good goal states for unsupervised skill discovery.  The idea of using count-based exploration in this context is interesting and, to my knowledge, novel in this context (though it's possible I'm unaware of some other work in this area). \n\nWeaknesses:\nThe exposition of the algorithm, as well as its motivation, are confusing, requiring me to go back and forth several times. And even then, it is still not entirely clear to me what the motivations for each component of the fairly complex algorithm are. Furthermore, I'm not sure about the validity of some claims.\n\nDetails:\n\nI) Confusing writing.\nI struggled to make sense of the algorithm on my first read-through. While part of that might be due to the complexity of the algorithm, I believe the exposition can be improved. Some ideas/suggestions:\n* I think the clarity would strongly benefit from a much clearer statement of which problems the algorithm is trying to solve and how, and why the proposed complexity of the algorithm is necessary. To do so, it would also probably benefit from a short background section on Skew-Fit, which seems to be closely related.\n* Some things should be introduced earlier. For example, I found it impossible to understand much without knowing what the _two_ different buffers are which are mentioned. Unfortunately, those were only introduced towards the end of the method section. \n* Similarly, the authors often refer to \"the distribution\" (without any further clarification) and only introduce what they mean by it in the very last paragraphs of the method section. \n* Discretization: I think the motivation for discretization should be made much clearer - I'm not sure one is given at all at the moment. Furthermore, at first, it seems unnecessary since we sample clusters based on the number of states in a cluster and then sample a state in a cluster uniformly -> We might as well just sample all states uniformly. Later on it becomes clear that the clustering allows a count based exploration bonus, but this motivation for discretization is never explicitly spelled out. There might also be another motivation, which I'm currently missing? \n* OEGN is not explained at all in the (main) paper. Similar to Skew-Fit, I think a short description in the background section of the main paper is warranted as it's an integral part of the algorithm. While I realise there is a description in the appendix, the main paper should be understandable by itself - at least on a high level. \n* Definition of intrinsic policy reward: I'm not sure it's mentioned at all in the paper?\n\n\nII) Validity of Claims: I'm unsure about several claims or algorithmic choices\n* Distortion: It is not clear to me how eq. (2) addresses the problem of distortion due to the policy dependence of the learned embeddings space. In fact, I'm not sure this can be addressed at all, other than to modify the exploration policy.\n* Collapse: I do not see why there is a danger of collapse which has to be mitigated by delta. In fact, the experimental results in Fig 5 show no such collapse for delta = 0, only a smaller scale. I would hence argue that both delta, as well as the temperature purely influence the scaling of the learned embedding, but do not lead to any qualitative change. Delta might still be necessary for numerical reasons, but that is a very different argument than the one made in the paper.\n",
            "summary_of_the_review": "Overall, find the current exposition confusing and I'm not convinced of the validity of some claims and explanations. As such, while this might be a very interesting algorithm, I'm not confident I understand it well enough to confidently judge its capabilities and utility. \n\nQuestion to the authors/when would I raise my score:\n* Validity of claims: Please correct me if my understanding of \"collapse\" and \"distortion\" is wrong.\n* Confusing exposition: I think this would require either a significant re-write of the method section or other reviewers disagreeing with me in that they found the exposition clear. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a method for learning skills in RL. An agent is encouraged to learn skills that have a diverse terminal state in a latent space. These skills can then be transferred onto a goal conditioned policy. The method for training a goal conditioned policy follows (Pong 2020). ",
            "main_review": "DisTop is not the first paper to consider the problem of bottom up skill discovery plus representation. For example, Diversity is All you Need. Any Goal-Conditioned RL method. Go-Explore. The authors claim that in contrast to these methods, their method explores by learning diverse skills and specializing these skills to a task when provided. But, I’m not sure what that means and it’s not sufficiently clarified. In practice this doesn’t seem to be an important distinction. \n\nIt is mentioned that previous methods for exploration force sequential exploration and don’t allow an agent to explore many areas at once. But most methods of skill discovery focus on partitioning the skill space, discovering several new skills at once. Similarly, most methods for exploration don’t suffer from such a bottleneck, with methods like Go-Explore specifically avoiding detachment. \n\nI’m not sure why you’re learning intrinsic rewards with UVFA in the first paragraph of 2.1. Especially if you’re going to apply these rewards in a HER setting, where it’s known that continuous rewards are less performant. This analysis is confused. \n\n\n\nThe method itself is complex and can’t be quickly summarized accurately. There is an objective derived from InforNCE that attempts to cluster together states that are close in terms of reachability. However, the objective only considers consecutive states. I believe Zhang 2021 (World Model as  Graph) considers a more general version of this exact clustering problem, showing how to transform the Q function to estimate next step distance, and then using this for clustering. That work also cites several other works that attempt this kind of clustering (sparse graphical memory). I do not see any of this mentioned in this paper. \n\nThe paper next tries to “move the continuous embedding topology to a discrete one,” which so far as I can tell means that they want to cluster over the latent space. They take an off the shelf method (OEGN), which allows for a growing number of clusters, and apply it. It was not clear to me from the experiments section if this is needed, or if a fixed number of clusters would be sufficient (and hence a simpler architecture). \n\nFinally, there is some density fitting step where they approximate a distribution over goals and then sample low density states. Or states corresponding to low density skills? I can’t tell. In any case, they sample new goals that visit low density regions. A simple softmax style objective is used to accomplish this. \n\n\nThe algorithm is evaluated on a robotic push environment that is known to be quite easy. It is also evaluated on a robotic visual door environment that is a more fair baseline. Skills are learned over 48 x 48 images, which is small but probably fine for this type of application. It seems that by the author’s own admissions the skills learned are roughly on par with skew fit. Both disTop and DisTop min are plotted. I’m not sure why it’s appropriate to cherry pick the minimum distance reached in the author’s method but not also report the minimal distance reached in the baseline method. This is a borderline unethical plot. \n\n\nThe results on Ant Maze appear alright. I would like to see a comparison with other HRL or even HER based methods, which do very well on this environment. There are many algorithms (HIRO, MEGA) that solve this environment. The plot seems to indicate this is not the case, ignoring prior art.  \n\nIt seems like Figure 3 suggests OEGEN is critical to learning, which is surprising. How many clusters does this method end up with? Is it more than 200? \n\n\nI would prefer an algorithm box or architecture diagram in the main paper. It was very difficult to understand the full model pipeline without referring the the appendix. \n\n\nThe writing in this paper needs significant revision to improve clarity. “Using this topology, a state-independent agent hierarchical policy can select where the agent has to keep discovering skills in the state space” is strange and clunky. This type of issue is frequent throughout the manuscript, and I would encourage another pass to smooth the writing. This paper does not clearly slot into a niche. I was not sure if I was reading an HRL paper or a goal based learning paper or a visual RL paper or something else entirely. ",
            "summary_of_the_review": "This paper seems to take ideas from a lot of areas in HRL and multi goal RL and glue them together. It doesn't have a clear reason for gluing together the particular components it chose. Many of the choices are poorly justified. The results are lackluster. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper introduces DisTop, a method for combining intrinsic rewards for learning state-covering skills while also learning to reach goals relevant to a task. This method uses environment transitions to build a representation of the state space that captures its topology, thereby enabling a sampling of episode goals that balances broad skill discovery (i.e. state coverage) with task completion. The authors introduce their method, provide a high-level overview (with details in the appendix), and contribute some experimental validation in a handful of environments.",
            "main_review": "# Strengths\n- This paper presents an interesting synthesis of ideas, mostly built on the idea of exploiting a learned topology of the state space. This approach draws plenty from prior work but leverages the topology in a novel way (to my knowledge).\n- DisTop seems to perform well more broadly than other methods, but consistent comparison is somewhat lacking.\n- The authors provide some characterization of how DisTop responds to different hyperparameter choices.\n\n# Weaknesses\n- The clarity of the writing could be improved substantially. Descriptions are often vague, which makes the technical details harder to understand. I think it's fine to give high-level intuitions separate from low-level details, but the current writing invites confusion. For example, at the start of Section 3, the references to buffers and clusters are vague. The text refers readers to where these concepts are described, but the high-level description doesn't really give a clear picture, making the text that follows harder to understand.\n- Ideas are not always presented clearly. For example:\n```While sampling low-density or rewarding states is attractive to solve a task, it is not easy to sample new reachable goals. For instance, using an embedding space $R^{10}$, the topology of the environment\nmay only exploit a small part of it, making most of the goals pointless.```\n- Along the same lines, at the start of the Experiments section, when reading ```the ability of DisTop to select skills to learn``` I am left to wonder what this \"ability\" and \"selection\" refers to. This is not a criticism of word choice. The issue is that the previous section did not set up these ideas.\n- Sections of the results do not seem to actually address the experimental question they are motivated by (that is, the question at the paragraph header). In general, this paper tends to draw conclusions that seem only speculatively supported by the results.\n- Overall, the paper is not particularly easy to follow. The presentation lacks a clear intuition for how the pieces fit together and the experiments have little to hang on to as a result.\n- The conclusions drawn from the experiments are not particularly convincing. While there is some positive validation, demonstration of the *topology* learning's success is lacking. There are some portions of the appendix that get at this, but the analysis feels incomplete. Personally, I am much more convinced by a demonstration that the underlying pieces of the algorithm are viable than by seeing that, when they are all put together, the training curves look better.\n\n### Questions/Comments:\n- The second paragraph of 2.1 is hard to follow. If the technical details are important, it may make more sense to work them into a different area of the text.\n- The same applies to 2.2. The technical details are hard to follow.\n- You claim \"In consequence, we avoid using a hand engineered environment-specific scheduling\" on page 4. Does this suggest that the $\\beta$ parameter and the $\\omega'$ update rate are environment independent?\n- Why do DisTop and Skew-Fit have such different starting distances for Visual Pusher (Figure 1, left middle)?\n- It is somewhat strange phrasing to describe Skew-Fit as having \"favorite\" environments (page 6).",
            "summary_of_the_review": "Overall, I cannot recommend accepting this paper in its current state. Some of the ideas are intriguing, but they are not presented clearly enough. I am left with only a vague understanding of how DisTop is implemented, which makes it very difficult to confidently recommend this paper. I do not think that this problem is from lack of details. The level of detail in the main text is low, but acceptable. The issue is that the information is organized in such a way that very little intuition emerges. Unfortunately, I found it difficult to build such an intuition by re-reading, so I think something of an overhaul is needed.\n\nIn addition, the experiments do not provide much support for the method. It is encouraging to see that DisTop does well, including in settings where many other methods fail. However, we don't get a particularly clear characterization of *what* DisTop learns, especially in comparison to other methods. We get performance curves, but, for example, average distance-to-goal over training (Fig 1, left) does not tell us about the diversity of states discovered by DisTop versus by Skew-Fit (but the authors use the former to describe the latter). In sum, the analyses feel incomplete.\n\nOne hesitation I have about this review is that it creates somewhat impossible criteria given the page limit. However, I do believe that a more careful organization of the material, and a bit more attention to which details matter, could provide sufficient space to showcase both the performance of the model and its learning properties. Hopefully, this would be accompanied by a better overall organization/presentation of the ideas and how they fit together.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}