{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors propose a new activation function, called ZeroLiers, to prevent overfitting in neural networks. The authors claim that the proposed activation function eliminates the need to use Dropout on fully connected layers. ZeroLiers has a hyperparameter k, and the authors propose a variant of ZeroLiers, called ZeroLiers-L-k, where this hyperpararmeter is learnt. Experimental results are shown comparing the validation performance of ZeroLiers (and ZeroLiers-L-k) with Dropout on different architectures and for different activation functions.",
            "main_review": "My two main concerns about this paper are as follows:\n\n- This paper suggests that overfitting is caused by outlier values in the activations, and the proposed method works by eliminating outlier activation values with the hope of improving generalization performance. However, I do not think there is any evidence presented in the paper that confirms that the hypothesis that overfitting being caused outlier activation values is indeed true.\n\n   The only support for this statement is given in Figure 1, the results of which I find very unconvincing. There is some previous work to suggest that Dropout has the effect of regularizing the norm of the output activations when using the MSE loss. For example see Dauphin & Cubuk (2020), “Deconstructing the Regularization of BatchNorm”. So one can think of Dropout as regularizing both outlier and non-outlier activations, which would explain the results in Figure 1. However, this is very different than simply dropping the large activations and this figure does not provide enough evidence to suggest that large activations cause overfitting.\n\n   I think further experiments are needed to justify why ZeroLiers works. For example, a method like ZeroLiers could be biased towards learning the easier-to-learn examples in a dataset, and ignoring the harder-to-learn examples in a dataset, which might explain small benefits on the validation loss, but this is not due to preventing overfitting.\n\n- I also find the experimental section of the paper to be fairly weak. The MLP results are on very small networks. In a number of experiments, the setup of standard benchmarks needed to be significantly changed to induce overfitting, which raises the question of how important solving the problem is. Furthermore, there is also no consistent performance improvement of the algorithms proposed. For example, ZeroLiers-L-k performs worse than the baselines on the language modeling tasks.\n",
            "summary_of_the_review": "I am not convinced of the main claim of the paper that large outliers cause overfitting. Furthermore, I find the experimental validation of the method to be weak.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a new activation function, called ZeroLiers, for fully connected layers. ZeroLiers aims to prevent overfitting by replacing large outliers in ReLU-like activations with zeros. Experiments on CIFAR-10 using MLP, Autoencoder, and Denoising Autoencoder show that ZeroLiers is effective for ReLU-like activation functions and outperforms dropout. The authors further evaluate the method on BERT, GPT-2, CNNs, and Transformers and show that ZeroLiers can speed up the training and improve the generalization performance in some cases.",
            "main_review": "\n==== Strengths ====\n\n1) ZeroLiers can be seen as a complex version of activation clipping. It is interesting to see that such a simple trick can improve the generalization performance in some cases.\n2) Interesting observation regarding dropout. The number of values that deviate from the group around zero is much smaller, and their activation values become much smaller too.\n3) Interesting observation regarding the l2-norms of the gradient. The value of ZeroLiers is consistently greater than those of Dropout and the baseline, which explains its faster convergence than Dropout and the baseline.\n\n==== Weaknesses ====\n\nFirstly, one of the major concerns of the submission is the contribution of the paper. 1) The proposing method is a simple trick that only works for fully connected layers, which has quite limited application. 2) The authors argue that one of the limitations of ReLU-like activation is the additional computation introduced by dropout, which is indeed not significant. The proposing method also introduces some more computation. Could the authors provide a quantitative comparison of how much computation can be saved?\n\nSecondly, the way the authors motivate the proposing method does not convince me. 1) Is there any fundamental problem with dropout rather than computation cost? Why replace? Are there any problems when we combine ZeroLiers with dropout? 2) The authors aim to address the overfitting problem, why limit the scope to dropout? What about other regularization techniques, like L2 regularization, data augmentation, adding noise? In practice, people never use a single regularization technique. What happens when ZeroLiers is used together with those techniques? 3) Why large value is an “outlier” that is harmful to neural networks? Will there be a case that it is a useful feature for neural networks? If you believe the large value is a real problem, have you tried to make the function flatter after a certain point? Or try some normalization? Or try to remove the top 1% activation.\n\nThirdly, there are some concerns regarding the experiments: 1) The hyperparameter tuning procedure seems problematic, it seems that the authors tune on the test dataset. On page 6. “To monitor progress and detect the overfitting behaviour, we use the test images of CIFAR-10 as a validation dataset.“, ” We perform experiments with various values of k for ZeroLiers and choose the value that achieves the best validation accuracy. ” 2) Need to include experiments that combine ZeroLiers and dropout. Also need more experiments to compare to other regularization techniques if the authors position their paper as a way to address overfitting. 3) More analyses need to be done to understand the proposing method. Why ZeroLiers can provide regularization effect is not clear to me. Is the proposing activation function less expressive than the normal ReLU function, as it has constrained output? Or when we constrain the output value, it is easier for optimization, as the case for gradient clipping? The authors provide the figure that the activation becomes smaller. Is that simply because you constrain the output value in the previous layers? Will the L2 norm regularization achieve a similar effect? How ZeroLiers affect the learned weight. Will the norm decrease or increase?\n\nLastly, I list some minor issues: 1) Figures have different styles. 2) Figure 8: Could you give an explanation why “without dropout” has larger training loss? Given that activation clipping may reduce the model capacity. 3) what if you train the model long enough? Does it still prevent overfitting? Many figures indicate that the performance has not saturated. 4) If the authors want to position their paper as a way to address overfitting. Then, the related work section needs to take more regularization techniques into account and emphasize the connection between the proposing method.",
            "summary_of_the_review": "While the paper has some interesting nuggets, the paper needs more work to be done. Due to limited novelty and insufficient experiments, I recommend an initial rating of 3.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a new activation function that zeros out outliers. The paper also defines an outlier as an activation that is a given number $k$ of standard deviations higher than the mean activation of a layer. $k$ can either be a hyperparameter or learned as a layer parameter. The paper presents several experiments comparing various activation functions wrapped with the proposed ZeroLiers versus Dropout.",
            "main_review": "Strengths\n-------------\n\n- The idea of removing large activations is quite interesting and can have several effects worthy of studying in several architectures\n- The proposed method is very simple to implement and understand\n\nWeaknesses\n-----------------\n\n1. The most important weakness of the paper is the relatively poor experimental evaluation and results. For instance, it is relatively hard to get an accuracy less than 55% on CIFAR-10 with a 7 layer MLP even using only 100 epochs. This raises several questions regarding the tuning of the CIFAR experiments. Moreover, in the BERT experiment the results are mixed with the learnable ZeroLiers maybe performing slightly better while for autoregressive language modeling the results clearly show the baseline is best.\n\n2. The paper is quite unclear with respect to how $\\mu^l$ and $\\sigma^l$ are computed in practice. Judging from the provided code they are simply the average and standard deviation across the batch. This introduces several potential issues including the model not working out of the box for a batch size of 1 when trained with a larger batch size.\n\nSuggestions\n-----------------\n\n- In order to make the CIFAR experiments more convincing, I propose to simplify the experiment setting to what is the best possible score achieved with a fixed architecture with/without Dropout and with/without ZeroLiers. If that is what we are seeing in table 1, then more tuning is required as the experiment is simple enough and common enough that people can check that 54% is actually a pretty small number.\n- Table 4 is not enough to define the architectures. Judging from the code the hidden dimension is fixed to 1024 for all layers which could easily be added in the table. Ideally, the network should be reproducible from the information in the appendix but even referring the reader to the code in the supplementary would be enough (especially if a specific file is mentioned).\n- Figure 12 has obvious errors (eg is 12a the validation or training loss?) but more than that the colors change from 12a to 12b and there is an oscillation that clearly shows that there is a moving average of some sort instead of the per iteration value.\n- Top-N accuracy is not what is reported here. It is not even clear what is reported here. Top-10 accuracy for CIFAR-10 should always be 1.0 since CIFAR-10 has 10 categories.\n- There are several figures with dark and light lines where the dark line is assumed to be the moving average of the light line. This is not mentioned anywhere. It would be good if it was mentioned exactly what it is (eg moving average, average per epoch, exponential moving average). Moreover, it would be good for the readability of the paper if the plots did not contain all points but a point per epoch since it would make scrolling and zooming significantly smoother.",
            "summary_of_the_review": "My recommendation is based on the lack of convincing experiments for ZeroLiers and the general quality of the manuscript as mentioned in the weaknesses section and the suggestions section.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a neural network component built on activations similar to ReLU. It deactivates a neuron (outputting 0) if it surpasses the average of the activations at the same layer by $k$ times their standard deviations. $k$ is a hyperparameter or can be learned by some modification proposed by the authors. They treat this component as a new activation function and name it ZeroLiers. The authors claim that ZeroLiers can replace the functionality of Dropout and show experimental evidence with various network structures.",
            "main_review": "Major comments:\n- Although the authors claim ZeroLiers is an activation function, I disagree with that. ZeroLiers is fundamentally different from traditional activations because the output of ZeroLiers depends on neurons within the same layer. It would be more natural to put ZeroLiers in the same group of network components with Dropout, layer normalization and etc.\n- The main focus of the paper is to argue that ZeroLiers can achieve the benefit of Dropout. In Figure 1 and 4, it shows that with Dropout or ZeroLiers, the outputs of the activation grow slower. In Table 1, it shows that ZeroLiers perform even better than Dropout. However, I think the way that ZeroLiers selects outliers is closer to layer normalization than Dropout. Dropout aims to reduce the variance of the product of $w_i^{l+1}$ and $y^l$, instead of $y^l$ only. On the other hand, layer normalization makes neurons in the same layer having std equals 1, which in most cases reduces the std, works very similarly to ZeroLiers. Therefore, I prefer to view ZeroLiers as a variant of layer normalization. In this context, I check the codes submitted and found there is no layer/batch normalization in their MLP experiments. I am very curious about how these tables and figures change if layer/batch is added. \n- In CNN and various Transformer experiments, ZeroLiers shows no clear evidence to me that it is better. I am not sure how the hyperparameters $k$ and $k_0$ are tuned (why 3 and 4 are chosen?). The slight difference in Figure 8 might be just caused by hyperparameter selection. Also, could you show how BERT performs without dropout and ZeroLiers? The reason ZeroLiers not working great may be that CNN and Transformers have batch/layer normalization.\n\nMinor comments:\n- section 2.1, define $q$.\n- Figure 1 & 4, the demonstration is confusing. They should be called histograms of activation at iteration 1, 130k, 260k, 390k if I understand them correctly. Also, could you specify how many iterations an epoch consists of?",
            "summary_of_the_review": "My current evaluation of this paper is a rejection because I am not convinced that the network component proposed is as useful as claimed. I have questions about its performance when batch/layer normalization is also utilized.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}