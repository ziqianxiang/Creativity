{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors propose an SVD-based method for action exploration under certain safety criteria. The intuition of this paper is clear that SVD-based decomposition can make the action space into two subspaces where one is contained and the other is free. ",
            "main_review": "This paper proposes method that can make the policy satisfy the constraint as much as possible while minimizing the loss of reward. They also experimentally show that off-the-shelf RL algorithms can be augmented with the ADR module such that certain constraints are satisfied. \n\nThe paper is clearly written both theoretically and empirically.\n\nThe experiments are valid on simple environments such as keep it straight. The authors also show that under sudden constraints, this method would also work.\n\nThe main concern about this paper is that whether the proposed ideas can be applied to more realistic domains such as games and robotics.\n\n",
            "summary_of_the_review": "Good paper with straightforward ideas. Might need to be tested on harder tasks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a non-training method \"Action Decomposition Regular\" (ADR) to resolve the constrained reinforcement learning problem, which decomposes the action space into a constrained dimension and free dimension using SVD on top of the linear dynamics model. It theoretically proves the applicability of ADR on convex action space and the experimental results demonstrate its advantage over the unconstrained methods and reward shaping.",
            "main_review": "$\\textbf{Strengths}$:\nThe method is well-motivated and the technical contribution is significant. The problem considered is interesting and the proposed method is novel, especially the idea to decompose the action space, which I believe will be inspiring for future works. \n\n$\\textbf{Weaknesses}$:\nBasically, the writing and organization of this paper need improvement. Some parts of the paper are very confusing, I have the following questions and suggestions for the authors.\n\n(1) I highly recommend the authors assign the number for those important equations\n\n(2) For the first objective functions in section 4.2, I don't understand why the constraints inside $\\arg\\min$ are equations, and in the following text, these equations become functions, i.e., without \"$=0$\".\n\n(3) Clarify the definition of some notations, including $F$ for agent dynamics and $G$ for constraints, dimensions to be defined. Also for the pseudo inverse at the very end of section 4.2, the convex set $\\mathbb{D}$ at section 5.2, the definition doesn't follow their first appearances, organizations here need improvement. \n\n(4) In the main paper, only the decomposition of the action at time $T-1$ is considered.\n\n(5) The equation of reward shaping is in the wrong format.\n\nFor the experiment parts, I also have the following concerns.\n\n(1) ADR on DDPG only is not convincing, I expect more continuous RL algorithms to be tested.\n\n(2) Could authors explain why no constrained RL algorithms are used as baselines?\n\n(3) For the two experiments, especially \"passing the intermediate station\", the performance of DDPG and DDPG + ADR are almost the same at the end of the training from the plots, but in the analysis, it's claimed that DDPG + ADR achieves much higher cumulative rewards and much smaller violations of constraints, I expect the author to detail the scales here.",
            "summary_of_the_review": "This paper is innovative and well-motivated, and its applicability is theoretically supported. However, the writing and organization of the paper need to be improved a lot and the experimental results are not convincing. I recommend rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper considers reinforcement learning, a common model which has seen success recently in many areas (e.g. games, robotics, autonomous vehicles).  Prototypical reinforcement learning algorithms explore the action space in order to maximize their rewards as much as possible, potentially ignoring impacts of the chosen actions or safety constraints.  Typical to many real-world scenarios, though, are constraints on the selected actions allowing the algorithm designer or practitioner to enforce certain constraints on the selected actions in the environment (e.g. ensure that the autonomous vehicle stays within the dictated lines on the road, etc).  While many RL algorithms have been designed in this setting, they mostly focus on modifying existing training algorithms for RL to ensure satisfying the constraints by either modifying the objective used in policy gradient algorithms, projecting back to a 'safe' policy set, or linearizing certain objective functions for more readily easy computations.  In contrast, the authors in this paper take a different view, ensuring a post-processing step which projects the chosen RL action to the 'feasibility set' which is closest, simultaneously ensuring the constraints are satisfied while also maximizing the reward (by picking an action close to the action dictated by the RL algorithm).  \n\nTo be more specific, the authors consider the typical RL model with an MDP characterized via $(S, A, P, r, \\gamma)$.  They consider a (potentially random? but known time) in which there are a finite set of linear equality constraints that need to be satisfied over the next $T$ periods (the time horizon here being artificial and just being a time horizon established to have to satisfy the constraints).  Once this event occurs, the algorithm must satisfy a specific set of constraints where the dynamics satisfy $s_{t+1} = \\theta (s_t, a_t)$ (i.e. linear dynamics on the state space), $\\psi (s_t, a_t) = 0$ (i.e. linear constraint on the state and actions selected).  The hope is to pick actions which satisfy the constraints and equations dictating the dynamics over the next $T$ time periods while also maximizing the observed rewards.\n\nThe authors propose a novel method, termed 'Action Decomposition Regular' which is a post-processing step using action decomposition.  They provide a simple example, but the intuitive description is as follows.  The set of actions must satisfy a specific set of equations in order to satisfy all of the given constraints.  Write these linear constraints as an optimization problem, minimizing the $\\ell_2$ norm of the equation (without the equal to zero) constraint.  Clearly a solution which satisfies the equation will have that value equal to zero, and hence any solution satisfies the constraint.  This optimization problem can be solved, in closed form, giving that the actions and states must satisfy a specific series of equations.  Taking the SVD of these constraint matrices allows you to decompose the actions into two different terms.  One term, the 'free' dimension, can be used in order to steer the algorithm to take actions which are 'close' to the one dictated by the RL algorithm.  The other, the 'constrained' dimension, is then used to ensure that the actions satisfy the set of constraints.\n\nTo complement the algorithmic framework, the authors present a set of synthetic experiments to compare the efficiency and constraint violation of the resulting policies of their method and others in the literature.  In particular, they test the algorithms on a set of movement tasks with the goal of 'keeping the algorithm straight' or 'passing an intermediate station'.  They compare their ADR technique combined with a deep RL algorithm to just the naive deep RL algorithm.  Obviously, their algorithm which additionally enforces the constraints will satisfy them, with a minor loss in performance.",
            "main_review": "### Originality:\n\nThe authors present a novel technique for RL settings with the addition of linear equality constraints.  The approach presented is a simple post-processing step (taking the SVD of the constraint matrices to decompose the action space into two terms, one of which used to ensure constraints and the other to best approximate the RL algorithm).  However, the authors make it clear that their approach has advantages in that it can easily be included in any RL algorithm.  Moreover, there are no theoretical guarantees for their approach (outside of a pareto-type guarantee which is not explained well).\n\n### Quality:\n\nThe submission seems technically sound and the theoretical claims are moderately well-supported, but the notation and description of the algorithm are confusing and difficult to follow, with some notation which is never explicitly outlined in the main paper.  The authors are honest and upfront in the new techniques used in their modeling and algorithm development, namely:\n- Knowledge of the set of constraints which needs to be satisfied\nHowever, the numerical experiments done are not robust and there are no comparisons to other related algorithms in the literature.  They simply compare their post-processing step to an algorithm without the post-processing, and obviously their algorithm will satisfy the constraints as it is designed to do so.\n\n### Clarity:\n\nThe submission is well-organized.  However, the submission could use some extensive rewriting to help with clarity to better describe their algorithm design, approach, provide real-world examples with these constraints, and highlight the technical novelty.  The authors should take a pass through the paper and address the following:\nAt a high level please address the following:\n- Include spaces before a parenthesis (including a citation) to help the paper read better\n- Do not start sentences with 'And'\n\nAnd more specifically:\n\n- The first two sentences in the abstract need to be rewritten, don't provide anything useful\n- 'making policy strictly' in the abstract\n- 'physically-based environment'? in abstract\n- 'prevail' in abstract-\n- Typo in first sentence of introduction\n- Grammar issues in last sentence of first paragraph of introduction\n- Second paragraph 'in the application' - which applications?\n- The example provided in the second paragraph should be more concrete\n- Grammar issues in first sentence of third paragraph\n- 'In fact, the constraint guarantee for the agent's behavior....' is awkwardly written\n- On page 2 - what is 'model-based technology'?\n- The major contributions on page 2 (i.e. points 1-3) could be better explained and clarified (e.g. 'show good results')\n- 'agree to be model-based'\n- 'when facing with'\n- Last paragraph of section two is confusing as your algorithm also requires knowledge of the constraints\n- 'dynamic knowledge' in start of section 3.2 - what is this referring to?\n- Make equations on top of page 4 have text for the words\n- 'speed coordinate system' - also, shouldn't the picture just be an exact circle instead of an oval?\n- 'the solving technology'\n- Equal to zero inside of norm on bottom of page 4 should not be there\n- double space in \"Appendix  A\" on page 5\n- What is the set D in section 5.2 - this entire section was confusing as the two 'objectives' are never described in words, and never referenced explicitly with respect to the RL algorithm\n- \"Suppose to exist\" in Theorem 5.1\n- \"constraints exceed convex action space\"\n\n### Significance:\n\nThe algorithm presented in the paper is a simple SVD style approach to decompose actions to ensure satisfying the constraints while simultaneously selecting actions close to the one provided by the RL algorithm.  While the novel theoretical techniques are very limited, the work can be built upon by performing more robust experiments, explaining more scenarios which satisfy these simple linear constraints, and better - explaining the theoretical guarantees.\n\n### Strengths:\n\nThe main strengths of the paper are as follows:\n- simple approach that can be included with any RL algortihm, requires no additional training or meta-parameters\n- easy to compute and does not add to the computational complexity of the algorithms\n\n### Weaknesses:\n\nThe main weaknesses of the paper are as follows:\n- the final model, experimental details, and algorithmic approach should be better explained (see clarity section)\n- experiments should be compared against other algorithms designed to satisfy constraints (instead of the simple comparison provided here)\n\n### Questions:\n\n- When do the \"constraints\" happen? It is interesting the constraints are modeled with a finite time horizon but the authors consider the MDP setting with an infinite horizon and discount.  The notion of the time horizon is a bit confusing.\n- What are the point of the experiments? Clearly your algorithm will satisfy the constraints, but why are there no 'learning' in the algorithms? Or is it difficult to see the curve of the line due to the axis chosen?\n- What is the relation between constraint-to-go and the rewards, seems like they got lost somehow?  ",
            "summary_of_the_review": "The paper focuses on an action-decomposition approach that can be incorporated as a post-processing step on any RL algorithm to satisfy linear equality constraints.  While the approach is simple and intuitive, the authors provide no robust experimental results, the paper is difficult to follow, and there are no theoretical guarantees on the approach.  While the approach is nice, the authors need to help clarify their approach.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a method called Action Decomposition Regular (ADR) for guaranteeing safety in RL problems with linear equality constraints. ADR is inspired by SVD and decompose the action space into constrained dimension and free dimension separately. The authors empirically compared their proposed method with several baselines (e.g., safety-agnostic one, reward-shaping).",
            "main_review": "I think it is an interesting idea to use SVD for safe RL is interesting. As far as I know, the key idea is novel and would be helpful under a strong assumption that the dynamics are linear.\n\nI have several concerns.\nFirst concern is experiment. The environments used in this paper are multi-agent particle world, and this task is known to be easy. Given there is no theoretical result (regarding the performance of the algorithm) in this paper, I think the authors should have tested their proposed method in more complicated environments. Recently, Safety-Gym has been a popular as a testbed. Also, the baselines are safety-agnostic DDPG in Section 6.1 and safety-agnostic DDPG and reward-shaping in Section 6.2, which are also rather weak baselines. I do feel that the authors should have compared their proposed method with more powerful baselines (e.g., CPO, PPO-Lagrangian) in more complicated tasks (i.e., Safety-Gym). I think it is possible to implement CPO or PPO-Lagrangian as the authors conducted for reward-shaping (i.e., Section 6.2.2).\n\nSecond concern is that mathematics in this paper is hard to understand. I think this situation may be resolved by, for example in Section 4.2\n- Use $\\top$ instead of $T$. In this paper, symbol for transpose and time are same, which is very confusing.\n- Clearly write the dimension of each matrix (e.g., $\\mathbb{R}^{d_1 \\times d_2}$) and add some figures.\n- $r$ is defined in two different meanings (reward and rank)\n\nAlso, Section 5.2 is hard to follow. I guess major reasons would be that the \"effective solution\", \"efficient solution\", and $\\mathbb{H}$ are not explained. I would recommend the authors to explain the motivation or intuition behind this analysis.\n\nFinally, I consider that important aspects have not fully discussed. in the Introduction, the authors emphasize the importance of guaranteeing safety and never violating constraint. However, in Figure 6, the authors' proposed method violate the safety constraint even after a long training episodes. The authors should have discussed why this happens (I guess it is due to non-linearity). ",
            "summary_of_the_review": "Though the key ideas are interesting and new, this paper is badly written. The empirical evaluation has not been fully conducted; that is, the authors' proposed method has not compared with reasonable baselines in reasonable environments. Hence, I vote for rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}