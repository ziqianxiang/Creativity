{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a method for rejecting adversarial examples that are actually misclassified. This is done by employing a classifier with an extra rectified rejection head which is trained to predict the classifier's predicted probability for the correct class.\n\n",
            "main_review": "Major issues:\n\nThe 'TPR-95' accuracy which is the main evaluation metric used in this paper makes limited sense to me. It means that accuracy is evaluated on a different number of samples for each method. If a model has no correctly classified examples, it is not well defined. If the model is correct on only very few outputs, it might have a better score than one that is correct on many but because of this is evaluated with a lower rejection threshold. Perhaps a more in-depth explanation why this metric is interesting in practice would help. The number/fraction of rejected samples should be given in or with Table 2. A comparison of other metrics used for evaluating methods with adversarial detection should be included. \n\nPerformance against non-adaptive attacks is not interesting (beyond an ablation), so the focus of the results section should be on results obtained from attacks that also target the RR module. It is important to discuss which kind of attacks make sense and why, and to use adapted state-of-the-art schemes. It is not clear (or explained) why an adaptive attack should target the RR loss and not maximize R-Con. The evaluation with such attacks that properly fit the task is not sufficient, and for the few attacks that are shown in 6.3 it is dubious if they have the correct objective.\nWhen adjusting for the TPR-95 evaluation, attacks on all models need to be modified.\n\nThe theoretical considerations that are made make very strong assumptions on the possibility of training a good robust rectified rejector, but it is not made clear (theoretically or experimentally) that obtaining one is realistically possible.\n\nMinor comments:\n\nIn the caption of Table 1 (or in the text of 3.1 or a footnote) the details of the used models should be referenced.\n\nSome of the theoretical considerations are quite trivial. For example, 3.2 only presents a lemma that is immediate from its assumptions and can in my opinion not be called 'intriguing' in any way.\n\nThe definition of binary cross-entropy BCE in Eq. (9) in C.1 misses minus signs. Was it used like this as a loss, or is it just in the write-up of Eq. (9)?\n\nIt would be good if the paper described an intuition behind Definition 1: Why does it need 2 alternative bounds (not just the easier second one), and what motivates the first one?\n\nThe intuition about the learnability of A given after Theorem 2 looks wrong to me. There is no reason that comparing the number of classes with very different tasks is usually informative for the difficulty of a classification task.\n\nThe discussion of rectified rejection vs. binary rejection is not very convincing to me, since I see no reason why binary should be more susceptible to being 'overwhelmed' or why ease of optimization should differ between M and A. Taking advantage of model sharing can be done (and has been done, and also is implicitly done when using a reject class) in the same way for binary rejection.\n\nThe 'All' accuracy mentioned in the text is missing in Table 2; it should be given.\n\nHaving an adversarially robust estimator of A that is accurate, i.e. fulfills Definition 1 for a useful xi is not a mild assumption (cf. abstract). Separability in 4.2 is only 'certified' if A is xi-error, which is a fact that can only be 'certified' if its result is already known to be correct. Calling this 'Certified separability' is akin to calling an AT classifier 'certifiably robust' at an input if that input is simply classified correctly. This is not the usual notion of certifiability.\n\nWhat is meant by the numbers for l-infinity in Table 7?\n\n",
            "summary_of_the_review": "The paper is mostly well written and formatted, but the evaluations are not sufficient for showing that the method actually has the robustness that it needs to have to be useful, and the theory only makes sense under the assumption that such a sufficiently strong model can be made available.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this work, the authors propose to use true confidence (T-Con) (i.e., predicted probability of the true class) as a certainty oracle, and learn\nto predict T-Con by rectifying confidence. They prove that under mild conditions, a rectified confidence (R-Con) rejector and a confidence rejector can be coupled to distinguish any wrongly classified input from correctly classified ones.",
            "main_review": "Strengths:\n\n1. The empirical evaluations are comprehensive.\n2. The authors theoretically justify their approach.\n\nCons:\n1. Is there any early-stopping in training? How is the early-stopping hyperparameter tuned?\n2. Are the experiments based on one single run? What will the ACC be under different random seeds?",
            "summary_of_the_review": "Some confusion exists empirically. I recommend weak-accept at this stage.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper targets at improving neural networks' robustness against adversarial attack and distributional shifts. This paper proposes to incorporate a module to reject misclassified predictions via coupling confidence and T-Con which is the predicted probability on the true label. Further, since T-Con is not available during testing time, this paper trains a rectified confidence (R-Con) to approximate T-Con. The coupling between confidence and R-Con can still achieve good performance, which is proved in *Theorem 1*. Next, this paper tries to quantify the difficulty of learning the R-Con bassed rejector via comparing it to the CIFAR-10 and CIFAR-100 classification problems. Finally, this paper evaluates its method on adversarial attacks and distributional shifts. This paper outperforms other methods in most of the cases.\n\nTo summary, this paper's main contributions are:\n- Propose to utilize T-Con and confidence to reject misclassified predictions;\n- Approximate  T-Con by R-Con via rectifying confidence;\n- Quantify the difficulty of training R-Con;\n- Evaluate robustness against adversarial attacks and distributional shifts.",
            "main_review": "\n## Strenghth\n\n- This paper targets at rejecting misclassified inputs, instead of adversarial inputs. This is more reasonable because many adversarial samples are correctly classified.\n- This paper proposes to utilize T-Con, instead of the commonly used confidence, to reject misclassified samples. It then leverages a R-Con module to approximate T-Con during testing time.\n- This paper's RR module can well incorporate with different Adversarial Training frameworks.\n- This paper evaluates on both adversarial attacks, adaptive attacks and distributional shifts (CIFAR-10-C). They results outperform baselines.\n\n## Weakness\n\n- Baselines might not be the state of the art. For instance, SOTA performance on CIFAR-10-C can be found here: [CIFAR-10-C Benchmark](https://paperswithcode.com/sota/domain-generalization-on-imagenet-c). It would be good if the authors could evaluate their method on all corruptions on CIFAR-10-C and report their mean Corruption Error (mCE).\n- *Theorem 2* needes more explanation. *Theorem 2* compares learning R-Con to CIFAR-10 and CIFAR-100 and makes the claim that the test accuracy is expected to be between 90% and 70%. First, learning R-Con, which is a regression problem, is different from the image classification problem; second, the claim *under the similar data distribution, the classification problems with a larger number of classes are usually (not necessarily) more difficult to learn* is vague. It would be good if the authors clearly define *under similar data distribution*. Also, it would be appreciately if the authors can provide some reference supporting their claim. \n- R-Con performance is hard to evaluate on unseen samples. This shares the same concern with confidence calibration based method. Specifically, R-Con is trained on training data while the confidences on testing samples during inference may not follow the same property, similar to the problem with calibration problems in *Related Work* section. It would be good if the authors can address this concern.\n- $A_{\\phi}(x)$ might be hard to train. The authors define $A_{\\phi}^*(x) = \\frac{f_{\\theta}(x)[y]}{f_{\\theta}(x)[y^m]}$. One concern with this definition is that the denominator, $f_{\\theta}(x)[y^m]$ will keep changing during the training process. As a result, $A_{\\phi}^*(x)$ will keep changing during the training process, which may make the training process hard to converge. It would be good if the authors could address this concern.\n- For multi-class classification problems, if the confidence threshold is set to $\\frac{1}{2}$, the resulting rejection module might be too conservative in that even if T-Con $\\leq \\frac{1}{2}$, the classifier could still make correct predictions.\n- In section 4.1, the authors introduce the *stop gradients* mechanism. One concern is that R-Con will not learn to approximate T-Con when $y^m=y$. During inference, R-Con will mislead the rejector.\n\n## Suggestions and Questions\n\n- In *Introduction* section, the authors writes that *Stopping gradients on the confidence $f_{\\theta}(x)[y^m]$* when $y^m=y$. It would be good to define $y^m$ here.\n- In section 4.2, the authors make the plot with different $\\xi$. I do not understand why $\\xi$ is a hyperparameter here? My understanding is that it is determined by the R-Con.\n- Also in section 4.2, the authors mention that T-Con are not guaranteed to be order-preserving with respect to $\\tau$ among different inputs. Why is the claim important here? How is this claim related to how $\\tau$ is selected?",
            "summary_of_the_review": "Generally, this paper proposes some novel ideas about improving the robustness against adversarial attacks and distributional shifts. Their results can outperform some baselines. The main concern is that they might not compare to the SOTA results and some of their theorems need to be further explained.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose to use true confidence (T-Con) as a certainty oracle and learn to predict T-Con by rectifying confidence. Theoretically, they prove that under mild conditions, a rectified confidence (R-Con) rejector and a confidence rejector can be coupled to distinguish any wrongly classified input from correctly classified ones. Also, they quantify that training R-Con to be aligned with T-Con could be an easier task than learning robust classifiers. Empirically, they evaluate the proposed rectified rejection (RR) module on CIFAR-10, CIFAR-10-C, and CIFAR-100 under several attacks, and demonstrate that the RR module is well compatible with different AT frameworks on improving robustness, with little extra computation. ",
            "main_review": "I think this paper has the following strengths: \n\n1. The authors explore a new direction to improve adversarial robustness by introducing a rejection option, which is promising and interesting; \n\n2.  They have theoretical analysis to show that the proposed rectified confidence (R-Con) can be used to distinguish any wrongly classified input from correctly classified ones and training R-Con could be an easier task than learning robust classifiers; \n\n3. They perform extensive experiments to evaluate the proposed method R-Con and show that it is effective in improving adversarial robustness. \n\nHowever, I think this paper has the following weaknesses: \n\n1. An important related work [1] is not discussed in the paper. [1] proposed True Class Probability (TCP) for the task of failure prediction (i.e. detecting misclassified inputs) and provided theoretical guarantees for TCP in the context of failure prediction. They also proposed to learn the TCP criterion on the training set by introducing a specific learning scheme adapted to this context. I think the idea in [1] is very similar to the idea in this paper. The authors should compare their method to the approach proposed in [1]; \n\n2. The authors propose to only reject the inputs that are misclassified. That is they don't want to reject adversarial examples (or perturbed inputs) that are correctly classified by the adversarially trained models. Then a natural question is: will the proposed rectified rejection (RR) module reject perturbed examples that are correctly classified? I think they should evaluate their method under the adaptive attacks that try to generate perturbed examples that are correctly classified and rejected. But it seems such attacks are not evaluated in their experiments. Thus, the current experimental results cannot show that the proposed method could distinguish correctly classified inputs from misclassified ones; \n\n3. Some adversarial attacks used in the evaluation are not strong. For example, in Table 2, they only evaluate the models under the PGD attack with 10 steps. I think such evaluation is insufficient in evaluating models' adversarial robustness. The authors should follow the guidelines in [2] to evaluate adversarial robustness. I suggest the authors use more attack steps and some random restarts for the PGD attack to make it stronger. The results in Table 2 should be updated with strong attacks; \n\n4. I think the models' adversarial robustness should be evaluated under the strongest adaptive attacks. That is the attacker should know all the details about the defense mechanism. The defense framework considered in this paper is a classifier plus a rejector. Thus the attacker should attack both the classifier and the rejector. The results in Section 6.1 are less interesting since they are evaluated under the attacks that only target fooling the classifier.  Although the authors also evaluate their method under the adaptive attacks that try to fool both the classifier and the rejector in Section 6.3, the adaptive attacks proposed may not be strong enough. In their second adaptive attack where they fix the maximal perturbation size to $8/255$ under $\\ell_\\infty$-norm, the adaptive objectives may not be strong enough. There might be other adaptive attack objectives that could make the attack stronger.  For example, they can use $\\min(\\mathcal{L}\\_{CE}, \\mathcal{L}\\_{RR})$ instead of $\\mathcal{L}\\_{CE} + \\eta \\cdot \\mathcal{L}\\_{RR}$. Also, they can try different loss functions for attacking the rejector (i.e. replace $\\mathcal{L}\\_{RR}$ with other loss functions such as using $\\text{BCE}(f\\_\\theta(x)[y^m]\\cdot A\\_\\phi(x) || 0)$ or using $f\\_\\theta(x)[y^m]\\cdot A\\_\\phi(x)$). They should take some effort to demonstrate that the adaptive attacks used are strong enough. Besides, they should consider attacking the classifier plus the rejector adaptively in two directions: generate perturbed inputs that are accepted but misclassified, and also generate perturbed inputs that are rejected but correctly classified; \n\n5. Some experimental details are missing. Could the authors give the detailed training setup for CCAT? Do they use the exact training setup specified in the CCAT paper? If they use different training settings, they should specify them. In Table 2, they should also report the accuracy or robustness of the adversarially trained models without rejection (i.e. we don't use the rejector and accept every input). For the adaptive objectives proposed in Section 6.3, they should write the exact loss functions for $\\mathcal{L}\\_{CE}$, $\\mathcal{L}\\_{Con.}$ and $\\mathcal{L}\\_{RR}$ (e.g. the equation (4) in the paper). \n\n\n[1] Charles Corbière, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, Patrick Pérez: Addressing Failure Prediction by Learning Model Confidence. NeurIPS 2019. \n\n[2] Carlini, Nicholas, et al. \"On evaluating adversarial robustness.\" arXiv preprint arXiv:1902.06705 (2019).",
            "summary_of_the_review": "I think this paper studies an important problem and has some interesting theoretical results. However, an important related work that shares similar ideas is not discussed and the empirical results are insufficient to show that the proposed method could improve the adversarial robustness (or distinguish misclassified adversarial examples from correctly classified ones). Moreover, the adaptive attacks considered may not be strong enough. Thus, I think this paper is not ready for publication. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}