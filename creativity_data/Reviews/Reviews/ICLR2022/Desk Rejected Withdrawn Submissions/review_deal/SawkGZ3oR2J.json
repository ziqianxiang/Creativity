{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes to use split learning framework in federated learning to reduce computation, communication and latency of FL. The idea is straightforward and simple. All clients will perform FedAvg on a much smaller network than the original one. The small network contains the first few layers of the original network and a very small auxiliary network. At each iteration or round, the clients send the client-side model activations to the server to train the remaining layers in the original network. Compared to the vanilla FedAvg algorithm, both computation and communication are reduced. The authors provide theoretical guarantee as well as experimental results to validate the effectiveness of the proposed method.",
            "main_review": "In general, I think the proposed idea is promising and interesting. Basically the algorithm is equivalent to performing FedAvg on a smaller neural network. Meanwhile, the server will train another network using the client's outputs. During inference, these two parts of neural network will be combined together.\n\nBut I have few concerns listed below.\n1. It is unclear how to allow local updates in the proposed algorithm. In both the main paper and appendix, the authors only describe the algorithm in terms of only one local step. I'm confused does the algorithm allow local updates? Do you allow local updates for the proposed algorithm in your experiments? I feel allowing local updates is crucial in the algorithm in order to ensure a fair comparison with FedAvg.\n2. Related to the above point, it is unclear when the clients send the activations to the server. Do they send the activations after every local iteration or after multiple local iterations?\n3. Compared to vanilla FedAvg, clients now transfer many additional information to the server, including the labels of each mini-batch and the activations of the client outputs. The authors mentioned that they will discuss the privacy issues in the Appendix. But I didn't find any. Do I miss anything here?\n4. It seems that the proposed latency model is not very practical. For example, the authors assume that the local computation is in proportion to the model size. This is not true for CNNs. It is well known that in a neural network, fully connected layers have more parameters but is cheaper to compute, convolutional layers have fewer parameters but is more expensive to compute. I notice that the authors used this latency model as the x-axis in all plots. I'm afraid that the conclusions might be misleading, given the latency model is not practical. I encourage the authors to evaluate the real training time.",
            "summary_of_the_review": "While the proposed algorithm is promising and novel, the current paper did not describe the algorithm very clearly. And I have a few concerns on the privacy issue and latency model.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose to split models such that a central server contains part of the model and each client the other part and use local learning to minimize the communication cost. The goal is to allow more efficient federated learning with lower client side computation cost. Unlike prior proposals server side processing is increased but can be performed in parallel to  client updates thus allowing for overall lower global training time\n",
            "main_review": "\nStrengths\n\n-The authors propose a promising direction for distributed and federated learning that improves on the SplitFed proposal substantially\n-The proposal is interesting and studied in several experimental settings showing gains over SplitFed and one variant of traditional FL\n-The authors show the theoretical complexity in communication, computation and latency\n\nWeakness/Concerns (* indicate most important concerns)\n\n-*Although the proposal improves in communication on SplitFed, the forward communication cost of both SplitFed and the proposed local version will be large as the model sized increases, depending on the split point. The current experiments are using small sized images and models from MNIST and CIFAR-10, it seems to the reviewer the communication cost of SplitFed and Proposal would scale poorly compared to FL as the width of the model and size of the images increases. \n\n-Related to above I would like to see what happens to the curves in Figure 2 for larger convolutional models especially in communication, and latency. I think it would be useful, at least in the appendix, to have Figure 3 with the axis being communication rounds at some point as well to better understand the convergence of each method in terms of communication rounds. \n\n-The authors assume the computation is directly proportional to model size, which is not the case in many situations. I think the analysis would be more clear if it simply split the client and server compute cost instead of implying its proportional to parameters. \n\nDescription of Splits - The details of this split point should be discussed in more. The authors have some analysis (that again really should be computation time versus model size), but the model accuracy will be affected by the split point as well and the communication cost will be affected (e.g. if the split point follows a pool operation or not). Also the details of the split are poorly described for VGG-11, the authors simply state the amount of parameters in client and server which is uninformative as the number of parameters is not always related to computation particularly in a model with lots of FC layers like VGG-11. \n\n*Accuracies- the accuracies obtained are quite low in some cases. For example for MNIST in iid settings the authors report accuracies of 92% for “FL” which is FedAVG (right?), which is relatively low on MNIST. Indeed the number of clients is 1000 but it would be good to have some reference settings that follow previous work to confirm the baseline is strong. E.g. in Mcmahan 99% is obtained with FedAVG using 600 clients. In general would have liked to see stronger baselines and effects at lower number of clients. Previous work used different learning rates and took results for the best performing one (e.g. McMahan et al), what is the hyperparameter selection approach her for the optimization/lr schedule\n\nPrivacy - Classic Federated Learning provide some simple protections on data privacy of individual nodes, although of course many recent attack vectors have been proposed. SplitFed and the proposed method seem that they would be more vulnerable to a privacy attack since they would require transmission of per sample representations versus an aggregate vector. The authors mention this and point to a discussion in the appendix which I could not find.  \n\n*Reproducibility/Code - Although some details are provided I believe the results would be challenging to replicate without the code. Do the authors plan to release code with the submission?\n",
            "summary_of_the_review": "Overall the work propose a promising direction and has interesting insights and experiments. The proposal is both novel and technically challenging and I lean towards acceptance but hope the authors can make some substantial clarifications during the rebuttal. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a new approach to federated learning that combines ideas from split learning with decoupled greedy layer-wise training. They use split learning to reduce the computational burden at the client by offloading parts of the model parameters to the server. Then, they further decouple the client and server-side updates to avoid communication and latency burdens of backpropagating gradients in every update step.\n- The author theoretically analyze the latency of their method and illustrate how it depends on compute power, transmission rate, model splitting, model size and data size. They conclude that from a latency perspective it is best to allocate as many layers as possible to the server. \n- A convergence analysis for the local client and server objective is presented, extending previous results on decoupled layer-wise training to an additive client objective. It gives conditions on the loss function under which the method converges to a stationary point.\n- From an empirical side the method is compared to FedAvg and FedSplit in terms of communication efficiency and latency on MNIST, CIFAR10 and FMNIST.\n",
            "main_review": "The core idea of combining split learning with greedy layer-wise training to reduce the communication and latency bottleneck between client and server is interesting and seems practically relevant. The approach is quite straight forward, and well described.\n\nMy main criticism is that a critical evaluation of the proposed method that clearly reveals its strengths and weaknesses is lacking.\n\n1. On page 6 you promise a discussion of how the split learning approach might compromise privacy by sharing labels and transmitting activations. But I could not find a single paragraph in the appendix. \n\n2. I would like to understand better what the role of the auxiliary model is in your approach. Your approach crucially relies on this auxiliary model to improve over SplitFed. To me this aspect seems important but is not sufficiently discussed in the paper. Let me elaborate on this in the following three point:\n \na) In Section 3 you ignore the contribution of the auxiliary model when analyzing the communication and computation overheads of your method without further discussion it. You will be in the regime of small $P_C$, small $R$ and small $\\alpha$ so it seems that this contribution is not negligible in the limit. There must be some overhead of synchronizing and training the auxiliary model in addition to the client model.\n\nb) The theoretical analysis in Section 4 does not reveal how the quality of the final solution depends on the choice of auxiliary model. It is proving convergence to a stationary point give any choice of auxiliary model.\n\nc) In the empirical study you only analyze one fixed choice of auxiliary model. It would be nice to vary the complexity of this model to see how the performative of the algorithm depends on this choice.\n\n3. The second interesting parameter would be $\\alpha$. Here you are also only experimenting with one value. Intuitively it seems easier to do local loss-based training with a larger alpha, because the part that you need to approximate with the auxiliary model becomes less significant. How does $\\alpha$ impact the gain over FedSplit? \n\n3. In your empirical experiments when you show accuracy as a function of communication load and training time. I don’t understand if this is an actual measured of your algorithm or if it is an extrapolation from the number of steps using your model from Section 3? Could you please clarify! If you are using the model my concern from 2a) applies here too. \n\n\nSmall comments:\n- in 3.3 it is not clear what $\\beta$ is\n- You state \"It is shown in (Li et al., 2019) that this algorithm converges to the optimal solution of the above objective function\". it would be good if you could be more precise that this requires additional assumptions.\n- multiple references are missing, e.g. [Li et al].",
            "summary_of_the_review": "Overall I like the proposed idea of combining split learning with greedy layer-wise training, but the paper lacks some important discussion and empirical insights on the sensitivity of the method with respect to the critical design parameters. \nA discussion of the privacy issues that come with using their method for federated learning is promised but missing from the appendix.\n\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors proposed different approaches using local loss functions in both client and server sides for accelerating federated split Learning. And they theoretically analyzed the convergence of the proposed methods. Evaluations under some datasets show its outperformance over existing FL and SL-based methods.",
            "main_review": "Strengths:\n1) The different approaches using local loss functions in a SL setup are proposed, reducing the computation, communication and latency of current FL/SL systems.\n2) The proposed method for convergence guarantee is theoretically analyzed.\n3) The experimental results look promising compared to some existing FL and SL-based ideas.\n\nWeaknesses:\n1) I guess the authors should compare their methods with He et al.(2020) more deeply, theoretically and experimentally. The final goal of this paper and He et al.(2020) should be the same, are to get an optimal global model, although their intermedium goals are different. \n2) The experimental setting is too simple. a) The models are too simple. b)The baseline should include VL of SL Variants of SL, e.g., He et al.(2020).\n3) The claim and the experiments are not totally aligned. The experimental results should prove the improvement and comparison of the proposed methods in terms of computation, communication, latency.",
            "summary_of_the_review": "The main contribution is using local loss functions in an SL setup for accelerating federated split Learning. But they lack deep comparison with other methods, especially He et al.(2020). The experimental setting is too simple. So they cannot show the major novelty and significance. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}