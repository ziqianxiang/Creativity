{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes to robustify pretrained image classification models by integrating modules between sections of the network and then using adversarial training to train these modules while keeping the parameters of the pretrained model fixed. This way the authors achieve robustness to adversarial attacks similar to adversarial training while keeping the clean accuracy almost at the level of the pretrained model. Additionally they show that when trained with only 10 or 1% of the training data they still achieve very good results on both clean an adversarial robustness.",
            "main_review": "The proposed method is interesting because it is a simple addition to a pretrained network that can be trained quickly and with a small amount of data. I think approaches like this can have a lot of merit as they allow to give robustness to fixed models relatively cheaply. The approach also preserves clean accuracy mostly which is a very important property. The paper is well written and gives a good explanation on the method.\n\nI think the main problem with it are that the analysis is not extensive enough to allow a full evaluation of the capabilities of the method and its generality. Additionally I'm not fully convinced on the usefulness of the defence if training on AutoPGD doesn't transfer to the far simpler FGSM attack. I don't think such a model is really as robust as the evaluation on the trained attack makes it seem. \n\nThe ablation studies are very brief. For the δ parameter only 3 values were tried (where δ=0 is equal to not using the fixer modules), for λ only two values were tried. Trying a few more values for each of them while keeping the others fixed would allow a better analysis of how the approach behaves. Also the ablation is only done for CIFAR10, so it is not clear if the parameters depend on the dataset.\nAlso neither the compression factor ρ, nor the position of the fixer modules is investigated, which makes it hard to gauge how universal the specific architecture is or if it needs to be tailored to a specific dataset and model. \n\nAccording to the footnote on page 6 all the confidence intervals come from splitting the test set in 5 parts and averaging the results. Normally confidence intervals for the performance are given by training the model with multiple random seeds to give an idea how much variance there is in the results and if any differences are significant. Calculating it based on splitting the test set doesn't help with this. Especially for the cases where the models were trained with only 1 or 10% of the training data it would be more important to see how dependent the results are on the exact subset that is chosen.\n\nThe model is only tried on CIFAR10 and a medical dataset I'm not familiar with. The results on the latter show that it is probably too simple to give good insights on the merit of the proposed method. While it is good to show models working on realistic datasets I think it would have been better here to use a second dataset that has been studied widely in terms of robustness like MNIST or better a more challenging dataset like tiny imagenet.\n\nOther comments and questions:\n- Have you tried adversarially attacking the whole network (pretrained and fixer modules) to see how robust the full model is?\n- Shouldn't the l2 distance in equation 4 be between the clean and adversarial samples, i.e. l2(h_i^δ, H_{i, adv}^δ) + l2(h_i^0, H_{i, adv}^0)?\n- One page 5 there are two cases where citations should be in parentheses: Bortsova eta al...) and Oner et al...\n- Figure 4 and 6 would be better readable if they were barplots like figure 3, 5 and 7. The only advantage I see is that the symbols make it better accessible for colorblind readers but then this could be achieved in using more shadings for the bars. \n- Figure 3,5 and 7 say \"Error bars are smaller than the size of the symbols in the figure caption, despite not using symbols. \n- In table 1 the base models gets a robust accuracy of 4.6% but in the right column it says that the robust accuracy is 0% of the samples that are correctly classified by the base models. Does that mean that all samples that are correctly classified under the AutoPGD attack are incorrectly classified when clean? I'm a bit surprised by that. Also I'm not sure how helpful the right column is, mostly it is just a rescaling of the left column and I don't think it gives more insights.\n- On page 7 one \"observed\" is too much: \"[...] as was observed also observed when training [...]\"\n- The abbreviation \"dns\" for the denoiser is only used a few times on the bottom of page 7 and it is not used in the tables or figure legends which makes it a bit confusing, especially when \"drn\" is used throughout for the distribution regression network.\n- The second sentence of the conclusion is a bit odd: \"Adversarial attacks are particularly worrying due to their ability to fool human observers\". Normally we use human perception as the gold standard in terms of robustness, so the models are fooled by the attack. If the attacked image would look like a different class to a human then it wouldn't count as a successful attack.\n- The table in the appendix about the attacks seems to be broken. It doesn't have a number or caption and the reference to it in A.2 doesn't work (\"These are summarized in table.\")",
            "summary_of_the_review": "While presenting a promising method I think the paper doesn't analyse the method deeply enough and the results are not good enough to warrant acceptance to ICLR.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors proposed a two-stage method of constructing a prediction model robust to adversarial attacks.\nIn the first phase, a prediction model with clean training data is learned. In the second phase,\na small networks are added in each layer of the prediction model learned in the first phase.\nThe small networks are learned such that the resulting final prediction model is robust to adversarial attacks.  ",
            "main_review": "Strength\n - The idea is new and interesting\n - The paper is well written.\n\nWeakness\n  - If I understand the proposed method correctly, the final model is robust to adversarial attacks with respect to the original prediction model learned at the first phase (i.e. Equation (2)), but not clear whether\nthe prediction model is robust to adversarial attack with respect to the model itself (i.e. adversarial examples are constructed by the final prediction model instead of the original model). \n\n- The results of numerical experiments are not well matched with the previous known results. For example, it is hard to understand that the robust accuracy of AutoPGD is higher than that of FGSM in Table 1 and Figure 3. AutoPGD is a method to construct harder adversarial examples by applying FSGM iteratively. In most other papers, the robust accuracy of FGSM  is higher than that of Auto PGD (eg. see\nreferences 2,3,4,5).\n\n- Even using 10% of the whole training data, the proposed method outperforms the SOTA, which is\ntoo good to be true. It may be because adversarial examples for the proposed method are constructed with respect the original prediction model.\n\n- It is not clear how complex the ResNet9 that is used for the base model. For adversarial robustness, WideResNet would be a base model for comparison (e.g. Reference 2). For fair comparison, a same architecture should be used.\n\n- AutoPGD is used for teat attack. What does AutoPGD mean? Is it an ensemble of APGD-CE and APGD-DLR? \n\n- In many recent studies for adversarial robustness, AutoAttack (Reference 1) which is an ensemble of\nAPGD-CE, APGD-DLR, FAB, Square attack, is popularly used and the proposed method should be tested \nwith AutoAttack.\n\nReferences\n[1] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. arXiv:2003.01690 [cs, stat], August 2020.\n[2] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards Deep Learning Models Resistant to Adversarial Attacks. arXiv:1706.06083 [cs, stat], September 2019.\n[3] Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adversarial robustness requires revisiting misclassified examples. In International Conference on Machine Learning (ICML), 2020.\n[4] Alex Lamb, Vikas Verma, Juho Kannala, and Yoshua Bengio. Interpolated adversarial training: Achieving robust neural networks without sacrificing too much accuracy. In Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security, pp. 95–103, 2019.\n[5] Tianjin Huang, Vlado Menkovski, Yulong Pei, Mykola Pechenizkiy. Bridging the Performance Gap between FGSM and PGD Adversarial Training, arXiv:2011.05157\n\n\n\n\n",
            "summary_of_the_review": "Even though the idea is new and interesting, there are flaws in the methodology as well as numerical experiments. I recommend \"reject\". But if the authors reply my concerns successively, I would be willing to increase the score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to add auxiliary networks to an existing classifier to enhance its robustness to adversarial examples. At training time the parameters of the original classifier are frozen, and the parameters of auxiliary networks are updated to reduce the effect of adversarial examples. The authors compare the proposed method with several baselines and demonstrate that the method is able to retain the accuracy of the original classifier on clean data, and enhance the robustness of the original classifier using much less data.",
            "main_review": "Strengths:\n- A new method for enhancing the robustness of an existing classifier. The authors show that the method retains the accuracy of the original classifier and enhances its robustness with less data.\n\nWeaknesses:\n- The evaluation of the proposed method has several serious flaws. The evaluation is based on black-box attacks, while the consensus is that we should always report white-box results. The evaluation is only on CIFAR-10 and a medical image dataset which is rarely used in adversarial ML literature. The authors didn’t compare with established baselines; even standard adversarial training (Madry et al., 2019) was not considered.\n- The authors didn’t explain why they chose to use the focal cross entropy loss to train models.\n\n",
            "summary_of_the_review": "Although the proposed method is new, the evaluation of the method is not very convincing. In particular, the authors focus on black-box test, which cannot verify the true robustness of the method. In addition, the authors didn’t compare with strong baselines. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}