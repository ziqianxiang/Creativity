{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, it proposes an adaptive feature aggregation module based on attention. It applies multi-scale pooling in the attention module to reduce computational costs and proposes two variants (Multi-Scale Self-Attention and Multi-Scale Cross-Attention) for feature aggregation. It boosts Cascade Mask-RCNN by 2.2% for AP_box and 2.7% for AP_mask on the MSCOCO dataset.",
            "main_review": "Pros\n- The proposed feature aggregation approach boosts the baseline model (Cascade Mask-RCNN) with small extra computational cost.\n\nCons\n- The contributions are not significant. It simply applies attention for feature aggregation.\n- It is not clear if the recent proposed other feature aggregation approaches (the ones reviewed in Related Works) also boost baseline approach a lot. In the experimental parts, more comparison experiments with these approaches had better be added.\n- It is not clear why the attention is useful for the feature aggregation. It had better add some visualization of the attention to illustrate its effectiveness.",
            "summary_of_the_review": "The experimental results show the effectiveness of the proposed feature aggregation modules. However, the contributions are not significant. It simply applies attention for feature aggregation.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose a method for feature aggregation on the top-down path of CNNs used for object detection/instance segmentation. The paper argues that current approaches (eg, summation or concatenation of top-down/lateral features) do not handle “capture global context from another feature scale” . The authors propose two slightly different aggregation layers: MSSA and MSCA. They show results on two different methods, Mask RCNN and Cascade Mask RCNN, evaluate in two tasks (object detection and instance segmentation) and in two datasets (COCO and LVIS).",
            "main_review": "### Pros\n+ The paper is easy to follow and the proposed method is well explained.\n+ The proposed approach improves over the baselines: the attention aggregation achieves better performance than the summation aggregation on Cascade Mask RCNN framework.\n\n### Cons\n- My main issue with this paper is the lack of novelty. The main contribution of the paper is to use attention layer (at this point, becoming ubiquitous in visual recognition) to aggregate features on a very specific architecture (FPN in the context of Mask RCNN). It seems almost obvious that such a layer would improve a simple non-parametric aggregation layer like summation.\n- A better baseline for the model (that has not been compared on the manuscript) would be other aggregation layers (eg, based on cnn or fully-connected and with comparable number of learnable weights). Or applying attention layers in other parts of the network that are not the aggregation layer.\n- Why are the improvement in performance on Mask RCNN so much smaller than on Cascade Mask RCNN? It would be nice to have some intuition on why that is the case (ideally with quantitative/qualitative experiments to demonstrate it).\n- Although ablation studies are important to justify important design choices, I feel that the ablation experiments performed in this paper are not very informative (some of the are obvious while others does not provide any insight to the contribution of the paper).",
            "summary_of_the_review": "Based on the notes above, my current rating is \"below acceptance threshold\".",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes two types of attention-based modules, i.e., Multi-Scale Self-Attention (MSSA) and Multi-Scale Cross-Attention (MSCS), which can be used in the feature pyramidal networks (FPN) for feature aggregation. Experiments are performed on MSCOCO and LVIS-1.0 datasets for object detection and instance segmentation.\n",
            "main_review": "Pros:  \n1. The proposed approach is simple but effective. It improves the feature fusion of FPN with simple attention operation.   \n2. In the attention modules, a simple multi-Scale Pooling is used to extract multi-scale feature representation while introducing acceptable computational overhead compared with the single-scale pooling.   \n3. The experiments demonstrate the improvements achieved by the attention modules on two detection benchmarks on top of different baseline detectors, especially on Cascade Mask-RCNN with more than 2% mAP improvements with ResNet50 backbone.   \n\nCons:  \n1. This work can be seen as a straightforward application of the attention module in object detection. The novelty is limited.   \n2. Since the proposed approach increases the computational complexity, it is supposed to compare with other improved FPN methods, e.g., PAFPN [1], rather than the vanilla FPN.  \nBesides, improving the multi-scale feature aggregation in FPN has been introduced in other works. For instance, Deformable DETR [2]'s encoder also aggregates multi-scale features with attention, which can also be an FPN alternative. [3] also improves feature aggregation in FPN with attention, which is not discussed and compared in this paper.   \n3. From the experimental results, MSSA and MSCS are used independently and produce similar results. Therefore, they are two instantiations of the attention module rather than two contributions. From this point, the technical contribution of this of is weakened.  \n4. This paper claims that the proposed module can \"mitigate noisy feature pixels from the shallow layer.\" How is this claim supported?  \n5. Why Multi-Scale Cross-Attention (MSCS) still need to be summed up with the shallow layer feature (Equation 4)? From my view, the attention module already aggregates features from the shallow layer feature.  \nThe explanation \"... the MS-Att operation allows the deep feature Up($P_{i+1}$) to highlight its valuable pixels based on the query of shallow feature\" is confusing to me. What does \"highlight its valuable pixels\" mean? And why?  \n6. Strange speed results in experiments: In Table 1, Cascade Mask-RCNN with ResNet-50 runs at 13.0 FPS with the vanilla \"Sum-up\" FPN. However, in Table 5, it still runs at 13.0 FPS with MSSA (Singel-Scale) and even speeds up to 14.1 with MSSA (Adaptive).   \n\n\nMinor:  \nIn section 3.2: \"The only different regarding\"-> \"The only difference regarding\"  \nIn section 3.3: Duplicated statement: \"Typical transformer module require a fix-length learnable embedding ...\"\n\n\n[1] Path aggregation network for instance segmentation, Liu S, et al, in CVPR 2018.  \n[2] Deformable detr: Deformable transformers for end-to-end object detection, Zhu X, et al, in ICLR 2021.  \n[3] A2-FPN: Attention Aggregation Based Feature Pyramid Network for Instance Segmentation, Hu M, et al, in CVPR 2021. (not referenced)  \n",
            "summary_of_the_review": "In general, the method is effective and can be plug-in with FPN to improve the feature aggregation with attention. However, as pointed in the main review, I would like to see 1. more comparisons to validate the method. 2. more explanation about the MSCS design. 3. explanation of the weird results and unclear claims.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper considers a multi-scale object recognition problem. Existing methods have two issues: 1) the brute-force operations are sub-optimal for feature extraction. 2) the brute-force operations cannot be adaptive to various aggregation situations. To address these issues, the authors have proposed an attention-based feature aggregation method that includes two components, MSSA and MSCA. The MSSA captures the global information and the other utilizes the attention to make interactions between two features. The proposed method with the state-of-the-art object detectors has achieved good improvement in detection performance on COCO and LVIS datasets. ",
            "main_review": "Strengths\n\n++ The paper is well written and easy to follow. The motivation of using attention for aggregating features effectively is convincing to me. \n\n++ The authors demonstrate the generalizability of the proposed method by using various object detectors. In addition, the proposed method has achieved new state-of-the-art performances. \n\nWeaknesses\n\n-- I think that the evaluation results should include small object evaluation results. Since the argument of the paper is to improve multi-scale object recognition, the small object evaluation is necessary. This will show the effectiveness of the proposed method more clearly. \n\n-- The figure is somewhat hard to understand. It would be more great if the authors provide pseudo code or algorithm summary along with the figures. \n\n-- Roughly speaking, the key contribution of the proposed method is to use the self-attention module in OM. I am not sure that the proposed method is sufficiently novel. But as a disclaimer, I am not an expert on this field. So, I will reconsider this based on other reviews.",
            "summary_of_the_review": "Based on the strengths and weaknesses, I lean to borderline/weak accept. My biggest concern is the small-object evaluation I mentioned in the weaknesses section. I will raise my rating happily if I see that the proposed method has good performance on the small-object evaluation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}