{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a new approach for unsupervised representation learning through paired augmentations and mutual information maximization. A lower bound on Shannon mutual information between the random variables corresponding to the representations for two augmented instances is maximized. The surrogate is based on a form of von Neumann information from quantum information theory, along with a lower bound based on an assumption of small Gaussian perturbation between the variables in the representation space. The approach is applied to benchmarked image data sets and compares favorably for small data sets with existing unsupervised representation learning. \n\n ",
            "main_review": "Strengths:\n\nThe idea of using an information theoretic principled approach to unsupervised learning representation with data augmentation makes sense.\n\nThe perturbation analysis approach with von Neumann conditional entropy appears to be novel and provides a somewhat meaningful lower bound.\n\nThe experimental results are promising. The methods try to adopt approaches previously choices of augmentation. \n\n\nWeaknesses:\n\nThere is some issues with clarity and organization.\n\n1) Reading the introduction, at the point on page 2, \"we adopt cosine similarity\" it is not at all clear to me as a reader how a similarity measure would be useful for gauging the conditional entropy.  Furthermore, the phrase \"cosine similarity is equivalent to the loss term\" isn't clear either. These statements need more explanation for a reader who hasn't seen the main results.\n\n2) The paper does not clarify the main restriction assumption to apply von Neumann entropy to sets of vectors. It should be noted on page 2 that Shannon entropy can be directly applied to the eigenvalues since the uncentered second moment for points on the hyper-sphere yields a trace-1 matrix, where the trace is the sum of the eigenvalues.  Also the definition of the entropy alone does not clarify to the reader how the conditional entropy will be calculated even though more relationships between it and cosine similarity are discussed. \n\n3) It is not clear to me that the physical nature of entanglement directly relates to the concept of disentanglement in representation learning. This sounds like a profound discussion but is not elaborated on.  In the beginning of Section 3 the domain of $a$ and $b$ should be clarified as the complex field in general and that the sum of their magnitudes is 1.  Nonetheless, much of the prelude of Section 3 and 3.1 could be better condensed and readers could be referred to Nielsen & Chuang's book for the background that is not essential to the paper. Instead the paragraph could relate the description of states to the feature space of learning representations, especially with the claims of entanglement. \n\n4) The notation in 4.1 is not clear. $\\mathbf{Z}$ is a random vector but then $\\mathbf{z}^i$ is said to be a member of $\\mathbf{Z}$. The definition of the second moment does not appear to be correct in the population case, unless it is a discrete measure of over a countable number of points as indicated by the sum and indexing. Rather it should be clarified that there are a sample of $n$ i.i.d. random variables coming from the discrete distribution described by $\\mathbf{Z}$ now a set, and probability mass function $p(i)$. \n\n5) In section 5.1, a formal statement of the approximation of cosine similarity to inner product under weak Gaussian noise perturbation should be stated.\n\n6) The bound appears to depend condition number of the covariance. If the covariance is not positive definite it appears the bound is non-sensical because the covariance is singular, the Gaussian distribution is degenerate, and the smallest eigenvalue is zero. The method and bound's dependence on the covariance should be clarified.  Starting with the fact that the calculation of von Neumann entropy has to deal with $0\\cdot \\log(0)=0$ when the output of the projection layer is less than the batch size. \n\n7) Theorem 5.1, a single $\\Sigma$ is stated but the piecewise linearity would mean that the covariance depends on $X_2$. There is not a single covariance. That means that $\\lamba_1$ and $\\lambda_d$ depend on the input and it would seem. This seems to loosen the bound and the validated of this proof is questionable. \n\n8) In practice $\\beta$ has to be picked. It doesn't appear the paper uses the theory to choose it.  The supplement states that it  must linearly scale from 1 to 10. Is this across epochs or the hyper-parameter search? How is this range selected? \n\n9) Figure 2 caption is not direct. Is this the same network with different metrics, or a different network for each metric?\n\n10) I find it confusing/misleading that there is stated in the abstract that existing methods have a batch size dependence, but Figure 3 clearly shows a dependence on performance with batch size, albeit at smaller batch sizes than existing. \n\n11) A didn't find a thorough description of how hyper-parameter (batch size, learning rate) were chosen. I assume through validation set but only the selected ones are reported. \n\n",
            "summary_of_the_review": "Update after the revision:\nI thank the authors for the revision and engagement with my concerns as well as the other reviewers. I'd say that most of the revisions are all in the right direction. (The subsection \"Quantum representation\" in the discussion didn't actually offer much insight and seemed out of place at that point of the paper. It seems to me that the von Neumann entropy as an approach to encourage batch normalization to also yield disentangled (uncorrelated) representations would be a more concrete discussion.)\n\nWhile some of my concerns have been addressed, item 4 and 7 above were not fixed. As item 7 was expanded in depth by reviewer heVY (whose more formal analysis was useful for everyone), and the authors concede that the theorem 5.1 is flawed,  I believe the paper is still lacking in aspects needed for acceptance at this time. The experimental results are promising but mixed. It seems that on some data sets it performs marginally better and on others it performs substantially worse than some existing methods. My concern is that there is no analysis (theoretical or experimental) that indicates why this would be the case. The experimental results show a reduction of variance in the updates, but how this translates to practice is unclear. \n\nThus, although the paper sets out a new approach for framing self-supervised loss in terms of quantum (matrix based) information theory, the theory is not rigorously applied. Thus it is unclear how much the perspective actually helps, which decreases the contribution of the current version of the paper. I maintain my score of borderline reject.\n\n===============\nThe paper is promising in both theory and results. However, there are some issues with the main presentation of von Neumann entropy in the finite sample case, as well as some unaddressed implications of singular covariances and other details. Addressing these issues along with some revision and reorganization will improve the clarity and logic of the paper, and I am willing to raise my score. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes an approach for unsupervised representation learning (with a focus on visual problems) that minimizes a loss function that can be regarded as an upper bound of the negative mutual information. \n\nThe loss is constructed by nothing that $I(z_1; z_2) = h(z_1)-h(z_1|z_2)$, by using von Neumann's entropy as lower bound for Shannon entropy $h(z_1)$, and by noting that the conditional entropy term can be upper-bounded as $h(z_1|z_2) \\leq \\alpha - \\beta E[\\cos(z_1, z_2)]$ for some $\\alpha$ and $\\beta$.",
            "main_review": "**Strength:** Empirical results seem promising.\n\n**Weaknesses:** \n\n**I) The Main Claim is Off:** \nThe paper claims to propose \"a principled mutual information estimator that is not contrastive and utilizes a difference of entropies formulation\". However, as a mutual information estimator, the approach proposed herein has too many shortcomings:\n\n- The estimator is *not consistent* and is *asymptotically biased* (i.e. we can't get close enough to the true mutual information, irrespective of the amount of data available).\n\n- The estimator only applies to vectors of the same size (otherwise we wouldn't be able to calculate the cosine similarity). However, many usecases require estimating the mutual information between vectors of different sizes (e.g. [2]).\n\n- The proposed algorithm has many flaws (see below).\n\n**II) Technical Issues:** \n- **$\\beta$ should not be a hyper-parameter (if the authors want the loss function used to estimate the negative mutual information)**. Theorem 5.1. provides that for two random variables $z_1=f(x_1)$ and $z_2=f(x_2)$ ***there exist*** $\\alpha$ and $\\beta$ for which $h(z_1|z_2) \\leq \\alpha - \\beta E[\\cos(z_1, z_2)]$. To obtain a lower-bound on the mutual information, $\\alpha - \\beta E[\\cos(z_1, z_2)]$ *does* need to be an upper-bound of $h(z_1|z_2)$. Not only can the authors not choose any $\\beta$, but the right $\\beta$ actually depends on $f$ (i.e. technically speaking, it is a function of parameters of the encoding and projection networks).\n\n-  ****$\\alpha$** has been unduly omitted from Algorithm 1**. On the face of it, it does not seem as though $\\alpha$ depends on any parameter, and as such one would think it can be conveniently omitted from the loss function. However, the values of $\\alpha$ and $\\beta$ as per Theorem 5.1. *do* depend on $f$ (i.e. on the parameters of the encoding and projection networks). \n\nIf the authors want the loss function to be an upper bound of the negative mutual information, both $\\alpha$ and $\\beta$ should be included in the loss function, and their dependencies on parameters of $f$ should be explicitly taken into account in gradient calculation.\n\n\n**III) Incomplete Litterature Review:**\n\n[1] Showed that most of the fundamental limitations described by [3], in particular the $O(\\log n)$ upper bound and the high estimation variance, can be alleviated when working in the copula-uniform dual space (i.e. the image of the input space by the probability integral transform of each coordinate). Additionally, [2] (Appendix A) proved that, when applied in the copula-uniform dual space, the DV characterization of the KL divergence can be viewed as a mini-max copula entropy problem, and the resulting estimator does not suffer from the formal limitations in [3].\n\n\n[1] Samo, Y.L.K., 2021, March. Inductive Mutual Information Estimation: A Convex Maximum-Entropy Copula Approach. In International Conference on Artificial Intelligence and Statistics (pp. 2242-2250). PMLR.\n\n[2] Samo, Y.L.K., 2021. LeanML: A Design Pattern To Slash Avoidable Wastes in Machine Learning Projects. arXiv preprint arXiv:2107.08066.\n\n[3] McAllester, D. and Stratos, K., 2020, June. Formal limitations on the measurement of mutual information. In International Conference on Artificial Intelligence and Statistics (pp. 875-884). PMLR.\n",
            "summary_of_the_review": "The approach proposed herein can hardly be regarded as a mutual information estimator. \n\nDespite the interesting link to quantum information theory, and the effort to relate the loss function used in Algorithm 1 to an upper bound of the negative mutual information obtained as a difference of entropies, at the end of the day, the authors simply use as similarity metrics the function $\\mathcal{S}(z_1, z_2) := S(z_1)+\\beta E[\\cos(z_1, z_2)]$, not the mutual information $I(z_1; z_2)$.\n\nIt is not surprising that $-\\mathcal{S}$ works well in practice as a loss function to find good feature representations:\n\n- The von Neumann entropy term $S(z_1)$ effectively mitigates redundant features and high variance features. Indeed, it promotes uniformity among eigenvalues of $\\tilde{\\rho}$. Noting that $\\tilde{\\rho}$ can be viewed as a sample autocovariance matrix of $z_1$, $S(z_1)$ implicitly pushes the eigenbasis of $\\tilde{\\rho}$ towards the canonical basis of $\\mathbb{R}^d$, and in so doing it also pushes features to be decorrelated and have the same variance $1/d$ (given that $\\tilde{\\rho}$ has trace $1$).\n\n- The cosine similarity term $E[\\cos(z_1, z_2)]$ on the other hand ensures that the learned representation is only mildly affected by small input perturbations.\n\nI would have had a more favorable opinion of the paper, had the authors not claimed to introduce a new mutual information estimator, and had they focused instead on unsupervised representation learning as an application.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a method for unsupervised representation learning based on mutual information estimation that does not involve a contrastive loss and therefore does not need a large batch size to be trained. This is achieved by estimation of the mutual information with the difference of von Neumann entropies as a bound for the Shannon entropies.",
            "main_review": "Strong points:\n1. The principled approach for representation learning based on the mutual information estimation\n2. What seems to be a novel idea to use the von Neumann entropy for mutual information estimation rather than the Shannon entropy\n3. The novel approach for the mutual information estimation avoids using the contrastive loss and therefore does not have to have a large batch size - a huge limitation of the methods based on the contrastive loss\n4. State-of-the-art performance demonstrated on image datasets\n\nWeak points:\n1. Loose mathematical derivation (see details below)\n2. (minor due to understandable space constraints) Not very clear background information on the quantum theory\n3. Missing details on implementation required for reproducibility\n\nIn the current version of the paper I would vote for rejection, but I believe it can possibly be changed during the discussion period to be ready for publication. The main drawback that I see in this submission is that it is claimed to be based on maths but in fact the maths is quite loose (to ignore admitted by the authors limitation of the specific case for weak Guassian noise augmentation). For example, Theorem 4.1 states that S(\\rho) \\approx S(\\tilde(\\rho)), but how close this approximation is? Or, in Lemma 4.2 the inequality holds with the approximations mentioned above it but it is never specified in the lemma itself, and again it is not specified how close these approximations are. \nTherefore, I see two ways how this submission can be amended to be publishable. Either the maths should be made more precise in the paper, or it should loosen the claims that everything is based on solid maths. If either is done during the discussion period, I am willing to increase my score. That is to say that I believe the paper contains an interesting idea and even without the strong reliance on the maths it has enough material to be published. It can be reformulated as a first step inspired by the principled approach (or if possible to make the maths precise and make the paper even stronger). \n\nSpecific comments/suggestions (not too important for the assessment unless supporting the statements above, but rather suggestions to further improve the paper):\n1. Notations H(Z) and S(Z) are redundant in abstract and only overwhelms it.\n2. Introduction, the first paragraph. “the need for negative samples” - this need has not been discussed yet (apart from the abstract) and therefore looks like coming out of nowhere\n3. All variables should be introduced\n4. Introduction. “our derivation is based on the inequality” - unclear and weird wording\n5. Introduction. “and no other trick is used” - maybe too informal\n6. All acronyms should be introduced, for example, MI\n7. Section 2, Non-contrastive methods. A little bit more context would be appreciated, e.g. for “representation collapse”, “cluster”, “stop-gradient”\n8. Definition of the superposition state in Section 3. Are there any conditions on a and b?\n9. Section 3 in general may be too brief. I appreciate it is difficult to compress the whole theory in a small section of the paper, but, for example, it was not clear for me from the initial reading that the density operator is a matrix apparently.\n10. Proof of Theorem 5.1. “For x_2 ‘in X_2… ” - unclear where this statement comes from\n11. Algorithm 1. f and g are not introduced\n12. ImageNet-10 and ImageNet-100 - what are the classes that were randomly subsampled?\n13. Table 1 font is too small\n\n\n\nMinor:\n1. Abstract. “It is also commonly known as A negative sampling size problem”\n2. Introduction, third paragraph. “frequently based on A non-contrastive loss”\n3. Section 3. “Note that THE hypersphere constraint…”\n4. Section 6. “Considering that we did NOT try”\n",
            "summary_of_the_review": "*Update after rebuttal* I have read the authors’ responses, other reviews and updated submission. I would like to thank the authors for their substantial revision and responses. Although the authors did address my point of adjusting wording in order not claim strong reliance on maths, the loose maths is still there. Moreover, other reviews raised valid concerns. In particular, the authors agreed that theorem 5.1 does not stand. Since no revisions are allowed anymore, I am leaving my score unchanged as I cannot see those changes the authors suggested to mitigate incorrectness of theorem 5.1. \nI would like to encourage the authors address all the comments and resubmit to another venue as I liked the overall idea of the paper.\n\n===============\n\n\nTo summarise the above, I liked the idea of the paper. I am not too much familiar with the related literature, but the idea appears to be novel and intriguing. It is always interesting to see how ideas are brought from different fields. And if in fact considering the different entropy other than Shannon can help with the big batch size problem for representation learning based on mutual information estimation, it would be a very valuable and important contribution to the community. However, in the current version of the paper, it is overselling the fact that it is based on principled maths when in fact the maths is not very precise. The content of the paper is interesting enough to be published without precise maths, but I believe the paper should be revised to reflect this. Therefore, I am willing to increase the score in two cases: a) the maths is made more precise, or b) the wording of the paper is adjusted to not claim the strong reliance on the maths.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a new loss for an already exiting framework of self-supervised learning. A classical formulation of a self supervised learning is used here and illustrated in Figure 1. Two views of the same image are created and propagated through a feature network and MLP head and the corresponding features are further normalized. The proposed solution suggests to maximize the mutual information of the features that correspond to the two views of an image. The paper derives this mutual information using Neuman Entropy (which estimation is claimed to be more stable. compared to Shannon entropy, when small batch size is used). The estimation of the Neuman Entropy suggested by the authors is summarized in the PyTorch pseudo code in Algorithm 1. Throughout the paper the authors emphasize that the derived approximations for entropy and conditional entropy (sections 5.1 and 5.2) only hold for a weak Gaussian noise augmentation.\nThe authors illustrate that the method can be effectively used even with relatively small batch sizes (Figure 3 )\nThe authors illustrate the effectiveness of their method comparing it to other state of the art self-supervised  approaches in Tables 1 and 2 and achieve SOTA performance in transfer learning task for COCO.\n",
            "main_review": "Strengths of the paper:\n1. It is always interesting to see how fundamental theories (e.g. quantum mechanics in this case) can be useful in relatively experimental field, such as deep learning for Computer Vision. The authors provide such an opportunity, outlining basics of quantum mechanics to explain the suggested estimation of the mutual entropy using Neuman  Entropy. In overall, it was interesting to learn how familiar cosine similarity can be interpreted as an approximation of a conditional entropy.   \n2. Beyond extensive theoretical derivations the authors provide basic experiments, which illustrate effectiveness of trained features on ImageNet classification task and transfer learning for Object Detection on COCO and PascalVOC benchmarks. The suggested method even achieves SOTA for Coco detection benchmark.\n\nWeakness:\n1. As much as it was interesting to refresh the memory of the basics of quantum mechanics (provided in Section 3), this review does not contribute to the paper in overall. The explanation about <braket> does not really add intuition or understanding of the further proofs. The only mention of  \"braket\" notation in the definition related to the final estimations appears in Definition 3.1 of the density operator. Instead of the elaborated presentation of the quantum mechanics basics, I would suggest the authors to provide more intuition (and maybe theory) on why their approximation based on Neuman Entropy allows to use smaller batch size compared to Shannon entropy. Another option would be to include the training details in the main text (instead of supplementary) \n2. Figure 2 is not clear to me, specifically, what is presented in the y axis - is it as stated - mutual information in (a) and entropy in (b) and (c) -  why would we like to compare mutual information with entropy? Also it is not clear if in this experiment the authors base their implementation of Shannon entropy on anything previously reported in the literature with respect to self-supervised learning. Previously, they say that typically contrastive learning is based on Shannon entropy, but then it is not clear why it fails in the experiment presented in Figure 2 (Variance analysis), while being successful as part of Sim-SIAM (Table 2 (b))\n3. It is concerning that different data augmentation policy is implied with the proposed approach, compared to Sim-SIAM (Table 5) -- it might happen that the observed difference in the performance actually comes from this different policy and not from the loss. It could have been helpful to see performance of the suggested method with the exact same augmentation policy.\n4. It would be helpful to have some discussion on difference in performance between Pascal VOC and COCO. Why are the learned representations more effective for COCO (at least with respect to other methods)? \n5. Finally the suggested loss is not symmetric with respect to the two views and this is very counter-intuitive. Why is not the loss calculated twice permuting the two views and summed? In other words, I would expect the following formulation for the loss in Algorithm1 :  \nLoss = (eig val(U1)*log(eig val(U1))).sum()  + (eig val(U2)*log(eig val(U2))).sum() - 2* beta * cosine_similairty (U1, U2) .mean()\n ",
            "summary_of_the_review": "I tend to weakly accept this paper, as it contributes a new loss that has an interesting theoretical background. The experiments are not strikingly impressive however: Sim-SIAM tends to perform equally well for batch size -128 (Table 2 b) and it is not clear why the learned features are effective for COCO but not for Pascal VOC (Table 3). Also the assumption of weak Gaussian noise augmentation is somewhat limiting the validity of the entire theoretical derivation, given that it was shown that diverse augmentation is a key for self-supervised learning success (and also given that the actual experiments are performed with other types of augmentations - Tables 4 & 5)",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}