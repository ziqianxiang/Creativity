{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposed a no-box attack method, with mixed reviews. While I appreciate the authors efforts which attempted to address the questions raised by the reviewers, they were not sufficient to remove the concerns. Based on  the reviewers comments and author responses, I won't be able to recommend the paper to be accepted in its current form."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Following the scenario of No-box Attacks, they propose a simple but efficient method, that crafts adversarial perturbations by three types of handmade images, i.e., concentric circles, concentric squares, and concentric rhombus, and add them to the high-frequency component of benign samples. Experimental results show that their handmade perturbations significantly outperform previous No-box attack methods, even competitively to some black-box methods.\n\n",
            "main_review": "The authors separate a benign image into two components: LFC(low-frequency component) and HFC(high-frequency component). They demonstrate that the high-frequency component is an essential part of the DNN model's performance. So they add some handmade images in the high-frequency part of benign images. The method seems novel, and the paper is well written and easy to follow. I think it is an interesting paper.\nHere I have some concerns:\n1. It is a little weak about the discussion of why the circle always performs better. Do the circle/square/rhombus perturbations always lead the victim models to predict certain classes?\n2. Can we optimize the perturbation to find an optimal proto-pattern?",
            "summary_of_the_review": "I think the contribution about HPC is not novel, since some works have pointed it out. The contributions of this paper are only marginally significant or novel.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The proposed approach involves creating adversarial examples in a training-free manner by manipulating the frequency components of the image. High -frequency information is used from simple geometric patterns and combined with low-frequency component from the input to create a hybrid image. The resultant image is shown to fool classifiers under multiple settings which presents an easy way to construct adversarial examples.",
            "main_review": "### **Strengths**:\n\n— The creation of adversarial example is simple and well analyzed showing improved performances compared to previous approaches across multiple settings. The construction of an adversarial example in a training-free manner without using gradients is an interesting phenomenon and can be used to improve robustness of Neural networks.\n\n— The paper is well written and easy to follow. \n\n### **Concerns**:\n\n— Looking at the qualitative examples in Figure 13, the resultant adversarial example consists of artifacts based on the type of pattern used. This can make it easier to identify based on manual visual inspection. More qualitative examples of the proposed approach compared with the examples from Li et al. , can improve be helpful.\n\n— The quantitative experiments are mainly presented for ImageNet dataset. It would be interesting to see the effectiveness of the proposed approach for other datasets such as Places365 or even other vision tasks. This would also provide an opportunity to discuss choosing different hyperparameters associated with the approach for custom datasets. \n\n— Attacking a real world recognition system as done in Li et al., would show the practicality of the algorithm.\n\n— In Table 3, the proposed attack is compared against gradient based attacks on different adversarially trained models. One question that arises is that why the proposed approach is better than even gradient based attacks for the defense models. Specifically, ResNet152_D which involves denoising operation specifically designed to suppress the effect of high frequency components. So it would be expected that the proposed approach that relies on the effect of HFC would not be better than gradient based attacks. More analysis or insight into this experiment would be useful. The authors are also encouraged to discuss the work by Yin et al. (arXiv:1906.08988) which is also along similar lines. \n\n— Metzen et al. ( arXiv:1702.04267 ) showed that auxiliary CNNs can be used to detect adversarial perturbations. It was shown later that gradient based adversarial examples can be constructed to fool both the classifier and adversarial detector (arXiv:1705.07263 ). However, since the proposed approach does not rely on substitute models or gradient based information, it would be interesting to see if such an auxiliary classifier can be an effective defense for the proposed approach which relies on  feature statistics from HFC.\n\n### **Minor comments**:\n\n— Figure 1 suggests that High-frequency component has more confidence than the original image itself for all data. Although the authors mention in the text that this is not the case for all input images, Figure 1 can mislead readers into thinking this is true for all examples. The authors are suggested to either include the text in the figure caption or alter the position of the image.\n\n— The related work section discusses some important works in the adversarial attack literature. Authors are encouraged to also discuss works related to adversarial patches which have been used to fool multiple vision tasks and how the proposed work (which does not rely on learning the perturbation ) is different from such works. Since both approaches rely on using high frequency information from the perturbation, it would improve understanding among readers.",
            "summary_of_the_review": "The proposed work is an interesting approach towards constructing adversarial examples in a training-free manner. Such attacks can provide more insight into understanding the architectural aspects of CNNs that make models vulnerable and hence can be useful to the community. I believe that evaluations against real world recognition systems and other defense algorithms can improve impact of the work to the community. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors have proposed an attack based on the modification of high-frequency components of an image through different types of masks such as circular, square, and rhombus. The experiments are performed using multiple networks on a subset of the ImageNet database. ",
            "main_review": "It is well known that the high-frequency components of an image are critical for both classification and robustness. Therefore, the authors need to revisit the text accordingly regarding the claim and highlight the text both in the abstract and the paper and refer to the following but not limited to the following papers. \n\n[1] High-Frequency Component Helps Explain the Generalization of Convolutional Neural Networks, Haohan Wang, Xindi Wu, Zeyi Huang, Eric P. Xing, CVPR2020\n[2] D. Anshumaan, A. Agarwal, M. Vatsa, and R. Singh,  WaveTransform: Crafting Adversarial Examples via Input Decomposition,  ECCV Workshop on Adversarial Robustness in the Real World, 2020, pp. 152-168,\n[3] Steganographic universal adversarial perturbations, PRL 2020\n\nThe comparison shown in the paper is extremely weak and does not use the existing works that utilize the same concept [2-6].\n\n[4] Guo, C., Frank, J.S. and Weinberger, K.Q., 2018. Low-frequency adversarial perturbation. arXiv preprint arXiv:1809.08758. \n[5] Sharma, Y., Ding, G.W. and Brubaker, M., 2019. On the effectiveness of low-frequency perturbations. arXiv preprint arXiv:1903.00073.\n[6] Hashemi et al., Transferable Universal Adversarial Perturbations Using Generative Models, 2020\n\nThe technical novelty of the paper is not clear as the authors use the concept of Hybrid image which is well known.\n\nThe attack is highly dependent on the proto-patterns and why only the circle pattern is more effective is not well studied. \n\nThe attack norms is also a concern, the authors have used slightly higher norm. The ablation study corresponding to different epsilon values such as 2, 4, 8, 10 needs to be added and a comparison with existing algorithms on the same norm strength can be added.\n\nAre the models trained using the data augmentation which usually performs corruption-based augmentation as well? Are those models are also vulnerable? What about ensemble-based models?\n\nAnother limitation of the attack is the dependency on the tile size. The paper lacks several such analyses to explain why this dependency exists and why hyperparameter is more successful than others.\n\nPlease add the comparison with the following transferable perturbations and defense as well.\n\nLearning Transferable Adversarial Examples via Ghost Networks, AAAI 2020.\nAdversarial Robustness Across Representation Spaces. CVPR\n\nWhat is the computation cost of PI-MI-DI2-FGSM?\n\n\n",
            "summary_of_the_review": "The technical contribution of the papers needs to be highlighted better, the comparison needs to be updated, and the analysis concerning several parameters used in the attack needs significant attention.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors propose a new method for generating adversarial images for image classifier. This is a No-Box attack named Hybrid Image Transformation (HIT) or hit attack which is both model free and data-free (no training required). In the experiment section the authors show the efficacy of the proposed attack on the ImageNet dataset for different DNNs.",
            "main_review": "The authors start the discussion by revisiting the properties of both white-box and black-box adversarial image classification attack. They highlight some important characteristics of such attacks and argue why most proposed attacks may not be useful in practice. Based on this discussion they argue in favor of previously proposed No-Box attack. A No-Box attack must be both model-free and data-free so that it is not dependent on the training process of one particular Neural Network. \n\nThe authors then propose an attack named HIT which is training free and uses the LFC and HFC components of an image to craft the adversarial image. As can be inferred from the previous statement, the main contributors of this attack are the LFC and HFC components of an image. To better understand the role of these components, the authors first split the raw images into these two pieces using Gaussian low-pass filter. This analysis shows that the DNNs rely on HFC components to classify images where as the LFC parts are more visible to humans. This finding helped the authors come to the conjectures that effective adversarial HFCs must be regional homegeneous, repeating and dense. They then craft the HIT attacks by adding such adversarial HFCs to the LFC of an image and then clipping the image to fall in an predefined inf-norm ball.\n\nThe authors then perform experiments on the ImageNet dataset with different DNNs. Their attack outperforms the previous state-of-the-art     by a large margin. Furthermore, their attack performs well when compared to Black-Box attacks as well. ",
            "summary_of_the_review": "I like this paper overall. It is easy to read and follow the argument. Furthermore, it sheds new light on the classification logic of DNNs with respect to adversarial examples. The attack methods they is also very strong and widely applicable as shown by the experimental results. \n\nQuestions for Authors\n1. Why is epsilon set to 16? Usually this is set set to 8 to the best of my knowledge.\n2. I have read the authors discussion regarding lambda and why a value of 1.0 is chosen. I would like to know if this lambda depends on some property of the dataset i.e., would an adversary need to tune lambda for different datasets or not?",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}