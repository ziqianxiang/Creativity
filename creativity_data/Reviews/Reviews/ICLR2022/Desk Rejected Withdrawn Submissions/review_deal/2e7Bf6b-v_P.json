{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper solves the problem of black-box optimization over a hybrid search space, consisting of a high-dimensional continuous space and a complex combinatorial space. In this paper the authors propose ES-ENAS, a simple joint optimization strategy by combining evolutionary strategies and combinatorial optimization techniques in a highly scalable and intuitive way. It is inspired by the one-shot or supernet paradigm introduced in efficient neural architecture search (ENAS). Finally, the authors empirically demonstrate that their approach by optimizing BBOB functions over hybrid spaces and combinatorial neural network architectures via edge pruning, and quantization on popular reinforcement learning benchmarks. ",
            "main_review": "# Reasons to Accept\n\n+ It solves an interesting topic.\n\n# Reasons to Reject\n\n- It is difficult to understand. This paper needs to be revised.\n- Title makes readers confused. I thought it is a paper to solve black-box optimization. However, it mainly focuses on a NAS problem.\n- The contributions of this paper are unclear. It does not degrade the contributions, but it is a simple modification of evolutionary strategies.\n- The experiments are confusing. In Figure 2, even if it is the same as vanilla evolutionary strategy where a variable for $d_{\\textrm{cat}}$ exists, why do you include the results? I do not think the vanilla evolutionary is not your contribution. Moreover, does Figure 4 show the benefits of the proposed method? It seems worse than a policy gradient algorithm. Figure 5 is also similar.\n\n# Questions to Authors\n\nPlease see Reasons to Accept and Reasons to reject.",
            "summary_of_the_review": "This paper solves an interesting problem, but it is generally difficult to understand. In addition, the contributions of this work are not clear and the demonstrations are also unclear. Thus, I would like to recommend rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed a general-purpose solver framework for black-box optimization on the continuous-categorical hybrid search space. The proposed framework, ES-ENAS, simultaneously optimizes the continuous variable (called the weights) and the continuous parameter of the probability distribution on the categorical variable (called the controller). The overall idea is the combination of the evolution strategy (ES) and the efficient neural architecture search (ENAS). Multiple solution vectors consisting of the continuous and categorical variables are sampled from the product measure of the normal distribution and the controller distribution. They are evaluated on the objective function. Then, the mean (theta) of the normal distribution is updated by estimating the descent direction of the objective by using the sampled solutions. The parameter (phi) of the controller is updated by, for example, applying the policy gradient approach, but it is left as the user's choice. Some instantiations of the proposed framework with different search components are tested on two sets of problems: synthetic tasks based on the well-known black-box optimization benchmarking (BBOB) testbed, and reinforcement learning tasks on Mujoco environments. ",
            "main_review": "Strength:\n1. simple and flexible algorithmic framework allowing the users to employ a domain-specific search component\n2. extensive evaluation of the performance of instances of the proposed algorithmic framework on Mujoco environments\n\nWeakness\n1. The proposed approach is derived rather straight-forward from the recent NAS approaches. Just plugging the product measure of the normal distribution and the categorical distribution as the controller distribution into ENAS framework leads to the similar approach. Indeed, such an approach has been taken in an existing NAS approach (1), where the authors employ the product of the (discretized) normal distribution and the categorical distribution to optimize some ordinal architecture parameters such as the number of kernels or the kernel size and some other categorical architecture parameters. There is also another related framework, information-geometric optimization (2), where one can do basically the same thing as above. Moreover, the variants of ES are known to be derived from information-geometric optimization. Therefore, the novelty and the usefulness of the proposed framework over these baselines are questionable.\n(1) Akimoto et al, Adaptive Stochastic Natural Gradient Method for One-Shot Neural Architecture Search, ICML 2019. \n(2) Ollivier et al, Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles, JMLR 2017.\n\n2. It is not clear how the synthetic tasks used in Section 3.1 reflect the difficulties appearing in real applications: (a) dependency of the optimal continuous variable on the categorical variable, and (b) independence of the objective function value on a subset of the continuous variables under some values of the categorical variable. In the synthetic tasks used in Section 3.1, all the continuous variables affects the objective function values. Their optimal value may depend on the choice of the categorical variable, but it is not discussed in the main text and in the appendix. Therefore, it is unclear how the results in Section 3.1 represent the difficulties in optimization on the hybrid search space. \n\n3. In Section 3.1, the performance is evaluated the normalized log optimality gap, averaged over 3 runs. No information of the deviations are given. The performance metric used in this paper is not the standard in BBOB platform. Since the average of the gap itself over different problems, even if its value scale is normalized, is affected by the greatest value. Therefore, it is rather skeptical if Figure 2 really shows the goodness of the proposed approach. What if the ECDF graph is plotted as is used in the BBOB platform?\n",
            "summary_of_the_review": "The results on Mujoco are promising. However,  because of weakness (1) above, the novelty and the usefulness over the existing approach is questionable. Weakness (2) and (3) also indicate that the evaluation of the proposed framework is questionable.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents a new search method that uses the main points of Evolutionary Strategies (ES) and ENAS. The propose algorithm is claimed to be theoretically sound and empirically superior. The experiment results show it is effective in solving classic black-box optimization problems and also in finding better neural network architectures. ",
            "main_review": "The paper tries to tackle a difficult type of optimization problems that include both continuous and binary decision variables. The scale of the optimization problem can be very large depends on the application areas. It is both a challenging and interesting research area. \n\nThe ES-ENAS method proposed by the paper is new and combines two strategies that have been shown to be useful in dealing with real-world problems including traditional black-box optkmizaiotn, hyperparameter optimization, and neural network architecture, etc. ES-ENAS is relatively a simple algorithm and easy to understand.  \n\nThe paper presents some theoretic explanaitons and quite extensive experimental results. \n\nI believe the paper has a few weakness:\n1) The proposed algorithm introduces a few extra hyperparameters including n, alpha, step size, etc, as shown in Algorithm 1. Need to provide better instruction on how to set them properly.\n2) In Algorithm 1, beside randomly selecting g (gradients), any other ways to choose them. Randomly selecting is easy, but is it effective? Are there better ways? When theta are weights of neural network, I'm not sure if it works well. \n3) I found the figures overall are not easy to understand. For instance, in Figure 2, not all color lines are in all the plots, sometime black line is used. In Figure 7, what is the orange line? Also, some figures heavily depend on color schemes and it is difficult for color blind people to understand them. I'll suggest to redesign the figures so that they are easy to understand and demonstrates the superiority of ES-ENAS clearly. \n",
            "summary_of_the_review": "The proposed method has some new ingredients, but not significant novel. The overall writing of the paper is not clear enough, especially the pictures. I'm not sure if the method will work for large scale problems given the simple random scheme and extra hyperparaemeters. The experiment results show ES-ENAS is effective, however, little is said about why it is. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}