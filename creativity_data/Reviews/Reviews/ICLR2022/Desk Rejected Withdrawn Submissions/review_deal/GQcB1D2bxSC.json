{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper considers a theoretical analysis of the value factorization problem in multi-agent reinforcement learning. To achieve this, they introduce a number of concepts: convex markov games, approximate core, coalition marginal contribution. They show that generalized shapley value lies in the approximate core of the convex markov game. They then prove rationality (among other properties) of generalized shapley value. Based on this, they derive shapley Q-learning which is a generalization of the bellman operation. They evaluate this algorithm on two datasets: Predator-Prey and multi-agent starcraft",
            "main_review": "Strengths of this paper\n\n+ Introduces a number of novel concepts and definition to theoretically explain the value factorization.\n+ Based on theory, they also derive a new operator and algorithm which empirically performs well on challenging datasets\n+ Comparison against pretty strong baselines: unlike many works in RL, they have done an pretty thorough job. \n\n\nWeakness of this paper\n\n- First, I found this paper a bit hard to follow. Overall, it took me quite a bit of time to understand how the various concepts introduced are related. I propose the authors significantly rewrite the paper explaining how the various concepts relate. For instance, the core concept here is generalized shapley value having rationality and fairness properties which leads to the shapley Q-learning algorithm. The notion of convex markov games and core etc are just a means to an end. Yet the first technical parts directly talk about these notions which seems like a distraction.\n- Second, looking at the experimental results, most of the CIs overlap. So in that sense, what is the main message of shapley Q-learning algorithm? Its not clear that it is strictly a better algorithm. Is the main takeaway that it can do as well as these other algorithms and also has strong theory for analyzing value decomposition? This relates to the first point above, the paper does not do a good job of connecting the various parts and giving a crisp takeaway.",
            "summary_of_the_review": "As above, most of my comments are around the presentation. This is related since I did not get a complete understanding of the main contributions of this papers; most importantly their significance. Addressing some of my suggestions above will definitely make this paper readable.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the theoretical framework for value factorisation in multi-agent RL, with interpretability, under the framework of Markov convex games. ",
            "main_review": "The paper is generally well-written, including some interesting ideas. However, the current form seems still need some modification before it is ready to be accepted. Moreover, I am a bit concerned about the generality of the results (due to the assumption on MCG) on general MARL, and the disconnection between the theory and the experiments. The theoretical results seem interesting, but not strong enough (Especially given the existing results in Wang et al., 2020c). I would then view the contribution of the paper mainly on the empirical side. I will leave it to other reviewers and the AC who are more qualified than me to evaluate the empirical significance of the work. Please see my other comments as follows:\n\n1. Some acronyms, e.g., MCG and SHAQ, has been defined multiple times. Please check all.\n2. What is the implication of Eq. (2), i.e., the definition of the MCG? It would be better if the authors could provide more justifications for the generality and usefulness of the model, e.g., by examples.\n3. Following 2, how the techniques in the current paper might be extended to the Markov games without the convexity structure?\n4. When citing equations, there should be parentheses. \n5. Some notations may need more explanation: what is $R$ in Eq. (7)? What does the $\\phi$ in the subscript of the definition in Eq. (5) and (6) mean? What is the definition of $w_i$ in Eq. (7), for those a that is not argmax?\n6. Why after Eq. (7), $Q_i^{\\phi*}$ can be written as linear function of $w_i$?\n7. Related to 5, how should one set $w_i$, and how restricted the condition in Theorem 2 is? \n8. For the simulations, do we know if the environments are MCGs? In other words, do they satisfy the convexity condition in the definition? If not, it might not be fair to say that the experiments \"verifies the theoretical results \".",
            "summary_of_the_review": "In general, the paper contains some interesting ideas and is in a good shape. However, it still has room for improvement before it can be accepted.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "If I understand correctly, this paper uses Shapley value as weights to improve learning in multi-agent reinforcement learning (MARL) problems.  The technique is demonstrated in two [symmetric?] environments - a predator-prey problem, and a Starcraft problem - achieving good performance in both.\n\nI found the exposition hindered my understanding of the paper, making it difficult both to understand the paper's contributions, and their correctness.",
            "main_review": "**Strengths**\n\nClearly, training RL agents is an important problem - and has served as the foundation of some significant developments in ML (q.v. DeepMind's work).  Further, the manuscript seems to validate its approach by showing good performance in two environments.\n\nFinally, the authors seem to have a strong grasp of the literature.\n\n**Weaknesses**\n\nBelow, I score the manuscript as a \"3 - reject\", and my confidence as a \"2\".  A large part of my lack of confidence owes to the paper's exposition, which I found poor - making it hard for me to understand what the paper is doing.  If I had difficulty with it, as someone with knowledge of both RL and game theory, I am confident that other readers will as well.  I think that it would be difficult to conduct and review a thorough rewrite of the paper during the review period.\n\nI would therefore suggest:\n1. rewrite for English quality.  At present, a lot of the English is confusing or imprecise.\n1. do less, better.  A whole jumble of techniques are deployed, often without being properly introduced: e.g. p.1 talks about cooperative, non-cooperative and coalitional game theory, without explaining the roles that each of these play in the paper - or seeming to recognize that 'cooperative' and 'coalitional' are synonyms when describing classes of games.  This continues.  For example:\n    1. rather than introducing the necessary game theory at the start of Section 2, we launch straight into Markov convex games - without knowing what makes them Markovian or convex.\n    1. Q-values are \"defined\" by presenting ranges of possible values, rather than actual definitions\n    1. a \"condition for MCG\" is presented, without us being told what sort of condition it is: necessary or sufficient, and to what end (e.g. do equations of this form characterize MCGs? is this some sort of supermodularity condition?)\n    1. the \"value assignment\", $\\\\bf{x}$, appears without explanation\n    1. the core is defined verbally\n    1. \"rationality\" is used without appearing to have any precise definition\n    1. I would probably avoid the term \"generalized Shapley value\", which is already used - in a different sense - by Marichal and co-authors.\n    1. what does $\\mathcal{C}_i \\cup \\\\{ i \\\\}$ mean?  Relatedly, what are \"independent\" policies?\n    1. the $\\sim$ in $C_i \\sim p(C_i | N \\backslash \\\\{i\\\\})$ suggests a distribution, which does not make sense in the context given.  What is this?\n    1. I think that \"almost no agents\" (p.4) actually means \"no agent has a large incentive\"\n    1. things like $\\\\bf{w}$ (presumably weights?) should be properly explained, and motivated\n    1. any claim of \"apparent\" or \"obviously\" should be reflected upon: is it?\n    1. what is the \"history of each agent\" in a Markovian game?\n    1. statements like \"a solution in the core of MCG with the grand coalition\" are a bit confusing: there is a MCG; a certain value will arise from the grand coalition; I'm not sure what the phrase as a whole means.\n    1. etc.\n\nIn conclusion, proper intuitions need to be supplied throughout: what are you doing, and why?  Why do the results derived make sense?",
            "summary_of_the_review": "I believe that the authors likely know what they are doing, but have not yet communicated it clearly enough for a reader to easily understand what they are doing.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes to generalize Shapley value in coalitional game theory to Markov convex game (MCG). It formally proves that the generalized Shapley value lies in the $\\epsilon^*$-core of MCG with grand coalition, so it can be used as a value factorization method for  global reward game (each agent can maximize its own generalized Shapley value in order to maximize the global value). The paper also proposes Shapley-Q operator and Shapley Q-learning (SHAQ). Experimental results on predator-prey and SMAC benchmark show that SHAQ can achieve good performance overall, while achieving better interpretability than baseline methods. ",
            "main_review": "- Strengths:\n\t- The proposed generalized Shapley value has some quite nice theoretical properties. The proposed Shapley-Q operator also seems solid. \n\t- Using Shapley value as a value factorization method with better interpretability is very interesting (but I'm not very sure how novel this idea is given existing work [1]).\n\n- Weaknesses:\n\t- The motivation for proposing generalized Shapley value is not very clear to me. The authors argue that the main motivation for this is although [1] \"gave an analytic form of Shapley value ... the effect of policy is neglected so that it is not promising to be within the core (or ε-core).\" I don't quite understand this motivation. \n\t- I'm not sure how significant the theoretical contributions are compared to the work in [1], especially what the *practical* differences are between these theoretical results.  \n\t- Empirical results (especially on SMAC) are not strong enough in my opinion. The authors argue that \"Many researchers in MARL regard the performance as the only standard to evaluate an algorithm, while we would argue that the interpretability is also significant, especially to the deployment in industry.\" While I certainly agree that the performance shouldn't be the only standard to evaluate an algorithm, I'd still expect the proposed method to be able to outperform all existing methods by a *significant margin* on at least one or two tasks, since in the two tested domains, I'm not very sure how useful the interpretability really is. \n\n- Major questions:\n\t- The authors provide some discussion about the main differences between their work and [1] (at page 6). Algorithmically, it is clear that they propose a new multi-agent value-based method (SHAQ), while [1] proposes a multi-agent actor-critic method (SQDDPG).  But they share the same key idea of using Shapley Q-values. Theoretically, it seems that the main difference is that the authors fixed a problem in [1] and then show that the generalised Shapley value is a solution in the $\\epsilon^*$-core of Markov convex game. I might miss some other important theoretical contributions. Can the authors clarify this? Also, what are the practical differences between your theoretical results and the ones in [1]?\n\t- The predator-prey tasks chosen in the paper are for modelling relative overgeneralization. Why were these type of tasks chosen? Why are SHAQ and VDN better at tackling relative overgeneralization than other baseline methods? Actually, the term \"relative overgeneralization\" was never even mentioned before the experiment section about predator-prey. I think more discussion should be provided regarding this. \n\t- On predator-prey, why is SQDPPG not compared against (I couldn't find it in the appendix either)? Also, the authors mention that \"W-QMIX does not perform as well as reported in Rashid et al. (2020). The reason could be its poor robustness to the increased explorations (Rashid et al., 2020) for this environment.\" How does W-QMIX perform with less exploration, say annealing $\\epsilon$ over 10k/50k timesteps instead of 1mil? \n\t- If the generalized Shapley value can be thought of as a linear value factorization, I think it might be useful to discuss the limitations some, as I'd assume some tasks might need a non-linear value factorization to solve. \n\n**Post-rebuttal:**\n\nI thank the authors' for their detailed rebuttal. It helped clarify some questions I had. However, my main concern regarding the significance of their theoretical contributions remains, especially in terms of the practical differences between their theoretical results and the ones in [1]. Therefore, I decided to maintain my score. \n\n[1] Shapley q-value: A local reward approach to solve global reward games. Wang et al. AAAI 2020. ",
            "summary_of_the_review": "I'm leaning towards rejecting the paper at this stage since I'm not sure how significant the theoretical contributions are compared to some existing work and the empirical results are not strong enough to me. I'm happy to revise my score though if I underestimate the contributions from either aspects. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}