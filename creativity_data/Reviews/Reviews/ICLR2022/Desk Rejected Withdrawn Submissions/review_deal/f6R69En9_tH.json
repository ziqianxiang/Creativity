{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a novel technique to detect software vulnerabilities by learning a max-margin hyperplane on the feature space. They then use the learned hyperplane to distinguish vulnerable vs non-vulnerable datasets. The paper evaluated their results on a well-used dataset and shows that they significantly improved the result.",
            "main_review": "+ In the context of vulnerability detection, combinations of GAN framework + kernel classifier is novel. The approach suits well for vulnerability datasets, where the number of vulnerable samples is scarce.\n\n+ The paper reports significantly better results, especially for the F1 score, over the other baselines.\n\n- Some settings in the evaluation dataset are not clear to me. What is the granularity for classification? Are you classifying each function or a statement as vulnerable? As far as I know the datasets the authors use contain function-level annotation. In such a case, why did you use statement-level encoding?\n\n- In the GAN framework, it is not clear what is the source and the target. Also, how domain variant features are extracted. \n\n\n",
            "summary_of_the_review": "The paper proposes a novel technique for vulnerability detection and shows improved results. However, at places the writing needs to be updated to clarify the details.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a machine learning approach to detect software vulnerabilities. The approach aims to solve two problems: (1) improving the prediction performance by learning an automatic representation as opposed to a handcrafted one and (2) solving the shortage of labeled data that is needed to train. For this, the learning is transferred from a small, labeled dataset onto a large unlabeled one. They, therefore, use a combination of domain-invariant features and kernel methods. Finally, the authors evaluate their method by comparing the different metrics to the state-of-the-art. With an ablation study, they show how the transferring capabilities change if their method is changed.",
            "main_review": "## Comments\n* A novel approach can make a contribution even if it is not beating the state-of-the-art. The reported results are mediocre, excelling in some metrics in different settings. The authors should additionally  provide reliable measures as means and standard deviations to allow for further interpretation.\n* The authors train with labeled source and unlabeled target domain data. It would be interesting to see how the method performs on completely unseen data?\n* T-SNE is not suitable for a reliable interpretation. I would suggest that the authors use a method that is less parameter-dependent. \n* “The process of labeling vulnerable source code is a tedious, time-consuming, error-prone, and can be very challenging even for domain experts.”  -- “a” seems redundant.\n* Unclear notation S = {(x_1^S)…\n* It is unclear how the code function x_i^S is defined and what its function parameters are\n* Its furthermore unclear how the embedding vectors are obtained\n* “On the other side, kernel methods are well-known for its ability to deal with imbalanced datasets” “its” -> “their”\n* In the caption of Fig. 2: “space while in the feature space our proposed cross-domain kernel classifier help to distinguish the vulnerable and non-vulnerable data.” “help” -> “helps” \n* Also in Fig. 2 caption: “At the end, when the source and target domains are intermingle, we can transfer our trained cross-domain kernel classifier to classify the data of the target domain” “intermingle” -> “intermingled”\n* Authors declare that the margin is invariant if parameters are scaled by a factor. But the factor is not mentioned later and its not clear what margin is meant, source or target. Finally its unclear what is left to maximize if the following assumption holds that the minimized minimum is 1.\n* In the equation 4 ff authors introduce a x_i wit i = 1… N_S+N_T the following equations iterate x_i for both domains T and S but start from 1. The same \\xi is apparently used for both x_i^T and x_i^S but there are N_S+N_T \\xi in total. Shouldn’t they be indexed with a offset of N_S ?\n* There are a lot needlessly long and complicated sentences in this paper, just as an example this one: “In the experiments, to demonstrate the capability of our proposed method in the transfer learning for software vulnerability detection (SVD) (i.e., we can transfer the learning of software vulnerabilities (SVs) from labelled projects to unlabelled projects where the labeled projects and the unlabeled projects are from different application domains), the datasets (FFmpeg, VLC, and Pidgin) from the multimedia application domains were used as the source domains, whilst the datasets (LibPNG and LibTIFF) from the image application domains were used as the target domains.” Covers 6 lines and is almost the whole paragraph.\n* Section 3.2.2 uses different characters for the variable “w”, sometimes it’s bold, sometimes it’s not. \n* Page 9: “the model performance in case (v) is much higher than that in case (iv) as well as the model’s performance in cases (iii), (ii), and (i)” is suddenly italic\n* The phrase “in almost cases” seems uncommon to me, maybe “in almost all cases” would be easier to understand\n* The whole ablation study is very difficult to read. The enumeration is done with “i)” in the first paragraph and “(i)” in the second one, which brings confusion, especially since brackets are used extensively in these paragraphs. Especially this paragraph is very hard to read: “We consider five cases including: i) without using domain adaptation and applying a feed-forward neural network to build a classifier trained on the automatic features of the labeled source domain obtained from the generator G, ii) without using domain adaptation and applying a kernel method with the max-margin principle to build a classifier (Nguyen et al., 2014) trained on the automatic features of the labeled source domain obtained from the generator G, and iii) without using domain adaptation and applying kernel methods with the max-margin principle to form a cross-domain kernel classifier ((Nguyen et al., 2014) for the labeled source domain and (Schölkopf et al., 2001) for the unlabeled target domain) trained on the automatic features of both labeled source and unlabeled target domains obtained from the generator G, iv) using domain adaptation combined with a feedforward neural network to build the classifier, and v) using our proposed DAM2P method (i.e., leveraging domain adaptation combined with our proposed cross-domain kernel classifier).”\n",
            "summary_of_the_review": "## Strengths:\n* novel approach marginally beating the state of the art in some settings\n* relevant problem in scope of the conference\n## Weaknesses:\n* language is partly hard to understand and needs improvement\n* unclear mathematical notations and symbols\n* mathematical notations look familiar but are not described in a comprehensible way\n* unclear network topology\n* mediocre results\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a cross-domain kernel classifier to improve deep domain adaptation for software vulnerability detection. The motivation is to exploit the imbalanced nature of the dataset where samples with vulnerability labels are very few compared to the samples with non-vulnerability labels. The proposed approach considers an adversarial domain adaptation setup based on GAN where the proposed approach applies a kernel-based classifier. The classifier combines labeled source data and unlabeled target data, and then learns a hyperplane to separate source non-vulnerable from vulnerable data and target data from the origin such that the margin is maximized.",
            "main_review": "Strength:\n\n+ Introduces kernel based classifier in existing GAN based unsupervised deep domain adaptation method for software vulnerability detection. \n+ Comparative results and ablation study show the proposed method improves the accuracy in the target domain.\n\nConcerns\n\nThere are simpler ways to handle dataset with imbalanced classes including sampling and weighting. The proposed method should be compared against one of these approaches. It is not clear if the baseline methods used for comparison actually use any of these measures in their classifiers.\n \nScarcity of labeled vulnerabilities is shown as a motivation of this work. However, it is not obvious how the label distribution actually impacts the domain adaptation problem in this case. A discussion on this would strengthen the motivation of this work.\n\nThe datasets used for experimentation seem smaller compared to the network architecture used for the training, and therefore, the result could potentially reflect an impact of overfitting of the data. Experiments with larger datasets could strengthen the reliability of the experimental results. What are the vulnerabilities included in the dataset? Does the dataset include different types of vulnerabilities?\n\nThe visualization in Figure-3 would be effective if a baseline domain adaptation method is also visualized.  Current visualization comparison with a method that does not perform domain adaptation exaggerates the actual contribution of this work. VulDeePecker is designed to train with multiple programs. Does this method include all the source programs for training in the experiments?\n",
            "summary_of_the_review": "The novelty of the proposed approach has some novelty in domain adaptation with respect to the specific application. However, a more detailed motivation of actual issue with imbalanced dataset and more appropriate use of baseline methods with more reliable datasets for experimentation would strengthen the claims.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper addresses the data imbalance issue in the software vulnerability detection task. Because the data are labeled software projects, and the labeling process requires significant human labor, the labeled data are very scarce. Existing methods have addressed this issue by transferring the learning from labeled data to unlabeled data, by learning domain-invariant features. The authors further argue that existing tools do not consider the fact that the vulnerable code is significantly less than the non-vulnerable data in the labeled data. Exploiting this fact in the training process can improve the performance of existing methods.\n\nThe authors introduce kernel methods to the domain-invariant learning process. In addition to using a generator that can mingle source and target data, the authors propose to learn a hyperplane that can separate the vulnerable data from the non-vulnerable data, and target data from the origin. The cross-domain kernel classifier can then be used to classify the data in the target domain. The generator and this kernel classifier are combined during the training process.\n\nThe authors conduct benchmark and ablation studies to show the effectiveness of the proposed method empirically.\n\n**update after the author response**  \nI thank the authors for their detailed response. Overall, my review remains unchanged unfortunately. The results of the latest study is not in line with the theme of this paper. I believe this needs further investigation. \n\n",
            "main_review": "Strength:\n1. The paper is overall easy to follow.\n2. Awareness of imbalanced data in the SVD problem can be an interesting direction to improve the performance of current tools.\n\nWeakness:\n1. The novelty of the approach is limited in the following ways: i) domain adaptation has already been studied in other existing methods, and using kernel methods in the feature space is also a classical approach and very straightforward application in this use-case, so combining these two does not provide enough novelty from the technical perspective; ii) Using kernel methods in the feature space generates a different featuring compared to without using kernel methods, however, this featuring does not provide insight to the field of SVD other than different high-dimensional points. The performance of the proposed method is not exceptionally better than the existing ones. Neither the techniques nor the results are enough novel or significant.\n\n2. The authors introduce the issue that imbalanced data in the training data can hurt the performance of existing tools, but it is hard to tell that the fact that kernel methods improve performance is because that it addresses the imbalanced data issue. For example, if combining other balancing data methods with existing tools, can those tools be improved? Otherwise, we can only draw the conclusion that kernel methods are more suitable for the SVD task based on empirical evidence. ",
            "summary_of_the_review": "The authors propose to combine kernel methods with domain-invariant learning, which achieves better performance for the SVD task. Even though the authors argue that this is because kernel methods can better handle imbalanced data, the evidence is not enough to support this claim. The technical novelty is also limited. Therefore, I recommend the rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}