{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a regularized VAE  that can learn group disentangled representation for groups of factors with weak supervision. The idea is to use paired data and add a kl divergence regularize on the ELBO. The regularize minimize the kl divergence between the latent variables that correspond to the group shared by the paired observations. ",
            "main_review": "strenth\n\nweakness: \n1. Even though the experiment result shows clear improvement on MIG metric over the baseline, the other results such as Fair classification results do not show very significant improvement.\n\n2. The experiment is only done on the data set that has only at most two groups, not sure how the model would perform when we have a real-world dataset where there are more groups than just two.\n\n3. As the method require labeling of the paired instances, this seems to restrict the applicability of the current method. ",
            "summary_of_the_review": "The paper is clearly written, the idea is neat, but the contribution of the method both in terms of theory and application seems to be very incremental and limited to the existing field.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a divergence based regularisation technique to obtain group-disentangled representations within a VAE framework in a weakly-supervised fashion. The factors of variation in the data are divided into two groups - content and style, and a KL-divergence regulariser is imposed on the approximate posteriors of the pairwise input data which share the same group. The regulariser only acts on the latent variables for the group which is shared by the pairwise input data. The idea is relatively simple and easy to implement. In addition, motivating the need for a metric to quantitatively measure group disentanglement, the paper proposes a metric based on the Mutual Information Gap (MIG) for group-disentanglement called as group-MIG. \n",
            "main_review": "The idea is based on KL Divergence between posteriors of pairwise input data with the same group. It is simple and can be implemented without introducing additional computational complexity. The paper is written well and reads easily. \n\nThe proposed approach is based on weakly supervised disentangled learning for groups and achieves good performance on group MIG metric as compared to existing group disentangled approaches like MLVAE and GVAE. The proposed group MIG is quite relevant to the task of group disentanglement as existing techniques do not consider group disentanglement into evaluation. It can be computed similar to the MIG but with a slight modification.\n\nHowever,  the approach assumes that the groups of data are known. In this context, the idea of using a KL divergence between approximate posteriors has been used in (Locatello et al. 2020) to estimate unknown shared group in pairwise data and also the size of the corresponding estimated group. While this KL divergence is not used as a regulariser in (Locatello et al. 2020), the possibility of using this KL regulariser is discussed. In this sense, it seems that the work in (Locatello et al. 2020) is more general than what is presented in this paper. Note that group disentanglement can be achieved as well in (Locatello et al. 2020) by simply assuming that the groups are known and then setting the latent vector indices to corresponding groups. I believe the current work would be much better placed if it discusses this in detail.\n\nThe exposition could be greatly improved in terms of motivation and related work. In the introduction, the authors mention that existing works “define disentanglement as learning to separate all factors  of variation in the data”. However, the definition of group disentanglement wherein some factors are entangled while groups of factors are disentangled is given in (Higgings et al 2018). Based on this, there are several algorithms like (Caselle-Durpe et al 2019), (Quessard et al 2020) and (Pfau et al 2020). None of this is discussed in the paper nor mentioned.  \n\nThe evaluation could have been more thorough. For example, it is a common practice to evaluate multiple metrics for disentanglement. They are relatively easy to compute. I understand the main objective of the paper is group disentanglement and these metrics do not necessarily capture this, but it would be nice to clarify why only MIG is reported. On the other hand, group MIG can be computed even for unsupervised models and AdaGVAE by giving the group information for evaluation. In addition, only the median values seems to be reported. I encourage the authors to report the confidence intervals along with central tendencies to give a complete picture of the results.\n\nThe regulariser is based on groups. However, from Table 1 it seems that it achieves better performance in terms of full disentanglement (disentangling all factors of variation) even as opposed to the ones which directly optimise (in a weakly supervised fashion) for full disentanglement in 2 out of 3 datasets. It seems a bit counterintuitive in the sense that a distribution based regulariser on sets of factors of variation does not seem to have sufficient inductive bias on its own to disentangle each factor of variation. I believe this needs clarification and needs to be discussed in the results.\n\nCurrently it seems that the implementation is only applicable if there are only two groups. Although the extension to multiple groups is straightforward, it is unclear if such a regulariser over multiple groups has sufficient inductive bias to obtain disentangled representation across many groups. For example, take the extreme case of setting each factor of variation to a single group, then can it be expected that such a KL regulariser can obtain full disentanglement (in terms of any of the metrics)?\n\nAdditional comments:\n -   In figure 1, what does shared weights $\\theta$ and $\\phi$ refer to? Are they supposed to be vice versa?\n -   What does $H(g_i)$ correspond to in Equation 8?\n\nReferences:\n\nFrancesco Locatello et al. Weakly-supervised disentanglement without compromises. arXiv preprint arXiv:2002.02886, 2020.\n\nHiggins, Irina, et al. \"Towards a definition of disentangled representations.\" arXiv preprint arXiv:1812.02230 (2018).\n\nCaselles-Dupré, Hugo, Michael Garcia Ortiz, and David Filliat. \"Symmetry-based disentangled representation learning requires interaction with environments.\" NeurIPS, 2019.\n\nQuessard, Robin, Thomas Barrett, and William Clements. \"Learning Disentangled Representations and Group Structure of Dynamical Environments.\" NeurIPS (2020).\n\nPfau, David, et al. \"Disentangling by subspace diffusion.\" NeurIPS (2020).\n",
            "summary_of_the_review": "My current assessment of final score is based on some existing questions (see above). I am willing to raise my score assessment if my major concerns are sufficiently addressed, i.e wrt how this method offers novel insights beyond what is presented in (Locatello et al. 2020) which seems to be more general, exposition of related work and also clarifications regarding results and evaluation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes weekly supervised regularization which encourages group-disentangled representation learning. The key part of weekly supervised regularization is to minimize the KL divergence between the latent variables of a pair of images that share one conept.",
            "main_review": "Strength:\n\n1 The task of (group) disentangled representation learning is important.\n\n2 The writing is clear and easy to follow.\n\n3 The application of fair classification is interesting.\n \n\nWeakness:\n\n1 Lack of novelty. The proposed method that uses paired data with shared concepts to help constrain disentangled representation learning is a common method in various papers:\n\n[1] Zero-shot Synthesis with Group-Supervised Learning, which proposes group-supervised learning to formalize paired data as graphs and mine the similarity between paired images and boost disentangled representation learning. This paper also proposes multiple applications such as data augmentation.\n\n[2] Dual Swap Disentangling, which also uses paired images to help disentangled representation learning.\n\n[3] ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes\n\n[4] TransMoMo: Invariance-Driven Unsupervised Video Motion Retargeting\n\n[5] ReenactGAN: Learning to Reenact Faces via Boundary Transfer\n\nConstraining of disentanglement can be conducted either in latent space or image domain with paired images sharing the same attributes. \n\nThe author should compare the proposed method with these similar methods and claim the advantage. \n\n2 The claim of weekly supervised learning is not promising. The training still needs attribute labels of data, otherwise, the paired images can not build. This is full supervision instead of weekly supervision\n\n3 Evaluation dataset is too simple, synthetic controllable dataset. The real-world and more complex dataset should be conducted. E.g., ilab-20M [6] dataset, where each image has content, pose, and background attributes. RaFD [7], a face dataset with controllable \n\n4 More baselines: the method proposed in [1] GZS-Net and [2] can be used to compare.\n\nReference\n[1] Zero-shot Synthesis with Group-Supervised Learning\n\n[2] Dual Swap Disentangling, which also uses paired images to help disentangled representation learning.\n\n[3] ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes\n\n[4] TransMoMo: Invariance-Driven Unsupervised Video Motion Retargeting\n\n[5] ReenactGAN: Learning to Reenact Faces via Boundary Transfer\n\n[6] iLab-20M: A large-scale controlled object dataset to investigate deep learning\n\n[7] Radboud Faces Database\n",
            "summary_of_the_review": "Due to the lack of novelty, simple experiments, and lack of important baselines. I recommend to rejection.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a group-disentangled representation learning method based on VAE. A standard VAE learns a latent state and aligns it with an isotopic Gaussian distribution. A lot of recent VAE variations like $\\beta$-VAE, FactorVAE, $\\beta$-TCVAE pursue learning precise representations for finer factors. But this might not be practical or desired. Therefore, this new method, GroupVAE, learns to disentangle 2 groups of factors, by leveraging weak supervision and paired observations that always share a group of factors. In practice, GroupVAE separates the latent state to two latent groups (content and style), the input pair would share one of the latent groups and differ in the other. Which group is shared is given to GroupVAE. Then the latent factors corresponding to the shared group would be used to compute a group-wise KL specific to that shared group. The sampling process would guarantee the shared group would be alternating between content and style. The authors further propose group-MIG, to evaluate the effectiveness of group disentanglement methods. More importantly, the method is validated on the disentanglement metrics, fairness classification and 3D point cloud tasks.",
            "main_review": "Pros:\n1. The paper is really clear and concise in the method description. GroupVAE naturally derives from the standard VAE with the assumption on the group-disentanglement. The given preliminaries are also very sufficient. I can understand the proposed idea easily.\n2. The evaluation tasks are indeed quite comprehensive. Besides the disentanglement metric, group-MIG, other tasks like fairness classification and point cloud generation really motivate the design of the method.\n3. Both quantitative and qualitative results are given, supporting the claims made earlier.\n\nCons:\n1. I feel the problem setting is a bit makeup. If you know the shared latent group of two samples, it is actually a very strong supervision. I don't know how generalizable this assumption is in the real-world scenarios.\n2. If you know which group of latent variables is shared, you don't have to stick to VAE. Recent trending contrastive learning could be a good alternative.\n3. I think you can give more examples on scenarios with more than 2 groups.\n4. The results would be more convincing with more compared methods like $\\beta$-TCVAE.\n5. For the fair classification, what if you simply discard the sensitive factors and only learn on the non-sensitive factors (for different methods)?\n6. I wonder for the compared methods, whether you can provide the same supervision to them to ensure a fair comparison? (By slightly changing their architectures to accommodate the extra supervision)\n7. I think if you relax the total correlation term from $\\beta$-TCVAE to become a group-wise total correlation term, you might have similar formulations. It should be worth comparing with these related methods more in detail.\n8. Group-wise disentanglement can also often be seen in the sequential setup. It is worth to discuss the related works [1, 2] in this paper.\n\n[1] Bai, J., Wang, W. and Gomes, C.. Contrastively Disentangled Sequential Variational Autoencoder. NeurIPS 2021.\n\n[2] Han, J., Min, M.R., Han, L., Li, L.E. and Zhang, X.. Disentangled Recurrent Wasserstein Autoencoder. ICLR 2021.",
            "summary_of_the_review": "In general, it is comfortable to read the paper and walk through the method. It is natural to extend dimension-wise disentanglement to group-wise disentanglement. However, given that the prior works mostly work on unsupervised setup, it is slightly weaker when you introduce the extra known supervision. If you do have such supervision, you can alternatively attempt contrastive learning and some other invariant representation learning methods. That's why in this case, more compared methods would make your claims more convincing. For example, given that your derivation is so close to $\\beta$-TCVAE, you'd better include it in the experiments. If it is an extension of the $\\beta$-TCVAE, then the novelty is incremental. In general, I think the authors should keep improving the paper and add more analysis experiments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studied the problem of **weakly supervised disentanglement** and suggested using a **KL-divergence** term to encourage the distributions of latent variables corresponding to a shared group to be close.\nThe proposed method was evaluated only empirically.",
            "main_review": "## Strengths\n\nThis paper is easy to follow (in other words, predictable).\nThe problem formulation and the illustration are clear.\nThe proposed algorithm is intuitive and should be easy to implement by any experienced researcher.\n\n---\n\n## Weaknesses\n\nOn the other hand, if I didn't miss anything, this work brings little to the community.\nEqs. (1)-(5) are basically the standard VAE approach.\nThe proposed method is based on the regularization term Eq. (6).\n\nThe motivation seems to be based on the following:\n\n> Existing group-disentangled approaches (Bouchacourt et al., 2018; Hosoya, 2019) enforce disentangled group representations by using an average or product of approximate group posteriors.\n> However, as group representations are dependent on the observations used for the average or product, observations belonging to the same group may not be encoded to the same latent representations.\n> We address this inconsistency challenge by incorporating a simple but effective regularization based on the Kullback-Leibler (KL) divergence.\n\nHowever, it is not very understandable and convincing to me.\nWhy observations belonging to the same group should be encoded to the same latent representations?\nIn this problem setting, \"a group might include several factors of variation\" so there would be intra-group variation.\nEnforcing them to be the same would probably lose information and hurt the generation performance.\nOverall, the proposed method might be effective for certain settings, but its motivation was not well explained and its behavior was not sufficiently analyzed.\n\n---\n\n## Questions\n\n- Pairs $x$ and $x'$ are equal in position, but why _asymmetric_ KL-divergence is used in Eq. (6)?\n- In practice, how do you choose the dimension for each group? How do you evaluate and justify it?\n",
            "summary_of_the_review": "This paper proposed a practical solution to weakly supervised disentanglement where pairs of instances that share a set of factors are available.\nThe proposed regularization would be easy to implement.\nHowever, the justification of this choice is insufficient and not well explained.\nThere is no theoretical analysis or guarantee and the proposed method was only evaluated empirically.\nTherefore, in my opinion, this paper is on the borderline.\nI don't mind seeing it accepted but I would not champion it.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}