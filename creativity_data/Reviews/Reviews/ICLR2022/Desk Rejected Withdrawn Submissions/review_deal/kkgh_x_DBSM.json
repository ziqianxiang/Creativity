{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors provide a method for modifying data prior to its release, inspired by emerging approaches in targeted data poisoning, to drastically reduce the generalization of a deep learning model trained on the data, therefore making the data useless for rivals. The proposed method may be utilized in real time, allowing companies to protect their data as it is being released.",
            "main_review": "The paper's contributions are adequate, but it needs considerable polishing before it can be published.\nI have the following comments:\nThe two significant problems of the work are the poor structure of the paper, and the document is difficult to follow due to the poor grammatical quality.\n\nThe introduction section is not well organized. It is quite short and does not provide detailed information about the background, technique, gaps and focus of the paper. The authors must insert some detailed information and set the paper's hierarchy.  \n\nRelated work should not be a sub-section of the Introduction section.\n \nThe authors must provide arguments why their technique is better than other methods. What are the author's limitations in this work? What is the disadvantage of their work? It is preferable to include this information in the experimental discussion section.\nIn terms of English quality, I believe the paper should be reassessed.  \n\n",
            "summary_of_the_review": "The article presents an interesting approach in adding noise to data. The idea presented in the paper is significant and timely and it can be published after a major revision on the arrangement and structure of the work. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a data anti-scraping approach through data poisoning. Thus, slight manipulations of the proprietary datasets released by companies or organizations enable to protect the value of these datasets, preventing competitors from training accurate machine learning models when using these datasets for training. The poisoning approach relies on a gradient matching strategy, similar to the one proposed by Geiping et al. 2020. The authors include a comprehensive experimental evaluation including white and black box settings as well as scenarios similar to online learning. ",
            "main_review": "Strengths: \n-\tThe idea of using data poisoning as a mechanism for anti-scraping is very interesting, novel and relevant from a practical standpoint. \n-\tThe use of gradient matching strategies allows to scale up for datasets like ImageNet.\n-\tThe experimental evaluation supports the usefulness and impact of the proposed approach to poison the datasets. \n\nWeaknesses: \n- The novelty of the paper is very incremental. The authors just rely on a previous data poisoning technique (Witches’ Brew in Geiping et al. 2020) and the differences with respect to this technique are very minor. \n- The scenarios considered are restricted to the case where the victim trains the machine learning model relying completely on the poisoned dataset, which is very restrictive. However, in practice, in many scenarios the victims would collect data from different sources or they can even have a smaller proprietary dataset as well. Thus, having a combination of both poisoned and non-poisoned data points can make a difference, especially for the defender. It would be interesting to see if the attack is effective against existing defenses in those scenarios.  \n- The comparison with other poisoning methods (see Table 2) just includes TensorClog and random noise. It would be interesting to compare with approaches that approximate the solution of the bilevel optimization problem in Eq. (1)-(2) as for example Metapoison (Huang et al. 2020), at least for a smaller dataset like CIFAR. \n- The authors stress the importance having a scalable technique for manipulating large scale datasets. In this sense, I am missing a more rigorous analysis (even empirical) of the scalability of the different methods compared. For example, the arguments against Feng et al. 2019 (and, perhaps, Huang et al. 2021) are arguable. \n",
            "summary_of_the_review": "Although the idea of using data poisoning as an anti-scraping technique is interesting and promising, the novelty of the proposed method is very incremental compared to Geiping et al. 2020. On the other side, the scenarios considered assume that the victim trains the model with 100% of poisoned data, which may not be the case in many practical settings. In this sense, I think that the authors could have the opportunity to explore other scenarios, where, possibly the victim has more chances to mitigate the effect of the poisons. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work describes a data poisoning technique that operates at the dataset level (poisoning all data) to achieve drops in the availability of models trained on this data, all while adding imperceptible perturbations. Through experiments on both mid-scale (CIFAR10) and large-scale (ImageNet) datasets, the authors demonstrate the effectiveness of their technique. In addition to being architecture agnostic, the authors also show how the same technique can be modified to work while using surrogate models and poisoning only a fraction of the training data, pushing the threat model closer to practical settings.",
            "main_review": "# Positive feedback\n\n1. Section 4.2: The finding of models with larger capacity being better at generating poisons is quite exciting and might pave the way for stronger poison attacks.\n2. The fact that using only a fraction of data, all while having a surrogate model with not-so-great performance is sufficient, is also quite exciting and paves the way for these attacks on a larger scale. Although 10% of a large dataset is still quite large (and hard to poison this much even if the data is collected online), it is undoubtedly a step in the right direction. \n\n---\n\n# Criticism\n\n1. This work stresses how prior works require access to \"all\" data to begin poisoning. It should be noted that the party that releases the data indeed **does**  have access to all the data (unless it is an online setting, in which even the data releaser gets access to the data as it is collected). Thus this requirement is not too far-fetched. Moreover, most of the experimental results (and the majority of focus in this work) are for the setting where access to \"all\" the data is anyway assumed.\n2. Table 1: Validation accuracy on Unlearnable is not too far from the proposed method. Without any error numbers, it is hard to say if the difference is even statistically significant. Please provide a '+-' bound on these numbers. Also, is there a reason why numbers for Unlearnable (16/255) are missing? It feels odd not to have those numbers.\n3. Table 2: Any reason for not including results with \"Unlearnable\"? \n4. Section 3.4: \"Whereas Huang et al. (2021) requires access to all the data....\".  Given that the proposed technique also requires access to all the data for most of the results in the paper, it is unfair to stress prior works also doing the same. The authors cannot mention this as an unrealistic scenario and then present most of your results in that scenario itself. Additionally, the performance gap (Table 4) is too high to use the given reason as a justification.\n5. Section 3.5, page 6: \"Furthermore, since we poison the entire...\". A party can always sample other data from a similar distribution (from the Internet or any other source) and compare latent representations to potentially identify poisoned data. As far as defenses that assume some percentage of poisoning are concerned- what is the effect of running such defenses anyway? They may not be very effective, but it would be interesting to see how much damage they can prevent if run anyway.\n6. Section 4, page 7: \"By and large,....in the embedding space\". This is not true- can the authors please cite a survey paper or any other references that suggest this is the case? A lot of work does not use a k-nearest neighbors approach or transfer learning to solve this task that the authors seem to have missed out on (entire techniques like siamese-networks combined with score thresholding, etc.). \n\n---\n\n# Minor comments\n\n1. Introduction: \"...openly accessible to scraping..\". Even though such public data is open to scraping, the legality of using such data to train models may conflict with GDPR rules. The authors might want to modify their narrative while motivating their work slightly.\n2. Related Work: \"...also takes 5-7 GPU days\". Although this is a non-trivial resource requirement, it is not too high for a motivated adversary.\n3. Related Work: \"...from using scraped data to train their own models\". The attack is technically a model-stealing one here; not apparent how this is relevant to the current paper.\n4. Please label all equations.\n5. Might want to shift equation (4) to before (3) since (3) mentions a notation that is not introduced until (4)\n6. Section 3.5, page 6: \"...drop in validation accuracy we saw in previous...\". Please mention the exact number\n7. A nice way to better test the \"stability\" of poisons could be: measure the drop in performance when X% of the data is randomly dropped (not used) by the victim for varying values of X, and see how attack success rates vary (as well as variance in results).\n8. Typo: page 6, \"Such data is may be...\"\n9. Table 8: which dataset and model are these numbers for?\n\n",
            "summary_of_the_review": "Overall the paper seems to move in the right direction as far as empirical results for poisoning are concerned. Although there is not much technical novelty, the results seem to make up for it. The biggest issue is how the paper is advertised- the authors stress current attacks not being capable of working in the online setting but do not perform a proper evaluation in the same setting. Similarly, prior works are criticized for assuming access to the full dataset, even though the paper's first half focuses on the same setting. I would urge the authors to reword their contributions.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper follows a recent research direction of protecting proprietary data by poisoning data before release. Social media companies can safely release images with subtle perturbations so that other organizations cannot use these images to train a competitive model. Compared with previous works, the proposed method, called Alignment, is claimed to be more flexible and less computationally expensive. Primarily, it supports online modification: perturbations can be computed with a target gradient approximated from each subset of the dataset, while the crafting gradient can be computed independently. Experiments on ImageNet, CIFAR-10, and facial datasets prove the effectiveness of the proposed algorithm.",
            "main_review": "### Strengths:\n- The paper is technically sound. \n- The online modification is new and proved to be effective\n- Experiments on ImageNet, CIFAR-10, and facial datasets prove the effectiveness of the proposed algorithm.\n\n### Weaknesses:\n- The claim about the advantages of the proposed method compared with the previous works is not convincing. \n   - Alignment still requires access to a large dataset/subset and does expensive gradient computation. \n   - As provided in the Appendix, section A.4, the perturbation crafting process on CIFAR-10 (140 minutes) is much longer than the model pre-training (12.5 minutes). On ImageNet, it requires 17 hours to craft perturbations for a batch of 25000 images. It is unclear if the crafting gradient is computed jointly or independently.\n   - The online modification mechanism is quite general, and I argue that it can be applied to DeepConfuse. DeepConfuse can train the auto-encoder once on a subset of the dataset (no need to compute the gradient for every new image).\n   - As I understand, Unlearnable has a sample-wise version, which only requires access to the sample itself. Except it needs a pre-trained classifier f', but Alignment also uses such a pre-trained classifier. It removes the advantages of Alignment.\n   - In general, the authors should provide clear evidence of their advantages. For example, speed comparison between techniques in a fair configuration.\n\n- The experiments in Section 3 are messy. The experiments and analyses in Tables 1, 2, and 4 should be applied on both CIFAR-10 and ImageNet. It is weird to have a separate subsection for ImageNet to do one analysis that is not distinctive for ImageNet. Table 1 does not have DeepConfuse's result; it is also not comparable to Table 4 due to dataset mismatching.\n\n- According to Table 4, Alignment performs worse than Unlearnable and DeepConfuse in the black-box setting (which is more realistic than the gray-box one in Table 1). The excuse provided by the authors is not convincing, as pointed out in the first weakness.",
            "summary_of_the_review": "I lean towards rejecting the paper. The claim about the advantages of the proposed method compared with the previous works is not convincing and lacks supporting data. Alignment performs worse than Unlearnable and DeepConfuse in the black-box setting. Also, the experiments are messy and should be revised.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I find no ethical concern.",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}