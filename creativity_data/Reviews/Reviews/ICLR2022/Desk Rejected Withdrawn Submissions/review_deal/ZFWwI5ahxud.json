{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper addresses the problem of unsupervised domain adaptation in a setting where we don’t have access to the full test distribution at training time. The authors differentiate between two types of distribution shift, static and continual. Where static shift is the standard setup where we have a train and test distribution, and in the continual shift setup, the test distribution is continuously changing (similar to continual learning). The difference with the gradual domain adaptation setting is that the performance of the model is evaluated on the full spectrum of the changes rather than just the final stage. Also, there is no assumption about smoothness of the transitions, i.e., the gap in the distributions in step 1 and 2 can be as big as the gap in the distributions is step 1 and 10. \n\nThey study this problem in the context of an image classification problem on the MIT-state dataset. In this dataset each image example is annotated with the name of the object in the image as well as a set of attributes.\nThe configuration the authors use is to train a neural network (ResNet-18) with two classification heads, one for recognizing the attribute and one for recognizing the object label and the models are evaluated in a continual learning setup based on their class accuracy (object label accuracy), attribute accuracy, and joint accuracy (both class label and attribute). \n\nTwo new adaptation methods are proposed in the paper to deal with both static and continual shift. Both methods are based on adding an adapter layer before the classification head.\nThe first method is to use a two-layer LSTM as the adapter layer, where the input  sequence fed to the LSTM is the batch of examples. The intuition is to learn a recurrent mechanism for modulating image features given the history of images so far. \nThe second method is to replace the first layer in each head (which is a two layer MLP) with a hebian layer, which again processes each batch sequentially. \n",
            "main_review": "### Strengths:\nI think the paper is pointing to a very interesting and practical direction, which machine learning algorithms should be able to deal with.\nSmart Idea to use the MIT-state dataset for evaluating models under distribution shift. \nThe paper proposes new methods, two types of adapter layers, to deal with continual distribution shift.\n### Weaknesses:\n- I got the feeling that the paper is written as if it is introducing a new setup for distribution shift, but I don’t think it is fair to consider this setup novel considering all the prior work in domain adaptation, gradual domain adaptation and continual learning. I think we could benefit from having a standard framework and terminology for all these different setups. Maybe this paper is putting some effort into this direction in the introduction, but I think one could do a much better job in doing that. \nGenerally speaking I think the paper is not well contextualized within the prior work.\n- The proposed methods are not well explained, maybe a diagram can help to explain things a bit more clearly. But also not enough intuition or theoretical analysis is provided to tell us why the authors think these methods should work.\n- Baselines used in the evaluation do not lead to a very convincing story and the ablation experiments could be a bit more extensive.\n- Lack of insightful discussion about the results, e.g., let’s say you are comparing the LSTM method with the hebbian layer method, based on your experiments, which one is better in which setting and why is that? \n- The evaluations are only limited to the MIT-state dataset. I think this can still be a good paper, even if it’s only evaluated on one dataset, but the paper needs to have a convincing story from all the other aspects and the limit of the scope of the findings need to be discussed.\n\n\n### Suggestions:\n- Table 1, is a bit confusing to me. e.g., to me it seems domain adaptation is a general term that can include all various forms of supervised or unsupervised domain adaptation settings. I think it would be more helpful to maybe just have a table (or a plot) to illustrate all the different settings for domain adaptation (e.g., whether we have access to the target domain labels or not, whether we have access to the target distribution during the initial training stage or not, the type of shift (covariate shift, prior probability shift, concept shift, etc.)). And then to give an even more complete picture you can mention which algorithms are applicable in which settings. \n- It would help a lot if you can extend the caption of Figures so that they are self-sufficient in terms of being understandable. E.g., In Figure 2, it is not clear what each x and y axis is, without going back to the main text. \nI think the experimental setup, the datasets and how you train and test models, e.g., what are the evaluation metrics, etc, can be explained in a more clear way. I had to go back and forth a few times to understand what was happening. \n- Other baselines that you could consider:\n(1) Self-training (in similar settings as applied for gradual domain adaptation tasks),\n(2) Other, simpler, types of adapter layers, e.g., MLP?\n\n\nTypos:\nLast sentence in section 4.2 reference is referring to ??.\n",
            "summary_of_the_review": "The paper investigate an interesting problem, i.e., continual distribution shift, and proposes two techniques to techniques to deal with this challenge. However, the methods are not very well explained and are not intuitively or theoretically grounded. Additionally the scope of the experiments is limited, in terms of the number of datasets (just one), and the considered baselines. Hence, I am leaning toward rejection, and I think the paper can be improved by addressing the above concerns as well as having a more fair and detailed comparison with prior work. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "*Summary*\nThis paper introduces a new setting of test time adaptation, where the adaptation aims to tune a model online given unlabeled test data. In their new setting, they assume that test distribution includes labels unseen during the training phase (static shift) as well as a continual shift in the label space. They find that existing methods for test time adaptation do not show good improvements on this task while their introduced models, “learn to adapt”, via recurrence and learned Hebbian update rules outperform such baselines with a good margin. \n\n\n\n",
            "main_review": "*Strong Points*\n1. They introduced a new problem setting in the field of test time adaptation. Their high-level goal in the setting is realistic and interesting\n2. They observe that the existing methods perform well under some conditions, which does not hold in their problem setting, thus fails. \n3. Their proposed simple baseline performs much better than the baselines in this setting. \n\n*Weak Points*\n1. Although I support their high-level goal in this task, assuming the test distribution that includes data unseen during training, I do not think their actual problem setting is discussed enough. They assume that a test set consists of class-attribute combinations that are unseen at train time while all attributes and classes are seen at train time. Although the combination of class and attributes is different from training time, their labels (category) are still identical with the training time. This was a little confusing part. \n\n2. Comparison in the current test-time adaptation configuration is absent. The question is related to point 1. According to their definition of the problem, i.e., the combination of class-attribute can change across test time, some settings in test-time adaptation may be applicable, e.g., adaptation in the weather condition or lighting condition change across time. This kind of adaptation is more realistic than their experimental settings, but they do not try it. \n\n3. In addition, the experiments are done only on one dataset, which is hard to justify their claims. Also, without a comprehensive study of the existing baselines and their proposed baselines, it is hard to see what made degrade in the performance of existing methods and why the proposed baselines are better. At this point, the insights from the paper are limited. \n\n\n\n\n",
            "summary_of_the_review": "Due to a lack of discussion on their actual experimental setting and lack of experiment, I recommend rejecting this paper. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents an approach for performing adaptation to shifts in the semantic space of a model. In particular, the article considers a multi-label task, where the goal is to recognize objects and attributes present in the image, the article presents the following adaptation challenges: (i) static shift, where unseen compositions of attributes and objects are seen at test time, (ii) continuous class shift, where the object classes are learned in multiple steps (as in continual learning) but attributes might be shared across them, (iii) continuous class-attribute shift, where both classes and attributes are disjoint across learning steps. The experimental results show that LSTM and Hebbian Learning outperform various baselines in these benchmarks.",
            "main_review": "**Strengths:** \n1. The problem tackled in the article (i.e. learning under semantic shift) is a crucial one. Training sets bring inherent semantic limits and breaking them through time is essential toward building autonomous agents that can recognize new semantic categories over time.\n\n**Weaknesses:**\n 1. The focus of the article is unclear. Most of the introduction refers to learning under input distribution-shift, citing domain adaptation and domain generalization works. Even after introducing the semantic-shift problem (end of page 1), the introduction describes works not considering shifting label distributions, but continuously adapting to evolving shifts at the input level (e.g. weather, lighting, etc.). This gives the reader the impression that the article is the first tackling the semantic-shift problem, however, the static-shift is already addressed in compositional zero-shot learning (CZSL, e.g. [a,b,c]) and the continuous shift (despite without multi-label predictions) is addressed in the continual learning (CL) literature. The introduction should have focused on clarifying the differences between the current work and CZSL and CL literature, that are most relevant for the proposed problem formulation. As a result, the contribution remains unclear.\n 2. Following on the previous, the experimental comparisons do not contain any baseline from CZSL and CL literature, but only on continuous adaptation to shifts in the input distribution. Since the former is the best competitor for the proposed setting, a comparison with them (or even combinations of CZSL and CL methods) is important to assess that the presented LSTM and Hebbian rules are competitive in the proposed scenario.\n3. A natural expectation the reader has in reading the title is to find not only a new experimental setting but also a tailored approach for the proposed problem. However, both LSTM and Hebbian learning are known techniques applied as they are (section 4.2) thus are more baselines than proposed approaches. Moreover, they do not work well in the scenario presented in the introduction (i.e. domain-shift) as shown in Table 3 of the appendix.  These questions both the technical contribution and the claim on the experimental results.\n4. Some relevant works are not referenced in the manuscript. For instance, CZSL approaches [a,b,c], learning under both domain and semantic shift [d,e] and works performing incremental learning under domain-shift [f]. Referencing these works and highlighting their differences (as well as reporting them in the experimental comparison) would strengthen the motivation behind the work and the experimental comparisons.\n5. The article seems to have been rushed in some parts of the supplementary: there are missing references (e.g. ??) in the main text and Table 3 of the supplementary does not have a caption.\n\nReferences:  \n[a] Misra, Ishan, Abhinav Gupta, and Martial Hebert. \"From red wine to red tomato: Composition with context.\" CVPR 2017.  \n[b] Nagarajan, Tushar, and Kristen Grauman. \"Attributes as operators: factorizing unseen attribute-object compositions.\" ECCV 2018.  \n[c] Li, Yong-Lu, et al. \"Symmetry and group in attribute-object compositions.\" CVPR 2020.  \n[d] Luo, Zelun, et al. \"Label efficient learning of transferable representations across domains and tasks.\" NeurIPS 2017.  \n[e] Yang, Yongxin, and Timothy M. Hospedales. \"Unifying multi-domain multitask learning: tensor and neural network perspectives.\" Domain Adaptation in Computer Vision Applications. Springer, Cham, 2017. 291-309.  \n[f] Kundu, Jogendra Nath, et al. \"Class-incremental domain adaptation.\" ECCV 2020.  \n\n",
            "summary_of_the_review": "While I find the problem formulation interesting, the article does not present specific approaches to address it and the position w.r.t. to the related works is unclear. Moreover, important baselines miss in both the experimental comparisons and the related works. For these reasons, my current rate is negative. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper defines two more difficult semantic distribution shift settings: static shift and continual shift and evaluates different methods on these settings to show their efficacy on the newly introduced settings. The paper is well written and easy to follow.",
            "main_review": "Strengths:\n\nThe proposed settings are interesting and useful to evaluate the current test-time adaptation methods for domain generalisation.\n\nWeakness:\n\n1. The technical novelty of the paper is very limited. The methods evaluated in the paper are all proposed in prior works. \nThere are some interesting conclusions from the experiments, e.g., the fine-tuning recurrent model improves performance on static shift, current methods struggle on continual shift with static shift, however, there is no analysis for these conclusions. \n\n2. More analysis of the experimental results will make the paper better.\n\n3. Standard deviations of the experimental results (both tables and figures) are necessary for readers to understand the stability of the methods.\n",
            "summary_of_the_review": "The proposed distribution shift settings for evaluation of current test-time adaptation methods are interesting but there isn't much technical contribution. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}