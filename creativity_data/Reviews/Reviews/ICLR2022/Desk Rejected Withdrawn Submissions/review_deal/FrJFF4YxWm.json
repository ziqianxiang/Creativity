{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents a framework for learning how to perform a collection of tasks (encoding acyclic state machines) in a fixed (deterministic) workspace. Importantly, the guards/transition symbols that the task is defined over, is apriori unknown. The key contribution of this work is to simultaneously learn classifiers to map world states to task symbols and create an agent that can perform the tasks in the workspace.",
            "main_review": "The key contribution of this work seems to be in grounding the symbols of the task description using a collection of (task, demonstration) pairs and variant of Boltzmann rationality, as per [1], and a custom mapping from task to reward function. Once grounded,  the paper describes using classic planning (A*) techniques for achieving that task. Unsurprisingly, once grounded, the Ratskills framework is able to perform extremely well since the dynamics are fairly simple/deterministic and the tasks are a short sequence of reach problems - which A* is known to be well equipped to handle.\n\nThe particular contrastive  approach suggested for grounding is interesting, and I have not seen it used for grounding tokens in formal languages (although this is outside my area of expertise). This addresses a common criticism of formal controller synthesis, which requires grounding the predicates for planning.\n\nI have three concerns with the paper:\n\nMy primary concern with the paper is with the baselines. For example in the crafting domain, did IRL get access to the state 260 state features? Was the classic MaxEnt RL algorithm (e.g., no deep learning) run on those features? Maybe this is mis-guided, but it seems that no deep learning is required to run MaxEnt RL for this domain and may perform better? \n\nMy secondary concern is generalizing to stochastic transition models and/or partial observability, where A* may prove an insufficient planner. Perhaps the proposed approach could be combined with the hierarchical planning considered in [2]?\n\nFinally, I am concerned about the scalability of the enumerative approach to search for tasks to connect to the demonstrations. While sufficient for the very restricted concept class presented, this could be a concern when generalizing to more expressive task domains.\n\n------\nTypos and small concerns:\n1. The task classifiers, G, are used as both [0, 1] and {0, 1} valued variables. Could this abuse of notation be made explicit?\n2. The the context of Boltzmann rational models, the inverse temperature is often called the rationality, which is slightly confusing in this context.\n3. The paragraph before experiments seems to contain several typos (e.g., we use compute).\n\n[1] Ziebart, Brian D., et al. \"Maximum entropy inverse reinforcement learning.\" Aaai. Vol. 8. 2008.\n[2] Jothimurugan, K., R. Alur, and O. Bastani. \"A Composable Specification Language for Reinforcement Learning Tasks.\" Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019. 2019.",
            "summary_of_the_review": "I believe this paper provides an important bridge to classic planning by automatically grounding predicates given expert examples. To my knowledge this and their approach is a novel contribution.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a skill-learning framework. As input, the algorithm takes a series of demonstration trajectories $\\tau_1, \\tau_2, \\ldots, \\tau_N$, each annotated with corresponding task specifications $t_1, t_2, \\ldots, t_N \\in \\mathcal{T\\!L}$ provided in some stuctured language $\\mathcal{T\\!L}$ (e.g. maybe $t_i = \\text{turn-on-music} \\textbf{ then } (\\text{play-with-ball} \\textbf{ or } \\text{play-with-monkey})$). Unlike most existing skill-learning work, the resulting skills are represented as option-like subtasks rather than complete policies; in this sense, the skills are \"rational\" (i.e. correspond to maximising some reward function). This has allows for more flexible skill composition than policy-based representations, and in some environments these goals might also be easier to learn than policies.",
            "main_review": "Main strengths of this paper:\n\n- **S1:** The high-level goal—namely, construction of reusable high-level skills from demonstrations—is important to the field, and this approach (particularly the use of structured annotations on demonstrations) is novel so far as I'm aware.\n- **S2:** The paper argues well for the use of reward functions rather than policies to define skills. The compositionality advantage (it's easier to interleave achievement of two reward functions than interleave execution of two blackbox policies) particularly resonated.\n\nOn the whole, I think this paper would be a valuable addition to the program, which is why I'm leaning \"accept\". However, I think there are also significant weaknesses to the approach and the way it is presented in the paper:\n\n- **W1:** The experimental evaluation doesn't seem to report performance on the training set. Specifically, Table 1 and Table 2 contain results for the _compositional_ and _novel_ splits, but not for the train split (which I assume is different for the first two). I think this is important for evaluating how well the baselines have actually fit the data; it could be that their poor performance is due to inadequate expressivity or optimisation, and not because of some generalisation failure (e.g. due to lack of compositional structure in the algorithm).\n- **W2:** Clarity issues. I could not understand some parts of the method and experimental analysis, as described in the \"further requests for clarification\" below. There were also some areas where I felt the paper could be reworded for clarity, described in \"minor issues and suggestions\".\n- **W3:** The algorithm must use blind search over tasks when planning without unknown task description $t$. The paper acknowledges this as a weakness.\n- **W4:** Parts of the algorithm feel like they could be more principled. e.g. $\\mathcal L$ looks a lot like a trajectory log likelihood under a Boltzmann model, but isn't quite a log likelihood (it's not obvious how to fit the task-contrastive term and the $\\log G$/$\\log I$ terms into a probabilistic generative model). This doesn't influence my score much because it isn't the core conceptual contribution of the paper, but it might be interesting to think about how this framework could be grounded as maximum likelihood (or similar) on a probabilistic model.\n\nFurther requests for clarification:\n\n- **CL1:** Section 3: Why is there an explicit initiation condition $I_t$ for each task $t$? $I_t$ could also be left implicit: you could say that any state from which $G_t$ is attainable must satisfy $I_t$.\n  \n   Indeed, It seems to me that this property is necessary to interleave skills. As an example, say you want to achieve $t = a \\textbf{ then } b$. In the course of achieving $G_a = u \\land v$, the planner also achieves some prerequisites $p \\land q$ to satisfying $G_b = p \\land q \\land r$. However, this means that $I_b$ needs to be invariant to whether $p$ and $q$ have been satisfied: if $I_b$ is only true when $\\neg p$ or $\\neg q$ hold, then achieving $G_a$ (and $p \\land q$ along the way) is going to make it impossible to initiate the skill for $b$.\n  \n  If we extend this argument, then it seems that $I_b$ needs to be true for _any_ state along _any_ trajectory that achieves $G_b$, so that the agent can start executing $b$ midway through. But if this is the case, then why not go the whole way and explicitly define $I_b(s)$ as true iff $G_b$ is attainable from $s$?\n- **CL2:** Section 4.1: \"For planning, we use the built-in deep-Q-learning algorithm.\" The original maxent IRL does (tabular, exact) soft-value iteration in order to plan. Why is deep Q-learning necessary for these tasks? How does the planner implement the entropy penalty requireed by maxent IRL?\n- **CL3:** Section 4.1: \"For inverse planning, we rank all candidates by the consistency between the observation and the task-conditioned policy.\" Why use the policy instead of directly using the return to compare trajectories, given that $p(\\overline s, \\overline a) \\propto \\exp R(\\overline s, \\overline a)$ in the (deterministic) maximum entropy model?\n- **CL4:** How does the BC-FSM baseline incorporate an FSM? This was not clear to me from the description in Section 4.1.\n- **CL5:** Do the experiments use NLM feature extractors for all baselines, or just for RatSkills? (I think the answer is \"yes\", just want to double-check.)\n- **CL6:** Section A.2: I was quite confused by this description of FSM-A\\*, since it does not seem to resemble the description in the appendix. What does it mean to \"prune\" in A\\*? Why is this pruning only applied after the first $b$ layers? Psuedocode may be useful here.\n\nMinor issues and suggestions:\n\n- **M1:** When I saw \"abstract language descriptions\" in the abstract, I assumed the paper was going to focus on _natural_ language. It wasn't until I got to page 4 that I understood that it was referring to a structured, logical language. This could be clarified by referring to rephrasing slightly, such as by calling them \"abstract logical descriptions\", or by making the given example look more formal, which is what I tried to do above by writing, e.g., $\\text{turn-on-music}$ instead of \"turn on music\" (the current underlining in the abstract helps but was not sufficient for me to intuit the structure of the language).\n- **M2:** In Section 3.3, the description of A\\* refers to the priority used to pop nodes from the priority queue as a \"heuristic function\" (\"Each node is associated with a heuristic value which adds up the total cost of the agent reaching this state and an estimated cost-to-go\"). This is a bit weird; in the planning community, a \"heuristic\" is generally something that estimates cost-to-go. I'd recommend stealing terms from Artificial Intelligence: A Modern Approach, which calls the combined function an \"evaluation function\" $f(n) = g(n) + h(n)$, which is  the sum of the \"path cost\" $g(n)$ and the \"heuristic value\" $h(n)$.\n- **M3:** While reading the \"task language\" section, I assumed \"primitive skills\" were the same thing as \"primitive actions\". It seems like this isn't the case; atomic skills in the task language exist at a strictly more abstract level than primitive actions from the MDP. I recommend exclusively referring to these as \"atomic skills\" or similar, and reserving \"primitive\" only for actions in the base-level MDP (which I think is typical nomenclature in hierarchial RL).\n- **M4:** Section 4.2: the appendix notes that the Crafting World environment has been modified relative to that used in past work. The fact that the experiments use a modified environment should be mentioned in the body of the paper, and the changes should be explicitly justified in the appendix.\n- **M5:** Appendix A.2: the sub-section which discusses optimality of A\\* search should also note that the heuristic must be admissible in order to guarantee optimal solutions (this will typically not be the case for learned heuristics).",
            "summary_of_the_review": "This paper proposes a novel and interesting approach to skill learning. Although I'm convinced of the significance of this work, I was confused by some important details, particularly regarding the experiments. I would like to have these clarified so that I can increase my score (and confidence). I also think that some of the wording could be slightly rejigged for clarity, although this was not as big a factor in my rating.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper looks at the problem of learning skills from annotated traces, which can then be used for planning and task identification. In particular, the paper looks at plan traces annotated with a task specification, which is provided in a task language introduced in the paper that the authors argue to be a subset of LTL. The language consists of strings that are recursively constructed from a set of atomic skills and some connectives. Each atomic skill is defined in terms of an initiation state set and termination state set, each operationalized by using classifiers for each set. They use a contrastive learning method to learn the classifiers for the initiation and termination set and discuss how the learned skills could be used for planning and task identification. The planning makes use of an augmented state space, where each task state is augmented with finite state machine states that captures the progress of the task execution. The planning and inverse planning performance of the system is evaluated on two tasks and compared against an IRL-based and a behavior cloning-based baseline. They additionally consider an LSTM based classification method for task identification. The initial results seem to be positive and show their method outperforming the baselines.\n",
            "main_review": "I think the paper looks at an interesting and important problem. Being able to take input from people in a natural way is an important capability for autonomous agents. Also as pointed out by the paper itself, procedural control knowledge of the form is particularly useful for people to specify task knowledge. I also appreciate the authors looking up and citing relevant works from larger AI literature and not just related to Deep RL works. It's also great that the authors try to formalize the language of the space of instructions that will be accepted by the method. With that said, unfortunately, I don’t believe that the paper is quite ready to be accepted for publication yet. My concerns with the paper include a lack of clarity with many technical discussions, missing details, vague and sometimes incorrect claims about the method. All of these factors make it difficult to correctly gauge the technical validity of the approach proposed here. Many of the choices made in terms of techniques or methods used to model and solve the problem aren’t provided any justification and this makes it difficult to understand the work as some of the choices made in the paper run counter to how similar problems are handled in wider literature (for example how you incorporate Finite State Machine transition into state dynamics). Below I have provided a more detailed discussion of my concerns regarding the paper.\n\nTranslation to Finite State Machines: First off, the algorithm for generating FSM for a given task specification seems to be missing from the appendix. This is a central part of the paper, as most of the later methods revolve around the choices being made at the level of the finite state machines. Also from a reader’s perspective, the algorithm and related proof of the soundness of construction is important information to have because in general LTL cannot be specified as a Finite State Machines. In the most general case, they require a Buchi automaton, which allows for the acceptance of infinite strings. You can capture finite LTL fragments using FSMs and I assume that’s what’s happening here. I believe the authors must be making an unspecified assumption that the language has some specified upper limit to its task sequences. I didn’t see this mentioned anywhere, nor did it strike me as being true from reading the language description. Additionally, there is an implicit assumption (at least I didn’t see it being explicitly stated anywhere) that each state in the FSM corresponds to an atomic task. Most of the LTL_f translation methods I am familiar with, convert the specification into a non-deterministic finite machine, which is then converted into a deterministic (usually through power-set construction). Just from the few examples provided, I can’t tell that arbitrarily complex task specifications (particularly since language definition allows for recursion) allowed by the task language definition can be represented by an FSM where each FSM state exactly corresponds to an atomic task. I feel without the algorithm and a corresponding proof of correctness, the paper would be incomplete. \n\nEncoding Finite State Machine: Which brings me to the next point of confusion I had with the paper. Generally, most of the works that augment states of a planning task, with an FSM state do so by encoding the transitions of the FSM into the transition dynamics of the planning problem. This would avoid issues that could happen when the agent just chooses to make some transition even when the transition isn’t warranted. I see that the authors try to add a stop-gap to this problem by including costs to transitions that don't correspond to that skill. But I am confused by the fact why the authors chose to use this formulation, not to mention I can still create pathological scenarios where the agent would force a transition if the probability of the classifiers are just small and not zero (I would be surprised if the learned NLM based classifier assigns zero probability to any of the states). Not only is this a less elegant formulation, but to the best of my understanding, it is this choice that further necessitates the need to use a dynamic programming method in both inverse planning and learning components to figure out what the optimal assignment of FSM states was. Maybe the authors did all this to avoid introducing stochastic transitions (which might come up because of the use of probabilistic classifiers), but I don’t see that as being a major drawback. You could have easily used a probabilistic planner or an MDP solver. I currently don’t think the paper is gaining much from focusing on A* search. \n\nInverse Planning: In general, the context in which the term inverse planning is generally used involves the agent recognizing the intention and the goal of another agent (usually a human). This is extremely helpful when you are either planning to assist the agent or in adversarial cases, you are trying to hinder them. In this particular case, it seems the goal is to identify the task after the fact, which I am not sure counts as the whole of inverse planning and I am also not sure it's as useful as being able to identify the agent’s intentions. At the very least, I would recommend clarifying the distinction between what is achieved by the method and the more general case (better yet, use a different term for it). Also, it would be useful to provide some discussion on the utility of such task identification.\n\nInitiation Set for Task and Consecutive Tasks: Is there a reason why the subtasks are characterized by both an initiation set and termination set and not just a subgoal. I understand that this aligns with the traditional view of options, but in this case, is there any advantage one gets from requiring that the transition point corresponds to both the subgoals for the task and the initiation set for the next task. Particularly for fully connected state-space, the initiation set could potentially be extremely large and it would be hard to learn an accurate classifier for that set and which could have negative effects on the planning part as it now introduces potential costs due to inaccurate classifiers. Also at least in the case of A* search, since it's a systematic search, there is no concern of the search getting stuck in a dead-end state.\n\nA* Search and Scalability of the Method: Which brings me to the search part, in particular to the A* search. Multiple parts in the text say that the method could use a learned heuristic and that the search can still identify the optimal solution with some changes to the algorithm. This can’t be true unless the heuristic is admissible. I didn’t see any discussion on how the heuristic was learned and how it was guaranteed to be admissible. There is a note on the evaluation that says “planning in the high-level is currently done with a blind search”, does this mean there is no learned search heuristic? This makes me quite worried about the scalability of the method, as I see no clear way to scale up the methods without learning useful heuristics, which in itself is an open research problem. And without assuming more structure to the problem, I don’t see how you can get more heuristic information. Also, Figure 4 says the max plan length is 4 steps, was this the same throughout all the experiments, these plan lengths are too small to provide any useful evaluation information.\n\nSmaller Comments:\n\na. One related work that might be worth looking at and also connecting, are the works that use the declarative model to guide the low-level RL agent (examples include [1], [2])\n\nb. It wasn’t clear to me how the discussion about being able to execute multiple tasks at the same time connects with the view of having a state associated with a single FSM state variable corresponding to a task. It might be helpful to provide more information in this regard\n\nc. The composition versus novel task split used in the evaluation, are these only meaningful categories for the baselines? Because after all for the proposed method, as long as the method has learned classifiers for all the atomic tasks, it can be applied to a novel task specification (consisting of a specific formula defined over the tasks are presented). If this is the case, you should clarify it in the evaluation section.\n\n[1] Lyu, Daoming, et al. \"SDRL: interpretable and data-efficient deep reinforcement learning leveraging symbolic planning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. No. 01. 2019.\n\n[2] Yang, Fangkai, et al. \"Peorl: Integrating symbolic planning and hierarchical reinforcement learning for robust decision-making.\" arXiv preprint arXiv:1804.07779 (2018).\n",
            "summary_of_the_review": "While the paper looks at an important and interesting problem, I don't believe it is quite ready to be published yet. Currently, the paper has a lot of missing information, which makes it hard to correctly gauge the correctness of the method. This includes missing algorithms and proofs, unspecified assumptions, and technical choices that are not properly motivated or explained. There are also some statements, particularly in regards to A* search that seems at the surface to be wrong. I also have some concerns about the scalability of the method.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper is about a representation of high-level skills for sequential decision making contexts (in this particular paper: planning-based based on full models, instead of reinforcement learning). The idea is to take a \"next\"-based representation in which high-level skills can be put in order in a task. Each skill is characterized by (learned) functions predicting whether a state either supports the initialisation or the termination of a skill, and tasks are accompanied by simple automate over skills. Several variations of planning (A*/IDA variants) and methods for learning the two mentioned functions are covered. Experiments in two domains are performed against related baselines.",
            "main_review": "The fields of planning, robotics, reinforcement learning etc have a huge history when it comes to abstractions over action sequences (skills, options, high-level actions, abstract actions, etc). This paper is positioned somewhere in this field, and uses a simple automata-based representation of task in terms of skills (with a \"before\" ordering) and two functions for precondition and termination checking. Learning them is based on data that consists of sequences of actions and the high-level task structure. The paper defines several tasks (planning with skills, planning with one goal condition, and inverse planning). I do not think that there is much technically wrong with the paper, although some things are unclear (see below), and I also think the experiments show viability results, but my main issue with the paper is novelty/significance of the approach compared to the vast literature on actions. Basically I do not see a clear contribution compared to the literature, but I do see a (in essence fairly useful) mix of existing methods that seems to work well for these problems (although the experiments raise some questions, see below). Overall I have several questions which need to be answered too. In addition, the paper is written fairly well, although it contains some small things like typos. I will elaborate (and mix that with questions I have).\n\n- First of all: why call it \"rational\" skills? First of all, \"RAT\"skills may not sound nice. But more importantly, the rational here is relatively meaningless. I think what is meant here, is \"reasonable\", or \"relatively fault-free\", or \"believable\". Throughout the paper rational means that, for example, the data contains trajectories that have not been generated randomly, but are close to regular or optimal trajectories. Is that right? If right, I would more clearly talk about these other aspects.\n- The introduction is ok, but why not frame it in existing formalisms such as \"options\", which are well-known in reinforcement learning, and come with aspects such as states where the option (skill) is applicable/terminating. Many papers have built on this, and hierarchical planning methods exist too.\n- The related work description contains many papers, but the positioning of the paper is weak. Modular skill models are described, and they contain things like LTL, but this paper is much simpler than the usual setting there. The paper only uses a simple automaton with a temporal structure (next-based) for tasks to endow the current (real) state with the current skill being executed. Hierarchical RL (and planning), which is also used in robotics, would have to get much more focus and description than now; there is much more related literature in this field. Even more so since several HRL techniques learn skills and decompositions while learning, for example by finding \"bottleneck states\", \"subgoal discovery\", or milestones, which can be used for delineation of tasks from data. Furthermore, I think that there is also lots of related work in the area of \"activity recognition\", since what the paper does (segmenting an action sequence, and putting labels on segments) is technically the same. Inverse planning is described briefly, but activity and behavior recognition would be relevant too. Also, I think that work in the general field of Hidden Markov Models for such tasks is relevant. Basically the paper utilizes similar methods for segmenting the task and learning predictors for precondition/termination, just like HMMs can use neural networks to induce latent labels. Finally, I would also expect maybe algorithms like dynamic time warping (DTW) to be mentioned since using dynamic programming for this alignment is known. I am not saying everything needs to be mentioned in this form, but I do think the positioning especially wrt hierarchical reinforcement learning and planning is not well done.\n- Why focus on deterministic processes (Section 3.1 first line) while many other, related methods, in this field cover stochastic environments? Where does this limitation come from exactly?\n- Neural Logic Machine are mentioned, but never explained in the paper. In fact, only some words reveal that (some form of) neural networks are used for learning, but the details of how, and why, are completly undescribed.\n- There is some abuse of notation on page 3 when going from actions to sequences of actions, which is ok, but can be better described.\n- I like the idea of models vs learning policies (last sentence of page 3) but I am not sure that for all environments full plans can be generated, especially when things are computed online, or if there is uncertainty at play.\n- There is some confusion about the execution of skills. The text says things about \"interleaving\", but the formalism seems to enforce strict orders (and completion of skills), for example in Figure 2, but also later (page 4) \"indicate THE skill\" and \"executing THE current skill\". If you still insist that skills can be run in parallel, please specify how, and also how many can run in parallel.\n- How is lambda chosen? In addition, the motivation behind the definition of C' sounds ok, but the technical implementation needs more description for me. In the definition, do you mean/assume that v and v' are not equal?\n- The sentence starting with \"The finite state...\" occurs twice(!)\n- The first two tasks on page 5/6 sound like standard planning problems, which is interesting in general, but if there is a contribution here, please specify.\n- The Bayesian inverse planning is not well motivated or explained. It looks like standard approaches ranging from related inverse planning, to HMM-like methods, to DTW, and so on. If there is a contribtion, please specify.\n- The text mentions that Fig 3 shows the value function, but I have not idea what it means here or what I can see in it.\n- What is the \"score\" in the rationality definition? And how does this definition relate to standard Boltzmann/softmax action selection and so on?\n- The section on learning the ratskills needs much more explanation on how this objective is actually learned in the neural logic machines, and also why it makes sense to do it exactly like this. The text assumes the reader knows (\"since\") that the functions are differentiable, but this is nowhere introduced yet. Some things like \"running back-propagation through the inverse planning\" do not make sense to me.\n- For the experimental section, some baselines are introduced, but it is hard to find out that this is the actual state-of-the-art in this domain, or for these problems. This is made worse by the weak positioning of the work.\n- The results in the tables seem to be very favourable for the method that is described, but even for the planning tasks the competing methods perform very weak, which seems strange. I have no idea whether this makes sense (Table 1).\n- I think it is fairly confusing how the \"compositional\" and \"novel\" tasks are described and what they actually mean. \n- I think one of the main results could be the sentence on page 8 \"learning goals from demonstrations is more sample-efficient than learning policies\". This sounds intuitive, and could be true. However, I do think it is also related to the deterministic nature of the problems considered.\n- The result for the planning problems sound ok, but I have difficulties finding out the relation to the SOTA, but the final experiments on inverse planning are inconclusive to me. Overall, the experiments are not intuitively structured (for me at least). I think more targeted experimentation, with more clear goals, would be helpful. Now all settings, including learning, and the primitive, compositional, and novel tasks are all somewhat mixed up.",
            "summary_of_the_review": "A mix of methods in the action domain, mixing some high-level skill representation, deterministic planning, and some skill learning with behavioral segmentation from data, but it is unclear where the paper is positioned in a huge field of action-related papers (affecting contributions, novelty and significance), and the experiments are somewhat inconclusive. Technically, I think the paper is mostly sound, fairly well written. In addition, there are several things that are not fully described yet.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}