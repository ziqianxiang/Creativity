{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the privacy-preserving mechanism achieved by structured noise injection. The contributions include (1) identifying the privacy guarantees of structured noise, (2) designing noise for a given privacy guarantee, and (3) discussing applications in a control problem.",
            "main_review": "Strength:\n- The paper is well-structured and studies an important setting for data privacy\n\n\nWeakness\n- The related work section omits many existing works that utilize similar ideas, like [1] and [2]. It'll be great to discuss the technical differences from these works.\n[1]: https://people.cs.umass.edu/~mcgregor/papers/15-vldbj.pdf \n[2]: https://arxiv.org/abs/1801.00823 \n\n\n\n",
            "summary_of_the_review": "Despite the weakness, overall, the paper presents novel differential privacy mechanism to achieve enhanced utility for protecting correlated data so I would recommend to accept the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper develops mechanisms for achieving differential privacy, when data are correlated in an affine way. An application to cloud-based control and numerical experiments are also presented.",
            "main_review": "The paper discusses mechanisms to achieve differential privacy of data satisfying affine constraints. The motivation is explained both with an example and an application to a control problem. It provides a new notion of adjacency for the affine-correlated data and uses this to create to mechanisms for achieving diff. privacy (once with gaussian and once with laplacian noise). Numerical experiments also show that structured noise injection behaves better than random noises. \n\nI am not very familiar with the literature on differential privacy, thus I cannot provide a confident evaluation of the relation with related work and technical strength. Below are a few questions and critisism:\n\ni) The term manifold-dependency in the title and the main paper seems a bit bold to me, I started reviewing thinking that this work has to do with Riemannian geometry. Since this is not the case and the data constraints are just affine, I would suggest to change the terminology to something like \"differential privacy under affine constraints\".\n\nii) As far as I understand, Example 1(ii) provides some worse case differential privacy levels for when data are affine-correlated. Can you prove that these levels are tight, i.e. someone cannot further improve on them?\n\niii) I struggle to understand the intuition behind the third line of equation (5) in the definition of adjacency, could you please provide more details on how this is related with the technicalities in the proof of privacy of your derived mechanisms?\n\niv) How do you practically compute $\\sigma$ in equations (9) and (19)?\n\nv) Second par. of related work, third line, seminar $\\rightarrow$ seminal",
            "summary_of_the_review": "The paper builds on existent work on differential privacy and takes a reasonable step further to investigate mechanisms for when the data are dependent in a simple way. The motivation is well explained and numerics seem to be in favour of the presented methods. Thus I recommend acceptance, although I am not really familiar with the relevant literature to provide a confident review.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper tries to adapt differential privacy concepts to data correlated through a manifold dependency. However, their concept of privacy relates more to statistical inference than an actual privacy guarantee and fundamentally misunderstand the distinction between personal and private information. ",
            "main_review": "This paper, like a number of papers on the 'problems' with correlations and differential privacy is flawed. The paper does not do a good job of clarifying exactly what the problem is, and in what sense differential privacy, or the specific neighbourhood definition used, creates it.\n\nI recall one interpretation of differential privacy:\n\"Regardless of external knowledge, an adversary with access to the sanitized database draws the same conclusions whether or not my data is included in the original database\"\nSo, to take the extreme example where everybody's data is exactly the same. Then of course, knowing the data of one person in the database reveals everybody's data. More generally, if there are strong correlations, then knowing everybody else's data gives some information about you. This is not related to differential privacy. Even if your own data is encrypted or not even included in the actual database, somebody can still learn a lot about you. So what is the problem?\n\nMcSherry has a nice blog post where he talks about this. The one-line takeaway is \"However, differential privacy's guarantees only mask the presence of those records received from each user, they do not mask larger statistical trends that may reveal information about each user. \"\n\nhttps://github.com/frankmcsherry/blog/blob/master/posts/2016-08-29.md\n\nI think it is not reasonable to say that manifold (or other correlations) are a problem for differential privacy. You should instead change the definition of what you want to keep private. It is perhaps interesting to consider a neighbourhood definition in a manifold, which is what this paper is doing (as the authors helpfully clarified), but since the standard definition of neighbourhood where x N x' if x has one more/less datapoint than x' also suffices for this setting. (The definition where x differ only in one point may not be applicable at all in a manifold setting, as no such neighbouring points may exist).\n\nIf you want to go in that direction, then there is a nice set of papers  which define a different notion of privacy, precisely for the case where you need strong statistical inference guarantees. See e.g. \n- Composition properties of inferential privacy for time-series data\n- Pufferfish: A framework for mathematical privacy definitions\n -Pufferfish privacy mechanisms for correlated data\n\nIn summary, the main example is not sufficiently rigorous, and the paper's main claims are misleading. The authors do not explain why the standard more/less records definition does not apply here, and what a new definition offers. If the paper is rewritten appropriately, it might be a worthwhile contribution. You must:\n\n(a) specify what you are trying to protect\n(b) show why one of the two standard neighbourhood definitions doesn't protect it\n(c) show that your definition does.\n\nThanks for the discussion",
            "summary_of_the_review": "This paper is misleading and insufficiently motivated.",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}