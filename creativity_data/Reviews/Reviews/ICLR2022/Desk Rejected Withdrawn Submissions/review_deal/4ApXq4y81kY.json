{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposed a new sample selection method for robust deep learning against label noise. The method is based on co-teaching framework with two cooperated deep neural networks, while selecting samples by considering both the classification loss and prediction inconsistency between the two networks.",
            "main_review": "The motivation of the proposed sample selection strategy is reasonable, and the experimental results seem promising.\n\nMy major concerns lie in the following aspects:\n\nFirst, the novelty is not significant enough, since it seems that the sample selection strategy is a straight forward combination of two previous ones, namely, selecting samples with small losses and prediction inconsistency. Such a combination provides no new insights about robust deep learning to the community.\n\nSecond, the newly proposed loss seems not be well defined. Specifically, since $\\alpha<0$ while $\\mathcal{L}_{\\rm{CONTRAST}}\\geq0$, the overall loss might not be bounded below, which is not a preferable property for defining a loss function. The author should justify this.\n\nThird, the experiments are also not sufficient. Specifically, the authors only provided experiments on datasets with relatively few classes, i.e., no more than 20 classes, while datasets with more classes, such as CIFAR-100 with 100 classes and TinyImageNet with 200 classes, are better to be included for comprehensiveness. ",
            "summary_of_the_review": "The motivation of this paper is reasonable, but the overall novelty is not significant enough for this conference",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Training DNN with class noisy samples is still a great challenge. This submission describes a technique to select part of samples to train the model. The selected samples are considered clean and easy to classify. The authors propose a joint loss function which considers both Classification loss and contrastive loss, and use the selected samples to train DNN. Extensive experimental results show the effectiveness of the proposed model.",
            "main_review": "Strength: 1. the authors propose a joint loss function for evaluating candidate samples and considers both Classification loss and contrastive loss, and use the selected samples to train DNN; 2. extensive experiments show the effectiveness of the proposed algorithm.\n\nweakness: 1. the idea of the work is simple; 2. More theoretical analysis is required to discuss why the work is effective.",
            "summary_of_the_review": "This submission describes a technique to select part of samples to train the model. The selected samples are considered clean and easy to classify. The authors propose a joint loss function which considers both Classification loss and contrastive loss, and use the selected samples to train DNN. Extensive experimental results show the effectiveness of the proposed model. Generally speaking, this work is easy to understand  and be reproduced. However, more theoretical analysis is required to discuss why the work is effective, and there is margin to improve the quality of writing.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a sample selection method to handle noisy labels . The idea is to encourage the examples with different prediction probabilities from two networks to participate in optimization. The authors first explain the necessity of keep the divergence of two networks during sample selection. However, the existing method is rather sample-inefficient. To address the issue, this paper relaxes the “disagreement” condition to the “high-variance” condition for sample selection. After this, more clean training examples can be involved into training, following better performance. Besides, this paper shows that the proposed method can work well, especially on the unbalanced noisy datasets. ",
            "main_review": "\nStrengths\n-\tThe motivation is very clear. It is of significance to keep the diversity of networks in the co-learning paradigm. However, the method Co-teaching+ is sample-inefficient, which impairs generalization. This paper proposes a more sample-efficient method. \n-\tThe idea is simple but effective. It is not very new to emphasizes high-variance examples during training, which is also mentioned in this paper. However, in learning with noisy labels, this idea is contributing. It can conveniently select clean hard examples, while keeping the network diversity. Expect for the small-loss trick, the idea can also be applied to methods that use other modes for sample selection. \n-\tExperiments are comprehensive. The experiments on a series of tasks are presented. On the unbalanced noisy datasets and Clothing1M, the proposed method outperforms baselines clearly. The discussions on results are also provided. \n-\tGenerally, the writing and organization are great. It is easy to follow the technical details. \n\nWeaknesses and Suggestions\n-\tTo achieve a good trade-off of losses and high variances, this paper needs to introduce a hyperparameter.\n-\tThis paper does not give enough empirical valuations for the motivation. For example, this paper needs some empirical evaluations to show that the proposed method is more sample-efficient than Co-teaching+. \n-\tAlthough the proposed method can be applied to different network structures, the authors only provided the results with a 9-layer CNN. The network structures may influence the extraction of hard examples. Therefore, the authors should exploit popularly used structures such as Resnet or Wide-Resnet to present more convincing results. \n-\tMinor comments:\n(a)\tIn Section 1, the paragraph that describes Co-variance is a bit long, which reduces the legibility of this paper. \n(b)\tFor the generation of two types of long-tailed datasets, this paper only gives two illustrations. The experiments on long-tailed datasets are important to affirm the contribution of this paper. Therefore, more details should be added. \n(c)\tPerhaps, due to the limited pages, the tables and figures are dense to me. As the experiments on long-tailed datasets are more important for this paper, the authors can use more pages for them. \n",
            "summary_of_the_review": "The paper shows good performance and claims the approach can be used to mine clean hard examples. The convenience and effectiveness are the strengths of this paper. As mentioned, more explanations and experiments can enhance this paper. I like this paper and think it can contribute to the community. Therefore, before rebuttal, I recommend accepting this paper. The score is “6: marginally above the acceptance threshold. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}