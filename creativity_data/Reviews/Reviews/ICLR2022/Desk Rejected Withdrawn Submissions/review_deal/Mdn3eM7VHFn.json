{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes an approach to learn grounded text representation by understanding the image at object level instead of feature map level. The paper employs off-the-shelf object detection features for visual representation to improve the alignment between Visual and Textual modalities. The paper presents its results on various language tasks of GLUE and SQuAD datasets and compares its results to the existing transformer and cross-modal transformer architectures.",
            "main_review": "Strengths:\n-- The paper proposes to learn object-level grounding language which uses objects and their positions to learn the relationships among different entities  unlike previous approaches that looked at the whole image.\n\n-- Paper presents a systematic approach to learn the visually grounded language representation using transformer architectures for visual, textual and cross-modal representations.\n\n-- Proposed approach performs better on various downstream language tasks of GLUE and SQuAD dataset when compared to other existing approaches such as Vokenization (  Tan et al) and other BERT variants.\n\nWeaknesses:\n\n-- The novelty of the paper is limited to employing object detection features as opposed to image-level features. While this does lead to improvements in performance, there is no analysis done for the compute cost of the approach since object detection approaches have heavier CNN backbones as a tradeoff for higher performance.\n\n-- The paper's ablation studies are lacking analysis of the performance of each of the individual contributions. For example, what are the gains when standard feature maps are used instead of object detection features or what happens when standard embeddings are used instead of transformers..\n\n-- The paper doesnot emphasize enough on the novelty of the architecture.. For example, was cross-modal transformer employed before to align visual and textual modalities? If yes, was it used in similar form?\n\n-- The writing of the paper should be revised significantly by someone proficient in english. There are multiple places where it is difficult to parse the language\n       == section 1 youngsters -> children?\n       == the constraint -> constraints?\n       == apprehend -> comprehend?\n      == trigger many noises -> learn noisy representations?\n      == significantly outperforms -> significantly outperform\n      == Section 3.1; objects receives -> objects received; VE stand -> VE stands\n      == Section 4.3 ; language base model is freeze -> language base model is frozen\n      and so on",
            "summary_of_the_review": "Overall, while the paper provides a stronger baseline compared to the existing methods that train on multi-modal grounded data; the gains are incremental and cannot justify the lack of novelty in the paper. The paper also needs a re-write improving the language to remove confusion and also, to state its contributions/novelty if any. Owing to these reasons, I am recommending a weak-reject for the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors study the problem of learning better language representation by grounding text segments onto visual regions. The claimed contribution is to first adopt region-based image representation in the task of learning language representation with visual information. The method is benchmarked on multiple NLP tasks and datasets and shows SOTA performance.",
            "main_review": "Strengths: \n1. The problem of language representation learning with visual information is important. The study explores using region features to help language representation learning and achieves SOTA performance on multiple datasets.\n\n2. The paper contains comprehensive experiments on multiple datasets and complete ablation studies on the core model components.\n\nWeaknesses: \n1. One major concern is the lack of technical contribution. Specifically, the study seems very similar to the vision-language pretraining studies (e.g., Vilbert), from feature encoding, architecture to training objectives. The study contains minor modifications for the language representation learning problem. Therefore, the study looks like an analyses paper on how do existing VLP models perform on NLP tasks. However, if so, additional analyses might be necessary, as in the second point.\n\n2. The two motivations for using the region-based features in the abstract are intuitive. However, those high-level motivations are not well supported by experiments and analyses. It would make the study stronger by showing how the region-based framework is better than previous studies  with whole images, and whether the improvement is relevant to the discussed “undescribed object” and “object relationships.”\n\n3. Method. One major technical improvement is the “grounding embedding” in the object grounded BERT. However, it seems that the grounding embedding is projected with an additional set of learnable parameters (Eq 5) with no grounding-related design or training objectives? As mentioned in the first point, I fail to find many methods designed for the “language representation learning with visual information” problem.\n\n4. An open suggestion is that it might make the work stronger and more comprehensive by incorporating previous studies on the same problem. For example, the contrastive training objective to enforce the cross-modal grounding. Such supervised grounding objective can also be applied at the region-text level, and could make the study on using region features for language representation learning more comprehensive.\n",
            "summary_of_the_review": "The problem of language representation learning with visual information is important, and using the region/grid feature instead of the whole image seems to be the correct direction. However, I think the technical contribution and results analyses can be further improved.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes the OGBERT - a transformer-based vison-and-language pre-training framework including a visual encoder, a text encoder, d a cross modal transformer to learn the alignment and representation of image-text context. Experiments are conducted on downstream language understanding tasks.   ",
            "main_review": "Strengths\n\nThe experiments are conducted on common benchmark datasets.\n\n\nWeaknesses\n\nMissing important related work, including recent work on vision-and-language pre-trained transformers. The introduction only emphasizes the difference between the proposed method and CNN-based methods; however, the difference between the proposed method and existing vision-and-language pre-trained transformers is missing. Also, the related work only includes some pre-training work and strong baselines are missing. \n\nWeak experiments results. The paper does not compare with recent strong baselines and only reports the prior weak baselines. The recent strong baselines (e.g., UNIMO, RoBERTa, XLNet) outperform the proposed method significantly. \n\nThe novelty is also somewhat limited. There is no essential improvement, even difference, for existing vision-and-language pretraining. The motivation is also not clear.\n\nThe paper is poorly written. For example, the font sizes of different paragraphs are different on page 8.",
            "summary_of_the_review": "The paper has a number of significant shortcomings and is not ready for publication.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}