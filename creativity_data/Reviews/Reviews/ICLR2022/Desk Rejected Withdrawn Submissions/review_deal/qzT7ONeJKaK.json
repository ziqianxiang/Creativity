{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors proposed an effective and confident data-free black-box attack, CODFE, which steals the target model by queries of synthetically generated data. The proposed framework contains a substitute model which imitates the target model and  a generator which generates most representative data to maximize the confidence of the substitute model. The authors showed the theoretical convergence of the proposed model stealing optimization and empirically evaluated its success rate on various datasets. ",
            "main_review": "1 . The model stealing part of the proposed method is kind of unintuitive to me. Different from (Truong et al., 2021), which proposed a min-max formulation for the same model extraction task, the proposed framework minimizes the Ls and Lg independently without making the objective adversarial. My concern is, if the generator is not updated in an adversarial fashion (i.e., update itself to generate even harder examples), would it actually learn a good generative power? It could easily lead to mode collapse or degenerated cases. The authors might want to further check for performance of the generator, also should compare the proposed model stealing part with the result of (Truong et al., 2021).\n\n2 . In the convergence result (Theorem 1), the authors make an assumption that seems to imply that the classification result of the student and the teacher is the same after each training iteration. If so, this assumption makes no sense. There is another assumption saying that the confidence of T’s output is higher after each iteration. Again, there lacks any justification or explanation towards the two assumptions made by the authors and they seem to be quite unreasonable to me.\n\n3 . The proposed method is essential combining the task of model stealing and the task of substitute model based black-box attacks for data-free black-box attack. However, the problem is, optimization-based black-box attacks such as ZOO [1], NES [2], FW [3], Bandit [4], Square [5] also do not require any training data and their attack success rate is nearly 100% with much fewer queries if I remember correctly. Therefore, I am not sure what is the advantages of the proposed method against those baselines. The authors might want to comment and compare with those optimization-based black-box attacks.\n\n[1] \"Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models.\" Proceedings of the 10th ACM workshop on artificial intelligence and security. 2017.\n[2] \"Black-box adversarial attacks with limited queries and information.\" International Conference on Machine Learning. PMLR, 2018.\n[3] \"A Frank-Wolfe framework for efficient and effective adversarial attacks.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020.\n[4] “Prior convictions: Black-box adversarial attacks with bandits and priors.\" arXiv preprint arXiv:1807.07978 (2018).\n[5] \"Square attack: a query-efficient black-box adversarial attack via random search.\" European Conference on Computer Vision. Springer, Cham, 2020.\n\n4 . Page 5, 1st paragraph citation typo.\n",
            "summary_of_the_review": "This paper considered data-free blackbox attack by first using model stealing to get a substitute model. However, the major issue is that compared to the optimization-based black-box attacks, this method may not be advantageous.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes a framework (CODFE), which steals target model into a substitute model by queries of synthetically generated data. They also present a training procedure that steers the synthesizing direction based on the confidence of substitute model and demonstrate the effectiveness of the method with experimental results. ",
            "main_review": "This paper presents a model stealing framework, which learns a substitute model through querying the target model via data generated synthetically. The generator and the substitute model work collaboratively, aiming to maximize the confidence of generating synthetic images by minimizing the cross entropy loss of the substitute model. \n\nI personally feel that \"the generator generates most representative data to maximize the confidence of substitute model\" is a little counter-intuitive given that we might want to synthesize new data, with whose labels queried from the target model will further close the gap between the target model and the substitute model, i.e. examples that lie in the region of disagreement between them.\n\nThe paper demonstrates with experimental results that the proposed framework can steal the target model using a low number of queries.",
            "summary_of_the_review": "The paper is well written with supporting experimental results. However, it is not fully convincing that the proposed framework is effectively applicable.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes an efficient confident data-free model stealing framework, which alternatively optimize a generator and a substitute model such that the generator can have high confidence on the substitute model, while the substitute model achieving similar performance as the target model on the generated data. Theortical analysis are provided for the convergence of the proposed model stealing process, and the substitute model can be used as an effective way to generate blackbox attacks.",
            "main_review": "## Strengths\n1. Novelty: The idea of training the generator to maximize the confidence of the generated example on the substitue model is novel and well motivated, potentially impactful to the field\n2. The theortical analysis on the convergence is thorough and correct. Though some of the assumptions are not emperically verified (see weakness)\n3. The proposed method shows higher model stealing accuracy, more efficient convergence and more effective transfer attack than baseline methods.\n\n## Weaknesses\n1. Reproductability: there's only limited information on the model architecture used for the experiments (in Sec. 4.1). Other than that there's no discussion on the model hyperparameter choices, training schemes and adversarial attack configurations etc. Also no reproduction statement nor codes are provided, leaving doubt to the reproductability of the proposed method.\n2. Some statements or assumptions are not well supported. \n(1) In the convergence analysis of Sec. 3.3, assumption is given that the confidence of the target model should be higher after generator training step. This assumption is not emperically verified, and may not hold if the substitute model is overfitting to the original generator's data, thus behaving differently than the target model after the generator updates. \n(2) Also in Sec. 3.3 for using the corss-entropy loss rather than the KL-divergence, the justification provided in Appendix A isn't clear. My understanding is that KL is not suitable for label-only scenario; yet for your analysis no matter which objective or setting you are using we can always assume T(x) as constant. So gradeint vanishing shouldn't be an issue. It would be better to provide ablation study results to verify your choices.\n3. The experiment results seems selectively reported and not well-organized. For example, in Sec 4.2 no model stealing accuracy on CIFAR-10 dataset is provided. Some important information for the experiment is also missing, such as (1) what is the strength of the adversarial attacks used? Attack performance under different strength should be reported for a more complete comparison. (2) What is the attack success rate on the substitute model? What's the percentage of a successful attack on substitute transfer successfully to the target model? (3) How would the performance change if the substitute model has a deeper/larger architecture? It doesn't make sense to limit the substitute model as a shallow model, unless there's scalability issue with the proposed method preventing the use of deep model. \n\n## Additional comments\nFor the purpose of generating blackbox adversarial attacks, we actually want to match the performance of the substitute and target model under adversarial attack, rather than just the performance on clean input. For the proposed training method the substitute model may be overfitting to the clean data, leading to a high model stealing accuracy but poor performance on transferring adversarial attack. Some discussion or future works along this line may be interesting.",
            "summary_of_the_review": "I have a mixed feeling of this paper. As I find the porposed method interesting, the emperical evaluation provided in the paper aren't enough to well support the claim, making it hard to decide the technical merit of the proposed method. The lack of reproduction information is also concerning. Thus I would suggest a reject for now, and would like to see if more complete evaluations can be provided by the author during the rebuttal period.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes CODFE, a new data-free black-box attack which steals the information of target model by queries of generated data. The model for the attack consists of a substitute model and a generator. The paper proposes a collaborative training algorithm for the model, which trains the substitute model and the generator alternately. The paper also proves the theoretical convergence of the training algorithm. Empirical results show that the proposed attack significantly outperforms the state-of-the-art data-free black-box attacks.",
            "main_review": "The paper proposes a novel data-free black-box algorithm. The data-free black-box attack is a class of realistic attacks, and the research for the attack is an important field for developing robust models. CODFE utilizes a collaborative training method as opposed to existing approaches using min-max game, and the algorithm is interesting. The paper is easy to follow.\n\nHowever, there are a lot of concerns about both theoretical and empirical analysis, and more thorough discussions are needed. Unless the following concerns are properly addressed, it is difficult to assign high score.\n\n--------Major comments for theoretical analysis--------\n* The assumption “after the training of $\\mathcal{G}$, the confidence of $\\mathcal{T}$’s output is higher” is ambiguous and not sufficient in several points. First, $\\mathcal{T}$’s output should be described more concretely. Second, the confidence may reach the upper bound and not be higher. For example, if the output of the target model is 1-hot, the confidence is always 1. Finally, even if the confidence is higher, the highest class $k$ could change because the confidence does not contain the information about the class. Thus, in the proof of Lemma 1 in Appendix B.1, the inequality in the line 3 of the proof may not be satisfied. Namely, the assumption is not sufficient to prove the Lemma.\n\n* The above assumption is strong and not realistic, even if the assumption is modified. This assumption means that the updated noise vector also increases the confidence on the target model, although the loss function for the training of the generator does not include the term for the target model. Thus, this assumption implies that the target model is already similar to the surrogate model. This is questionable, because the acquisition of the surrogate model that is similar to the target model is the “purpose” of the training.\n\n* Theorem 1 only holds when the number of noise vectors is 1. On the other hand, Algorithm 1 considers multiple noise vectors, and thus there is a large gap. This gap should be discussed well in the paper.\n\n--------Major comments for empirical results--------\n* The paper only compares the proposed method with a previous study. However, it seems that MAZE [1] is also a state-of-the-art data-free model stealing attack. Although the paper did not evaluate the method under adversarial attacks, the surrogate model achieved high accuracy and thus the method will work for black-box attacks. The author cited the paper. Why are there no comparisons with MAZE?\n\n* It is difficult to understand the relationship between the experimental results in the tables and sentences. For example, in Section 4.3, the authors stated that “ASR of CODFE significantly outperforms DaST from two to fifteen times high”. However, in Table 1, ASR of CODFE is sometimes similar to that of DaST. I do not think the statement is accurate. The sentences should be consistent with the table. Moreover, the numbers in the conclusion (e.g., 14%-56% higher accuracy) are not appeared in Section 4. These numbers should correspond to the results explicitly.\n\n* The discussion about label-only and probability only (in the first paragraph of the page 9) is not accurate. The authors stated that “higher attack success rates are achieved under the probability-only scenarios”, but it is not true on the MNIST. \n\n* Section 4.4 evaluates the query efficiency of the model stealing process to compare attacking efficiency, but the relationship between the efficiency of training and the efficiency of attacks is not trivial. Although it is expected that these efficiencies are positively correlated, the surrogate model with low accuracy might be useful for attacks. Therefore, it would be better to analyze the relationship explicitly.\n\n--------Other comments--------\n* In the paper, the definition of the subscript $k$ is ambiguous. For example, in the Equation (2), $k$ depends on the index $i$, but it is not mentioned. In addition, $k$ can be not unique. The definition should be given more precisely.\n\n* In the line 3 of the proof of Lemma 1, $\\mathcal{T}$ does not take $\\theta_s$ as inputs.\n\n* The line 2 of the proof of Lemma1 does not always hold. Some assumptions about the update (e.g., the learning rate is sufficiently small) are needed.\n\n* In the proof of Theorem 1, The sentence about $f(\\theta_s)=0$ is not correct when the output of the target model is classification probabilities. \n\n* There is no information about experimental settings such as hyperparameters, stopping criteria, and attack settings. The settings should be described for the reproducibility. \n\n* In Section 4.4, the authors mentioned that one of the reasons for the efficiency is the light-weight networks. This is a bit unfair, because DaST will also be able to utilize light-weight networks. \n\n--------Typos--------\n* Some citations such as Zhou et al. 2020 in the page 2 should be cited by \\citet option.\n* On the page 5, a citation is missed (next to Lis & Sethi, 2006).\n",
            "summary_of_the_review": "Although the topic of the paper is important and the proposed algorithm is interesting, the theoretical analysis is held under strong assumptions, and the empirical analysis is not sufficient (e.g., the lack of comparison with a recent SOTA method). \nMoreover, some statements seem not accurate.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "Although the paper proposes practical data-free adversarial attacks, the paper does not mention the ethical impacts.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}