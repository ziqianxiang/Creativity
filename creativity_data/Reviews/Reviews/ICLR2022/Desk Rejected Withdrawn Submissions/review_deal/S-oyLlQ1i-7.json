{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper tackles robustness problems from a new perspective by investigating the relationship between robustness and 3D vision. Toward this end, the work proposes to build a vision system upon 3D reconstruction pretraining models. By introducing a novel dataset Geon3D, which is derived from objects that emphasize variation across shape features, the authors conduct a series of experiments and analyses. Experimentally, the work finds that CNNs pre-trained on 3D reconstruction is more resilient to the viewpoint, rotation and shift variations than regular classification CNN.",
            "main_review": "\n-- strengths  \n1. The technique is solid and easy to understand and the writing is simply excellent. \n2. The authors create a novel dataset Geon3D, which is valuable for researchers to investigate synergies between robustness and 3D computer vision.\n\n-- Weaknesses\n\nI don't really have any major criticism, but I am curious about the following issues\n1. How is the 3D reconstruction performances of pre-trained and fine-tuned models? It would be interesting to see some analysis from the 3D reconstruction perspective.\n2. For the 3D reconstruction method, why choose the self-supervised method and not other supervised methods like OccNet, DeepSDF? Is there any reason for this?\n3. Have you tried to jointly train a model with classification and reconstruction loss? From my understanding, classification prefers “identity”-sensitive features, but not every detail on objects. While 3D reconstruction (with differentiable render) attempts to recover as much detail as possible, which might distract classification. \n",
            "summary_of_the_review": "The motivation is clear and the experiments are well designed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Neural network-based vision models trained on large datasets are prone to failure under natural and artificial corruptions. This paper hypothesizes that like humans, robust vision systems should be able to infer the 3D property of the environment; and hence it proposes to use the task of 3D reconstruction for pretraining the features of a neural network. The paper creates the Geon3D dataset for this purpose and conducts various experiments to verify this hypothesis.",
            "main_review": "Strengths:\n- I agree with the paper that the vision systems that can understand the 3D structure of the environment could likely be more robust to natural and adversarial corruptions.\n- I appreciate the efforts the paper puts into demonstrating how 3D vision could help build more robust vision models. I think this is a promising direction and should be explored more. \n- The paper is well written and mostly clear.\n\n\nWeakness:\n- Although I agree with the paper that the hypothesis could be insightful, I think the overall empirical evidence provided in the paper is relatively weak. Following, I have put together a list of things where I feel the experimental evidence is weak and how it might be addressed.\n\t- The findings in Figure 2 are expected and might not demonstrate that 3D reconstruction is a good pre-training task in general. This is because the downstream classification is directly based on the geometrical features, cross-section and arch of the shapes. As an ablation, the paper could consider running a pretraining task where the model predicts the intermediate parameters like cross-section and arch and not the final geon category. I expect such a method would work equally well as the proposed reconstruction task. If so, it would demonstrate the particular pretraining strategy is very much dependent on the eventual downstream task and might not help in making generally “robust” vision models.\n\t- It is unclear as to why the only DVR baseline (pretraining on 3D reconstruction) has not been included in Table 4 and Table 5. I assume it would strengthen (or weaken) the case made by the paper that 3D construction “alone” is a good pre-training task for robust models. If not so, the paper could consider discussing why it is not the case. \n\n\n- Missing discussion to related works. Two sets of related works that the paper could consider adding are:\n  - CLIP [1] - Clip shows one can achieve robustness to many different adversarial attacks by pretraining on a different task. Since the objective of the current work is about achieving robustness, I think it would be worthwhile for the paper to mention this alternate paradigm. \n  - Rel3D [2] - This shows how 3D information could be useful in different tasks for grounding spatial relations. It also shows how one can achieve robustness by using less biased data. Since the current work focuses on leveraging (pretrained) 3D information and aims to build robust models, it is worthwhile to discuss it.\n\n[1] CLIP: Connecting Text and Images, Arxiv\n\n[2] Rel3D: A Minimally Contrastive Benchmark for Grounding Spatial Relations in 3D, NeuRIPS 2020\n",
            "summary_of_the_review": "Overall, I think the paper explores an interesting direction. However, in my opinion, the experiments do not support or disprove the hypothesis. Hence, the overall contribution of the work is significantly reduced. Therefore, I am leaning towards a (weak) rejection of the current version.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new dataset, GEON3D, to study how to leverage 3D reconstruction as a pretrained task to improve robustness of image classification toward sources of common corruptions. Specifically, GEON3D model consists of a list of carefully created simple 3D shapes (i.e. geometric icon or Geon) that process nice properties including viewpoint invariance and distinctness. The paper proposes pertaining the image encoder with DVR, which takes latent code and turns it into a neural field that will be rendered back to the image. Several experiment results show that pretraining with DVR and image reconstruction losses improves robustness in GEON3D dataset.",
            "main_review": "### Strength\n\n**Vision of the paper** I really like the vision of this paper. I think the vision to find robustness of image recognition and classification in the 3D community is well motivated in the introduction. The authors draw inspiration from the psychology studies which also echo with many existing ongoing research of the vision field. I think if any paper is able to show some convincing results that 3D is either needed or not as needed in 2D image recognition, it will be a very important contribution to move the field forward. So this paper is definitely studying a very important topic.\n\n**Novelty** To the best of my knowledge, there doesn't exist a dataset of simple geometric primitives developed to evaluate network architecture's robustness toward different types of corruption when pretrained with 3D information. The authors have also made a convincing case distinguishing the proposed dataset from other datasets in the Related work section.\n\n**Some results are positive.** Experiment results testing on GEON dataset support the basic claim that pretraining with 3D reconstruction improve robustness toward pixel rotation and shifting and 3D pose change. \n\n### Weakness:\n\n**The experimental evidence might not be sufficient to support the main claims**\nThe main hypothesis of the paper is that 3D shape prior can improve robustness compared to 2D pretraining. To verify such a hypothesis, I think it might not be sufficient to conduct experiment training and testing both on the GEON3D dataset. Since all the experiments are done in GEON3D datasets (i.e. pretraining, training, and fine-tuning for testing), the claim these results seem to support is \"there exists a dataset with which pertaining with 3D reconstruction task provide better robustness toward 2D classification\". In order for the major claim of the paper to work while working with this dataset, I think it might be interesting to see how some existing pretraining methods (e.g. pretraining on ImageNet then finetune on this dataset) perform on robustness test of GEON3D. Alternatively, it would be very good if pretraining on GEON3D with 3D reconstruction-related tasks can improve robustness on other datasets (ideally real-world). In general, I think it would be better if the paper can show results how the proposed pertaining task in the proposed dataset help beyond the dataset itself (i.e. to real-world or to other classification benchmarks the community care about).\n\n**Motivation needed in the introduction and the DVR baseline** Related to the previous point, the second to the last paragraph of the introduction seems a little bit hard to follow - it might need some more words on how this novel dataset can help verify the main hypothesis of the paper, and why does \"These results suggest that the Geon3D dataset provides a controlled and effective measure of robustness\" a conclusion to the main hypothesis \"shape bias—learning representations that enable accurate inferences of 3D from 2D, which we refer to as “3D shape bias”—will induce robustness\". It's also not clear to me why DVR baseline is the right choice - why do the authors choose to focus on neural fields and differentiable rendering with neural fields instead of predicting other types of 3D shapes?\n\n**Missing Baseline in Section 3.5**  One potential baseline that can help illustrate the importance of the 3D pertaining task is to use 2D image reconstruction (i.e. AE on 2D image task) to learn the features.\n\n**Explanation of lack of improvement in corruption such as contrast** In Section 3.2, I think it will be great if the authors can explain a little bit more about why the 3D pretrained model is not performing well in certain types of corruption (e.g. fog, contrast, brightness) comparing to Stylized. It occurs to me that this will alter the claim to \"3D pretrained tasks can improve robustness on certain tasks but not the others\". \n\n\n",
            "summary_of_the_review": "While I like the vision of the paper and I think the paper is set up to answer a very important question the community has, I'm not very convinced that the paper currently provides sufficient experimental evidence to support the hypothesis. I think the paper can be viewed as a good first step toward answering whether 3D information can be used to help with 2D recognition tasks. I think the paper can be made more clear about what's the main hypothesis it's trying to verify and how the experiment set-up help to verify it. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes Geon3D Benchmark which is dataset comprised of simple shape primitives and variations. The dataset will facilitate study of part-level robustness and shape bias of a class of 3D reconstruction models, and is potentially useful for investigating how 3D shape bias influence the robustness of neural vision systems. ",
            "main_review": "Strengths:\nThe  grand vision of this paper  is to develop robust vision systems. 3D geometry perception is used as an example to illustrate the main argument.  It provides a new 3D dataset which contains objects characterized by a novel way, and it provides the ability of analysing if 3D shape bias is useful for improving robustness.\n\nExperiments under several state-of-art methods show that categories defined in Geon can be efficiently classified.\n\nResults using adversarial training show that 3D pretrained models can improve part-level robustness under rotation, translation and viewpoint change. \n\n\nWeaknesses:\n\nThe vision of this paper is commendable, and the approach it took is interesting, however the paper made numerous ungrounded claim or overstatements. All the experiments are conducted on the Geon-10 dataset which is a tiny and toy-ish.   Given the Geon dataset only contains very simple shape primitives  it is to early to jump to a conclusion claiming this shows 3D shape bias can improve robustness of network for real world objects. In fact the paper opens from a  noble and high level motivating statement: \"Building robust vision systems is a major open problem\" or \"Robustness research in machine vision faces a challenge\", yet what it has finally delivered is a set of rather primitive and premature Geon framework, dataset and  a small set of experiments. This is rather disappointing, and the paper fails to deliver what it promised in the introduction section. At least I find the experiement section is very weak, insufficient, and far from convincing.  \n\n\nThere are also many other ambitious claims, which fall short in providing supporting evidence in the main body of the text.  For instance:\n\n\"While aiming for higher accuracy on ImageNet-scale benchmarks is important, the current landscape\nof robustness research shows that we face a clear challenge. In fact, the consensus\nseems to be that large models and large training data work well for some distribution shifts, but\nnothing consistently help in all variants of ImageNet robustness benchmarks, awaiting methodological\ninnovation to achieve human-level robustness .\"   and e.g.  \" To unblock the situation, we\nadvocate closer collaboration between the robustness community and the 3D vision community, in the hope of fostering new types of robustness research.\"  \n\n\n\n",
            "summary_of_the_review": "The GEON3D benchmark dataset is a useful addition to the literature.   The conducted set of experiments are inspiring.  However, the paper fails to deliver what it has promised to deliver in the first two pages of the paper.  Many of the experimental designs are neither clear, not well motivated, for example, the adversarial training and the reinforcement learning both come out of the blue, without providing necessary definition or explanation.   I doubt many readers of the paper will find it very easy to follow baselines like \"AT-L1\" and \"DVR-Last\",  or \"DVR+AT-L2\"  and DVR+AT-Linfinity\".     The  paper will benefit significantly from a careful rewriting before it is ready to be considered again.  In particular,   it should be made clearer what the main argument that paper set out to answer.  The authors attempted to make a valuable baby step attempt to solving the grand problem of robust vision.  In its current shape the paper is far from ready for ICLR.     ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}