{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this work, the authors propose an automatic portrait video matting method by incorporating motion and context information. The main technical contribution comes from the context-motion updating operator that combines semantic features, motion features, and optical flow. The experiments on the Video240K SD dataset shows that the proposed method outperforms other methods significantly.",
            "main_review": "This work presents a very practical solution to automatic portrait video matting with strong performance. The strengths can be summarized as follows.\n+ The paper is well-written and is easy to follow.\n+ The performance appears to be outstanding, according to the experimental results on the Video240K SD dataset.\n+ The approach is clean and well-motivated.\n+ There are ablation studies to verify the effectiveness of different model designs, although some aspects are missing.\n+ The whole system seems to be well-tuned and optimized for the best performance. \n\nTo me, the weaknesses of the work are as follows. \n- The core idea of using optical flow sounds quite straightforward to me. Although this method is the first deep learning that utilizes optical flow for portrait video matting, using optical flow is the default approach to process videos. I feel the technical novelty of using optical flow is not very surprising.\n- It seems to me that the method is only evaluated on synthetic composite data (as shown in Fig. 5). The experiments on real-world data are missing. It is not clear if the approach performs well on the real data.\n- The authors applied multiple data augmentation strategies but there is no ablation study on this. Is the data augmentation critical to the proposed approach? Do the baselines also get the same type of data augmentation for fair comparison?\n- There is no evaluation of the temporal consistency of the resulting videos. I think the authors can at least show some example resulting videos by the proposed approach.\n\nThere is a minor typo:\n- In Page 2, \"and calculate the\" -> \"and calculates the\"\n",
            "summary_of_the_review": "In summary, I think the performance of the presented method appears to be good. However, there are several issues in the current manuscript without some important experiments and visual results. The technical novelty also seems incremental.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper describes a method that performs an automatic portrait (i.e. recover foreground from a portrait video) video matting based on a context motion network as motion of foreground and background can be different. The method takes a sequence frames and predicts the alpha map for the foreground.\n\nThe method is based on ConvGRUs to take several frames into account, which are augmented with an estimated optical flow using PWC. An Unet architecture finally outputs the estimated alpha maps along with the foreground image which are trained in a supervised way. \nIn addition, the authors extend Video240K SD (foreground content) with videos collect from YouTube serving as background content for training data.",
            "main_review": "The paper is well-written and to the point. And the results are convincing (Note, I am not familiar with most related work, so it is likely that I missed work which is stronger.)\n\nThere are some open questions about the experiments.\n\n- The results section explains the importance of the optical flow. However, it is not explained how important the PWC is as part of the network or if it just acts as an augmentation of the input (What happens after 150k when still keeping the PWC weights fixed?). Optical flow has already a segmentation of foreground and background if the motion is strong enough.\n- In the context motion updating operator paragraph: For the baseline model I see no connection between consecutive frames besides the optical flow. Were simpler operations evaluated? (e.g., Concatenation)\n- How much impact had the data generation procedure (using youtube videos as background). How do the other methods perform using such new training data? I checked related work and found that the networks are public and could have been. Comparing method performance when trained on different datasets is almost impossible. Especially the effect of the H264 noise would have been interesting to see. In general, I missed an ablation study for different parts of the data generation. (Either these steps are novel, then it would be interesting to see the effect. If these augmentation steps are not novel, the paper misses a reference to prior work.)\n- I was missing for a qualitative result section. How do these results look on videos in-the-wild?\n- A big part of the work is based on optical flow. How does the network performs on mostly still videos with small amount of object motion (kind of violating the underlying assumption of this work: large enough object motion)\n\n\nMinor things:\n- I wonder why Eq. (15) requires the indicator function as \\alpha seems to be \\in [0,1] according to (1).\n- Page 12: The \"You may include other additional sections here.\" could have been removed.\n- This is a paper about \"video matting\" without any result video \n- Figure 1: Might be a wrong claim -- depending on the impact of finetuning PWC: So far any semantic segmentation network can be incorporated into larger network to augment the input image. Hence, the claim of not requiring an extra input is not necessarily a strong one.\n- Is \"For a fair comparison, we evaluate the whole image for all comparing methods\" true?",
            "summary_of_the_review": "Like I wrote in my main review, the paper is well crafted and the (empirical) results (Table 1) is convincing. My only concern is the weak ablation study wrt. to the method and impact of data generation. The network model described in Figure 2 has several components which necessity is explained in the result section but not the design itself. How complex does the context motion updating operator has to be? ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper tackles the  portrait video matting task. By proposing a set of Convolutional Neural Networks divided in four stages,  predicts  alpha maps and foregrounds for each frame. The predictions are quantitative and qualitatively competent to the state-of-the-art approaches. The key contributions for the results are the feature extractor network, optical flow network, and context motion updating operator.",
            "main_review": "Strengths\n\n1.- The paper takes one of the hot tasks in image processing, video matting, and contrary to its counterparts, the model is capable of predicting Alpha maps and foregrounds without additional data like trimaps.\n\n2.- The proposed model itself  overcomes the state-of-the-art methods with a large margin. For example, RVM published in 2021 is overcome, in MSE, with 0.62 difference. There are similar large differences with other metrics.\n\nWeaknesses\n\nDue the proposal is mainly the  CNN architectures or  a set of models, and with the aim to enhance the quality of the paper.  The three tables in ablation study are a little hard to follow, which may give us a misunderstanding. I suggest merging those tables or at least merge the first two . Additionally, I need to see more experiments in the ablation study to validate each part of the context motion network for  portrait video matting.   \n\n1.- In Feature extraction network: concretely in Table 2 ResNet50 gives the best result. I would like to know why those results are different from the best of Table 3 or Table 4.  According to the result in Table 2, we do not need PWC-Net nor +Motion.\n\n2.- Optical Flow network: What is the difference when the proposed model does not use optical flow network?\n\n3.- Your Dataset: What is the contribution of your dataset in the results?\n\n4.- Finally, concerning ablation study, it  needs more detail, personally I am a little confused with the real contribution of each part (may you add here the Upsampler stage?). \n\nMinor Errors:\n\n1.- Paper page 3: Check the second paragraph of Video matting there is a repeated word. And the optical flow paragraph, the last part, needs double checking.\n\n2.- Page 4 repeated the word, “the the'' in the optical flow estimation section.\n\n3.- Page 8: At the end of the paragraph “Table 1 reports the...”  there is a typo “is our result is more accurate than others”",
            "summary_of_the_review": "Overall, I vote marginally above the acceptance threshold.  The main decision states the Strengths of the papers stated above. For example, the similar approach,  RVM(2021),  which works without extra input, reaches 0.89 in MSE; the final proposed model gets 0.27, which is a large difference. Hence, we need to see clearly the contribution of each part of the proposal, even the contribution of your new dataset.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper solves the problem of alpha matting, i.e. extracting an alpha transparency map, for each frame of a video, focused on human portraits. Previous works which produce high-quality results require additional human inputs. Research works that do not require any input often focus on semantic information only. This work utilizes temporal information, i.e. foreground and background objects move differently in a video. To use temporal information the authors use optical flow as an intermediate representation of relative motion. Then it involves Convolutional GRUs to capture temporal information. The paper trains on the newly released Video240K dataset and shows superior performance compared to existing approaches. The paper is well written, easy to read and understand.",
            "main_review": "Strengths:\n- Easy to read and understand\n- Superior performance on Video240k dataset\n- Optical flow representation for handling motion is a nice principled approach to tackle this problem.\n\n\nWeakness:\n\nA) The results on the Video240k dataset are exceptionally good. However, there are a couple of issues with this experiment that need to be pointed out:\n\n(i) *Quality of the dataset*: Alpha mattes presented in Video240k are not that high quality compared to the Adobe Image Matting dataset. This is because alpha mattes in the Video240k dataset are automatically created by using an off-the-shelf green screen software vs Adobe Image Matting dataset which is manually created by an artist and thus higher quality. Thus I am not exactly sure if the improved result is just a sign of overfitting to this dataset vs a general improvement, see later points.\n\n(ii) *Training data of existing approaches*: To my knowledge, most of the existing approaches, do not train on Video240k, and even if they do they use many other datasets in training (Adobe Image Matting, Distinctions, etc). For example, RVM (Lin 2021) trains on Distinctions646 and Adobe Image Matting dataset at the end of the training scheme. So for a fairer comparison, all these existing approaches need to be trained on the Video240k dataset and then test on it.\n\n(iii) *Test on real data*: As I have pointed out Video240k is not a perfect dataset, so improvements here should be taken with a grain of salt. It is important to show results on real-world videos, both qualitatively and quantitatively with user studies. Real-world videos and Video240k test set have a very important difference: the background is Video240k is artificial i.e. composed separately with the foreground. In situations like this the color blending, noise pattern, or just in general perspective and illuminations differ between foreground and background which can make the separation easier than in real images/videos. For example, the input images shown in Fig 5 look highly unrealistic in terms of the foreground-background match. This is why results on 'real' videos are very important.\n\nB) It is not clear if the proposed method can capture long-term temporal information. During training, it only uses 3 frames. Arguably, during testing, the ConvGRUs can store temporal information, and help it capture a longer time horizon. A simple experiment to show this effect can be: If we only care about the quality of the alpha matte in frame 1, does the quality improve if I test it on a sequence of 3 frames, 10 frames, 30 frames, ... so on. I would expect the quality to improve as the method sees more frames during test time and then plateau after a point because convGRUs can't necessarily memorize long time horizons.\n\nC) The authors need to report the memory, run-time, and operating resolutions of the proposed method. From the limitation section, it seems like the proposed method needs significantly larger memory and run-time compared to some of the existing approaches like RVM. It can't also operate at 4K resolution. It is not clear if a 2x improvement in quality at expense of 10x (a random number I used) more memory and 10x slower runtime is helpful, especially if it can't handle high-res videos like 4K.",
            "summary_of_the_review": "The idea of the paper is promising, but the experimental evaluation is flawed and probably exploits overfitting. Thorough and fair evaluation on the Video240k, Adobe Image Matting dataset, and real data is important to understand the advantages of this method. This should be also backed up with speed, memory, and resolution comparison.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposed to utilize optical flow and contextual information for automatic portrait video matting. The proposed network proposes each video frame sequential with additional branch to incorporate optical flow inputs between consecutive frame. With the additional optical flow information, the proposed method can segment the human portrait automatically with temporal consistency across different frame. Experiments on Video240K SD dataset shows that the proposed method outperform pervious works that process each frame individually.",
            "main_review": "Strength: The paper writing is easy to understand, and the usage of optical flow is effective in improving temporal coherence across video frame.\n\nWeaknesses: \n\nThere are several major weaknesses:\n\n1. When we are talking about matting, we are expecting the proposed method can handle large area of semi-transparency such as long hair. However, in the presented examples, I can hardly find such example in the experiments. It looks to me that this is only solving video portrait segmentation instead of video portrait matting.\n\n2. In term of matting problem, again the most different part is to solve the alpha mattes in the long hair semi-transparency regions. However, optical flow estimated within those regions are hardly reliable, as discussed in Sun et. al. CVPR 2021. How does the proposed method resolve such optical flow estimation errors in the matting problem. From the current proposed method, I can hardly find such mechanism in handling the optical flow errors.\n\n3. In term of video segmentation, there are already many existing methods in improving the temporal coherence of segmented regions. As a baseline, I would expect the proposed method can also compare with those video segmentation method, instead of only the matting techniques. Indeed, by just incorporating optical flow information into existing technique is a trivial solution to improve temporal coherence. It does not look to me that the proposed method is significant.\n\n4. The demonstrated examples are all synthetic. How well is the proposed method applying on real-world scenarios?",
            "summary_of_the_review": "I would suggest to reject this paper as discussed above in the paper weaknesses.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}