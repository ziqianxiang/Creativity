{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper shows \n- [theoretically] that a modified InstaHide is differentially private and\n- [empirically] that it is secure against several data poisoning and backdoor attacks.\n",
            "main_review": "\nDPSGD is provably secure but suffers from significant accuracy loss, while InstaHide offers better accuracy but is shown to be insecure. It would be great if a modification of InstaHide enjoys both. In this sense the question and the idea of the paper is of vital importance.\n\nHowever, I do have several comments.\n\n1. Theorem 2 is a straightforward application of the classical tool of amplification by subsampling. In fact, this theorem does not achieve its goal: $\\epsilon\\leqslant\\max\\{A,B\\}\\leqslant \\frac{1}{k\\sigma}$ completely ruins amplification. Instead, for $\\frac{k}{N}<0.1$, we should be able to show that $\\epsilon\\leqslant \\frac{2N}{n\\sigma}$. The authors also ignored advanced composition [[KOV](http://proceedings.mlr.press/v37/kairouz15.html), [Gaussian DP](https://arxiv.org/abs/1905.02383), [Renyi DP](https://ieeexplore.ieee.org/abstract/document/8049725/?casa_token=brNjuyqnOF4AAAAA:NefSuKWk2qfS_68uqcYE01Ret7o6QWo4QY-QwmfkHFg9QDeQtbjj3yfSZpLKcyD_ZWRFqy0G)], with which $(\\widetilde{O}(\\frac{\\sqrt{N}}{n\\sigma}),\\delta)$-DP should be possible. This increases the sample size $N$ by a huge amount.\nHowever, the combinatorial flavor proof is quite interesting. It looks quite similar to that of Theorem 11 in [this paper](https://arxiv.org/abs/2106.08567). It's unfortunate that the authors did not realize the potential of this proof and ended up with a very sub-optimal result.\n\n2. The y-axis of figure 2(a) (which should be $\\epsilon$) is log scale but that of (b) is linear scale. Why do they match? Or equivalently, how should $\\epsilon$ determine poison success, even intuitively?\n\n3. Because of my comment 1, the theoretical bound is loose. But it still appears to match what is observed in the empirical result. The only reason I can think of is  the attack is somehow equally weak as the bound.\n\n4. I recommend reorganizing experiments. There are too many empirical results in the paper and the readers can miss important messages easily.\n\n5. The citation [Cynthia Dwork, Aaron Roth, et al.] is imprecise. The book only has two authors. And DP is not developed by this book. A better saying is this is a standard reference and more references can be find therein.\n\n6. I wonder if the algorithm is still secure against the attack by [Carlini et al] (presumably yes?) and I'd like to understand why. It's not a necessary part of the paper but I personally would find it very helpful.",
            "summary_of_the_review": "Because of my comment 1-3, I think the findings are not very sound, so I cannot recommend the paper in its current form. However, the question is important and the idea looks promising, so I would recommend the authors re-submit after a more careful study.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies how mix-up (data augmentation) operations, once combined with Laplacian noise after the mix-ups, can achieve differentially private learning. A similar approach was proposed by Huang et al, in a scheme called InstaHide. Hence, this paper’s proposal is coined as DP-InstaHide. \nA similar approach was proposed by the (cited) work of Lee et al ISIT 2019 using Guassian noise rather than Laplacian.\nThe paper then studies how such mechanisms can help defeat poisoning attacks. It is known that DP schemes are more robust to data poisoning attacks, in which the adversary aims to hurt the model by changing (or adding to) the original clean examples. The paper then empirically studies how their DP-InstaHide scheme as well as other schemes based on the similar design (that pick a mix-up method and then add Laplacian noise) can resist certain data poisoning attacks. \n",
            "main_review": "Please list both the strengths and weaknesses of the paper. When discussing weaknesses, please provide concrete, actionable feedback on the paper.\n\nPositive side: this paper shows how to improve the (now broken) InstaHide proposal and put it on solid foundations. The idea is simple but effective, and the way we know it is effective is by the theorem in the paper, as it is not possible to fully evaluate privacy without a proof. \nAnother positive aspect is the in-depth study of the role that the DP-InstaHide can play for robustness against poisoning. It was previously known that DP schemes actually imply (levels of) robustness against poisoning, simply because the learner becomes less sensitive to changes in the data, but the paper aims to study how the particular DP-InstaHide scheme can resist certain attacks.\nAnother positive aspect is the ethical paragraph at the end, in which the author(s) clearly say that their experiments are not proof of security. It is rare to see such (correct) statements.\nThe main thing I am looking for is more comparison with the work of the (cited) work of Lee et al ISIT 2019. Despite being cited, the comparison is not clear. After all, the key idea of “mix up + additive noise” is already there, and the fact that DP mechanisms imply robustness against poisoning was also known before this work. Hence, more comparison with the work of Lee et al is needed to clearly show (e.g., in a graph) the advantages of this paper’s scheme. \n\nFurther comments\nI suggest the author(s) revisit the name DP-InstaHide. Note that InstaHide is called so (as far as I can tell) to indicate that their scheme is “instance hiding” as opposed to satisfying the stronger “indistinguishability based” criteria of DP. But you do achieve DP, so the name “DP-InstaHide” is not the most natural name.\n\nWhere is the proof of theorem 1? Is it borrowed from another paper?\n\nEnd of page 4: when you say “relevant dataset P”, should not the dataset be D?\n\nOn a related note on Page 5: it seems M_P shall be written as M(P) to match your notation defined earlier?\n\nYour approach seems to give something that is somewhere between central and local DP (because you process the whole data set into being DP, but you do not perturb examples individually). Shuffling based approaches to DP have the same idea. Are there more connections between your approach and shuffling based approaches?\n\nTo better understand and appreciate Theorem 2, one also needs to know the utility that comes out of it. Of course you study the correctness of the schemes that you experiment with, but can you say anything provable about the utility of DP-InstaHide?\n\nPage 5: \"+ N is given by\" should be \"+H is given by\" ?\n\nIn the figures, the left graphs show the Y axis as \"average poison success\"  but at least in Figure 2 that seems to be a typo?\n\nThe exact attack setting for the experiments are not described (in the main body). Can you clarify those please? For example, what is exactly the goal of the adversary? To misclassify a randomly selected test point that is given to the attacker? I was also confused whether Table is for simple poisoning attacks or for backdoor? The table’s explanations are not clear.\n\nWhy is Poison Frogs attack written as FC?\nTable 5: What are HTBD + CLBD?\n\nWhat is \"security-performance trade-off\" for the left figure in Fig 3?\n",
            "summary_of_the_review": "The paper does a nice job of showing how mix-up operations (that historically were invented to improve generalization and more recently were used for privacy by an attacked proposal InstaHIde) can be actually modified to be DP. The experiments about the poisoning resistance are also nice. The main issue I have, as explained in my review, is that the two key ideas of the paper seem to be someone known already: 1. Mix-up + additive noise leads to DP 2. DP implies robustness to poisoning. Of course the paper still has merit in showing that their exact approach is more effective and does many experiments. But I still think for a venue like ICLR, the paper needs to explain better and justify the novelty of the two steps in light of known results. So, I am open to changing my score based on the response and clarifications.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studied data augmentation, specifically, the mixup-based data augmentation, as a defense method against data poisoning and backdoor attacks. The paper first demonstrates that by mixing up k examples, the differential privacy guarantee is enhanced k times, which provides insight into why mixing up can potentially lead to better defense. Then inspired by this observation, the paper proposed using the mixup technique to augment the original training data. As a result, the robustness to data poisoning attack improves substantially. Extensive empirical evaluations of mixup data augmentation on both data poisoning and backdoor attack, as well as on a variety of datasets illustrate the effectiveness and efficiency of the proposed defense method.",
            "main_review": "Strengths:\n\n(1). The paper provided a very solid empirical evaluation of the proposed mixup-based data augmentation defense against poisoning attacks. Specifically, the paper evaluated the defense on both data poisoning and backdoor attacks. Moreover, a variety of datasets are used to support the result in the paper. Besides that, comparisons against several baselines consolidate the superiority of the mixup-based defense. Overall, the empirical results are very convincing.\n\n(2). The paper is easy to read. The topic is also well-motivated.\n\nWeaknesses:\n\n(1). The ideas in this paper are not surprisingly novel. Mixup-based augmentation has been studied previously. The robustness of differential privacy against data poisoning attacks was also revealed in previous work. The main contribution of this paper is that it provided a rigorous differential privacy proof of the mixup augmentation, and performed extensive evaluations on using mixup-based augmentation as a defense against poisoning attacks. Technically, the contribution is not very substantial and most of the ideas are already in the existing  literatures. That being said, I wonder if the authors could better articulate their main contribution?\n\n(2). I am wondering if the authors ever thought about whether the robustness to poisoning attacks really come from different privacy or the bounded norm constraint of data points. There are recent papers, e.g., [1] that argue the defense ability of DP actually has a deeper reason. The authors may want to point that out in the related work.\n\n[1] DOES DIFFERENTIAL PRIVACY DEFEAT DATA POI- SONING?\n\n",
            "summary_of_the_review": "Overall, the paper studied an interesting problem, but I am not exactly sure about the novelty and technical contributions of the paper. Potentially the paper needs to be improved.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper addresses robust training against data poisoning and adversarial attacks by way of differential privacy, namely by adding Laplacian noise on top of mixup. Due to mixup averaging k samples from the database per training example, the sensitivity of changing a single entry in the database is lower than if the training were done on examples in the database. Thus, less Laplacian noise is necessary, which results in better accuracy compared to simply adding Laplacian noise to training examples immediately extracted from the database.",
            "main_review": "Although Laplacian noise can be added to standard training examples to give differential privacy, the noise is often too large and results in inaccurate models. Heuristics such as mixup, which takes linear combinations of k examples from the database, and InstaHide, which adds a random mask, were proposed as alternative ways of robust training without needing to add large Laplacian noise to the training examples. However, these heuristics did not have provable guarantees and attacks on InstaHide are well-known. \n\nThis paper notes that since mixup takes linear combinations of k examples (crucially without replacement), then the necessary Laplacian noise to ensure differential privacy is lower than standard training examples. I think this is an elegant observation that gives a much-needed provable guarantee. On the other hand, mix-up already decreases the quality of the training examples due to the linear combinations. This somewhat explains why less Laplacian noise is needed. Thus the main question becomes how does linear combinations+smaller Laplacian noise vs. standard training+larger noise compare in practice? Unfortunately, this does not seem to be the focus of the empirical evaluations.",
            "summary_of_the_review": "In summary, although I believe the proof of the main privacy guarantee follows naturally from the definition of sensitivity and the Laplacian mechanism, the observation that lower Laplacian noise is necessary to ensure differential privacy of a linear combination of k training examples is nice. However, there seems to be a disconnect from this theory of differential privacy on top of mixup to the experimental section of the paper, which instead seems to focus on other data augmentation techniques.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}