{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In the manuscript, the authors propose RDOQ, which assigns quantization bitwidth in a channel-wise manner in order to improve the final accuracy.\n\nBesides the Lagrangian method, RDOQ also considers the hardware efficiency.\n\nThe authors show hardware simulation results on TPU and Eyeriss.",
            "main_review": "Strengths:\n1. I think the manuscript is generally well-written and easy to follow.\n2. The illustrations are of good quality.\n3. Experimental results on various neural architectures (including mobilenetv2) are provided on large-scale dataset ImageNet. \n4. Have simulated results of speedup on different hardware platforms (TPU, Eyeriss).\n\nWeaknesses:\n1. I think it is misleading to say the channel-wise mixed-precision method run more efficiently than layerwise on hardware. The speedup is solely from the hardware-aware part of RDOQ rather than channel-wise. As a matter of fact, channel-wise mixed-precision is not easy to implement and will lead to overhead, which is the main reason why previous works mostly focus on layerwise mixed-precision. It is crucial for RDOQ to have a hardware implementation to justify the feasibility and efficiency of channel-wise mixed-precision (the simulated speedup is a good start).\n2. There are many other papers trying to efficiently solve mixed-precision problems, for example, AdaQuant (integer programming), HAWQV2 (pareto frontier), and so on. They should be mentioned in related works. In 2021, there are also new hardware-aware mixed-precision quantization papers to note.\n3. How is the 2-bit defined/performed in Table1? Does RDOQ include binary quantization as well? Otherwise the gain cannot be originated from mixed-precision since 2-bit (ternary) is the lowest precision. Additionally, it would be good to provide a bitwidth map.\n4. The hardware-aware method tries to keep things on-chip. Consequently, it is not useful when the hardware capacity constraint is satisfied? This is a limitation comparing with optimization based on latency or co-design.\n5. The efficiency of the RDOQ optimization is based on the additivity property, which is an assumption wildly used in previous arts (I appreciate that this is discussed in the manuscript). This limits the novelty of RDOQ to some extent.\n6. In Table 3, the difference between RDOQ and HAQ on mobilenetv2 seems marginal, in terms of accuracy and speedup.\n\nSmall Issues:\n1. Page 6, ETS -> STE\n2. Page 7, reference rate -> inference rate\n",
            "summary_of_the_review": "I think the manuscript provides a decent solution for channel-wise mixed-precision quantization. However, some clarifications and modifications are required before publishing, and the novelty could be strengthened. As a result, I rate RDOQ as borderline reject.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposed an efficient bit-allocation method for mixed-precision DNN inference. The proposed method is based on the rate-distortion formulation of quantization error, which can be solved as a constrained optimization problem with linear complexity. The authors also introduced on-chip memory constraints to further optimize bit-allocation considering hardware capability. The proposed methods are evaluated on standard ImageNet CNNs for accuracy comparison and TPU/Eyeriss simulation for hardware speed.\n\n ",
            "main_review": "The proposed methods (the linear-complexity bit-allocation algorithm and on-chip memory constraints) seem to be a practical approach for mixed-precision quantization. \n\nHowever, many details are missing in the experimental results, raising concerns about fair evaluation and reproducibility.\n- What is the quantizer used in this work? Although the authors claimed that their methods are complementary to the quantizer, the readers should know what kind of quantizer is used for a clearer understanding of the accuracy comparison and reproducibility. \n\n- What are the bit-configuration results of RDOQ in Table 1 (2-bit) and Table 2 (4-bit)? Since the main goal of the proposed method is a proper bit-allocation across the layers, the readers should know the bit-configuration found by RDOQ. Similar to HAQ, layer-wise bit-allocation for weights and activations would provide great insight into the sensitivity analysis and the novelty of the proposed method. (Although the authors provided Size info in Fig. 7, it is not intuitive as it mixes up weight and activation.)\n\n- What are the hyper-parameter settings (e.g., learning rate schedule, batch size, regularization methods, data augmentation, etc.) used for fine-tuning the networks quantized by RDOQ bit-allocation? Also, typical fine-tuning involved in quantization-aware training (QAT) requires tens to hundreds of training epochs. Whereas the proposed method used only 10 epochs of retraining, which seems to be too small. How does the loss curve look for fine-tuning? Since the convergence of quantized neural networks is an important factor for the success of bit-allocation, providing detailed information about the fine-tuning settings and the results would be highly desirable.\n\n- Hardware evaluation settings seem to be flawed. It seems that the authors simply adjust memory cycles with full on-chip data reuse for RDOQ. In other words, the authors seem to assume that variable-precision data is (somehow) packed in memory and densely processed in full precision without incurring memory/cycle overhead. This assumption is highly unfair for the equal-bit quantization methods (like PACT), which are naturally favorable for dense processing thanks to their regular bit-width. Such an unfair assumption might mislead the readers about the hardware cost and overhead of quantized neural network acceleration.\n\n- What are the bit configurations of the quantization cases in Table 3? \n\n\n",
            "summary_of_the_review": "Although the proposed idea of a linear-complexity bit allocation algorithm sounds interesting and practical, there are many important details missing in the current manuscript. Without sufficient information about experimental results, it is not possible to properly evaluate the proposed method and understand their novelty, not alone the reproducibility concerns. Therefore, I am inclined to reject now; but I am open to modifying my scores based on the authors' rebuttal.\n\n ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents novel quantization-aware training approach which assigns different bitwidth for different weight channel. In contrast to some conventional AutoML approaches such as DARTS or HAQ, this paper forms the bitwidth searching problem as a discrete optimization problem. By relaxing this variables to continuous variable, this problem can be solved using Lagrangian. The optimization problem can be further extended to consider the on-chip memory constraint, so that the input activation and weights of a single DNN layer can completely fit to the memory. Experiment results show that the proposed solution can achieve superior performance than the baseline approach while obtaining more than 3x speedup on popular deep learning accelerators.",
            "main_review": "Strength:\n+ The authors consider a practical issue from the perspective of DNN hardware implementation. The problem is well-defined and well-motivated.\n+ The solution is presented clearly. The paper is easy to understand.\n+ Some hardware performance is reported, although its produced by simulator rather than real on-board measurement.\n\nWeakness:\n- The evaluation results shown in Table 1 and 2 are kinds of as expected, because channelwise bit allocation allows a much greater flexibility on weight quantization assignment.\n\n- From the computation perspective, I don't think the compute engine in the hardware accelerator can leverage this per-channel quantization to achieve further computation saving. For example, to multiply a 3-bit quantized weight value with a 4-bit activation value, a 4-bit multiplier is still required. In addition, in order to save the filter weights with channelwise quantization, extra bit are required to encode this information. Authors should provide justifications from the hardware implementation perspective.\n\n- Besides the RL-based AutoML approach, authors should compare their solution against differential architecture search (e.g. DARTS [a1])\n\n[a1] Liu, Hanxiao, Karen Simonyan, and Yiming Yang. \"Darts: Differentiable architecture search.\" arXiv preprint arXiv:1806.09055 (2018).",
            "summary_of_the_review": "Overall, I like this approach, which formulates the bitwidth allocation problem as an optimization problem, and solves it using the Lagrangian multiplier. However, the authors should provide more justification on the applicability of the channelwise bit allocation approach on real hardware platform. Without this, it is hard to convince reader on the usefulness of this approach.\n\nIn addition, to further save the memory access cost, authors may consider to save the input activation, weight, output activation (i.e., the input of the next layer) concurrently in the on-chip memory. So the memory cost associated with the input activation will be eliminated completely. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to quantize weights and activations at the channel granularity. unequal bit-width are allowed for different channels within the same tensor. Since this causes an explosion of the design space, the authors propose a search solution which solves the problem in linear time, thanks to a rate-distortion algorithm. Experimental results show SOTA accuracy on various ImageNet networks.",
            "main_review": "The paper presents interesting concepts but there are several issues that the authors need to address:\n\nThe rate distortion setup formulation in Section 2.1 and use of Taylor series expansion in Section 2.2 look very similar to the work on predicting the accuracy loss due to quantization in (Sakr et al., ICML 2017, 'Analytical guarantees on numerical precision of deep neural networks' - and later works from the same authors looking at various granularities, etc.). it is possible that the authors were un-aware of this work and independently reached the same conclusion. However, it would be good to add a discussion on how the proposed work differs from prior art here, or at least some reference indicating that these results are not novel.\n\nRegarding eq.(4) and (5) and the proposed algorithm, it seems to me that the solution can be formulated in terms of auto-differentiation modules and therefore, it could be implemented using GPU(s). Can the authors investigate if that's possible (I think it is). In that case, more than 50 images could be used for the optimization, as was done in the paper referenced above.\n\nRegarding the formulation in eq. (7) and (8), since R=K*B, what is the point of keeping the first constraint on R in the formulation? It is redundant to the new constraints on B and K.\n\nThe experimental results appear impressive, however there are important details which seem to be omitted on purpose. For instance, the results in Table 1 claim that SOTA accuracy is achieved using 2-bits. But that is not what the authors have done, they have assigned unequal bit-widths across different channels. This caveat needs to be included, and I suggest an extra column for the aver age number of bits in all representations (for weights and activations, respectively) be added.\n\nThere are many typos in the paper. For instance, straight-through estimator is abbreviated as ETS rather than STE; state-of-the-art is STOA instead of SOTA, etc. Please proof-read your paper.",
            "summary_of_the_review": "An interesting paper, however I have doubts on the novelty of the proposed analyses and on the deceptive exposition of the results. I recommend a weak reject.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}