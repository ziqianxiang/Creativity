{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a contrastive learning-based method for anomaly detection model training. The contrastive loss is known to be powerful for self-supervised representation learning, however, it does not comply with anomaly detection loss resulting in it being difficult to deploy in anomaly detection models. The authors suggest the idea of mean shifting to make contrastive loss compatible with anomaly detection learning. Quantitatively the proposed method outperforms recently proposed anomaly detection (out-of-distribution detection) methods. The extensive analysis explains the difference between the proposed methods from the previous ones and the limitations of both the proposed and previous approaches.",
            "main_review": "[stengths]\n- Applying contrastive loss to anomaly detection problems is a non-trivial problem.\nAs discussed in the paper, contrastive loss incurs uniformity in the feature space while anomaly detection representation requires concentration of the distribution. Therefore, the idea of combining the contrastive loss with anomaly loss is an interesting and remarkable finding.\n\n- Clear writing. The claims are well supported by the analyses.\nThe paper addresses a series of important discussions.\nI acknowledge that all these discussions are valuable and closely related to the research topic.\nThe list of items discussed in the paper is as follows.\n\n* Does the proposed method work if trained from scratch?\n* Is the pretrained network effective for different domains?\n* How severe is the catastrophic collapse in the proposed method?\n* Angular representation is better than euclidean for anomaly detection.\n* Why rotation-prediction methods do not benefit from pretrained features.\n* Self-supervised methods do not benefit from large models.\n* ablation study on the training objectives.\n* The analysis of multi-class anomaly detection scenario.\n* Ablation study on the detection scoring functions.\n\n- The proposed method shows strong performance against recent state-of-the-arts.\n\n[weaknesses]\nContrastive learning is a powerful unsupervised (self-supervised) learning approach, often deployed in representation learning.\nHowever, the method proposed in this paper requires a pretrained network learned with supervision like PANDA.\nBecause of this requirement, the method is not categorized as an unsupervised method, less closely related to previous contrastive learning approaches in terms of the setting (requires labels for pertaining).\nAlso, this condition limits the applicability of the idea to real applications.\n\n[Questions]\nIn the illustration of Figure 3. (d), the shifted features are normalized onto the hypersphere.\nHowever, in equation(4), the shifted features are not normalized.\nIs the normalization of shifted features only performed during inference but not in the training phase?\n",
            "summary_of_the_review": "The paper proposes an interesting idea by combining contrastive loss and anomaly loss which are not compatible originally.\nThe experiments show high performance. The discussion section addresses very important points of the research supported by extensive analyses.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes a method to fine-tune ResNet/ImageNet pre-trained networks for the purposes of anomaly detection, using a modified contrastive loss function. It is argued and demonstrated empirically that using a standard contrastive loss function for this fine tuning does not work well, since the uniformity objective of such a loss mixes anomalous and normal samples (such disadvantages of a standard contrastive loss for anomaly detection in general were also previously shown by e.g. Sohn Et al. 2020). The authors then propose a modified (“mean-shifted”) version of this fine-tuning contrastive loss that adapts features with respect to the center of the the normal samples. The result is to concentrate the representation of normal samples around this center, which empirically helps separate them from anomalous samples. The authors provide experiments on several datasets and compare with previous methods with and without pretrianing, and provide discussion of the results and methodology.\n",
            "main_review": "The paper is well presented and fairly clear in its exposition. Overall it builds upon ideas from the PANDA method (Reiss et al 2021). Aside from the use of pretraining, the ideas themselves presented here are reasonable, and the arguments for failure of standard contrastive loss on top of pretrained features make sense (note that such an approach wasn’t suggested before, likely because of its failure). The proposed modifications using the MSC loss make sense as an improvement on this baseline; however I find the arguments of compactness (e.g. figure 3) more compelling than the suggestion of maximizing angles between negative pairs (Figure 2). I would suggest for more elaboration or justification of this argument.\n\nMy main criticism of the paper is on the use of pre-training methods for anomaly detection themselves - this critique may be equally applied to the prior work on pretraining (PANDA) which I believe to be flawed / an unfair evaluation in the same way. I find this scenario somewhat unconvincing in terms of truly evaluating the realistic power of these methods versus other methods in the literature that train only from scratch on the normal samples from the fine tuning domain. This is because of the overlap in nature of the pre-training samples and the unseen anomalous samples. \n\nFor example, when testing on CIFAR, CatsVsDogs, or other natural image datasets, it is clear that the pretraining methods would have the ability to learn via supervision to distinguish representations that classify and distinguish Cats, Dogs, Airplanes, etc. - thus the domain shift to the anomaly detection datasets is small - one would expect these methods to do well without any fine tuning in such a scenario, just as the DN2 method from PANDA (and its simple improvement with the angular objective here) shows. As such the pretraining idea is missing the key objective of the anomaly detection problem: anomalies (outliers) may come from an unknown distribution and similar anomalous examples should not be seen before test time! Having such pre-training data available makes the comparison with truly self-supervised (not pretrained) methods an unfair comparison.\n\nSuch cases where some anomalies/outliers are presented at train time are also given in the “outlier exposure” scenario, as in\nHendrycks et al. “DEEP ANOMALY DETECTION WITH OUTLIER EXPOSURE” (2019),\nand indeed shown to provide an improvement in anomaly prediciton score. \nTherefore much of the benefit of the results presented from pretraining methods are not really comparable with methods such as CSI, MHRot, DROC etc. - perhaps outlier exposure would be a better class of methods to compare with.\n\nAside from this main point, another criticism of the comparison is due to the size of the networks used, namely ResNet152. While the authors do include a paragraph explaining that after testing with CSI “Self-supervised methods do not benefit from large architectures” and no benefit was gained there with such a large backbone, I also feel that this is an unfair advantage to the proposed method, and a fair comparison would have used the same sized backbone (e.g. ResNet50 or ResNet18) that other previous methods in the literature used for presentation of the main results. While CSI as presented may not improve with a ResNet152, that might not be the case for other prior methods, or there may be other ways to increase capacity that do improve results (e.g. additional ensembling). I believe it is important to compare like with like, and include a similar amount of computation / model capacity in each case.\n\nAnother recent work that showed results on pretrained networks is \n“DO WE REALLY NEED TO LEARN REPRESENTATIONS FROM IN-DOMAIN DATA FOR OUTLIER DETECTION?”, Xiao et. al. (2021).\nThe authors should compare and contrast with this method and results. Note that this method also uses ResNet-50 like other competing methods, for a fairer comparison on this point (even if the criticism of comparing pretraining methods versus purely self-supervised methods for anomaly detection in general still holds).\n\nRegarding datasets such as DIOR in Table 2, where clearly rotation invariance becomes an issue for methods that require non-rotation invariant samples I would suggest that the authors should show results using non-pretrained methods other than CSI that do not rely on rotation. \n\nI would also suggest that in a revision, additional datasets should be tested on where the domain shift is larger, and there is very little overlap with ImageNet in terms of textural content. DIOR and MvTec, whilst more distinct than e.g. CIFAR, still contain natural image content that is likely seen as part of the pretrianing. For example, I would propose SVHN, or FashionMNIST as standard datasets reported in other anomaly detection works that should be considered, whereby I would not expect pretraining to transfer well directly, and it would be more interesting to see if the proposed fine tuning actually benefits learning representations that work for this task.\n\nIn Sec 4.3 “our proposed mean-shifted contrastive loss maximizes the angles between the negative pairs“:\nit’s not clear exactly why the proposed loss should maximize angles between negative samples wrt the center? What is it about the negative pairs and the proposed loss that indicates this?\n\n\nFinally I would like to see results under some other scenarios presented in recent works - e.g. outlier ratios (outlier data mixed in with the inlier training data) as in DROC, or Han et al. “Elsa: Energy-based learning for semi-supervised anomaly detection” (2021); also the Multi-class / OOD detection setup (one dataset v.s. another, rather than just one class v.s. another, as used in CSI (Tack et al), MHRot (Hendryks et al.) etc).",
            "summary_of_the_review": "In conclusion, I believe the paper is well written and presents some reasonable analysis when comparing to one previous pretraining based method (PANDA), but flawed in a critical way when it comes to the fair evaluation of results compared with prior works that do not use ImageNet pretraining for anomaly detection. That is that comparison with other methods that do not use external datasets such as ImageNet to learn representations that contain examples similar to the anomalous data gives an unfair advantage in benchmarking. The whole point of anomaly detection is that at test time, anomalous data may appear that is unrelated to that seen at training time.\n\nThis point should be more fairly addressed, for example by testing on datasets that do not contain comparable classes or related examples from within ImageNet, comparison with Outlier Exposure methods, and at least discussed as a limitation of the general applicability of such pre-training methods. Furthermore, networks with a standard capacity (i.e. ResNet-50) should be tested for fair comparison with prior works.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This manuscript proposes the use of pretrained representations for contrastive learning in the one-class setting. The authors show that a simple trick that combines a contrastive loss with pretrained parameters does not result in strong performance, and therefore propose to mean-shift the learned representation for it to work.",
            "main_review": "# Strengths\n\nThe paper is organized logically and its main ideas are presented well & are therefore easy to understand. Existing methods, such as Deep SVDD are introduced and discussed well. The experiments look at a few cases, with potentially the most interesting application being the evaluation of recent multi-modal (or semantic) benchmarks.\n\n# Weaknesses\n\nMy main concerns are around the incremental novelty of this work. In particular:\n\n1) the observation that self-supervised methods do not directly benefit from pretraining is arguably of limited relevancy, since the main conceptual property of self-supervised learning methods is that they are in fact self-supervised, i.e. do not require any pretraining or access to external data/distributions. I realize performances are an important factor to consider as well, however the performance gains appear to be modest at best vs. PANDA of Reiss et al. (CVPR 2021). Moreover, the majority of reported performance gains focus on benchmarks that have recently been criticized and are becoming increasingly less relevant to the community (e.g. CIFAR-10 in Table 1), c.f. \"Rethinking Assumptions in Deep Anomaly Detection\", Ruff et al. (ICML UDL 2021) and \"Semantic Anomaly Detection\", Ahmed & Courville (AAAI 2020) for recent discussions of this aspect. This limits the empirical novelty.\n2) the methodological modification in this work, mean shifting, is extremely simple. In fact, some sort of mean-shifting is an essential preprocessing step known to benefit many ML applications. Besides this, the proposed method only recombines existing approaches, in particular CSI (Tack et al., NeurIPS 2020), and adds a pretrained representation to this. For technical novelty, more would be required.\n\nWhile progress is reported on some benchmarks that were recently proposed in anomaly detection such as multi-modal detection, this particular aspect warrants more discussion in the paper: in particular it should be related to \"Detecting Semantic Anomalies\" by Ahmed & Courville (AAAI 2020), who introduced this benchmark (and initial solutions) for it. A recent publication by Deecke et al. (ICML 2021) couples these ideas with transfer learning, and it may also be interesting to connect such discussions to recent work on systematic generalization, see Ahmed et al. (ICLR 2021).\n\nLastly, I wonder if readers would be fully convinced as to whether simpler optimization settings might suffice in making CSI synergize with some sort of transfer learning approach or pretrained model parameters. For example, it is not clear whether choosing smaller learning rates might already prevent the behavior reported in Figure 1 (b)? Maybe the authors would find it worthwhile to look into this. A minor remark: Figure 1 chooses to only report results for the airplane class. It would be interesting to see whether such results hold for other classes or datasets as well (e.g. MVTec).",
            "summary_of_the_review": "This work studies a combination of self-supervision with pretraining which appears to be quite specialized and yields relatively small performance gains on (mostly) outdated benchmarks. Moreover, the modifications from Tack et al. (NeurIPS 2020) – which this manuscript extends – are incremental.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper addressed the anomaly detection problem. Compared to previous method, the authors propose to combine contrastive learning and pre-training features from external dataset. The authors used a mean-shifted contrastive loss to optimize the network and reported performance improvement on CIFAR-10 dataset.",
            "main_review": "## Strengths\n\n+ The paper is clearly written. The technical details and motivation are easy to understand.\n+ The proposed method achieves the state-of-the-art performance on the anomaly detection on several dataset, including DIOR, MvTec, CIFAR-10\n\n## Weakness\n- Lack of novelty: The mean-shifted contrastive loss is highly similar to the Contrastive-Center Loss (CCL) [R1] except three aspects:\n  * CCL is applied in multi-class classification while Mean-Shifted Contrastive (MSC) loss is applied to One Class Classification (OCC) problem.\n  * CCL calculated the $\\ell_2$ loss while MSC calculates the angular similarity\n  * MSC introduces temperature hyper-parameter $\\tau$.\n\nThe major motivation of CCL and MSC is very similar. It would be highly recommended for authors to explain the difference between MSC and CCL further, and include the ablation study of CCL in anomaly detection.\n\n- Typo: The authors seem to miss some words in the caption in Fig. 1 (before \"(b) Contrastive objective\").\n\n- Ablation study: The authors argue that the pre-trained features are not a good initialization for traditional contrastive learning objective. However, in the experiments, the authors only use ImageNet pre-trained features. In order to show that MSC works well generally for different pre-trained features, I would recommend to use different kinds of pre-trained features as intialization to test MSC loss ($e.g.$, MS-COCO pre-trained features).\n\n\n[R1] Contrastive-center loss for deep neural networks, IEEE International Conference on Image Processing (ICIP), 2017",
            "summary_of_the_review": "Considering that the major contribution of MSC loss highly overlapped with published paper [R1], I am concerned that the submission may not meet the standard of ICLR. I would wait for authors response before making the final decision.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}