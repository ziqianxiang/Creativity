{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposed a Human Perceptibility Principle for XAI to generate human-understandable explanations. The solution is based on the human visual system and psychophysics model to provide human-understandable explanations for the deep fake images. The experimental results show that the proposed method made more perceptible/understandable explanations for humans.",
            "main_review": "Strengths:\n* Generating understandable human explanations is a popular research area in the field of vision and medical imaging.\n* Obtained better human-understandable explanations on deep fake images\n\nWeaknesses:\n* The experimental section is weak, and hard to understand many details.\n** How do authors obtain human and machine understandable features?\n** There is no experimental procedure on how authors obtained the set of distinguishing features. Whether authors considered both CNNs and ViT? Whether they considered the features from early / intermediate / final layers of CNN or ViT?\n** There is no clear explanation of the results in Table 1. Why is the AUC-IoU score so low? Any justification here?\n** Figure 5 spatial frequency distribution using which particular method?\n** The authors considered the attention as the explanation from the ViT-16 model. Whether considered attention maps from all the 12 layers or only the last layer?\n* No novelty in terms of method\n* Any stats on the survey? Are the participants provided with any guidelines? Why did the authors only prefer the VIT-16 as the Vanilla model? Whether authors try any pre-trained CNN model? \n* There are no experimental details, and it is hard to reproduce the results.\n* Whether authors tested on any medical imaging datasets?",
            "summary_of_the_review": "Although the paper addresses an interesting problem, I do not see proper evaluation metrics like the previous methods and no experimental details for the model evaluation.\nThe experimental section is weak, and it is hard to reproduce the results, so the paper does not introduce a significantly better technique.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors show a new method for explaining neural network decisions. The idea is to preprocess images with a bilateral filter (which according to the authors is aligned with the human visual system) and then use established heatmap techniques. Results are shown for two binary face classification datasets. In the first dataset, the authors induce synthetic features to real faces. For the second dataset, they use fake vs. real images for classification. In both cases, the authors obtained heatmaps that are more focussed compared to heatmaps on non-pre-processed images. Furthermore, a small human experiment is conducted. Here, the authors show that human observers prefer the new method compared to previous methods.",
            "main_review": "# Pro\nExplaining AI is a very recent topic within the ML community and the topic is important for many applications. I like the simplicity of the proposed approach. Compared to other papers, the idea is beautifully simple and easy to understand for a broad audience. I highly appreciate this. The (limited) results shown by the authors are impressive.\n\n# Major concerns\n\nUnfortunately, I see two major concerns.\n\n- Limited scope\n\nThe authors use attention as the reference “established” explanation technique. However, there is a debate (at least in NLP) whether attention is a good proxy for explanation, see [1,2]. Therefore, I think it is required to compare the bilateral filtering approach to other heatmap explanation techniques, which the authors also mention in the very first sentence of their paper. The comparison of a single metric to human decision is not novel. Papers that compare different heatmap approaches for human explainable AI can be found in [3]. To my knowledge, the ML community is missing a rigorous comparison of different metrics to human decisions.\nFurthermore, it is right now unclear for me if the authors approach generalizes beyond face recognition. A second non-face dataset with multi-class classification should be used.\n\nAt this stage, I can not judge if the claims being made are correct or if it because only one metric and one type of dataset is used. In short, I recommend a comparison of different metrics on different datasets.\n\n\n- Human experiment:\n\nThe authors ask human observers which explanation is better. This does not mean that the heatmap reflects a human judgment. I propose a different approach: Observers classify images and indicate the most important regions for their decision. Then you can compare human responses to different heatmap techniques. You might also want to use eye-tracking as a more objective measure. \n\nFurthermore, there might be a different explanation for the preference of the bilateral approach. Maybe human observers are disturbed by the amount of information provided by the attention-based heatmaps. One approach would be to clip (or normalize) the attention-based heatmap. Then both approaches have the “mass’’ and thus give the same “amount” of information.\n\nUnfortunately, there are (even in the appendix) only limited information about the experiment itself. The paper deals a lot with spatial frequencies. Especially for this, it is important to use a proper psychophysical setup (e.g the viewing distance influences the viewing angle and thus the cyc/deg). Please see the paper of Gheiros (2019, which you also cite) for a reference of a carefully conducted experiment.\n\n# Medium concerns\n\n- Definition of the human visual system\n\nSometimes I got the impression that the authors do not have a strict definition of the human visual system. Examples:\n“constrained by the Human Visual System (HVS) and psychophysics, need to be taken into account” (abstract) &“Therefore, we revisit the Human Visual System (HVS) and Psychophysics model” (p.3): Psychophysics is a method, not a model?\n“Human eyes”(p. 5): Do you want to talk about the human visual system/human brains/or really human eyes?\n\n- High/low spatial frequency bias of CNNs and humans\n\nThe authors claim throughout the paper that CNNs have a high-frequency bias and humans have a low spatial frequency bias.\nI think it is not that straightforward, e.g. humans are biased towards shape (high frequencies) and CNN towards textures (more low frequencies). I think a detailed discussion is needed here. You might also want to discuss the work of [4]. Furthermore, on page 5 you claim that “[biletral filtering] can smooth high-frequency regions, narrowing the difference between the frequency distribution of fake and real images, as well as preserve the edges, where humans are sensitive to perceive, which exactly meets the requirement of the HVS.” Do you know any paper which shows this? I agree that the human spatial contrast sensitivity function has an upside-down U-shape. But how does this relate to bilateral filtering?\n\nThe authors claim that putting a 15×15 white square on the image (with size 224x224) at a random location induces a low spatial frequency clue. I am not completely sure about this. It depends on the ratio of patch size to image size. On the one extreme, if the patch has the size of the image ---of course-- you only have one spatial frequency with 0 cyc/px. On the other extreme, by adding a patch of size with one pixel, you induce a high spatial frequency clue. I made a small script to check frequencies for images with a size 224x224px and a patch size 15x15px. Depending on the image content, this lowered or increased the spatial frequencies. I only used a few images, so this might not be representative. Thus, I would like to see figure 5 for the control experiment.\n\n\n# Minor concerns\n- Is there a specific reason that you do not include code for the submission?\n\n- I would like to get more (technical) details, e.g. Which filter+parameters do you use exactly? Additionally, more information about the psychophysical experiment is required. The information in the appendix is not enough (See again Geirhos, 2019).\n\n- Report decimal places of percentages for the human experiment does not give meaningful information if you have less than 100 observers.\n\n- Figure 2/3/5/6 has no legend. Please explain at least the colour scheme.\n\n- p.5 top paragraph gives me the impression of decorative math.\n\n- figure 6/7 you wrote: We use red bounding boxes to indicate some human-understandable regions on these images. Where do the red boxes exactly come from?\n\n\nLiterature: \n\n[1] Jain, Sarthak, and Byron C. Wallace. \"Attention is not explanation.\" arXiv preprint arXiv:1902.10186 (2019).\n\n\n[2] Serrano, Sofia, and Noah A. Smith. \"Is attention interpretable?.\" arXiv preprint arXiv:1906.03731 (2019).\n\n[3] Alqaraawi, Ahmed, et al. \"Evaluating saliency map explanations for convolutional neural networks: a user study.\" Proceedings of the 25th International Conference on Intelligent User Interfaces. 2020.\n\n\n[4] Abello, Antonio A., Roberto Hirata, and Zhangyang Wang. \"Dissecting the High-Frequency Bias in Convolutional Neural Networks.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.",
            "summary_of_the_review": "Again, I like the simplicity of the approach and the work points towards an interesting research direction. However, there a many points that are not addressed. This limits the work considerably. The incompleteness makes it impossible to judge whether bilateral filtering makes a real difference in explaining AI. \n\nTherefore, I can not recommend this work for publication.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes that to improve understandability, explainable AI methods for visual inference systems must be visible for humans (the \"human perceptibility principle for XAI\"). To this end, the paper proposes that networks should be \"steered towards focusing on human-understandable cues during training\". Through several demonstrations and a subjective human survey, the paper claims to provide a demonstration that the principle produes explanations that \"better align with human intuition\".\n",
            "main_review": "This paper seems to confuse the concepts of \"visibility\" and \"understandability\". It is trivially true that a visual explanation for a network's decision must be visible for humans (in the sense of being detectable) before it could be understandable. But visibility is only a necessary condition for understandability: explanations could be easily perceived but fail to improve understanding.\n\nThe \"human perceptibility principle\" is that networks should be trained to use human-visible features rather than other features. This may be a valuable idea, and makes intuitive sense, but the idea is not well demonstrated or supported by the results in the paper. Furthermore, I am not sure how extensible it ultimately is: human perception is full of context effects and nonlinearities that quickly make the translation from *image* to *understanding* extremely nontrivial.\n\nTo demonstrate its main claims, the paper focuses on spatial frequency, presenting a very rudimentary literature overview of the human visual system, with jumbled explanations missing key concepts. For example, the key property of the human visual system relevant to the paper's main manipulation is the contrast sensitivity function (Campbell & Robson, 1968; see also Watson & Ahumada, 2016), which is not named or discussed. The contrast sensitivity function provides the window of visibility for spatial variations under ideal conditions; in real images, visibility is far more complex (e.g. effects of masking; e.g. Legge and Foley). \n\nThe controlled experiment uses two image manipulations that, while claimed to be about \"high\" and \"low\" spatial frequency, are actually spectrally broadband (i.e. they contain power at all spatial frequencies). Rather, I believe the key difference here -- and this might actually make the results interesting, but not for the reason the paper claims -- is that one manipulation is spatially localised whereas the other is spatially dispersed. I suspect convolutional networks more easily pick up on spatially-dispersed features (since there is evidence for them at every step of the convolution). So I think this experiment demonstrates something about training networks to detect spatially-localised features (rather than anything to do with spatial frequency per se). I think this explanation can hold also for the qualitative examples in Figures 6 and 7: eyes and mouths also have *localised* high frequency components.\n\nThe human survey shows that people prefer localised explanations over spatially-dispersed explanations. Disappointingly, the survey does not objectively assess whether the new explanations actually help humans to identify the fake image. This would have been relatively easy to do: present humans with 50% real and 50% fake images, ask them to label the image. Now, do this experiment with (1) no explanations, (2) vanilla model and (3) present model. If the localised explanations really help, accuracy will be higher in condition three. For a paper using a similar objective quantification of an explanation method, see one from this year's ICLR: https://openreview.net/forum?id=QO9-y8also-. \n\nMinor quibble: Page 5 states that if the learned features are not within the set of human-perceptible features, then \"the used features are not perceptible by humans and no human-understandable explanation can be generated\". I'm not even sure this is true in principle: individual human-imperceptible features could be combined to yield new features that are visible to humans.\n\n## References\n\nCampbell, F. W., & Robson, J. G. (1968). Application of Fourier analysis to the visibility of gratings. The Journal of Physiology, 197(3), 551–566.\n\nWatson, A. B. and A. J. Ahumada (2016). “The pyramid of visibility.” Electronic Imaging 2016(16): 1-6.\n\nLegge, G., & Foley, J. (1980). Contrast masking in human vision. Journal of the Optical Society of America, 70(12), 1458–1471.",
            "summary_of_the_review": "Training networks with only features falling into the spatial contrast sensitivity function of human vision means that networks would have to use features that humans can in-principle *see*. This makes sense as an idea, but the paper does not provide convincing evidence in support.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose what they call the \"Human Perceptibility Principle for XAI\". They demonstrate a specific institution of this principle on image classification, developing an approach that forces a classifier to learn using features that appear to be more easily interpreted by humans.",
            "main_review": "Strengths:\n* This work tackles an important problem and its direction certainly seems promising.\n* The authors demonstrate a clever specific instantiation of their proposed principle. It's technical experiments seem valid and quite interesting.\n* The results seem somewhat strong and exciting.\n\nWeaknesses:\n* The author's proposed principle is quite similar to the motivation behind concept bottleneck models and much other related work in this space. As the authors discuss in their related work section, there are certainly practical differences between their proposed principle and what existing work does, but the high-level idea (forcing a model to train on human-interpretable features) remains quite similar.\n* There is little discussion or meaningful experiments demonstrating potential drawbacks of this principle. A classic debate in machine learning interpretability is whether making models more interpretable requires sacrificing performance. I would have liked to see the authors present a deeper discussion of how their proposed principle fits in to this debate.\n* While the authors propose a highly generalized principle, they only examine it for one specific task. I would have liked to at least see an in depth discussion of how their principle could feasibly be applied to tasks in other areas of machine learning, particularly in which heatmaps are not a viable explainability mechanism.\n* The human evaluation felt pretty limited. The wording of the questions seems quite odd, and the investigation of their approach's effectiveness was quite surface level. For instance, there was no investigation that aimed to understand the importance of the specific features their approach showed in the heat map or how much coverage their model achieved of the most important features. Was there any important information lost by using their approach versus today's standard approach?",
            "summary_of_the_review": "The authors present a clever interpretability approach, but I have concerns about their higher level claims and human evaluation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}