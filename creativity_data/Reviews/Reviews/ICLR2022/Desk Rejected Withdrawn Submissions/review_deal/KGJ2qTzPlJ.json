{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper analyzes whether transfer learning can be made provably robust. The paper attempts to achieve provable robustness guarantees on the transfer-target domain by controlling the Lipschitz constant of the last few layers of the network (the layers after the pre-trained feature extractor).\n",
            "main_review": "The authors propose using a new classifier after the pre-trained feature extractor that has a controlled Lipschitz constant. This is similar to prior works that the authors mention, but the authors use L_infinity normalization instead of spectral normalization or orthonormal weight matrices. The authors claim that this alleviates some of the problems associated with the prior approaches.\n\nHowever, I am quite confused, either by the writing (maybe there are key typos) or the math. In particular, I think some of the equations in the paper are incorrect as they are written. First, inequality 1 is not a proper application of Cauchy-Schwarz; the left-hand-side should be an inner product, and the L_infinity norm is not an inner product. As written right now, I think there are very simple counterexamples to inequality (1), which seems to be a key component of the rest of the analysis in the paper (it directly leads to inequality 2, which is used to justify why L_inf normalization ensures Lipschitz-ness and therefore robustness). Here is the counterexample: set W^(T(i)) = [2 1; 2 1] and (z’-z) = [2, 1]. Then the LHS has L_inf norm 5, whereas the RHS is 4, and we get 5 <= 4. Perhaps the authors meant to change some of the L_inf norms to L_1/L_2 norms or other norms?\n\nFurther along in the paper, the authors suggest a two-step process for finding adversarial inputs. I also would like to know, why do we first find an adversarial representation, then find an input matching that representation, rather than performing a search for an adversarial input directly?\n\nLater, in Table 2, is the column p_z always going to equal 100 if you are trying to find adversarial representations? After all, it seems like any distance change is allowed, and is indeed measured in the next column.\n\nI do appreciate that the authors did extensive experiments. With that said, I was confused about the setup of using a multi-layer classifier vs. a 1-layer classifier at the end. Perhaps investigating this avenue further could also be fruitful, as the 4-layer classifier appears to result in better results even for standard accuracy.\n",
            "summary_of_the_review": "I recommend a reject. The math appears incorrect/unclear, and I am also unsure of what the main takeaways/conclusions are as a result.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed building provably robust models on target datasets by smoothing the feature extractor (on the source dataset), and using a 1-Lipschitz constrained classifier to obtain certified predictions on the target dataset. The paper argues that this gives better guarantees than smoothing the full model on the target dataset. The paper conducted experiments on various transfer tasks to verify their claims.\n",
            "main_review": "### Strengthes:\n+ The introduced method is sound and novel. The paper studies the problem of guaranteeing transfer of robustness from the source task to the target task for the first time.\n+ The paper includes details about the experimental setup.\n\n### Weaknesses:\n+ The paper is not very well written and can be improved a lot. It was hard to follow.\n+ The presentation of some results is confusing. For example, I am not sure what the takeaways of Table 1 and 2 are even after reading the text.\n+ I found some key parts of the paper confusing. I understand that the paper is trying to sell provable transfer as an alternative for empirically transferring the robustness since we cannot really guarantee the latter. But what I don’t understand is what is being presented in S4. I think S4 is trying to argue that: “combining a fixed Lipschitz constant with adversarial training is not strong enough to make this transfer learning framework sufficiently robust.” So is it about the weakness of empirical defenses? Or about them not guaranteeing any robustness?\n+ The considered transfer tasks are toy tasks. I wonder why more practical transfer tasks such as ImageNet to CIFAR10 were not conducted? The introduced method relies on randomized smoothing of the pretrained model, which has been shown to easily scale to ImageNet in Cohen et al. and follow up work.\n\n### Detailed comments and questions:\n+ It is not clear what the setting in Table 1 is. Is it a robust transfer setting? Or standard transfer? What is “WN”? (Okay I can see WN is defined in the following paragraph. I think it is useful to add it to the caption of the table too)\n+ I found S4 to be really confusing (see my above comment). In addition to what I have mentioned above, I have the following questions. When you are checking if a representation is reachable from the input space, do you constrain the input to be within a bounded region around the clean datapoint? Otherwise even if you find an input that maps to the “adversarial feature”, this input is not adversarial as it might be “far” from the clean input. So I don’t understand what benefit the Lipschitz classifier is giving you is the backbone does not give guarantees?\n+ In S5, it seems that if you smooth the feature representation and then “propagate the guarantees” to the Lipschitz classifier, you obtain better guarantees than smoothing the whole model on the target dataset right? So does this mean that doing this two step verification process on a non-transfer task (e.g. ImageNet to ImageNet) will give better results than randomized smoothing applied directly on the full ImageNet model?\n\n### Small fixes:\n+ Last sentence in S3: f(h(.)) -> h(f(.))\n\n",
            "summary_of_the_review": "Overall, while I think the idea is novel and potentially gives insights into how to make transfer learning probably maintain robustness, the current state of the paper is not yet ready for publication in my opinion. I would encourage the authors to improve the presentation of the paper, and address the above questions I asked.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies how adversarial robustness can be transferred in the transfer learning setting. ",
            "main_review": "Strength:\n1. This paper provides thorough numerical experiments and illustrates that enforcing a given Lipschitz constant would help the adversarial robustness in the target domain.\n\nWeakness:\n1. This 9-page paper has 3 pages of tables, which may occupy too much space.\n2. The title says \"provably\", but there are no theory insights are provided in this paper. \n\n\nMinor issues:\n1. there is a typo on page 2. \"The prediction for input x is obtained as x = h(f(x)).\"",
            "summary_of_the_review": "The title is a bit misleading with the word \"provably\", which is typically followed by some theoretical insights in the paper. I am not an expert in numerical experiments, so my assessment for this paper will be not accurate. It would be better to seek the opinions from other reviewers who are more experienced in numerical experiments. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
        },
        {
            "summary_of_the_paper": "This paper studies how to transfer robustness to the target domain, and provide light theory regarding guarantees in l-infinity norm under single-layer neural networks. The guarantee for multiple layers NN is by randomized smoothing. ",
            "main_review": "The paper is clearly written. \n\nHowever, this paper lacks novelty. First, transfer robustness has been extensively studied, for instance in Shafhi et al. cited by the authors. Meanwhile, using Lipchitz constrained NN to improve robustness is also a well-studied topic. The theoretical guarantee for one layer classifier is quite trivial and for multiple layers NN, using randomized smoothing can only guarantee certified robustness and also has been proposed by Cohan et al. a long time ago.\n\nI really did not see anything novel or theoretical challenging in this paper. ",
            "summary_of_the_review": "Given lack of novelty, I do not think this paper is ready to be published.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The main concern is that this paper lacks novelty neither in methodology or theory.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}