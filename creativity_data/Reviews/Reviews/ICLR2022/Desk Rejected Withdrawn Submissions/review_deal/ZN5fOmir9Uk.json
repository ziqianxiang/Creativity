{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a completely new dataset for studying emergent communication. It is light-weighted to sample and large enough to prevent overfitting. The authors try a few models proposed by prior work on their dataset. \n",
            "main_review": "Pros: \n\n1. The synthetic dataset could be a good platform to train and evaluate emergent communication models. \n2. Some of the experiment results are interesting, although the authors didn't claim their hypotheses clearly.\n\nCons:\n\n1. It is not clear to me what the goals of the empirical studies in this paper are. For example, what is the hypothesis on different sender architectures? Why models performing better on TexRel than Shapeworld means that TexRel is better?\n2. This dataset can be used by other researcher to fast prototype their emergent communication models. However, it is not clear why this is different from Shapeworld?  Shapeworld repo on Github can also be used to generate larger dataset. ",
            "summary_of_the_review": "To summarize, the contribution of this paper is mainly on the creation of this dataset. However, it is not clear why previous proposed packages cannot address the problem of overfitting. The empirical comparison in this paper is also aimless. It is hard to draw conclusion from the experiments. This paper doesn't meet the standard of ICLR papers. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new dataset TexRel, for studying emergent communication in a referential task setting. The paper compares different sender/receiver model architectures on this dataset and compares against related datasets. The paper also introduces new metrics to measure language expressivity and consistency as well as a modified version of an existing metric for compositionality.",
            "main_review": "Overall, introducing a low cost to train dataset that is also large enough that emergent communication models don't overfit is a good contribution.\n\n**Strengths:**\n\n- The paper introduces a new relations dataset that can be used to study emergent communication in a referential task setting. This dataset uses textures for the images as opposed to solid-textured shapes to improve learnability for CNNs on this task.\n- The paper provides examples of settings where existing metrics for compositionality fails which is interesting to see.\n- The paper extensively investigates several different sender/receiver model architectures on this dataset which is nice.\n- I really liked the fact that the paper proposes new metrics to measure consistency and expressivity and compares these on some related datasets.\n\n**Weaknesses:**\n\nI have included actions that can be taken to strengthen this paper. If the actions are addressed, I would be happy to increase my score.\n\n- **Novelty and Motivation:**\n    - The novelty and necessity of this dataset is somewhat unclear. [1, 2] already provide generators to build relations dataset.\n        - **Action:** Contrast TexRel to [2] which has a dataset generator of a similar kind which can in theory be used in place of this.\n- **Introduction:**\n    - The paper argues that the symbolic datasets [3] often used in the emergent communication literature is insufficient and recommends the usage of this new dataset instead. However, this is not well supported. Why would we be unable to study the same/similar hypothesis about compositionality, expressivity, generalization etc. of emergent languages and draw the same conclusions? Section 5.5 for example, has some evidence showing that conclusions drawn using TexRel contradicts the relationship between generalization and compositionality from prior work. More evidence would make this claim stronger.\n        - **Action:** Provide further supporting evidence for using TexRel instead.\n- **Section 3.2:**\n    - The paper includes some comparisons to existing metrics for compositionality and introduces new metrics for consistency and expressivity. Including other existing metrics used to study emergent communication like those introduced in [4, 5] would strengthen the paper.\n        - **Action:** Include other metrics to investigate emergent communication on this task.\n- **Section 5:**\n    - Most experiments are only run for a fixed number of iterations. In particular, to draw conclusions in section 5.4, the models would need to be trained till convergence.\n        - **Action:** Train models to convergence or show training plots for the different tasks and models.\n    - This section is difficult to follow as experiments have very different themes.\n        - **Action:** Reorganize the experimental section such that the empirical contributions are easier to follow.\n- **Typos:**\n    - **Section 2.4:**\n        - Andreas showed ... → Andreas et al. (2018) showed\n    - **Section 3.1:**\n        - ... has $n_{v}al$ ... → ... has $n_{val}$ ...\n\n[1] Andreas, J., Klein, D., & Levine, S. (2017). Learning with latent language. *arXiv preprint arXiv:1711.00482*.\n\n[2] Kuhnle, A., & Copestake, A. (2017). Shapeworld-a new test methodology for multimodal language understanding. *arXiv preprint arXiv:1704.04517*.\n\n[3] Lazaridou, A., Hermann, K. M., Tuyls, K., & Clark, S. (2018). Emergence of linguistic communication from referential games with symbolic and pixel input. *arXiv preprint arXiv:1804.03984*.\n\n[4] Lowe, R., Foerster, J., Boureau, Y. L., Pineau, J., & Dauphin, Y. (2019). On the pitfalls of measuring emergent communication. *arXiv preprint arXiv:1903.05168*.\n\n[5] Dagan, G., Hupkes, D., & Bruni, E. (2020). Co-evolution of language and agents in referential games. *arXiv preprint arXiv:2001.03361*.",
            "summary_of_the_review": "This paper proposes a new dataset TexRel, for studying emergent communication in a referential task setting. The paper compares different sender/receiver model architectures on this dataset and compares against related datasets. The paper also introduces new metrics to measure language expressivity and consistency as well as a modified version of an existing metric for compositionality. The paper's weaknesses outweigh the strengths at this time. As such, I do not recommend acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents TexRel, an artificial image-based dataset that captures emergent communication of relations between objects, which is a richer meaning space than typical image-based referential games that require communication of only a single object. TextRel correposnds to simple images of \"shapes\" represented not by borders, but by \"textures\" of pixels. A concept in the game is a spatial relation between two such shapes, e.g. \"texture 1 above texture 2\". The goal of two agents playing the game is for the sender agent to identify the latent concept given positive and negative examples of the concept and transmit a message to a receiver, who is tasked with identifying further images encoding the concept.\n\nThe most similar dataset in this space is ShapeWorld, which supports a wide variety of concepts beyond just relations, but authors claim that TexRel is a much larger dataset and is more lightweight, enabling faster \"green\" experimentation.\n\nThe authors investigate a wide variety of sender and receiver architectures on this game, finding that a simple sender model that stacks positive and negative images together is most effective at developing a compositional language. They then use a variety of metrics to measure compositionality of the learned language, including the typical topographic similarity metric. However, they point out counterintuitive behavior and limitations with this metric, namely that topopgrahic similarity does not control for language expressivity. To correct for this, the authors suggest metrics based on precision and recall with the ground-truth clusters, and a modified variant of the recently-proposed TRE metric.",
            "main_review": "# Strengths\n\n- New dataset seems like a useful, lightweight way of prototyping models for emergent communication, though more explanation for why one should use TexRel over ShapeWorld is needed (see weaknesses).\n- I appreciate the analysis of how changing environment complexity results in unintuitive changes to language compositionality, and more generally pointing out of the limitations of topographic similarity for measuring compositionality. This has been a de facto way of measuring compositionality and I have also had suspicions that it may not actually be measuring what we care about. The explanation given in the paper is quite intuitive.\n- Using clustering precision and recall as ways of evaluating the systematicity of the language and/or alignment with the ground truth concepts is a natural contribution. This looks quite similar to proposals of using entropy and mutual information to measure language systematicity, though, and the paper could use some discussion on this connection ([Lowe et al., 2021](https://arxiv.org/abs/1903.05168)) .\n\n# Weaknesses\n\n- My main concern with this paper is that the benefits of using TexRel over ShapeWorld is unclear, and so a skeptic might ask why we need TexRel in the first place.\n    - TexRel seems to excel at testing relations between objects, but that seems to be (currently) the only supported kind of semantic concept being proposed here. One of the purported benefits of the ShapeWorld (Kuhnle and Copestake, 2018) dataset is the ease of generating concepts that align with a wide variety of rich linguistic phenomena beyond just spatial relations: quantification, logic (e.g. *red or blue*), etc. The paper would be stronger if authors propsoe to extend TexRel to support such concepts, or alternatively explain why ShapeWorld by itself is insufficient for developing datasets with such richer concepts.\n    - Another purpoted benefit of the TexRel dataset is larger number of training examples included, but see \"Some issues with experimental evaluation\" below—can't we just generate more data for ShapeWorld? In fact, isn't the number of training examples for both ShapeWorld and TexRel arbitrary given that they can be generated fairly cheaply, on the fly?\n    - While TexRel may be cheaper to run than ShapeWorld, is this difference really meaningful given today's compute abilities? ShapeWorld is also fairly low resolution 64x64 images and are also sparse in that most of the inputs are 0s (black background). Is TextRel really much more \"green\" than ShapeWorld?\n    - The end-to-end results (Table 6) suggest that TexRel is an *easier* task than ShapeWorld, since ShapeWorld agents obtain chance accuracy on the task. Is this a desired attribute of TexRel? Naively, wouldn't we want harder tasks, which serve as more difficult benchmarks and targets for developing compositional languages?\n- Limited technical novelty\n    - I'm primarily evaluating this paper based on the TexRel dataset contribution and the new metrics for evaluating compositionally, as I don't see significant novelty in the architecture search. If the paper is trying to make claims about the suitability of various sender/receiver architectures, I'd expect more of an investigation of how these architectures generalize across multiple datasets, including ShapeWorld but possibly other non-relation datasets. Otherwise, right now I see this as primarily an architecture search within a single dataset whose conclusions may not generalize externally.\n- PTRE is somewhat odd\n    - It's unclear to me whether simply dividing TRE by cluster precision is the right way of better thinking about TRE. This does not seem like a ratio with meaningful upper and lower bounds (unless there is some interpretation I'm missing here), so presenting it as a ratio seems to obscure the original quantities. Wouldn't it be better to simply present TRE and cluster precision separately, to allow us to more precisely examine the tradeoff between compositionality and expressivity? There's no need for all of language compositionality to be captured in 1 metric (and perhaps that is precisely the problem that topographic similarity suffers from).\n- Some issues with experimental evaluation:\n    - The dataset of Andreas et al. (2018) is generated from an artificial data generating process (Kuhnle and Copestake, 2018), where it should be possible to generate more than just the 9k training images (though I understand if generating additional data isn't doable due to reproducibility issues). I feel like experiments should be run with a consistent number of images across both TexRel and ShapeWorld, otherwise the comparison doesn't seem particularly fair.\n    - A large experimental weakness: the methodology of \"optimizing for 5 minute training time\" seems arbitrary and undesirable. Authors should justify this design choice, but initially it's really not clear to me how realistic/useful this constraint is—I can't imagine us needing to train models for emergent communication in resource constrained scenarios, let alone with a budget of minutes. I would strongly encourage authors to adopt the more typical training and evaluation pipeline where we train the models to convergence, doing early stopping on a held-out validation set. More complex models may require more flops and wall clock time but may be more sample efficient, for example. Or they may take longer to learn, but when trained would demonstrate higher communication performance.\n\n# Questions\n\n- Any insight into how the languages change depending on the size of the target and distractor sets? At low sizes it seems like the distractor images placed into an image could by chance encode some spurious relation that could be communicated instead. Do you guard against this in the data generating process?\n- Did you consider a setting where the prototype architecture embeds both positive examples into a prototype, and negative examples into a negative prototype?\n\n# Minor\n\n- I feel like the usage of the term \"green\" in the title is a bit ambiguous and not commonplace. Given environmental considerations etc. nowadays such terminology should be more commonplace, but right now it feels ambiguous. Would prefer something like \"lightweight\".\n- The distinction between symbolic and non-symbolic (image) inputs is a little bit blurred once we are using images of this level of simplicity :) - I didn't see the dimensionality of the images reported anywhere.\n- \"In our own work, we wish to represent relations, so we extend to the more general case of multiple labeled sender examples\" - one citation that may be relevant here: [Mu and Goodman, 2021](https://arxiv.org/abs/2106.02668). Also uses a prototype architecture for both positive **and** negative examples, and so is relevant for the earlier question on architectures.",
            "summary_of_the_review": "To summarize I'd like to comment inline on the paper's list of contributions:\n\n- propose a new dataset, TEXREL, which provides a playground for emergent communications\n    - uses non-symbolic inputs, i.e. images\n    - fast to train on\n        - *Is it that much faster than ShapeWorld? Would be nice to actually show this.*\n    - is much larger than comparable existing emergent communication datasets\n        - *Isn't this arbitrary given that both datasets allow us to generate as much data as we want for a task?*\n    - provides a high dimensional underlying meaning space\n        - *Is the meaning space actually richer than ShapeWorld? The number of meaning dimensions n_att is the same, right? Are there more unique colors/textures?*\n- we provide extensive baselines and empirical studies using TEXREL:\n    - compare potential sender agent architectures\n        - *Generalizability of these experiments outside TexRel seem limited.*\n    - compare TEXREL with Shapeworld\n        - *Some of the comparisons here seem unfair w.r.t. data size for TexRel and ShapeWorld. What is the takeaway that TexRel is slightly easier than ShapeWorld in end-to-end communication?*\n    - examine the effect of meaning space size and dimensionality on compositionality\n        - *I appreciate pointing out the limitations of topographic similarity here, suggesting that we need other ways of measuring compositionality, and the metrics proposed next are decent alternatives.*\n    - provide a case-study of using TEXREL in place of symbolic inputs, for fast experimentation\n- propose new metrics:\n    - clustering precision and clustering recall as measures of language expressivity and\n    consistency, respectively\n        - *This is a nice contribution, though it seems similar to using entropy/MI to measure language compositionality.*\n    - PTRE: derivative of TRE which assigns low compositionality to languages having low\n    expressivity\n        - *As I mentioned, I find this a rather odd presentation, and think it's more useful just to present both measures (TRE and precision) separately.*\n\nOverall, I think this is a nice dataset which could be useful in future studies of emergent communication, but I think right now the novelty is limited, particularly as to why we need TextRel compared to ShapeWorld. Finally, some fairer and more reasonable experimental evaluations are needed before I would like to unequivocally recommend acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes a new dataset with the aim of avoiding overfitting when learning emergent languages. It extends a previous dataset by Andreas el al. 2018 which had limited number of training images and thus allowing agents to overfit. The authors take an alternative approach where instead of images, low dimensional 'textures' are used that are easy to encode using convolutional neural networks. This allows for faster training and less attention to the outlines of the object which are irrelevant to solving the given task. They also propose new metrics to evaluate compositionality in these emergent languages.",
            "main_review": "The issue identified in this paper is quite relevant and needs to be addressed for deriving solutions to learn compositional emergent languages. However, the approach taken is not consistent with the issues and lack motivation and comparison to the previously defined metrics. This relates to the novelty of this work since most of the proposed metrics and hypothesis have already been established in previous work.\n\nComparison of the proposed hypothesis and metrics with prior work has been vastly ignored. For eg, the effect of meaning space on compositionality has already been well explored in prior literature (Chaabouni et al. 2020, Resnick et al. 2020) Furthermore, the authors use topographic similarity to compare the dataset performance besides the accuracy. Given that previous work (Chaabouni et al. 2020, Andreas 2018) has already shown limitations of using such a metric and proposed alternatives that are open-sourced and easy to implement, I do not understand why the authors chose to evaluate using outdated metric. Another missing work is that of Slowik et al. 2020 that focuses on learning structural relations in emergent languages. The precision and recall metrics have already been explored in Resnick et al. 2020. \n\nThe emergent communication task used in the paper involves end-to-end learning using a differentiable communication channel. Although not limiting, to allow for a fair comparison with Andreas 2018, it would be be better to use separate training for both Sender and Receiver as is also done in prior work using reinforcement learning. It would also help bolster the claim that the findings from the proposed dataset are robust to different training methods.\n\nThe choice of vocabulary size and utterance length seems to be quite ad-hoc. Do the findings transfer to changes in these parameters?\n\n- Resnick et al. 2020. Capacity, bandwidth, and compositionality in emergent language learning.  \n- Chaabouni et al. 2020. Compositionality and generalization in emergent languages.  \n- Choi et al. 2018. Compositional obverter communication learning from raw visual input.\n- Slowik et al. 2020. Exploring Structural Inductive Biases in Emergent Communication.",
            "summary_of_the_review": "The paper extends a prior dataset to overcome the issue of overfitting when learning emergent languages. The paper lacks novelty in various sections and some of the choices made are ad-hoc. I would encourage the authors to do a brief literature survey and clearly make the contributions of this paper separate from prior work. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes TexRel, a new image-based dataset for the study of emergent communication. It is designed for a sender-receiver setup in which the sender has to encode labelled train images as a linguistic utterance, and the receiver has to predict test image labels based on the received utterance. The dataset is larger than similar existing datasets, provides a higher dimensional meaning space, and enables efficient training. The paper presents experiments with TexRel that demonstrate how it can be used to study different aspects of emergent languages. Furthermore, new metrics for language expressivity, consistency, and compositionality are proposed, and used to evaluate models with TexRel.",
            "main_review": "Strengths:\n\n1. The TexRel dataset would be a valuable contribution to a relatively new line of research (deep learning for emergent communication). It is much bigger than existing datasets, while remaining relatively “green”.\n\n2. The cluster-based metrics and PTRE are useful for studying emergent languages. Recently there has been an increasing awareness of the limitations of existing metrics for compositionality, so the new metrics address a known problem in the literature.  \n\n3. Overall the paper is clear and well written. It ties together several views on emergent communication research, and does so without confusing the reader.\n\n##########################################################################\n\nWeaknesses:\n\t\n1. The existing metrics for compositionality are introduced (in section 2.2.1) as straightforward measures of compositionality, but this is not in line with recent findings in the literature. \n\n    (1.a) Holdout accuracy has been shown to sometimes have no correlation with compositionality e.g. by Chaabouni et al. (2020), “Compositionality and Generalization in Emergent Languages”.\n\n    (1.b) It should be made clear that TRE only works as a measure of compositionality as defined by a prespecified compositional structure, and that this is a limitation.\n\n2. One of the major claims of the paper is that some of its counter-intuitive results cast doubt on the validity of topological similarity as a metric for compositionality, but I’m not sure about this. In particular, the findings of section 5.4 are dubious. \n\n    (2.a) Is it not possible that the topological similarity decreases in graph 4(a) simply because the task difficulty increases with higher meaning dimensionality? The same pattern is visible in the acc_same graph, so isn’t it just that the models are struggling to train effectively because it’s a harder task? If you want to stick to your claim that topological similarity does not reflect ground truth compositionality, you have to provide evidence (or argue convincingly) that the decrease in graph 4(a) is not because the task difficulty increases.\n\n    (2.b) All the models for these experiments were trained for 5k training steps, but it’s possible that models trained on higher meaning spaces required more training. It would be a fairer comparison to compare all these models until trained to completion (e.g. based on some validation criteria). Perhaps then the reported metrics wouldn’t decrease as the meaning dimensionality increases.\n\n    (2.c) Last paragraph of section 5.4 - this claim doesn’t seem obvious based on figure 4(d). In fact, it looks untrue for all but n_att = 3, which only has a few data points. I would omit this paragraph, or revise it, unless you have stronger evidence. \n\n##########################################################################\n\nMinor comments:\n\n1. Figure 1: shouldn’t the leftmost image label be true?\n2. Section 1 last paragraph before listed contributions: The Khurshudov (2015) citation is for a blog post. It would be preferable to cite a peer-reviewed paper here, since the claim is the main motivation behind adding texture to the dataset.\n3. Section 2.4 paragraph 1: Andreas (2018) used GRUs, not LSTMs.\n4. Section 3.1 paragraph 2 line 3: fix subscript text n_{val}.\n5. Remove (or shorten) mentions of datasets in the related work section that are not relevant to emergent communication. This will free up space for the rest of the paper.\n6. Section 4 paragraph 2: TexRel has 25.6 million images, not 2.5 million.\n7. Section 5.3 paragraph 1: change “‘Learning with Latent Language’ provided (?) with ‘Learning with Shaped Language’“ to be more readable.\n8. Section 5.3 paragraph 2: Figure 6 -> Table 6.",
            "summary_of_the_review": "The main contributions of the paper are novel and well motivated. The TexRel dataset offers several benefits over similar existing datasets and the newly proposed metrics seem useful. However, some of the authors’ claims (specifically those related to compositionality) do not fully take into account the limitations revealed by existing works. Additionally, some of the experimental findings are dubious, given the limited results. The paper would be improved by expanding on the known challenges and limitations in studying the properties of emergent languages, and taking these into account when analysing results.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}