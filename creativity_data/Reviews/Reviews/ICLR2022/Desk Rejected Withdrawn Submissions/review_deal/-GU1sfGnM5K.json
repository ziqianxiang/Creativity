{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper \"A Reinforcement Learning Environment for Mathematical Reasoning via Program Synthesis\" formulates the a mathematics dataset as a reinforcement learning problem and applies standard DQN to the problem.\n\nThe paper makes the assumption that seq2seq models are not adequate to solve mathematics datasets. And then it suggests to instead formulate the problem as a reinforcement learning problem. The reinforcement learning environment, however, simply asks the agent to produce sequence that is then evaluated to get the reward (1 if the result matches the expected result, and 0 otherwise). \n\n\n",
            "main_review": "I see several problems with this approach. First of all, the assumption on which this paper is written contradicts the recent literature on the use of Transformer models for mathematical reasoning tasks. With appropriate training, these models appear to be able to solve interesting mathematical problems. For example, see:\n- Lample&Charton: Deep Learning for Symbolic Mathematics\n- Rabe et al.: Mathematical Reasoning via Self-supervised Skip-tree Training\n- Polu&Sutskever: Generative Language Modeling for Automated Theorem Proving\n- Hahn et al.: Teaching Temporal Logics to Neural Networks\n\nMy second problem with the paper is that the reinforcement learning approach is also just a seq2seq model, which is what the approach seeked to replace. There is no intermediate reward, and there is no evaluation of partial expressions. Afaics, this is a seq2seq model with a binary loss function. The \"success\" observed in this paper may just be because of significant help that is given by defining the action space and masking invalid actions etc.\n\nThird, this paper essentially suggests to translate mathematical expressions into a symbolic mathematics solver. But the dataset is rather synthetic. Looking at the examples given in the paper, I could easily write a program that translates these expressions into Mathematica or other solvers without the need for any learning whatsoever.\n\nFourth, I do not agree that we should call this \"program synthesis\", as the paper title suggests.",
            "summary_of_the_review": "I do not see that this reinforcement learning approach improves over seq2seq models as the authors claim. The paper also misses related literature.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents an RL environment for the DeepMind Mathematics Dataset. The environment presents a program synthesis problem that has been widely studied previously in formal methods literature mostly in a whitebox setting. The RL formulation presents an alternative formulation. To set up the environment, actions correspond to addition of operator or input in a discrete computation graph. Reward is based on the correctness of the answer. A baseline method based on Double DQN is presented. \n\n",
            "main_review": "Strengths\n\n- A non-whitebox setting for program synthesis has also been studied in formal methods as a class of techniques called oracle-guided inductive synthesis [OGIS] (see for example, https://dl.acm.org/doi/10.1145/1806799.1806833 ). This formulation establishes synthesis as a query/response exchange between an oracle and a learner. But the learner is \"scripted\", that is, fixed to use a specific learning strategy. The presented reinforcement learning formulation adds further automation and will help make program synthesis require even less human guidance. It would be useful to discuss richer interfaces between the learner and the oracle, that is, a richer class of actions that include partial evaluation of programs. \n\nWeaknesses:\n\n- The use of just one dataset makes the overall presented environment of limited value, particularly because this dataset itself is limited. While the presented work would still push the state of the art, the reviewer is not entirely convinced that the overall contribution of the paper matches the expectation from an ICLR paper.  Given the applied/experiment-platform nature of the contribution, the availability of this gym environment is critical to have impact from this paper. But the open-source links are currently broken making it difficult to evaluate the state of the implementation :\n-- This reviewer cannot access  https://anonymous.4open.science/r/math_prog_synth_env-D46E/ \n-- This reviewer cannot access https://anonymous.4open.science/r/dm_math_solvers-C310/\n\n- The paper mentions that \"The full set of hyperparameters used to produce the results reported here is provided in the code (https://github.com/joepalermo/dm math - solvers/blob/master/hparams-for-paper.cfg).\" Is this cfg file being reused from some previous work? If so, could you please discuss this relationship?  If not, this might potentially violate anonymity. ",
            "summary_of_the_review": "The paper presents a gym environment that could facilitate RL methods for program synthesis. The limited novelty in the paper and the rather straightforward contributions make the reviewer less excited about the paper, despite its possible utility in spurring a nascent area of research. The value of this paper depends heavily on whether this environment is open-source, friendly to extend and use, and available to community. The provided links are broken. Also, the use of cfg file on a non-anonymous repo needs to be clarified. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Other reasons (please specify below)"
            ],
            "details_of_ethics_concerns": "The paper mentions that \"The full set of hyperparameters used to produce the results reported here is provided in the code (https://github.com/joepalermo/dm math - solvers/blob/master/hparams-for-paper.cfg).\" \n\nThe reviewer has reached out to the authors to explain this cfg file being hosted on a non-anonymous repo. \n\nIs this cfg file being reused from some previous work? If so, could you please discuss this relationship?  If not, this might potentially violate anonymity. \n\nThis is not any research integrity or other serious ethics concern. This could just be an inadvertent error on the part of the authors. But if there has been an error in not maintaining anonymity, the reviewer would request AC/SAC/PC to decide what needs to be done. ",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper treats mathematical question answering problems as program synthesis problems where the task is first to generate a program and then execute this program to answer the mathematical question. The authors propose a reinforcement learning approach to learn a policy that builds the program one step at a time. The main benefits of such an approach are interpretability and the ability to incorporate domain knowledge through user-desired program components. The authors present potential challenges with such an approach (combinatorial explosion and noisy rewards) and present a decent baseline using Double DQN.",
            "main_review": "**Strengths**\n\n — The idea of learning a program to answer the question is interesting (although not new).\n\n— The proposed reinforcement learning approach with techniques to avoid combinatorial exploration using type information is a promising start. \n\n**Weaknesses**\n\nThe paper severely lacks in evaluation and does not place the work with respect to other prior works. \n\n— There is no discussion about any related works. In particular, there is a wide body of works that try to learn a program first and then execute the program to answer a question (broadly under the domain of semantic parsing). While most of these domains have annotations for the intermediate programs in the dataset, there are also prior works without this annotation. For e.g. a closely related work is [1]. In addition, there are several related works on using reinforcement learning techniques to do program synthesis (for e.g. [2] and [3])\n\n—This paper needs a more thorough evaluation by testing it on multiple benchmarks and comparing the results on various baselines. Even the current experiments are very ad-hoc. For example, why do the authors consider only uncomposed modules? The paper gave the impression that the approach is proposed to handle compositions. In table 1, what are the corresponding numbers for an end-to-end neural approach? The paper needs more statistics regarding the benchmarks — for e.g. the lengths of the programs in each module. And finally, experiments to measure the bottlenecks of the proposed approach will strengthen the paper (e.g. how does performance change with the length of programs, with the number of components in the DSL, etc.)\n\n**Other comments**\n\nThe authors raise the issue with noisy rewards, but it does not look like the authors have a solution to that problem. \n\n[1] Chen, Xinyun, et al. \"Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension.\" International Conference on Learning Representations. 2019.\n\n[2] Chen, Xinyun, Chang Liu, and Dawn Song. \"Execution-guided neural program synthesis.\" International Conference on Learning Representations. 2018.\n\n[3] Ellis, Kevin M., et al. \"Write, execute, assess: Program synthesis with a repl.\" (2019).",
            "summary_of_the_review": "Due to the lack of proper evaluation and discussion about related works, the paper is not yet ready for publication. It is a promising direction, and I hope the authors will use the feedback to improve their paper in the future. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Other reasons (please specify below)"
            ],
            "details_of_ethics_concerns": "The paper has a non-anonymous link to the code and has an acknowledgements section that could potentially reveal authors' info. ",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}