{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a mixture-of-expert method to tackle the content hallucinations in abstractive summarization. They train their factual experts with different training data subset, and with RL rewards based on different metrics. Then, they combine the experts with the BART\nmodel through weights or logits ensembling. They conduct experiments on XSUM and CNN/DM datasets. Different metrics show that their MoFE method improves over the BART baseline, while they might lose some ROUGE scores slightly. ",
            "main_review": "Strengths:\n1. The proposed mixture-of-expert method is well-motivated for the problem, and the author tried a pretty comprehensive set of experts trained with different dataset or objectives.\n2. This work employs a very comprehensive set of automatic evaluation metrics for evaluating factuality, and the results are supportive for the claims.\n\nWeaknesses:\n1. The proposed framework - MoFE, can be regarded as an model-ensemble method, which itself lacks enough novelty. And the method of training each expert is also not new, either.\n2. Regarding the baseline, I believe it's unfair to compare MoFE with the BART single model. Instead, a simple baseline the author should try is to train equally number of BART models with different seeds and combine them for summarization. ",
            "summary_of_the_review": "The mixture-of-expert approach is not novel enough, and the baselines are not fair.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the important problem of reducing hallucinations in abstractive summarization by using mixture of factual experts. As first step, the authors use entity overlap and  dependency arc entailment to capture extrinsic and intrinsic hallucination. Six experts are trained using reinforcement learning to optimize entity precision, recall and DAE on model based pivot summaries or reference-based summaries, respectively. The experts are then mixed by either weights or logits and tested on XSUM and CNN/DM task. The models show improvement on DAE and entity based metrics, as well as QA-based factuality metrics and Bertscore. Small drop of Rouge is also observed. ",
            "main_review": "====== strengths ======\n\n(1) The work is well motivated and the idea of using different experts to capture different type of factual errors to improve abstractive summarization is theoretically sound and interesting. \n\n(2) The study on performances with reference based and model based pivot summaries are interesting, and can help the community understand how factual quality of training data impact downstream performances. \n\n(3) The paper is well written.\n\n====== weaknesses ======\n\n(1) This work designed mainly two type of experts, DAE based one, and NER based ones, to capture different type of errors, intrinsic or extrinsic errors. However, in table1 and table2, the most important experts are DAE, and then NER-R, while NER-P is found to have negative impact on the results. Moreover, the authors agree that DAE can effectively capture many extrinsic errors, even though it is chosen to measure/learn intrinsic consistency. Thus, most of model improvement coming from DAE based models. \n\n(2) For DAE, NER-P and NER-R, which one of reference and model based experts are chosen?\n\n(3) In table 4, NER-R results are missing and why the bart model results are the same for training using all data and filtered data?\n\n(4) In equation (1), the reference summary is used as y_hat for fact recall-based metrics. What is used as y_hat for NER-P experts?\n\n(5) How does the model perform with a weighted rewards of DAE, NER-P, NER-R and similar RL based training?\n",
            "summary_of_the_review": "The paper is well motivated and clearly written. However, the major results does not support the claim that different factual experts reduce intrinsic or extrinsic respectively. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a method (MoFE) to alleviate the hallucination problem in abstractive summarization models, specifically extrinsic and intrinsic hallucinations.\nThe proposed method trains different factual expert models with REINFORCE objective that optimizes towards the factual evaluation metrics (rewards). They also add a KL-divergence term to trade-off between mode collapse and exploration.\nThe final model is an ensemble of the original pre-trained summarization model and expert models.\nThe proposed method is evaluated on a variety of factual evaluation metrics and the reduction in hallucination is demonstrated while largely preserving the summarization performance evaluated by Rouge scores.",
            "main_review": "**Strengths**:\n\nAuthors have conducted very extensive experiments to evaluate the model on various metrics, which shows a more clear picture of how the model performs through the lens of different metrics. The experimental setups are also generally clear to me, in addition, the paper is clearly written.\n\n**Weakness**:\n\n(1) Generally speaking, the method presented is relatively complicated and it seems that the method is not easily scalable to a different domain or a new dataset as it depends on many other hallucination evaluation systems and is susceptible to cascaded errors. The method needs to filter dataset and train three different expert models with not super clear roles. Using REINFORCE for training is expensive in time and computation and the KL loss also requires two copies of model (\\phi and \\theta) at training time. Second, in terms of inference model, it is shown that the logits ensemble performs better than the weight ensemble model, however, logit ensemble clearly is very expensive at inference time which requires multiple copies of models and runs of generation. Also, how the experts are incorporated into the ensemble model requires grid-search of hyper-parameter tuning. One possible solution would be using parameter-efficient fine-tuning methods (e.g. adapter modules) to learn these factual experts.\n\n(2) Since the purpose of this work is to reduce factual errors of summarization models, how do extrinsic and intrinsic hallucinations directly relate to factual errors? For examples, the model might hallucinate a new location of the event which does not exist in the document, however this location is correct information. This case belongs to extrinsic hallucination but not factual errors. Then, how do the evaluation metrics reflect on this?\n\n(3) I don't have a good sense of how the metrics used correlate with human evaluation, for example, as far as I know, FEQA has a poor correlation with human eval of hallucinations and entailment-based systems are in general very brittle and inaccurate. For better evaluation, a more systematic human evaluation should be performed.\n\n(4) A very simple baseline from the paper is to directly use your filtered dataset with MLE training, however, this baseline is missing from the paper. And it's interesting to compare it with the REINFORCE method.\n\n(5) There are two very relevant works missing from the reference, which also aims at reducing hallucinations in neural sequence models. If not compared with, at least should discuss them.\n   \n    [1] Detecting Hallucinated Content in Conditional Neural Sequence Generation, Zhou et al., ACL-Findings 2021\n    [2] Improved natural language generation via loss truncation, Kang and Hashimota, ACL 2020\n\n(6) There are few typos in the paper, e.g. \"then\" is used in multiple places where \"the\" should be used. Moreover, the math symbols don't use correct format in latex, e.g. $log$ should be $\\log$ and softmax. \n\n**Questions**:\n\n(1) why do you use greedy decoding to compute the baseline reward? do you have a good intuition?\n",
            "summary_of_the_review": "In summary, this paper present a method for reducing hallucinations in abstractive summarization models. The proposed method is relatively complicated and less scalable. Evaluation methods are less reliable. Also, the results are not surprising.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents an abstractive summarization system that tackles factuality in generated summaries. It consists of mixing four models trained with different objective in mind. First, the untouched BART model is trained to maximize the likelihood of the token sequence. Then a BART model is finetuned to maximize dependency arc entailment, as predicted by an arc-level entailment classifier described in previous work. Next, an entity overlap model maximizes the precision of entities in the generated summary with respect to the input document, and finally the last model maximizes the entity recall with respect to the reference summary. The models are combined with different strategies tuned on a development set. Evaluations on XSum and CNN/Dailymail show that the combined model has higher performance according to the factuality metrics for which the subsystems were trained, at the cost of a small decrease in term of ROUGE scores.",
            "main_review": "The work presented in this paper is important for the community: tackling factiuality in automatic summarization and the large scope of text generation is one of the difficult problem addressed by current research. The proposed approach is original in that typical approaches approach the problem by building a predictor of the factuality of summary, and selecting a system output based on that metric.\n\nStrengths:\n- addresses an important problem for the community\n- originality of the approach\n- improves a selection of factuality metrics compared to maximum likelihood training\n\nWeaknesses:\n- quite confusing writing\n- lack of formal description of the approach\n- the approach is not compared to existing work\n- missing an ablation study\n- no significance testing of very close results\n- no human evaluation of factuality\n- complexity and lack of stability of the method\n\nDetailed questions/comments:\n\nWhat is the role of pivot summaries? What are they in practice?\n\nTypes of hallucinations should be formally defined.\n\nDifficult to see the proposition when it is interleaved with related work. In 3.1, what is the actual list of metrics used in this study?\nAlso mixes experimental setup with method description.\n\nMention of three kinds of experts, but only two kinds are described in 3.3.1 and 3.3.2.\n\nHow is the entity recall expert trained?\n\nWhat is the domain of i in eq. 3 and 4?\n\nWhat are the forumlas for each expert? How are the rewards computed according to metrics M? This section should be formalized carefully.\n\nThe reader should not have to read the DAE paper to understand how the metric is computed.\n\nSelecting experts according to a 95% performance threshold, and hand-tuning the minimum expert weight to the dataset undermines the impact of the method, showing that it is unstable.\n\nIn table 3, bertscore is not a measure of factuality but it is presented alongside QA metrics. It would be better to present the corresponding results in a way that precludes confusion between both kinds of metrics, by for example putting it alongside the ROUGE results table.\n\nIt would be useful to describe FEQA and QEval in more details and not force the reader to follow the refs.\n\nThe SummVis evaluation presented in 4.3 on 30 examples is hard to follow. What protocol was used for evaluation? What metric was used? How are conclusions drawn from this metric?\n\nThe conclusion should list the conclusions drawn in the results section, supported by quantitative judgements.",
            "summary_of_the_review": "The description of the proposed approach is confusing, mixing related work, contributions and experimental details.\nIn addition, the main weakness of the study is that the approach is not compared to existing work in experiments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethical concern to address.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}