{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a novel method for adaptive noise injection for differentially private federated learning. The main idea is that the amount of noise injection for each gradient should inversely proportional to the \"importance\" of the corresponding network parameter. This is in contrast to the conventional approach, which is to inject noise equally to all parameters. Two methods are proposed for determining the \"importance\" of each parameter at each iteration: a sensitivity based method and a variance based method. Simulation results for the variance based method on MNIST using two-layer MLP show some promise.",
            "main_review": "### Strengths\n\n\n1. The writing and figures are clear.\n\n\n2. Presentation of background material is very thorough.\n\n\n3. Experimental results, though quite preliminary, show that the variance based method for adaptive noise injection may enable a significant accuracy boost without paying a privacy cost.\n\n\n4. Adaptive noise injection for DP is interesting and merits further exploration. This paper introduces two novel methods in this space which show enough promise to motivate follow up work by the community.\n\n\n\n### Weaknesses  \n\n\n1. The results section is missing a comparison to (Lui et al. 2020), an earlier work on adaptive noise injection for DP.\n\n\n2. While promising, the experimental results are extremely preliminary in that they only consider a two-layer MLP and the MNIST dataset. Further evaluations with larger networks and more complicated datasets are necessary to validate the efficacy of the proposed approach.\n\n\n3.  Is this not the case that the proposed method would apply to conventional DP and not just DPFL? Why isn't this discussed in the paper. Why is the emphasis of the paper FL?\n\n\n4. The paper states the following: \"Comparing Fig. 6 and Fig 7. emphasizes the necessity of choosing appropriate proportions and noise distributions for adaptive DP\". I agree that the results clearly show this. However, this introduces a key issue for the proposed approach in that the paper does not present a method for a priori selection of the proportions or noise distributions. This is is a big issue because hyperparameter tuning incurs a significant privacy cost in DP.\n",
            "summary_of_the_review": "I want to accept this paper as the proposed adaptive noise injection methods are interesting and intuitive solutions to the important problem of poor privacy-utility trade-off in DP and DPFL. However, the paper has a number of key issues: (a) empirical results lack depth; (b) how to select the newly introduced hyperparameters is not discussed; (c) comparisons to prior work are missing; and (d) the emphasis on FL is confusing.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents an approach aiming at redistributing the noise injected into parameters of deep neural networks in federated learning to improve the model utility under the same privacy budget. ",
            "main_review": "The paper is poorly written. The reviewer does not think that the proposed approach is correct in providing DP guarantee. It is unclear how the noise is redistributed and how the importance of the weights to the output is captured. This research line has been studied extensively in the literature. However, the paper failed to highlight the novelty and theoretical advance compared with existing works. Experimental results are unconvincing. Only the toy dataset MNIST was used. \n",
            "summary_of_the_review": "There is significant room for improvement. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes to improve privacy/accuracy tradeoffs for differential privacy in a federated learning setting via two adaptive noise techniques. Specifically, they propose two heuristic methods, Sensitivity and Variance based, to identify important features in the first layer of an MLP to adapt the noise for the least important features and the most important features. The paper provides several experiments on MNIST.",
            "main_review": "Strengths:\n - This paper tackles an important problem, namely privacy/accuracy tradeoffs in differential privacy, and pursues an interesting direction, i.e. adaptive noise selection, to improve this tradeoff. \n\nWeaknesses:\n - Paper proposes to leverage two heuristic schemes (i.e sensitivity and variance based) to define feature importance as an alternative to backprop-based approaches, stating that the backprop approach is too computationally expensive\n   - This should be further justified and studied. I believe this should be included as a baseline and compared. If the core advantage of the heuristic schemes is speed, then a speed analysis should be provided. \n   - Moreover, it’s unclear why this scheme should only be applied to the first layer of the MLP or its efficacy on other common architectures. \n- While the paper to apply different amounts of noise to X% of the parameters (chosen based on a notion of importance), it is not clear how this scheme changes expected epsilon / delta privacy guarantees. Theoretical results are needed to translate the proposed scheme (i.e noise X% of parameters by something and the other parameters by some other value) into a final DP guarantee. \n",
            "summary_of_the_review": "While the paper tackles an important challenge and offers an interesting direction (i.e. adaptive noise), it does not offer a rigorous comparison to baselines, theory to explain the impact of this scheme on achieved DP guarantees, or computational analysis to justify the choice of heuristic schemes over alternative feature importance methods. I recommend rejecting the paper.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents an idea for federated learning with differential privacy. The idea is to add different amount of noise to different parts of the gradients. ",
            "main_review": "There are several weaknesses of the paper as follows. First, the paper does not provide a formal description or pseudo-code of the proposed solution. This makes it difficult to understand the exact details of the proposed solution. I had to resort to educated guesses when reviewing this paper. \n\nSecond, the idea of adding different amounts of noise to different parts of the gradients is straightforward, and has been explored in previous work. \n\nThird, the paper does not provide any formal proof of the proposed solution’s privacy guarantee. As a result, it is unclear whether the proposed solution satisfies differential privacy.\n\nFinally, the paper does not present any experimental comparison with the existing solutions. This makes it difficult to evaluate the significance of the proposed solution.",
            "summary_of_the_review": "There are issues with the paper's novelty, significance, theoretical analysis, and experiments.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper aims to improve the accuracy of neural network models that are trained in a federated manner and are perturbed with differentially private noise at the clients’ side. The authors offer two approaches for ranking the parameters of a deep neural network, so they can add Gaussian noise to the model parameters based on their importance rankings. Their experiments on the MNIST dataset using a single-layer MLP model shows that the proposed approach achieves better accuracy than a baseline approach that adds noise to all parameters in the same manner. In this approach, the server is considered as a trusted party and the overall privacy guarantee provided to the data owners is against an active eavesdropper during the communication stage.",
            "main_review": "** Strengths:\n+ Trying to address an important and relevant problem.\n+ Explaining the background well and providing detailed preliminaries.\n+ Using practical methods for computing the relative importance of features.\n\n** Weaknesses:\n1. The presentation of the proposed methodology is ambiguous.\n2. The threat model is confusing to me and the explanation could be made clearer.\n3. The provided privacy guarantee seems weaker than what the community expects for practical purposes.\n4. The experimental results (MNIST with a Single-Layer MLP) are not sufficient to validate the main idea of the proposed method. \n5. No comparison with the main prior work and other alternative approaches are provided.\n\n** Detailed Comments:\n\nI am happy to see authors have identified a very important problem. Improving the utility of privacy-preserving algorithms is of utmost importance and I appreciate the authors’ efforts in tackling this problem from a new point of view. The authors give a good background/discussion on the prior work in Introduction and explain the preliminaries in detail. The two practical methods for computing feature importance seem useful for a better understanding of the models’ behavior. \n\nWhile I encourage authors to pursue this direction, I believe the current version of this paper needs major revision and enhancement. In the following I explain my concerns and suggestions in more detail:\n\n(A) My first and foremost concern is about the considered threat model and the definition of privacy. The paper (especially Section 3) does not clarify the proposed method:\n\n1. The main privacy concern in federated learning is about untrusted parties that will observe the trained model after training. But this paper does not consider such a privacy threat at all. Basically, when training is finished, the final model does not satisfy a convincing notion of privacy.  The major assumption is that the server is “trustworthy” and “information leakage can only happen during transferring model parameters from the clients to the server and vice versa”. To me, this is a weak notion of privacy. I believe the authors should consider more realistic threat models when we also need to protect the privacy of data owners against anyone who will get access to or can make queries to the trained model in the future. \n\n2. The authors do not discuss the composition property and the relation between parameter “L” and parameter “T” in Equation (5). Also, it is not clear to me what “the maximum number of exposures of the local parameters during uplink transmission” means. How do these parameters change, and how do we keep track of such changes if the important/irrelevant features are not fixed during training? Figure 4 clearly shows that important/irrelevant features are not a fixed set during training and in fact, they are changing a lot.  No discussion and experimental results about the effect of multiple global iterations (T) are provided. It is very important to know how the privacy parameters compose during the time and how much privacy loss each client suffers from sending more and more updates.\n\n3. Reading Section 3 twice, I could not understand what is happening after ranking the model parameters. Do we only add noise to 20% more/less important parameters and only upload this 20%? Or, do we only add noise to 20% more/less important parameters but we upload all the parameters? I believe authors need to write their methodology in a clear manner. I would write down the proposed algorithm in a structured and detailed way similar to the way Algorithm 1 in your main reference (Wei et al., 2020) is presented. I would also explain how the selection of important/irrelevant features can affect the privacy parameters. As we know, any use of the sensitive training dataset will cause some privacy loss, and these two methods (section 3.3 and 3.4) clearly use model parameters before adding noise to them. Can we use this knowledge without causing any privacy loss?  \n\n(B) The evaluation setup and experimental results are not sufficient:\n\n1. The current common practice is using DPSGD (arXiv:1607.00133), but this paper does not talk about this method at all. The paper mentions the work by (Shokri & Shmatikov (2015)) but does not provide any comparison with this paper and how their work is related to that work. \n\n2. I believe the paper could be more self-contained which would help clarify the presentation. Instead of referring readers to other papers, like “global differential privacy parameters are chosen based on the principles proposed in NbAFL (Wei et al., 2020)”, or “Based on the threat model and the analysis in Wei et al. (2020)”, it is much better to clearly explain this rather important parts of your work. \n\n3. Using only the MNIST dataset and a single layer MLP is not enough to challenge a proposed methodology in deep learning. Authors need to use more complex datasets and to do evaluations on “deep” models that are used more in practice.\n\n4. The main contribution of this paper is about improving the accuracy by detecting and perturbing model parameters based on their importance. Therefore, it is important to see how these methods work when there is more than one layer and especially for layers that are far from the input layer.  Also, how do you compute these for other types of layers, such as Convolutional or LSTM layers? Do these methods only work for fully connected layers?\n\n5. For the MNIST dataset, as the authors mention “the MNIST dataset has few important features”, so how does your method perform if the number of relevant/important features are many? I suggest using datasets like CIFAR-10 or CIFAR100 as well to better find the strength/limitations of your proposed method.\n\n6. Authors mention that “raw pixels of a picture are the first layer neurons, and hence, the parameters directly in touch with them are the most effective parameters for the value of the network output.” For having such a claim the authors have to provide more evidence and experimental results on multi-layer models, instead of only using a single layer MLP.\n\nI would like to reiterate that the problem of this paper is very important and the paper’s idea is interesting. I believe that making the perturbation of the model’s parameters adaptive to the importance of each neuron/layer and hence improving the accuracy-privacy tradeoff is worth to be explored more. I encourage the authors to continue working to make the paper stronger for future submissions.",
            "summary_of_the_review": " As I have explained in my comments: (1) the proposed methodology is not well explained, (2) the privacy model is not well justified, and more importantly (3) the experimental results are not sufficient for properly evaluating the proposed method. While I believe the problem is very important and highly encourage the authors to continue working on their paper,  I believe the current version of the paper has major issues that cannot be easily addressed by shepherding or minor revision. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}