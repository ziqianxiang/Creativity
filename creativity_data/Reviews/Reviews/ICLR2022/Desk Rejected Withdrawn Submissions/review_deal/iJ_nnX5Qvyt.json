{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Towards improving the execution efficiency of the deep neural network on resource-constrained Edge TPUs/GPUs, this paper motivates to optimize the computational graph compiling. The author achieves a deep reinforcement learning (DRL) based scheduling framework to imitate the behaviors of some optimal scheduling algorithms such as “as soon as possible” (ASAP) scheduling and “as late as possible” (ALAP) scheduling. The proposed model uses an encoder-decoder architecture as the RL agent and uses a supervised-learning reward function to measure the similarity of behaviors between the RL agent and the optimal scheduling algorithms. The experimental results show that the proposed framework has ~2.5x runtimes speedups over the commercial Edge TPU compiler. And the proposed RL scheduling method outperforms traditional brute-force methods in scheduling runtime significantly.",
            "main_review": "\n## Strengths\n\n1. The proposed reinforcement learning based framework can perform near-optimum scheduling on edge computing for arbitrary sized DNNs computational graphs, which solves the problem that existing algorithms need to limit the size of the computational graph and the solution is not accurate enough.\n2. The topological feature embedding is qualified and efficient. Instead of some sophisticated learning-based embedding, the proposed method directly describes the order relation between nodes in a directed graph.\n\n## Weaknesses\n1. The proposed method with deep reinforcement learning is completely a general black-box implementation. The motivation of this paper is to directly train a deep neural network model from the data generated by a specific algorithm.\n2. The definition of the computational graph scheduling framework is indistinct. The author fails to give a straightforward description of how the scheduling action executes on devices. For instance, is there a one-to-one correspondence between an artificial neuron and a computing unit? When the number of TPU edges is less than the number of nodes in the computational graph, should another scheduling strategy be introduced?\n3. Reinforcement learning algorithm implementation is inordinate. The policy function should be non-temporal and stable. However, the author calculates the policy function as the product of several sub-policy functions, which is temporal and should be conducted to calculate the state transformation in RL.\n4. In both abstract and conclusion, the authors have mentioned that \"the proposed RL scheduling improves the scheduling runtime by several orders of magnitude over the heuristics and brute-force methods\". However, the authors do not give some relevant experiments or explanations to support this statement.\n\n\n## Detail Comments\n1. The content of section 2 (background) only contains the literature on reinforcement learning based combinatorial optimization. But personally, I think it is more important to include relevant literature on deep learning scheduling methods.\n2. Figure 1 requires more interpretation. For instance, the \"node's memory\" is confusing. Also, the authors can give a concrete and complete example to describe the figure to help readers understand the key ideas you have mentioned because it is a complex figure containing too much knowledge.\n3. Notations in Algorithm 1 do not match the corresponding interpretations, e.g. \"idx_i\" in Algorithm 1 and \"K_i\".",
            "summary_of_the_review": "I personally think reinforcement learning (RL) should not be applied for computational graph scheduling problems. This work is actually a supervised learning task with RL. The \"Policy\" is constructed from a graph by the product of all sub-policies on the vertices of a graph (this one is also quite tricky).\nMoreover, there is an architectural shortcoming in the model proposed. Since the state space in this paper is original computational graphs, the state space in the RL algorithm is infinite. So, it may be unsolvable for a new graph (a new state in RL).\nBesides, for the efficiency improvement, I wonder whether it is achieved by moving the compiling computational graph from CPU to GPU other than using the proposed framework by RL. Also, the authors do not give much explanation about the scheduling runtime improvement of the RL scheduling over the traditional brute-force algorithms, and I think it seems to be an important result in the paper which has been shown both in the abstract and conclusion.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "This paper has no ethic concerns.",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes to learn how to allocate hardware resources, namely edge TPUs, to each operator of the dataflow graph that represents a deep neural network. It proposes to use a neural network architecture based on pointer networks, and implements 2 flavors of this architecture, one based on LSTM, the other based on transformers. This model is trained using the classic reinforce rule, and the reward is defined using the cosine similarity between the resource allocation proposed by the policy and the optimal resource allocation.",
            "main_review": "The paper needs work to improve its clarity. It needs to be proof read, and several areas of confusion must be addressed:\n * The notation can be inconsistent. For example, the vertices V_i of the graph are later on referred to as nodes N_i. Similarly, the objective in section 3.1 is poorly defined. The paper first talks about schedules being a sequence of length n+1 s_0, s_n , but then in the example the s_i are being redefined as the ids of the hardware nodes to which each vertex is allocated.\n * Some notation is never introduced (e.g. the m_i in figure 1 probably correspond to the memory usage of node m, but that should be made clear).\n  * Starting with its title, the paper claims to be solving a graph scheduling problem. Yet the paper also mentions relying on the commonly used ASAP technique for scheduling. The word schedule in this paper is overloaded, and the paper would be made much more readable if it used a specific term for the problem it is trying to solve (let's call this the resource allocation problem) and another word to refer to the order of execution of the nodes, which is computed using the ASAP scheduling techniques.\n\nThe authors do not seem to be aware of highly relevant work in this area, such as the \"Device Placement Optimization with Reinforcement Learning\" paper by Mirhoseini et al or the following work by various authors. As a result, they don't compare their approach to these baselines, which makes it impossible to evaluate the significance of their contribution. This is unfortunate since their approach differs from these previous approach in one significant way, which is the way the reward is defined. In this paper, the reward is defined by comparing the output of the policy against the optimal solution, computed using an exact algorithm. Contrasting the 2 approaches would have been very informative.\n\nFurthermore, I find the evaluation lacking. The paper contrasts the quality of the resource allocation generated by their policy against that of the edge TPU compiler. However, without any information on the approach implemented in the edge tpu compiler, it is impossible to judge the quality of this baseline. More interestingly, the paper plots the testing mismatch wrt the number of training epochs on their testing set. Given enough training epochs, the paper reports a mismatch of less than 0.1. Given their definition of mismatch in equation 7, this means that if the policy makes a single mistake for 10% of the model it is able to reproduce the optimal resource allocation for the remaining 90% of the models,. I find this hard to believe given the previous state of the art, which leads me to wonder the the definition of mismatch is consistent between figure 2(c) and equation 7.",
            "summary_of_the_review": "The paper proposes a novel approach to train a policy capable to efficiently allocating hardware resources to run a neural network. In particular, the way the reward is designed differs completely from what was done before.\nHowever, the paper needs work to improve its clarity, the evaluation of the approach needs to be more rigorous, and the results need to be compared against previous work in this area.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work formulates the DNN scheduling problem into a reinforcement learning. The paper first embeds the DNN graph into a table and uses RL formulation to gradually find a better policy to schedule networks. Evaluation is done against Edge TPU paper by Google.",
            "main_review": "The paper formulates the DNN scheduling problem into a reinforcement learning framework that utilizes transformers. Overall formulation is sound, and the approach is reasonable. The performance compared to Edge TPU paper from Google seems to suggest potential of this method.\n\nFirst, the paper creates a table of model DAG and uses reinforcement learning to learn a scheduling policy that would reduce the overall runtime. I believe this is a natural extension to the ML for systems works. Including the ones cited in the paper, I believe there are several works that have tried to achieve similar goal using RL. However, it seems that the evaluation compared to them seem to be rather deficient.\n\nAlso, while the paper claims that this can improve scheduling time compared to brute-force, results are not present in the paper. Especially, I believe this paper should be evaluated for what percent of performance this work can achieve, compared to the optimal runtime performance.\n\nMore comments and questions are as stated below:\n\n* Could you provide runtime comparisons against brute-force and heuristics? Especially, it would be interesting to know how close it is to optimality.\n* Could you provide the scheduling overhead of the approach when used by edge devices? \n  * Considering that edge devices usually have very small compute units, does the runtime gains from this work legitimize the potential scheduling overhead? considering that this paper uses neural networks to schedule, I am very curious if this is the case.\n  * If this scheduling is to be done off-line, why can't we just use brute-force? Overnight compilation is a general practice in industry with large code base.\n* How does this relate to the following works\n  * Reinforced Genetic Algorithm Learning for Optimizing Computation Graphs @ ICLR 2020 - https://openreview.net/pdf?id=rkxDoJBYPB\n  * Ordering Chaos: Memory-Aware Scheduling of Irregularly Wired Neural Networks for Edge Devices @ MLSys 2020 - https://arxiv.org/abs/2003.02369\n* In terms of performance, how does it compare against\n  * Learning scheduling algorithms for data processing clusters @ SIGCOMM 2019 - it is open-sourced. While the paper claims that this is domain specific, actually, the paper can be formulated the same way.",
            "summary_of_the_review": "I believe the work is an effort to improve ML systems with reinforcement learning. Considering the success of reinforcement learning in various domains, it is a natural follow-up work. However, I do not think the overall insight nor the ideas merit a full publication in ICLR.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}