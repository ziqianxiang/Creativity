{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the problem of privacy-protected multi-domain collaborative filtering, in which a \"win-win\" deal for source and target domains can be achieved. The proposed framework, MDFNet, contains multiple local clients and one global server. In each client, the encoder achieves feature separation, and the decoder constructs original data based on separated features. Experiments on benchmark datasets demonstrate the best performance of the proposed MDFNet compared with baselines.\n",
            "main_review": "The positive points are as follows.\n1. The defined problem of privacy-protected multi-domain collaborative filtering is new and interesting. It is different from the existing UDA or SFDA problem.\n2. The proposed methods are evaluated on benchmark datasets, where various settings of source-target domains are tested. The results show promising performance of the proposed MDFNet, on both source-domain performance and target-domain performance. The \"win-win\" deal is achieved well by the proposed MDFNet.\n3. The paper is overall good-written. The figures are clear and easy to understand.\n---\nThe negative points are as follows.\n1.  The choices of baselines. The authors compared with proposed MDFNet mainly with traditional domain-adaptation methods and ignored federated learning ones such as FADA (although it is discussed in the introduction). This work is at the crossing point of federate learning and domain adaption, while the authors do not fully discuss the literature of federated learning. For example, could we adapt/extend some federated learning methods designed for labeled data such as FADA or to the setting of partly labeled data (no label for target domain)?\n2. The experimental setup. The encoder of each client is designed for feature separation. In the experiments, the encoder of each client is pre-trained on the ImageNet dataset. It seems the encoder design does not appear in baselines, which is not fair since the pertaining serves as prior knowledge of network parameters.\n3. The auxiliary communication cost introduced by the server may be a concern, compared with existing works.\n4. The main results are under the two-domain setting, while the multiple-domain setting is more important (in the current version, the results of the multiple-domain setting are in the supplemental file and not so enough).\n\n***Updates after rebuttal***\nIt is really nice to see the authors' replies address some of my concerns. I have updated my recommendation score.",
            "summary_of_the_review": "This paper studies the problem of privacy-protected multi-domain collaborative filtering for achieving the \"win-win\" deal of multiple domains. The weaknesses in experiments lead me to an overall score of rejection. However, I am willing to hear the responses from the authors.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper aims to solve a multi-domain collaborative learning problem with their privacy protected. Different from previous domain-adaptation problem, their goal is to make the model work well on both source and target domain.",
            "main_review": "This paper aims to train a model which can work well on both source and target domain when protect their privacy. First, a masked federated network is designed to train private source and target data. Two masks are used to extract domain-specific and domain invariant features in a disentangle way. The n the centralized server refines the global invariant model using both domains.\nWhen the amount of the source data and target date are imbalance, will the performance of the model be influenced?",
            "summary_of_the_review": "The motivation of this paper is clear and the proposed model is reasonable. But the novelty of each part of this work is a little bit limited.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors identify a new and practical federated cross-domain classification problem called Privacy Protected Multi-Domain Collaborative Learning (P2MDCL), which is illustrated in Figure 1. In particular, the authors design a novel solution called Mask-Driven Federated Network (MDFNet), which is shown in Figure 2, where one domain is modeled as a client with an encoder, a decoder and a classier. ",
            "main_review": "Strengths:\n\n1 The authors identify a new and practical federated cross-domain classification problem called Privacy Protected Multi-Domain Collaborative Learning (P2MDCL). The difference between the studied scenario and the previous works are well described, especially of the issues in transfer learning (cross-domain classification) and federated learning, i.e., drift and privacy.\n\n2 The authors design a novel solution called Mask-Driven Federated Network (MDFNet), for which the authors derive some theoretical bounds.\n\n\nWeaknesses:\n\n1 The authors do not analyze the security (i.e., protection of the privacy) of the proposed framework.\n\n2 The authors do not analyze the communication cost between each client (i.e., domain) and the server. In a typical federated learning system, the communication cost is a very important issue.\n\n3 The way of using an encoder and a decoder, or a domain-specific part and a domain-independent part are well known in existing cross-domain or transfer learning works.\n\n",
            "summary_of_the_review": "In this paper, the authors identify a new and practical federated cross-domain classification problem called Privacy Protected Multi-Domain Collaborative Learning (P2MDCL), which is illustrated in Figure 1. In particular, the authors design a novel solution called Mask-Driven Federated Network (MDFNet), which is shown in Figure 2, where one domain is modeled as a client with an encoder, a decoder and a classier.\n\nThe authors derive some theoretical bounds and also show the effectiveness of the proposed framework on some real-world datasets.\n\nSome comments:\n\n1 The authors do not analyze the security (i.e., protection of the privacy) of the proposed framework.\n\n2 The authors do not analyze the communication cost between each client (i.e., domain) and the server. In a typical federated learning system, the communication cost is a very important issue.\n\n3 The way of using an encoder and a decoder, or a domain-specific part and a domain-independent part are well known in existing cross-domain or transfer learning works.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a Mask-Driven Federated Network (MDFNet) to reach a “win-win” deal for multiple domains with data protected to solve the Privacy Protected Multi-Domain Collaborative Learning (P2MDCL) problem. Specifically, each domain is armed with an individual local model via a mask disentangled mechanism to learn domain-invariant semantics. Second, the centralized server refines the global invariant model by integrating and exchanging local knowledge across all domains. Moreover, adaptive self-supervised optimization is deployed to learn discriminative features for unlabeled domains. ",
            "main_review": "Strengths:\n1. This paper proposes the Mask-Driven Federated Network (MDFNet) to reach a “win-win” deal for multiple domains with data protected.\n2. The proposed MDFNet is developed to cope with the domain shift in federated training mechanism.\n3. This paper derives the generalized error bound for the proposed model.\n\nWeaknesses:\n1. It’s not clear of the motivation of this method for coping with the P^2MDCL. Specifically, why does this paper use orthogonal masks? How does the two orthogonal masks achieve domain-specific/invariant features?\n2. It’s not clear about the adaptive self-supervised optimization. For example, it needs more explanations in updating process of adaptive clustering optimization for Eq.(3). Specifically, what’s the parameters of updating the class prototype \\mathcal{O}_{k}?\n3. It fails to clearly introduce all implementation details. To be specific, how do the hyperparameters affect the performances? such as \\sigma in Eq.(4) and other related parameters in the training stage.",
            "summary_of_the_review": "This paper firstly proposes a model to reach a win-win deal for multiple domains with data protected. Besides, this method can deal with the domain shift problem. In addition, the authors also conduct theoretical analysis about the generalized error bound of the proposed model. However, there exists some weaknesses in this paper. Such as, it's not clear of the motivation why this paper use orthogonal masks to achieve domain-specific/invariant features, because the aim of this paper is to protect privacy, how about the relations between orthogonal masks and data privacy? Besides, it needs more explanations about the updating process of the adaptive clustering optimization Eq.(3). Finally, how do the hyperparameters affect the performances? such as \\sigma in Eq.(4) and other related parameters in the training stage.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}