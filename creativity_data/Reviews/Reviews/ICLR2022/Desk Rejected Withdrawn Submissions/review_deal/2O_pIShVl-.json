{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The goal of this work is to bridge the gap between the theoretical qualities of Langevin based algorithms for optimisation and their empirical inability to match SOTA optimisation methods such as SGD or ADAM. To begin with, the authors propose a new iterative scheme, similar to TUSLA [1], involving an adaptive preconditioned for the gradient term only, and a temperature parameter for the noise term (equations 7-8). They show convergence of the proposed scheme, in a very similar way to what is done in [1]. Finally, they test a modified version of their algorithm, in which the returned value is the average value of past iterates, and show competitive performance compared to SOTA algorithms.\n\n[1] A. Lovas, I. Lytas, M. Rasonyi, and S. Sabanis. Taming neural networks with tusla: Non-convex learning via adaptive stochastic gradient langevin algorithms.",
            "main_review": "My main concern about this work is the theory/practice gap, while a big motivation is to specifically close this gap for Langevin-based methods for non-convex optimization. More specifically, :\n- Use of eta=0 , while the objective might not be dissipative (as suggested in Lovas et al.)\n- The returned value of the algorithm in the experimental section is the average iterate. The sentence (bottom of p.7) \"Theorem 3.1 implies that THεO POULA converges, under suitable decreasing step size regime, to the invariant measure πβ and thus its performance can be further improved by averaging\" is not clear to me. This would make sense if the averaging is thought of as an average distribution, meaning that one would return an iterate selected at random from the past iterates. However, the distribution of the average iterate is very different from the average distribution, hence the convergence guarantees do not hold anymore.\n- The algorithm is run using a fixed learning rate, while the algorithm is shown to converge for vanishing learning rate.\n\nFurther comments:\n- The motivating example (section 2) was already introduced and fixed in [1]. This is not clearly stated in the text.\n- The proof technique is very close to [1].\n- Practical improvement compared to AdaBelief is not striking.\n- As a sanity check, due to its close similarity, I would suggest to include TUSLA in the experimental comparison, in order to demonstrate the impact of the proposed modified gradient step.\n\nTypos:\np.6 (last line) behaves like ? check\np.7 constatns, regmine\nP.8 Section 4 title: Emprical; verified theoretical \nThe variable q below eq.5 is used before its definition in Assumption 3.1.",
            "summary_of_the_review": "If the goal of this work is to propose a new algorithm for non-convex optimization, then the experimental part would need to be stronger. On the other hand, if the motivation is more theoretical, and about demonstrating the success of Langevin-like algorithms which enjoy good theoretical guarantees, then the theory-practice gap in the paper should be drastically reduced.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a new stochastic optimization algorithm inspired by the tamed Euler discretization of the overdamped Langevin diffusion. Convergence guarantees in the Wasserstein metric are established for this algorithm under mild assumptions, primarily local Lipschitz continuity and dissipativity conditions on the (stochastic) gradient of the loss function. In addition to a taming function, the proposed algorithm \"Th$\\varepsilon$O POULA\", employs a boosting function to improve performance in flat regions of the loss landscape. Along with a synthetic example, the authors demonstrate the effectiveness of Th$\\varepsilon$O POULA for training deep neural networks for image classification and language modelling.",
            "main_review": "I quite like the proposed methodology for a number of reasons:\n1. Firstly, I believe the Th$\\varepsilon$O POULA algorithm is novel and I find it very intuitive. The taming function will lead to small steps when gradients are large and the boosting function will lead to larger steps when the gradients are small. It is also clear that as $\\lambda\\rightarrow 0$ each step of Th$\\varepsilon$O POULA will be $O(\\lambda^{1.5})$ close to the standard Unadjusted Langevin Algorithm (ULA). Thus the $O(\\sqrt{\\lambda})$ Wasserstein estimates derived in the paper is in line with one's expectation and the best known convergence rates.\n\n2. Secondly, Th$\\varepsilon$O POULA has just three hyperparameters and shows strong performance in the experiments. Moreover, one of the hyperparameters, $\\beta$, simply scales the Gaussian noise applied in each step and thus probably doesn't need to be \"finely tuned\". Overall, Th$\\varepsilon$O POULA looks very appealing from a practical standpoint.\n\n3. Last but not least, the algorithm would likely open up new avenues of further research as it shows strong performance and neatly combines ideas from  stochastic optimization and SDE numerics.\n\nWhilst the experiments in the paper do not involve a wide variety of problems, they use very reasonable benchmarks (VGG11, ResNet34, DenseNet121 on CIFAR-(10,100) and a language modelling task) and several popular optimization algorithms are compared against (AdaBelief, Adam, AdamP, AdaBound, AMSGrad, RMSProp and SGD). Overall, I think this is promising but strong claims like \" confirming the superiority of our algorithm\" on page 9 should still be avoided (especially as Th$\\varepsilon$O POULA does not significantly outperform averaged SGD in the language modelling experiment).\n\nThe paper is clearly presented and proofs in the appendix seem to be carefully written (though, admittedly, I have not checked  the appendix in detail). Thus, I believe that this is a strong paper and Th$\\varepsilon$O POULA would be of great interest to the community. I only have a few minor points / typos to discuss:\n\n### Minor points\n\n* It seems more natural to me to include the dissipativity condition (B.1) in the paper and, as a remark, discuss that this can be always be satisfied by adding a regularization term. I think removing the regularization term would improve the presentation of the algorithm in equation (8), especially as the taming/boosting functions are the key concepts to convey. Moreover, $\\eta$ is set to $0$ in the numerical experiments anyway.\n\n* It is not clear to me why $\\sqrt{\\lambda}$ is used instead of $\\lambda$ in the taming and boosting functions. In particular, the Tamed Unadjusted Langevin Algorithm (TULA) involves a taming function of $1 + \\lambda|\\nabla U|$ which is consistent with the SDE literature (such as in [Hutzenthaler et al. 2012](https://arxiv.org/abs/1010.3756)). Even if the choice of $\\sqrt{\\lambda}$ is solely due to empirical performance, it would be worth discussing. More generally, it may be worth discussing what are the key properties of the taming function. That is, if one uses $1 + f(\\lambda)|G^{(i)}(\\theta, x)|$, what are the key properties of $f$?\n\n* I am not a fan of the acronym \"Th$\\varepsilon$O POULA\". It is quite long and I'm not sure why it is \"$\\varepsilon$-order\" (to me, $\\varepsilon$ is a hyperparameter and not directly related to any order of convergence). I think something in line with TULA would be more appropriate, such as TABULA (Tamed And Boosted Unadjusted Langevin Algorithm). Ultimately, this doesn't affect my review or score. I just imagine other readers will have similar thoughts. \n\n* In the experiments, it would be nice to see how Th$\\varepsilon$O POULA performs for training GANs (where AdaBelief is a strong performer). In addition, since SGD can generalize better than Adam, it would also be interesting to see how Th$\\varepsilon$O POULA compares to performing Adam followed by SGD (see [Keskar et al. 2017](https://arxiv.org/abs/1712.07628)).\n\n* Is it possible to state the dimension dependence in Section 3.3? Certainly, dimension dependence is an important to the analysis of ULA (I believe [Durmus et al. 2019](https://jmlr.org/papers/v20/18-173.html) showed linear dimension dependence under standard convexity and smoothness assumptions).\n\n### Typos\n\n* Sometimes $\\epsilon$ appears instead of $\\varepsilon$. In particular, $\\epsilon$ is used in \"$\\epsilon$-order\" on page 2 and \"order $\\epsilon\\ll 1$\" at the top of page 5\n\n* \"derive” instead of “drive” on page 7 of the appendix\n\n* The equation for $M$ could use large curly brackets on page 10 of the appendix\n\n* Sometimes equations are missing full stops (e.g. on pages 2, 4, 9, 11 and 12 of the appendix)\n\n* There is a \"$+$\" slightly outside Table 6 on page 21 of the appendix",
            "summary_of_the_review": "The Th$\\varepsilon$O POULA algorithm proposed by the paper is novel, intuitive and shows strong performance across a variety of experiments. Whilst the experiments could be more thorough (for example, by including GAN training like [Zhuang et al. 2020](https://proceedings.neurips.cc/paper/2020/hash/d9d4f495e875a2e075a1a4a6e1b9770f-Abstract.html)), they demonstrate the effectiveness of the proposed algorithm against popular stochastic optimization algorithms (including AdaBelief, Adam and SGD). In addition, as Th$\\varepsilon$O POULA relates to interesting ideas within SDE numerics, it would likely open up new avenues of future research. For these reasons, I would recommend the paper for acceptance into ICLR.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The author(s) propose the TheoPouLa algorithm to overcome unstable gradient (exploding / diminishing) problems by utilizing Euler's polygonal approximation. Several experiments are presented on image classification and language modeling to justify the effectiveness of these models. The biggest contribution of this paper is to attempt to tackle stability issues generated by the fast-growing gradients and non-global Lipschitz constants.",
            "main_review": "\n**Pros**\n\n1. The problem seems to be helpful for developing efficient sampling algorithms in DNN.\n\n2. Good empirical performance, although almost no Langevin features due to the ignorable temperatures.\n\n**Cons**\n\n1. **Ad-hoc**: The design of formula (8) is somehow ad-hoc. Although three advantages have been provided to show the efficiency, the design still looks too complex to me with unclear physical meanings. I am quite curious if the algorithm still converges to the desired distribution.\n\n2. **Why it is not importance sampling**: The adaptive learning rate mechanism with respect to only the gradient is similar to the importance sampling algorithm [1], where the latter proposes a small (or even negative) learning rate around local optima and a large learning rate around saddle points (flat). Some discussions are welcome. More importantly, the adaptive learning mechanism in [1] converges to a flat (reference) distribution instead of the original invariant measure; but this paper still converges to the original except with some bias depending on $\\lambda$. Any comments over here?\n\n3. **Overclaim in experiments**: \"The first Langevin-based algorithm to outperform SGD, Adam, etc\". Based on the experiments in Table. 2, I see similar prior results have been conducted in [2].\n\n4. **Ignorable temperatures**: The inverse temperature $\\beta$ is $10^{10}\\sim 10^{12}$, which is equivalent to no added noise. I understand that cold posteriors are common given data augmentation [3,4], the authors still should try much much larger temperatures, which may yield better performance in both uncertainty estimation and sampling. \n\n5. **Lack of uncertainty experiments**: The Langevin algorithm nowadays requires some experiments in uncertainty estimation.\n\n[1]. A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions. NeurIPS'20.\n\n[2]. Non-convex Learning via Replica Exchange Stochastic Gradient MCMC. ICML'20.\n\n[3]. A Statistical Theory of Cold Posteriors in Deep Neural Networks. ICLR'21.\n\n[4]. How Good is the Bayes Posterior in Deep Neural Networks Really? ICML'20.",
            "summary_of_the_review": "Based on the four arguments provided above, I believe this work is not ready for publication yet. **I am rather suspicious if the adaptive learning rate mechanism converge to the same invariant measure when the noise is not adjusted**. More experiments with much lower temperatures should be conducted and uncertainty estimation is also required to be tested.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}