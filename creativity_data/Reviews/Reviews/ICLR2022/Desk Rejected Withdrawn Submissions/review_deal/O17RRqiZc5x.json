{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper discusses an approach to train a teacher and student model together to improve distillation. In addition, it describes a technique called Differentiable Adjoint Networks which learns a scaling parameter for each layer, alpha, that determines the number of filters in each convolutional layer in the student model.\n\nThese methods are evaluated on standard image classification benchmarks such as ImageNet and CIFAR-100 with ResNet and DenseNet. architectures.",
            "main_review": "**Strengths:**\n1. Clarity\n\nThe clarity of the paper is good and it contains relevant details about the training procedure.\n\n2. Baselines\n\n\nThe knowledge distillation baselines used are useful in comparing this approach with other existing alternatives.\n\n**Weaknesses:**\n\n1. Novelty\n\nThe proposed method is a combination of previously explored ideas such as feature sharing, structure learning and co-distillation/online distillation and therefore lacks novelty.\n\nCo-distillation/Online distillation: \nDeep Mutual Learning by Zhang et. al (CVPR 2018) - cited\nOnline Knowledge Distillation via Collaborative Learning by Guo et. al (CVPR 2020)\n\nStructured pruning:\nSlimmable Networks by Yu et. al. (ICLR 2019) - cited\nUniversally Slimmable Networks and Improved Training Techniques Yu et. al. (ICCV 2019)\n\n\n2. Correctness\n\nIn Definition 2 (Differentiable Adjoined loss), the new term added nf doesn't seem like a differentiable term and it is unclear how this is useful to add to the loss function. More details on the how nf is computed and affects training are needed.\n\n3. Experimental evaluation\n\nWhile the experiments on ImageNet and CIFAR-100 provide some evidence towards the performance of the approach, evaluations on additional datasets and tasks would strengthen the empirical evidence of the approach.\n\n4. Organization\n\nThe organization of table 3 could be improved. In the ImageNet section, the DAN-100 results are grouped with ResNet-50 instead of being grouped with the ResNet-100 base model. And in the CIFAR-100 section, a result is provided for DAN-100 without its corresponding base model.\n\n**Minor notes:**\n- Section 4 is titled: 4 ONE-SHOT REGULARIZATION AND COMPRESSION\nHowever, the term one-shot is usually used when describing a method that learns from a small number of training examples. Overloading this term could cause confusion to the reader.\n- The statement at the end of section 6: \"Thus, adjoined loss \"encourages\" such parameters to not change by a lot thereby enabling compression\" needs more elaboration.\n- Regarding the loss in definition 1, why was groundtruth only used to train the teacher and not the student? It seems natural to train the student with available groundtruth in a model compression setting.\n- The semantics of alpha is inconsistent between Section 1 and the rest of the paper. Alpha is said to be \"1/4 or 1/2\" in the introduction while it is a whole number in the rest of the paper.\n- The point about additional regularization of the teacher model seems a bit out of place to the main goal of model compression and makes the narrative a bit confusing. The motivation for why this is useful could be elaborated upon more if it is an important result. ",
            "summary_of_the_review": "Overall, the paper is clearly written and contributes an approach to knowledge distillation that is supported by some empirical evidence.\nHowever it lacks novelty and there are several areas that the paper could be improved in terms of organization and additional details.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a novel framework for distilling knowledge and compressing a student network. The authors propose the concept of Adjoined Networks (AN) where the student and the teacher network have some common layer-wise trainable parameters and are trained together. Building on this concept, the authors also propose Differentiable Adjoined Networks (DAN)  that augments AN by using NAS which helps in the layer-wise learning of compression parameter alpha for the student network. The proposed framework is tested on ResNet50 architecture as a teacher and on the task of image classification on ImageNet.",
            "main_review": "Strength:\nThe idea of Adjoined Networks is exciting and novel. The evaluation performed showcases that the proposed framework can significantly reduce the space-time complexity of the student network while achieving better results than other relevant works.\n\nWeakness:\n1. The authors compare their work with a minimal set of results in the area of network pruning and knowledge distillation. Therefore, the authors should compare with more works such as deep mutual learning and other works where intermediate features are used for distillation.\n\n2. The authors used binary matrices M with a particular structure. Did the authors also tried other structures where the whole matrix M consists of binary values (with uniform and non uniform distribution) and not just few rows? I think that such structure will also save computation and parameters, although the number of channels will remain. I agree that, in that case, the design of alpha will change, but the authors can comment on this. ",
            "summary_of_the_review": "Overall I am positive about the paper, however, there is a need to extend the evaluation. The area of network pruning and knowledge distillation is vast, so minimal comparison does not give the complete picture of where this method stands.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Deep neural networks (DNNs) have been achieving state of the art performance in various tasks, but such models tend to be overparameterized and difficult to be deployed on edge devices such as mobile phones and IoT devices. To address the challenges, this paper proposes Adjoined Networks (ANs) and Differentiable Adjoined Networks (DANs). AN is a novel training framework inspired from knowledge distillation, which trains larger and smaller networks (base and compressed models respectively) jointly, with a new loss function specifically for this new framework. DAN searches for some of the hyperparameters in ANs, including choices of  pre-defined subnetwork architectures.  Using the CIFAR-100 and ImageNet datasets, the authors empirically demonstrate that the proposed approach achieves prediction performance comparable to more complex models while saving inference cost (model size and GFLOPs).",
            "main_review": "**Strengths:**\n1. This paper works on an important problem that it is difficult to deploy and use powerful deep neural models due to their model size. While there are a lot of studies addressing this problem such as knowledge distillation and model pruning with software/hardware support, the authors proposed a unique solution that trains larger and smaller models jointly. Different from deep mutual learning (DML), which trains two separate student models jointly, their proposed AN (and DAN) models share some of the trainable layers (e.g., a compressed model uses a subset of the larger base model). Compared to DML, the property that “shares trainable layers shared between teacher (base) and student (compressed) models” looks like a new paradigm and could reduce 1) training cost in terms of model size on memory and 2) design cost for student model as the student in this paradigm looks pruned version of the teacher.\n\n2. Using the CIFAR-100 and ImageNet datasets, this paper performed a sufficient amount of experiments to discuss the effectiveness of their proposed approach. The experiments involve comparisons not only to knowledge distillation methods, but also to model pruning approaches. Besides the main experiments, the authors performed ablation studies to discuss the effects of different model size and regularization by their proposed approach.\n\n3. While addressing the challenging task, the approaches themselves look relatively simple and straightforward. e.g., the new loss function is an extension of knowledge distillation loss by Hinton et al. (2015). I think this should be a positive aspect as the authors did not unnecessarily make the approach overcomplex and achieving the goal by the simple approach should be appreciated in the research communities.\n\n4. For reproducibility of this work, the authors provide details of their hyperparameter choices (image transforms, data augmentation, number of epochs, optimizer, learning rate adjustment, and more) besides their source code (as part of supplementary material). Due to limited access to computing resources, I did not run their code, but the combination of the description about hyperparameters and source code look good enough to attempt to reproduce their experimental results.\n\n\n**Weaknesses:**\n1. Given that the effectiveness of the authors’ proposed approach is supported by their experimental results, there are some concerns in models the authors chose for numerical comparison, specifically for CIFAR-100 datasets.\n\nResNet-18, -50 and DenseNet-121 were originally designed for higher-resolution images (e.g., 224x224) and a more complex task, which is the ImageNet dataset.  Though many of experimental results for CIFAR-100 are based on the models, these specific models have not been well benchmarked for CIFAR-100 in prior studies, and I suspect that the authors pruned some layers in the models to work with smaller images (32x32).\n\nSuggestion: To make the experimental results on CIFAR dataset convincing (for a fair comparison), I would suggest ResNet and DenseNet models designed and validated for CIFAR datasets in their original papers such as ResNet-20, -32, -44, -56, and -110 and DenseNet-BC (k=12, 24, 40).\nAlso, why did the authors build a new model, ResNet-100 instead of using ResNet-101 validated for the ImageNet dataset in the original paper?\n\nIf the authors could apply such a quantization technique to their proposed method using their model weights pretrained on ImageNet as a starting point and still preserve the comparable accuracy with INT8 version, this paper can be stronger.\nhttps://github.com/pytorch/vision/tree/master/references/classification\n\n\n2. While this paper proposes both the Adjoined Network and Differentiable Adjoined Network losses (Eqs. 3 and 6), the essential difference between these two loss functions look unclear.\n\nThe authors defined Eq. 6 by adding γ n_f to Eq. 3 where γ is a balancing factor (hyperparameter) and n_f is the total number of floating point operations or FLOPs of the network. However, the extra term (γ n_f) is not differentiable with respect to the input x, thus will be ignored when computing gradients of model parameters. In other words, Eq. 6 would be essentially the same as Eq. 3 as a loss function.\n\nSuggestion: If the authors used Eq. 6 as metric for model selection i.e., to choose which version of their (D)AN models should be used for evaluation, it should be re-formulated in a different way and referred to as an evaluation metric (for model selection) rather than a loss function optimized by SGD/Adam.\n\n3. Similarly, the value of γ for the constant term n_f is set to e^-15 or e^-19, which is prohibitively small, thus the contribution by Eq. 6 is not clear from the current experimental results, compared to that by Eq. 3. Did the authors use Eq. 6 just as a metric for search (model selection)? \n\nSuggestion: To show the importance of Eq. 6., I’d suggest the authors use γ=0 to obtain new results.\n\n4. This is not critical, but a suggestion to strengthen this work. As the authors stated “In this paper, our focus is to compress existing architectures while ensuring inference time speedups as well as maintaining prediction accuracy.”, adding numerical comparison to (even simple) quantization techniques would make this paper stronger.\nFor instance,  “DAN achieves ResNet-50 level accuracy on ImageNet with 3.8× fewer parameters and 2.2× fewer FLOPs.” can be achieved by INT8 quantized ResNet-50 (75.9% accuracy for ImageNet), that has the same number of parameters as ResNet-50 but saves 75% in model size by 32-bit float -> 8-bit int.\nhttps://pytorch.org/vision/stable/models.html#quantized-models\n\nIf the authors could apply such a quantization technique to their proposed method using their model weights pretrained on ImageNet as a starting point and still preserve the comparable accuracy with INT8 version, this paper can be stronger.\nhttps://github.com/pytorch/vision/tree/master/references/classification\n\n\n**Typos:**\nThere are repeated typos I found in this paper:\n- “Differentiable Adjoined Networks (DAN)” -> “Differentiable Adjoined Networks (DANs)”\n- “Adjoined Networks (AN)” -> “Adjoined Networks (ANs)”\n- “in the supplementary .” -> “in the supplementary material.”\n- “in supplementary .” -> “in the supplementary material.”\n- “Table 1 and Figure 1 compares” -> “Table 1 and Figure 1 compare”\n\nThe references section is incomplete. Many entries miss the venues and/or contain wrong information.\ne.g., “Kim et al. (2020)” -> “Kim et al. (2018)”. The factor transfer paper was published in NeurIPS 2018.\n\nThe following abbreviations/acronyms should be consistent and corrected:\n- “Eq.” vs. “Eqn.”\n- “Def.” vs. “Defn.”\n- “For eg.,” -> “For example,”\n\n**Minor:**\n- Do rows in “first c_out / alpha rows” in page 4 refer to channels/filters? If so, either channels or filters would be more intuitive.\n- What do “stem layers” in page 7 mean?\n- Do “res-blocks” in page 7 mean residual blocks? If so, I’d suggest you write so.\n- What does “a super-net” in page 6 mean?\n- It is not clear if Section 6 is necessary as part of the main content since it looks isolated and not well connected to the neighboring sections.\n- What is a white box at the end of “Proof sketch” ?\n- How did the authors define GFLOPs? Some papers refer to only those of convolution and linear layers as FLOPs, thus should be clarified.\n- I would suggest the authors provide more instructions (+ exact commands) to reproduce the experimental results reported in this work.\n- The considered baseline KD methods look a little bit old. For image classification, I would suggest the prime-aware adaptive distillation (PAD) and knowledge review (KR) as additional baselines besides the methods (at least for future studies):\n  - Zhang et al. \"Prime-Aware Adaptive Distillation\" @ ECCV 2020  \n  - Chen et al. \"Distilling Knowledge via Knowledge Review\" @ CVPR 2021\n  - I found both the implementations available in torchdistill https://github.com/yoshitomo-matsubara/torchdistill\n",
            "summary_of_the_review": "This paper performed a sufficient amount of experiments with model pruning and knowledge distillation baselines and showed comparable model accuracy with compressed model size and reduced flops. I also found some novelty in the framework (e.g., training larger and smaller models jointly, time-dependent loss term).\nAs described above, my main concerns are models for the CIFAR-100 dataset, numerical comparison to quantization (as the goal is the same as this paper),  and essential differences between AN and DAN given that the new term in DAN is constant and will be ignored when computing gradients.\n\nOverall, I’m inclined to suggest rejection.  I left many concerns and small questions above. If the authors could resolve them, I’d be happy to increase my score. The key issues are\n1. inappropriate model choices for numerical comparison (mostly for CIFAR-100),\n2. potentially critical issue in the loss function for DAN,\n3. importance of the loss function for DAN compared to that for AN, and\n4. (optional) comparison with simple quantization techniques that could easily achieve the objective set in this paper. \n\n[EDIT after discussions]\nNow I’m inclined to suggest acceptance given that some of the main concerns above were resolved by the authors' clarifications.  ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors claim a well-performed network compression training scheme is proposed, where the original network and the targeted compression one are trained together, thus achieves better performance compared with the conventional network compression methods.",
            "main_review": "There are numerous weakness with this paper, only very small part of them are listed here:\n\n1. The writting should be improved. Currently, the writing is not on the level of academic papers.\n\n2. Since the authors claim the outputs of the networks as 'possiblities', it should be clarified that the experiments are setted as classification tasks. However, this clarification is missed, thus leads to confusion.\n\n3. The authors claim 'good teachers are lifelong learners'. However, is there any solution disigned for the overfitting problem in the teacher network? Is the lifelong learning for new knowledge or simply the pre-defined knowledge? Without clear definition of these two terms, this claim may not correct.\n\n4. In the Adjoined Networks section, the compression parameter M is fixed, which may leads to poor generalizability.\n\n5. The authors claim 'This also has a regularizing effect as it forces the network to learn from a smaller set of parameters' and list this effect as a benefit for network compression. However, in order to prove this statement, an experiment for comparing the original network and a maksed network should be conducted (not includes the compression operation).\n\n6. In the regularization term \\lambda(t), the variable t is hyper-parameter dependent, which may not stable and functional enough when the totoal_number_of_epochs are set as small number.",
            "summary_of_the_review": "Given the poor writing quality, problemtic setting, unstable performance for this paper, it cannot be accepted as any academical publication.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}