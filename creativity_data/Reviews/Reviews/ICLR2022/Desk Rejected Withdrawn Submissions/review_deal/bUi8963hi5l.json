{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose a cross-modal retrieval approach where the main idea is to calibrate the similarity measure for the probabilistic embeddings. The task is cross-retrieval between text and images. The authors leverage pre-trained models for the functional forms of the mapping functions. Multiple experiments are conducted with two datasets and comparisons are made with the sate-of-the-art showing promising results. ",
            "main_review": "This is a very interesting topic and the authors present an approach that has promising results, at least for the two datasets presented in the experimental part.\n\nGenerally, related work seems to be well covered even though authors could also include works on metric learning and also be more clear in the novelty of their approach. I believe, such approaches should discussed in the paper. Essentially, they try to do something similar with the similarity calibration.\n\nIn some cases description is not in the correct order. While the authors talk about the functional form of the embeddings they do not elaborate and present some details later on (use of pre-trained models etc). It would help readability if such section could move earlier.\n\nAlso, I quite missed of what exactly is implemented on top of the neural models that export the embeddings. Maybe authors can clarify on that. Also readability can improve with some diagram or algo.\n\nIn the experimental part, could you please give some details whether the pre-trained models you used are frozen or you do some fine tuning? Also, how did you tune the parameters? For, example you fix joint embedding space at 512. Not sure how you came up with this number? You used some validation for that?\n\nIn the part where you evaluate the pre-training tasks, did you also check whether similar results could observed for other approaces, like TIMAM? Is this only specific to your model?\n\n",
            "summary_of_the_review": "Generally this is a very interesting paper. Results in the two datasets at least are promising. Drawbacks of the paper is that some parts are not well explained, especially the one with the neural implementation. I believe the authors can greatly improve the paper on that aspect. Also, more deep evaluation would be required, as for example for the pre-training tasks (after all is this something important for the main claims of the paper?). I would expect also, that the authors would leverage more work here from metric-learning approaches. Overall the idea is of good potential and can greatly be improved.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper extends the work on using probabilistic embeddings for cross-modal retrieval to have a calibrated probabilistic embedding. Further, they perform a detailed analysis to compute model effectiveness and impact of different pretraining strategies. In their empirical evaluation they show their approach to outperform existing approaches. ",
            "main_review": "Overall the paper is reasonably well-written, but lacks some key details in their method section which makes is a bit difficult to follow.\nBelow are some questions/comments I had when reading the paper:\n* Given cosine similarity is used to measure similarity you are sampling an embedding everytime, this means it is difficult to get consistent result right? Also equation 1 is missing normalization (that makes it cosine instead of Euclidean distance) unless the embeddings are assumed to be normalized.\n* Not sure if I missed it but, what is $I$ and $T$ in equation 2? I am assuming image and text, will be good to explain all details. Also if $I$ is images, then the $\\hat{I}$ doesnt make sense as it says $i \\neq I$. Given it is stated to be hard example, I guess $I$ is positive and $\\hat{I}$ is the hardest negative (not sure if in batch or dataset or something else though).\n* I am not too familiar with the Equation 3 version of triplet loss, are the expectations there because the embeddings in this case are not single vector but a distribution?\n* Similarity calibration seems interesting, but when you learn wouldnt the space automatically align? Isnt that part of the joint training that is generally done (I guess adding another set parameters can help it more)? Also the second proposition of multiplying by a constant should not affect cosine distance right? Once you normalize they will be the same right? And why not just learn another set of weights instead of $\\frac{p(u)}{p(v)}$.\n* I am not sure how equation 5 is derived. Is this a new loss function being introduced, if so it needs to be made clear. If it is somehow an extension of equation 3 then I am not clear on how the $log(max(\\alpha - s(I, T) + s(I, \\hat{T})))$ gets transformed to $log(s(I, T))$. It will be good to add some explanation around this.\n* In equation 7 is the $\\Sigma$ also learned or is it fixed?\n* While the approach has been evaluated against several approaches, (may be I missed something) it is a bit unclear to me on why the baselines change for different datasets.",
            "summary_of_the_review": "Based on their experiments their approach seems to have good value. But there are several questions which arose when I read the paper which make it hard to follow and truly comprehend the novelty, complexity and effectiveness of their approach. It is possible that I have missed some details and hence my comments and questions are moot, but my current impression is that the paper can be improved by addressing many of my questions. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces the similarity miscalibration problem for cross-modal retrieval. The similarity miscalibration problem is due to the modality heterogeneity. Such a characteristic causes inaccurate similarity measurement between image-text pairs. The idea of the paper is straightforward. The authors propose to estimate the density ratio to calibrate the similarity measurement in the embedding space, i.e., the ratio of the probabilistic embeddings from the image and text. They simply add it to the cosine similarity function for calibration. The authors experimentally validate their method on MSCOCO, Flickr30K, and compare against some baselines showing reasonable improvement.",
            "main_review": "Pros\n\n1. The introduced similarity miscalibration problem is interesting. Although a few previous works tackle similar motivation in cross-modal retrieval tasks, this work could be new to the probabilistic embeddings for cross-modal retrieval.\n2. Experimentally, the authors show reasonable gains when performing the similarity calibration for cross-modal retrieval.\n\nCons\n\n1. The approach is simple as it only assigns a weighting ratio to the similarity function, which is computed as the ratio of probabilistic embeddings from the image and text. The other used techniques are already exited in the community. Thus the technical novelty is limited.\n2. The writing of this paper is subpar. Some important details are missing. Especially the author gives no clear and formal definition to the similarity miscalibration problem. Moreover, the author claims this is a crucial problem for cross-modal matching while providing no further discussion or proof to show its significance. As for the corresponding figure (Fig.1), it is hard to understand what the authors are trying to convey.\n3. PCME is the first work to cross-modal retrieval by using probabilistic embedding. It also proposes a new metric, Plausible-Match R-Precision (PMRP), for comprehensive evaluation of the MS-COCO. Since the author tackles a similar problem to model the multiplicity of correspondences from the image-text pairs, it is reasonable to provide the evaluations on the new metric.\n4. The evaluation of three pre-training tasks of language models for cross-modal retrieval is unnecessary and even unrelated to the main focus of this paper. I didn't see any further discussion in the introduction or method about this evaluation. There is little merit in the evaluation results without further insightful perspectives.",
            "summary_of_the_review": "This paper introduces an interesting and undiscovered problem in the probabilistic embedding for cross-modal retrieval. The proposed method directly estimates the modality ratio for similarity calibration. However, I have many concerns about this paper as mentioned above such as the technical novelty, the experiment evaluation, and the writing.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}