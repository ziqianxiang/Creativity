{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors describe a bias in most datasets where object categories are correlated with certain settings, and say that this is undesirable. They collect a dataset called FOCUS, which presents object categories in both common and uncommon settings. They choose the 10 categories from CIFAR-10, as well as several environments. They use the Microsoft Bing search engine to construct queries for each (object, environment) pair, and pay workers through Amazon Mechanical Turk to additionally label the resulting images with the time (day vs. night) and weather condition. Next, they categorize each (object, attribute) pair as common or uncommon, recognizing that this may be subjective. The authors take several models pretrained on ImageNet and evaluate their performance on increasingly \"uncommon\" subsets of their dataset. Performance in almost every object category shows a consistent decline as the images have more uncommon attributes. They perform an experiment to test the level of information of other attributes that remains in the extracted features from ResNet50. To do this, they train linear classifiers for each object category to determine whether a selected attribute is present or not, and find that most cases can predict this with greater than chance performance.",
            "main_review": "Strengths:\n\nThe proposed dataset has a richer level of annotation than several other datasets collected to examine bias, which usually have just one \"bias factor\" (e.g., Waterbirds (bias = background), ColorMNIST (bias = color)). I also like that the dataset contains real images, as opposed to synthetically constructed ones. The authors placed considerable effort into gathering lots of images per category, through hiring workers on AMT.\n\nThe P_0 through P_3 splits are useful subcategories for comparing performance on progressively harder \"bias\". I appreciated that the authors' acknowledgement that their uncommon category selection was subjective.\n\nWeaknesses:\n\nEvaluating classifier performance on object categories in uncommon contexts is an old field, and this paper does not address many prior works in this area (for instance, none of the references to other datasets in the short related works section are older than 2018). Some older, yet relevant works: Context Models and Out-of-context Objects by Choi et. al. (2011), The role of context in object recognition by Oliva & Torralba (2007), Context Based Object Categorization: A Critical Survey by Galleguillos & Belongie (2010?). I am sure there are more. These all address a point which this paper does not discuss at all: that context is often useful for recognition. It is certainly desirable to recognize out-of-context objects, yet I'd argue any paper addressing this task is incomplete without a discussion on the utility of common context. This paper simply writes that common settings are \"spurious features\" without further discussion.\n\nHow would the authors describe \"under-represented\" in \"objects in uncommon settings are severely under-represented in many of the popular datasets in use today\"? Posing this as a negative statement seems unfair, as I assume many popular datasets purposefully seek images of object categories in common settings. One could argue that the common settings are under-represented in FOCUS, because they appear more frequently in the real world. I am not arguing one way or another here - I would just like to point out that the topic of dataset bias is nuanced, which deserves more careful discussion - especially when these ideas are extended beyond categories like horses and frogs, and into race and gender.\n\nI did not take much away from Section 4.4 - it's very common to use a model pretrained on ImageNet as a general \"feature extractor\", and so it's entirely expected to show that information on the object context remains in the features (which is also probably desirable for ImageNet, where common context is often useful for recognition) - I'd be interested to see opinions of other reviewers (and the authors' response) on whether this experiment is useful.\n\nInstead, I'd urge the authors to evaluate methods from prior works which specifically train models to ignore potentially distracting context. The performance from several of these methods would be more valuable than the performance of several off-the-shelf models pretrained on ImageNet (though both are important).\n\nOther questions/suggestions:\n\n1. The related works section could be split differently (given the current content) - one paragraph on datasets, and another on identifying and addressing failure modes for models on OOD data.\n2. Citation for 3DB is broken at the end of the related works.\n3. Cite PyTorch if using their pretrained weights (unless I have missed the citation somewhere); and/or cite the sources where PyTorch obtained the weights\n4. Where are the error bars coming from in Figure 2? How are they calculated?\n5. Figure 3 seems to be mentioned in the text before Figure 2\n6. Thoughts as to why \"models are not hurt as much by uncommon time as they are by uncommon weather or location\"?",
            "summary_of_the_review": "I believe that the dataset could be useful as a source of real images with objects in uncommon settings, for one measure of model robustness. However, lots of background and discussion is missing, given the well-studied field. This perhaps leads to missing experiments (what about models specifically trained to ignore context?).",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "- This paper proposes a dataset for stress-testing the generalization power of deep image classifiers. \n\n- The idea is to collect data by doing compositional search from public search engines, for example, using <object> <preposition> <attribute>, where the object categories, preposition and attributes are all selected from a predefined dictionary, e.g. time of day, weather, locations.\n\n- The authors have benchmarked different network architectures on the collected datasets, e.g.ResNet50, Wide-ResNet50-2, MobileNet-v3, EfficientNet, etc.\n",
            "main_review": "- Strengths\n    - I think the motivation is good, it is well-known that data-driven approaches for training models will lead to trivial solutions. As a community, we should definitely try to understand the \"uncommon\" situations, as in many cases, \"uncommon\" cases are usually the ones that may lead to catastrophic outcome.\n\n- Weakness\n    - I think the contribution of this paper is not enough to be accepted by ICLR.\n    - Idea-wise, the dataset plays the similar role as ObjectNet, which is also to do stress test for classification models.\n    - Experiment-wise, the recent literature has shown the effectiveness of CLIP or ALIGN models for tackling the openset or images under \"uncommon situations\", and the authors fail to evaluate them.\n    - Despite the issue has been raised by this paper, there is no potential solutions proposed.\n",
            "summary_of_the_review": "Overall, I think the weakness of the paper outweighs its strengths, and the critical drawback are, \n- Share similar idea to some work in literature, such as ObjectNet,\n- Insufficient experiment benchmarks, e.g. CLIP, ALIGN, etc.\n- Fail to propose any potential solutions.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This submission proposes a new dataset towards the study of shortcut learning and model generalization in deep neural networks. A set of images containing objects, captured under both 'common' and 'uncommon' settings, is collated via websearch. 'Uncommon' settings are defined across the three axes of 'time of day', 'weather' and 'location'. Several standard classification models are evaluated under the proposed dataset and reported experimental evidence show consistent drops in model performance when classifying images in 'uncommon' settings. Model reliance on spurious features is further investigated using a ResNet50 backbone. ",
            "main_review": "--Strengths--\n\nThe problems being addressed here are real and important -- principled solutions and progress towards techniques that can successfully address shortcut learning and improve the generalization ability of dataset-dependent object detectors will be of high value to the community and will additionally result in useful practical benefits. I encourage the authors to consider thinking about these problems. Writing is of a reasonable standard in general and authors promise in future to release dataset, code towards the benefit of the community and aiding further work. Such intentions can be appreciated and praised.\n\n\n--Weaknesses--\n\nThe submission follows an interesting direction yet the current version of the work raises several concerns:   \n\n(1) the size and sufficiency of the technical contributions\n\nThe paper notes that the Internet provides a 'predominant source for building a dataset'. Dataset building via leveraging an image search engine indeed provides one passive Internet-based strategy for data collection. While worker-based human labels provide additional value, the problem with the approach is that if available samples are extremely limited or non-existent for an interesting scenario (due to the authors' second definition of 'uncommon', pg4), then there currently exists no means to mitigate this. Lack of available samples in turn leads to the problems currently evident in the manuscript of requiring to exclude rare data partitions ('P3') and reporting of results ('P2' upticks) that clearly lack statistical significance. Multiple data partitions contain fewer than 10 images. \n\nAlternative Internet-based strategies for bias-controlled dataset building include eg. [a] where internet workers are, in contrast, tasked with actively capturing objects and thus are provided more direct control over the currated image properties. Further related work on active (non-Internet based) data collection of rare scenarios has also been widely explored (eg. low-light [b] and adverse weather [c]). These alternative data collection strategies are clearly of higher cost yet also therefore arguably offer higher value. Actionable feedback here is difficult as the weaknesses highlighted are inherent to the chosen dataset-building strategy.\n\n(2) Questions over the novelty of hypotheses and experimental investigations\n\nThe sole explicit hypothesis found in the work is stated on pg.8 as 'information about the environment (that is spurious with respect to the true object) may have been encoded in the deep features of the model, causing drops in model accuracy in uncommon settings'. \n\nIt is well understood that models are susceptible to learning irrelevant patterns and that dataset biases have been long known to be problematic for the related algorithms. Instances where the collected data can be solved using algorithms that do not generalize to the true data distribution are also well documented. Specifically, for the dataset-dependent nature of existing object detectors, it is understood that environmental information (encoded in both object and non-object pixels) provides a natural source of correlation between images and their labels in object recognition. Many prior works have shown that models may use environmental factors in classification (Zhang et al., 2007; Ribeiro et al., 2016; Zhu et al., 2017; Rosenfeld et al., 2018; Zech et al., 2018; Barbu et al., 2019; Shetty et al., 2019; Sagawa et al., 2020; Geirhos et al., 2020). In summary my concern is that the paper lacks new insight sufficient for this venue on what are well understood problems. I did not find the Grad-CAM results overly convincing or illuminating. Actionable feedback might involve more indepth thinking towards new insights on the role of spurious features. As an example; [e] recently report that removal of spurious features can *hurt* accuracy. \n\n\nMinor:\n(3) strange methodological choices regarding both object classes and environment characterization\n\nAuthors note that they work with the CIFAR-10 object classes (pg. 3) and yet proceed to employ pre-trained ImageNet models for their experimental work. This decision entails definition of an arbitrary class mapping function that has an effect on classification accuracy (potentially only a small effect?). Actionable feedback; why not simply employ models pre-trained on CIFAR instead? or otherwise further disentangle the effects of class mapping functions from the effects of uncommon scenarios?         \n\nI question whether or not 'time of day' is the correct terminology, axis on which to parition the dataset. I believe authors are attempting to study the effects of lack of illumination. Currently this is susceptible to uncommon-uncommon (meta-uncommon?) scenarios eg. if I capture an image indoors under well lit conditions (or indeed outdoors in the arctic summer) at midnight, should such scenarios belong in a 'night' partition? Actionable feedback; rename this axis based on available image illumination, see eg. [c].   \n\nMisc:\n* Suggest to cite Grad-CAM as [d].\n\n* Missing citation for '3DB' (pg3)\n\n* Increase language precision (eg. 'healthy mix' definition? pg. 4, 'samples corresponding to that pair is low' definition? pg. 4)\n\n* The (self-referential) subsection 4.3 appears to be missing or incomplete? Suggest reorganise subsection content + structure.\n\nReferences\n\na. ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Barbu et al., NeurIPS 2019.\n\nb. Learning to see in the dark. Chen et al., CVPR 2018. \n\nc. Seeing through fog without seeing fog: Deep multimodal sensor fusion in unseen adverse weather. Bijelic et al., CVPR 2020.\n\nd. Grad-CAM: Visual explanations from deep networks via gradient-based localization. Selvaraju et al., ICCV 2017.\n\ne. http://ai.stanford.edu/blog/removing-spuriousfeature/",
            "summary_of_the_review": "As stated above, the direction is valuable and promising however I feel this submission is currently in a somewhat premature state. Lack of clear exposition of novelty and new insights in the explored problem setting and fundamental concerns over the dataset building strategy weigh rather heavily in the rating. I encourage authors to work on some of the methodological suggestions provided.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}