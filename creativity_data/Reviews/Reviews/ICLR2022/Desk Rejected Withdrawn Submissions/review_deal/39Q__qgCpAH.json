{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors propose a two-phase adaptive learning rate decay schedule to improve performance when using large minibatches during optimization. The first phase of the proposed schedule is based on the observation that performing a larger number of epochs with a larger learning rate improves performance when using larger batch sizes. The second phase of the proposed schedule uses a learning rate based on an estimate of the maximum Hessian eigenvalue to improve convergence. Empirical results are provided with standard benchmark models on CIFAR10, CIFAR100, SVHN and ImageNet.",
            "main_review": "Strengths\n-\n\n- The problem of improving large batch performance is an important one and has the potential of significantly speeding up training time through parallelization. The authors explore how learning rate schedules need to change as the batch size increases, which I think is an important and interesting question.\n- I found the experiment shown in Table 1 quite interesting, although I think it would have made the result more concrete if the authors showed that the same trend didn't hold for small batch sizes.\n- Paper is fairly well-written and easy to follow, although some parts were not clear to me as mentioned in the Additional questions section below.\n\nWeaknesses\n- \n\n- The authors do not explore what the optimal compute budget (in terms of epochs) is for each method. Because the proposed method requires extra compute per epoch and has some additional hyperparameters, a natural question is whether validation performance could be improved by simply using a longer compute budget instead of using that compute for the proposed schedule.\n- It is not clear to me how effective the second phase of the proposed schedule is. What if I use a simple decay schedule that decays the learning rate twice at the end of phase 1, similar to what is explored in Table 1? I find Figure 3 unconvincing because the eigenvalue at the end of training could possibly be primarily driven by when one starts to decay the learning rate.\n- While I understand that the space of learning rate decay schedules is very large and it is not possible to compare with all different learning rate schedules, I think a comparison with a particularly popular schedule like the cosine decay schedule is very important.\n\nAdditional questions\n-\n\n- Were the learning rates tuned only for the large batch setting, and the small batch learning rates taken from previous papers? This is not clear to me from the paper.\n- How is the loss plateau calculated exactly? Is the training loss calculated on the whole training dataset or just the current minibatch? Typically the training loss is a fairly noisy quantity, so I am wondering how this is taken into account when calculating the loss plateau? Furthermore, if the method relies on calculating the loss on the full training dataset at the end of every epoch, how much extra computation does this add?",
            "summary_of_the_review": "I think the authors tackle an important problem, and have some interesting observations. However, I think more ablations and experimental comparisons are required to make the observations in this paper more concrete and rigorous.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes a set of heuristics for learning rate adaptation in the large-batch\nsetting and compares them experimentally to fixed decrease schedules on a number of ResNet training problems.",
            "main_review": "The paper proposes a set of heuristics for learning rate adaptation in the large-batch\nsetting. Deep learning is largely driven by heuristics so, in and of itself, this\nis not a problem. However, in my opinion, the heuristics proposed here are not\nadequately motivated. The reasoning used to justify them is over-simplifying at best\nand confusing at worst.\n\nSection 3.1 describes a relationship between a plateau in the training loss and a margin criterion\n(as a proxy for generalization performance). The observation is that, for \nlarge batch size, the margin criterion\nstill increases after the training loss plateaus. This is used to justify a \nlonger \"exploration phase\" for large-batch training. This poses several\nquestions for me:\n- Why is keeping the step size large for longer the solution? Why do we assume that decreasing the step size will adversely affect the evolution of the margin\ncriterion? \n- If the problem really is that generalization performance plateaus later than\ntraining loss, the straight-forward remedy would be to decrease the step size\nupon detecting a plateau in, say, validation loss. Are practitioners doing this?\nIt would be a simple baseline to compare to in order to check the reasoning.\n\nSection 3.2 describes the step size adjustment in the \"convergence phase\".\nIt makes the assumption that the loss function is locally described by a quadratic function. Again, this reasoning poses several questions that remain unanswered.\n- The reasoning in Equations (3)-(5) is purely \"geometrical\" and ignores stochasticity. For a non-stochastic quadratic optimization problem, no step size decrease is required in the first place. A suitably-chosen constant step size will converge. So this is not a good starting place to derive a step size decrease criterion.\n- The assumption of a local quadratic approximation is **immediately** contradicted by the subsequent paragraph, which proposes to track \nthe evolution of the largest Hessian eigenvalue (which we just assumed to be \napproximately constant). In particular, Figure 2 is sort of a carricature for a method \nthat is derived from a quadratic approximation.\n- If you want to guarantee a step size such that $1-\\eta_t h_\\text{max} < 1$ and\nyou obtain an estimate of $h_\\text{max}$, why wouldn't you just set\n$\\eta_t = h_\\text{max}^{-1}$?\n\nIn general, the distinction between two \"phases\" is somewhat arbitrary and not\nreally justified. Why is the *first* step size decrease special? I think\nthere are several arguments for this in the literature, but the present paper\ndoes not reference those.\n\nThe experimental comparison also raises some questions.\nOne aspect that isn't clearly explained in the paper is how the authors obtain\nthe base step sizes (\"LR\" column in tables 2--5). It seems to me that the step\nsize used for the _large-batch_ baseline are based on the linear scaling rule\n(i.e. tuned in the small-batch setting and extrapolated) whereas the step size for \nother methods have been tuned in the large-batch setting. In that case, I would find\nthe comparison problematic, since the goal is to compare _decrease schedules_, so \nwe would like to remove the effect of an inadequately tuned base step sizes. If the authors could please\nclarify that aspect.",
            "summary_of_the_review": "While the proposed heuristics seemingly lead to small improvements over fixed step size decrease schedules, the motivation presented in the paper is questionable in my opinion. Without a more coherent motivation (be it theoretical or through carefully designed experiments), the empirical results alone do not warrant publication at ICLR in my opinion.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a two-phase adaptive learning rate adjustment framework for the generalization issue of the large-batch training, through a common hessian approximation method.\nThe first phase (a.k.a. exploration phase) is controlled by the loss plateau, where $\\alpha$ extra training epochs will be added when encountering the loss plateau.\nThe second convergence phase uses an adaptive hessian-aware learning rate decay schedule.",
            "main_review": "# Strengths\n* The paper is well-structured.\n* The paper uses CIFAR, SVHN, and ImageNet to empirically verify its proposal.\n\n# Weaknesses\nThe significance of the proposed two-phase adaptive learning rate adjustment is unclear to me.\n1. Even though the authors claim that they follow the setting reported in [1], they did not tune the learning rate for each setup, while [1] did tune the learning rate for most of the cases.\n2. Missing comparison with other baselines proposed for large-batch training.\n3. The performance gain is not significant enough, and the considered batch size is not challenging enough. Authors need to consider larger mini-batch size.\n4. Authors are also encouraged to compare the results in terms of time-to-accuracy, where other baselines can slightly increase their computational budget as they have no computational overhead.\n5. Authors also need to justify why the claim 'we set $\\alpha$ to $x/3$' is sufficient and can be generalized to different tasks.\n6. It is unclear to me if the tuning of decay factor $\\beta$ in other baseline methods could bring any performance gain or not. It is also interesting to check the number of learning rate decay in the proposed adaptive learning rate decay schedule.\n\n## Minors.\n1. the notation of $\\alpha$ is used for different meanings.\n\n# Reference\n1. Don't Use Large Mini-batches, Use Local SGD, ICLR 2020.",
            "summary_of_the_review": "Given the current status of submission (weak empirical results), the reviewer is tending to reject this paper. \nAuthors are encouraged to address the raised concerns accordingly in the rebuttal phase.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Summary: This paper proposes a heuristic for choosing the decay intervals in a step-decay learning rate schedule, specifically targeting large-batch training. The heuristic involves initially monitoring for a loss plateau, then employing an approximation of the hessian to decide when to decay. Results are presented on several image datasets.\n\n",
            "main_review": "My take:\nThis paper is reasonably well presented, but suffers from the all-too-common flaw of proposing a complex heuristic to improve upon an older baseline technique which has largely been superceded, and only attaining incremental results. My score is accordingly set at around a 4 or a 5 (mild reject).\n\nFirst, fixed-interval step decay has not been a part of the standard recipe for training models of interest (e.g. state-of-the art ImageNet models or large transformers) for quite some time. Warmup with cosine decay is by far the most common, and even InceptionV1 used periodic exponential decay (with followon works like other Inceptions and EfficientNets inheriting this setup). As a straightforward example, in this reviewer’s ImageNet pre-activation ResNet-50 implementation (which follows the standard “default” preproc recipe common to recent SOTA networks and so is slightly different from the one used in the paper, but does not employ advanced augmentation or regularization), training with SGD + Nesterov Momentum, batch size 16384, and LR 6.4 for 100 epochs, step decay using the settings from this paper attains a final eval top-1 accuracy of 75.4%, while cosine decay gets 76.1%. The omission of a comparison to cosine annealing and exponential step decay is a substantial flaw and would be misleading to a reader who is not already familiar with the literature. Indeed, the way cosine decay is presented (“While all these works tackle the poor generalization issue from different angles, the impact of learning rate decay is largely overlooked.”) seems to imply that cosine decay is not a learning rate scheduling technique at all.\n\nEven if this element of the empirical section were rectified, the fact would remain that the proposed method is substantially more complex (both in terms of implementation and compute cost) than simply using cosine decay, and its use would have to be accordingly justified by substantial improvements in validation performance. This brings us to the second point, which is that the improvements shown here are relatively small (an improvement of ~0.5% over LARS) *at best, after tuning* and do not justify the increase in complexity. I consider it highly unlikely that any practitioner, let alone the highly limited subset engaging in extremely large batch training, would be willing to adopt the proposed method.\n\n\nMinor:\n\n\n-Figure 1 is a comparison for a fixed epoch schedule while the batch size is varied, which ignores the fact that the larger batch size models will be trained for substantially fewer steps. This is important when considering that the paper goes on to say that the large-batch training takes longer to flatten the loss than the small batch training scenario. While there are clear differences between the curves, this comparison does not hold up because the dynamics of large-scale deep net optimization tend to care less about the number of *epochs* and care more about the number of *steps* taken. In order to properly say that the large batch setup requires longer to reduce the loss or increase the margin, the X-axis needs to be presented in terms of steps, not epochs.\n\n-In the CIFAR experiments for figure 1, is the learning rate scaled up for the large batch training, or is the same learning rate used for both small and large batch?\n",
            "summary_of_the_review": "This paper is reasonably well presented, but suffers from the all-too-common flaw of proposing a complex heuristic to improve upon an older baseline technique which has largely been superceded, and only attaining incremental results. My score is accordingly set at around a 4 or a 5 (mild reject).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}