{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a novel graph kernel, based on combining tree edit distance with optimal transport tools, namely Wasserstein distance. Experimental results are provided to demonstrate the effectiveness of the method, in comparison to the state of the art. The major contribution of the paper is the use of tree edit distance, in combination with Wasserstein, while proposing an efficient algorithm for the computation. The authors claim the approach is particularly suitable to account for structural similarities among graphs. \n",
            "main_review": "The paper is written with clarity, both the notation and preliminary knowledge information are well described. The paper has a solid structure and scope, nevertheless I think that a major theoretical or experimental contribution is missing. The method is an incremental extension of previous approaches, and the experimental results do not properly support the benefit of the proposed method. \n\nDetailed comments are provided below.  \n\n\nClarity & Method\n\n- In section 4.1 the authors mention the success of the WL test with the graph isomorphism problem. It would be best here to specify that the WL test is not equivalent to the graph isomorphism problem. In fact, it could be that two graphs with the same WL representation are non isomorphic. \n- Could the authors specify how exactly the tree edit distance is computed? \n- Could the authors clarify how the number of iterations is used in the computation of WWLS? In fact, the subtree features (eq 3) seem to be equivalent to the full histogram of WL features. \n- Could the authors extend their discussion on the indefinite nature of the kernel? Possibly, the use of Krein SVM would be a better option in this case\n\n\nExperiments \n\n- Overall, the experimental results do not show a clear superiority of the proposed method\n- I would recommend to include the NCI1 and NCI109 datasets as they are a common benchmark for graph kernels. \n- What is the rationale for using distance based methods (1-NN)? It appears that the performance is overall lower than the kernel counterpart.  \n- The standard deviations in Table 1 and 2 are higher than what reported in the literature. Do the authors have a justification for that?\n- Besides, due to the high standard deviation there is no statistically significant difference in performance among many of the methods (including WWLS). \n- I would recommend adding a runtime comparison. Considering the complexity of computing the tree edit distance and the Wasserstein distance it would be useful to see how the method scales, especially in comparison to the other Optimal Transport based approaches.  \n",
            "summary_of_the_review": "While some aspects of the method might benefit for clarification (see comments above), I believe the paper is overall well written and the structure is easy to follow. The scope is also clear, though I think the paper lacks a major contribution to the field. The theoretical framework, while novel per se, is only a minor variation of previous graph kernel methods based on Wasserstein distance. The experimental results are lacking a strong conclusion in terms of advantage of the current approach, as compared to the state of the art. ",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new embedding for graphs, inspired by the one obtained by the WL kernel. They propose to use this embedding in a Wasserstein distance to obtain a notion of distance between graphs. ",
            "main_review": "The introduction is fairly well written, the problem is quite well exposed. I think that some references are missing concerning kernels on graphs and especially the extensions of the WL kernel [1-4]. The idea of using an embedding that allows to approximate a distance between the trees obtained by the WL kernel is interesting. \n\nThe main problem of this article, in my opinion, is the description of the proposed method. This corresponds to parts 4.1 and 4.2. Overall I find that the claims are not supported enough by references and/or technical/theoretical arguments. The arguments remain rather vague and it is difficult to understand what is done in practice. I think the article would benefit from a rewrite of some impotent points. See details below. \n\n- 1) About Section 4.1:\n\nI think there is a lack of references or experiences to support the following facts: \n\n  a) \"However, many experiments have shown that GNNs and the WL-based kernels have the highest accuracy on graph classification tasks when k takes a small value such as one in the range [2, 4]\" \n  b) \"Even for other tasks, such as node classification and edge prediction, GNNs also take a small k.\"\"\n\nMoreover some points are not clear:\n\n  c) \"It is difficult to determine whether the mapping exists\": the notion of \"diffuclty\" is rather vague. For example, for graph isomorphism there is no polynomial algorithm that allows to solve graph isomoprhism see [5]. Do authors refer to this kind of notion ? \n\n  d) \"we can determine the consistency between two nodes with high probability\": This notion of \"consistency\" apperas several times to justify the use of the authors' method but is never clearly defined. It is quite difficult to understand what it refers to. \n\n  e) \"While the WL test had great success with the graph isomorphism problem capturing the structural information between graphs is another problem\": this sentence is not clear to me. What does \"great success\" mean here? Why is there an opposition between graph isomorphism and \"capturing the structural information between graphs\"?\n\n  f) I think the Hamming distance should be properly defined here. \n\n  g) \"In other words, neither WL nor WWL works well when comparing two graphs with significantly different structures because the obtained node labels will be almost different\". This sentence lacks technical or theoretical supports and remains too vague. As it justifies the use of the authors' method I think that the limitations of WL/WWL should be detailed/illustrated in a more rigorous way.\n\n- 2) About Section 4.2 :\n\n  a) \"The tree edit distance is MAX SNP-hard\".  I think a reference is missing as well as a definition of \"MAX SNP-hard\". \n  b) I find Figure 2 not particularly illuminating. I particularly have trouble understanding how the embeddings are constructed from this figure. \n  c) It also lacks a reference or calculation for $d_{\\phi}(v_1,v_2)/(2h+2)\\leq d_{TED}(v_1,v_2) \\leq d_{\\phi}(v_1,v_2)$: it is not clear how this is obtained.\n\n  d) The most important problem of this part is the description of the method to calculate the embbeding $\\phi$ (paragraph \"In order to compute...\"). I find this part very vague and it is difficult to understand what is done in practice. The \"Schwartz-Zippel lemma\" is called without any context, it is difficult to say how it allows to \"desgin a good hash function\". What is the intuition behind it? What is its purpose? \n  Moreover, the \"Euler tour of T in Depth-First Search (DFS) order\" is not defined and there is no reference either. What exactly is this \"Euler Tour of T in DFS\"?\n  Finally I don't understand why introduce this \"random number $r_{i \\neq h}$\" and what is its interest.  \n  e) The rest of the procedure is not clear to me either: how are the random numbers drawn? How is $M$ chosen ? How does the upper bound of theorem 1 give a \"reasonable\" collision number?   \n\n- 3) Section 4.3 (Wasserstein distance between graphs)\n\nMy only comment on this part is the claim \"The EMD-L1 can be computed with a time complexity $O(n^{2})$\" which is not really true. In general, and as stated by the authors, the worst case complexity is $O(n^{3} \\log(n))$. As described in (Ling \\& Okada, 2007) it is empirically shown that authors achieve a $O(n^{2})$ time complexity on average, but this is only empirical. I think it should be detailed. \n\n- 4) Experiments. \n\nThe experiments focus on classification on \"standard\" benchmark datasets. Overall, the improvement is very small compared to WWL. This does not really show that the embedding proposed in this paper is able to overcome the difficulties of WWL. Also, some results are quite surprising. For example there is a huge gap between the results of FGW with respect to the original paper (for example ENZYMES/IMDB-B/IMDB-M datasets). I believe the range of $\\alpha$ within $\\{0.2,0.5,0.8\\}$ is not well chose because it discards a lot of potentially interesting values (in the original paper it said to be validated \"via a logspace search in $[0, 0.5]$ and symmetrically\n$[0.5, 1]$\"). How do the authors explain these differences? \n\n- Minor comment:\nI think there is a problem with the formatting of definition 1, I think a paragraph should be used there. \n\n\n[1] Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks. Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, Martin Grohe\n\n[2] Faster Kernels for Graphs with Continuous Attributes via Hashing, Christopher Morris, Nils M. Kriege, Kristian Kersting, Petra Mutzel\n\n[3] Scalable kernels for graphs with continuous attributes, Aasa Feragen, Niklas Kasenburg, Jens Petersen, Marleen de Bruijne, Karsten Borgwardt\n\n[4] Glocalized Weisfeiler-Lehman Graph Kernels: Global-Local Feature Maps of Graphs, Christopher Morris; Kristian Kersting; Petra Mutzel\n\n[5] Graph Isomorphism in Quasipolynomial Time, Laszlo Babai",
            "summary_of_the_review": "Overall I find that the central idea of the article is interesting, but that the article does not manage to highlight sufficiently how the approach allows to overcome the different obstacles that are formulated: the description of the method is rather vague, the claims are not sufficiently supported from a theoretical/experimental point of view and many points are, in my opinion, difficult to understand.",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes an extension of the Wasserstein Weisfeiler Lehman Graph Kernels (Togninalli et al. 2019) paper. It defines a distance function on Weisfeiler Lehman subtrees (corresponding to vertex labels at a certain depth) and uses these distances as ground distances to compute a distance between graphs using optimal transport. Subsequently, the paper proposes an approximate algorithm to compute the Weisfeiler Lehman subtree distances and empirically evaluates the accuracy of learners (SVM, KNN) based on the proposed distance. ",
            "main_review": "The paper proposes an extension of the Wasserstein Weisfeiler Lehman Graph Kernels (Togninalli et al. 2019) paper. The authors argue convincingly that a Hamming distance between Weisfeiler Lehman labels is too coarse to capture similarities between vertices with different starting labels but very similar neighborhoods. As a result, the ground distances defined in Togninalli et al. (2019) will always take the maximum value in such a situation. This might then result in poor performance in downstream learning tasks.\n\nInstead, the authors propose to define a different distance function on the Weisfeiler lehman subtrees corresponding to the Weisfeiler Lehman labels. They propose to define a feature vector for each such Weisfeiler Lehman subtree that is defined as the multiset of trees that are isomorphic to full subtrees of said tree.\nThey then use the $L_1$ distance on those multisets and motivate their choice by a connection to a tree edit distance variant (sadly not defined in the present paper). \nThis ground distance can tehn be used to define an optimal transport based distance between graphs as multisets of Weisfeiler Lehman subtrees corresponding to the labels of vertices after a few rounds of relabelings.\n\nThe choice of ground distance differs from Togninalli et al. (2019), that define the ground distance (in the discrete case) as Hamming distance between sequences of WL labels with a dimension for each relabeling step.\n\n\n## Approximate Computation of $L_1$ distances\n\nThe remainder of the technical section is devoted to a description of an approximate solution to compute the $L_1$ distance between two Weisfeiler Lehman subtrees. However, I cannot follow the authors arguments on the necessity of such an approximate solution. It seems to me that each *complete* subtree of a Weisfeiler Lehman subtree is a Weisfeiler lehman subtree itself (possibly of an earlier iteration). Hence, enumerating all of these subtrees is already happening when running the Weisfeiler Lehman relabeling algorithm and the only thing necessary to obtain the feature vectors presented in Fig. 2 of the present paper is to remember what you have done during the algorithm. As this has to be done only once per dataset or per pair of trees it is unclear to me why an approximate solution would be necessary at this point. It seems even more expensive (in practice) to traverse the full WL subtree after construction to compute a hash vector than to store a (sparse) multiset of all the labels that you see in each iteration of WL compression.\n\nAs the authors don't present empirical results regarding runtime and space complexity of their proposed approximate solution, and as the number of *complete* subtrees of a rooted tree is equal to the number of nodes in said tree, I would recommend dropping the approximation and computing the value exactly, or by including more detailed explanation why this is beneficial or necessary and an ablation study.\n\nFurthermore, it is not obvious to me that the bound claimed below Eq. (3), namely $d_\\phi (v,w) / (2h+2) \\leq d_{TED} (v,w) \\leq d_\\phi (v,w)$ holds after applying the Schwartz-Zippel lemma to approximate $d_\\phi (v,w)$.\n\n\n## Positive Semidefiniteness of the Resulting Kernel and Empirical evaluation\n\nCurrently I see another main drawback of the paper in the empirical evaluation that uses a kernel function that is indefinite with a Support Vector Machine that requires a positive semidefinite kernel function to work properly. In such a situation, you should use a Krein SVM (Loosli et al., 2015), or take (and describe) additional precautions to ensure the applicability of SVMs (e.g. by adding a constant to the diagonal of your Gram matrix).\n\nGiven the empirical results in Tables 1-4 I wonder whether the improvements by the WWLS based classifiers are indeed statistically significant. Sadly, no such analysis was performed; the tables only show best (and second best) average performance per dataset. When looking at the +- standard deviation intervals of the top 5 methods, they seem to be not separated. In particular, WWL and WWLS (the proposed novel method) are typically rather close. \nTo me, the experiments are therefore not conclusive that the proposed extension of WWL to WWLS (on the benchmark datasets that were selected here) really outperforms previous approaches. I don't consider this a necessity, though.\n\n## Minor issues\n\n- There exist many variants of 'the' tree edit distance. Not all of them are MAX SNP hard to compute in all cases. In particular, the special kinds of trees arising as WL subtrees might be special cases that could be solved efficiently. I recommend a more detailed discussion of the particular TED variant that you use as motivation.\n- I would recommend a round of editing involving a native speaker. I am not a native English speaker but would write several sentences differently. \n- Some sentences that I did not fully understand due to (my limited knowledge of) grammar:\n\t- Nevertheless, is should be emphasized the study of structural information and the distance between graph structures received less research attention in recent years.\n\t- Our key insight is that, from a geometric perspective, the WL algorithm compresses the structural information of subgraphs with hash values, and this results in describes of the measurement power of structural similarities and differences.\n\n- '$\\mathbb{R}^n_+$ is the **space of** nonnegative n-dimensional vector**s**, and $\\mathbb{R}^{m\\times n}$ denotes the **space of** nonnegative $m \\times n$ size matri**ces**.' \n- oftentimes, I find 'filed' in your text, where I would expect 'field'\n\n\n## References\n- Matteo Togninalli, Elisabetta Ghisu, Felipe Llinares-LÃ³pez, Bastian Rieck, and Karsten Borgwardt. Wasserstein Weisfeiler Lehman graph kernels. In Advances in Neural Information Processing Systems (NeurIPS), pp. 6436â€“6446, 2019.\n- G. Loosli, S. Canu, and C. S. Ong. Learning SVM in Krein spaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(6):1204â€”1216, 2015.\n\n",
            "summary_of_the_review": "The paper mentions a relevant issue with Togninalli et als. (2019) prior work and propose an interesting extension to address it. Their empirical evaluation shows that they achieve competitive results. \nThe experiments have the drawbacks of missing statistical analysis, as well as a theoretically *not* supported application of a SVM using an indefinite kernel resulting from the proposed distance function.\nMy main issue with this submission, however, is the approximate solution to the computation of the $L_1$ (approximation of the tree edit distance) that I don't deem necessary and that likely destroys the connection between the approach and the tree edit distance that was used as motivation.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to use an approximate subtree distance to define distance between nodes in WL labeling scheme of graphs. The distance is then fed into Wasserstein distance between graphs for graph classification tasks.",
            "main_review": "The paper's main objective is to solve the problem of \"not fully capture important structure information\". It does not define exactly what is the structure information that WL based distance fails to capture.  \nThis is important point, but I could only find it hidden in the last part of the paragraph of section 2. \nThe claim is that if two nodes are different, their distance are always \"unchaged\" in WL labeling scheme. It seems that paper want to say that, if their neighborhood are similar, then they should have smaller distance. In fact, it they have similar neighborhoods, the distance among their neighbor nodes are counted toward the final distance between graphs. So the point to discuss is whether \"similar neighborhoods\" being counted once in WL labeling scheme based distance loses any important information? Whether the proposed method help? This, I think, should be the point to clarify. \n\nSome of the sentences in the paper make no sense. For example:\n\n\"Thus, for two nodes with different initial labels, the distance between them is always zero.\" Zero distances are for those with the same label, I guess.\n\n\"While the WL test had great success with the graph isomorphism problem, capturing the structural information between graphs is another\nproblem. In the WL algorithm, only the discrete measure can measure the difference between two nodes, and consequently, it is impossible to define a valid node distance.\"",
            "summary_of_the_review": "- Lack of novelty: direct use of common techniques on top of one another.\n- Lack of soundness: does not properly define what is the problem and prove that the method can solve it.\n- Writing, presentation, results are not up the standard of publication.",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}