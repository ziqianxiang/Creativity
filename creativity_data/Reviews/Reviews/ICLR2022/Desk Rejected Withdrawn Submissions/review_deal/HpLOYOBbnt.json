{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose a new entropy regularization using the idea of reward shaping, compute an approximate policy gradient, and propose a soft stochastic policy gradient algorithm. The authors verify the effectiveness by testing the proposed algorithm in a series of computational experiments.\n",
            "main_review": "Strengths:\n\n(1). The proposed entropy regularization penalizes the immediate reward by adding a policy's entropy at the next state into it. This idea itself is interesting by the motivation of uncertainties in both state and action. I believe it is useful in practice, e.g., exploration, and it allows the improvement of many existing reinforcement learning algorithms.\n\n(2). The authors calculate the policy gradient of the new regularized value function. The policy gradient shares a similar structure as the usual policy gradient and this further yields a stochastic policy gradient algorithm. \n\n(3). Some comparison experiments show the effectiveness of the proposed method for solving some benchmark problems.  \n\n\nWeaknesses:\n\n(1). It might be useful to establish any convergence guarantees on the proposed stochastic gradient algorithm. I feel it is hopeful as the experiments suggest.\n\n(2). How do you justify the better performance of your proposed entropy regularization in exploration?  \n\n(3). How do you combine the proposed entropy regularization with some existing reinforcement learning algorithms, e.g., PPO?  ",
            "summary_of_the_review": "The paper proposes a new entropy regularization. I like this idea of exploration. However, I can't support the acceptance. The main concern is the theoretical support and generality in applications of existing reinforcement learning algorithms. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose adding the agent's policy entropy at the next state instead of the current state to provide better exploration and control policy. They modify the existing work Policy invariance under reward transformations: Theory and application to reward shaping by A. Ng et. al by changing the policy entropy at the next state. They add the entropy term to the reward as an intrinsic reward, rather than as a regularizer. The motivation is that maximizing the policy’s entropy at the next state will perform better exploration than maximizing\nthe entropy at the current state because the current one has already been visited. The authors present a policy gradient theorem for the modified reward function. Then they propose a soft policy gradient algorithm with this new reward function and demonstrate it on Mujoco environments. \n",
            "main_review": "The idea of using entropy term in the reward has been used before. Here, the contribution is to modify this term for the next state than the current state. But my question here is that we are considering Markov policies which means the action taken will only depend on the current state (and hence we can parameterize the policy space accordingly) so what does the entropy at next state capture? Also, the policy gradient theorem doesn't change, right?. Furthermore, in theorem 4.1, the expectation is wrt (s,a,s') rather than (s,a) in gradient theorem in Sutton's paper. So how is this incorporated in the algorithm? I am confused because in (14), we need to compute expectation for entropy at next state (particularly eq (7)). This might be okay when we deterministic environment, but when there is uncertainty in the next state, this computation can be prohibitive. In addition to that, I'd like to understand the performance of the proposed algorithm compared to when we just have entropy term for current state. Please let me know any of the algorithms in the paper is already doing that. ",
            "summary_of_the_review": "I believe that adding entropy term for next term can have a very high computation cost than the gain it provides over entropy term with current state (on environments with uncertain next states). Also, a comparison of algorithms with two entropy terms (current and next state) will be more helpful to understand how it is exploring more. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper defines a new reward including the entropy of the policy at the next state as an intrinsic reward. The modified reward may capture not only action uncertainty but also state uncertainty, so it can encourage better exploration than the existing standard RL approaches, in other words, this paper proposed a new exploration-driven RL framework. Also, the authors claim that the new framework value functions satisfy the Bellman backup equation due to the reward, so this framework is easily applicable to the existing standard RL framework unlike the maximum entropy RL framework. ",
            "main_review": "Strengths: This paper proposed a new framework called generalized maximum entropy RL, and the reward in the framework includes the entropy of the current policy at the next state as an intrinsic reward. The modified reward encourages the agent to move to a state with high policy entropy, so they claim the new framework has better exploration than the existing maximum entropy framework. They also claim that in this framework, the Q-value function and state-value function satisfy the Bellman equation like the standard RL while the reward includes the entropy term. Thus, the new framework can apply to the existing RL methods and may help the standard RL approaches be more explorable. If their claims are true, the proposed framework is a general RL framework and more explorable than the existing maximum entropy RL framework.\n\nWeaknesses: It seems that there is not much difference between the proposed scheme and SAC.\n\nIn the maximum RL,  the policy is learned to increase the entropy of the entire trajectory plus the total return, i.e.,\n\nJ_maxEnt = r_t + H(pi( .|s_t)) + r_t+1 + H(pi(.|s_t+1)) + r_t+2 + H(pi(.|s_t+2) + .....\n\nignoring the discount fator.\n\nThe soft Q-function is defined as Q_soft = r_t + r_t+1 + r_t+2 + ...  + H(pi(.|s_t+1)) + r_t+2 + H(pi(.|s_t+2) + ...\nNote that the reward starts from t but the entropy starts from t+1 (future entropy).\n\nNow,  the policy is updated so that \n\npi_new = argmax_pi  E_{a_t ~ pi(.|s_t} [   Q_soft^old (s_t,a_t) +  H(pi(.|s_t))]\n\nHere, Q_soft^old is the estimate for the total entropy and the FUTURE entropy.  So, the policy is updated such that the entropy of the current action at the current state is maximized (for diverse actions) and  the action is learned to have large future entropy.\n\nSo, SAC already targets high future entropy plus diverse current action.\n\nHowever, in the proposed method, the reward is \n\nJ_maxEnt = r_t + 0+ r_t+1 + H(pi(.|s_t+1)) + r_t+2 + H(pi(.|s_t+2) + .....\n\nwhich contains the total reward starting from t but only the future entropy.\n\nSo, the policy will be learned such that the action is chosen for high total return and high future entropy. But, the policy entropy is not maximized at the current time. So, there is no force to take diverse actions for given current state in the proposed scheme contrary to SAC.\n\nSo, the proposed formulation is a worse formulation than SAC.  ",
            "summary_of_the_review": "The proposed metric is a slight modification from the maximum entropy RL and the proposed scheme seems missing an important step of maximizing the policy entropy itself. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposed an alternative formulation to the  maximum entropy reinforcment learning framework. The previous formulation of soft-Q and SAC relies on special Bellmann relation on the value function thus previous RL algorithms can not be directly applied. The alternative formulation is based on the next step policy entropy, which can be augmented to the intrinsic reward.\n\nThe policy is parameterized by Gaussian, thus entropy calculation is tractable.\n\nIt seems that the algorithm is trained off-policy. The critic is trained in SARSA style Bellman equation. The actor is trained through a decomposition of plicy gradient on Q function and entropy.\n\n\n",
            "main_review": "## Strong points:\n- It is innovative to  consider using the next state policy uncerntainty as augmentation to the reward, which enables maximum-entropy reinforcement learning applicable under previous RL methods in terms of having a consistent Bellman equation.\n\n- Experiment is done with 5 repetitions, in comparison with SAC, PPO, TRPO, over 6 different environments and showed promising results.\nIn comparison with reported results from previous publications, the proposed method also showed significance.\n\n## Issues\n1.  I feel like there is a need to prove the policy improvements or offer some insights into that direction. Especially, speculating Algorithm 1, the algorithm seems to be learned off-policy, but without importance sampling and the critic loss seems to be an on-policy loss.\nThis is a critical issue that need to be addressed or elaborated.\n\n2. In the last sentence of section 2, why \"Moreover, the policy gradient proposition can hardly be generalized to the standard reinforcement learning approaches\".\n\n3. The proof for Theorem 1 is missing, could you offer a sketch of the proof in the rebuttal?\n\n",
            "summary_of_the_review": "I like the idea of the paper, however, important information like appendix is missing from the PDF which makes some important justification difficult. Issue 1 has to be addressed or elaborated. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "# Summary of Review\n",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}