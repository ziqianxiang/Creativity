{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Oral)",
        "comment": "This paper proposes mixed distributions over convex polytopes, and provides theory for mixed distributions that is relevant to the machine learning community. All of the reviewers were positive, and agree that this is a solid contribution. I agree, and I believe that this paper stands a chance of being a foundational paper for future work in probabilistic ML and structured learning."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes mixed distributions over convex polytopes such as the probability simplex. The proposed distributions are a discrete mixture over the faces of the polytope of \"continuous\" distributions on the corresponding face (formally, absolutely continuous wrt the Lebesgue measure on the face). For example, for the 2-simplex there is a distribution over the triangle, over each edge, and over each vertex. The authors formalize the dominating measure of these distributions, which they call the direct sum measure (given by the mixture over the counting measure over faces and Lebesgue measures on the faces); and derive formulas for entropy and KL divergences between such distributions, as well as characterizing maximum-entropy distributions.",
            "main_review": "This paper was a pleasure to read. The idea is simple and intuitive, and it addresses a recurring issue with commonly-used simplex-valued distributions, allowing to better model sparcity while avoiding diverging likelihoods in the presence of 0s. The paper is clearly written, and the mathematical exposition is formal and well presented. I also believe the idea presented in this paper will be easily used to propose new simplex-valued distributions and be valuable to the community: while the authors propose several instances of mixed distributions, one can easily think of potential alternatives.\n\nMy only complaint about the paper is that the experiments focus mostly on sparcity, and not on avoiding ill-defined log-likelihoods, which I actually believe is another benefit of the proposed distributions. For example, the distribution proposed in [1], which is discussed in the paper, addresses learning $p(y|x)$ when $y$ is simplex-valued in the presence of 0s in the data; which can be handled in a more principled way through mixed distributions. Finally, the experiments are carried out against the Gumbel-Softmax, but not more recent improvements upon it, e.g. [2,3,4]. Nevertheless, I believe the contributions of the paper are enough to warrant publication.\n\nMinor things:\n\n-Section 3.1, when defining a face, I believe \"A face P is any intersection of P with a halfspace such that none of the interior points of P lie on the boundary of the halfspace\" should be replaced by \"A face P is any intersection of P with a closed halfspace such that none of the interior points of P lie on the boundary of the halfspace\" for added clarity.\n\n\n[1] The continuous categorial: A novel simplex-valued exponential family, Gordon-Rodriguez et al., ICML 2020\n\n[2] Estimating Gradients for Discrete Random Variables by Sampling Without Replacement, Kool et al., ICLR 2020\n\n[3] Invertible Gaussian Reparameterization: Revisiting the Gumbel-Softmax, Potapczyski et al., NeurIPS 2020\n\n[4] Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator, Paulus et al., ICLR 2021\n\n========================================================================================================\n\nUPDATE 1 AFTER REBUTTAL\n\n========================================================================================================\n\nI have read the author's rebuttal as well as the other reviews, and my opinion on the paper remains that it should be accepted. I particularly appreciate the authors adding the suggested additional experiment to an already strong paper.",
            "summary_of_the_review": "This paper proposes a well-motivated and mathematically elegant family of distributions on convex polytopes, allowing to place positive probability not only on the interior of the polytope, but on its faces as well. While there is room for improving the experiments, I believe the presented experiments and theoretical developments -- which can easily enable future work -- should be accepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this paper, the authors build rigorous theoretical foundations for mixed random variables. They first define a natural measure for mixed random variables, which looks like a direct sum. Then they define the entropy and KL divergence based on the proposed measure. They also give two strategies for representing and sampling mixed random variables. At last, they conduct some experiments to illustrate the usefulness of their framework.",
            "main_review": "Strengths:\n\n1. The technique in this paper is very solid. I believe the theoretical foundations built in this paper will be useful in the future research on mixed random variables. \n\n2. The idea of this paper makes sense, and this paper is well written and easy to read. \n\n3. The experimental results demonstrate the usefulness of the proposed framework. \n\n\nWeaknesses:\n1. My only concern is that whether ICLR is a proper conference to publish this paper, since it involves some concepts about measure theory and probability theory.  \n",
            "summary_of_the_review": "This paper is very solid. It builds some important theoretical foundations for the mixed random variables. It seems that this will be useful for the future research on this topic. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents multidimensional extensions for mixed random variables originating from discrete-continuous hybrids based on truncation and rectification, which have been proposed for univariate distributions. The proposed extension replaces truncation by sparse projections to the simplex. The authors also propose a direct sum base measure definition on the face lattice of the probability simplex and intrinsic sampling strategies motivated by “manifold stratification”. Based on these introductions, new entropy and Kullback-Leibler divergence functions that subsume the discrete and differential cases and have interpretations in terms of code optimality, are presented. ",
            "main_review": "**Reasons to accept:**\n- The paper is well-written and appears relevant to recent developments. \n- The motivation of the paper in relevance to bridging the continuous representations computed by neural networks and other machine learning models with the discrete representations that characterize humans is interesting. Table 1 nicely summarizes the contributions of this work.\n- The mathematical coverage also seems reasonable. \n- Extensive experiments on three different tasks, including an emergent communication benchmark. Supplementary material also provides qualitative examples.\n\n**Suggested improvements:**\n- Mixed random variables are a standard topic in probability theory. Definitions 1,2,3 appear to be natural in this theoretical context and the same applies to Propositions 1,2. More references in the mathematical and probability literature would be helpful due to the coverage of mixed random variables and associated theory in this literature. \n\n**Update after rebuttal:** I have read the reviews and the author's responses. This is solid well-written work, my review remains unchanged. ",
            "summary_of_the_review": "This is a paper with mathematical rigor and connections to emergent communication, which is a current topic of interest for specific machine learning communities. Albeit the submission of this paper would better fit a probability and statistics venue, to the best of my knowledge, the definitions 1-3 and propositions 1,2 appear to be novel. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors wish to introduce a new kind of random variables that are consisted of both continuous and categorical data.\nThey also provide the corresponding theory and take into account information theoretical aspects.",
            "main_review": "The authors are encouraged for their good work.\nThough they do try to introduce \"mixed variables\" (ie random variables consisted by both categorical and continuous data) there are a few unclear points: for example, they consider a (ie one) categorical variable with alphabet K. What is the case when there are more than one categorical points, with different alphabet (eg ordinal data with alphabet X) ?\nAlso, what is the algorithmic complexity with respect to the number of observations, the number/rate of continuous and discrete outcomes and the number of variables (in the case of multivariate mixed variable?\nFinally, Table 1 is quite unclear to me.",
            "summary_of_the_review": "My main point is that the authors\n1) do not provide information regarding the case of having more than one discrete outcomes with different alphabets; \n2) what is the algorithmic complexity with respect to the number of observations, the rate of continuous vs discrete outcomes;\n3) they still work on the code and they do not provide it unless the paper is accepted",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}