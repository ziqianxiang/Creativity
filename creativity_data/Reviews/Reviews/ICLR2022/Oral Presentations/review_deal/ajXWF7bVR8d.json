{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Oral)",
        "comment": "Current meta-learning algorithms suffer from the requirement of a large number of tasks in the meta-training phase, which may not be accessible in real-world environment. This paper addresses this bottleneck, introducing a cross-task interpolation in addition to the existing intra-task interpolation. The main idea is very simple, which can be viewed as an incremental adding-up to existing augmentation methods. However, the method is well supported by nice theoretical results which highlight the relation between task interpolation and the Rademacher complexity. In fact, this is not a trivial extension of existing work.  Authors did a good job in the rebuttal phase, resolving most of concerns raised by reviewers, leading that two of reviewers raised their score. All reviewers agree to champion this paper. Congratulations on a nice work."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper the authors propose a meta-learning method for few-shot learning. The propose approach, MLTI, creates new (artificial) tasks by interpolating two (existing) tasks form the training set during (meta-)training. The new tasks are generated by interpolating features/labels of two sampled tasks from the training set. The authors show that the proposed approach achieve good results in multiple datasets (both in regression and classification), multiple settings (”label sharing” and “non-label sharing”) and different algorithms (MAML and ProtoNets).\n",
            "main_review": "### Pros\n+ The paper is well written and easy to follow.\n+ The idea of interpolating tasks in a meta-learning setting is novel, intuitive and simple.  Although previous work exists that augment the number of tasks, this is the first approach that augments across tasks (rather than within task).\n+ The authors shows good result on different datasets, settings and backbones.\n\n### Cons\n- I feel like results in more “traditional” (and larger) FSL datasets are missing. For example, it would be nice to see results in tieredImagerNet or metaDataset.\n- I also feel that the authors introduce the method as being a general meta-learning approach, but only show results on image. classification/regression. It would be nice to see results in other domains such as RL/NLP/etc tasks.\n- I find the theoretical analysis difficult to follow and potentially not very informative to the rest of the paper (that been said, I am not an expert on generalization theory/Rademacher complexity and cannot properly validate it).",
            "summary_of_the_review": "I recommend this paper for acceptance. The proposed idea is simple and novel, the paper is well written and the empirical evaluation is well executed. \n\n---\n**Post-rebuttal update**\n\nI thank the reviewers for the rebuttal and I keep my rating of 8. \nCongratulations for the nice work!",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "[Summary]\nThis work tackles a scenario where there may not be a large number of training tasks available, which increases the susceptibility of meta-learning algorithms to meta-level overfitting/memorization problem. In particular, to cope with the scarcity of tasks, the paper proposes to augment the given task set through interpolation of tasks. The paper reports better performance than other methods on benchmarks that have fewer training tasks. \n",
            "main_review": "[Strengths]\nThe work provides extensive theoretical analysis to provide theoretical guarantees as to how the proposed MLTI task interpolation method achieves better generalization. In contrast, previous methods that have employed common augmentation methods (e.g., label noise, CutMix, MixUp) without theoretical guarantees. \nThe work introduces scenarios that are more challenging than standard benchmarks by limiting the number of meta-training tasks.\nThe work provides extensive experiments across various datasets under such challenging scenarios and demonstrates better performance than previous methods, providing empirical support for the effectiveness of the proposed task interpolation method.\n\n[Weaknesses]\nI believe the work has minor technical novelty compared to the related work by Ni et al [1]. In particular, Ni et al. [1] performs several augmentations for meta-learning, one of which is MixUp for tasks. Using MixUp between any given pair of classes, Ni et al. [1] also creates new tasks.  \n\n[Comments]\nIn Related Work section, the work states that compared to work by Ni et al. [1], the proposed method directly desnifies the task distribution. But, doesn’t [1] effectively densify the task distribution, where a new task can consist of new classes that are constructed by using MixUp on pairs of classes? As such, I believe more discussions on this issue should help better differentiate the work from the related work.\nWhy does the proposed method randomly sample a location where features are to be interpolated? Is there an ablation study on the sampled location? I wonder if this technique is what makes the proposed method perform better than other works. I’m curious as to whether the proposed method, without this technique, still performs better than other works. The ablation study on this would be helpful for better understanding of differences from other works.\nAlso, how does it compare with related works on standard benchmarks, such as miniImageNet. I think that the proposed method should still work with a larger number of tasks and believe that these experimental comparisons can strengthen the contributions of the proposed method.\n\n[1] Ni et al. Data Augmentation for Meta-Learning.",
            "summary_of_the_review": "[Recommendation]\nDespite strong experimental results and analysis, at this point, I believe the technical novelties are not significantly different from the work by Ni et al. Thus, I believe the work is marginally below the acceptance threshold. If the above comments are addressed, I’m willing to increase the score.\n\n-----------------------------------------\n[Post-rebuttal]\nI thank the authors for the response, along with clarifications and updates in the manuscript. \nAs the authors have addressed most of my concerns, I'm happy to increase the score accordingly. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes an interpolation strategy for meta-learning to improve the learned model’s generalizability. The interpolation strategy is quite simple - interpolate between a pair of tasks, in contrast to existing methods such as adding label noise or data augmentation on each task individually. The authors show that the resulting gradient- or metric-based meta-learning framework (MLTI) induces a data-dependent regularizer that controls the Rademacher complexity leading to better generalization. MLTI is tested on specially curated datasets derived from standard benchmarking datasets. Furthermore, MLTI is also compared against existing data augmentation and interpolation strategies for meta-learning to illustrate its effectiveness.\n",
            "main_review": "strengths\n\nAlthough nifty, the idea of pair-wise task interpolation is an incremental change over the existing data augmentation approaches. The theoretical results, highlighting the relationship between task interpolation and the Rademacher complexity, are non-trivial extensions of the Zhang et al. ICLR 2021 and Yao et al. ICML2021 to account for pair-wise task interpolation. I view this as the primary contribution of the paper.\n\nThe comparison against existing data-augmentation baselines for both metric- and gradient-based meta-learning approaches is quite exhaustive. Furthermore, MLTI is tested on a wide variety of datasets. While the improvement on each dataset is only marginal, the consistent improvement in all datasets and across all approaches strengthens the paper’s contribution.\n\nThe paper is well-written and easy to follow.\n\nConcerns\n\nCurrent approaches in meta-learning rely on heavier backbones such as ResNet. As the goal of all the meta-learning methods is to improve the model’s generalizability, I think it is fair to evaluate the effectiveness of MLTI with heavier feature extraction backbones. Such a comparison is relevant as the proposed task interpolation is conducted on the features extracted from some intermediate layer of the network.\n",
            "summary_of_the_review": "Overall, the paper proposes a simple extension to standard data/task - augmentation methods for meta-learning but justifies it with rigorous theory. The theoretical results are non-trivial extensions/combinations of existing work. The effectiveness of the approach is evident from the extensive empirical evaluation. The contributions are strong, albeit limited to the meta-learning research community.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a task augmentation method via task-interpolation for data efficient meta-learning. While the traditional meta-learning methods highly rely on a large amount of data to retain diverse training tasks, the proposed method, MLTI generates tasks by interpolating the tasks which are obtained from training data. The experimental results on variety of few-shot learning dataset show that MLTI is effective when the meta-training data for constructing training tasks is not enough, for both gradient-based and metric-based few-shot learner. ",
            "main_review": "Strength\n1.\tThis paper proposes a novel task-augmentation method, which is affected by Manifold Mixup, which can be applied to many existing few-shot learning tasks.\n2.\tThe theoretical analysis shows that the proposed MLTI augmentation has a regularization effect and leads the meta-learner to have a better generalization capability.\n3.\tExtensive simulation results on variety of few-shot learning datasets and two representative few-shot learning methods show that the proposed MLTI is highly effective for meta-learning with fewer data. \n\nWeakness\n1.\tComparison with the prior methods in large dataset is missing. For example, in Table 3, the comparison results are provided only for small datasets or reduced version of large datasets. However, the proposed method is not restricted to small dataset. The ablation experimental result in Figure 2 shows that proposed MLTI is still effective when the full miniImageNet/DermNet dataset is used, although the performance gain becomes small when the full dataset is used. I suggest the authors to include the comparison of MLTI and prior methods with full size of miniImageNet and DermNet.\n\nQuestion\n1.\tIn Section 3, the authors mentioned that it is intractable to calculate prototypes with mixed labels. However, in prior work on semi-supervised few-shot learning [1], the prototypes are computed using soft-labels. What happens if we compute prototypes using soft-labels as done in [1]? \n2.\tSome additional studies on the interpolation layer would be helpful for understanding the proposed method. In Algorithm 1 and 3, the interpolation layer $l$ is randomly chosen in step 7. What happens if we fix $l$ instead of randomly sampling $l$ for every iteration? In that case, how are interpolating at lower layer and interpolating higher layer different? \n\nTypo: In last line of page 4, there is a typo (regularizaiton -> regularization)\n\n[1] Ren, Mengye, et al. \"Meta-Learning for Semi-Supervised Few-Shot Classification.\" International Conference on Learning Representations. 2018.\n",
            "summary_of_the_review": "This paper proposes a novel task-augmentation method of MLTI and shows the effectiveness of proposed MLTI through the extensive simulation results and theoretical analysis. The proposed MLTI can be applied to both optimization-based and metric-based few-shot learning methods. Adding some experimental results would make the readers to better understand the proposed method.\nHowever, I believe the idea of this paper is valuable for few-shot learning field, and I recommend to accept this paper.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper describes a method for augmenting task selection in meta-learning, by interpolating support and query sets between two random tasks from the base dataset.  This is examined in two scenarios, label-shared LS and non-label-shared NLS, differing in whether the label space is the same between tasks (e.g. pose estimation) or different (classification to different discrete class sets).  In the former, label targets are interpolated as well as support set inputs, while in the latter, new classes are constructed by random cross-task pairings.  Comparisons are made to other interpolation augmentation approaches, including MetaMix, which interpolates in query set but not the support set.  The approach results in significant performance gains on multiple benchmarks in both settings.\n",
            "main_review": "I found the approach to be simple and relatively well explained, including ablations studies on large-point questions I had while reading, including its behavior and effectiveness for different sizes and number of classes in the original base dataset, as well as effects of inter- and intra- task interpolations.\n\nThe key difference between this work and MetaMix (Yao et al 2020) is incremental but important:  MetaMix will run the inner loop on the unmodified support set only, and use a mix of support+query in outer loop comparison optimization, whereas this work interpolates support set in inner loop as well.  This difference enables between-task interpolation which adds additional augmentation particularly in settings where few tasks can be drawn from the base data.\n\nI didn't follow much of the theoretical sections in detail, and had to look at the appendix proofs to even understand some of the notation in the main text.  In my somewhat limited understanding they seem reasonable.  These claim to show a theoretical generalization improvement in simplified settings (binary classification of single layer model, linear protonet feautres).\n\n\nAdditional questions:\n\nNLS:  In addition to a single set of correspondence pairs, the input examples for each class can be mixed with all-pairwise-combinations.  How many combinations are used?  That is, for two sets of k examples {xs_i} and {xq_j} (i,j in 1..k), one can form k^2 interpolated examples {a xs_i + (1-a) xq_j} using each i,j combination.  Are all of these combinations formed or just a single set of k pairings?  If using more than k pairings, this would change the task from k-shot to k^2-shot; but the l-layer features for each of the k^2 combinations could be computed, and then up to (k^2 choose k) tasks could be selected from these and used in the upper layer loss comparisons.  For k=5 that would increase interpolated pairs from 5 to 25, but potentially get up to 53130 upper layer loss comparisons from each task pair sample -- would this get even more benefit from this task augmentation technique?\n\neq 5:  what does the name of the subscript \"cr\" mean (does it stand for something)?\n\nIt could be useful to have a more explicit explanation of differences with MetaMix.  MetaMix will run the inner loop on the unmodified support set only, and use a mix of support+query in outer loop comparison optimization, whereas this work interpolates support set in inner loop as well.  This is already mentioned at a high level (fig 1 caption and sec 5 last paragraph), but I think it could be even clearer by pointing out the difference in the discussion around eq 5, that support set H^s,Y^s in the inner loop is mixed between tasks, whereas in MetaMix, only the H^q,Y^q are replaced by mixing.\n\n",
            "summary_of_the_review": "Overall, the approach is described well enough to understand the approach, and is emperically shown to result in decent performance gains in the low task data settings for which it is intended.  The theoretical sections corroborate this, but I found them hard to follow.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}