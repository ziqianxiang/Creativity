{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Oral)",
        "comment": "Exciting work at the intersection of continual learning and representation learning. The reviewers have all commented that the proposed work addresses a number of issues related to catastrophic forgetting, which is very encouraging. The work also shows that the representation learning with the proposed method is more general than the one learned with supervised CL. The reviewers have praised the work as being well-written and with thorough experiments. There was a robust back and forth between the reviewers and the authors during the rebuttal period, in which the authors appear to have addressed most of the concerns. Given the insights, results and potential impact of this work, I think this work definitely should be published at ICLR."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Unlike most of the continual learning approaches in the literature that perform supervised training at each learning stage, the authors propose to perform unsupervised representation learning on the sequence of incoming data and then classify the samples at each stage using K-NN. Experiments on standard CIFAR10/100 and Tiny-ImageNet shows that the proposed method alleviates catastrophic forgetting and generalizes better in different scenarios.",
            "main_review": "Strengths\n* The submission is well written and easy to follow. The proposed concept is well motivated with various quantitative and qualitative justifications. \n* While many unsupervised/self-supervised training approaches require pre-training on massive unlabeled data, the proposed method here works well with the help from Mixup and does not require additional pre-training set, largely making it more applicable to real-world use cases. \n* I especially enjoy reading Sec 5.3 regarding the analyses on feature similarity between different learning approaches, visualization of feature space, and loss landscape visualization. This section provides additional justification besides the absolute accuracy improvement over the supervised continual learning counterparts.\n\nWeaknesses\n* I think the main limitation of using the proposed pipeline in practice is runtime and memory constraints. To run K-NN in a lifelong learning setting is challenging due to ever-growing storage requirements for storing samples coming from different stages. During inference, the algorithm also needs to compute distances between the query and many stored data points. Can the authors shed some light about the runtime and memory comparison between the proposed method and supervised counterparts?\n* It would be great to also compare with other recent supervised continual learning approaches as well such as [A1, A2]\n\n[A1] Zhao et al. Maintaining discrimination and fairness in class incremental learning. CVPR 2020. \\\n[A2] Liu et al. Mnemonics training: Multi-class incremental learning without forgetting. CVPR 2020.\n",
            "summary_of_the_review": "Overall a good quality submission with novel and interesting ideas. It would be great for the authors to address the weaknesses mentioned above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the problem of representation learning in an unsupervised continual learning(UCL) setting. It shows that the representation learned with UCL is more general than the one learned with supervised CL (SCL), and investigates why UCL is more robust to catastrophic forgetting than SCL by analyzing the similarity of learned features and visualizing loss landscape. The authors also propose to apply mixup technique to UCL setting and present a LUMP algorithm to further improve the performance of CL.This paper studies the problem of representation learning in an unsupervised continual learning(UCL) setting. It shows that the representation learned with UCL is more general than the one learned with supervised CL (SCL), and investigates why UCL is more robust to catastrophic forgetting than SCL by analyzing the similarity of learned features and visualizing loss landscape. The authors also propose to apply mixup technique to UCL setting and present a LUMP algorithm to further improve the performance of CL.",
            "main_review": "Strong Points\n\n* The paper takes one of the most import issues in CL: learning robust representation in unsupervised setting. For me, the problem itself is real and practical.\n\n* The paper provides comprehensive experiments, including both qualitative analysis and quantitative results, to show the effectiveness of UCL and the proposed  LUMP algorithm over SCL methods.\n\n* Overall, the paper is well written. In particular, the Related Work section has a nice flow and puts the proposed method into context. Despite the method having limited novelty, the method has been well motivated by pointing out the limitations in SOTA methods.\n\n* The authors provide code for reproducing the results in the paper.\n\n\nWeak Points\n\n* The proposed LUMP algorithm is adopted from supervised mixup technique(Zhang et al, 2018). So the novelty is limited.\n\n* The authors conducted extensive experiments in task-incremental setting. It would be interesting to see how UCL and the proposed method perform in class-incremental and task-agnostic CL settings.\n\n* Fig 3: In general, higher layers have lower feature similarity than lower layers, and similarity between UCL models are higher than that of SCLs. However, there is an exception in Layer 4 of DER method -- the similarity of SCL is higher than that of UCL. It is worth some discussion on this exception.\n\n",
            "summary_of_the_review": "Overall, I vote for marginally accepting. I like the idea of unsupervised continual learning and handling it by the proposed LUMP method. My major concern is about the limited novelty of the proposed method -- adopted from mixup in supervised learning, and some additional experiments on class-incremental or task-agnostic settings(see weakness above). Hopefully the authors can address my concern in the rebuttal period.\n\n[After rebuttal]\nThe authors addressed most of my concerns, so I would like to raise my score.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to tackle the continual learning problem in an unsupervised setting. It shows that recent self-supervised learning methods are efficient tools to learn image representation with lower catastrophic learning problems. Two recent self-supervised methods are evaluated: SimSiam and BarlowTwins. In both cases, the superiority of unsupervised features is demonstrated. \n\nThe widely used mixup method is also adapted to the UCL problem. Straightforwardly, current images are mixed with images of the past tasks sampled from the replay buffer.",
            "main_review": "Strengths:\n- First the paper is well written and easy to read. \n- The experiments are rich and well-chosen to better understand the superiority of unsupervised representations in the context of CL. I especially appreciated the experiments in Fig 2 that investigate the impact of the size of the training dataset.\n- The conclusions are enlightening and will be very helpful to design new supervised or unsupervised CL methods.\n- The code is publicly available and looks clean and  easy to use.\n\nWeaknesses:\n- Some details are unclear: \n   - Sec 5.1: Knn classifier: which set is used for NN? the replay buffer? validation set? \n   - The Knn is used both for supervised and unsupervised experiments, right?\n   - Sec 5.3: more explanations about CKA are required for the reader that is not familiar with this measure\n- I recommend changing the title. \"Rethinking the Representational Continuity\" is much too strong. The conclusions of the paper are great but it does not provoke a real rethinking of the problem.\n- I am not really convinced but the visualization in Fig4. It seems that LUMP has sparser activations. The shapes of the objects are more clearly visible in its feature map. Does it simply mean that it learns lower-level features (similar to edge detector)? Maybe a TSNE visualization would help to see how the features of old tasks are affected when learning a new task.\n- In Fig.5, we can notice that the range of value gets smaller in T19 (from [4.6,5.6] for T0 to [4.4,4.6]). Any idea why?\n\n\n",
            "summary_of_the_review": "The paper shows interesting results that confirm the potential of self-supervised learning methods in the context of continual learning. The technical novelty may look incremental but the experimental conclusions are very interesting for the community. The paper is clear and well written.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper attempts to bridge the gap between **unsupervised representation learning** and **continual learning** by extending various supervised continual learning methods to the unsupervised learning framework.\nIt builds upon two recent unsupervised feature learning techniques (*SimSiam* and *BarlowTwins*) and a powerful data augmentation technique *MixUp*.\nImproved performances have been demonstrated in various experimental settings, together with comprehensive feature visualizations.",
            "main_review": "### Strengths\n\n* Rethinking continual learning with unsupervised representation learning is interesting, and empirical results indicate that most supervised continual learning methods can be improved by the proposed approach.\n\n* A bunch of experiments have been conducted to demonstrate the effectiveness of the proposed approach in various settings. And several visualizations have also been included for a better understanding of the learned features.\n\n### Weaknesses\n\n* **[Missed Comparison with CURL]** \\\nThough the authors criticised that *Continual Unsupervised Representation Learning framework (CURL)* to be limited by digit-based gray-scale datasets, no direct comparison with CURL is done by following their evaluation protocol. Adding this result could better reveal the difference between the proposed method and CURL in terms of effectiveness. \\\nBeside, the evaluation of cluster quality used in CURL seems to be an important evaluation metric in unsupervised continual learning, which has not been used in the paper.\n\n* **[Degraded Performance of DER and MULTITASK]** \\ \nIn the Table 1, we see that the proposed unsupervised continual learning can improve all baseline methods except DER and MULTITASK. A clear explanation about this performance drop should be added.\n\n* **[Qualitative Analysis]** \\ \nWhy the visualization of feature maps stops at $\\mathcal{T_{13}}$, while the loss landscape visualization continues to $\\mathcal{T_{19}}$ ? And in Figure 5, the difference between SCL-DER and LUMP is hard to interpret.\n\n* **[Why Not Directly Use Unsupervised Learned Presentations]** \\\nAs an important purpose of the unsupervised representation learning is to learn a powerful embedding space that can be quickly fine-tuned for latter down-stream tasks. Why don't we consider a baseline where the feature backbone is initialized with SimSiam or Barlow Twins, and directly fine-tune them on a sequence of tasks. This is probably not considered in the standard continual learning, but the results of this baseline could be informative to the community of both domains.",
            "summary_of_the_review": "This paper attempts to rethink the standard continual learning in a new point of view by considering unsupervised representation learning methods. This purpose is interesting, but the major difference between the SCL (supervised continual learning) and UCL (unsupervised continual learning) seems to be just adding a unsupervised representation loss at the backbone and freezing it in the second stage of predicting head finetuning.\nMoreover, some parts of the empirical results are not clearly presented or explained, an improved version of experimental results could be helpful for better validating the contributions.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}