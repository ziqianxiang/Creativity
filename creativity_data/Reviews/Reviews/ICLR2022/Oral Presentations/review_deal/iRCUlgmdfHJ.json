{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Oral)",
        "comment": "This paper makes an important evaluation study to further the understanding of the representation capacity of DNNs. The novelty is in using the multi-order interaction proposed in Zhang et al. (2020) to understand the complexity of interactions in DNNs. The authors discovered an interesting representation bottleneck phenomenon, i.e., in a normally trained DNN, low-order and high-order interaction patterns are easy to be learned, while middle-order interaction patterns are difficult to be learned. They also propose two novel loss functions, which allow the model to encode interactions of specific orders, including middle-order interaction. All reviews are positive."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper discovers and theoretically proves the representation bottleneck phenomenon that indicates cognition gap between DNNs and humans. This paper proposes losses to encourage or penalize the DNN to control interactions of specific complexities, and analyze the representation capacities of interactions.",
            "main_review": "Strengths\n- The paper is well-written and easy to follow. In particular, Figure 1 and 2 support the concept of representation bottleneck reasonably well.\n- Two questions the authors raise about DNNs are rational, and prove convincingly that this phenomenon is a common problem of DNNs.\n- The idea of describing the representation of the DNN using multi-order interaction utility is good and easy to understand.\n\nWeaknesses\n- The authors propose two losses to encourage/penalize DNNs to learn interactions of specific orders and show the results in Figure 4. This loss can stimulate interactions of specific orders well. High-order DNN seems to have been fully explained and experimented, but not on middle-order and low-order interaction. I wonder why the authors didn't explain it in detail.\n- The authors conduct several experiments, but these show similar results, making it difficult to gain new insights. And it is difficult to understand what Table 1 means and shows. \n- Although this paper shows good enough insight to others, it would be good to present directions for solving the problem of representation bottleneck or providing a clear problem definition for future works.\n- I know it’s due to the limitations of the amount, but putting too many formulas and terms into the sentence. Instead of including all the formulas in the sentence, it would be better to reorganize it and make it easier to legible.\n",
            "summary_of_the_review": "This paper introduces and proves the representation bottleneck, which is a common  phenomenon in the DNNs. This is well-written and easy to understand. There seems to be some shortcomings in the experiments and conclusion, but the paper provides good insights to others.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the representation ability of DNNs from the perspective of interactions (the multi-order interaction). The authors discovered an interesting representation bottleneck phenomenon, i.e., in a normally trained DNN, low-order and high-order interaction patterns are easy to be learned, while middle-order interaction patterns are difficult to be learned. Moreover, they give a theoretical explanation that the bottleneck origins from that different interaction patterns have different learning strengths (gradients). To relieve the bottleneck problem, the authors propose two novel loss functions, which allow the model to encode interactions of specific orders, including middle-order interactions. The experiments have validated the effectiveness of the proposed losses. Finally, the authors explored some properties of DNNs encoding different-order interactions. ",
            "main_review": "[Strength]\n1. This paper discovers a very interesting tendency (called representation bottleneck) in existing DNNs, i.e., a DNN is usually more likely to encode low-order and high-order interactions, but difficult to encode middle-order interactions. $\\\\\\\\$\n2. This paper analyzes theoretically the reason behind the bottleneck. Moreover, the simulated results based on Theorem 1 well match the real cases, which validates the theoretical reason. $\\\\\\\\$\n3. From the perspective of the bottleneck, the authors explore the difference between DNN learning and human learning, which provides the key idea of the proposed two losses. $\\\\\\\\$\n4. To relieve the bottleneck phenomenon, the authors propose two novel losses to enable DNNs to learn interactions of any specific orders. $\\\\\\\\$\n5. Experimental results demonstrate the effectiveness of the two losses, and show that middle-order interactions can provide useful information for classification. $\\\\\\\\$\n\n[Weakness]\n1. The explanation for the representation bottleneck is not intuitive enough.  $\\\\\\\\$\n2. There is a lack of discussion why the trained DNNs do not achieve a significant improvement than a normally trained DNN.$\\\\\\\\$\n3. The experiments on adversarial robustness can not well support the conclusions, since they were only conducted on tabular datasets. $\\\\\\\\$\n\n[Suggestions]\n1. I would suggest the authors give an additional illustration of the intuition behind the proof for the bottleneck. $\\\\\\\\$\n2. It’s expected to give guidance on how we can use the two losses to boost the classification performance. $\\\\\\\\$\n3. It’s not clear why the output change in Eq.(5) can represent interactions of specific orders. $\\\\\\\\$\n4. The authors are suggested to report results in terms of adversarial accuracy on more datasets such as image data.  $\\\\\\\\$\n5. The authors are suggested to clarify whether the proposed two losses will increase the time complexity.  $\\\\\\\\$\n\n***\nAfter carefully reading the response, other reviews, and the revised version of the manuscript. \n\n1. The added proof skeleton makes the theory more clear and intuitive. \n2. New experimental results show a strong connection between representation capacity and interaction orders, which may inspire follow-up studies. \n\nOverall, I think this work is insightful and may make a great impact on both training and explaining of deep neural networks. I have modified the score accordingly.\n\n\n\n",
            "summary_of_the_review": "This paper discovers and theoretically explains an interesting representation bottleneck in existing DNNs, and relieves the bottleneck by proposing two novel losses. As the discovered bottleneck reflects the common difficulty to learn middle-order interactions, this work may provide a new direction to train DNNs. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper looks at representation bottleneck of deep neural networks, from the multi-order interaction (within subsets of pixels) point of view. Specifically, the paper provides a novel definition on the relative interaction strength, and then analyzes this quantity both theoretically and numerically. The paper next proposes two loss functions that encourage or penalyze interactions of certain orders, and conducts multiple experiments on different combinations of these loss functions. ",
            "main_review": "The research topic of this paper is significant in understanding the representation power of deep neural networks, and is interesting to the entire community of deep learning. The writing of this paper is clear enough, and the main contents are easy to follow.\n\nStrengths:\n- The definition of the relative interaction strength is elegant.\n- The theoretical and numerical analysis in Section 3.2 is novel and interesting. \n- The loss functions to encourage or penalize interactions of certain orders are also novel and very natural.\n- There are experiments showing the effects of these loss functions and connecting them to properties such as accuracy and robustness.\n\nHowever, there are several concerns of this paper. The following are about theory.\n- Regarding Thm 1, how reasonable is the assumption that $\\frac{\\partial}{\\partial W} \\Delta v(i,j,S)$ is a Gaussian? At least some numerical validations should be made here. \n- Regarding the simulation in Fig 3, there is still a tiny gap between the blue and orange curves, which does not look like a random pattern. Can you explain where this gap comes from? \n- Regarding equation 5, there are two issues. First, it is confusing to reload the notation $\\Delta v$. I would recommend using another symbol. Second, are you missing an expectation in this equation?\n- In equation 7, it is clearer to write $L^+(r_1,r_2)$ rather than $L^+$. Simimlar in equation 8. \n- Regarding equation 9 and simulations in Fig 4, it is possible to use multiple $L^+$ or $L^-$ with different $(r_1,r_2)$ pairs? If so, why are you not using it? I feel this can provide more flexibility and is therefore potentially more useful. \n\nThere are also concerns regarding experiments. In summary, the experiments are not extensive and cannot well support the findings.\n- Regarding accuracy, what can we read from Table 1 (left)? The numbers seem random.\n- Regarding structural representations, Fig 5 & 6 make some sense, but what about other masking methods? \n- Regarding robustness, Table 1 (right) tells us that penalizing low-order and boosting high-order interactions harm robustness. What about doing the opposite, i.e. penalizing high-order and boosting low-order interactions? Does it improve robustness?\n- In general, the experiments are not extensive. In order to better understand the effects of the proposed loss functions, there should be experiments carefully adjusting the $\\lambda$'s and $(r_1,r_2)$'s so that we can extract general patterns of the effects. \n- In addition, I do not see any significant improvement by using the proposed loss functions. Are there scenarios where using the proposed loss functions outperforms standard training in any of the metric (accuracy, robustness)? \n- Finally, figures and tables are a bit small so hard to read. \n\nScores can be modified based on authors' feedback and additional experiments. \n\n----------------------\nAfter rebuttal: I have carefully read the response and added experiments.\n\n- The revised theory with milder assumption is great.\n- The additional experiments are excellent and provide a much more complete picture of the proposed theory. \n\nI have modified the score accordingly. ",
            "summary_of_the_review": "Theory is excellent but there is space for improvement in the experiments. \n\n----------------------\nAfter rebuttal: experiments are extensive and insightful enough. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors study the complexity of interactions between input variables in deep neural networks (DNNs). They use a multi-order interaction utility between pairs of variables to represent the interaction complexity between input variables encoded in DNNs. The results can be summarized as follows:\n\n1) DNNs are more likely to encode simple and complex interactions between pairs of input variables but aren't good enough to encode the intermediate ones. The authors define the simplicity of interaction depending on its contextual complexity.\n\n2) The authors develop novel loss functions to encourage/penalize the interactions of specific complexities and then, investigate the representation capacities of the learned DNNs.\n\n",
            "main_review": "Strengths:\n\n1) The novelty of the paper lies in using the multi-order interaction proposed in Zhang et al. (2020) to understand the complexity of interactions in DNNs. \n\n2) The major strength of the paper lies in the extensive study of different neural networks and datasets to back the claim. Also, the effect of the proposed loss functions $L^{+}$ and $L^{-}$ on deeper networks like AlexNet, VGG-16, and ResNet-18/20 is quite interesting.\n\n\nSuggestions:\n\n1) The number of sets $S$ s.t. $|S| = m$ is given by $n \\choose m$. This function increases from $m = 1 \\ldots \\frac{n}{2}$ and decreases thereafter till $n$. Thus, for achieving large $I^{(m)}(i, j)$ at $m = \\frac{n}{2}$, the model needs to capture dependence between the variables $i$ and $j$ for significantly higher number of contexts, compared to the cases when $m=3$ (local context) or $m=n-2$ (global context). One may not need the dependence to be large for all possible contexts in the intermediate context length case.  Hence, can the authors give an example to explain why the increasing number of contexts in the intermediate contextual interactions doesn't necessarily lead to a small $I^{(m)}(i, j)$ always?\n\n2) Since, the authors consider $v(S|x) = \\log \\frac{1 - \\Pr[\\hat{y} = y^{\\star}]}{  \\Pr[\\hat{y} = y^{\\star} }$ for computing $J^{(m)}$ (which depends on the probability scores of a data example), I believe the magnitudes of $v(S|x)$ aren't necessarily comparable for different examples. Then, isn't the following definition of $J^{(m)}$ a proper relative interaction strength to measure?\n\\begin{equation*}\n    J^{(m)} = \\mathbb{E}_{x \\in \\Omega} \\frac{ \\mathbb{ E }   I^{(m)} (i, j \\mid x)   }{\\mathbb{E_k } \\mathbb{E} |I^{(k)} (i, j | x) | \n }\n\\end{equation*}\n\nThe above interaction measure measures the strength of the contextual utility for each example.\n\n3) In the adversarial robustness experiments, are the low-order and intermediate-order interactions more robust to adversarial attacks? It will be great to have some experiments for intermediate-order interactions, to possibly showcase the fact that neural networks are susceptible to adversarial attacks because of the cognition gap.\n\n\n\n\n",
            "summary_of_the_review": "Overall, the paper makes an important evaluation study to further the understanding of the representation capacity of DNNs. The novelty is in using the multi-order interaction proposed in Zhang et al. (2020) to understand the complexity of interactions in DNNs and the proposal of different loss functions to encode/penalize interactions of specific complexities. Hence, I am leaning towards a positive score for the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}