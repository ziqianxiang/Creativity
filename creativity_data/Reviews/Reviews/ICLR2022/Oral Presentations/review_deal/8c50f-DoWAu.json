{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Oral)",
        "comment": "The paper is exceptionally well summarized by Reviewer QC5G which is difficult to improve up on. I will save the readers the effort of reading more text (without adding more substance). The reviewers unanimously rated this paper highly. The discussion has been robust,  enlightening and also has improved the revised paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a method to perform voice conversion through recent methods in Diffusion Based Probability Modeling (DPM) through Scochastic Differential Equations (SDE), building upon recent work Grad-TTS and Glow-TTS. The architecture is an encoder-decoder setup, trained separately, described in upcoming paragraphs. Viewed at a high level, the encoder maps input to 'noise', and the decoder inverts noise to output, in keeping with the SDE formalism. To transform average features to output voice, speaker info is conditioned on the average voice features before feeding to decoder. \n\nThe encoder learns to map input voice features to an average mel spectrogram. In order to do this, the Montreal Forced Aligner is used to align mel frames to phonemes. The average output voice (for encoder training) is obtained by aligning and averaging out corresponding phonemes in the dataset. \n\nThe main novelty in this paper is claimed to be in the decoder setup solving the reverse SDE. The authors derive a modified SDE that does MLE inference on the original reverse SDE. Through this approach, the claims are that the likelihoods are better, and its estimation is 'faster' (ostensibly, owing to getting around step size limitations). \n\nEvaluations are carried out for VCTK and LibriTTS, and quite well done, and the samples provided show that the approach works. Comparisons are made against several other approaches, include subjective evaluations and FID scores, and look for similarity and naturalness metrics. They also compare against different flavors of SDE setups (Variance Exploding, Preserving, etc.).\n\n\n",
            "main_review": "Post rebuttal\n-----------------\nMy initial review was largely positive, but for a few issues with the motivation \n- what does the MLE solver give over the usual Euler-Mayurama integration setup?\n- some checks on the formulation and intuitions behind proof.\n\nThe authors have sufficiently answered my concerns:\n- The MLE solver gives an optimized trajectory for data, even though the overall scheme is still first order, as shown through a simple example. The authors argue that while second (or higher) order schemes are very much possible, they would need more function evaluations and therefore not necessarily faster (this point could use another demo). \n- The formulation as far as I can see is correct, as shown in the author rebuttal. Intuitions behind proof are also summarized.\n- I think a lot more work could be done in exploring diffusion probability models on the lines of this paper, in regards to numerical integrators and formulation setup.\n\nI am raising my score.\n\nInitial Review \n-----------------\nI reviewed this paper with great interest. It is rather remarkable how much progress has been made in the last few years in speech modeling. It is also interesting that the newer models (and this is one of them) compute the input-output alignment differently from the attention mechanism used in works such as Tacotron. Likewise, it is inspiring that generative modeling research (e.g. flow based, and DPMs) finds application in speech models quite rapidly, and with telling practical use (e.g. WaveGlow vocoder).\n\nStrengths \n-------------\n+ Builds upon current work in DPMs (Grad-TTS, Wave-Grad) to come up with a setup for voice conversion. \n+ Uses a sensible scheme involving computing an 'average voice' into which speaker information is added and output voice is decoded. \n+ Paper contains a good review of existing work, and it is possible to piece together developments in score based modeling and adaptations in speech. \n+ Maximum Likelihood solver formulation for reverse SDE is well motivated and gives good results. It produces a first order scheme - not very different from Euler-Mayurama - with three additional hyperparameters. Supposedly, this results in an improvement in sample quality through optimized likelihood.\n\nWeaknesses\n-----------------\n- I found the mathematical derivations not very clearly explained. The development is generally easy enough to follow if we work out the slightly tedious derivations. However, I think it would benefit from a few lines of 'intuition'. What is the intuition behind the derivations, why does it work and in what types of SDEs does such a procedure work? \n- There seems to be an inexactitude (or mistake) in the derivations (equation 40). I think we get $ k^*_{t,h} = 1+ \\hat{k}_{t,h} $ (i.e. the original derivation needs an extra 1 on the right hand side).\n\n- The motivations behind the MLE derivations aren't very clear (at least to me). Is it that since we maximize likelihood, the setup is more optimized and therefore produces better samples, or are we talking about being able to take 'larger' step sizes (smaller N)? \n\n- As such, it seems to me that the scheme is O(h), and should not work well when timesteps are large. It would be nice to see some explanation or analysis of why the setup converges in such a small number of iterations. I would expect a second order scheme to converge faster. Likewise, a predictor-corrector scheme (as in the stem SDE works by Yang Song) improves performance. \n\n\n",
            "summary_of_the_review": "On the whole, the work looks plausible for voice conversion, even if it only used the base Euler-Mayurama scheme. The derived scheme produces better samples, or does so allowing use of much larger timesteps (~ N = 6). Nevertheless, I am not totally convinced because\n1) It is not explained why the optimization 'works' in an intuitive sense. I assume that it provides a path with better likelihood since it has been optimized\n2) The derivations ought to be gone over very carefully, and it is crucial that the details are correct. I would like the other reviewers to take a careful look at the derivations, especially so as I feel that there is a mistake in equation (40), which could carry over to the other steps as well. \n\nI rule this work as a marginal reject, but can change my decision after discussion and clarification. ",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a diffusion probabilistic model-based voice conversion method for one-shot voice conversion scenario.  The proposed method can generate high-quality converted speech compared to state-of-the-art approaches.  Furthermore, to improve the real-time factor, a novel stochastic differential equations solver is proposed which makes the diffusion model faster.  The proposed solver is also suitable for other generative tasks.\n",
            "main_review": "Strengths:\n(1) A diffusion probabilistic model-based voice conversion method has been proposed for the one-shot voice conversion scenario.  \n(2) The proposed method can generate high-quality converted speech compared to state-of-the-art approaches. \n(3) To solve the disentanglement problem, a new approach in which the encoder predicts the \"average voice\" is proposed.  This idea is interesting and novel.\n(4) For the decoder part, the diffusion-based method is adopted which reconstructs the converted voice from the \"average voice\".\n(5) To solve the problem of slow inference of the diffusion probabilistic model, a novel inference scheme is developed which significantly reduces the number of iterations.\n(6) A novel stochastic differential equations solver, named maximum likelihood SDE solver, is proposed with theoretical analysis and empirical studies.\n(7) A lot of experiments have been conducted to validate the effectiveness of the proposed method.\n(8) Besides the experiments on VC, the maximum likelihood sampling scheme is also validated by the CIFAR-10 image generation task.\n",
            "summary_of_the_review": "Based on the above main review (especially the strengths of the paper), the proposed method is novel enough and the experiments can well support the effectiveness of the proposed method.  think the paper could be accepted for publication on ICLR.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper tackles the problem of one-shot many-to-many voice conversion (with both unseen source and target speakers) using a diffusion model on mel-spectrograms. The authors propose a dedicated architecture able to generalize to unseen speakers naturally (without relying on Phoetic Posteriorgrams (PPGs) as in previous works) by conditioning the diffusion model on \"average-phoneme\" spectrograms together with the target speech. They also address the important problem of the errors arising from using the Euler-Maruyama scheme with large discretization steps by proposing a novel SDE solver, allowing to perform voice conversion with as few as 6 diffusion steps. They demonstrate the applicability of this solver on other modalities (CIFAR-10) using on unconditional model. Extensive and rigorous experiments are conducted on the voice conversion task and an accompanying web page is provided with numerous and convincing voice conversions.",
            "main_review": "This paper is well-written and remarkable on many aspects:\n- A novel SDE solver than can be of interest for anyone interested in Diffusion models is proposed. This solver is provably better than Euler-Maruyama discretization in the setting of a small number of diffusion steps,\n- The idea to condition the diffusion model on the \"average voice\" is novel and fully exploited by the proposed architecture,\n- There are extensive experiments that demonstrate the relevance of this approach,\n- A website showcasing many voice conversions is available and the examples are convincing,\n- The relevant literature is properly addressed,\n- Code will be available.\n\nSmall remarks:\n- p. 2: Chen et al. 2021a also reported results (for vocoding) using only as few as 6 diffusion steps.\n- \"most of them lack theoretical grounds\" may be a bit harsh.\n\n\n\n",
            "summary_of_the_review": "A paper of high quality, featuring substantial contributions that can be valuable to anyone interested in diffusion models. Code and models are available and may have practical usages.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Same potential harmful applications as any voice conversion method.",
            "recommendation": "10: strong accept, should be highlighted at the conference",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}