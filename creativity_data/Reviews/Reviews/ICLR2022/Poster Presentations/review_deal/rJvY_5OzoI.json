{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "In this paper, the authors investigate a multi-task RL actor-critic technique, where a single actor is used while multiple critics are trained (one per task, where each task corresponds to a different reward function). Experiments on several environments demonstrate that this method works quite well in practice.\n\nAll reviewers found the proposed approach sensible and effective, in spite of its simplicity. The main concerns were:\n- Lack of novelty: although this is indeed not a particularly original idea, the specific instantiation in the actor-critic setup is novel and well motivated\n- Some confusing / unconvincing experimental results: after receiving this feedback, the authors were able to upload a new revision that addressed the main concerns\n- Focusing on the \"multi-style\" aspect when this is essentially a multi-task algorithm: although I agree that framing it as a specific case of multi-task learning would make sense and would probably make more appealing to multi-task RL researchers, I do not consider this to be a major issue\n\nIn spite of being a relatively straightforward paper, I believe it is good to have strong empirical evaluation of such basic techniques disseminated to the research community, and I thus recommend acceptance, in accordance with reviewers' recommendations after the discussion period."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper wants to propose a simple method to deal with multi-task (style) problems.",
            "main_review": "strengths:\n\n1. The method is straightforward, extending the existing deep RL method via multiple value networks.\n\nweaknesses:\n\n1. The method can not generate different styles under the same environment, or put another way, this method can not generate different styles under the same reward function.  We only have one task or environment in many practical applications, but we want to generate behaviors with different styles. However, this paper can not generate multi-style behaviors when receiving the same reward signal. The algorithm proposed in this paper is more like a multi-task algorithm instead of a multi-style algorithm.\n\n**update**\nI have read the response of the authors and increased my score. I still think the title of this paper is misleading.",
            "summary_of_the_review": "This paper proposed a simple method for multi-task problems. The experimental results are convincing, but the usage of this method is limited. Because this method can only be used for environments with multiple reward functions, and their approach can not generate multi-style behaviors for the same task. I think it would be more reasonable for the author to reformat their paper as a multi-task paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes to extend the actor-critic frame to tackle multi-objective (multi-task) reinforcement learning. The key idea is to learn multiple critics that correspond to different reward functions. Then a single policy is optimized for a weighted combination of these critics. The paper applies the multi-objective RL to learning different styles of completing tasks, such as aggressive or defensive style in a boxing game. The paper shows that the proposed algorithm can beat several multi-task learning baselines.",
            "main_review": "Strength:\n1) The problem of multi-objective RL has many important application in the real world, such as games and robotics. This paper tackles the exact challenges in multi-objective RL.\n2) The proposed algorithm is simple, but effective.\n3) The examples in the evaluation (e.g. Pong and the boxing game) demonstrates an useful application of the proposed method: designing AI for games.\n\nWeakness:\n1) One important reference in multi-objective RL is missing: [1] \"Multi-Objective Reinforcement Learning using Sets of Pareto Dominating Policies, Van Moffaert et al., JMLR, 2014\". The proposed algorithm is very similar to Section 2.2.1 in that paper. The difference is that this paper extends the same multi-critic-single-actor idea to Deep Reinforcement Learning. However, the claim in this paper that \"seemingly no prior work explores the use of a single actor with multiple critics\" is not true. The existence of [1] significantly reduced the novelty and technical contributions of this paper.\n\nAdditional questions and comments:\n1) To train the multi-critic-single-actor (eq. 5, 6, 7, 8), do you need to specify a fixed set of weights w? Or are the w randomly sampled during the training stage?\n\n2) Most of the results are shown in a tabular form. It would be great to visualize the learning process with learning curves. Learning curves gives a lot more insights than the final reward, such as learning speed, sample efficiency, and the progress of learning different styles. It could also be a way to validate the claim that the proposed algorithm mitigates the negative interference between tasks.\n\n3) Section 5.3, \"MTPPO agents perform worse on average than agents trained on single-levels\". Are the agents \"trained on single-levels\" just trained on one out of the 17 levels, or trained on all levels but the policy does not take the one-hot encoding of the level as an additional input?",
            "summary_of_the_review": "The paper proposes a simple and yet effective algorithm for multi-objective reinforcement learning. The paper is well written and the results are convincing. However, it misses an important prior work that has similar high-level ideas. The examples in the Evaluation Section of this paper show its great potential in real-world applications, especially for designing AI in video games.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a single-actor, multi-critic approach to address the well-known problem of negative interference in multi-task RL (MTRL). The work focuses on a special case of MTRL described as “multi-style RL”, where the goal is to learn several distinct behaviours under the same environment dynamics. Experiments were conducted on a wide variety of environments: a path following domain, Pong, Sonic the Hedgehog, and a UFC fighting game. Results provide evidence that the proposed approach achieves a better final performance than a single-actor, single-critic MTRL baseline.",
            "main_review": "The proposed ideas are presented clearly and the exposition on related work throughout the paper is useful in guiding the reader.\n\nThe main technical contribution is the idea of training separate critics or value heads while maintaining a single, shared actor for multiple tasks. As the authors pointed out in the rebuttal, the choice of a single actor is significant for many compute-sensitive applications (e.g. video games), where inference can be done in parallel (e.g. computing the actions of several NPCs executing different “behaviours” in a single forward pass). I think the paper’s claims are interesting and somewhat surprising since they suggest that negative inference disproportionately affects value learning over policy learning. Furthermore, this challenges the need for the explicit separation of policies for different tasks, which is a common technique in the multi-task RL literature, e.g. [1,2].\n\nThe experiments consider several unique, well-motivated environments and tasks. Following the paper’s revisions, I find that the results convincingly demonstrate the claims and address the majority of my initial concerns. However, a comparison against a state-of-the-art multi-actor, multi-critic algorithm could have strengthened the experiments. Currently, the “single-style” baseline is meant to fill this role, however it is difficult to compare this to the authors’ proposed approach in terms of sample efficiency since: (1) the single-style is only trained on a single task vs multiple simultaneous tasks (2) single-style does not observe sample efficiency gains from shared multi-task training. \n\nAdditional comments:\n- Tables and graphs need to report the meaning of the error margins. i.e. are they reporting standard error, 90% confidence intervals, or something else? \n- Typos: “mutli” (second paragraph, first sentence), “mulit” (appendix B header)\n\n\n[1] Yang, Zhaoyang et al. “Multi-Task Deep Reinforcement Learning for Continuous Action Control.” IJCAI (2017).\n\n[2] Teh, Yee Whye et al. “Distral: Robust multitask reinforcement learning.” NIPS (2017).\n",
            "summary_of_the_review": "Following the rebuttal and the paper revisions, I vote to accept. The paper demonstrates that several distinct behaviours can be successfully learned with a novel single-actor, multi-critic setup. While the technique is simple and the main idea shares some motivation with previous work, I believe this will be a valuable contribution to the multi-task RL literature.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces a variant of actor-critic reinforcement learning algorithms where multiple critics are trained in a reduced version of multi-task training where a task allows multiple reward functions. Two variants are proposed for handling multiple reward functions: one where a backbone value function is trained with multiple heads and one where completely distinct value functions are trained. Evaluation considers three domains: tracing different shapes, Pong with aggressive-defensive styles, and Sonic the Hedgehog game levels (where levels are styles). Results are ambiguous as to the improvement of the architecture over classical methods. Some qualitative results are shown for a fighting game trained with the multi-critic approach.",
            "main_review": "# Strengths\n\n1) Simple yet novel approach\n\t- Exploring multiple critics for multiple reward functions for a single task is a natural idea that has not been done before.\n\t- There are clear ways to take these ideas further when needing to combine behaviors and author/control the behavior of RL agents trained with this method.\n2) Intriguing qualitative results\n\t- The UFC game videos are promising for the way behaviors are distinguished, even if the quantitative analysis is less clear. Without baselines it's hard to know if this would not obtain with other methods, but it's clearly promising.\n\n\n# Weaknesses\n\n1) Lack of statistical rigor in results\n\t- The results do not compare algorithms using tests for statistical significance of differences. This makes it hard to tell which results hold up under repeated trials.\n\t- From a glance the error bars in the Sonic results all overlap for most algorithms (Figure 4).\n\t- Similar overlaps seem to obtain for Pong (Table 1). For example, the SAC Defensive setting play time of MT w/One-hot is 1446±442, which overlaps the MN-MultiCriticAL 1637±113.\n\t- The UFC results do not report any measure of variance.\n\t- Action: Provide statistical testing of these differences. Or consider reporting other metrics recommended for RL evaluation (for example: https://arxiv.org/abs/2108.13264)\n2) Lack of baselines in the UFC domain\n\t- The authors note that the domain is meant for demonstration only and not as a benchmark. I found the qualitative results shown most compelling for this case and so was disappointed to not have baseline comparisons (acknowledging the costs for doing these experiments). \n\t- Action: Adding these baseline comparisons would be a great improvement to the paper. I recognize this may not be feasible, but want to indicate to the authors that this would be a strong demonstration of the work. An alternative may be to consider some other simple continuous domain (walker, half-cheetah, &c.) that more clearly demonstrates the potential to learn stylized behavior.\n3) No direct test of the interference hypothesis\n\t- One of the more interesting ideas in the paper is that MTRL may suffer from interference between task heads when sharing a backbone. In the methods this is the difference between the MH and MN variants of MultiCriticAL.\n\t- No tests in the paper or analysis investigate this claim.\n\t- Action: It would strengthen the paper to show statistical tests that clearly demonstrate this claim.\n4) Some missing technical details\n\t- Below are some minor details that should be easy to address to improve the paper\n\t1) Why do the single task networks perform so well? Are they only trained and tested in a single task setting? This was not explained in the methodology.\n\t2) What is the evidence that UFC agent \"smoothly transition between the different fighting styles\"? The results show different styles but do not directly assess transitioning between styles.\n\t3) How does one interpret Table 3 given agents were trained on joint parameter combinations, rather than the single dimensions shown? Are these roughly the extreme quadrants and center of a 2D grid?",
            "summary_of_the_review": "The paper presents a technique with promise for training agents to perform tasks in a variety of ways. The current empirical results are ambiguous when comparing the proposed method to baseline methods, leading me to recommend rejection.\n\nMy score would increase if the authors can provide clear (statistically) conclusive results of superior performance of the proposed method.\n\n**Update**\nI have increased my score in light of the additional experiment seeds added and clearer differences in performance from the resulting reductions in variance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper considers the setting in which a single RL policy has to be learned for several tasks (or styles), to be selected at run-time after the agent has been trained on all the tasks. The core of the paper is the identification of a gap in the literature, where it has not been tried to train several critics and a single actor for multi-task RL, with each critic seeing only one task to be learned. The paper proposes to do just that, and shows promising experimental results in a variety of environments.",
            "main_review": "The paper is very well written, and to me, easy to understand and to follow. It is also well-motivated, and the related work section seems quite complete. It seems that the authors did not believe themselves when they found out that single-actor multiple-critics settings are rare in multi-task RL, and spent a considerable amount of time looking for these papers.\n\nSpeaking of related work, it cites the Actor-Mimic paper [3], in which N critics are trained on N tasks, and then the actor is distilled to be good at all these tasks. The different with this paper, I think, is that the Actor-Mimic actor does not observe a task descriptor. Am I right? I have the feeling that the Actor-Mimic may deserve a bit more than an entry in a citation list in the background section, as it seems quite close to the main contribution of this paper.\n\nOne small remark regarding the experiments is that the authors focus on the impact of multiple critics (which is well-motivated). Our of curiosity, I would have been interested to see at least one experiment that compares MultiCriticAL with the state of the art (actor-based) of multi-task RL, just to see what fancy actors allow to do in comparison with multiple critics.\n\nRegarding the related work section, already very complete, a mention of Bootstrapped DQN could be added (with a mention that it is not multi-task) in case the paper is attacked on the novelty of multiple critics. Multiple critics are common in single-task RL, to provide uncertainty measures and good exploration [1, 2], but I'm not aware of an application to multi-task RL.\n\nOverall, I really like this paper, and recommend accepting it. It identifies an interesting problem and omission in the literature, and proposes a simple solution that performs well in many experiments.\n\n[1]: Osband, I., Blundell, C., Pritzel, A., & Van Roy, B. (2016). Deep exploration via bootstrapped DQN. Advances in neural information processing systems, 29, 4026-4034.\n[2]: Steckelmacher, D., Plisnier, H., Roijers, D. M., & Nowé, A. (2019). Sample-Efficient Model-Free Reinforcement Learning with Off-Policy Critics. arXiv preprint arXiv:1903.04193.\n[3]: Parisotto, E., Ba, J. L., & Salakhutdinov, R. (2015). Actor-mimic: Deep multitask and transfer reinforcement learning. arXiv preprint arXiv:1511.06342.",
            "summary_of_the_review": "The paper is well-written, easy to follow, and well-position regarding related work (with a note on the Actor-Mimic). The proposed method is simple, yet unique and performs well. An extra experiment that compares MultiCriticAL with policy-based multi-task approaches would have been nice, but I overall still recommend accepting this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}