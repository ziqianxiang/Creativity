{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The manuscript describes a method for improving the computational efficiency of randomized ensemble double Q-learning for continuous action RL, by using a small ensemble of Q-functions equipped with dropout and layer normalization, achieving matched sample efficiency at considerably less computational cost.\n\nReviewers praised the method's simplicity and achievement of its stated objective of reducing the computational cost of deploying ensemble Q functions. In general, the paper was found to be easy to understand and well written. Several expressed concern about the lack of interrogation of why this combination of dropout and layer norm worked so well and an overall lack of novelty. Other miscellaneous criticisms were well-addressed in rebuttal and extensive new analyses in the Appendix were noted by several reviewers as adding much to the work.\n\nIn the AC's opinion, this is an example of a simple but non-obvious combination of well-known ideas that works very well. The review process has improved the level of empirical rigor that has gone into understanding the properties and trade-offs of this method. I'm happy  to recommend acceptance, though would echo reviewers concerns that dubbing the method \"Dr.Q\" will lead to confusion and would strongly urge adopting another name for the camera ready."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work proposes a new algorithm based on REDQ. The new algorithm Dr. Q. uses a small ensemble of dropout Q functions along with layer normalization. The proposed algorithm achieves similar sample efficiency as REDQ but better computational efficiency. ",
            "main_review": "Strengths:\n\nThis was a very interesting paper. Contrary to the ensemble approach for bias reduction which is rapidly gaining popularity in recent years, the authors proposed an alternative which allows an SAC-like algorithm to work in high UTD settings by adding dropout and layernorm, which in my opinion is the main contribution of this paper. The presentation of the paper is quite clear and it was very easy to read. Experiments are quite thorough and very well done. The ablation studies are also quite clear in presenting how the different components of the algorithm affect the performance of the algorithm.\n\nWeakness:\n\nMy main concern is that it is not entirely clear from the paper why the method presented by the authors work so well. Dropout, layernorm, and high UTD have all be separately applied to and studied in off-policy algorithms and it is not clear why combining all three leads to such a boost in performance. Intuitively dropout has similar effects as ensemble models since they add noise to the Q networks thereby injecting uncertainty into the target, but based on the authors' ablation studies and previous work, having dropout alone is not sufficient. More analysis on this front would do much to strengthen the authors' overall arguments.\n\nOne final minor point, I suggest modifying the name Dr. Q, I don't think adding the period after Dr will help with avoiding name conflicts with DrQ as the authors have suggested.",
            "summary_of_the_review": "Though the paper could benefit from additional analyze on explaining the performance of the proposed algorithm, I think this paper brings about some valuable insights which may benefit and impact how off-policy algorithms are implemented in the long-run.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper attempts to address computational efficiency in Ensemble-based Q-learning. The authors claim that previous, REDQ is too inefficient computationally as it requires too many Q-networsk in the ensemble. Thus, they propose an alternative approach that employs dropout (and layer normalization to stabilize the dropout layers), in order to get the same benefit of a larger ensemble in uncertainty estimation, while keeping computational costs low. The results and analysis on standard continuous control benchmark tasks show that the proposed approach maintains all the benefits of REDQ without the computational costs. ",
            "main_review": "Strengths: \n\nThis paper is well-written and easy to understand. The authors present a compelling reason to use ensemble-based Q-learning and why ensembles can be harder to work with from a computational perspective. I thinkt he obser The results and analysis support the hypothesis that dropout and layer normalization can have the same effect as a larger ensemble. \n\n\nWeaknesses: \n\nI believe novelty is a major weakness for this approach. Many of the individual components presented in this paper (dropout for Q functions, ensembles, etc) have been discussed in previous works. The authors claim that these have not been studied in high UTD settings, however the authors do not present any evidence that there is a difference between how REDQ and their method work at high UTD or lower UTD. Such a set of experiments will really help make this paper's claim stronger. Additionally, since the authors use M=2, this approach is very similar to a vanilla SAC implementation. It would be good to see an analysis of SAC with the dropout architecture (essentially this method but without any of the improvements from REDQ). ",
            "summary_of_the_review": "The paper presents a useful and interesting improvement to REDQ but novelty claims are not backed by experiments. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Authors propose to use dropout and layer norm to make Randomized ensembled double Q-learning more computationally efficient. The proposed algorithm uses simple modifications to achieve comparable sample efficiency while being faster in computation. Authors also conduct empirical analysis on how dropout can be used effectively. ",
            "main_review": "======Strengths=======\n\n- Simplicity: the proposed method requires minimal modification to existing methods, making it easy to implement and use\n- Significance: REDQ has achieved very strong sample efficiency but the ensemble of Q network leads to slower computation. How to achieve the same sample efficiency while reducing computation seems to be a very important question to solve. \n- Analysis: comparison on computation and memory is good, shows significant reduction in computation and memory usage\n- Results: results is nice, showing that the proposed method can achieve same sample efficiency of REDQ while being much more computation and memory efficient.\n\n======Weaknesses=======\n\n- Clarity of writing: some of the writing is not very accurate or not clear and can be refined\n- Lack of in-depth investigation on why dropout and layernorm alone do not work well, and why dropout+layerrnorm works well, lacks comparison to other regularization techniques\n- A number of other issues in the paper\n- Proposed method lacks novelty: the proposed method (dropout+layernorm) seems to be a bit incremental. \n\nConcerns, comments and questions:\n- In page 1, \"the UTD ratio is defined as the number of Q-function updates...\", in the REDQ paper, this is actually defined as \"the number of updates taken by the agent compared to the number of actual interactions with the environment\", which is slightly different from your definition here (only the Q update), although for the main results reported in the REDQ paper, they focus on only changing the Q update number to isolate the effect of the policy update, you might want to be more clear as to prevent confusion. In your case, you can further clarify your writing, perhaps be more explicit and say its general definition and then say in your paper you define it to be only focused on Q update. \n- Authors should check the writing again for small typos and mistakes. For example, should be MuJoCo instead of Mujoco. Figure 2 typo \"and average and\"\n- top of page 2, \"REDQ runs more than 2 times slower than SAC\", you might want to further clarify here (and in other places in the paper where you mention this), are you talking about the computation time per update (where REDQ is a bit slower due to ensemble)? Or per environment interaction collected (where REDQ is a lot slower due to ensemble and high UTD)? Or the computation time for running until a performance level (where REDQ is not that slow due to reduced sample usage, I feel like this one is what you are talking about)? It is very important to make this point crystal clear since a majority of your discussion is on this subject. And helping the reader to gain a clear idea of exactly how much slower REDQ can be would help readers see the significance of your contribution.  \n- What is the full name of DUVN? Is this sth proposed in previous work? It is a bit unclear how this variant relates to your proposed method. Might want to explain it in a bit more detail. \n- page 6 definition of the error (bias), the current definition is a little weird, it might be better to define the error as the difference of Q values, and then say we can estimate this error with MC return, instead of defining Q as the MC return. (I believe typically Q value is defined as expectation of MC return, not the other way around)\n- end of page 6 \"the most memory-intensive process... (applying the ReLU layer)\" are you saying relu activation takes a lot of memory? \n- Figure 2 hopper result seems to be lower than reported in the REDQ paper, not sure why? \n- Ablation shows that layer norm helps with performance, but in the paper there is very limited discussion and analysis on why exactly layer norm makes dropout work better. What is so special about layer norm? Previously dropout had mixed results in DRL, why layer norm can fix the problems? What about other regularization schemes? Adding comparison to other schemes, and more analysis on how layer norm and dropout work together, either theoretically or empirically can help improve the significance of this paper. Otherwise the layernorm seems to be just a random hack added to boost performance. \n- Though the name of Dr.Q is technically different from DrQ, I'm still a bit concerned whether this will lead to confusion \n",
            "summary_of_the_review": "I think the paper has potential to make some good contribution, the authors already obtained some good results showing that Dr.Q can achieve the same performance as REDQ while greatly reduce computation and memory consumption, which is very nice. However, what is lack is some more in-depth investigation on why the proposed method works. Why dropout on its own does not work? And why layer norm will deal with the difficulties? What are the unique advantage of this combination over other methodologies? Very little discussion and empirical/theoretical analysis is provided in the paper. I think if the authors try explore these questions more, they should be able to make a bigger contribution and make this paper much stronger. \n\nThough the paper studies an important problem, and has some good results, due to this major concern, and considering other issues in the paper, I don't quite think the paper is ready for publication in its current state, thus I currently lean towards rejection. \n\n==============================================================\n\nPost-rebuttal: after reading the authors' response and the revision, I feel that a large number of my concerns are addressed and the new results ablation and analysis look quite interesting. Though I still have some concerns, I now increase my score to 6. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}