{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes an algorithm for achieving disentangled representations by encouraging low mutual information between features at each layer, rather than only at the encoder output, and proposes a neural architecture for learning. Empirically, the proposed method achieves good disentanglement metric and likelihood (reconstruction error) in comparison to prior methods. The reviewers think that the methodology is natural and novel to their knowledge, and are happy with the detailed execution. The authors are encouraged to improve the presentation of the paper, by providing rigorous formulation of the \"Markov chains\" to avoid confusions, justification of the independence assumptions behind them, and more in-depth discussions of the learning objectives."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper formulates the disentanglement problem from an information theoretic perspective, but focusing on an objective that encourages a compositional disentangled feature space on the layers that precede the final latents.\n\nWith objective, the authors describe a new method using Gate of Mixture-of-Experts to implement the compositional disentangled reconstruction objective. Some of the terms require mutual information estimation, for which they use MINE estimators.\n\nThey run experiments across dSprites and 3DShapes and look into reconstruction error and different disentanglement metrics, observing that they method outperform existing beta-VAE-like baselines, without any compositional incentives. They also analyse the loss components with different architectures and observe that degrees of compositionally in the architecture yields better disentanglement.\n\nFinally, they look into some ablations of the regularisation pressure and into data efficiency in downstream tasks.\n\n ",
            "main_review": "Overall, I am pretty happy with the paper. It's mostly well written and organised.\n\n1. Positives\n\n* Session 2 (Compositional disentanglement learning) is well organised and sets up the scene well for the method in session 3.\n* Good level of implementation detail is available, such as architectures, estimator used, etc. The experiments are well conducted and common mistakes were avoided AFAICT.\n* Use of standard datasets and metrics well stablished in the field.\n\n\n2. For improvement:\n\n* There has been some progress in the use of hierarchical VAEs, which can be interpreted as applying disentanglement regularisation to other layers and making it compositional in a similar fashion to this work, e.g. NVAE (A Vahdat, J Kautz 2020). \n* I would be a bit more careful with the tone on claims about the requirement of compositionality for disentanglement. Figure 1 is only an evidence in a toy example, not an actual demonstration. So statements as \" fig 1 shows when ... is not effectively disentangled.\" (session 1) and \"To achieve better disentanglement between ... their input feature sets .. are expected to be disentangled as demonstrated in our case study\" (session 2.2) could be watered down a little.\n* It's unclear if there are benefits from using MINE and the architecture in some of the experiments. For example, if I understood correctly, the beta-VAE objective yields better metrics on figure 6 than in figure 3.\n* The paper left me wondering what are the disentanglement metrics on the preceding layers, looking into MIG/SAP/DCI-D on m^L-1 and m^L-2 seems like a straightforward analysis that should be in the appendix or even in the main paper.\n* Some ablations on the architecture itself also seem to be missing (ablations on the loss are good). My intuition is that Gate of Mixture-of-Experts fits quite well with the disentanglement that we want, because of the top . For example:\n  * just learning a linear+softmax instead of the Router\n  * no routing at all, just some fixed assignment: e.g. split m_l in d+1 equal slices and pass through the encoders.\n  * using a transformer instead of the Recursive Modules, perhaps over fixed slices as well.",
            "summary_of_the_review": "My main concern is the discussion of related hierarchical models missing from related work and the emphasis on this being the only work to apply some disentanglement pressure outside the main z latents. This should be an easy fix for this paper.\n\nThe compositional objective is interesting and novel and the implementation method is clean. The experiments were well conducted and the well analysed. Overall, I am confident that the authors will be able to address the main issue above and that this paper will award acceptance in this venue.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a new approach for learning disentangled variational autoencoders. In addition to pushing the sufficiency, minimal sufficiency, and disentanglement of the latent representation, the paper proposes to also regularize those on earlier features in the network. Experiments demonstrate promising results.",
            "main_review": "Overall, the idea is technically sound and the results look promising. I have some questions and suggestions and hope the authors could clarify them during the rebuttal.\n\n* Section 2.1: what is the meaning of defining Markov Chain as \\hat{x} -> x -> z, given that the generative process is actually x -> z -> \\hat{x}? I looked at the cited work (Achille & Soatto, 2018), and they seem to discuss a different setting, where they have a dataset of data points x and associated labels y, and in that case, y -> x -> z makes sense to me.\n\n* Definition 5: Similarly, what is the practical meaning of m_j^{l+1} -> x -> (m_i^l,m_j^l)?\n\n* Figure 6 left: do you use compositional objective (Section 2.2) and recursive disentanglement network (Section 3) in this experiment? If so, lambda_2=0 is not equivalent to beta-VAE as the vanilla beta-VAE does not have the compositional objective.\n\n* Figure 2: the 'w_j^l' in green should be 'w_{d_{l+1}}^l'?\n\n* I would suggest adding GAN-based approaches into the comparison tables. This would be very helpful for readers who want to pick techniques for their downstream applications. \n\n* The decomposition and discussion of the losses around Eq. 2 and Table 1 have been partially discussed in prior work (e.g., https://arxiv.org/abs/1706.02262), and I believe it is not your key contribution. I would suggest highlighting this fact better as it sounds like these are your discoveries from the current writing.",
            "summary_of_the_review": "In summary, the idea is interesting and the results look promising. I hope the readers could clarify these questions and I will adjust the score accordingly.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents a VAE variant that disentangles the features of the inference network at every layer, with disentanglement defined in terms of mutual information between features. The approach is implemented as \"recursive disentanglement network\" based on a switch network (aka Mixture-of-Experts gate, introduced in Shazeer 2017 and used in the switching transformers). The results in dSprites and 3DShapes dataset suggest this variant performs better than well-known disentanglement VAE networks (from few years back) in dSprites and well in 3DShapes (though not the best in all measures). In terms of VAE loss, the approach is presented as a generalization of various other disentangling VAEs.",
            "main_review": "Strengths\n+ The paper presents an approach that is theoretically justified and implemented in a convincing manner.\n+ The results appear convincing and beating relevant baselines (though with the disclaimer that mostly these baselines seem rather old by now; some of the relevant comparisons from the last 2 years might be missing, but I cannot name any).\n+ Empirical evaluations are sufficient (though only barely), except for the ablation (*):\n\nWeaknesses\n- I suspect that including layer-wise disentanglement has occurred to many people before, but it has not been attempted due to the computational burden. (That said, I have not seen anyone actually try it.)\n- It is unclear whether this approach *solves* the computational burden and scales to more complex datasets and larger image resolution\n- I am not convinced that the loss formulation (Eq. 4) is that significant wrt. InfoVAE, for example. What would the authors say against the criticism that Eq. 4 is basically just reshuffling the InfoVAE loss? (I'm not saying it is, but it would be helpful if the authors could shoot down this potential concern.)\n- (*) It is unclear to what extent does the performance originate from the loss (Equation 4), and to what extent does it come from the switch-based architecture. Could the authors clarify this? It would seem that one could implement Eq. 4 loss without the switch architecture?",
            "summary_of_the_review": "The paper appears to simultaneously give a clear slightly novel generalization to disentanglement losses of prior VAE variants, and to provide an architectural approach to implement their approach.\n\nI have concerns about the scalability, and I am suspicious about whether such a heavy-handed disentanglement can be maintained in larger models.\n\nIt also was not clear to me whether the results originate from the loss or the architecture advances (which build on existing switching architectures).\n\nHowever, either way, I think this is a novel approach and potentially a significant addition to the VAE disentanglement research, and I lean towards acceptance in this case, though I urge the authors to address my questions.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a recursive disentanglement network (RecurD) for the learning of disentangled representations from information theoretic perspective. The experimental results show RecurD outperforms some existing baselines on two benchmark datasets.",
            "main_review": "Pros:\n1. Developed a compositional disentanglement learning, called RecurD, that directs the disentanglement learning process across the compositional feature space.\n2. Provide some theoretical analysis based on information theory. \n\nCons:\n1. Optimizing the lower bound of Eq.2 does not mean obtaining the optimal objective function of  $\\beta$-TCVAE paper [1] on the right-hand side of Eq 2. As far as I know, the right-hand side of Eq 2 is the objective of $\\beta$-TCVAE. If we optimize the objective function on the left-hand side, it does not hold for optimizing $\\beta$-TCVAE. Thus, I am afraid that the proposed objective function in Eq 1 fails to be generalized to the existing $\\beta$-TCVAE and FactorVAE. In contrast, optimizing the objective of $\\beta$-TCVAE is approximately equivalent to optimizing the proposed objective function in this paper. What is more, in Table 1, $\\lambda_c =1$ for the original $\\beta$-TCVAE in their paper. \n2. Do not specify the number of Gate of Encoders (GoE) for different datasets. It is hard to know how many GoE should be used for a new dataset, like CelbeA, that contains 40 latent variables. Also, I am curious about the complexity of the proposed network?\n3. The upper bound and lower bound are confounding. On page 9, the author said: mutual information I(x,z) is the upper bound of KL divergence. In fact, based on the proof in prior work [2], I(x,z) is the lower bound of KL divergence. \n4. The Markov Chain in Eq 7 is incorrect. Based on my understanding, the next state $X_{t+1}$ of Markov chain is only related to the current state $X_t$. Hence the joint probability of p(a,b,c) = p(a)p(b|a)p(c|a) rather than p(a)p(b|a)p(c|b). This is because c is conditionally independent on b as you mentioned, b->a->c.\n5. Missing baselines in recent years. The author only discussed and compare the proposed method with the baselines before the year of 2019. There are some recent works [3] [4] [5] on improving the disentanglement and reconstruction error. For instance, ControlVAE [5] dynamically tunes the weight \\beta on the KL term to achieve a good trade-off between disentanglement and reconstruction error.\n6. Did not conduct experiments on complex datasets. The authors should do experiments on 3D chairs or CelebA to demonstrate the good performance of the proposed method.\n7. The result in Fig 5 does not perform well. We can observe that for orientation and scale factors, they are slightly entangled. Besides, the reconstruction quality is not as good as ControlVAE and FactorVAE in the paper. In particular, ControlVAE and FactorVAE  can disentangle both 5 latent factors which are better than those in this work. \n8. There are some typos in this paper, please proofread this manuscript. For instance, priopr work --> prior work on Page 5.\n\nReference:\n[1] Chen, R. T., Li, X., Grosse, R., & Duvenaud, D. (2018). Isolating sources of disentanglement in variational autoencoders. arXiv preprint arXiv:1802.04942.\n[2] Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, Sergey Levine: Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow. ICLR 2019.\n[3] Patrick Esser, Johannes Haux, Bj rn Ommer: Unsupervised Robust Disentangling of Latent Characteristics for Image Synthesis. ICCV 2019.\n[4] Srivastava, Akash, Yamini Bansal, Yukun Ding, Cole Hurwitz, Kai Xu, Bernhard Egger, Prasanna Sattigeri, Josh Tenenbaum, David D. Cox, and Dan Gutfreund. \"Improving the Reconstruction of Disentangled Representation Learners via Multi-Stage Modelling.\" arXiv preprint arXiv:2010.13187 (2020).\n[5] Shao, H., Yao, S., Sun, D., Zhang, A., Liu, S., Liu, D., ... & Abdelzaher, T. (2020, November). Controlvae: Controllable variational autoencoder. In International Conference on Machine Learning (pp. 8655-8664). PMLR.\n\n",
            "summary_of_the_review": "I think the authors have addressed most of my concerns. I will increase the final rate.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}