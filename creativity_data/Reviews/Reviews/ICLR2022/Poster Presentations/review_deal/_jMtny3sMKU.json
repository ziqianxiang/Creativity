{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "All reviewers give acceptance scores. \nOne reviewer also commented that they would like to increase their score from 6 to 7 (which isn't possible in the system).\nI encourage the authors to add the substantial new results generated during the rebuttal into the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposed a method for few-shot NAS tasks. It found some operations may behave similarly and can be grouped into one and others may not. Those patterns should be considered to save the computation as well as improve accuracy. It proposed a graph construction and spitting pipeline to achieve the goal. Specifically, they used gradient matching and cosine distance to construct the graph among layers. Then, splitting the graph by solving a graph clustering problem. \n\nIt conducted experiments on three benchmarks (cifar10, cifar100, and ImageNet) over three search spaces (NASBench-201, DARTS, MobileNet Space) and compared with various baselines. The results show the strength of the proposed method. ",
            "main_review": "The studied direction is important that we need to endow NAS algorithms quickly adapting to new tasks or data. The work takes a step further based on previous methods. It carefully examines the potential effectiveness improved by grouping operations. The introduced techniques are resonable, like gradients matching, cosine distance, and etc. Overall the paper is very easy to follow and the Figure 2 clearly shows what parts of the problem the method focused on.\n\nIn addition, this paper conducted extensive experiments, especially on the comparisons with previous methods, including DARTS, RSPS, SNAS, ProxylessNAS, and OFA and search spaces NASBench-21, DARTS, and MobileNet Space. On three benchmark datasets, it achieved consistent improvements and some of them are significant (Table 3,4,5). Several ablation studies are also conducted, including comparing different similarities measures, edge selection, warmup epoch numbers, and restart.\n\nThe reviewer especially likes Appendix B.1 and E where give some instances of the method and the reader can have a certain high-level sense regarding the method. Probably, can also mention some of the contents in the main paper.\n\nRegarding negative points, the reviewer mainly doubts the novelty of the paper since it focused on a small part of the whole problem by utilizing existing tools. Also, the reviewer didn't see many around cost comparisons in the main paper, like time and paranum.\n\nFurthermore, the paper used a very simple algorithm for graph splitting. Probably, more discussion and implementations should be introduced here and add them into the ablation section.\n\n\n-----------------------------after rebuttal---------------\n\nThe reviewer thanks the authors' efforts. Yes, it is figure 1, and the reviewer is sorry for the missing points. But, the reviewer doesn't get convinced by the arguments about the novelty. This work currently studied a relatively small point with existing techniques. It is ok to be accepted, but the reviewer will not fight for it. Also, as the author said, they focused on the splitting criterion, and thus, the reviewer encourages the author to add more comprehensive studies around the graph splitting algorithm. ",
            "summary_of_the_review": "Overall, the reviewer thinks the paper is solid while lacking some novelty. The reviewer appreciates the details mentioned in both the main paper and the appendix.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work is a further exploration in the direction of Few-Shot NAS. It proposes a method of partioning supernet based on gradient-matching score. Compared with the method based on exhaustive-spltting, the method proposed in this paper can achieve better results. Its contributions are as follows:\n\n1. Point out the problem based on exhaustive-splitting method;\n2. A partioning method based on gradient-matching score is proposed, and this method can achieve good results on multiple search spaces and datasets.",
            "main_review": "### Pros:\n1. This work answers two questions about how to partion supernets in Few-Shot NAS: 1) Is exhaustive-splitting good? 2) How to partion is better? I think this work has basically answered the above two questions.\n2. This work proposes a partioning criteria based on gradient-matching score and formulate the splitting as a graph clustering problem. This partioning method further improves the performance of Few-Shot NAS and  the derived GM-NAS outperforms its Few-Shot and One-Shot counterparts while surpassing previous comparable methods in terms of the accuracy of derived architectures.\n\n### Cons:\n1. I don’t quite understand what is the meaning of showing Train Loss in Table 2? Low Train Loss does not mean that the accuracy of the model on the validation set is high. I think you should show the effect of share weight on the model’s performance on the validation set under different Grad Similarity conditions.\n2. I don't quite understand why the size of  $\\mathcal{U}$ should be restricted in Eqn.(3)?\n\n### Some typos: \n1. On the left of Table 2 in Section 3.3, lowercase should be used after the semicolon.\n",
            "summary_of_the_review": "Overall, I vote for accepting. First, the overall logic of the paper is relatively clear, basically answering two questions in Pros of **Main Review**. In addition, I think it is reasonable to consider the supernet partition problem from the perspective of gradient, and this work proves the effectiveness of this method through experiments. Hopefully the authors can address my concerns in the rebuttal period. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper improves the sub-supernet splitting strategy in few-shot NAS with a gradient matching (GM) score. During supernet optimization, the GM explicitly computes the \"agreements\" among different operations on an edge in the supernet, which is then treated as the criterion to split those most mismatched operations into different sub-supernets. Although the basic intuition makes some good points and is easy to grasp, I have several concerns on the methodology and the experimental designs of this paper. ",
            "main_review": "This paper is well written and easy to follow. I appreciate the efforts made by this paper on alleviating the weight-sharing problem in one-shot NAS approach by proposing a more reasonable few-shot NAS method from a supernet-splitting perspective. Without many bells and whistles, the proposed method also demonstrates consistent performance gains over its counter-part algorithms.\n\nThough promising, I have three main concerns about this paper:\n\n1, According to section 3.4, greedy algorithm is employed for edge selection, where in each split phase only the edge with maximum sum of GM score is considered to be split. I am doubtful about this specific design because the multi-step split process constitutes a decision chain where local-only criterion usually does not serve for the global purpose. I am willing to see more theoretical (or at least experimental) justification/analysis on this part.\n\n2, All supernet (search space) involved in this paper have orders magnitude more number of edges than three (i.e. the number of edges to be split). Does it suggest the proposed approach can only slightly reduce the weight-sharing extent in the scope of whole supernet? If so, why the searched architectures are extremely close to the optimal one (e.g., Table 3) . \n\n3, There are no ablation studies on the performance changes w.r.t. the different settings of \nT\n without considering search cost. It might serve as an important indicator to help understand how much extent the weight-sharing affects the sub-network evaluation accuracy and might also provide some hints for my last question.\n\nOther minors:\n\n1, What is the Random (baseline) method in Table 3. Does it refer to the random partition?\n\n2, Missing T and B settings for NASBENCH-201.",
            "summary_of_the_review": "This paper presents a new partition strategy for few-shot NAS, which empirically seems to be a more efficient solution to alleviate the weight-sharing problem in one-shot NAS. The intuition/algorithm details are well demonstrated and the experimental evaluation is clearly stated.\nOverall I believe the quality of this paper has met the bar of ICLR community, but my concerns about the justifications for its technical designs and some numerical results make me indecisive on posing an acceptance. \n\nI would like to raise my rating if authors properly address my concerns.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper aims at improving few-shot NAS by proposing a gradient-guided schema to partition the supernet into sub-supernets during the search. The proposed splitting criterion is based on the cosine similarity between the gradients from different child models. Namely, child models that have similar gradients are more likely to be grouped together without splitting, so as to save computational costs and to allow the splitting of more layers than the vanilla few-shot NAS, achieving better performance. Extensive experiments show its superiority over the few-shot NAS.",
            "main_review": "Pros\n++ The paper identifies a drawback of the vanilla few-shot NAS, which is the exhaustive partitioning. \n++ The paper provides a criterion, ie. graph min-cut on the gradient similarity between operations, to partition the (sub-)supernet. \n++ Some toy examples (Table 1 and 2) are provided to support the claim of the paper regarding (1) the necessity of designing a partitioning rule and (2) that the gradient similarity is highly related to the search quality.\n++ Extensive experiments show that it is better than the vanilla few-shot NAS. Moreover, it is shown in the ablation study that the ranking correlation in indeed improved by the proposed splitting criterion (Table 7).\n\nCons\n-- The experiments of the proposed GM are mainly designed to match the search budget of few-shot NAS for a fair comparison. But how about the performance of GM when more or less search budge is considered? For example in Table 5, what would happen if less search costs are used when combined with proxylessNAS? Also, what would happen if more search costs are used when combined with OFA_Net?\n-- What is the performance of GM+DARTS and GM+SNAS on ImageNet?\n-- The improvement over OFA_Net in Table 5 is marginal. ",
            "summary_of_the_review": "Overall, it is a paper of good quality. I lean toward accepting the paper because of the intuitive splitting rule proposed in the paper and the supportive results. I will consider raising the rating if the authors can address my above-mentioned questions regarding the experiments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}