{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper presents an approach for spatio-temporal representation learning using Transformers. It introduces a particular architecture design, which shows an impressive computational efficiency. The reviewers agree that the experimental results are strong, and unanimously recommend the paper for acceptance. The reviewers find their concerns regarding the details of the approach/setting address after the authors' response.\n\nWe recommend accepting the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a new computation block that unifies 3D convolution and transformer block for video action recognition. Starting from the transformer block, the core of the proposed block is its attention module which is equivalent to 3D CNN or transformer block depending on the design of the learnable parameters. On the standard action recognition benchmark, the proposed method outperforms prior art in both accuracy and computation efficiency. The comprehensive ablation studies provide insights on the design choices.",
            "main_review": "Strength:\n- The paper makes an insightful connection between X3d's conv block and transformer block, and build a unified computational block. Compared to current methods focus on adding good properties of conv blocks to new transformer blocks (e.g., swin-transformer), the proposed method is more elegant conceptually.\n- On the standard video action recognition benchmark, the proposed method achieves SOTA result, which is impressive.\n- The paper provides comprehensive ablation studies on the design choices.\n\n\nWeakness\n- Lack of reference of previous works on the relationship between conv layer and transformer block.\n- Table 2 and Table 3. The bold highlights are inconsistent. Table 3 highlights the best accuracy results among all methods, while Table 2 highlights results are not so. In addition, highlights in Table 2 may make readers ignore the fact that MoViNet-A6 results are comparable in both efficiency and accuracy.\n- Lack of explanation of DPE. Currently, DPE is briefly introduced as an extension of CPE in the subsection of MHRA. However, DPE is not part of MHRA and it'll be better to have it as its own subsection.\n- Lack of comparison with MoViNet-A6+transformer. Despite the bigger design space of L and G, the ablation studies show that the CNN+Transformer architecture works the best. Thus, it is unclear if the proposed method can outperform MoViNet-A5+Transformer blocks with similar GLFOPs. This is only a minor comment, as the proposed method already achieves significant enough performance for publication.",
            "summary_of_the_review": "This paper proposes a novel computational module that is elegant conceptually and effective empirically. Conceptually, the proposed block bridges the design choices between the 3D CNN and transformer block, which can be influential to future works on designing better structures in the attention module. Empirically, this paper achieves SOTA results in both accuracy and efficiency. Thus, I r",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A.",
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new transformer model for video understanding task. The proposal algorithm has the benefit of both 3Dconv to efficient capture local context and transformer for global reasoning. The other innovation is the dynamic position encoding. The proposed method is validated on Kinetics and Something something where it set new accuracy record.",
            "main_review": "+ Well-written \n+ Thorough ablation study, including clear visualization of the attention\n+ Clean formulation for the local and global affinity token and how they imitate 3Dconv and the standard transformer model.\n+SOTA accuracy with 10x more efficiency compared to strong baselines.\n\n-It would be nice to understand how the method apply to fine grained tasks such as human keypoints or segmentation as transformers are usually not very good in such task (vs. convolution). \n\n",
            "summary_of_the_review": "Solid paper with clean formulation, experimental analysis, and well written. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, a new architecture, Uniformer, is proposed to learn spatial-temporal pattern in videos. \nThe new architecture is claimed to effectively aggregate both local information and global information.\nIn contrast, the previous two main approaches, 3D CNN and vision transformer, can only take care of one aspect.\nThe main building block for Uniformer is a Multi-Head Relation Aggregator, which behaves differently at shallow layers and deep layers, to aggregate local and global information respectively.\nExtensive empirical studies demonstrate the strength of the proposed method.  ",
            "main_review": "Strength:\n- The empirical study is sufficient, both the state-of-the-art comparison and ablation study.\n- The general idea of taking the best from 3D CNN and vision transformer is interesting. \n\nWeakness:\n- The related work is pretty brief, which does well explain the relation w.r.t. to the literature in similar direction. E.g. (Liu et al., 2021b).\n- The phrase \"local redundancy\" appears quite vague to me. According to the paper, the redundancy seems to refer to the pixels within a local patch, which can be reduced by CNN with 3D convolution. For me, what carried by a local patch is more of \"local structure\" than \"redundancy\", which should be capture instead of \"suppress\". \n- The paper introduces \"DPE\" module and the description can barely be found. It is not clear why DPE is able to \"dynamically integrate 3D position information\".\n- Also, the \"FFN\" module is introduced with only one sentence.\n- In shallow layers, the affinity map A_n becomes a learnable parameter matrix a_n. In this case, A_n is fixed for any test video. This sound counterintuitive. Please correct me if I mistook it.\n",
            "summary_of_the_review": "This paper explores an interesting direction on how to enable transformer-like architecture aware of both local and global information.\nThe experiments are carefully designed and results are encouraging. The reviewer would appreciate it if the authors address the above concerns.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a UniFormer architecture, which combines 3D convolutions and spatiotemporal self-attention for efficient and effective video classification performance. Specifically, the authors show that using local 3D convolution-like layers in the early stages of the network and global self-attention layers in the later stages of the network provides a good tradeoff between efficiency and accuracy. The proposed method reports state-of-the-art results on several major action recognition benchmarks.",
            "main_review": "Strengths:\n+ The authors tackle an important problem of interest to many researchers in the video community.\n+ Very strong results on multiple action recognition benchmarks.\n+ Impressive efficiency considering how good the action recognition accuracy is.\n+ Extensive ablation experiments.\n\nWeaknesses:\n- The paper is poorly written and is quite difficult to understand.\n- The technical novelty of the paper is limited.\n\nIn my view, these two weaknesses are connected as I will discuss next. As discussed in Section 3.4, the proposed local UniFormer block is very similar to depthwise 3D convolutions. I appreciate that the authors included this section in their draft, as it makes it easier to understand their approach. However, before this section, the authors spend several paragraphs \"selling\" this local UniFormer block as a novel transformer block design. I think that's a bit misleading to the readers, and it also makes the paper much more difficult to read (in my view). At this point, most readers are probably familiar with depthwise 3D convolutions. Therefore, in my view, from the very beginning, the authors should describe their local block design by connecting it closely to the concept of depthwise 3D convolutions. Instead, section 3.2 is presented as a novel transformer block with many important details only included in the Appendix. This makes the draft convoluted and harder to read than it should be. Furthermore, the missing technical details wouldn't even be necessary to include if the proposed block was formulated as a depthwise 3D convolution extension. \n\nConnected to my point above, I don't know if the authors are using this strange formulation of their local block to increase the technical novelty of their proposed approach. However, considering the similarities with a 3D MobileNet block, I think the technical novelty of the proposed approach is quite limited. The proposed model is essentially a combination of 3D CNNs and Vision Transformers (with some bells and whistles). Personally, I wouldn't even have an issue with this because, in my view, understanding how to combine 3D convolutions and spatiotemporal self-attention is an important research question. However, I have a problem that the paper is \"being sold\" as a novel transformer-based architecture, as opposed to an extensive and valuable empirical study of combining 3D convolutional, and self-attention blocks. In a way, this paper reminds me a lot of the R(2+1)D paper \"A Closer Look at Spatiotemporal Convolutions for Action Recognition\", and I think it should be written in a similar style as this paper (i.e., empirical paper as opposed to claiming a novel architecture).  The authors might argue that the proposed local block is different than a depthwise 3D MobileNet block by pointing to the results in Table 4. However, in my view, those results largely support my claims, i.e., the gain of using the proposed block over the standard MobileNet block is marginal. Furthermore, considering my points above, I'm also not sure if UniFormer is the best name for the proposed approach. It's vague and it doesn't indicate in any way that the unification is between 3D convolutions and self-attention (as opposed to different modalities, different video tasks, or something else that's much broader).\n\nLastly, I also wanted to mention that the section on DPE is not very informative. The authors simply mention that they \"extend the conditional position encoding (CPE) (Chu et al., 2021), utilizing a simple 3D depthwise convolution with zero paddings as dynamic position embedding (DPE)\". It would be useful to add more details on this for those readers who are not familiar with CPE. \n\nThere are also typos, and grammar mistakes in the draft (a few of them listed below):\n- \"Sprots\" -> \"Sports\" in Table 2\n- Wrong ViViT citation in Table 3\n- \"Spatial-Temporal\" -> \"Spatiotemporal\"\n\nI'd be interested to hear how the authors plan to address my concerns in the camera-ready version of the draft. ",
            "summary_of_the_review": "Despite the shortcomings listed above, I'm still largely in favor of accepting this paper. It provides a very valuable empirical study of combining 3D convolution-based operators with self-attention. Furthermore, the authors obtain very impressive results at a relatively small computational cost (especially compared to prior video transformer papers). The presented ablation studies would also be highly beneficial for the research community.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}