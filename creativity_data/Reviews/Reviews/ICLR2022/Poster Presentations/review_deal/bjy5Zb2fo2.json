{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The submission develops a rotationally equivariant scattering transform on the sphere.  Many developments in deep learning make use of spherical representations, and the development of a rotationally equivariant scattering transform is an important if not unexpected development.  The reviews are split with half of the reviewers believing it to be slightly above the threshold for acceptance, and half believe it to be slightly below the threshold for acceptance.  In the papers favor, it solves an important case of the scattering transform framework, which has been demonstrated to be important in diverse machine learning applications such as learning with small data sets, differentially private learning, and network initialization.  As such, continued fundamental development in this area is valuable, especially in the context of representation learning, the focus of ICLR."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes spherical scattering networks, which are scattering networks defined on the sphere and carry the nice properties of scattering networks such as invariance and stability.",
            "main_review": "I thank the authors for this submission. It is a nicely written paper with a thorough explanation of the theory behind scattering networks and also some of the fundamentals such as Sec. 2.3. However, there are two fundamental problems:\n\n- The technical contribution is not really high as most results come from previous works and the paper essentially contains a rather simple idea.\n- The experiments miss a good application with a clear evaluation against previous work. I also could not see any quantitative results/comparisons on efficiency/ scalability, which is one of the main points of the paper.\n\nI am thus more on the reject side, unfortunately. \n\n",
            "summary_of_the_review": "This is a nicely written paper that contains an interesting idea. The lack of technical novelty and practical results lead me to rejection.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose to process signals on the sphere via an equivariant CNN where, as a pre-processing step, a scattering network is used. As these scattering networks can be computed efficiently at high resolutions, this allows for spherical CNNs to be used on higher-resolution signals.\nAs experiments, the authors compare a spherical CNN with and without the scattering network pre-computation step on spherical MNIST and a cosmic background radiation dataset and find that the pre-computation step aids performance.\n",
            "main_review": "Strengths:\n-\tAs the precomputation step can be done more efficiently than a some spherical CNNs, this step allows such methods to scale to higher-resolution data.\n\nWeaknesses:\n-\tThe authors claim that scattering networks yield rotationally invariant features. It seems natural to me to have a pre-computation step for a spherical CNN generate spherical or SO(3) signals. So how are the scattering network outputs used? Are they added as a constant signal to the sphere as extra channels? If so, why didn’t the authors explore pre-computation steps that result in non-constant signals? Please let me know if I misunderstand this.\n-\tThe authors claim that some methods, like DeepSphere [Perraudin 2019] are not equivariant. Can the authors clarify their claim, as this directly contradicts the claims in that paper? As this graph-based method seem very scalable to high resolutions, I don’t see why DeepSphere doesn’t solve the problem the authors invent a new method for.\n-\tThe experimental section is very weak. The authors cite many other spherical CNN papers, but only compare to a single instance of one other paper. This is insufficient in informing the reader when the proposed method is best used. For example, why didn’t the authors compare to the DeepSphere method, which also experiments on CMB data?\n-\tI find the presentation of the wavelets and scattering transform unclear. For example, it doesn’t say how explicitly the wavelets are constructed. Also, I don’t follow the discussion of the dilation parameter.\n-\tThe method has limited novelty: doing a pre-computation step before applying a neural networks is very widely explored. Also, the theoretical contributions seem incremental changed to previous theoretical results.\n\nMinor points:\n-\tThe usage of $w$ an $\\omega$ together is confusing.\n-\tWhy is the output of the convolution with the wavelet a spherical signal, rather than a SO(3) signal? Does the wavelet contain a SO(2) symmetry?\n-\tIn thm 1, $V_\\zeta$ is undefined.\n",
            "summary_of_the_review": "As far as I can tell, the authors do a pre-computation step which results in a constant signal over the sphere, which I find an odd choice.  The experimental section is severely lacking, and the method has limited novelty. Hence, I cannot recommend acceptance.\n\nUpdated my score to a 5.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes an approach to scale rotationally equivariant convolutions on spherical domains to work with signals with arbitrary resolutions. This is accomplished by leveraging the properties of the scattering transform, by converting the input signal to the corresponding wavelet-based representation, before feeding it to some arbitrary spherical CNN architecture. This can be considered as a preprocessing step for the data, as the transform does not contain any learnable parameters. The obtained representations are isometry invariant up to a given scale and stable to small diffeomorphisms proved theoretically extending the proof in [Perlmutter et al. (2020)]. The approach is validated experimentally by testing the properties of rotation equivariance, as well as being agnostic to resolution on synthetic datasets, and the frequency coverage of the representation. enabling to scale up to high resolutions for the input signals thanks to the compactness of the representation and showing a significant improvement over using a harmonic representation.\n",
            "main_review": "##########################################################################\n\nSummary:\nThe paper proposes an approach to scale rotationally equivariant convolutions on spherical domains to work with signals with arbitrary resolutions. This is accomplished by leveraging the properties of the scattering transform, by converting the input signal to the corresponding wavelet-based representation, before feeding it to some arbitrary spherical CNN architecture. This can be considered as a preprocessing step for the data, as the transform does not contain any learnable parameters. The obtained representations are isometry invariant up to a given scale and stable to small diffeomorphisms proved theoretically extending the proof in [Perlmutter et al. (2020)]. The approach is validated experimentally by testing the properties of rotation equivariance, as well as being agnostic to resolution on synthetic datasets, and the frequency coverage of the representation. enabling to scale up to high resolutions for the input signals thanks to the compactness of the representation and showing a significant improvement over using a harmonic representation.\n\n\n##########################################################################\n\nReasons for score:\n\n \n\nOverall, I am leaning toward acceptance of the paper.  I like the simplicity of the idea and its effectiveness. Nevertheless, I find that some conceptual points in the cons section need to be addressed, in order to make the paper clearer in some of its parts. Hopefully, the authors can address my concerns in the rebuttal period.\n\n##########################################################################\nPros:\n\n\n1. The paper improves current approaches to apply convolution on spherical domains to high resolutions by leveraging the compactness of the scattering transform representation.\n\n2. The idea is simple and yet provides several advantages over existing approaches, as shown in the experimental section. \n\n 3. The paper is well written and enough references and context are provided for non-experts in the field.\n\n\n \n\n##########################################################################\n\nCons and questions:\n\n\n1. It is not entirely clear to me how the CNNs can operate in a rotation equivariant way if the representation obtained from the scattering transform is invariant (up to a scale) to isometries since the latter class of transformations contains the former. Wouldn't in this case the CNN be invariant to rotations as well? Please clarify this point.\n\n2. It would be interesting to show statistics about time for processing the scattering transform and training the network on top of this representation: E.g. does the network converge faster applying the scattering transform?\n\n3. Please, to improve the self-containment of the manuscript, report somewhere (could be in the Appendix) the definition of the equivariance error metric as well as the details for the equivariance test.\n\n\n4. Concerning the spherical MNIST experiment:\n\n    (1) It would be interesting to report results also on the test set with no rotated digits, to better evaluate only the amount of invariance to changes in resolutions.\n\n    (2) Does the network applied to harmonic representations have the same number of parameters in the first layer (I.e. the number of input channels is the same) with respect to the one applied to the scattering transform?\n    \n    (3)  It would be interesting as well  to test the robustness to diffeomorphisms experimentally on  this dataset\n\n\n##########################################################################\n\nQuestions during rebuttal period:\n\n \nPlease address and clarify the cons and questions above\n\n \n\n#########################################################################\n\nI spotted some typos:\n\n(1) Page 2 section 1:  are there advantages-> there are advantages\n\n(2) section 2.2: except its mean-> except for its mean\n\n(3) section 2.3: elaborate these -> elaborate on these\n\n(4) section 2.3: a isometric -> an isometric\n\n(5) section 3: which follow by direct -> which follows by a direct\n\n(6) Figure 1, caption: propagated though -> propagated through\n\n(7) section 5.2: after application -> after the application\n\n(8) section 5.3: becomes increasing -> becomes increasingly\n\n(9) section 5.3: in following spherical -> in the following spherical\n\n\n \n",
            "summary_of_the_review": "Overall, I am leaning to accept the paper. I like the simplicity of the idea and its effectiveness. Nevertheless, I find that some of the points in the cons section, need to be addressed, in order to make the paper clearer in some of its parts. Hopefully, the authors can address my concern in the rebuttal period.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a method to scale spherical CNNs to high resolution inputs. This is an important problem since the high computational cost prevents the use of spherical CNNs in many applications. The solution proposed is to define a spherical scattering network to pre-process the high resolution inputs, producing a lower-resolution channels that still carry some of the high frequency information. This is shown to improve performance when compared with a simple downsampling of the high resolution inputs.\n",
            "main_review": "*Strengths*\n    \nThe paper is very well written. The contributions are clearly and concisely explained, the approach is well motivated and the relation with existing work is well described. The paper proposes an elegant solution to the practical problem of scaling spherical CNNs to large resolution inputs. \n\n*Weaknesses*\n\n1) The scattering transform utilized has complexity $\\mathcal{O}(L^3)$ and consists of cascaded convolutions with axisymmetric wavelets and nonlinearities. The spherical convolution as defined by Driscoll and Healy [2] also considers axisymmetric (zonal) filters and can also be computed in $\\mathcal{O}(L^3)$ while faster methods scaling as $\\mathcal{O}(L^2 (\\log L)^{2})$ for the forward transform also exist. The spherical CNNs of Esteves et al [3] is based on such faster spherical convolutions with zonal filters. What would be the effect of using this faster kind of axisymmetric spherical CNNs instead of the scattering transform? It seems to me that the learnable instead of fixed filters might be advantageous, but they might also be more expensive because pre-computing would not possible. In this case, perhaps using cascaded Driscoll-Healy convolutions with random, localized axisymmetric filters might make sense? Please elaborate on such comparisons; it would be even better to provide quantitative results in terms of accuracy and training/inference speed. \n\n2) For the experiments on rotated MNIST (section 5.3), what is the number $n$ of channels after the scattering transform? Since the baseline starts from a single channel at $L=32$, I believe there are two factors that explain the better performance of the scattering pre-processing: 1) the preservation of high-frequency content and 2) the larger number of filters at the first layer. I am assuming that the first non-scattering layer has the same number of channels $c$ for both models, which would translate into $c$ filters for the non-scattering model but $cn$ filters for the scattering. I suggest an extra baseline to disentangle the two effects: use $cn$ channels followed by a nonlinearity and projection to $c$ channels on the first layer for the non-scattering model. It would also be interesting to see the effects of the scattering pre-processing for $L=32$ -- I wonder if it can improve performance even on lower resolutions, which would make it more widely applicable.\n\n3) There is no mention of a code release in the paper. It seems that the generalized spherical CNN of Cobb et al [1] is used in the experiments, for which there is no public codebase available as far as I know. So releasing the code of this submission, besides ensuring reproducibility of the results claimed, would also provide the community with an easy way to build upon [1].\n\n*References*\n\n[1] Cobb et al, \"Efficient Generalized Spherical CNNs\", ICLR'21.\n\n[2] Driscoll and Healy. \"Computing Fourier Transforms and Convolutions on the 2-Sphere\" (1994).\n\n[3] Esteves et al, \"Learning SO(3) Equivariant Representations with Spherical CNNs\", ECCV'18.     \n",
            "summary_of_the_review": "This is a good, well written paper that proposes an elegant and principled way of scaling spherical CNNs to high resolution inputs. My suggestions are mostly in the direction of making the experimental section more convincing and reproducible, and I will be happy to increase my score in case they are addressed. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}