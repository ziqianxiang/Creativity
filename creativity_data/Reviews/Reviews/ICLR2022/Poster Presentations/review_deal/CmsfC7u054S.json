{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper proposes a Bayesian approach to learning in contextual MDPs where the contexts can dynamically vary during the episode.\nThe authors did well in their rebuttal and alleviated most of the reviewers' concerns. During the discussion there was an agreement that the paper should be accepted.\nPlease take all reviewer comments into account when preparing the final version."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies reinforcement learning where the environment is mathematically modeled as a contextual Markov decision process (CMDP) with finite action-space and context-space, and continuous state-space. The contexts, which are assumed unobservable, may abruptly evolve according to a Markov chain -- hence the name, discrete Markovian context evolution. \n\nThe main contribution of the paper is to present an algorithm for this setting, whose design follows a Bayesian approach. It also relies on Hierarchical Dirichlet Processes (HDP) as priors for context transitions, and uses a context distillation procedure to remove spurious contexts. The performance of the proposed method is demonstrated through numerical experiments. \n",
            "main_review": "CMDP is a powerful framework capable of modeling a broad range of applications. There is a growing literature on RL in CMPs, but most such works, to my knowledge, assume adversarially chosen contexts or contexts sampled in an i.i.d. fashion. The paper is amongst very few works considering the so-called Markovian contexts, an evolution model which, I believe, is relevant in many applications. Learning in CMDPs is a challenging task and so is the problem studied in this paper.  \n\nThe studied problem, while interesting and important, is also highly relevant for the ICLR community as learning in this setting involves some representation learning. \n\nThe paper is well-written and mostly well-organized, and presents an adequate literature review. I have limited knowledge about learning in the CMDP setting here and the Bayesian approach employed. It appears to me that the authors rely on some elements from existing works while some elements sound novel. Nonetheless, due to challenging nature of the learning problem considered, I consider a fruitful orchestration of all these as a contribution. It is also evident that they solidly compare their model and chosen algorithmic elements to existing ones. \n\nIt is so pity that almost all pseudo-codes are presented in the appendix, and a large part of the main text is occupied by experimental results. Perhaps bringing a high-level pseudo-code of the main algorithm to the main text could improve the current organization. \n\nI have one technical comment: The authors use a context distillation procedure to remove spurious contexts. Doesn’t removal of such contexts affect the Markovian property of the contexts?\n \nMinor: \n\n- p. 2: this is case => this is the case\n\n- p. 3: independent and identically distributed => independently … \n\n",
            "summary_of_the_review": "This well-written paper presents a Bayesian learning algorithm for RL in CMDPs, which is an interesting and challenging task, and of high relevance to ICLR.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a novel approach for policy learning in a context-dependent reinforcement learning setting (a subset of POMDPs). In particular, the approach is a Bayesian method using a generative model based on a hierarchical Dirichlet process.\n\nThe experiments demonstrate that the proposed method performs well even on tasks where the benchmark algorithms chosen by the authors struggle or even fail.\n",
            "main_review": "Weakness:\n- It would be informative to see some experiments with problems that have already been tackled in the literature  (in the context-dependent RL or PO-MDPs framework), to see how the developed method compares to existing ones on problems they were optimized for.\n- Results for the running time are not presented. It would be interesting to see though; it is not clear how efficiently the proposed algorithm can be implemented.\n- (4) seems to be quite a simplification. Can you provide some intuition on how good/bad it is in some realistic examples?\n- The paper is lacking a concise presentation of the algorithm\n\nStrength:\n- The method is based on an elegant approach with Dirichlet Processes\n- Solves a previously untackled but realistic problem\n- The algorithm outperforms the baselines\n- Clear intuition is provided for most steps\n\n\nFurther questions:\n-----------------\nCould the proposed method work in the more general transition learning setup?",
            "summary_of_the_review": "The proposed method is based on an elegant approach with Dirichlet Processes, and performs well on tasks where the benchmark algorithms chosen by the authors struggle or even fail. Additional experiments, metrics and explanations would be informative though.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes to model mode transitions (where each mode corresponds to a different MDP) in reinforcement learning with the Hierarchical Dirichlet Process (HDP) prior in a contextual MDP. It makes some assumptions about the evolving environment, such as how it can only take on a finite number of modes and the mode transitions are Markovian. During model learning, it also prunes out modes that have low transition probabilities into them to avoid modeling spurious contexts.",
            "main_review": "Strengths:\n- This paper studies a pretty difficult problem, and proposes to leverage the Hierarchical Dirichlet Process, which can accurately model changes in the agent’s environment and adapt its behavior accordingly.\n\n- It is overall well written and very thorough. Similarly, the experimental section studies several alternative priors and RL algorithms in non-stationary settings, and probes their learned model in various ways.\n\n- The experiment in Appendix A5.4 shows (in a simple experiment) that the model is flexible enough to accommodate new contexts, which is an important capability for continual/lifelong settings.\n\nWeaknesses:\n- At times, the paper becomes convoluted and overloaded with details. For example, the presentation of the HDP could be condensed. I would also suggest a table of contents for the appendix.\n\n- In the experiments, there seems to be a tension between the selected K (cardinality of context set) and distillation threshold. Specifically, if one overestimates K significantly, then the distillation threshold needs to be larger to compensate for an increased number of spurious contexts. However, the threshold can’t be so large that it prunes out relevant contexts. So, careful selection of these hyperparameters is important.\n\n- The experimental settings are relatively simple as they operate in low-dimensional state-action spaces (the largest being 12-dim states and 4-dim actions). The settings also have a relatively small number of contexts that they switch between. I’m curious how well the HDP prior can model large numbers of modes (for example, 100) which meta-RL algorithms often operate in.\n\n- The contexts in all three experimental domains correspond to multiplying the actions by some constant. To see how scalable the model is, it’d be useful to study contexts with multiple variables that parameterize the transition dynamics in more complex ways.\n\n- Assumptions about the environment have to be made for their model to work well, which could be unrealistic in some setups. How effective is the HDP when the assumptions do not hold? For example, what if the mode transitions are not Markovian and depend on earlier modes, or depend on the states and actions? What if we underestimated the cardinality of the context set?\n\nFinally, there are some details that are unclear to me:\n- In the comparison to RNN-PPO, is there a reconstruction loss on prediction of the next state s_t+1? This supervision should be useful for the RNN model, but it’s not clear if it’s added based on the description in Appendix A4.1. \n\n- How do the model learning and policy learning components interact, e.g., are they trained simultaneously? This detail seems to be missing in Section 3.\n\nOther comments:\n- Some of the titles and axes of Figures 5(b-d) are cut off.\n\n- In Tables 2 and 3, it’d be clearer if the highest number of each column were bolded.\n",
            "summary_of_the_review": "The paper studies a challenging problem setting, and demonstrates the effectiveness of the HDP to model this particular setting. The experimental section presents a lot of varied analysis of their model already, but is missing results that demonstrate its scalability to more complex environments. Since the main argument of this work is that the HDP is best suited here, I think there are aspects (described in the main review) that still need to be evaluated.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed a contextual Markov Decision Process with a Hierarchical Dirichlet Process transition prior (HDP-C-MDP) that aims at solving the problem of non-stationary changing environments. Specifically, the context variable is obtained via a truncated variational inference and a context distillation method is applied to remove the spurious ones. The authors also provide theoretical studies concerning the distillation method and the intuition that performance improvement could be brought by knowing the underlying contexts. The authors compared their proposed method with a few reasonable baselines and demonstrated the effectiveness of their method. ",
            "main_review": "Pros:\n- This paper is well-written and the proposed methods are discussed in detail. The literature review is pretty comprehensive and the authors did a good job at classifying the related works. \n- As far as I can tell, the idea is novel although it is built on a line of previous works.\n- The theoretical results are principled and clearly defined. I didn’t check all the details but it seems the proofs are good.\n- The experiments are relatively well done, although I would suggest including more baselines.\n\nCons:\n- More details need to be added to the algorithm part. For example, you could point out the update rule or algorithm rather than just say “Update generative model/policy parameters”. Also, I suggest putting one overall algorithm in the main text. In addition, is the distillation step supposed to be included in algorithm A1? \n- In general, the model is relatively complex. Ideally, a principled Bayesian inference procedure estimated the posterior distribution for all the parameters. In this work, the parameter (e.g. the transition functions) are obtained by point estimation. What will be the error caused by this approximation? \n- Experiments are not sufficient. In terms of the learning curve, only comparing with CEM could not be sufficient. It seems that CEM is even outperforming the proposed method in this case. Adding more natural baselines and the corresponding training curve will be a strong plus. \n\nMinor concerns:\n- It would be better to introduce the full name of the methods (e.g. Soft Actor-Critic) before using their abbreviations.\n- Figure 5 is wrongly cropped.\n\nQuestion:\nIn the related work [Xu et al., 2020], their method can model an infinite number of environments based on the Dirichlet process. In this work, the number of contexts is always truncated by a fixed number, is there any possibility that this work can deal with the potential infinite contexts with HDP?\n",
            "summary_of_the_review": "This paper addresses a very interesting problem where there exists unknown nonstationarity in RL problems. The proposed method takes the Hierarchical Dirichlet Process as the transition prior. Overall the idea of using HDP is original. However, this paper still lacks sufficient experimental results that make the proposed method substantially stand out. I will give a 6 but and I am still on the boardline. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper considers the problem of MDPs with underlying context variables. As these context variables shift, so too does the environmental MDP an RL agent encounters. In particular, this paper considers domain shift in state transitions and reward functions. A new method which relies on hierarchical dirchlet clustering is introduced. This model, as far as I can tell, uses bayesian probabilistic inference to see what parts of the underlying environment probability distribution are useful across context shifts. \n\nA theorem is provided which draws a relationship between the number of mis-identified context switches and the error in the estimated value function. This quantifies the robustness of the algorithm under sudden context shift. ",
            "main_review": "This paper considers the important and timely problem of context shift in reinforcement learning. This is a popular area of research at the moment. Recent works such as SODA and DRQ have invigorated the communities interest in domain shift and related problems. The treatment given to the problem here is Bayesian, using HDPs and probabilistic inference to handle the test time context switching. It's nice to see such a treatment, as this was a gap in the current literature. \n\nThe domain shift problem is in this paper cast as a contextual MDP (C-MDP). As best as I can tell, a contextual MDP is just an MDP where the state transitions and reward distribution shift around, possibly during training and possibly just at test time. (small note: why does the reward function lack the C subscript if it depends on context?) This machinery seems to play nicely into the eventual HDP model the author's develop. Although, I'm not entirely convinced this additional competing standard is needed. There is already a lot of interesting work on domain shift in robotics. I didn't feel like the new framework was justified, beyond making it easier to recast the problem as probabilistic inference. This is a minor complaint and just an observation. There are always issues with too many competing standards and all that. \n\nIt seems that the number of different contexts must be finite and a hand designed heuristic is required to prune connections and avoid spurious correlations. Perhaps this is a standard part of the package whenever you're using HDP. In any case, often the core problem in domain shift is that the learned distributions are not wide enough. And I worry that this sort of limit on the number of contexts might be a hinderance for wider adoption of this model (which I would like to see!) Do we know where the upper bound on problem dimensionality is? There are new domains (distractor control suit) where you can set distractor intensity, in essence producing arbitrarily many distinct contexts for the environment. Is this limit on the number of atoms going to be the first thing that breaks down as the number of contexts scales? Or something else? \n\nThe trick used to derive equation (4) is cute. Thank you for calling this to my attention. \n\nThere are more clear descriptions in the appendix, but it was not clear to me from reading section 3 what the step by step algorithm is. I could not reproduce these results from the main body of the paper alone. \n\nCan you discuss a little how RNN-PPO relates to RL2, which in essence just fuses the belief state estimate and PPO training steps? There is a general lack of comparison with and concern for both meta learning and domain shift baselines in this paper. Perhaps the authors feel that contextual MDPs is a sufficiently distance problem and prior art in these areas is therefore irrelevant. If that is the case, I would like to see a little further justification. Domain shift in general is a rich area of research (SODA, DRQ). Is this paper justified in ignoring that prior art? Have I fundamentally misunderstood your problem setting?\n\nThe interpretability experiments in Result A are strong. I really like seeing results like this. \n\nThe choices of Drone and Intesection are a little strange. Are there no Deep Mind Control Suit or similar environments that could work? I haven't seen these environments before in RL, so it is hard for me to evaluate how impressive these results are. A lot of the results are done on cartpole, which is fine for trying to pull apart the method. Although it is a bit concerning because cartpole famously works even when the underlying RL algorithm is broken. I would like to see results on further hard RL environments. In Table 3, the environment is called Highway. At the bottom of page 8 (bold text) it's called Intersection. \n\nI am more than willing to bump my score if some of the questions I asked are answered. It's mostly a few points of clarification. ",
            "summary_of_the_review": "This paper is an interesting look at domain shift from a bayesian probabilistic modeling perspective. The experiments do a good job of taking apart the design choices, in particular results A and B. I have a few lingering questions about this paper's relationship to prior work in Meta Learning and Domain Shift. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}