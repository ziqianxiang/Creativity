{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper demonstrates that deep networks can memorize adversarial examples of training data with completely random labels, which motivates some analyses on the convergence and generalization of adversarial training (AT). The authors identify a significant drawback of memorization in AT that could result in robust overfitting and propose a new algorithm to mitigate this drawback. Experiments on benchmark datasets validate the effectiveness of the proposed algorithm. One of the reviewers is concerned about (1) the validity of stability analysis where 80% of the data labels are noisy, and the perturbation (64/255) is large, (2) the gap between theory and practice, and (3) novelty. The authors have made a great effort to address these concerns. Although there is still no consensus after the author's response, the majority of the reviewers are in strong support. I, therefore, recommend acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents the memorization effect/behavior in adversarial training in perspective of model capacity, convergence and generalization. They first investigate the difference of memorization behavior between two adversarial training methods, PGD-AT and TRADES, with random labels and provide the proof of gradient instability for PGD-AT in terms of Lipschitz constant. Also they suggest that previous complexity measures are inadequate to explain robust generalization. Lastly, they investigate the cause of robust overfitting in AT and propose a mitigation algorithm by a regularization term for avoiding the excessive memorization of adversarial examples.",
            "main_review": "Strength\nThis paper is easy to follow and well written and the hypothesis regarding the difference between PGD and TRADES is well analyzed by separating the gradient of each loss to provide an easy-to-understand analysis. To validate its observation, the authors designed a set of comprehensive experiments for evaluation and comparison purposes. This paper also proposes a new hypothesis about the cause of robust overfitting, which can be intuitively understood and supported by comprehensive experiments.  Also in section 5, It is experimentally well designed and confirmed that the proposed method by the regularization term using the temporal ensembling approach alleviates robust overfitting, and showed better robust accuracy than the existing methods.\n\nWeakness\n1) Complexity measures : In Figure 5, the authors show that the previous complexity measures cannot adequately explain and ensure the robust generalization performance in AT. However, since there are few model plots shown in the graph, there seems to be insufficient justification that the previous approaches are not suitable. The spectral norm and flatness-based measure in Fig. 5 may be able to explain the generalization gap if the scale is different and more model plots are expressed. It would be nice to alleviate the expression that all these measures are inadequate. It would be better if more abundant model settings and experimental results were presented. Moreover, there is a lack of explanation of why previous measures are not adequate in this setting and it would be better if the authors could clearly suggest a new alternative.\n2) Robust overfitting : Although it has been shown empirically that overfitting is improved through the Temporal Ensembling (TE) approach, there is a lack of novelty for the method itself. The proof of hypothesis by Fig 6 leaves little doubt, since the phenomenon in Fig 6(b) is natural because robust overfitting will not occur if we do not use AT. Also in Fig 6(c), the phenomenon can be due to adversarial transferability[1].\n3) Overall, It would be nice if all experiments were configured in mean/std format in various initial parameters (e.g., random seed) of the model.\n4) In section 4.2, authors claimed that most of the approaches for noisy labels are not suitable for AT, since excluding the noisy label leads to the reduction of the training data. However, these approaches already showed good performance with small datasets like MNIST and CIFAR. It is hard to understand why this suddenly became a problem, even all natural examples can be treated as ‘clean examples’ in AT. If the lack of training data is a main problem, will it be automatically relieved when we use a bigger dataset, like ImageNet? \n\n5)  An adversarial example is a variable of model parameters, and it is continuously changed as training proceeds. Is it appropriate to apply a Temporal Ensemble to this problem, which assumes that the training data is unchanged and takes a moving average of prediction for several epochs? \n\n6) In Theorem 1, the authors claimed that the upper bound of the inequality is tight, because the equality can be reached. However, in order for the inequality to be equal, four gradients - adversarial example with theta 1, clean example with theta 1, clean example with theta 2, adversarial example with theta 2 - must exist in a straight line within a millions of dimensions of space. And the equality of Lipschitz constraints should be reached for both points. It is hard to believe that this can be occured in a real experiment. Furthermore, even assuming that the equality can be reached, it does not provide any information about the magnitude of the expected gap in the inequality. Experimental evidence is needed to continue to assert this. \n\n7) Apart from that, the authors claimed that the right side of the inequality is large due to the Lipschitz constant K. But the Lipschitz continuity of the norm of the gradient is not a widely used concept, and the typical value of Lipschitz constant K in deep neural networks may not be common sense. It would be better if there is a justification about why 2*epsilon*K is (obviously) bigger than the difference between the gradients.   \n\n[1] Adversarial Examples Are Not Bugs, They Are Features (https://arxiv.org/abs/1905.02175)\n",
            "summary_of_the_review": "This paper is easy to follow and well written and the hypothesis regarding the difference between PGD and TRADES is well analyzed. They present the problem of memorization in AT by robust overfitting and propose the regularization term by temporal ensembling approach with their hypothesis, also show the effectiveness of the proposed method in systematic experiments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper provides comprehensive studies on several facts about the memorization of adversarial training algorithms (AT). Specifically, the authors uncover two findings: (1) Given a sufficient model capacity, PGD Adversarial Training cannot fit the whole training set (with 100% labels flipped). While, TRADES can fit the whole random flipped set. It may be because the objective function of TRADES has a clean loss, which can improve the stability of training. (2) Memorization of one-hot labels in AT methods can be one important factor to the robust overfitting issues. Based on these findings, the authors propose a method Temporal Ensembling (TE) approach to avoid fitting all adversarial examples with one-hot labels. ",
            "main_review": "Overall, the paper presents several interesting insights about the behavior of adversarial training algorithms. And the proposed methods are shown to out-perform PGD-AT and TRADES. However, the findings in the paper are disconnected and some of them lack explanations. Therefore, it is hard to fully understand the importance of these findings. Moreover, the proposed method, the Temporal Ensembling (TE) approach, does not have significant novelty compared to previous methods. Next, we detail our concerns.\n\n1. In Section 3.1 and Section 3.2, the authors make great efforts to compare the difference between algorithmic stability of PGD-Train and TRADES, when 100% training labels are flipped. However, the stability difference will not result in any impact to the models, under the cases if fewer labels (0-80%) are flipped (Figure 2). Therefore, it is hard to justify the contribution of this analysis, if this difference is always oblivious under general scenarios.\n2. In Section 3.3, the authors also mention that currently existing complexity measures cannot adequately explain the robust generalization performance of AT. What is the reason for the gap between theories and the practices?\n3. Moreover, in Section 4.1, the authors deploy experiments to demonstrate that there are “hard” training samples which are consistently hard to be classified. However, why the existence of hard training examples can support the claim they can result in overfitting issues? What if removing these “hard” / “noisy” samples from the training set, will this resolve the overfitting problem?\n4. For the proposed method, the Temporal Ensembling (TE) approach, it is not very different from an existing work [1]. Although the Equ.(7) in this paper and Algorithm 1 in [1] only have minor differences, both the papers are designed to (adversarially) train one model, with a self-adapted label instead of the one-hot labels. Furthermore, these two papers have the same way to generate the self-adapted labels. The only difference is the training objective. However, they also have high similarity in terms of definitions and experimental effects.\n \n**References**:\n[1] Huang, Lang, Chao Zhang, and Hongyang Zhang. \"Self-adaptive training: beyond empirical risk minimization.\" Advances in Neural Information Processing Systems 33 (2020).\n",
            "summary_of_the_review": "Overall, the findings of this paper lack some clarifications about their significance. Moreover, the proposed adversarial training method does not have enough novelty beyond currently existing methods. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Memorization has been investigated in deep neural network classifiers but not that much in adversarial training. The authors investigate the ability of adversarial training to memorize random datasets. They use PGD-AT and TRADES to do the adversarial training and see different convergence properties, which differs significantly from training on true labels. They also discover that robust overfitting can occur in adversarial training with memorization and use temporal ensembling to mitigate it.",
            "main_review": "**Abstract and introduction:**\n\nThe abstract and introduction are very well written and clear. This is a big strength. Figure 1 is also very clear.\n\n**Section 3.1:**\n\nThe fact that PGD-AT fails to converge and differs so much from TRADES is not intuitive and a great contribution. The fact that this happens over several datasets, architectures, and threat models does indeed imply that the adversarial training algorithm has a significant impact on the convergence and not the capacity of the network.\n\n**Section 3.2:**\n\nThis section is very intuitive and it makes sense that the authors consider the gradient norm and stability. It makes sense that the instability of the gradient would cause PGD-AT to fail to converge, However, even though the upper bound in Theorem 1 is tight, it can still be far off for specific values of the inputs. So the theoretic justification that the gradient of the PGD-AT loss will vary drastically based on this upper bound is not very strong.\n\n**Section 4.1:**\n\nThe author’s hypothesis that memorization of one-hot labels causes robust overfitting is convincing and they provide evidence of this. This is a strength. \n\n**Experiments:** \n\nOverall one of the biggest strengths of this paper is the thoroughness of the experiments. The authors did a great job of providing a significant amount of evidence for their claims. \n",
            "summary_of_the_review": "Overall, the paper is very well written, what the authors do is clear, makes sense, and is supported by a significant amount of data. There are really few weaknesses with the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a thorough investigation of the dynamics of adversarial training (AT) when a network is trained on a dataset with random labels. This methodology was very successful at identifying theoretical gaps in our understanding of neural networks for standard training, so here the authors propose to replicate those experiments in the context of adversarial robustness. Surprisingly, the authors find that PGD-AT is incapable of fitting random labels when trained from scratch, while TRADES or a warmed-start version of PGD-AT can. They also suggest plausible explanations for this behavior. Finally, the authors use their novel findings to explore the reasons behind robust overfitting, and propose a new method to prevent it.\n",
            "main_review": "## Strengths\n1. **Clean methodology and in-depth analysis**: I admire the effort made by the authors to provide a very in-depth analysis of their results, thoroughly checking different metrics, datatasets, and training setups, and trying to identify the root cause of all the described phenomena.\n2. **Relevance of the results**: The fact that PGD-AT cannot memorize random labels due to its gradient instability, while TRADES or a warmed-up version of PGD-AT can, is quite significant. It points towards the importance of the early learning phase for adversarial training and it can spark new research avenues to understand the dynamics of adversarial training. Similarly, the fact that different complexity measures proposed in the literature to explain generalization do not correlate, and sometimes anti-correlate, with actual performance, might have been extended from prior results on standard training, but it is still of great relevance.\n3. **Connection between robust overfitting and label noise**: The suggested cause for robust overfitting, i.e., introduction of label noise by the adversarial attacks, is bold, and the proposed method to overcome it seems quite effective.\n\n## Weaknesses\n1. **Theorem 1 does not say anything about stability of TRADES**: Although Theorem 1 strengthens the claim made by the authors explaining why PGD-AT has a greater gradient instability than standard training, it does not say anything on how TRADES can overcome it. As far as I understand it, a similar result might be true for the TRADES regularization term. This slightly weakens the theoretical arguments of the authors.\n2. **Connection between robust overfitting and label noise could be more compelling**: As I said, I appreciate the bold claim made by the authors to explain the phenomenon of robust overfitting, and its connections to memorization in AT. However, I still believe some of their arguments are a bit speculative and could be better supported by the data. In particular, the fact that models share the ranking of difficulty of different examples is, in my opinion, a rather indirect way to measure the presence of label noise in a dataset. Something that would make the claim of the authors more convincing is to show that the \"hard\" instances they identified in the dataset do correspond to incorrectly classified or very noisy samples.\n3. **Too many details off-loaded to appendix**: In my opinion, the authors postpone too many important discussions to the appendix which hurts the readability of the paper. Specifically, I believe that Fig. A8 and Appendix A2 should belong to the main body of text.\n\n## Other comments\n1. **Fine-tuning experiments**: I am of the opinion that the paper, in its current form, deserves to be accepted to the conference. However, I still believe there are certain experiments which, if included in the paper, would greatly strengthen the relevance of this work, and could lead me to increase my score. Specifically, there are two important works in the literature which study the effect of pre-training with random labels in performance (Liu et al. 2020, Maennel et al. 2020) and whose transferrability to the adversarial setting is currently unknown. Considering the fact that some of the experiments in this work point towards the importance of the early stages in training in the memorization behavior of AT, I wonder how different pre-training schemes with clean and random labels affect the findings of the authors. Namely, does temporal ensembling work better when the network is standardly trained on clean labels? Do neural networks robustly generalize when adversarially pretrained on random labels? Do they learn meaningful representations? I truly believe that answering any of these questions would be of great significance to the community, and therefore would make this work much more relevant.\n   - Shengchao Liu, Dimitris Papailiopoulos, Dimitris Achlioptas. \"Bad Global Minima Exist and SGD Can Reach Them.\" NeurIPS 2020\n   - Hartmut Maennel, Ibrahim Alabdulmohsin, Ilya Tolstikhin, Robert J. N. Baldock, Olivier Bousquet, Sylvain Gelly, Daniel Keysers. \"What Do Neural Networks Learn When Trained With Random Labels?\". NeurIPS 2020\n2. **Inconsistent x-axis in Fig. 4**: Fig.4a and Fig. 4b are technically showing complementary views of the instability of gradients in the PGD loss landscape. However, the x-axis in this plot is not consistent. One of these plots shows the change of gradient around a specific weight location, and the other the change of gradient during training. This looks slightly suspicious (although I feel it is minor) and I would encourage the authors to either use the same metric in both plots, or show the analogous result for the other x-axis in the appendix.\n\n",
            "summary_of_the_review": "This paper presents a very rigorous empirical investigation of the dynamics of memorization in adversarial training. The provided insights are novel, properly isolated and clearly explained. Besides, the results are not only insightful from a theoretical perspective, but they also provide actionable insights. The inclusion of a few additional experiments could strengthen more the paper, but in the current form, I believe the paper is good enough for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}