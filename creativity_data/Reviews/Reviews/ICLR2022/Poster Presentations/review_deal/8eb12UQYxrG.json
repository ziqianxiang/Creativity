{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "It can be prohibitively expensive to train a reinforcement learner from scratch &mdash; particularly in cases where experience is expensive to obtain, such as with a physical robot. So, we might hope to speed up RL in a couple of ways: first, by pre-training a representation that makes subsequent RL need less data; and second by running our RL on a cheaper proxy environment such as a simulator. For pre-training, we hope to be able to take advantage of available pre-collected data, and we hope to be able to use supervised learning or reconstruction tasks since they can be cheaper than RL. For either pre-training or a proxy environment, we have to deal with distribution shifts: the properties of the environment may change between pre-training and RL, and between RL and testing the learned policy.\n\nThe paper presents an empirical study of how different pre-trained representations and different distribution shifts affect RL performance. It evaluates a number of representations trained by different VAEs (differing in aspects such as loss and hyperparameter settings) under various scenarios of distribution shift. It also asks whether we can predict the performance of the learned policies from properties of the representations, before going to the expense of training and evaluating our reinforcement learner.\n\nThe paper concludes that it is possible to significantly reduce RL data requirements using pre-trained representations, even in the presence of significant distribution shifts &mdash; including demonstrating zero-shot sim2real transfer. And, the paper concludes that inexpensive measurements of OOD performance on supervised tasks can at least partially predict success in generalization.\n\nThe reviewers praised the extensive experimental evaluation, including a large number of experiments on a physical robot, as well as the investigation of less-expensive ways to predict generalization.\n\nSome reviewers were concerned that the choice of environments was limiting &mdash; e.g., that the distributional distance between in-distribution and out-of-distribution tests was limited, or that the results might not generalize to other related robotic environments. However, in the end there was support for the conclusion that the experiments cover a sufficiently general and interesting question."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors study the problem of generalizing out of distribution for RL.  They conduct an extensive empirical study, using simulation and real robotics, and observed properties that predict how well an agent will generalize.\n",
            "main_review": "Primary confusion/clarity concern: I had trouble understanding the rank-correlation results.  I think the clarity issues begin at the following quote in Section 2: “As factor prediction mode...as GS-OOD1, GS-OOD2-sim, and GS-OOD2-real accordingly.”  After reading these 4 sentences many times, I still think they are very difficult to understand (primarily on my first and second readthrough, but even now that I think I mostly understand the approach, I still think these sentences are confusing).  Similarly, sentences like “A simple supervised metric to evaluate a representation is how well a small downstream model can predict the ground-truth factors of variation” and “we use the MLP10000 and GBT10000 metrics (simply MLP and GBT in the following), where MLPs and GBTs are trained to predict the FoVs from 10,000 samples” and “rank correlations between representation metrics and training reward” need to be explained better.  (If space is a factor, then pointing to an appendix for more detailed/intuitive explanations is a good strategy.)\n\nMinor problem: “Correlations with p<0.05 are color-coded.”  How are they color-coded?  This is probably obvious to a reader without any of the primary confusion above (I think it’s red, but even then, which shade of red?), but making it explicit would be better.\n\nMinor grammar edit: “As factor prediction model, we will likewise...” missing article adjective?\n\nStrengths:\n- Outstanding limitations discussion; this frank discussion of limitations strengthens the paper.\n- Thorough overall results with some fascinating insights.\n- The sim-to-real experiments were very nice.\n\nQuestions:\n- MLP stands for multi-layer perceptron, correct?  This should be defined.  GLB should be defined.\n- What are the differences between MLP vs MLP(1-7)?\n- Can you give any intuition about why MLP(1) might have a negative correlation in Figure 2?  Is this just noise?\n\n**Update**:  After reading the authors' responses and other reviews, most of my concerns have been addressed (assuming the two changes I suggest are made).  I am updating my score to an 8; I feel that this is a strong paper. Regarding other reviewers' concerns:\n\nI understand reviewer BEPU's concerns and agree with some of them; but do not think they are deal-breakers.  The paper has its limitations, but I think it is extremely strong despite these.  Robotics papers do tend to have more limitations than papers based exclusively on simulated/game environments, since real-world robotics experiments are much more time-consuming to perform (but they are also often much more convincing, since they are much closer to real-world problems of interests, and not games or simplistic simulations).\n\nI respectfully disagree with many of Reviewer YvcV's concerns, many of which amount to \"the authors should have done [some other nice thing]\".  I think we can always ask authors to do more, and think the many of the reviewer's suggestions would be nice to do, but I think the contribution is sufficient in this case, and that rejecting because the authors did not do even more is not the correct response.  This paper's results are impressive and extensive despite the limitations that Reviewer YvcV points out.  I think part of the disconnect may be non-robotics RL researchers forgetting how time-consuming and worthwhile real-world robotics experiments are (compared to simulated or game experiments).",
            "summary_of_the_review": "This empirical paper has some clarity issues, but is exceptionally strong otherwise.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper discusses a study of different learned autoencoder based representations in the context of generalisation (in and out of distribution) in reinforcement learning. It focuses on two types of VAE models and a robotic block reaching/pushing setting. Comparisons are performed to separately test OOD data for the representation and learned policy as well as generalisation to a real world setting after training in simulation. The analysis shows correlations between various properties and metrics and final downstream RL performance\n\n",
            "main_review": "The paper is well written and intuitive. Further studies of generalisation in robot learning are overall highly relevant and this study contributes to better understanding. That being said, it seems to be very close to prior existing work Dittadi et al 2021 (citation in paper) and some insights are close to common knowledge. \n\nThe evaluation of generalisation also remains limited. After discussing the 7 different factors of variation in the domain, the full investigation purely focuses on color which limits the meaningfulness of the evaluation. \n\nOn the positive side, it’s great to see the focus on investigating proxy metrics which could be evaluated as a replacement to full reinforcement learning and this part of the paper is highly relevant.\n\nIt’s interesting that a sparsity inducing regularisation on the first policy layer has no effect on the usefulness of disentangled representations. Maybe it would be useful here to evaluate with sparsity regularisation across the whole policy.\n\nMinor:\nThe initial section only cites papers starting 2019 regarding the problem of generalisation. This question has a much longer history in ML research which should be taken into account.\nTable 1 on page 2 includes acronyms which are explained many pages later. Please make sure that an acronym is explained before using it on its own.\nFigure 1 likely presents representative view points and not agent inputs. Additionally providing these would be more helpful in understanding the sim to real gap.\nWhy is there no OOD1 in real?\nPlease add an argument for the specific choice of the 2 types of VAEs. What is the benefit of adding Ada-GVAE in addition to beta-VAE?\nWhy not evaluate with beta=0 to include the regular autoencoder case?\nThe dimensionality of the latent space could have a big impact. I’d recommend at least a minor ablation with the best known parameters and sweeping over the size of the latent space.\nWhy are the correlations ‘milder’ on the more complex pushing task? It does not seem directly intuitive and it would be useful to add a hypothesis.\nSome related work on representation learning for reinforcement learning is missing [1,2,3]\n\n[1] Kulkarni, Tejas D. et al. “Unsupervised Learning of Object Keypoints for Perception and Control.” ArXiv abs/1906.11883 (2019): n. pag.\n[2] Wulfmeier, Markus, et al. \"Representation matters: Improving perception and exploration for robotics.\" 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021.\n[3] Higgins, Irina et al. “Towards a Definition of Disentangled Representations.” ArXiv abs/1812.02230 (2018): n. pag.\n",
            "summary_of_the_review": "Overall a relevant investigation into the effect of representations on generalisation in RL. While some insights are trivial and the only investigated factor of variation for generalisation is color, there are aspects in the paper worth distributing and discussing (in particular the evaluation of proxy metrics for downstream performance).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents an extensive empirical evaluation of the VAE-based pretrained representations for OoD generalization of RL agents in a robotic setup.\n\nFrom the reported experiment results, it draws two conclusions:\n1) agents build on top of the pretrained representations can generalize to some challenging OoD scenarios; \n2) the prediction error of the ground truth values of the underlying factors of variations can be used as a proxy metric for selecting representation for OoD agents.",
            "main_review": "Strengths \n- OoD generalization of RL agents is a very active area of research. The paper provides a systematic and thorough empirical study on the impact of pretrained representations.\n- The correlation analysis between different proxy metrics and the OoD generalization performance is extensive and sound.\n\nWeakness\n- While the key claim of the paper lies in OoD generalization, a large part of experiments are essentially not OoD. Among the four scenarios (IID, OoD1, OoD2, and real-world), only the OoD2 presents a clear domain shift to the pre-trained encoder. \n- It seems not very clear to which degree the *agents generalize OoD*. A more direct comparison between IID, OoD1 and OoD2 would perhaps make claim 1 about OoD generalization more convincing.  \n- The core of claim 2, i.e., the GS metric can be used for representation selection, looks over-stretched to me since:\n  - it requires labeled samples from the OoD domain, which is often not realistic\n  - it requires ground truth values of the underlying factors of variation, which is often not realistic neither\n- Finally, the experiment design overall looks quite incremental to [A Dittadi et al. ICLR'21].",
            "summary_of_the_review": "While the empirical evaluation is extensive and rigorous, the two key claims made in the paper are debatable. I, therefore, recommend reject. I'd be happy to change my rating if the author can better support the two claims during rebuttal. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper is an analysis paper that uses well-known existing pretraining techniques to study the effect of pretrained representations on out-of-distribution generalization performance. The paper mainly uses the VAEs to train the visual representation network. While I believe this is an important topic, the paper only analyzed one pretraining technique and in a visually simple environment.",
            "main_review": "Strengths:\n\n* The paper studies the effect of pretraining visual representation on the downstream out-of-distribution task performance.\n* The paper conducted a relatively thorough analysis on this issue. More specifically, the paper studies the effect of VAE pretraining on the TriFinger environment. The studied factors include the measuring metric, network regularization, input noise, different OOD distributions.\n* While most of the conclusions are intuitive to understand, it's good to see that the paper has many experiments to support them.\n\nWeaknesses:\n\n* While the paper did a thorough analysis on VAE and in the TriFinger environment, I found these two are limiting. First, there are many different pretraining techniques such as contrastive learning and using auxilliary loss. This paper only studies VAE. Therefore, the paper title is a bit over-claimed. Second, TriFinger is a challenging control task but not necessarily a difficult task in terms of perception. As we can see in the example environment figures (Figure 1), the image input to the policy is relatively simple and has a clean background. Therefore, it is possible that the perception network can easily learn to only focus on the object in the image and ignore other things during training. And such a network can be more robust than expected and can transfer to the real world better. Therefore, the conclusions drawn in this paper only apply to VAE pre-training and TriFinger environments. It remains unclear whether the conclusions still hold in visually more complex environments such as ProcGen environments.\n\n* Even just in the TriFinger environments, a more systematic analysis should include the testings on the TriFinger environments with different backgrounds. As we can see in Figure 1, the bowl environments are very clean and look similar in simulation and in the real world. Since the pretraining is mainly about training the perception module, OOD tests should also change the bowl texture, color during the testing.\n",
            "summary_of_the_review": "I believe the paper is analyzing an important issue in reinforcement learning. However, more experiments on TriFinger with different visual backgrounds and other visually complex environments such as Procgen, and experiments with different pretraining techniques are needed to make the analysis more thorough and convincing.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}