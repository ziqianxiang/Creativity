{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "For pairs of pieces of text, the central idea of this paper is to combine the approaches of using bi-encoders (where a vector is formed from each text then compared), which are easily trained in an unsupervised manner, with cross-encoders (where the two texts are related at the token level), which are normally trained in a supervised manner. The chief contribution of this paper is to train a combined model (as a \"trans-encoder\") by doing co-training of both model types in an alternating cyclic self-distillation framework. The paper suggests that this allows unsupervised training of a cross-encoder. This claim met some pushback from the reviewers, since the method does require good quality aligned text pairs (much like a traditional MT system does), and so the result is a  task-specific sentence-pair modeling approach rather than a generic unsupervised learning approach. \n\nIn the discussion, downsides included the claims of \"unsupervised\" being overstated, the genuine remaining need for related sentence pairs, the lack of a more theoretical understanding of why this works, and the feeling that the paper is not yet fully mature. Upsides include solid work building from existing models, big performance improvements over SimCSE, novelty in combining previous ideas in a new way for a new problem, and good experiments. To my mind, while the requirement of related sentence pairs does mean the model is task-specific and less than fully unsupervised, this is still a common and useful scenario, the performance of the model is strong, and, while the proposed model is built from existing components and ideas, they are combined in an interesting new way to achieve an intriguing and strong new way of training models, and the discussion here (and now in Appendix A.2) of what the authors had to do to get the model to work in terms of choosing different losses, etc., convincingly demonstrated that the authors had thought significantly and deeply about the nature of their proposal and how to get it to work well. Moreover the authors were able to work expeditiously during the reviewing period to address other weaknesses, such as now providing results with other methods than SimCSE (DeCLUTR and Contrastive Tension) and on other language models (RoBERTa).\n\nAs such, although this paper is clearly somewhat borderline rather than an unambiguous accept, I find myself quite convinced by the novelty, thoroughness, and intriguing nature of this work, and so my vote is to accept it."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper focuses on the unsupervised training of bi-encoder and cross-encoder for sentences. A self-distillation framework is proposed, in which the bi-encoder and cross-encoder may iteratively distill knowledge from each other. The experiments are performed with typical sentence pair datasets like STS, QQP, QNLI, MRPC, where improvements are achieved over SimCSE and mirror-BERT. ",
            "main_review": "S1. The proposed self-distillation framework is interesting and technically sound. The joint training of bi-encoder and cross-encoder goes beyond the existing self-distillation framework, which I believe is the major novelty of this paper. After going through the presented methods in this paper, I'm quite positive that improvements got to be achieved over baselines like SimCSE. \n\nS2. Comprehensive experiments are performed, which clarify most of the critical aspects about the proposed methods.\n\nW1. My biggest concern. I'm afraid that the proposed framework is not really unsupervised (I would call it pseudo unsupervised instead). In fact, it relies on labeled data: although labels are discarded, the pairwise relationships of the sentences are preserved, in which a large portion of the sentence pairs are positively correlated. The paper has to demonstrate that it may still work on a plain corpus where no such paired data is available; otherwise, it will be unrealistic as an unsupervised method. \n\nW2. A minor concern towards the experiment. In table 1, TENC (cross) underperforms TENC (bi) in many cases, which is counter-intuitive, and kind of goes against the statement made in section 2.2.",
            "summary_of_the_review": "The paper is interesting and technically sound. The experiments are comprehensive and inspiring. However, the assumption of pair sentences will probably make it unrealistic in reality.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "# Summary\n\n- This paper highlights the fact that bi-encoders (sentence encoders) and cross-encoders (sentence-pair encoders) have been considered somewhat separately in the literature of sentence-pair modeling, and that cyclic knowledge distillation from one to another (and vice versa) can be effective for improving the performance of both the encoder stereotypes.\n- The authors combine the two sentence modeling paradigms into an iterative joint framework, which is called TRANS-ENCODER, to simultaneously learn enhanced bi- and cross-encoders.\n- Specifically, given the original SimCSE or Mirror-BERT checkpoints provided by prior work, the proposed framework first (pseudo-)labels sentence pairs sampled from the task of interest by utilizing the off-the-shelf sentence encoders. And then, the labeled sentence pairs are utilized to improve the quality of cross-encoders. Further, the fine-tuned cross-encoders can also be exploited to label other sentence pairs which are again able to be used as a dataset for tuning the bi-encoders (sentence encoders). The framework repeats this process until both the bi- and cross-encoders become satisfactory in terms of their performance.\n- The proposed method is evaluated on semantic textual similarity (STS) tasks and binary classification datasets such as QQP, QNLI, and MRPC. The results demonstrate that the method improves both the bi- and cross-encoders to be more suitable for sentence-pair modeling tasks.",
            "main_review": "# Strengths (Reasons to Accept)\n- Solid work. What is claimed by the paper is clear and persuasive.\n- This paper points out the convention in the sentence-pair modeling literature that bi- and cross-encoders have been considered separately. From this, it proposes an intuitive combination of the two encoder paradigms.\n- When sticking to the experimental environment reported in the paper, the improvement in performance looks significant.\n\n# Weaknesses (Reasons to reject)\n- It is hard to say that the proposed method is novel, especially considering that all of the technical components used in this paper (e.g., pre-trained language models, contrastive learning for sentence modeling, knowledge distillation between two similar models, pseudo labeling for data augmentation/generation) are quite common in the literature. I believe that the main novelty of this paper only comes from the fact that it proposes for the first time (to the best of my knowledge) to combine bi- and cross-encoders. However, the specific way introduced to merge the encoders is not that special.\n- Sentence modeling using (fine-tuned) pre-trained language models is one of the most active research areas in NLP. Considering this, it is questionable whether the proposed experiments are comprehensive enough to back up the paper's claim. Even though there exist so many other concurrent and similar works for sentence modeling other than SimCSE, this paper only focuses on incrementally refining SimCSE. To claim that the proposed method is universally effective, it would be desired to test the method on top of other similar sentence modeling frameworks in addition to SimCSE.\n- After all, it is still unclear why the iterative and coupled fine-tuning of bi- and cross-encoders results in performance improvement. Assuming that the information contained in the same dataset is ideally equal, what is the specific factor that can be easily captured by cross-encoders but not by bi-encoders (and vice versa)? The reported improvement in performance is entirely due to the combination of two different encoder paradigms? How about using two separate bi-encoders or cross-encoders? What is the main and independent factor that contributes to the improvement most?\n",
            "summary_of_the_review": "- This paper clearly points out that bi- and cross-encoders have been evaluated and dealt with separately in the literature. From this, it proposes to combine the strengths of the two different paradigms by relying on cyclic knowledge distillation (or data augmentation).\n- The paper is generally well-written and easy to understand. However, it is a little bit doubtful whether (1) the proposed method is novel enough and (2) the reported experiments are comprehensive enough. Furthermore, I feel that this paper does not contain a thorough analysis on \"why\" the proposed method works, which is what I want to know the most.\n- Therefore, my suggstion is a weak accept (or on the borderline to be accepted).\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes TRANS-ENCODER, an unsupervised approach to training bi-encoders and cross-encoders for sentence similarity tasks such as information retrieval, natural language inference, semantic textual similarity and clustering. The core idea of TRANS-ENCODER is self-distillation that alternatively trains a bi-encoder and a cross-encoder with pseudo-labels created from the other. The authors also propose a mutual-distillation extension to mutually bootstrap two self-distillation models trained in parallel. The effectiveness of TRANS-ENCODER is verified by empirical evidence.",
            "main_review": "Strengths:\n1. TRANS-ENCODER is the first completely unsupervised cross-encoder for sentence similarity.\n\n2. TRANS-ENCODER outperforms state-of-the-art unsupervised sentence encoders on the sentence similarity benchmarks.\n\nWeaknesses:\n1. It is counter-intuitive that in the \"bi- to cross-encoder\" phase, the learnt cross-encoder outperforms the bi-encoder which produces the labeled data for the cross-encoder. It is also surprising that the student bi-encoder sometimes outperforms its teacher cross-encoder in the \"cross- to bi-encoder\" phase. The paper lacks the theoretical analysis and empirical investigation of why exactly these happened to TRANS-ENCODER.\n\n2. Although the labels are not used, the test sets of any dataset should not be used for training. They should be considered unseen and held-out in the training phase.\n\n3. What is the rationale for using different training losses for \"bi- to cross-encoder\" and \"cross- to bi-encoder\" (soft BCE vs. MSE)? Is it possible to use the same loss for the two phases?",
            "summary_of_the_review": "Overall, this paper introduces an unsupervised method that seems to effective in sentence similarity tasks, but it lacks the insights and empirical investigation of why this method works well. The work is not well-supported. So I would consider it marginally below the acceptance threshold.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper deals with tasks where we need to do pairwise comparison between two sequences. Specifically, the paper experimented on STS, QQP, QNLI, MRPC. The paper proposes integrating two models together: bi-encoders and cross-encoders. \n\nA bi-encoder encodes each of the two sequences separately and maps to a common embedding space. A cross-encoder first concatenates two sequences, and the concatenation is sent to the model. There’s more computation for the cross-encoder case, but the cross-encoder usually outperforms the bi-encoder. \n\nFigure 1 illustrates the authors’ proposal well. First, start with a pretrained language model. Then, the bi-encoder generates pseudo-labels for the cross-encoder to learn. Next, the cross-encoder generates (more accurate) pseudo-labels for the bi-encdoer to learn. We reiterate this process. This approach is named self-distillation. \n\nThe author proposes an extension called mutual distillation where the authors do self-distillation on multiple pretrained language models in parallel (but the paper only experiments with two--BERT and RoBERTa). \n\nOn STS, QQP, QNLI, MRPC, the mutual distillation approach archives good performances (much better than the bi-encoder baseline SimCSE). \n",
            "main_review": "Strengths\n- The author experimented on stronger models (RoBERTa-large) too, and the results still hold.\n- The writing is very clear. The figures are great. Reproducibility is likely strong. \n\nConcerns/comments\n- The paper will be stronger if the authors can include more hypotheses or justification on why, in some cases (see Table 1 and Table 8), TNec-mutual works much better than TEnc (cross), especially because the bi-encoders do not perform as well.\n- It will be great if the paper includes more details on the training time. How do the training time compare among different approaches, for GLUE tasks (especially larger ones)? How difficult is it to train the mutual distillation? \n- The mutual distillation approach requires doing self-distillation on multiple pretrained language models. What’s the aggregate training overhead of mutual distillation vs. cross-encoder only vs. bi-encoder only?  \n\nMore questions...\n- What's the training stability for self-distillation? Will results have a small standard deviation?\n\nMinor\n- I’m interested in seeing the performance on more difficult tasks that might require more world knowledge (e.g., MNLI), but this is not necessary. \n- Typo on page 2 “distil” -> “distill”\n- Page 3, paragraph 3, should be Liu et al. (2021) instead of \\citep?\n",
            "summary_of_the_review": "The approach is straightforward and nicely tied the bi-encdoer approach with the cross-encoder approach. The improvements compared to SimCSE is significant. The writing is clear and the reproducibility is good. However, I would expect more theoretical justification on why TEnc-mutual could be much higher than TEnc-cross. Moreover, training stability (of self distillation) and aggregate training time (for mutual distillation) deserve more discussion in my opinion.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}