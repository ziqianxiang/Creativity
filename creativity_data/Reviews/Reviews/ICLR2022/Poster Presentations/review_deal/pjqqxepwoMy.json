{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper studies the problem of using oracle information that's only available during training in RL. The key contributions are 1) a variational Bayesian approach that models the oracle observation as latent variables; and 2) a Mahjong environment for benchmarking RL with oracle guiding. The novelty of the proposed approach is limited, but reviewers find the problem intriguing and agreed that it's a reasonable application of Bayesian approach to RL with latent oracle information. In addition, the Mahjong environment could benefit the community and spur new work in this direction. Therefore, I recommend this paper to be accepted as a poster."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, reinforcement learning (RL) with training-time privileged information is studied. This scenario is the relaxation of traditional POMDP by allowing additional observations to be available during training time. A variational Bayes approach is proposed under this scenario. The main idea lies in learning a joint latent space for both testing-stage partial observations and the training-stage full observations.  Experiments over several benchmark tasks, including Mahjong, a large-scale multi-player game with imperfect information are conducted to verify the performance of the approach.\n\n",
            "main_review": "Strengths of the paper:\n- Adopting variational Bayes is an interesting and reasonable approach to make use of the training-time priviledged information. \n\nWeakness of the paper:\n- In fact, the studied problem is not absolutely new. There have been similar settings studied under imitation learning (IL),  such as [1]. Furthermore, applying variational Bayes for latent space learning is also not a very novel idea in both RL and IL. \n\n- According to the paper, one of the major drawbacks for previous approaches in this problem is lacking theoretical guarantees. However, in my view, the variational Bayes is also not possible to work in all situations. Why and when the proposed approach would indeed work? The theoretical analysis is missing in the paper. Even though the experimental results show some advatange of the proposed approach, after reading the paper, I still don't get solid justifications on why variational Bayes could outperform previous approaches in principle. \n\n- Some descriptions could be clearer:\n>- In Equation (1), how $v^{tar}_t$ is obtained? The paper just says that it could be calculated by any policy evaluation method. But policy iteration relies on knowing the transition model and the reward function. This is not assumed in the paper.\n>- In the Mahjong experiment, I wonder why VLOG-no oracle could improve over the comparison methods. According to Equation (1), if no oracle observation is available, VLOG reduces to minimizing the maximum-likehood loss of fitting the value function. I thought it would have no difference to the baseline method, i.e. vallina policy learning. \n\nReference:\n[1]  Learning by cheating. https://arxiv.org/pdf/1912.12294.pdf\n\n",
            "summary_of_the_review": "To summarize, I think the paper proposes a reasonable approach for RL with training-time privileged information. But I think further efforts should be made to better justify the proposed approach both theoretically and empirically, as discussed above.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents an approach to dealing with problems where agents could benefit from learning from state features that are only available during training, but not during evaluation. Their approach leverages Bayesian theory to propose a variational learning method and is evaluated on a variety of tasks, including a Mahjong simulation (which the authors are providing as part of their contribution).",
            "main_review": "I really like the motivation and idea behind this paper, and I think it is something that future researchers could make use of. In particular, I like the simplicity of the approach.\n\nMy main concern with the paper is that the evaluation on Mahjong:\n*  It is somewhat confusing to follow (for readers not already familiar with Mahjong), and it is not clear what the takeaway is (other than VLOG seems to work). \n*  The results are somewhat suspect, since the oracle does the worst in Table 2, which is not what one would expect.\n*  The method for scoring described at the bottom of page 8 (\"we simply added up the payoff of the two being tested agents\") seems quite strange and not really a fair evaluation. Is this really how Mahjong is played/evaluated?\n*  Why isn't the top-left value of table 2 bolded as well? The VLOG line in Table 2 has overlapping CIs, so should not be bolded.\n*  Finally, the experiments are combining too many things: offline RL, multi-agent RL, etc. This makes it harder to reliably ascertain what the benefits of VLOG are. While interesting, I'd recommend clarifying the exposition of the experiments, or finding another (better known) benchmark, and leave the Mahjong experiments as an extra set of appendix experiments.\n\nQuestions for the authors:\n1. In footnote 2 in page 3, is there any relationship between $X$ (set of executors) and $\\hat{X}$ (set of oracles)? I assume the latter is a subset of the full $X$?\n1. In remark 1, is the $v_t$ in $p(v_t | {\\bf x}_t)$ equal to $V({\\bf x}_t)$?\n1. In the last paragraph of section 5.2 it says \"VLOG-no oracle performed surprisingly well\", but this is not the case in the figures shown.\n1. What is \"suphx-style oracle guiding\"?\n1. What is \"OPD-style oracle guiding\"\n1. What is the \"(trained) baseline model\" used in Table 1?\n\n---\nMinor comments:\n1. The abstract starts with \"How to make intelligent decisions is a central problem in machine learning and cognitive science\". Minor nit, but I'd say cognitive science is more concerned with _understanding_ how decisions are made.\n1. In the abstract should say \"RL using variational method***s***.\"\n1. In the abstract should say \"decision-making tasks rang***ing*** from video games...\"\n1. At the end of the second paragraph of the introduction, it's not necessary to include \"etc\" since you started the sentence with \"including\".\n1. In section 2, remove \"different\" after \"Another\" in the second sentence.\n1. In section 2 should say \"make the student policy behave more simliar to ***the*** teacher policy. Both methods\"\n1. Title of section 3 should be \"PRELIMINAR***IES***\"\n1. In the first sentence of section 4 should say \"variable representing the environmental state\"\n1. In the first sentence of section 5.1, should say \"by leveraging oracle observation***s*** in learning.\"\n1. In the end of the first paragraph of 5.1, should say \"another grid if not ***blocked by a wall***.\"\n1. In the first sentence of 5.2 should say \"we tested it ***on*** a set of MinAtar video games\"\n1. When citing the ALE, you should cite [Bellemare et al, 2013](https://arxiv.org/abs/1207.4708) instead of Machado et al.\n1. In the third paragraph of page 8, should say \"are sophisticated ways to encode the state and action space of Mahjong (Li et al., 2020), we attempt to make simplifications with reasonable amount***s*** of approximations since our goal is not to create strong Mahjong AI\"\n",
            "summary_of_the_review": "See details above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper tackles reinforcement learning scenarios when the raw observation obtained from the executor is not very descriptive and there are high quality oracle observations available for the learning process to guide the policy training. The proposed method aims to derive a representative latent state via incorporating a variational inference perspective. Specifically, the oracle and raw observations are processed by two individual encoders and then the two latent spaces are fused and fed to a shared decoder. The latent distribution obtained from the oracle observations serve as the posterior distribution and that from the raw observation serves as the prior distribution. The distance two distributions is minimized via minimizing a KL divergence term. ",
            "main_review": "$\\textbf{Strength}$:\n- This paper is well written and easy to follow.\n- Leveraging Bayesian theory to tackle the representation learning problem in reinforcement learning is an important research direction.\n\n$\\textbf{Limitations}$:\n- The proposed method is not very general as it could only be used in the tasks where oracle observations are available.\n- The  formulation of ELBO might not be very valid (refer to the detailed comments)\n- The empirical evaluation domains are too simple and some of the experimental configurations are not properly specified.\n- The method is developed upon a very simple RL algorithm. \n\n$\\textbf{Detailed comments}$:\n1. The assumption that there are always oracle observations available during policy learning does not hold for many applications, such as Atari 2600 and noisy robotics navigation tasks. And in the more challenging problems with partial observability, it is unclear whether this method could be directly applied and how to effectively regularize the posterior distribution which meant to represent partially observed states. \n\n2. I feel the formulation of ELBO is not that sound. I do not know why the latent of oracle observations are set as the posterior instead of that from executor observation. Also, it seems that there are two execution paths for the ELBO, i.e., there should be one for the oracle and another for the executor observation. But in Sec 4.2, this is not properly specified as Eq (2) only tackles one of the path. \n\n3. It is unclear how the variational model is related to the policy model, e.g., is the input to the policy $x_t$ or $z_t$? \n\n4. The authors employ a very simple RL algorithm to deal with very simple tasks. For instance, in the first task of maze navigation, in such a simple case I'm very sure using something like PPO will successfully solve the problem. I don't understand why the problem could be unsolvable, i.e., the performance of baseline is such poor. I'm also concerned if the experimental settings are properly configured. For instance, in the maze task, the input is very shallow, i.e., with only ~3 dimensions, but the authors specify the latent to have a dimension of 128. I believe such setting is not very reasonable.  \n\n5. In many  cases, even though the oracle features might be available, they might   have very different  format  compared to the raw observations, e.g., (x, y, z) vs  high-dimensional images. In such scenarios,  would   the two-encoder-one-decoder architecture still work?\n\n6. Overall the novelty of this paper is relatively limited and there lacks discussion with a number of related works that combine variational inference with policy learning, such as:\n[1] Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model\n[2] Variational methods for Reinforcement Learning\n[3] Sequential Generative Exploration Model for Partially Observable Reinforcement Learning\n[4] Reinforced Variational Inference",
            "summary_of_the_review": "This paper proposes a method to leverage oracle observations with a variational inference framework. But the applicability of the method is rather limited as it  poses a very strong assumption that oracle observations need to be accessed for training the policy. The empirical evaluation domains are too trivial, e.g., very simple decision making tasks and there is no challenging POMDP task employed to demonstrate the KL regularization works. There are also some problems regarding to how the methods are implemented. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a variational-based model VLOG for solving the oracle-guiding RL problem. By minimizing the KL-divergence between the latent features from the oracle observations and the executor observations, the model can apply the oracle signals to boost the execution performance. The author(s) derived a lower bound objective as the training loss and empirically demonstrated the performance of their method under three different types of environments.",
            "main_review": "The paper is easy to follow and generally well-written. The proposed method is intuitive and effective, as an application of variational models. The oracle-guiding problem is novel and intriguing. It is similar to the offline RL, but there are some key differences that can distinguish them. I urge the author to formally define the problem to help the readers who are less familiar with the oracle-guiding framework. I summarize some of my concerns in the following:\n\nconcerns:\n1. Can you summarize the difference between offline RL and oracle-guide RL. I think the key difference is the training observations (or oracle observations) contain a richer signal than the observations in the testing environment (or executor observation), but there could be more. Please formally define the oracle-guiding RL problems anyway.\n2. The author claims \"VLOG is theoretically guaranteed to ...\", where are the theoretical results? I think we can show by maximizing this variational lower bound, the KL divergence between the true posterior and the approximate posterior will be reduced, but this is a well-known property. Where are the new contributions?\n3. How do you get the label v^{tar}_{t}? (in (1)). I assume a DQN or a critic network must be implemented. If it is the case, please define the model and its loss.\n4. Will it make things easier by replacing a hyper-parameter \\beta with another hyper-parameter D^{tar}_{KL}? I guess both hyper-parameters are difficult to determine. In fact, other works might design a linear or exponential scheduler for \\beta that allows it smoothly grow from a min (in most cases, 0) to a max value during training. As far as I know, this method works quite well in practice.\n\nMinors:\n1. Many sentences are repeated in the abstract and introduction, but readers with a reasonable RL/ML background can follow this message with one scan. Maybe use another example for better elaboration.\n2. In remark one, the paper says \"they can choose q(v_t|\\hat_{x}_t) to be p(v_t|x_t)u(v_t; [l,u])\". Are you multplying two distrbutions? In this sense, q will not define any distribution.\n3. Poker games involve two players competing with each other. In this sense, maybe a (PO)Markov Game model can better describe this environment than a (PO)MDP.\n\ntypos:\n1) the oracle-estimated one as posterior distribution -> the oracle-estimated one as the posterior distribution.",
            "summary_of_the_review": "This paper tackles an intriguing oracle-guiding problem. The proposed method is intuitive and effective, as an application of variational models. The paper is generally well written. The concerns I proposed are mostly minor and can be easily resolved within a reasonable time. I did not capture major bugs in this paper. The experiment is complete and promising.  I am happy to vote for an accept given other reviewers do not find major issues.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}