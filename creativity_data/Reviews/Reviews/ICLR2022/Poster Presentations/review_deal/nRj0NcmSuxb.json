{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "All reviewers agree that the presented approach to fair calibration of face verification models is interesting and needed in the field. The method does not require access to sensitive attributes for calibrating, which makes it sustainable. The reviewers are satisfied with the presented experimental studies in most cases. The rebuttal addressed a large majority of additionally raised questions. I believe that the paper will be of interest to the audience attending ICLR and would recommend a presentation of the work as a poster."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces a Fairness Calibration method for the task of face verification problem. The authors claimed that the proposed method can produce fairly-calibrated probabilities, reduce the gap in the false positive rates, does not require knowledge of the sensitive attribute, and does not require retraining, training an additional model, or retuning. Experimental results demonstrate that the proposed\nmethod can achieve state-of-the-art performance over the RFW and BFW databases.",
            "main_review": "The main strengths of this paper is that the propsoed method is a unsupervised method, and does not require retraining. However, there are some weaknesses of this paper. For example, the global accuracy improvements compared with the state-of-the-art methods are very limited  as shown in Table 2. Moreover, the presentations and layout of the whole paper is not clear to the reader. ",
            "summary_of_the_review": "Considering that the proposed bias reduction method is simialr to the idea of Fair Score Normalization, and the performance improvements are very limited, and moreover the writing of this paper is not good. Thus, I suggest to give the decision of rejection to this paper. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "Considering that the proposed bias reduction method is simialr to the idea of Fair Score Normalization, and the performance improvements are very limited, and moreover the writing of this paper is not good. Thus, I suggest to give the decision of rejection to this paper. ",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes an approach, called FairCal, for fairly calibrating face verification models. The method does not require access to sensitive attributes for calibrating, instead the paper uses k-means algorithm to find clusters of data and applies beta-calibrating algorithm on each cluster. The authors show that after calibrating with FairCal the model still achieves good accuracy and outputs more fairness-calibrated probabilities. Also, the method reduces the FPR gap across sensitive attributes. ",
            "main_review": "\nPros: \n\n1. The paper addresses an important problem in face verification: unfairness of classifiers with respect to demographic groups (different FPRs and FNRs). It was shown in previous and this work that it is impossible to choose a single threshold that would yield similar false positive and negative rates for different sensitive groups. Therefore, post-processing approaches focus on calibrating the model scores of threshold to get fair face recognition models. \n\n2. In most scenarios practitioners do not have access to or can not use sensitive attributes for calibrating the model and this work proposes an approach for calibrating model scores without access to sensitive attributes. \n\n3. The authors conduct experiments to demonstrate the effectiveness of FairCal and compare it to previous works. They show that FairCal achieves better overall accuracy, false positive rate and fairness calibration compared to other calibration methods, including ones that do have access to sensitive attributes.\n\n\nI have a few questions and comments for the authors: \n\n1. I do not exactly understand why fairness calibration is a desired property for face verification systems. Why should we expect to have the same proportion of positive matches across sensitive groups in the data? \n2. How was the number of clusters K chosen? \n3. In section 4.2 it is said that by converting scores to calibrated probabilities, one can extend FairCal to multi-class setting. Could the authors please elaborate on that? Was it meant in the context of face recognition? \n4. What loss was used for training models on VGGFace2 and CASIA datasets? SOTA face recognition models are usually trained with angular margin losses (ArcFace, CosFace, SphereFace etc.) to ensure better feature separability in the angular space. It looks that the TPR for ArcFace model is significantly higher than it is for other models (90% vs 69% on BFW dataset). \n5.  In the section 6.2 on fairness calibration it is said that prior methods can not be fairly-calibrated, therefore the beta-calibration is applied to their score outputs to compare with FairCal. Is that the case only for the results in Table 3 or was beta-calibration applied to these methods for all experiments (including ones in Tables 2 and 4)?\n6.  I think it would be better to explain the beta-calibration method in the main body of the paper as it is a \"kernel\" of FairCal method.  \n",
            "summary_of_the_review": "The main novelty of the paper is applying beta-calibration on data clusters found in an unsupervised manner for face verification models. One of the selling points of FairCal is that the method allows to fairly-calibrate the classifier, i.e. the calibrated classifier will produce equal probabilities of positive matches across sensitive groups. I do not exactly understand why that is a desired property (in contrast to having equal FPRs and FNRs across subgroups) for face verification system. It would be very helpful to hear from the authors on that! Because of that (and other questions I have) I give the score of 5 for this paper, but I am willing to increase it after the rebuttal. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose a simple cluster-conditional post-hoc strategy to calibrate the similarity scores obtained using a pre-trained network. In essence, this work proposes to use a cluster-specific (or group-specific in case of Oracle) calibration function (\\mu), which results in improving the scores pertaining to verification of a x1-x2 pair (where both x1 and x2 belong to the same demographic group) mapping function. This is similar to choosing group-specific threshold demonstrated in the BFW paper (Robinson et al, CVPRW 2020). Unlike existing methods such as AGENDA, FTC etc. FairCal does not require the protected attribute labels during training and testing.",
            "main_review": "Strength\n+ Simple approach for post-hoc bias mitigation.\n+ Improvement over SOTA Arcface\n+ Extensive evaluation using different fairness metrics.\n\nWeaknesses\n- (Major) No comparison with group-specific thresholding proposed by Robinson et al (CVPRW 2020). \nRobinson et al  (CVPRW 2020) propose a similar method and report results on BFW dataset. Additionally, they also report improvement in face verification accuracy. This makes me question the novelty of this work. The authors are advised to mention the key differences and add comparisons with this work. \n\n- Missing analysis of 'Baseline + Calibration (2nd plot in Fig. 2)' in Table 2,3,4.\n\n- Results (Table 7). \nFrom this table, can we infer that FairCal does not achieve Equal Opportunity (compared to FTC, FSN)?\n\n- Missing details about building calibration set.  How are samples in the calibration set chosen?\n- Missing notational details (For ex: in Section F1 and F2, what is P^{cal} )\n\n\n- (Minor) Missing citations and comparison\n'Group Adaptive Classifier' by Gong et al (CVPR 2021)  and 'Protected Attribute Suppression System'  by Dhar et al (ICCV 2021) recently proposed debiasing methods. While Gong et al (CVPR 2021) present an end-to-end method, Dhar et al (ICCV 2021) present a post-hoc de-biasing approach, that is comparable to FairCal.  \n\n- (Minor) IJB-C is one of the most widely used datasets for evaluating face recognition algorithms, which contains gender and skin tone labels. So, it is recommended that the authors add results related to skin tone  and gender bias on IJBC.\n\n-  (Suggestion, not weakness) The authors are suggested to add plots similar to Fig 1 for Arcface on BFW. Also, it may help to add some more intuition behind Eq 3.\n",
            "summary_of_the_review": "The topic of this paper is currently very important in the CV/ML community. Although the novelty of this work is incremental,  the results justify the superiority of FairCal (and oracle). While the readability and thoroughness of this work is quite high, there are still some issues in this work. So, I vote for weak accept. I hope the authors address my concerns (especially about similarities with the BFW paper and add comparison to this paper). ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}