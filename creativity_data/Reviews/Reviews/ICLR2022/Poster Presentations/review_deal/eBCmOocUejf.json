{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper tackles a relatively novel problem that is the result of recent work on prefix tuning - specifically the need to be robust to adversarial perturbation in the context of prefix tuning and they show a method for achieving this without requiring more storage and obtain good results.\n\nThere were some clarity issues that were addressed by the reviewers during the rebuttal. The main issue that was pointed out was the effect of batch size on the success of the model. The authors gave experiments with batch size 1 where results are less impressive but still outperform the baseline. Also the authors say that for now they are not considering the case where only some of the elements in the batch are adversarial, which I think is ok for a research paper on such a cutting-edge topic. \n\nThus, the result of the discussion is to lean to accept this paper given that it is now more clear, has experiments that make it clear what the benefits are in realistic settings and obtains improvements."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper focuses on improving the adversarial robustness of prefix tuning (Li et al. 2021), which is a recent parameter-efficient tuning method. Specifically, the paper proposes to add extra batch-level prefixes that are tuned for each test batch on the fly, to minimize the distance between hidden activations of the test samples and the canonical manifold obtained from the hidden activations from correctly classified training samples. The intuition is to optimize the added batch-level prefixes so that the geometry of hidden states from adversarial examples is closer to that of training examples. Experiments on three text classification benchmarks across several different adversarial attacks demonstrate the effectiveness of the method.",
            "main_review": "Below are the detailed strengths and weaknesses:\n\n*Strengths:*\n\n1. Adversarial robustness is an important problem and has not been explored much for relatively new prefix/prompt tuning approaches. Thus the topic of this paper can be of interest to a general audience.  Also, this paper is timely given the recent attention on prompts.  \n2. The idea of optimizing the geometry similarity to defend against attacks is interesting and novel from my perspective. Particularly I like test-time tuning which could adapt to different types of attacks on the fly.\n3. The experimental results are strong. \n\n\n\n~~~~\nUpdates after Rebuttal: Most of the following concerns have been addressed in the revision, and I have increased my score\n~~~~\n\n*Weaknesses*:\n\n1. What is the batch size at test time tuning? Is the added robust prefix the same for the entire test set, or the same within a batch but different across batches, or unique for every test example? This is an important point to assess whether the experiments are in an online setting where test data arrives in-stream or not.   \n2. Section 5 is not very convincing to me:   \n(1) there are only several case studies without any quantitative results; I think it may appear in the appendix only or just uses a short paragraph in the main body (because the statements in Section 5 can only be good hypotheses which are not well-supported by quantitative evidence, also, attention is not a convincing proxy for the explanation, see point (2)), yet it takes more than one page in the current version;  \n(2) the interpretation from the perspective of attention weights bothers me a bit, attention as an explanatory tool is known to not be faithful [1] – a larger attention weight does not necessarily mean the final prediction depends on it more than others, and vice versa, thus maybe not overread it too much. As said above, it is ok to do it this way in the appendix, but using over one page of the main content for this is not convincing to me.  \n3. In the attention visualization figures (e.g. Figure 4), why does a word attend itself as well instead of only attending to the contexts? For LM, I think that the diagonals in the attention figure should be zero, or am I missing something? Also, the presentation could be improved here, in the figure caption you can explain what the rows and columns mean in the visualization to make it easier to read.   \n4. Section 6 is difficult to follow without reading the appendix, and it is disconnected from the rest of the paper in terms of the paper structure – having a theoretical interpretation section after experiments at the end of the paper is not a good presentation structure in my view. If you think Section 6 is important to have, move it before the experiment section and clarify more details to make it more self-contained; if it is not very important, you can just put it into the appendix while briefly mentioning it in the main body.   \n5. Besides the presentation issues above, there are some other minor presentation places which could be improved, e.g.  \n(1) Eq 4, an undefined variable X_C suddenly comes in without explanation  \n(2) Better to explain Eq (8) with more text given that this is an important equation for the proposed method\n\n[1] Serrano et al. Is Attention Interpretable? ACL 2019\n\n\n",
            "summary_of_the_review": "There are novel contributions of the paper both technically and empirically, however some major analysis sections are not convincing and a significant portion of the presentation needs to be improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper investigates the robustness of prefix-tuning methods and proposes a simple yet effective method to improve the robustness. The experiments show that the proposed method can largely improve the performance in adversarial settings and slightly improve the performance in clean settings.  ",
            "main_review": "The authors study a novel problem in lightweight fine-tuning methods. Most studies aim to match the performance of full model tuning via updating a subset of parameters but rarely study the robustness of lightweight fine-tuning methods. \n\n\nStrengths:\n1. The authors study an important and novel problem.\n2. The authors provide a simple yet effective method with motivations. The proposed method is shown to be effective even combining with adversarial methods. \n\nWeakness:\n1. The authors argue that robustness is important for lightweight tuning methods. But I still think it is better to provide a comparison between lightweight tuning methods will full tuning methods. Provide a basic starting observation about whether lightweight methods bring more challenges on robustness or not/\n\n",
            "summary_of_the_review": "The studied problem is important and novel. The proposed method is simple and clear. The experiments justify the effectiveness of the proposed method. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "\nThis paper introduces a tweak to Prefix-Tuning to make it more resilient to adversarial perturbations of the input. The idea is to add a batch-level prefix at inference to the original one which enhances robustness. Critically, Robust Prefix-Tuning (RPT) does not require auxiliary model updates or storage, in contrast with other robustness methods.Thus, this approach makes prefix-tuning more robust while preserving its modularity and low storage requirements.\n\nThe authors conduct experiments on 3 text classification tasks, 5 textual attacks and different training regimes (normal training, adversarial training and adversarial data augmentation). In nearly all instances, their method improves robustness (sometimes considerably so) while preserving the accuracy on the original text.  \n\nThe authors also present RPT from an optimal control perspective and conduct a qualitative study that shows how RPT impacts attention weights.\n",
            "main_review": "Review note: I am not familiar enough with Optimal Control Theory to evaluate the soundness of Section 6 / Appendix E. I will leave it outside the scope of my review and hope other reviewers can fill in the gap.\n\nPros:\n- The results are quite convincing in nearly all settings (aside from important caveats in cons). While some scores are still quite low after RPT, they are consistently better than the baselines. Notably, the method can provide additional gains when used with other defense methods, such as adversarial data augmentation and adversarial training.\n- The need for this method is well-motivated\n- The paper is good at emphasizing different priorities rather than only the end accuracy. For instance, time to accuracy in Fig. 2\n- Although there are some questions about the validity of using attention weights (see https://arxiv.org/abs/1902.10186 which likely should be mentioned as a caveat), I found Section 5 insightful.\n\nCons:\n- Inference Batch, batch size and its importance:\nThe batch size at inference seems like an important variable. Indeed, my understanding of Section 3 and Eq. 8 is that the specific batches used at inference will play an important role as they impact P_psi. This is my **key concern with this paper** as it brings up quite a few issues that should be mentioned.\n  - Inference for a datapoint depends also on other datapoints in the batch. This is different enough from other ML setups that it should be highlighted. It also causes reproducibility issues. For instance, batch norm avoids this by fixing batch statistics at inference.\n  - It is not clear that the method works for low inference batch size (opt of Eq. 8). The inference batch size used is not mentioned anywhere. If the method requires batch size > some N (or if performance varies widely with batch size), this seems a strong assumption that should be made clear. You do not always get several samples of the attack on your system.\n  - It is not clear how well the method works when the inference batches are a mix of unperturbed samples and perturbed ones. This seems like a more realistic attack scenario. \n  - It is not clear how well the method works if there are different attacks in the inference batch, which also seems like a more realistic threat model.  \n\nOverall, I feel like some answers to the above would diminish my concerns, most notably:\n  - What is the inference batch size used in the experiments?\n  - How does performance vary with inference batch size (one or two settings should be enough)\n  - Does the method work when only x of N samples in the test batch are adversarial? Does the example work when they are two types of attack \n\nDataset statistics and mode prediction\n\nIt would be helpful to remind the reader for each dataset how many classes they are / what the mode/random pred accuracy is to help interpret results. For instance, on Table 1 under PWWS improves from 16.64 to 50 for SST-2 and 25 to 34 on SNLI but that is the same as the accuracy of a random predictor.\n\nWriting:\n\nThe writing could be improved substantially. Some examples: p2 “remaining the pretrained models unmodified” -> “keeping”. Also on p3 “remaining its lightweightness”. I suggest doing another pass as it does not align with the quality of the rest of the paper.\n\nRelated work:\n\nI feel like other methods for Parameter efficient transfer learning, such as Adapters, normal prompting should be quickly mentioned. Same goes for comparable approaches such as P-tuning. \nAs mentioned earlier, I would caveat Section 5 with discussions of the validity of using attention for explanation, such as the Attention is Not Explanation paper.\n\nEdit:\n\nGiven the author's response, I am raising my score slightly. There are still some concerns over the threat model but some of my questions on the impact of test batch size have been answered.\n\n",
            "summary_of_the_review": "The motivation, experimental settings and results look good overall.\n\nOne major caveat however is that it is not clear how flexible the inference setup is. This is critical as the predictions for a datapoint at inference depend on the other datapoints in the batch. Currently, it seems like the inference is being done with a batch of all the same attack. These assumptions are simply not realistic as a threat model. Without understanding how this performs under a more realistic threat model, I cannot score this paper higher. I have highlighted experiments that would provide more realistic results. \nDespite my \"marginally below\" score, I do not think the paper can be accepted without an answer on this point. Reject is too harsh for a paper that is otherwise promising.  \n\nThe paper would also benefit from another writing pass, mentioning missing related work and clarifying some properties of the dataset. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "\nThe paper is a focused contribution at the intersection of defending against text attacks and prompt tuning.\nThe paper requires the reader to understand the context and motivation of several different things before understanding the contribution of the paper. First, adversarial examples can attack a text classifier, such as UAT. Second, various techniques defend against these attacks in different way. However, these techniques requiring modifying the parameters of the LM or other additional computational burdens. These techniques can be used with prompt tuning, but then the benefits of prompt tuning go away. Hence, there ought to be a technique that improves the robustness of prompt tuning without removing its benefits over regular finetuning.\nThe paper proposes such a technique and do experiments for three text classification tasks and various adversarial attacks.",
            "main_review": "Strengths\n- The problem being addressed is cutting edge (we are barely understanding prompt tuning, and this paper already jumps ahead to adversarial defenses!)\n- The approach seems novel, though I haven't searched for related literature carefully. \n- Clear logic motivating the problem and what constraints need to be accounted for while solving it.\n- Clear research question.\n\nWeaknesses\n- The experiments are OK but could be a bit more streamlined. I guess two things came up when I was looking at them. \n\t- It may be good to expand the scope of the paper to generation tasks, as these are likely more suceptible to adversarial attacks. What I mean by this is that the worse case scenario is much worse for generation tasks: while in binary classification the worse case is bad accuracy, for adversarial attacks on generation tasks, potentially very harmful text could be generated, which is much worse than simply getting the answer wrong.\n\t- The other thing is that it is hard to contextualize how good the numbers in your framework are. For example, I have no idea how good 52% on VIPR for SST2 is. If previously proposed defenses are able to get 90% on the same task, then it is unlikely that the proposed method will gain traction, even if it is more computationally efficient with prefix tuning. So it would strengthen this paper a lot to have this comparison.\n\nOther comments\n- I think the paper may benefit from spending just a little more time describing how susceptible prefix tuning is to adversarial attacks compared with regular finetuning. It seems like it is very susceptible, especially for easy tasks like text classification.\n- Appreciate the candidness in Figure 2 in showing that adversarial training takes longer. \n- It will be good to see if the proposed method not only improves performance against adversarial attacks, but also if it improves performance against different paraphrases of expressing the context and question, for example.\n\nMinor points\n- The grammar of the paper will benefit from having a native english speaker proof-read it. E.g., \"as well as remaining the pretrained models unmodified\" can be re-written as \"without modifying the pretrained model parameters\".\n- Since a lot of work was put into the paper, I will pay my respect and make a picky point: the bibliography could be cleaned up a bit. E.g., capitalize RoBERTa correctly, add URLs consistently, capitalize all conference names (e.g., IEEE transactions).",
            "summary_of_the_review": "\nThe paper studies the very timely topic of adversarial attacks against prefix tuning. The paper proposes a method that maintains the advantages of prefix-tuning against finetuning, a formidible problem. The experiments are OK as of now, though expanding the scope to generation tasks could make the impact substantially larger. The characterization of the method could also be further improved by providing finetuning (with and without defenses) as baselines. Overall, I lean towards acceptance, though I am not an expert in adversarial attacks/defenses.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}