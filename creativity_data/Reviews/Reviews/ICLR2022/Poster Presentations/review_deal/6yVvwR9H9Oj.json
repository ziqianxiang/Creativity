{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "Addressed semi-supervised learning with the MNAR setting.  Well written paper.\nSeveral additional experiments were reported in response to the reviewer questions.  \nGeneral agreement amongst reviewers."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper addresses the semi-supervised learning problem under the setting of imbalanced label data by extending FixMatch with a class-aware method to reweight the loss and an adaptive thresholding method to select examples with pseudo labels. The two methods are termed Class-Aware Propensity (CAP) and Class-Aware Imputation (CAI), respectively. The paper also proposes the Class-Aware Doubly Robust (CADR) estimator by combining the two methods in the double robustness framework. Experiments are performed on Cifar-10/100, STL-10, and mini-ImageNet. ",
            "main_review": "\n### Strengths:\n\nThe paper made a clear review of the underlying theory for the pseudo-label-based semi-supervised learning framework. The framework can lead to an unbiased estimator if the unlabeled/labeled data status is fully random with respect to the overall data distribution.\n\nCAP: The imbalanced availability of labeled data across categories is a practically important issue. The paper presents clear and convincing descriptions of this problem setup. It revised the framework in a theoretically decent way to incorporate the class-aware propensity. In other words, the paper proposed a probabilistically rigorous method to reweight each sample’s training loss adaptively. The implementation of this idea in SGD with mini-batches and running average is also reasonable. \n\nCAI: The paper recognized the fixed threshold of FixMatch as a cause of bias across different categories. It proposed a formulation to set an adaptive threshold for each class based on the previous class-aware propensity. Though not very theoretically founded, the method agrees well with intuition (similar to the “focal loss” for object detection). The formulation can also gracefully fall back to the conventional FixMatch.  \n\nUsing the theory of double robustness to combine CAI and CAP provides a theoretically better way to combine CAP and CAI than simply summing them together. \n\nThe experimental results in the synthetic class-imbalanced settings demonstrated the effectiveness of CAP and CAI. They outperformed FixMatch and other previous semi-supervised learning methods significantly. In the fully random settings for labeled data, the proposed methods also did not hurt the model’s accuracy; in fact, it improved the model’s performance a bit.  \n\n### Weakness:\n\nIn comparison to the trivial combination of CAP and CAI, CDAR did not always lead to stronger results (though CDAR showed significant improvement over the trivial combination in some cases). Is there any explanation for the slightly mixed performance of CDAR?\n\nThe paper can be stronger if it can show experimental results on a real-world dataset (i.e., the labeled data are not synthetically imbalanced but originally imbalanced). \n",
            "summary_of_the_review": "The paper proposes a novel method to tackle the more practical problem of semi-supervised learning with imbalanced data. The experimental results are strong, and the ablation studies are informative. More discussions can be made about CDAR’s performance. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper formulates the problem of semi-supervised learning where the likelihood of an example being labeled is dependent on the class of the example. Given this assumption, the authors propose an approach to mitigate the resulting distribution shifts that occur in both the labeled and unlabeled data sets. Finally, experimental results are shown for the class-dependent labeling scenario, with the proposed algorithm outperforming standard SSL approaches that assume random labeling probability.\n",
            "main_review": "Strengths\n\n1. The paper does a good job of laying out the problem assumptions and differentiating from random labeling scenarios.\n\n2. The proposed solution is clearly explained and justified.\n\n3. The experimental results are well presented and the comparison across multiple different datasets/scenarios does a good job building the case for the proposed solution.\n\n\nWeaknesses\n\n1. Given the paper proposes the MNAR setting, it would help strengthen the justification/interest to point to a real dataset/setting that matches this assumption as the experimental results all are based on simulated class-dependent labeling. This is the major weakness of the paper to me as it weakens the justification/interest in the proposed problem.\n\n2. The experimental results seem strong, however for all of these datasets the dependence of labeling on class is extremely high (or zero in the demonstration of consistency in the missing at random case). It would be interesting to see the performance comparisons with baselines for label dependence in the intermediate range. This is especially true given the synthetic nature of the missing labels making it hard to determine whether the proposed labeling dependence is reasonable/realistic.",
            "summary_of_the_review": "I am in favor of accepting this paper as the problem and solution are well written and the experimental results seem reasonable. The main weaknesses are the lack of an empirical example to justify the proposed setting and demonstration of the proposed approach on only simulated cases of MNAR data.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work proposes a semi-supervised learning algorithm with systematic label missing, aka Missing Not At Random. The proposed solution is composed of two steps. First, to learn the classifier with the labelled data instances, a learnable weight, named propensity score, is proposed and attached to each labelled data instance to deliver an unbiased estimate (see Eq.10). Second, a class-wise threshold is tuned according to the class-specific propensity score (see Eq.11). The results show better semi-supervised classification accuracy compared to other baselines in the MNAR scenario. ",
            "main_review": "Attacking semi-supervised learning with the MNAR setting is an interesting and important topic for practical learning tasks. The empirical study is comprehensive and the proposed method provides a significant improvement compared to state-of-the-art semi-supervised learning approaches. \n\nWeakness: \nTwo SOTA baselines of semi-supervised learning, MixMatch (https://proceedings.neurips.cc/paper/2019/file/1cd138d0499a68f4bb72bee04bbec2d7-Paper.pdf) and ReMixMatch (https://openreview.net/pdf?id=HklkeR4KPB) are not included in the empirical study. ",
            "summary_of_the_review": "Please find our comments above. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}