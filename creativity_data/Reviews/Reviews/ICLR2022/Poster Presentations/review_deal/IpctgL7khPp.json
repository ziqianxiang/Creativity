{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "One way of avoiding catastrophic forgetting in continual learning is through keeping a memory buffer for experience replay.  This paper addresses the problem of online selection of representative samples for populating such memory buffer for experience replay.  The paper proposes novel information-theoretic criteria that selects samples that captures surprise (samples that are most informative) and learnability (to avoid outliers).  They utilize a Bayesian formulation to quantify informativeness. They provide two algorithms: a greedy approach, and an approach that takes timing (when to) update memory into account based on reservoir sampling to mitigate possible issues with class imbalance.\n\nPros:  The paper is well written and organized.  It was easy to follow. The formulation is novel and technically sound. The idea of taking learnability into account is novel and interesting.  It provides a nice way of avoiding outliers and balancing surprising information. The authors presented the motivation for each part of the framework well.  \n\nCons: To understand the contribution of each component of the formulation and competing criteria, an ablation study is needed.\nReviewers had several detailed suggestions and questions, including sensitivity to hyperparameters, additional citations, additional data sets beyond MNIST and CIFAR10, etc.  \n\nIn the rebuttal, the authors have addressed several of these concerns.  Please make sure to include and incorporate reviewer suggestions in the final revised version."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work targets the problem of Continual Learning (CL) and In particular, the problem of populating the buffer memory in the Experience Replay (ER)-based paradigm. In ER, we are allowed to store a small subset of the incoming data stream, therefore deciding on a good policy to populate the memory buffer is important. The paper proposes MEMORABLE INFORMATION CRITERION (MIC) which is a combination of the surprisal and learnability of an incoming example and further motivates this with a  bayesian analysis. Using MIC, it introduces two algorithms, namely, InfoRS and infoGS for buffer update. The paper includes experiments on standard CL datasets with a focus on General Continual Learning (GCL) guideline, which focuses on not having dependence on task boundaries. Finally, it reports the proposed method's performance on imbalanced data streams and shows that infoRS outperformed reservoir sampling, GSS, and some other baselines. \n",
            "main_review": "Strengths:\n\n1. I think that the paper was easy to follow overall, and the MIC criterion is intuitive. The tSNE plots (of the selected example for the memory of size 200) showing the ablation study was also quite interesting. \n2. The proposed algorithm outperformed RS and other mentioned baselines, without too much computational overhead over RS. \n3. Authors do study over other possible metrics than MIC, including Entropy Reduction, therefore empirically justifying the criterion. \n\nWeakness: This subsection is a collection of doubts, suggestions about some more experiments (including other baselines), and how this submission can be improved overall. \n\nBaselines and Performance metrics:\n1.  I suggest including the paper [1] and online k-medoid clustering. [1] “Coresets via Bilevel Optimization for Continual Learning and Streaming” NeurIPS’20. \n2. In my opinion, it is also important to report the backward transfer or forgetting metric, similar to the DER paper.\n3. While the manuscript contains final testing results for different imbalance levels, I think for CL papers a plot of accuracy after each task provides more information ( For example, see Fig 3 in the A-GEM paper -  EFFICIENT LIFELONG LEARNING WITH A-GEM).\n\nTechnical Questions:\n\n1. How sensitive is the performance with the thresholding hyperparameter?\n2. Fig (5) right, similar to infoRS, I think for RS one also needs the confidence bars. \n3. In the section about robustness to imbalance, does a different seed change the task ID in which we add more epochs? Moreover (more of a sanity check), but do authors keep the task-iD same across different “R” values?\n4. Algorithm (2): What is the need to compute A_M^-1 and b_M when the memory is not full? Cannot one just do that once we fill in the memory, and use features based on the new parameters?  \n5. Algorithm (2): Rather a technical question - why are we only choosing one example from each batch? There can be a case that we have two examples with similar (but not the same) and high MIC values and are learnable (two different classes, right when we begin a new task). \n6. Regarding GSS: Are authors using the GSS-IQP, or the GSS-Greedy? Moreover, one easy trick one can do to accelerate GSS is to consider gradient with respect to the last layer only, the way it is done in the paper [1] (See [2] as well). [1] Coresets for Data-efficient Training of Machine Learning Models, ICML’20. [2] Not all samples are created equal: Deep learning with importance sampling, ICML’18. \nRegarding GSS: Rather a sanity check, but do authors use DER++ loss in this case for the fair comparison?\n7. In Section E appendix (Proof), equation (19), where the KL is expanded, in my opinion, the numerator seems notationally odd. That is, why is x_{\\ast} mentioned as a conditioned variable and as a parameter at the same time. Wouldn’t it suffice to just use it as a conditioned variable? \n\nTechnical suggestions:\n1. Fig (2), please show how does the memory look like in the tSNE plot for the cases with small sizes such as 10/25 for RS and infoRS (w/ and w/o learnability)  \n2. What is mentioned here as DER is actually DER++ in the original paper.\n3. In the related works section, which talks about dataset distillation, there is another paper [1] (ICLR’21) that also talks about dataset distillation with applications in continual learning. [1] Dataset Condensation with Gradient Matching.\n\nWriting Suggestions:\n\n1. In equation (1), I think it will be good to not use \\theta for both h and g. \n2. Equation (3), during the overall update step, we update memory as well as the network parameters. Therefore, I suggest using notations accordingly.\n3. In the algorithm block, I would recommend adding line numbers, which improve readability and cross-referencing in a discussion. \n4. Section 2.1, last paragraph: Add spacing after the period right before the “Furthermore”.\n",
            "summary_of_the_review": "It is an interesting work, however, lacks discussion about some baselines (not mentioned in the paper) and other performance metrics (about forgetting, BWT). There are some questions (mentioned in the detailed review) that need to be answered in order to make it a strong submission. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper considers task-free continual learning in the context of online memory selection. The author consider the use of information-theoretic principles and propose the surprise and learnability criteria to select informative points and avoid outliers. The authors also propose InfoRS to sample among selective points with high information. The methods are validated on continual learning benchmark datasets, and show efficiency and improvements over existing approaches. ",
            "main_review": "Strengths:\n\n+ The proposed methods seem fairly natural. In particular, the notion of surprise and learnability seem well-suited to the current problem at hand. The computation of the Bayesian posteriors is also natural and the Gaussian assumption, together with conjugacy, results in efficient updates, which is nice. \n\n+ The InfoRS algorithm also seems natural for sampling. \n\nWeaknesses:\n\n- It would be preferable for the authors to consider the effects of each of their proposed modules separately -- surprise, learnability as well as InfoRS. Currently from the experimental section, it is unclear which module contributes to which aspect of the improvement. An ablation study would be useful. \n\n- I am wondering whether the authors have examined the effect of the many hyperparameters in their study such as \\eta, \\alpha and \\beta. \n\n- The datasets used seem rather simplistic, just MNIST and CIFAR10. ",
            "summary_of_the_review": "Generally, the paper has some novel contributions but also some minor weaknesses. If the authors can address the weaknesses/queries I raised in the main review, I'm happy to increase my score. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper considers an online memory selection task for continual learning applications. The paper proposes a new (information-theoretic) criterion to pick informative points and void outliers. This criterion is a combination of the surprise and learnability score. In addition, the paper presents a stochastic information-theoretic reservoir sampler (InfoRS) to sample among selective points with high information.\n\nThe paper empirically demonstrates the proposed criteria encourage to select representative memories for learning the underlying function. They consider continual learning benchmark as well as ablation studies of data imbalance. \n",
            "main_review": "Objective of the work:\n* The paper extends the existing research direction in general continual learning. The key contribution is proposing the criteria to select and replace the existing points in the memory buffer.\n\nStrong points:\n* A new method for online memory selection. The surprise captures how unexpected a new point is given the memory, and allows us to include new information in the memory. The learnability captures how much of this new information can be absorbed without interference, allowing us to avoid outliers.\n* The proposed Memorable Information Criterion (MIC) is a weighted combination of the proposed learnability and surprise scores. The authors also connect that MIC is related to Information Gain. In the appendix, the paper has compared these criteria (MIC, IG, ER) thoroughly.\n\nWeak points:\n* The surprise score is related to the uncertainty score in literature which has been widely used to select unseen points. While the learnability makes sense to distinguish the unfamiliar and outlier points. Having said that, the reviewer thinks that the learnability score can make the algorithm too preservative to learn new information.\n\n* The performance may critically depend on balancing how much we want to “surprise” versus “learn” through $\\eta$. It is highly recommended to have the ablation study with respect to $\\eta$.\n\n* Bayesian linear regression requires matrix inversion of a matrix $d \\times d$ where $d$ is the feature dimension. The Sherman-Morrison Formula, Eq. (11), can be used to efficiently update the inverse matrix. However, while the reviewer acknowledges the trick works well in practice, the reviewer doesn't see this is a great technical contribution of the paper.\n\n\nPresentation: \n* The paper is in general well written and easy to follow.\n\nReproducibility:\n* The results appear to be reproducible though I would encourage the authors to release their code to enable better validation of their claims.\n\n\nMinor points:\n* The author has demonstrated the effectiveness of the proposed criteria using a greedy algorithm to replace the existing points with the new points if certain condition is met. It could be interesting to consider either replacing (as the proposed framework) versus expanding without replacing. The reviewer thinks that we may need to enlarge the memory buffer overtime to learn more tasks. However, this would be out of the scope of the paper.\n\n* In Figure 2, it is not clear to compare the middle figure and right figure to see the effect of learnability.\n\n* The paper presents a scalable Bayesian model that can compute surprise and learnability with a small computational footprint by exploiting rank-one matrix structures. Instead of using a deep network and Bayesian linear model, I am wondering if it is better to directly train a Bayesian neural network for estimating the uncertainty?\n\n* a footnote must be placed at the end of a clause or sentence, such as adding the number after the comma or full stop.\n\n",
            "summary_of_the_review": "The key contribution is proposing the criteria to select and replace the existing points in the memory buffer. However, the technical novelty is limited. The paper includes several techniques which have been widely used elsewhere. The surprise (uncertainty estimation) has been widely used in literature. The Bayesian linear regression and the rank-one update trick are also well studied.\n\nEmpirically, the performance may critically depend on balancing how much we want to “surprise” vs “learn”. This should be well studied in the experiments. Perhaps, the paper can propose a technique to adaptively learn $\\eta$ over time.\n\nThe reviewer thinks the paper is currently at the borderline.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}