{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper considers the convergence of the Monte Carlo Exploring Start (MCES) algorithm, a basic method in RL. Although the method is very simple and known for a long time, the condition for its convergence is not completely understood.\n\nOne of the latest results is from almost 20 years ago (Tsitsiklis, \"On the Convergence of Optimistic Policy Iteration,\" 2002). That paper shows the convergence of the method under some restrictive assumptions on how the algorithm operates. In particular, that result requires that only the action-value function of the initial state-action pair of an episode be updated, as opposed to all visited state-action pairs. What is not known is whether we can update the action-values of all state-action pairs observed in a trajectory.\n\nIt is notable that one of Tsitsiklis (2002) result requires a uniform selection of the initial state-action. But he also describes a modification of the algorithm that allows convergence with a non-uniform initial distribution (see page 66 of that paper, close to the end of Section 3 - Optimistic Policy Iteration Using Monte Carlo for Policy Evaluation).\n\nOn the other hand, this paper establishes the convergence of MCES with no assumption on how the algorithm works. In particular, the algorithm updates the action-value function for all state-action pairs observed in an episode. As long as all state-action pairs are visited infinitely often (and not necessarily from a uniform distribution), the convergence is established.\n\nThere is a catch, however. The paper requires some assumptions on the class of MDPs. In particular, it requires the environment to be either Stochastic Feed-Forward (SFF) (more restrictive) or Optimal Policy Feed-Forward (OPFF) (less restrictive). The OPFF assumption states that under any optimal policy, a state is never revisited within an episode.\n\nThe proof technique of this paper is different from the usual stochastic approximation method, and may be considered simpler.\n\nWe have two positive reviewers (with score of 8) and two slightly negative ones (with score of 5). The concern of negative reviewers is that the OPFF assumption is very restrictive. And given such an assumption on the class of MDPs, the proof becomes very simple. Moreover, it is not clear that the proof techniques developed in this work is a step toward analyzing MCES for more general MDPs.\n\nMy related concern is whether OPFF vs. Non-OPFF is a good way to characterize MDPs for which MCES with every-state update is convergent. Figure 3 in the paper shows the current state of knowledge in the analysis of variants of MCES. We know a problem with \"No convergence\" within the fourth quadrant (which is Non-OPFF).\nIs the class of Non-OPFF a tight superset of the non-convergent ones?  Or is it a much larger superset? If it is tight, then OPFF is a good characterization of when MCES works or not. If it is not tight, OPFF may not be the right way to characterize the convergence of MCES.\n\nAll being said, I believe this paper positively contributes to our knowledge of a basic and fundamental RL algorithm. It does not fully resolve the convergence question, but it is indeed a progress. Whether OPFF characterization is going to be the right one or not remains to be seen in the future. I would act optimistically here and recommend acceptance of the paper.\n\nI encourage the authors to incorporate the comments by the reviewers in updating their papers, including:\n- fixing all the typos\n- providing more examples of problems that are OPFF\n- resolving the claim about alphaZero\n- providing empirical comparison with Q-Learning"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies Monte Carlo with exploration starts algorithm for solving the reinforcement learning problem. The writing is clear and I enjoyed reading this paper. As for the results, asymptotic convergence of the algorithm is established without needing strong assumptions in related literature. As pointed out by the authors, the result resolves an important open problem in RL. The proof is simple and intuitive. Numerical experiments corroborate theoretical findings.",
            "main_review": "Main comments:\n\nOverall I really like this paper. To prove convergence of stochastic iterative algorithms, existing results rely on the properties of Bellman equation, and use either super-martingale convergence or ODE approach. This paper uses induction along with strong law of large numbers, resulting in a much simpler proof. This paper does seem to be of interest to the broader community of ICLR.\n\nOther comments/questions:\n\n1 There are several typos, such as (1) \"Corrolary 1\" should be replaced by \"Corollary 1\", (2) the sentence before Corollary 1, the \"snd\" should be replaced by \"and\". The authors should carefully go over the paper again to fix them.\n\n2 As for the numerical experiments, I am curious to see the convergence rate comparison between MCES and model-free Q-learning, which is probably the most popular value-based RL algorithm in the literature.\n\nSuggestions on future directions: Now that convergence is shown for tabular MCES algorithm, there are many interesting potential future directions for this line of research.\n\n1 Convergence rate. The convergence rate of strong law of large numbers has been established in the literature. While the induction technique can be used to show asymptotic convergence, it is not clear if it can be used to show the convergence rate. It is an interesting future direction to show the convergence rate of MCES and compare it to that of Q-learning.\n\n2 Function approximation. The next question is about extending the result to the function approximation setting. Q-learning with function approximation is an major theoretical open problem in RL because of the deadly triad. I am curious to see if one can show any theoretical guarantees on MCES with function approximation, or if it suffers from the same difficulty as Q-learning.\n\n-------------After Author Feedback-------------\n\nThank the authors for their feedback. I would like to keep my score and vote for acceptance.\n\n\n",
            "summary_of_the_review": "This paper provides convergence guarantees of MCES algorithm under mild assumptions, hence resolving an important open problem in RL. I enjoy reading this paper and I think it has enough contribution to be published at ICLR.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies the Monte-Carlo Exploring-Starts algorithm on MDPs where states are never re-visited and provides convergence results for such MDPs.",
            "main_review": "The paper is written well; I enjoyed how it provides a thorough yet accessible introduction to the MCES algorithm and known results for it. The theoretical results are also clear and intuitive, as well as their proofs. I agree with the claim that the main advantage here compared to previous work is in using all states of the trajectory for the update, instead of only the first one.\n\nWith that said, this seems like a classic case where the simplicity and elegance of the theory is because it is indeed simple and easy to derive. The results are intuitive and not surprising since they are (in my opinion) almost trivial. Once you limit the environment to a DAG and are able to sample each state-action pair indefinitely, it seems obvious to expect to converge to the optimal policy. This follows from the basic dynamic programming concept where you begin from the last state and gradually update your value function to being optimal. Also, as the limited subclass of MDPs considered which are essentially DAGs, it feels like classic CS literature should have similar solutions so a literature survey in that area is advised. \n\nI think the contribution would have been more significant if, for instance, the indefinite initial state selection would have not been possible. In that case, you will not necessarily sample the terminal state or its neighbours and a probabilistic analysis would most likely be needed. Alternatively, if the rate of convergence would have been analyzed, we could understand how the sampling procedure affects convergence, which might help guide more efficient variants of the algorithm. \n\nLastly, two questions: I'm not sure that AlphaZero updates all states within a trajectory instead of just the initial one as claimed in the middle of p.2. A reference is missing there. Second, why do mujoco environments qualify as a feed-forward environment as claimed in the bottom of p.2? The walking tasks there are highly repetitive.",
            "summary_of_the_review": "To summarize, while I enjoyed reading the paper and its simplicity (which I appreciate in general), its contributions are correspondingly minor and not surprising. The combination of the highly restrictive realm of tabular MDPs in which states are never revisited, together with the strong assumption of ability to sample every state-action indefinitely, generate an easy problem that is not particularly hard or interesting to solve.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies the convergence of Monte Carlo Exploring Starts (MCES), in which the Q-function is estimated by averaging Monte Carlo returns and the policy is defined as the greedy policy w.r.t. this Q-function. The authors provide a technically simple proof of the convergence under different assumptions on the underlying MDP: (i) stochastic feed-forward MDP (in which states cannot be re-visited in an episode); (ii) optimal policy feed-forward (in which under the optimal policy states are not re-visited); (iii) finite-horizon MDPs. Given these additional assumptions, the analysis relaxes other assumptions employed in previous convergence proofs. Finally, experimental results to validate the considered set of assumptions is provided.",
            "main_review": "Pros:\n- The paper provides a novel convergence proof for MCES that relaxes the assumptions that have been previously employed for proving convergence of MCES\n- The proof, as the authors highlight, is simple and does not make use of hard mathematical/statistical tools\n\nCons:\n- The classes of MDP considered seem very constraining. Basically, it is required that (in the less restrictive case under the optimal policy) the states are never re-visited along one trajectory. The authors succeed in providing justifications for these assumptions, especially providing several realistic examples in which the assumptions are fulfilled. Nevertheless, I think that these assumptions are ensuring that within one episode a single (s,a)-pair will be updated at most once, significantly simplifying the analysis. My main concern is about the significance of these convergence results under such strong assumptions. To put it another way, is this analysis a step towards the understanding of MCES for general MDPs or just the analysis of a particular restricted case?\n\nMinor issues:\n- Pag 4: \"Maximizing (2) corresponds to the stochastic shortest path problem\". Is it true? I think that this holds only when the reward penalizes the steps needed to reach the goal state, not for general rewards.\n- Lemma 1: I think that some condition on the almost sure boundedness of the involved random variables is needed.\n- The ticks on the plot axis are too small\n- Algorithm 3, line 9: Q should have an additional argument t\n- Pag 1:  doesn’t -> does not\n- Pag 4:  isn’t -> is not",
            "summary_of_the_review": "Overall, I think that the paper provides a novel contribution. However, given that the considered assumptions are, in my opinion, very restrictive, I have concerns about the significance of the theoretical results. For these reasons, I opt for a borderline score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a proof of the Monte Carlo Exploring Starts algorithm given by Sutton and Barto (1998) for a class of MDPs.",
            "main_review": "Strengths:\n    Presentation is mostly clear\n    A clear comparison to related works\n    An important theoretical contribution to the field.\n    Simple idea and proof.\n    Empirical results echo the claims to some extend.\n\nWeakness:\n   The assumption of the uniqueness of the policy is not presented in the theorem.\n   More discussion about the empirical results would be helpful. For example, Tsitsiklis's algorithm requires a uniformly sampling of the start state in order to guarantee convergence, but empirically it also works well with a different way to sample the start state. I would like to see how the authors think about it.\n   Some expressions can be improved:  the word \"iteration\" is not in the algorithm but is in the proof to refer to something in the algorithm.\n   Some typos: 1. Algorithm3 line 9, it should be Q_t(S_t, t, A_t), 2. when citing a paper by two authors, sometimes it lists two authors' names and sometimes it uses et al.\n    ",
            "summary_of_the_review": "Overall I like this paper because of the strengths listed in the main review. There is some weakness but it doesn't weaken the main contribution of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}