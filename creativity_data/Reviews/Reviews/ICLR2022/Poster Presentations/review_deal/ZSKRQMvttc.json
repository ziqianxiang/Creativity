{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "There is consensus in the reviews that this paper convincingly demonstrates strong acceleration of policy learning using differentiable simulation in tasks involving contact-rich dynamics. The authors are encouraged to explore where the smoothness assumptions made in the simulator actually transfer to real robots. The paper may be further strengthened through more complex benchmarks involving contact rich manipulation."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper demonstrates the effective use of differentiable simulators for significantly speeding up the training of policy gradient methods. The main idea is to backpropagate through the simulator only on shorter partitions of the trajectory, which seems to lead to a smoother loss landscape and better gradient signal. To avoid getting stuck in local minima, a critic is used to incorporate a terminal cost. As the implementation can be parallelized and the simulation run on the GPU, the experiments show drastic improvements not only in sample efficiency but also in run time.",
            "main_review": "Strengths:\n* Idea is conceptually simple and provides strong results on a spectrum of established benchmark environments as well on one very high dimensional task.\n* Shows the potential of backward differentiation through time.\n\nWeaknesses:\n* Could be run on other benchmark environments, which might highlight more the limits of the proposed approach.\n* The analysis of why and whether the loss landscape is smoother could be more elaborate. Additional figures like for example a histogram of the gradient distribution evolving over time would, in my opinion, help the reader to have better insight than Figure 2.\n\nQuestions:\n* How important is the structure of the reward function for this approach?\n* Have you found out on which kind of environments the proposed method fails?\n\nAdditional Remarks:\n* Typo in the introduction: high-dementional\n* I found the truncation of the curves in Figure 4 visually misleading at first.\n* If section 3.2 would be more elaborate and the experiment would include more environments, this paper could be further improved.",
            "summary_of_the_review": "This paper shows that differentiable simulators can be leveraged to significantly speed up training even on very challenging high-dimensional tasks, making it an interesting empirical and conceptual contribution. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper focuses on the setting of reinforcement learning with differentiable simulation, where the authors assume access to a simulator which can compute the derivatives of the next state and reward with respect to the current state and action exactly. Under this setup, the authors propose an actor-critic style reinforcement learning algorithm that leverages the simulator derivative information to achieve better training sample efficiency and wall-clock time. Specifically, the authors expand the value function of the policy in h steps, and combine the sum of the simulator gradient of these h steps and the gradient of the learned value function at the h + 1 step to obtain the overall gradient for training the policy. The value function is then trained in the same way as in other actor-critic style methods.\n\nThe authors evaluate the proposed algorithm empirically on a few domains and the experiment results suggested that the proposed method achieves superior sample efficiency and wall-clock time compared to prior methods. \n\n",
            "main_review": "Overall the paper introduces an interesting idea of leveraging simulator derivative information to achieve fast and sample efficient reinforcement learning. The idea is well presented and I found the empirical results very convincing. However I do find a few shortcomings that I hope the authors can address.\n\n\nPros\n\n\n1. The empirical results of the proposed method are very strong. It can be clearly seen that the proposed method outperforms baselines significantly. I consider this as the main advantage of this paper, since the proposed method and the differentiable simulator could serve as good foundations for future work.\n\n\n2. The PyTorch GPU based differentiable simulator that supports complex environments is another important contribution of the paper. If released, it could be impactful for the field as it enables researchers to obtain controllers for complex systems much faster.\n\n\n3. The paper is well written. The overall structure of the paper is well organized and the presentation of the main contribution is easy to follow.\n\n\nCons\n\n\n1. I’m not convinced about the novelty of the proposed method. The proposed method is mostly the same as the method introduced in [1], where the only difference seems to be that the proposed method uses exact gradient from the simulator instead of gradient of a learned dynamics model. Although the proposed method is not new, a major novelty of this paper could come from the creation of the differentiable simulator that supports complex dynamics models such as the Humanoid MTU used in this paper. However unfortunately the simulator implementation is not discussed in detail in this paper.\n\n\n2. The sample efficiency of off policy reinforcement learning methods has also improved a lot in recent years, where some model free methods can even achieve sample efficiency close to that of model based methods ([2]). It would be important to compare to some of these methods, because this comparison would tell us how much improvement can we get from using the derivative information of the simulator.\n\n\nGiven these limitations, I cannot recommend acceptance of this paper at its current state. I highly encourage the authors to continue improving the paper by providing more discussion regarding the simulator and adding the comparison to more recent reinforcement learning methods.\n\n\nReferences\n\n\n[1] Clavera, Ignasi, Violet Fu, and Pieter Abbeel. \"Model-augmented actor-critic: Backpropagating through paths.\" arXiv preprint arXiv:2005.08068 (2020).\n\n\n[2] Chen, Xinyue, et al. \"Randomized ensembled double q-learning: Learning fast without a model.\" arXiv preprint arXiv:2101.05982 (2021).\n\n\n## Update after Authors' Response\nThe authors have updated the paper to include more description of the simulator and the missing references. Focusing more the the differentiable simulator, the manuscript now reflects the contribution of the work more accurately and therefore I believe it is a valuable addition of the community. Now I recommend acceptance of this paper.\n",
            "summary_of_the_review": "The paper presents an intuitive reinforcement method for leveraging derivative information from a differentiable simulator. The methods significantly outperforms prior methods in terms of both sample efficiency and wall-clock time. However, the proposed method is not novel as it is largely the same as that introduced in a prior work, and it is missing an important baseline comparison. Therefore I cannot recommend acceptance of the paper in its current state.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors introduce a novel differentiable simulator (built on PyTorch) that focuses on enabling smooth gradients with respect to expected return, especially with a view to ameliorating discontinuities in contact dynamics. In order to leverage this differentiable engine as effectively as possible, they also introduce SHAC, which is an analytical policy gradient (APG) method that features a terminal value function to truncate the value gradient calculation through time. The authors' show this results in a smoother objective function with lower training time, which empirically gives rise to faster/improved convergence in policy learning.",
            "main_review": "Pros:\n* The paper is clearly written and well organized.\n* The differentiable simulator introduced will likely have great value to researchers, as not only does it have the ability to be differentiable, but being built on PyTorch means that it is likely more interpretable and customizable than similar methods [1], and is also natively parallelizable, which will result in massively reduced experimentation times.\n* The authors show very convincing performance of their introduced SHAC method on this suite of benchmarks compared to competitive methods, including non-analytical policy gradient approaches (PPO/SAC), as well as other methods that leverage differentiable environment dynamics.\n\nCons:\n* Key design elements and motivations in the SHAC method have appeared in different guises in the literature before, and some of these aren't referenced. For instance, in MAAC [2], they too have an analytical policy gradient (APG) (due to differentiability of the world model), and then prevent issues of long-horizon BPTT by including a truncating value function. Going further back, in [3] the idea of terminating rollouts using a value function is also used/introduced (in their case also to improve computation time). Indeed, the paper cited by the authors [4] which uses 'enhancement' can be viewed as a one-step analytical policy gradient with termination. Perhaps the authors could describe why having a state-value terminal function (instead of an action-value function which supports action gradients) is beneficial in their setting? Also I am somewhat concerned by the use of different architectures for each task; could the authors run some sensitivity analysis by keeping architecture fixed for each task to demonstrate the robustness of SHAC to this?\n* Similarly, given the relative simplicity of the SHAC method, it's somewhat disappointing that the authors only introduced 4 environments, and no manipulator-based contact environments (which would surely be a good fit for the goal of smoothened contact physics). In my opinion, focus should have been placed more on either the method or the simulator, but at this point for me neither display the requisite gravitas. I would like to see some additional environments introduced, such as a Cheetah and Hopper derivatives, and this would help the adoption of such an environment to a wider RL audience.\n* Are the smoothness assumptions made in the simulator actually transferable to real life robotics? There should be discussion as to whether or not the modelling choices made are realistic (e.g., smooth joint limits, choice of differentiable contact physics). As has been pointed out in the past, non-APG methods can be successfully deployed on real robots without requiring differentiability [5], whereas APG approaches will need to to get over the issue of `sim2real`. I can see this in fact being further exacerbated by the differentiability of the simulator, which would allow the policy to very effectively exploit the simulator to obtain great solutions that don't transfer (for instance, see [6] where the solutions are able to exploit deficiencies the MuJoCo simulator). In this case, perhaps the humanoid MTU solutions of SAC aren't as optimal in the simulator, but could transfer better IRL. Is there a way the authors could show that this exploitation is avoided in their simulator?\n* I found the single weight perturbation method to demonstrate smoothness slightly odd as there is abundant literature that aims to visualize loss surfaces, such as [7]. Why did the authors elect to use their single weight perturbation approach?\n\nRefs:\n\n[1] Brax -- A Differentiable Physics Engine for Large Scale Rigid Body Simulation: Freeman et al., arXiv:2106.13281\n\n[2] Model-Augmented Actor-Critic: Backpropagating through Paths: Clavera et al., arXiv:2005.08068\n\n[3] Value Function Approximation and Model Predictive Control: Zhong et al., ADPRL ‘13\n\n[4] Efficient Differentiable Simulation of Articulated Bodies: Qiao et al., arXiv:2109.07719\n\n[5] Soft Actor-Critic Algorithms and Applications: Haarnoja et al., arXiv:1812.05905\n\n[6] On the Importance of Hyperparameter Optimization for Model-based Reinforcement Learning: Zhang et al., arXiv:2102.13651\n\n[7] Visualizing the Loss Landscape of Neural Nets: Li et al., arXiv:1712.09913\n",
            "summary_of_the_review": "Overall the paper introduces a novel differentiable environment suite, and aims to ameliorate a key issue in analytical policy gradients. However I believe there are deficiencies in both these things that I have listed above, so recommend a weak rejection for now.\n\nHaving said this I believe this paper has great promise, and think these issues are not major, and am happy to raise to an accept if they are sufficiently addressed.\n\n--------------- POST REBUTTAL ------------------\n\nThe authors have addressed most of my concerns, including the sensitivity of their approach to architecture, and included new environments aligned with canonical continuous control problems. Furthermore they focused more on the simulator in their updates, which is where I believe most of the impact of this paper lies, and produced additional interesting analysis about loss landscape smoothness.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "They implement a differentiable simulator based on PyTorch (actually it was a bit unclear to me whether they implemented it, or just used Isaac Gym; I have a question about this below, and hope to receive an answer from the authors), as well as a method to utilize the differentiable simulation to optimize the policy more efficiently. The method operates as follows:\n1. It computes a batch of episodes each of length H using a stochastic policy.\n2. It splits the episodes into subsequences of length h.\n3. It adds a terminal value function to each sequence, as well as discount factors on the rewards (starting from no discounting at the beginning of each subsequence), and computes policy gradients via backpropagation for optimization.\n4. The value function is learned by computing lambda return targets (note that lambda returns are not used in the policy optimization though).\n\nThere is also some discussion about exploding gradients that hinder optimization through long horizons, which is why they truncated their episodes into subsequences.\n\nThey perform experiments on cartpole swingup, ant, humanoid and humanoid MTU (a humanoid actuated by 152 muscle tendon units), and compare to PPO, SAC, Backpropagation through time and PODS. The proposed method (SHAC) outperformed the other methods both in terms of number of steps (by a large margin) as well as computation speed (though PPO was faster in terms of wall-clock time in the early stage of learning after which SHAC overtook it). Particularly on the humanoid MTU task, the final performance was much better than the other methods. In terms of wall-clock time, the required computation time was a few minutes for cartpole and ant, and around 1h for the humanoid tasks.\n\nThey also performed ablation studies looking into the necessity of the terminal value function (it was necessary) and also the length of the subsequences (intermediate length sequences were optimal).\n\nRecently some similar simulators, e.g. Brax or Isaac Gym have been made, but these works have not yet shown impressive results by taking advantage of differentiating through the model.",
            "main_review": "**Strengths**\n- The experimental results are impressive, and show the utility of using differentiable models.\n- The method is sensible and well-motivated.\n- Clarity is generally good, and method is explained in sufficient detail.\n- The experimental procedures seemed well done, and they included ablation studies on the necessity of the critic and the used short horizon length.\n\n**Weaknesses**\n- Novelty is somewhat low.\n- Discussion of gradient explosion seems inaccurate.\n- Some discussion of related work is missing, particularly about related works in the model-based RL literature.\n- Sometimes they are stating things as facts without providing evidence.\n- There was no detailed discussion of how the results with the current simulator relate to other recent simulators such as Brax. Are the wallclock times similar? The results seem mixed, but having a detailed discussion would have been useful.\n- The simulator is fast, so it may have been better to do more than 4 environments.\n\n**Recommendation**\n\nI recommend accepting the paper as I have not seen differentiable simulators used for tasks of the difficulty considered here (while also\ntaking advantage of differentiating through the simulator). I was considering a score of 6 or 8, but gave 6 for now.\n\n**Discussion of points brought up**\n\nNovelty:\nIt seems the contribution is primarily one of engineering, and they don't propose any surprising new idea. The idea of truncated backpropagation is old. Moreover, the policy training scheme resembles that of the the Dreamer algorithm (Hafner et al. 2019). Dreamer also uses short horizon rollouts together with a terminal value function, and backpropagates through these short horizons to optimize the policy. The differences are only: Dreamer uses lambda return weighting during the short horizons (why didn't you use this?), and the method of constructing the sequences is different (Dreamer samples start states from a replay buffer and performs model rollouts from these states, while the current work splits an episode into chunks). I am surprised that Dreamer was not discussed when explaining the methodology. Certainly it should be mentioned that there are prior works using a similar policy optimization procedure (with slight variations).\n\nDiscussion of gradient explosion:\nThere are other earlier works that do a more detailed job of discussing the gradient and landscape issues, such as PIPPS (Parmas et al. 2018), which should have been cited and discussed (moreover your methodology was very similar to these previous works). In your work, if gamma were 1, the value function were perfect and the policy were deterministic, the gradient you compute with your method should be exactly the same as the gradient that is computed using BPTT. From this point of view, your discussion is insufficient, as you do not explain why the loss landscape and gradients end up being smooth despite this fact. I can think of two possible reasons: 1. The value function is an approximator that ends up being smooth because of limited capacity to model the complicated landscape. 2. You are using a stochastic policy, and this stochasticty smooths out the landscape that the value function is estimating; hence it becomes smooth. However, one of the points brought up by Parmas et al. (2018) was that even if the landscape is smooth (due to averaging over policy or model stochasticity) the gradients computed by backpropagtion can be ill-behaved and lead to an explosion of the gradient variance.\n\nFor the other points see the raw notes at the bottom of this section.\n\nParmas, P., Rasmussen, C. E., Peters, J., & Doya, K. (2018,\nJuly). PIPPS: Flexible model-based policy search robust to the curse\nof chaos. In International Conference on Machine Learning\n(pp. 4065-4074). PMLR.\n\n**Questions**\n\nQ1. In section 3.1 you write that you built the simulator. But from section A.3.1. it seems you just used Isaac Gym. So which one is it: did you make the simulator or did you use Isaac Gym?\n\nQ2. In Figure 2, did you use a deterministic or stochastic policy? Was this the same policy as was used during training the value function? If the policy was stochastic, then how did you evaluate the landscape? This would require sampling many trajectories with the same policy and averaging. Are the scales on the left and right figures the same?\n\nQ3. Did you do any ablation study of the policy noise? Does the method still work when the policy is deterministic? How much does the performance drop?\n\n**Additional notes made during reviewing**\n\n\"for systems ranging from robots (e.g., Cheetah, Shadow Hand) to\ncomplex anima- tion characters (e.g., muscle-actuated humanoids) using\nonly high-level reward definitions.\"\nPlease provide references.\n\n\"A differentiable simulator provides accurate first-order gradients of\nthe task performance reward with respect to the control inputs.\"\nThis is speculation. You provide no evidence. Problems with accuracy\ncan arise when the task performance depends on a sampled initial\nstate (so that the gradients are inherently stochastic).\nPerhaps change to \"may provide\".\n\n\"However, despite the availability of differentiable simulators, it\nhas not yet been convincingly demonstrated that they can effectively\naccelerate policy learning in complex high-dementional and\ncontact-rich tasks, such as some traditional RL benchmarks.\"\nWhile this may be the case for differentiable simulators, there are\nseveral model-based RL works that showed effective learning (e.g.,\nDreamer. The difference with a simulator is just that\nthe model does not have to be learned. I think the claim here is\ndownplaying such previous contributions. Also, \"dementional\" should be\n\"dimensional\".\n\n\"There are several reasons for this: 1.(), 2.(), 3.()\"\nThese reasons are stated as facts, while they are speculations.\nPerhaps, \"possible reasons\" would be better. At least points 1 and 3\nshould be the same for model-free RL, so are they really the reason?\nNo references were provided.\n\n\"Because of these challenges, previous work has been limited...\"\nHow do you know that those were the challenges that limited the\napplicability of the previous methods? The publications themselves\ndo not seem to note your reasons as the reason why they limited their\nexperiments. For example, the PODS paper says their method overcomes\nthe issues of exploding gradients. \n\n\"In addition, we propose a truncated learning window to shorten the\nbackpropagation path to address problems with vanishing/exploding\ngradients and reduce mem- ory requirements.\"\nThis is known as \"truncated backpropagation\". It is plagiarism to\nclaim that you \"proposed\" this.\n\nIn Equation 1, please provide the definition of the Jacobian's and\ngradients. Usually, the gradient is a row vector, whereas you are\nusing a column vector.\n\n\"This makes the reward function smoother...\"\nWhat do you mean by this? The reward function is the same in all cases.\n\n\"In addition, we apply state normalization as is common in RL\nalgorithms.\"\nPlease explain what \"state normalization\" is.\n\n\"First, the terminal value function absorbs the discontinuity of long\ndynamics horizons and early termination into a smooth function, as\nshown in Figure 2 (Right).\"\nThis explanation is incomplete. If you had no discount factor,\na perfect value model and a deterministic policy, your computed\npolicy gradient would be exactly the same as that of BPTT.\n\n\"Finally, the use of short horizons allows us to update the actor more\nfrequently, which, when combined with parallel differentiable\nsimulation, results in a significant speed up of training time.\"\nDo you have an ablation study showing that it speeds up?\n\n\"In contrast, our method scales well due to direct access to the true\ngradients from differentiable simulation.\"\nYou don't have access to true gradients when you are using a\nstochastic policy. It may be better to explain this by referring to\nthe fact that reparamterization gradients are often more accurate\nwhen computing gradients of smooth functions.\n",
            "summary_of_the_review": "The main convincing result from the paper are the empirical results showing that using a differentiable model speeds up learning by a large margin in terms of step count on challenging tasks. I have not seen such results using differentiable simulators before, so I think it warrants publishing. The work itself seems primarily an engineering contribution: the methodology and ideas are not particularly novel, but executed in a sensible way. The quality of the work was generally good. Some more related work could be discussed, and the writing could be improved to be more precise.\n\n______________________________________________________________\n**Update**\n\nI have increased my score to 8, novelty & significance score to 3, correctness to 4, and confidence to 4.\n\nThe authors have done a good job of addressing the reviewer concerns, and have made several changes to the manuscript, including adding more references; running more ablation experiments, e.g. experiments with a deterministic policy; added 2 new environments, etc.\n\nI also note that their approach by differentiating through the trajectory and adding a terminal value function is not exactly the same as previous methods, such as Dreamer or MAAC. These previous works sample start states from a replay buffer of real data, and perform short rollouts from these states. The current work, on the other hand, operates on-policy, and it samples the start states from the end of the previous short rollout (this process is repeated until the total length of the short rollouts exceeds the episode length, and then the episode is restarted).  The policy is updated after each rollout, resembling a more typical truncated backpropagation procedure, but while adding a terminal value function. I think such an approach is tailored to using a simulator, because with a model-based approach, the predictions may diverge from the real trajectory when the task horizon is too long. On the other hand, if using a replay buffer based approach, such as is used in Dreamer or MAAC, the simulations to generate the replay buffer may be a waste of computational resources, so the approach presented in this paper seems like a natural idea to use in the setting when one has a simulator, and is not a direct copy of the previous works. The authors have also added a discussion of previous model-based RL methods compared to their work into Appendix A.4.6.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}