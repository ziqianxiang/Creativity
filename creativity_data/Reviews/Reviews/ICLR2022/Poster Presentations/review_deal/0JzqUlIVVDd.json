{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper derives a generalization bound on target loss based on training loss and reverse KL divergence between source and target representation distributions. Then, proposes an algorithm for DA using inverse KL on representations. they show that inverse KL term can be estimated efficiently without the need for additional networks and minimax objective. The experiments show the efficiency of the proposed algorithm in terms of improving target accuracy. The paper touches an important problem and the proposed idea is simple and effective. \n\nThere were several concerns regarding the paper that were addressed during rebuttal period, such as strength of assumptions, experiments with different values of beta, experiments on office31 dataset, novelty of theoretical results, significance of derived bounds and comparison to [3]. The remaining concern is on comparing the proposed method to recent work in domain adaptation.\n\nI ask the authors to add the following to the camera ready (1) visualizations they have promised that depicts their method leading to better alignment and (2) add the points raised in defending the novelty of theoretical results."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a generalization bound and a reverse KL term to match the marginal distribution.",
            "main_review": "Pros:\n\n1. Marginal distribution alignment is an importance backbone for domain adaptation while the popular adversarial one requires minimax optimization and can be unstable. This paper proposes an reverse KL which can be optimized together with the source classification loss directly. \n\n2. This paper points out a deficiency of Ben David's bound on domain adaption: it fails to consider the problistic labeling mechanism. When the true labeling function is non-deterministic, the bound by David may be inaccurate. \n\n\nCons:\n\n1. My major concern is that : the implementation is inconsistent with the derived generalization bound.  I highly appreciate authors' honesty in writing down that they also use the forward KL term as additional penalty, but when $\\beta_{aux}$ is large, the objective becomes another one. For instance, in the digits, $\\beta=0.3, \\beta_{aux}=0.1$, the forward KL term also contributes a lot to the final objective. A very important point is that I didn't find the ablation studies about this term. It would be better if authors could present baseline method (e.g., $\\lambda_{aux}=0$) results and show the reverse KL is effective. \n\n2. It can be too strong to assume that $I(z,y)=I(x,y)$ when z has much lower dimension compared to a high dimension input $x$.\n\nSide question:\n\nThis paper computes the density by the Gaussian density function and then compute the KL term. I'm wondering how close are the learned representations to the multivariate gaussian distributions, especially for the high-dimensional images, i.e., the Visda datasets. \n\nUpdate:\n\nI read other reviews and the author reponses, I'm good with authors' reply, especially the KL term experiments. So I raise my score to 6.",
            "summary_of_the_review": "This paper proposes an reverse KL term to match the marginal distributions in domain adaptation. The results are promising. The main concern is the implementation is inconsistent with the new generalization bound.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": " In this paper, the authors propose a generalization bound for domain adaptation where discrepancy between distributions (i.e. marginals and class conditionals) is estimated using the reverse KL divergence. Inspired by this result, the authors then proposed a learning algorithm and show  that this outperforms some existing methods.",
            "main_review": "**Strengths.**\n\n- The idea of a probabilitisc interpretation of DA is interesting.\n- The resulting algorithm seems  simple to implement in practice.\n\n**Weakness:**\n\n- **Major Concern #1** Novelty of the theoretical result.\n\nThe paper mentioned several times (e.g. first paragraph of related work) that existing theory of domain adaptation e.g. [1] is limited to the case of binary labels. While the results shown  in [1] are indeed for the binary classification setting,  I do not necesarily agree this  limitation exists, in [1] this was done for simplicity as it is typical, the extension to the multiclass setting just follows. Moreover, the multi-class setting using a novel discrepancy measure that extends [1] was analyzed in [2].  In [3] the authors  further extend the results from [1] to general f-divergences and also derived rigorous generalization bounds. The family of f-divergences include KL and r KL as particular cases.  A lot of work has also be done since [1] (2010) for example [1,2,3,4] and comparison, improvement wrt to them is not discussed .\n\nMoreover, if the goal is to show a bound where the discrepancy between marginals is estimated using the KL divergence rather than L1 (as in [1])  that could be  obtained by directly applying Pinsker's inequality on top of [1] Theorem 1.  How would that compare to the current setting?\n\n- **Major Concern #2** Significance of the derived bound.\n\nOne of the major motivation for the H-divergence introduced in [1] vs L1 and then several authors using hypot-based divergences such as [2,3,4]. It is that (quoting [1]) \"L1 is an overly strict measure that unnecessarily inflates the bound\". Pinsker's inequality shows KL  upper bounds  L1. Wouldnt the direct use of the KL then inflates more the bound? .  How does writing a generalization bound using the KL divergence which is further ubounded solves this problem? \n \n\n- **Major Concern #3** Comparison of the algorithm with modern baselines. The latest baseline WD is from 2018. Why there is no comparison to recent work in domain-adaptation? I would also recommend the authors to do experiments in the office-31 dataset as it has become standard practice. This will also make the comparison vs existing work easier.\n\n- **Major Concern #4** No experimental analysis is provided.\n\n[1] A theory of learning from different domains. Springer 2010\n\n[2] Bridging theory and algorithm for domain adaptation. ICML 2019\n\n[3] f-Domain Adversarial Learning: Theory and Algorithms. ICML 2021\n\n[4] On learning invariant representations for domain adaptation,ICML 2020 \n\n[5] Office-31 Dataset: [https://www.cc.gatech.edu/~judy/domainadapt/](https://www.cc.gatech.edu/~judy/domainadapt/)",
            "summary_of_the_review": "Overall, I consider this work to have some interesting ideas however I consider it needs a major rewriting and clarity on the contribution of the theoretical results. In my opinion, they look more as a way to motivate an algorithm which could be done following the results from either [1,2,3,4] rather than a new generalization bound as claimed. The writing should also be more careful when comparing and criticizing previous work. I also believe comparison to recent DA algorithms/methods is need it, Similarly, more experimental analysis in order to validate the effectivity of the proposed algorithm is also need it.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a domain adaptation method that is guided by the reverse Kullback-Leibler (KL) divergence between the target and source domains in the representation space. To this end, it is assumed that the representation z is sufficient and that p_S(y|x) = E_p(z|x)[p_S(y|z)].\nThe authors derive a bound on the test loss based on the training loss and a quantity based on the divergence of the joint distribution (y,z) between the target and source domain (Proposition 1). Since this bound requires labels from both domains, Proposition 2 helps to use the divergence computation on p_S(y|x) instead of p_S(y|z). The proposed method and optimization procedure are evaluated on multiple data sets and compared with multiple baselines.\n",
            "main_review": "- The paper is well written and easy to follow. The problem is well-motivated, and the derived bound and optimization procedure seem to be reasonable.\n- What I don't see convincing is the jump and oversimplification in Eq(15) where the last line replaces the expectation with the simple sum assuming that each of p_S(z) and p_T(z) to be uniformly distributed. This issue is neither mentioned nor discussed.\n- Similar to the previous comment, Eq(16) assumes that $p_S(x)$ and p_T(x) to be uniform!\n- I am wondering why the authors show selectivity when it comes to the results on the digits data sets. Why don't you show the different combinations? U-->S, S-->U, M U-->S.\n-Finally, the results in Table 1 show a clear failure of all the baselines on M75 and M60 *(achieving half of what KL achieves). I find it difficult to accept these results.\n",
            "summary_of_the_review": "All-in-all, I find the paper of a great potential contribution to the field. I only have two issues (i) the sudden strange simplifications in equations 15 and 16, and (ii) selective experiments besides the weird results in Table 1.\n\nUpdate:\n\nI read the other reviews and the authors' responses, I'm fine with the authors' reply. So I am keeping my score.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "this paper proposed a method for domain adaption.\nThe idea and method are quite simple. Especially, KL divergence on representation alignment is used. And the experimental result looks promising on the selected task.",
            "main_review": "Strength:\n1. the idea is simple and the paper is easy to follow.\n2. the experimental result looks good.\n\nweakness:\n1. My most concerning part is the change from equation (12) to equation (13). Although KL(p_T(y|x) | p_S(y|x)) is fixed over the whole dataset. But in training, we sample a small mini-batch for training, and this term could be changed and unstable.\n2. Experiments are weak since all datasets except VisDA-17 are not popular domain adaptation benchmarks. Why not perform experiments on Office-31 and Office-Home datasets? Also, the results in this paper on VisDA-18 are not consistent with existing work such as [Zhu'20]. In addition, [Zhu'20] is based on marginal MMD and you need to compare it.\n3. the novelty. I am not familiar with domain adaption, but the proposed idea in this paper is very simple and straightforward. So I actually don’t believe there are no previous works that also tried KL divergence for representation alignment (for example, I searched “KL divergence domain adaption” and got many results).  So what is the new in this paper? The KL divergence over probabilistic representation? \n\n[Zhu'20] Zhu Y, Zhuang F, Wang J, et al. Deep subdomain adaptation network for image classification[J]. IEEE transactions on neural networks and learning systems, 2020, 32(4): 1713-1722.",
            "summary_of_the_review": "The paper is easy to follow, and the idea is easy and effective. However, some assumptions seem too strong and some datasets are missed in the experiment part.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}