{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a novel benchmark for neural architecture search methods, which consists of 25 different combinations of search spaces and datasets. The main motivation is that existing NAS benchmarks, such as NAS-Bench-201, consider very small search space and few datasets, such that conclusions drawn with them do not generalize to unseen settings with different search spaces and datasets. The authors first describe the 25 different combinations of the search space and tasks for the given benchmark, and then conduct an extensive empirical study of existing NAS methods and performance predictors with the proposed benchmark, to show that architectures and hyperparameters found with the popular benchmarks do not generalize to other settings, which is consistent with their assumption.\n\n—\n\nAll reviewers were initially positive about the paper, and remained positive throughout the discussion period. The reviewers found the paper well-motivated, and the proposed benchmark useful, as they agree with the need of introducing a single, unified framework that can validate a NAS method under diverse settings, since existing benchmarks only consider specific datasets and search spaces. However, the reviewers were also concerned with the weak technical novelty (Reviewer 2xvD), and that the work lacks deeper insights that could guide the community towards better methods (Reviewer Gku7). \n\nI also agree with the authors and the reviewers on the necessity of having a unified benchmark that incorporates all different settings considered in the previous benchmarks, and find the extensive empirical study of existing NAS methods useful. \n\nHowever, I find the work as rather technically weak as mentioned by R2xvD, since the authors spent too much time describing and showing the limitations of existing benchmark methods, while what is more important for benchmarks, is to justify how the proposed benchmark can evaluate the performance of different methods in a fair manner, while being representative of the practical settings. In short, the authors need to justify their design choices. Yet, the 25 settings proposed in the paper seem to have been arbitrarily chosen, and it is not clear if having a good performance on this benchmark is indeed a fair evaluation, or well-reflects how the NAS method will perform in practice. The proposed benchmark also does not really consider a novel search space or setting that have been overlooked in the past either, and does not provide much insights on the problem, as mentioned by Reviewer Gku7. \n\nThus, although I recommend an acceptance for its practical value acknowledged by the reviewers, the authors need to put a considerable amount of effort in revising the paper, and If this were a journal submission, the paper may need to undergo a major revision. Most importantly, as described, the authors should justify their design choices as well as whether evaluating a model on the benchmark yields “fair” and “representative” results, focusing more on describing the proposed benchmark itself."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper present a wrapper for 25 NAS benchmarks and provide insightful analysis on NAS and predictor performances across the benchmarks. They show, among other things, that no NAS method is best across all tasks, that hyperparameters of NAS are not robust and that small tabular benchmarks are not representative of NAS method performances on larger benchmarks. ",
            "main_review": "Strengths:\n- Very important topic. Benchmarks are critical to assess performance of NAS algorithms.\n- Provide statistics across a wide array of NAS benchmarks, such as the distribution of accuracy across the search space.\n- They show that no NAS method clearly outperform all other method across all benchmarks. Some do better with HPO, but this is typically to expensive to computer for each benchmark.\n- Tests 2 relevant questions. 1) Whether NAS results generalizes from the small 101 and 202 benchmarks to larger ones and 2) NAS have robust hyperparameters. The third question seems to be to closely related to the second one: 3) We can cheaply optimize hyperparameters of NAS on tabular benchmarks and reuse them on another benchmark.. \n    - They show that 1) NAS algorithms do not generalize well from small 101 and 202 benchmarks to larger ones,\n    - 2) Hyperparameters are not robust for NAS algorithms.\n    - 3) Is not true obviously since hyperparameters of NAS algorithms are not robust.\n- They contribute a valuable wrapper for 25 benchmarks, although it is not clear to me how the search space representation with NetworkX allows for a dynamic search space definition. \n\nWeaknesses:\n- Out of the 3 questions, it seems to me the third one is to closely related to the first one. And the experimental answer to this question is not clearly outlined in the paper neither.\n- I am quite concerned about the maintainability of the library. The benchmarks are bash scripts that will be difficult to maintain. Some even contain specific slurm configurations for the cluster of the researchers. \n\nRandom walk plot is interesting. I would be great to have a confidence interval or at least a standard deviation for each curve.",
            "summary_of_the_review": "The wrapper of benchmark is a valuable contribution by itself, but this paper illustrates well the benefit of such benchmarks by providing insightful analysis of NAS algorithms. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I do not see any ethical concerns.",
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors collected most of the existing NAS benchmarks to construct a new benchmark.\nA unified API is provided to use these existing search spaces and architecture datasets. Based on this the authors re-analyze some NAS algorithms on this new large and comprehensive benchmark and have some interesting observations.",
            "main_review": "strengths:\n- The paper is well motivated and written.\n- This new benchmark can be used to do a more deep analysis for the NAS methods.\n- Some interesting observations.\n\nweaknesses:\n- The technical part is somewhat weak.\n- The table and figure captions are too short to explain themselves. Please at least explain the abbreviations used in the table or figure.",
            "summary_of_the_review": "The authors did much engineering efforts and emprical analysis, whereas the technical novelty is a little bit weak.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigates the drawbacks of current existing NAS benchmarks and the works that evaluate on them. Then it presents NAS-Bench-Suite, an extensible collection of NAS bench-marks which is easy-to-use to facilitate reproducible, generalizable and rapid NAS research.",
            "main_review": "There exist many NAS benchmarks currently across diverse tasks. Many NAS works conduct experiments on several benchmarks to demonstrate the effectiveness and believe. \nHowever, researcher find that NAS algorithms may perform differently across these benchmarks. Sometimes even reverse conclusions are drawn.\n\nThis paper investigates this problem and points out that the differences may come from the different search space design, training pipeline and hyperparameters, via comprehensive experiments across many existing benchmarks. It finds that conclusions from a few benchmarks do not generalize well to other benchmarks\n\nFurther, the paper proposes NAS-Bench-Suite, a collection of NAS benchmarks, with a unified interface, which is easy-to-use, to facilitate reproducible, generalizable and rapid NAS research.\n\nThe motivation of the work is clear. The experimental analysis is convincing. The paper is well organized. The claim is sufficiently addressed by the method.\n\nOverall, this is a good work for the NAS community.",
            "summary_of_the_review": "I think the paper is of good quality and will contribute to the NAS community.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a unified interface for access to a collection of Neural Architecture Search (NAS) benchmarks. With experiments performed on across multiple search spaces and datasets, the authors show that some conclusions drawn from a small subset of benchmarks do not generalize across diverse datasets and tasks.",
            "main_review": "The results are interesting and very useful for the community as it is - and the authors should be commended for their effort.\nHowever the work lacks deeper insights from the experiments performed that could guide the community towards better methods. For example: [Fig.2] Are there identifiable components of architectures that lead to the interquartile range differences between the CIFAR10 benchmarks (NB101, NB201, DARTS)? Or [Table 2] What makes XGBoost generalize well where others don't?\n\nSec.4.2: Transfer is always expected to lead to a performance drop. Is there evidence that NAS is especially bad in this respect, compared to HP transfer in other settings? (say, transferring ResNets HPs from one dataset to another)\n\nFig.5: Since the figure contains a lot of information, could the authors provide some useful summary statistics in addition?\n\nMinor: clarify whether $i$ and $j$ are columns or rows in Fig.5. ",
            "summary_of_the_review": "In my view the two main contributions of this work are (1) showcasing the limitations of drawing conclusions from single benchmarks; (2) providing the community with a larger analysis tool.\n\nI expect that the community will welcome these resources and thereby **recommend the paper is accepted** (provisional score: 8)",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}