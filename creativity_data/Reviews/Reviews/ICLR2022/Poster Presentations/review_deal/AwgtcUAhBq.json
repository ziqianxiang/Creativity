{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "Summary (from reviewer uzT5): This paper analyzes adversarial domain learning (DAL) from a game-theoretical perspective, where the optimal condition is defined as obtaining the local Nash equilibrium. From this view, the authors show that the standard optimization method in DAL can violate the asymptotic guarantees of the gradient-play dynamics, thus requiring careful tuning and small learning rates. Based on these analyses, this paper proposed to replace the existing optimization method with higher-order ordinary differential equation solvers. Both theoretical and experimental results show that the latter ODE method is more stable and allows for higher learning rates, leading to noticeable improvements in transfer performance and the number of training iterations.\n\nAll reviewers appreciated the contributions of this paper and recommended acceptance. While the methods themselves are not novel, the game perspective applied to this problem appears to be and the use of higher-order solves yield interesting theoretical and empirical improvements.\n\n== Additional comments ==\n\n1) For the comparison vs. game optimization algorithms (Figure 3), it would be nice to normalize the x-axis so that one \"epoch\" yields comparable computational cost among the different methods (as RK4 and RK2 is much more expensive than EG or GD per mini-batch). Given that EG had such bad performance there, it would not change the conclusions; but the current scaling is still quite misleading. Same comments for Figure 2.\n\n2) Note that modern approaches for stochastic extragradient recommend to use different step-sizes for the extrapolation step and the update step (see e.g. Hsieh et al. NeurIPS 2020 \"Explore Aggressively, Update Conservatively: Stochastic Extragradient Methods with Variable Stepsize Scaling\") I suspect that much bigger step-sizes could be used in this case while maintaining convergence, and this version should be added to Figure 3.\n\n3) In \"Related Work | Two-Player Zero-Sum Games\" -> note that Gidel et al. 2019a provided all their convergence theory and methods for stochastic variational inequalities and thus it also applies to three-player games, unlike seems to be implied by this paragraph. In particular, all the algorithms they investigated (Extra-Adam amongst others) could also be applied to DAL. While I can see that the specifics of the objective in DAL might be different than for GAN optimization, it would be worthwhile to acknowledge these alternative approaches more clearly, and I encourage the DAL community to investigate their performance more exhaustively for DAL than what was done in this paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The setting in the paper is the classic unsupervised domain adaptation problem, where we are given a labeled sample from a source distribution and an unlabeled sample from a target distribution. The goal is to minimize the risk on the target distribution. Theoretical results led to a breakthrough in practice - the Domain Adversarial Learning architecture (Ganin et al., 2016). \n\nThe paper suggests looking at the paper from a game theory perspective. This is natural, as the objective is to minimize the loss on the source distribution while maximizing the distinction between the distributions. \nThe optimal solutions of the game are characterized by the local NE.\n\nMotivated by the results from game theory, the authors suggest replacing Gradient Descent (due to its limitation in this optimization problem) with other optimizers - ODE (ordinary differential equation) solvers.",
            "main_review": "Strengths :\nThe game-theoretical formulation is natural for this important problem.\n\nThe experiments seem to support the asymptotic convergence guarantees of the optimizer.\n\nThe empirical results look somewhat significant.\n\nThe practical significance of this work reveals that using high-order solvers instead of Gradient Descent (GD) in this setting leads to better results.\n\nWeakness:\nThe technical novelty is only marginally novel and quite straightforward.\n\n\n",
            "summary_of_the_review": "My vote is -  marginally above the acceptance threshold due to the aforementioned reasons.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors exhibit a strong link between game theory and domain-adversarial training. They show the optimal point in the latter is a Nash equilibrium of a three players game. From this perspective, the authors show that standard approaches, like gradient descent, cannot work in this setting as the method is known to be divergent in such a case. Instead, they propose to use Runge-Kutta methods (for example) to discretize the ODE, which gives insights for novel algorithms with better convergence guarantees.",
            "main_review": "(I am not a specialist in domain adversarial learning. Therefore I am not entirely sure about the novelty and impact of the work in this field).\n\nThe paper is very well written. The introduction of both fields of domain adversarial learning (sec 2) and game theory (sec 3) is done properly, with an adequate study of the related work. Moreover, the authors did a good job convincing about the complexity of the three-players game setting.\n\nFrom this perspective, using known tools from game theory / variationally inequalities / numerical analysis for ODE discretization, the authors show necessary and sufficient conditions for the existence of local Nash equilibrium (prop. 1 and 2), showed that a modified dynamic of the gradient flow (equation (8) ) ensure global convergence of the Euler discretization, and developed an algorithm (RK-2, generalization of extra-gradient, equation (9)).\n\nFinally, they discussed several approaches with solid numerical experiments on various problems.\n\nIn terms of novelty, there are no \"new\" theoretical results, properly speaking. The novelty here is the derivation of the domain adversarial learning into a game, which is not straightforward but simplifies its analysis. Thanks to this new perspective, I believe this could open new doors and research direction in this field. For this reason, and because the paper is particularly well written, I recommend the paper to be accepted.\n\n\n*** Post rebuttal\n\nI have read the authors' answer and decided to keep my score. ",
            "summary_of_the_review": "The authors presented a link between game theory and domain-adversarial training. In similar topic, such as GAN, such link opened new research perspectives and impacted positively the area. I believe this could be also the case here.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper analyzes adversarial domain learning (DAL) from a game-theoretical perspective, where the optimal condition is defined as obtaining the local Nash equilibrium. From this view, the authors show that the standard optimization method in DAL can violate the asymptotic guarantees of the gradient-play dynamics, thus requiring careful tuning and small learning rates. Based on these analyses, this paper proposed to replace the existing optimization method with higher-order ordinary differential equation solvers. Both theoretical and experimental results show that the latter ODE method is more stable and allows for higher learning rates, leading to noticeable improvements in transfer performance and the number of training iterations. ",
            "main_review": "1. The authors claim that \"The gradient field of Equation (2) and the game's vector field (see Section 3.2) are equivalent, making\nthe original interpretation of DAL and our three-player formulation equivalent.\" Why does field equivalence induce formulation equivalence? What is the exact definition of \"equivalence\" here?\n\n2. The layout of the paper needs to be improved. For example, Example  2 should be an independent paragraph in the paper but not a \"window\" embedded in the other section. Some figures and tables have the same issue. \n",
            "summary_of_the_review": "This paper is well-written and easy to follow. The theoretical contribution is solid, and the experimental studies show the superiority of the proposed method on the domain transfer task. As a consequence, I think the quality of this paper is marginally above the acceptance threshold. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This manuscript considers the adversarial domain adaptation training problem, specifically the gradient reversal method, from the perspective of game theory. The authors show that gradient-based optimizers without an upper bound on the learning rate violate asymptotic convergence guarantees to local NEs. The authors further show that these constraints can be lifted by higher order ODE solvers. In the experimental part, the authors evaluate their method i.e. Runge-Kutta ODE solvers of order 2 and 4 with different general and also game optimized gradient-based optimizers on a MNIST/USPS digits dataset. Furthermore, they show hyperparameter robustness of their method and finally, the method is tested on more complex image and NLP datasets and compared to current SOTA methods. Overall better results are achieved.\n",
            "main_review": "Strengths:\n\n* Well written and understandable\n* Theoretical sound in both problem description and solution\n* Analyzing domain adaptation learning from a game perspective and analyzing the stability of the optimizer with the gradient reversal layer model is novel \n* Relevant related work is considered and compared\n* The experimental part is sufficient and supports the theoretical claims\n\nWeaknesses:\n* The theoretical analysis is limited to full batch training while in practice as well in the experiments mini-batches are used. However, the authors mentioned it in the paper.\n* As with gradient-based methods, this method could also converge to a non-Nash equilibrium as outlined in [1], however this is likely a rare case and not unique to this proposal\n\n[1] Mazumdar et al. On Finding Local Nash Equilibria (and Only Local Nash Equilibria) in Zero-Sum Games. https://arxiv.org/abs/1901.00838\n",
            "summary_of_the_review": "The proposed method is theoretical founded and the experimental part supports the claims and the analysis is novel. I vote for accept, good paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}