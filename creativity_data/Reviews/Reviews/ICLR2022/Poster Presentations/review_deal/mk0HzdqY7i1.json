{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "I would like to thank the authors for having managed a thorough discussion despite the complexity of the task at hand (e.g. BEvM). during discussion, the reviewers clearly converged to accepting the paper, praising the importance of the problem tackled and the setup put in place to effectively tackle the challenge at hand.\n\nAll this makes the paper an important contribution and a clear accept (and an enjoyable read), for which I can only recommend a further polish before camera ready to follow the latest inclusions.\n\nAC."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper looks at an important problem: combinatorial optimization that arises in several real-world settings. Next, the paper focuses on a influential paper in the field (Li et al. 2018, 200+ citations so far) that presents a GNN approach and reports impressive performance numbers. Next, the paper presents investigates if these results are reproducible by (1) running the publicly available implementation after fixes; and (2) re-implementing the algorithm as described in the paper and documentation. None of these versions replicates the reported performance. The paper also presents a benchmark suite to make comparison of this task easier.   ",
            "main_review": "The primary strength of the paper is it is opening up an important direction towards scientific accuracy. Citation counts often work as a proxy for the perceived importance of a paper. A paper with 200+ citations in less than 3 years of publication is thus likely to be considered as highly influential in the field. Through re-looking at the claimed results, this paper makes a significant contribution towards a research philosophy that seeks to validate influential papers. \n\nThe benchmark data set is an important contribution and the detailed results presented in Table 2 can immensely benefit future performance comparisons. \n\nThe writing of the paper is excellent and the paper and the literature review is extensive.  \n\nThat said, one potential risk in this paper is what if the current implementation is wrong and indeed the guided tree search is effective? ",
            "summary_of_the_review": "Our field is producing papers at a fast rate. Reviewers can provide scientific checks and balances only up to a certain level. This type of rigorous effort can offer valuable information about highly influential papers to the scientific community. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents an evaluation of deep learning-based tree serch solutions (that are based on graph neural networks) for solving combinatorial optimization problems. They present an open-source benchmark suite for the maximum independent set (MIS) problem (both weighted and unweighted) that includes instances from multiple random graph models, known benchmark suites (e.g., SATLIB) and other graphs from the literature. They conduct experiments on different configurations of neural-guided tree search and show that the results by Li et al. [2018] are not reproducible. They also show that general and tailored classical solvers outperform deep learning solutions.",
            "main_review": "Strengths:\n- I think the paper addresses interesting and important questions. Understanding what work and what does not work, and which parts of a solution actually contribute to the performance, is important.\n- The paper provides interesting insight on the performance and reproducibility of a previous work (Li et al. [2018]) based on thorough experiments.\n- The paper presents a new benchmark suite for MIS that includes a large number of benchmark instances and implementations of several popular approaches.\n\nWeaknesses:\n- All experiments are done on one problem type, MIS, while there is a lot of work on other graph-related problems such as TSP, VRP, etc. For example, Li et al. [2018] that is discussed in this work have considered other problems. It is hard to draw conclusions on combinatorial optimization from one problem.\n- The paper focuses on a single work (Li et al., 2018) that the authors were unable to reproduce and a single work that showed promising results (Ahn et al. [2020]). A study of a larger sample of deep learning solutions would be useful to support claims about the value of GNNs, neural-guided tree search, or reinforcement learning for combinatorial optimization.\n- The result that specialized solvers and even classical solvers are often better than deep learning solutions, especially on larger problems, has been reported for other computational problems (e.g., TSP [1]). Ahn et al. [2020] already reported that KaMIS outperforms Li et al. [2018] and that reduction and local search lead to improvements. Further, other works have looked at generalization of GNNs (e.g., [2]). These need to be cited and the similarities and differences with this work should be discussed.\n- While providing open-source benchmark suite, including implementations of several popular approaches, is important, as far as I understand this suite is primarily a collection of existing problems and randomly-generated graphs and does not introduce new benchmark datasets. I am not sure this is an important contribution of the work.\n\n\n\n[1] Joshi, C. K., Cappart, Q., Rousseau, L. M., Laurent, T., & Bresson, X. (2020). Learning TSP requires rethinking generalization. arXiv preprint arXiv:2006.07054.\n\n[2] Xu, K., Zhang, M., Li, J., Du, S. S., Kawarabayashi, K. I., & Jegelka, S. (2020). How neural networks extrapolate: From feedforward to graph neural networks. arXiv preprint arXiv:2009.11848.\n",
            "summary_of_the_review": "Overall, I think this type of works is important and can lead to important insight. However, I think experiments with more problem types and more solutions are needed to draw general and interesting conclusions. Also, the paper needs to discuss some relevant works that are currently not mentioned.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a software package for generating data and training/evaluation some ML and non-ML approaches for the Maximum Independent Set (MIS) problem in its both its unweighted and weighted versions. It is shown that a highly-cited method that combines supervised learning using a graph neural network (GCN) with a (complete) tree search does not actually need the GCN if the MIS instance is preprocessed appropriately using existing non-ML based codes. On the other hand, a recent deep reinforcement learning (DRL) approach for the same problem is shown to actually use its GCN’s predictions, obtaining better results overall. Last, non-ML solvers such as KaMIS and Gurobi are shown to find better solutions in a short amount of time for most instance datasets, putting into question the potential for ML-based approaches in general for MIS.",
            "main_review": "**Strengths**\n\n- Clarity: The paper is generally very well-written, although the presentation of the results can be improved substantially.\n- Motivation: Li et al.’s paper is being cited and compared against in tens of papers yearly, and so understanding its limitations and apparently fundamental issues is useful for the community.\n- Reproducibility: The authors are very systematic and transparent in how they generate datasets, implement the various methods, and evaluate them. I can see their code becoming widely used and built on in the development of ML-based methods for MIS.\n\n**Weaknesses**\n\n1. Presentation of experimental results: Tables 1 and 2 should be part of the paper’s appendix for sure, but you really need to find better ways of presenting those thousands of statistics in the main text. As things currently stand, the reader needs to zoom-in to read the numbers; there are so many columns that it’s hard to track which one corresponds to which method or what the trends are in terms of best method for a given dataset. Additionally, please label the paragraphs of section 3.1 so that the reader can immediately understand which aspect of the results you’re discussing. As things stand, I’ve had to decipher which columns I should query in the tables to see what you’re saying in the paragraphs of 3.1.\n\n2. Statistical metrics: The average optimality ratios and running times in Tables 1 and 2 are certainly indicative of some trends. However, some box plots of the distributions of the optimality ratios/running times might shed more light into the robustness of the methods. For instance, the running time average may be biased by outliers whereas a box plot factors that in. Combining this suggestion with the one above, you could consider moving the tables to the appendix and replacing them in the main text with two box plots per datasets, one for optimality ratio and another for running time. This way, the reader can visually compare different methods without having to zoom-in and read hundreds of numbers. Since this paper’s contributions are largely software/empirical results, you can also consider performing statistical testing for each pair of methods; see the two-sided Wilcoxon Signed Rank Test for example.\n\n3. Datasets: Please consider additional datasets which may be a bit more standard for MIS papers, e.g.:\n- http://vlsicad.eecs.umich.edu/BK/Slots/cache/www.nlsde.buaa.edu.cn/~kexu/benchmarks/graph-benchmarks.htm\n- http://lcs.ios.ac.cn/~caisw/graphs.html\n\nIn particular, the DIMACS implementation challenge graphs have been used in the KaMIS paper for example, among others. Also, this very recent dataset of large-scale instances may be of interest (even if only to evaluate and not train): https://arxiv.org/abs/2105.12623\n\n4. MIS heuristics and mathematical programming formulations: Please check Butenko’s dissertation (Butenko, Sergiy. Maximum independent set and related problems, with applications. University of Florida, 2003.), particularly chapters 2-3 and the experimental results later on. The binary linear programming formulation you used with Gurobi is not the only one possible; there are quadratic formulations (see eq. (2.3) in Butenko) which may be easier to solve in practice than the linear one. Also please check the famous GRASP heuristic for MIS and consider implementing it: Feo, Thomas A., Mauricio GC Resende, and Stuart H. Smith. \"A greedy randomized adaptive search procedure for maximum independent set.\" Operations Research 42.5 (1994): 860-878.\n\n5. Where do your results leave us? I would’ve expected you to identify datasets (existing or new) for which KaMIS and Gurobi underperform in some respect. Perhaps that is tricky; you’ve tried many datasets (though there are more you could try as mentioned earlier) and the two solvers did very well. You might then want to consider other harder versions of MIS, for example the Generalized Independent Set Problem, see (Colombi, Marco, Renata Mansini, and Martin Savelsbergh. \"The generalized independent set problem: Polyhedral analysis and solution approaches.\" European Journal of Operational Research 260.1 (2017): 41-55.) and (Hosseinian, Seyedmohammadhossein, and Sergiy Butenko. \"Algorithms for the generalized independent set problem based on a quadratic optimization approach.\" Optimization Letters 13.6 (2019): 1211-1222.). In this variant, some edges may be “purchased” and their endpoints may violate the independence requirement. This makes the problem much harder than MIS for integer programming solvers. Your paper should really push the community to advance the field.\n\n6. Solver parameter tuning as a baseline: Please consider adding a baseline in which KaMIS/Gurobi are “trained” on the same datasets as the ML-based methods by tuning their parameters using off-the-shelf tools like SMAC (https://www.automl.org/automated-algorithm-design/algorithm-configuration/smac/). Such a baseline combines the best of both worlds in a sense: the stability and generality of these solvers with the potential benefits of leveraging the instance distribution.\n\nMinor comments:\n\n- page 8, “Overall, we see that … cannot deal with some graphs”, this is not true for Gurobi though, correct? ",
            "summary_of_the_review": "Overall, I like this paper and think it makes a solid contribution to the intersection between deep learning and combinatorial optimization. However, I think a paper that “debunks” a highly-cited work should also establish convincing avenues for future research which are currently beyond the scope of (ML or non-ML) existing methods; I argue that this is missing at this stage. I would like to see the authors’ responses to my questions and concerns before making a final decision, but am generally positive about this submission.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper conducts a thorough experimental evaluation of a line of work on \"deep-learning guided tree search in combinatorial optimization\". This line of work iterates on a relatively generic greedy-style algorithm for NP-Hard graph optimization problems; with a neural network -- trained on a training set of instances -- guiding the greedy decisions. The popularity of this approach is anchored in the premise of \"deep learning will help us solve NP-Hard problems.\n\nFindings of the experimental evaluation are the following:\n1) Some earlier work is not reproducible\n2) Traditional dedicated solvers are comparable or better; particularly on harder/larger problem instances\n3) Performance of data-driven methods barely changes when outputs of trained neural nets are replaced by random values (i.e. all the algorithmic power comes from explicit and traditional heuristical components)",
            "main_review": "Strengths:\n\nThe empirical results of the paper have the potential to redirect (some) research in \"ML for combinatorial optimization\" to more fruitful directions in which it is indeed the learned components playing a vital role. I think this is fundamentally valuable.\n\nWeaknesses: \n\nI have multiple concerns regarding the presentation and methodology which I will list below. \n\n* Overall presentation\n\nFirst of all, Table 1 contains about 700 numbers none of which are legible on printed paper due to tiny font size (is it 3?). Given how strictly ML conferences force authors to respect font sizes, margins, and appropriate whitespace, I would almost think such a table merits a desk rejection. More importantly, even after zooming in, it is extremely hard for readers to navigate and draw meaningful conclusions from it. I suggest to a) decide on a subset of the most informative datasets (some are clearly too easy for all methods) and focus on that -- the rest can be in the appendix; b) similarly decide on the key subset of columns; surely the point isn't to compare all the heuristical components; c) This should win enough space to illustrate each of the main conclusions in a plot/table of its own (while displaying precisely the relevant information). d) Introducing a visual distinction between learning-based and classical method would be helpful.\n\nThe write-up assumes a lot of familiarity with tree-search methods and as a result, is uninviting even (!) to researchers from the wider MLxCombOpt community. I suggest the authors consider the following suggestions:\n\na) explaining tree-search basics in the main text\n\nb) explaining (some of) the heuristics in the main text -- it is important for the analysis of the results anyway.\n\nc) restructuring the related work (and introduction?) to categorize different ways ML is applied to NP-Hard combinatorial problems and explaining the place of \"tree-search\" in it (definitely add a discussion of learning to branch-and-bound as well as some details of (Nair 2021)). Categorizing by the degree of interaction between learned and algorithmic components is an option to consider. \n\nWith all of this in place, the paper would put itself in a position to frame its claims in much wider relevance (which I believe the claims deserve).\n\n* Confusion about the promise of DL for comb. opt.\n\nThe second paragraph of the introduction claims that learning-based approaches give a chance to learn to solve a *specific problem* (unlike Gurobi that doesn't make a distinction). I find this inaccurate. For one, there are obviously dedicated solvers to concrete problems; this feature isn't specific to DL. But mainly, the promise of DL, as I see it, is to learn solvers specialized to a *family of instances*. This view is well-motivated by industrial reality (e.g. Amazon's routing instances are almost the same every day) and it appears in the literature (see for example the introduction to (Khalil, 2017)).\n\nThe authors should be more explicit about evaluating the methods in a \"harder\" setup where the learned components are expected to generalize *across* families of instances (it seemed that training happens only on SATLIB). Evaluating the more favorable setting might also be interesting.\n\n* Treatment of Gurobi as a baseline\n\nThis paper has a unique chance to highlight a common issue with Gurobi comparisons. It has a lot of internal parameters that can be tuned to specific problems -- and in fact, they should, if other baselines are allowed to do it. Other than establishing the practice, it could further strengthen the points made by the authors.\n\n* Clarity about GPU utilization\n\nI assume that Gurobi and KaMIS do not utilize GPU whereas other methods do? Does rand use GPU? It seems it wouldn't need to. Given the massive computational advantage that 8 V100s give, the runtime comparisons should be very clear about it. Again, this clarity should go in favor of the main message: e.g. \"even with such computational advantage tree-search doesn't outperform traditional solvers\" or \"since tree search is comparable to rand; it is basically just a powerful GPU-friendly heuristical algorithm that is independent of any machine learning\".",
            "summary_of_the_review": "Despite the paper being purely experimental, its main point can have significant net-positive impact within the wider research area. Not only by casting doubt on the entire \"DL based tree-search\" but also by serving as a long-term warning -- claims about \"outperforming SOTA of classical combinatorial optimization\" can fall apart under proper experimental methodology.\n\n For this reason, I am comfortable disregarding the usual demands for technical novelty, beating benchmarks, or providing theory.\n\nHowever, in its current form, I do not believe the paper would realize its potential impact due to the issues listed above. I believe they require significant changes in the paper structure so I cannot, at this point, recommend acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}