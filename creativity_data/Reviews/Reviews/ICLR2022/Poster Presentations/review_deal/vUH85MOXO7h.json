{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper investigates the neural tangent kernel NTK of infinitely wide ensembles of soft trees having a particular \nsoft decision functions in their internal nodes. A closed form of the NTK is presented as well as a result bounding\nthe changes of the NTK during training. Implications for practical training procedures are briefly discussed and \nsome experiments are also reported. \n\nThe review and discussion phase were difficult with two rather uninformative but positive reviews and a negative \nbut detailed review. The latter had, however, some serious flaws. For these reasons I carefully read the paper on my own,\ntoo. In turned out that the criticized flaws in the presentation mentioned by the negative reviewer are baseless as long as \none already knows what an NTK is. Given the title of the paper and the history of NTKs, I personally think that \nsuch knowledge can and should be assumed. \nOverall, I find the paper be actually very well written. The main issue I see is that the results are not overwhelmingly \nsurprising. Nonetheless, this is a solid contribution, which deserves to be published."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents an extension of a neural network analytic tool\ncalled Neural Tangent Kernels to its use for decision trees and\nforests.  The focus is a previously proposed notion of soft trees.\nA few analytical results are presented about the properties of the\nextended notion, called Tree Neural Tangent Kernel, such as the\nexistence of some limiting deterministic form, how alternative\nprobabilistic tree ensembles would have their kernels converging to\nthis deterministic form, that this limiting kernel is positive\ndefinite, and that the kernel remains stable during training,\nequivalence of the kernels for one-level tree ensembles to those of two-layer perceptrons, etc.\nSome support for the analytical claims is provided by simulation experiments.\n",
            "main_review": "The work attempts to generalize the recently proposed Neural Tangent\nKernel method to trees and forests, and attempts to use that to\nanalyze certain properties and training behavior of a special type of\ndecision trees (the so-called soft trees) and their infinite ensembles.\nThis could be of interest to followers of the Neural Tangent\nKernel method and its use in analyzing neural-network-like tree\nstructures.\n\nFor the rest of the audience, the paper has not made an attempt to\nclearly lay out all the basics and the logic.  It relies heavily on\nexternal references for many critical concepts and results.\n\nFor a start, the notion of a \"soft tree\" involves some mixing of the\nfollowing properties of trees:\n(1) whether the splitting function is fixed in one shot (e.g. by\ninformation theoretic choices) or adjustable and trainable (e.g. by\ngradient descent) during learning, \n(2) during or after training, whether the split is deterministically\napplied to every input, and\n(3) whether the splitting function parameters are frozen by training or\ncontinue to be samples from some probabilistic distribution that has\nparameters estimated during training.\nThese concepts are never clarified in the paper, so when additional\nnotions like ensembles and infinitely large ensembles are brought in,\nthe confusion just grows.\n\nOne may wonder how the chosen definition of soft trees is related to\nthose of probabilistic decision trees [1] and neural trees [2].\n\n[1] Michael I. Jordan,\n A Statistical Approach to Decision Tree Modeling,\n Machine Learning Proceedings 1994, Morgan Kaufmann, 1994, 363-370,\n https://www.sciencedirect.com/science/article/pii/B9781558603356500519\n\n[2] J A Sirat & J-P Nadal,\n Neural trees: A New Tool for Classification,\n Network: Computation in Neural Systems, 1:4, 1990, 423-438,\n https://www.tandfonline.com/doi/abs/10.1088/0954-898X_1_4_003\n\nGiven that the work builds on a key concept in prior art, for the\npaper to be self-contained, the notion of \"Neural Tangent Kernel\"\nshould first be explained (why it is called a Kernel, why Tangent, why\nNeural, what is its significance, what new insight it has contributed,\n...).   A basic understanding of this is important for the proposed\nextension to have a sound footing. It should also be stated upfront\nwhat is expected of the proposed extension.   What insight contributed\nby the NTK do you expect to reproduce for tree ensembles?  What new\ninsight do you want it to bring on?\n\nMany other improvements are needed in the presentation of the logic,\nespecially on the sudden appearance of certain claims and arguments\naround or against them.  For example, when it is declared that trees\nof depth larger than 1 have some distinctive feature from MLPs of any\nnumber of layers, what assumption is being argued against?  Did\nsomeone conjecture that they are the same, and why was there such a conjecture?\n\nThe paper is not readable due to its lack of clarity in the basic\ndefinitions, a severe lack of connections in the logic, heavy\nreliance on external references, and a lack of explanation of the\nsignificance of the results. \n",
            "summary_of_the_review": "The paper may have some new results but is better fitting for a forum for theoretical analysis and\npreferably a journal where more space is available to fully elaborate on\nthe detailed definitions, assumptions, claims and their implications.\n",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "A version of the paper has been posted at https://arxiv.org/pdf/2109.04983.pdf  so it is no longer a blind submission.\nThe conference reaffirms that this is acceptable.\n",
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed the Tree Neural Tangent Kernel (TNTK) for tree ensembles. The proposed idea extends the NTK concept to tree ensemble models and enables ensembles of infinite soft trees. This paper provides theoretical studies to analyze the properties of the proposed TNTK. They also provide comprehensive experimental results to show the effectiveness of the proposed method. ",
            "main_review": "This paper proposed a new method called TNTK to ensemble infinite soft trees. It provides non-trivial theoretical studies of the TNTK. Their numerical experiments support their theoretical results. This paper is well-written and clearly organized. \n\nCompared with MLP-induced NTK and RBF, one of the advantages of TNTK will be the low computational complexity. It would be better to provide a detailed complexity analysis (e.g., memory cost, training time complexity, and inference time complexity) of the proposed TNTK. \n",
            "summary_of_the_review": "Please refer to my main review.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors derive a neural tangent kernel for soft trees, and prove several properties of the kernel. These include the stability of the kernel in the large ensemble limit, applicability to oblivious tree ensembles, degeneracy with large tree depth, and comparison to NTK. They then evaluate the utility of the kernel on 90 UCI classification data sets using a kernel regression classifier. ",
            "main_review": "The authors define the TNTK and prove a number of properties which one would like to know about the kernel. \nSuch as positive definiteness, and change during training.\nThe kernel shares some of the properties of the NTK, indicating that the TNTK can be used to understand training behavior. This is not surprising, considering that soft trees can be viewed as a neural network. This work offers some novelty in computing the kernel in the limit of infinite size of the ensemble, whereas naive application of NTK to large ensembles would be computationally intractible. \n\nThe depth degeneracy property is a result with practical utility. The TNTK framework offers a clear understanding of this phenomenon and can help select model hyperparameters.\n\nIt is often unclear how to initialize soft-tree ensembles. Many works use classical greedy algorithms to initialize, while others fix the structure of the tree and randomly initialize the weights. It would be nice if the authors can remark whether TNTK can shed some light on initialization strategies.",
            "summary_of_the_review": "This paper represents the development of an important tool for understanding tree ensembles. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}