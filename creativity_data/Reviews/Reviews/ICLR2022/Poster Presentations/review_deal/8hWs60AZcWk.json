{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "Summary: Authors present an approach to improve the robustness of vision transformers by mapping standard tokens into discrete tokens that are invariant to small perturbations. Method is applied to a variety of backbone architectures and evaluated on a range of out of distribution forms of ImageNet test set. Significant performance gains are measured across many of these tasks.\n\nPros:\n- Novel, simple, effective approach\n- General approach applicable across model variants, complimentary to other methods to improve robustness.\n- Comprehensive study, evaluated on many ImageNet robustness benchmarks\n- Well written overall\n\nCons:\n- Biggest issue: 3 reviewers point out concerns about validity of claims that ViT architecture is more reliant on local patterns and less on global context. This seems mostly a semantic issue around conjectures about why the method works – it does not invalidate the value of the new approach or its solid results.  Authors have responded to reviewer concerns by changing wording in paper to relax the claims, specifying “shape information” rather than “global information”.  They have also added experiments to measure shape bias, as defined in prior art, to backup these claims. \n- Paper missing baselines of data augmentation strategies. Authors have responded by including such comparative experiments. \n- Paper is missing ablation studies on changing the type of codebook. Authors have responded by including multiple variations of codebooks, and varying the codebook size.\n\nThis paper was a close call based on the reviews. However, in AC opinion, the critiques have been adequately addressed by the authors. This is confirmed by adding an extra expert reviewer to the pool, who agreed with some earlier critiques, and was satisfied with the changes and additional experiments presented by the authors. AC recommendation is to accept."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors have an observation that ViTs trained on ImageNet heavily depend on local features, but fail to use the global features (shape or structure). To address this issue and improve the robustness of ViTs, the authors proposed to replace the linear embedding layer by the a vector-quantized encoder. The authors claims that it can push the ViTs to learn the global information by this replacement. Experiments are conducted on ImageNet and other ImageNet variant datasets. Results show that the proposed method can improve ViTs' robustness on various benchmarks.",
            "main_review": "Pro:\n- How to improve the robustness of ViTs is rarely studied in the literature. The authors proposed a novel approach to improve the ViT's robustness, and made a comprehensive study on it.\n- The paper is well written.\n- Results showed the effectiveness of the proposed approach on improving the robustness of ViTs.\n\nCon:\n- The authors claimed that ViTs' fail to make adequate use of global context. I am not entirely convinced by this claims. From results of the paper, I believe that the proposed method can improve the robustness of ViTs, but it can not support the claim. The experiment of discarding position embedding can only prove that positional information contribute small information for the classification. It risk to claim that this observation shows that ViTs do not fully use the global context information. ",
            "summary_of_the_review": "The authors proposed an novel approach to improve the ViT's robustness. Comprehensive experiments are conducted on various robustness benchmark. Results showed the proposed method can improve the robustness of ViTs. My main concern is that the major claim of the paper, that ViT's fail to make adequate use of global information, is not supported by the experiments. I'd like to accept the paper if the authors can make the claim more supportive. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Authors propose discrete tokens (instead of the standard continuous pixel-values projected and fed as tokens to ViTs), to enhance the shape learning capability of ViTs, and thus make them robust against out-of-distribution data. The underlying assumption is that discrete tokens preserve the global structure and lose local details, and are therefore better for robustness.\n",
            "main_review": "Authors state that ViTs are overly reliant on local features and fail to make adequate use of global context. To me, this is counter-intuitive and is not supported by concurrent recent findings. \n\nInspired by https://openreview.net/forum?id=Bygh9j09KX that shows CNNs are biased towards texture; [1] conducts an extensive study, and demonstrates that ViTs are less biased towards texture, compared with their CNN counterparts. The authors show that ViTs have shape-bias, comparable to humans. See Fig5 of the paper. \n\n[1] “Intriguing Properties of ViTs” Neurips’21 https://arxiv.org/pdf/2105.10497.pdf\n\nTo make a claim on texture vs Shape bias, the authors must conduct a principled study as in “ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness”  https://openreview.net/forum?id=Bygh9j09KX\n\nIn recent works, ViTs demonstrate better out-of-domain generalization, as demonstrated in extensive experiments conducted in Fig14 of https://arxiv.org/pdf/2105.10497.pdf  and Fig5 of  https://arxiv.org/abs/2106.09785\n",
            "summary_of_the_review": "In the paper, the experiments are conducted primarily on ImageNet robustness datasets. It will be interesting to see, out-of-domain generalization of the features learned by the proposed approach e.g., ImageNet pre-trained features on fine-grained datasets (Flowers, Pets, Fungi, Sketches, Birds, etc).\n\nI am not entirely convinced by the shape-vs-texture bias claims made in the paper, due to lack of principled study as in  https://openreview.net/forum?id=Bygh9j09KX\n\nAlso, it will be interesting to see if the proposed discrete tokens enhance ViTs robustness against common corruptions (e.g., artifacts introduced by rain, haze etc), and adversarial perturbations. The considered robustness analysis is too restrictive in my opinion.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The author present an observation in this paper that discrete image token representations derived from a vector quantized image encoder is able to preserve shape and structure object information. Inspired by this observation, the authors propose a modification to ViT architectures that appending these token representations to the input and show that the resultant models generalize better for out-of-distribution data on ImageNet classification. The modification to the input is simple and can be integrated into variants of ViT. Experiments show that the proposed method consistently outperforms baseline models on ImageNet, ImageNet-Real and seven out-of-distribution datasets derived from ImageNet. The margin of improvement is especially larger for the tasks where textures and image style change significantly while object structure is more discriminative. The main contributions of this paper are two-fold: 1) a novel approach to enrich existing ViT architectures with shape and structure information derived from discrete representations and 2) improve the robustness over the baseline ViT models and achieve SoTA results on ImageNet-Rendition, Stylized-ImageNet and ImageNet-Sketch.\n",
            "main_review": "Strengths:\n1. The proposed method share a similar idea with BEiT using discrete VAE to \"tokenize\" image while apply the idea to improve ViT in different ways. To the best of my knowledge, concatenating discrete representation with the continuous representations for ViT has not been studied in previous work.\n2. The proposed method is technically sound and the writing is properly formatted, well organized and easy-to-follow.\n3. The proposed method is simple yet effective as shown in the experiments. It is a general approach to improve the robustness of ViT. The experiments show that the approach consistently outperforms baseline methods on the benchmarks and achieve SoTA on ImageNet-Rendition, Stylized-ImageNet and ImageNet-Sketch.\n4. The authors present the observation that discrete representations derived from VQ-VAE preserve shape and structure object information which can be integrated into ViT to improve the robustness. They also conduct insightful experiments to demonstrate this observation qualitatively. The quantitative evaluation on the ImageNet benchmarks seems to support this observation.\n5. The authors present extensive ablation studies as listed below. The results are convincing.\n - As the proposed method capture shape information, positional embeddings play important role for their proposed models comparing to plain ViT\n - the improvement does not trivially come from using larger models\n - analysis on codebook size is reasonable and outcome is consistent to ones would expect.\n\n\nWeaknesses:\n1. In Figure 3(b). the visualization shows that discrete representations promote ViT to learn structured filters. Why the proposed modification is able to achieve this behavior is unclear. According to the rest of the paper, the discrete representation itself carries structure information to improve the robustness. Ideally the filters should pick up the complementary information that are more related to style and texture.\n2. There are high attention scores on the corners for ViT In Figure 4 (ii). Why is this the case?\n3. The description of the optimization on \\phi in equation (7) is not precise. A term E_q(z|x)[logp(y|x,z)] can be factored out from the KL term and the gradient of this term w.r.t. \\phi seems not considered in learning VQ_VAE. As a result, the optimization is not exactly maximizing the ELBO described in the equation. However, this term seems impossible to address using pre-training and fine-tuning approach given that p(y|x, z) is not properly modeled during pre-training.\n4. The comparison to the SoTA methods presented In Table 3 might be unfair. The proposed method is based on ViT-B (86M parameters) which is bigger than ResNet-50 (25M parameters) on which the other competing methods are based. The competing methods might achieve similar performance using ViT-B. Some of the improvement might come from a bigger model and it would be great if the authors can clarify this and add the discussion in the paragraph.\n5. Minor issues:\n - Notation L seems used to denote two different things in Section 3.1. It denotes the length of the sequence in the third sentence (L=HxW/P^2) while it also denotes number of transformer layers in equation (2) and (3).\n- In the paragraph above equation (6), z_d takes value from {1, 2, .., K}. Maybe it is better to write z_d \\in {1, 2, ..., K}^L.\n - In the paragraph above section 4.3, the authors refer ImageNet-Rendition as ImageNet-R. It would be great to stay consistent using ImageNet-Rendition as ImageNet-R only appears here.\n",
            "summary_of_the_review": "The submission is pretty solid and the claims made in the paper are convincing. To the best of my knowledge, the proposed method is novel and the contributions are significant. The authors conduct detailed ablation study which are pretty interesting to learn. There are some observations unclear to me, but they are not critical issues given the solid experiments presented in the paper. Overall I believe the quality of this submission is great. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces Dr. ViT, which is conducted on 7 ImageNet robustness benchmarks. The motivation of this work is to demonstrate the robustness can be enhanced when adding discrete representation. Sufficient experiments demonstrate the effectiveness of this paper.\n",
            "main_review": "Pros.\n1. The start point to use discrete information to handle robustness is novel.\n2. The authors  provide some interesting opinions, such as \"Using discrete tokens drives ViT towards better modeling of spatial interactions between tokens, given that individual tokens no longer carry enough information to depend on.\"\n3. This paper is written distinctly, especially Introduction and Related work section.\n\nCons.\n1. #Abstract: (1) The claim ViTs \"are overly reliant on local features\" rather than \"make adequate use of global context\" leads ViTs to fail to generalize OOD, real-world data. #Introductioin: (2) ViTs's robustness comes from capturing globally-contextualized inductive bias than CNNs. (3) Spatial structure contributes less than 3% of ViT's performances. The (1)(2)(3) seems to be contradictory.\n2. The number of baselines is very limited. Missing many important methods, such as CutMix, Mosaic, Puzzle Mix, Cutout, Manifold.\n3. The reviewer is confused about whether the global information is the key weapon of your model or the discrete information.\n4. Missing enough discussion of the method itself. (1) After reading the paper, the reviewer still gets less information on the \"codebook\". (2)  Is VQ-GAN the chosen one, how about using other technologies, and so forth.\n5. Missing some related works such as Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions; Twins: Revisiting the design of spatial attention in vision transformers. \n\nOthers issues.\n1. In Figure 3(b), the author noticed that the \"Ours\" model captures more structural and shape patterns. And in 4.2 Qualitative results, authors hypothesize the robustness improvement stems from better capture of global contexts. So is that mean:  the ViT (or related models) learns higher frequencies is bad? \n2. Why using discrete representation can improve robustness? However, such an operation results in information loss. If it works, how about directly concat a global feature extracted by CNN?",
            "summary_of_the_review": "Suggestion.\n1. The authors organize splendid discussions in the introduction and related work section, and some discussions look interesting. But the latter paragraphs could not work well in concert with the previous writing.  It is suggested that supplement enough in-depth analysis to demonstrate the mentioned illustrations. After reading this paper, the reviewer could just observe \"ViT + some global information would be better\" rather than your method is necessary.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}