{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a novel method for the single-shot domain adaptation with the help of Generative Adversarial Nets. The proposed method is interesting, novel, and versatile. Moreover, the performance is impressive and better than the existing methods. However, the writing needs some improvement for better readability. More quantitive results should be provided in the revision for completeness."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a novel method for single shot domain adaptation for GANs. The approach achieves visually better performance than the current SOTA, allows more degrees of freedom, and is meanwhile efficient to deploy. ",
            "main_review": "**Strengths:**\n\n1. The visualization results demonstrate the proposed method can generate much higher quality images than the baseline StyleGAN-NADA [1], which seems promising to me. The ablation study shows the effect of each regularizer in the loss function.  \n2. The framework itself is clear and novel.  \n3. The training and testing are GPU-friendly and the cost time is acceptable.  \n4. The method is versatile in that it is also equipped with functions like image editing and domain gap controlling.\n\n**Weaknesses:**\n\n1. The biggest problem of the current version of the paper is writing and presentation, especially in Section 3. The writing and presentation is kind of messy to me. Just to mention a few: $v^{\\text{ref}}$ first appears on l.9 in Section 3 and later it is defined in equation (1), which means I do not have any idea what $v^{\\text{ref}}$ exactly is before reading till equation (1). What is $W+$ space? More details of II2S should be provided for readability.  \n2. The compared baselines are simply StyleGAN-NADA [1] and few-shot-gan-adaptation [2]. I think there should be more baseline methods in the style transferring area.  \n3. No quantitative inference time comparison with the baselines are provided.\n\nSome minor points: \n\n(1) Page 5 simply consists of two big figures, leaving much blank space and making the reading experience unsatisfying.  \n(2) There should be a comma at the end of eq (5).  \n(3) In the paragraph \"Training and Inference Time\"  of Section 4, there is something missing in the last sentence \"This embedding time is a few seconds using xx\".  \n(4) There is something wrong with the font style of \"P-norm\" in the paragraph \"latent space interpretation and semantic editing\".\n\n**References**\n\n[1] Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-guided domain adaptation of image generators, 2021.  \n[2] Utkarsh Ojha, Yijun Li, Jingwan Lu, Alexei A. Efros, Yong Jae Lee, Eli Shechtman, and Richard Zhang. Few-shot image generation via cross-domain correspondence, 2021.  ",
            "summary_of_the_review": "My current recommendation is borderline leaning to accept. I may change my score in the rebuttal if my concerns are not addressed well.\n\n--------\nThe authors addressed most of my concerns and hence I raise my score to 8. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposed a method for adapting a trained GAN to another domain using a single image from the target domain. The main technical contributions include a training framework which leverages pretrained CLIP, StyleGAN for adaptation, several new regularizers and other improvements.\n",
            "main_review": "Strengths:\n- The proposed method is novel based on best of my knowledge. The paper designed a framework to leverage the prior knowledge in pretrained CLIP, StyleGAN for adaptation. The idea of modelling domain shift as parallel vectors is interesting and seems to be effective.\n- The achieves results are very impressive, notably better than competing methods as demonstrated in fig 4. Also due to the latent space is coupled, it can be combined with works in latent code editing (e.g. Styleflow) for attribute manipulation.\n- The related work part is well-written and gives a comprehensive review of the related topics.\n\nWeakness:\n- Some of the claims are not fully supported by experiments. For e.g. authors claim that \"We greatly reduce the mode collapse/overfitting problem which often occurs in one-shot and few-shot domain adaptation.\", however I found it's not fully sufficient to support this with only fig 4. Besides the user study, perhaps more quantitative measurements can be reported for ablation study etc.\n \n- The writing can be further refined. Currently the methodology part is a bit difficult to follow and many details seem to be missing. In particular, what do the learnable weights in fig 3 represent? Would the authors elaborate further on which parameters are being learned?\n\nMinor questions:\n- For stylegan-nada and few shot gan in fig4, how many reference images are used?\n",
            "summary_of_the_review": "Overall I think the paper is interesting, as it proposes a novel way for one-shot GAN adaptation. However I do have some concerns about insufficient experimental validation and clarity in details.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper mainly deals with domain transfer tasks in pre-trained GANs, such as transferring the generated images from domain A to domain B by a single reference image from domain B. And experiments show that the proposed method surpasses the SoAT.",
            "main_review": "Strengths:\n1. This proposed method by combining StyleGAN and CLIP is elegant.\n2. The experiments demonstrate the effectiveness of their method, and the visual results in the paper are promising.\n\nWeakness:\n\n1. About Figure 6, why use `+` before every semantics?  It is obvious that the glasses were removed from the man's face instead of adding. And what is the difference between the `Ref` and `Dom.B` in the bottom group?\n\n\n==========Update==========\n\nAt first, the results in this paper left an impressive result in my mind, and then I gave a high score. However, after reading some papers that also have been submitted to ICLR2022, which do the same task, compared with them, I found some weaknesses in this paper and therefore changed my score.\n\n1. The whole paper lacks quantitative results, and all the presented results are some cherry-picked images.  And for lacking comparisons with SOTA proposed by other reviewers, although you have added some, there is still a lack of quantitative comparison.\n2. The main experiments are only style transfer on human faces and lack the attributes transfer. This task can not be called `generative domain adaption.`  Although some extra experiments on cars and cats are shown in the appendix, those results are also misleading. For instance, in figure 11, the caption says this is a style transfer task, but the dogs' structure changed vastly.\n3. The diversity of the transferred model is not validated as well, which is an important index to evaluate the generative models.\n4. When compared with other methods, using the CLIP model is kind of unfair.\n",
            "summary_of_the_review": "The results in the paper are a little amazing and can be used in style transfer on human faces.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}