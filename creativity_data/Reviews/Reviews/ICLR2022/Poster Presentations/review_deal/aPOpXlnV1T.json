{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper examines the approach of modeling aleatoric uncertainty by fitting a neural network, that estimates mean and variances of a heteorscedasitic Gaussian distribution, based on log likelihood maximization. The authors identify the problem that gradient based training on the netgative log likelihood (NLL) may result in suboptimal solutions where a high predicted variance compensates for the predicted mean being far from the true mean. To solve this problem, the authors suggest to adjust the log likelihood objective by weighting the log likelihood of each single data point by the corresponding beta-exponentiated variance estimate. This adjusted objective is referred to as beta-NLL.\n\nAll reviewers agreed that the identified problem and the proposed solution are interesting, that the paper is well written and organized, and that the contributions are significant and somewhat new. The main criticism was on the side of the empirical evaluation. It was criticized that the empirical analysis did not compare the proposed method to other approaches to solving the same problem, that the identified problem and the proposed method should be also analyzed on high-dimensional data, that the results on the synthetic experiments could be improved by investigating more than a single run and by incorporating the the MSE in corresponding Figure 1,  and that standard errors were not reported. \n\nBased on the reviews the authors added several new experiments and investigations in the revised version of their manuscript to improve their empirical analysis: 1) new experiments on high-dimensional data sets were conducted applying variational autoencoders on MNIST and Fashion MNIST and performing Depth-map prediction from images on the NYUv2 dataset. 2) For comparison several baseline methods were added to the experiments on the UCI and the dynamics datasets. 3) Three more UCI datasets (carbon, superconductivity, wine-white) were included in the empirical analysis. 4) An evaluation of calibration of predictive variance for the heteroscedastic sine dataset was added. 5) Several more repetitions of the experiment represented in Figure 1 were conducted. (6) An analysis of undersampling behavior on FetchPickAndPlace was added. Moreover, the authors reported two errors in their previous experiments that they discovered and corrected. \n\nAll reviewers were satisfied with the changes in the revised version and the answers to their specific questions and increased their scores accordingly, now commonly voting for acceptance. The paper should therefore be accepted."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "**Summary**\n\nFor regression tasks with heteroscedastic noise, a Gaussian model whose mean and variance are a function of the input is frequently used. Neural networks are then used as function approximators for the mean and variance and are fit using the negative log-likilhood. The paper identifies a problem with this approach: optimization can get stuck in configurations where the predicted mean is far from the true mean, as this is compensated for by a high predicted variance that also stalls learning. By adjusting the NLL objective, the authors compensate for the problem.",
            "main_review": "**Strong Points**\n\n- The discussion of the optimization issue caused by the NLL is very clear with nice synthetic examples.\n- The paper is well written and mostly easy to follow\n- Table 2 shows clear improvements in performance over the standard NLL approach\n\n\n**Weak Points**\n\n- Section 3.1 is difficult to follow. Figure 4 suggests the measurement in Equation (5) is important, but there is a lack of intuition for its significance.\n- No ablation study comparing other approaches to solving this problem.\n\n\n**Questions**\n\n- I wonder how optimizer dependent this issue is. Which optimizers were used and how were they tuned?\n- There is a long line of work about “critically” initializing neural networks so that their curvature grows exponentially with depth. I wonder if this is related to Section 3.1. The authors might consider looking at Poole et al., “Exponential expressivity in deep neural networks through transient chaos” and Raghu et al., “On the expressive power of deep neural networks.”\n- What is the noise distribution assumed by the beta-NLL? I understand gradients are not taken with respect to the beta-NLL, but I’m curious whether part of the issue is that the Gaussian assumption on the noise is inappropriate.\n- How is the bolding chosen in Table 1. It seems like there’s significant overlap between the performance of the different methods.\n\n\n**Additional Feedback**\n\n- The notation in Equation (9) is confusing. The expectation is over the random variable X, but X also appears outside of any expectation in the stop-gradient operation.\n",
            "summary_of_the_review": "**Recommendation**\n\nI like aspects of this paper, but there are some major outstanding questions. Currently, it does not meet the bar for acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The author(s) attribute the under-fitting of predictive means by neural networks that parameterize heteroscedastic Gaussian likelihoods to 1) initial inability to break symmetry and 2) Gaussian negative log likelihoods tendency to down weight poorly predicted points. The authors note the latter effectively under samples those points with poor predictive means, thus inhibiting their fit in a rich-get-richer scheme. The author(s) propose two alternative loss functions: moment matching (MM) and $\\beta$-exponential variance estimate with $\\beta \\in [0,1]$. The author(s) demonstrate their $\\beta$-exponential generally outperforms NLL ($\\beta = 0$) and MSE.",
            "main_review": "### Strengths:\n- The NLL loss scaling \"the gradient of badly-predicted points, effectively under sampling those points\" is an interesting perspective I have not yet seen. The conventional focus, based on my reading of the literature, is the infinite gradient problem (a perfect mean estimate is impossible to obtain using NLL because variance tends towards 0). \n\n### Weakness:\n- Figure 1 attempts to illustrate the problem of fitting a neural network to a noisy sinusoid using a NLL loss. However, a few things could be improved. Can the author(s) please confirm if the left panel is single run? If so, does rerunning the code under different initialization move the region with poor predictive mean estimates around? Lastly, it probably would be more clear to show the MSE predictive means in addition to its low RMSE (yes, RMSE is low enough to suggest a perfect fit, but it reduces a reader's burden).\n- Claiming the identification of \"high dependence of the gradients on the predictive variance as the primary culprit\" for NLL failures as a contribution is quite bold. Takahashi, et. al (2018) and Stirn, et al (2020) both discuss this issue, but admittedly with a different perspective--neither claims it as a contribution.\n- \"We observe a drastic difference between the NLL loss and the MSE/moment matching loss.\" I understand that the the MM loss for the mean is the same as NLL, but the MM loss also has a variance loss term. How then is 6.b both MSE and MM?\n- The second paragraph of section 4.2 mentions not yet seen experimental results. I recommend moving discussion of results to appear after the experiments.\n- The author(s) seemingly contradict themselves several times as to whether MSE is a fundamentally different loss from their $\\beta = 1$.\n\t1) \"For β = 1 the gradient w.r.t. μ in Eq. 10 is equivalent to the one of MSE. However, for the variance, the gradient in Eq. 11 is a new quantity with 2σ2 in the denominator.\"\n\t2) \"As expected, the same holds for the mean squared error loss (MSE) which is recovered for β = 1.\"\n\t3) The results (figure 8 and table 1) cast  ($\\beta = 1$) as MSE, but their loss term has an added variance term.\n- The author(s) should have MSE in their loss table since I don't believe ($\\beta = 1$) is equivalent. Please correct me if I am missing something!\n- The author(s) do not compare to ANY of the cited works, which often outperform them on the UCI regression tasks.",
            "summary_of_the_review": "I think this paper offers a fresh perspective on NLL loss failures as I mentioned in my main review. However, their only support for this a contrived toy example over a single run. The paper would be much stronger if:\n- The author(s) could apply their analysis to identify either of the two conditions from section 3 in a real world data set.\n- Their proposed methods offered comparable (using simpler methods) or superior performance to related works.\nUnfortunately, without either of these, the paper's potential significance remains limited.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors analyze the known phenomenon that the optimization of the Gaussian likelihood of a probabilistic neural network with heteroscedastic output variance using gradient ascent can get stuck at solutions with a suboptimal mean fit, compensated by large output variance: the likelihood amplifies non-uniform initial distributions of feature granularity due to variance weighting, i.e., the gradient w.r.t. the mean are scaled by the inverse variance leading to well-fitted samples dominating the gradient. The authors argue that this effect can be undesirable because it can prevent spending model expressivity on hard-to-fit regions, while it can be desirable in other situations because it allows the network to ignore outliers arising due to noise. This is complementary to the MSE objective which is dominated by badly-fitted samples (and thus hard to fit regions) but sensitive to outliers, and which does not allow to fit the output variance. The authors propose a modified loss function, $\\beta$-NLL, which interpolates between the likelihood and the MSE loss functions with the incentive to retain the \"best of both worlds\". They compare $\\beta$-NLL with standard NLL, MSE, and a moment-matching version on several datasets.",
            "main_review": "The problem of optimizing Gaussian likelihoods is ubiquitous and, thus, understanding its pitfalls thoroughly is important. While the phenomenon of variance weighting is known, the authors provide a nice analysis with explanations, visualizations, and derivations of  high quality. The claims they make in Secs. 3 and 4 are all well supported and fleshed out nicely. The presentation is very clear and well-organized. I really enjoyed reading the paper and found it quite insightful. The authors provide a pragmatic and easy-to-implement method to alleviate the identified problems in a situation-specific manner (by adjusting $\\beta$). \n\nUnfortunately, I feel the experimental evaluation is lacking:\n- The results in Tab. 1 and Tab. S1 seem to be not significant according to the reported standard errors? How many seeds were evaluated? Tab. S1 should be moved to the main part (assessing the MSE values is central to judging the effectiveness of the proposed method).\n- Why is the RMSE of MM in Tab. 1 worse than that of $\\beta$-NLL? It is optimizing directly for the mean and thus should yield better results, shouldn't it? Did you use two-headed networks? If so, this might explain the worse MSE of MM. But then, I'm still wondering why the MSE in the dynamics experiments (Tab. 2) is worse than $\\beta$-NLL as here you train on the pure MSE loss (not MM). Why don't you evaluate pure MSE on the experiments in Tab. 1/S1?\n- While I agree that the proposed method visually seems to yield better calibrated uncertainty estimates than MM in Fig. 8, I would also like to see a quantitative evaluation here.\n- The tasks are low-dimensional (1D, 3D). I would be interested in results on more complex, higher dimensional problems.\n\nMinor points not influencing my score:\n\n- Shouldn't Eq. 6 contain a factor of $Z$?\n- Sec. 5.1. misses a reference to Fig. 7.\n- On p. 8, paragraph \"Sinusoidal with heteroscedastic noise\", $\\xi$ should read $\\xi_1$ and $\\xi_2$.\n- Fig. 9: $\\beta$ is not rendered in the labels.\n- Sec. 6: wrong full stop before \"we reveal the underlying reasons\".",
            "summary_of_the_review": "The authors provide a very nice discussion and analysis of the known phenomenon of variance weighting arising in SGD on NLL loss of Gaussian likelihoods. While I judge this to be an interesting contribution on its own, I would like to see an improved experimental evaluation. Indeed, I am not yet fully convinced that the proposed $\\beta$-NLL is effective. In its current state I rate the submission as borderline but I will raise my score if the authors improve upon the mentioned points during the rebuttal.\n\n------------------------------\n\n**Update after rebuttal:**\nI thank the authors for their effort to improve the experimental evaluation. I raise my score to 6.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}