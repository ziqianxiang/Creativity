{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper presents a new formulation for the infinitely wide limiting case of deep networks as Gaussian processes, i.e. NNGPs.  The authors extend the existing case to incorporate a scale term at the penultimate layer of the network, which results in a scale mixture of NNGPs or a Student-t process in a specific case.  This formulation allows for a more heavy tailed output distribution which e.g. can be more robust to outliers.  The four reviews averaged just above borderline, with a 5, 8, 6, 6.  The reviewers found the approach to be sensible, technically correct and timely given the recent literature.  They found the experiments to be compelling for the most part, demonstrating the added robustness of this approach over the baseline NNGP.  The main concern raised by the reviewers is that the work is incremental, given that both NNGPs and Student-t processes are already established."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper extends upon the corpus of works that consider the infinite-width limit of a deep net. The foundation of the work is the Neural Network Gaussian Process (NNGP) model, which is taken as the infinite-width limit of a deep net under certain (mild) assumptions; that is the so-called Master Theorem. \n\nThe here proposed approach treats the variances, σ^2, of the penultimate layer weights as random variables. Then, it imposes an appropriate prior distribution on these and proceeds with a reiteration of the Master Theorem (and how this varies in this setting). \n\nThey show that, under this setting, the infinite-width limit of the network is a scale-mixture of NNGPs. In the special case of an imposed Gamma-prior, the so-obtained scale mixture reduces to a Student's-t process; this is a well-known result.\n\n",
            "main_review": "Strengths: The approach is interesting, somewhat novel, and easily applicable to a broad range of deep nets. The provided theorems are sufficient for supporting the method and correct. The derivations pertaining to posterior inference are correct and result in efficient algorithms.  The results are broad enough, with many comparisons and many datasets considered; these vouch for the usefulness of the method.\nCons: Some discussion on complexity is missing.",
            "summary_of_the_review": "A strong paper lacking some discussion on complexity. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "Although the connection between Bayesian neural networks and Gaussian processes has been a topic of interest for several years, the work on tensor programs facilitating the formulation of the Neural Network-Gaussian Process (NNGP) model has been influential in further emphasising the correspondence between the two classes of models. In this work, the authors propose an extension to the NNGP model that allows for the formulation of more general stochastic processes that may follow alternative distributions such as Student-$t$. This is achieved by way of introducing a prior on the scale of the parameters in the last layer of the neural network. This formulation allows for greater flexibility without requiring the more involved changes proposed in related works such as Favran et al. (2020) and Bracale et al. (2021). Following the analysis carried out in the initial formulation of NNGPs, the authors also investigate the correspondence between GPs and infinitely-wide BNNs configured with the proposed set-up and trained using gradient descent. The experimental evaluation features a mixture of regression and classification tasks, with a particular emphasis on how the use of models with heavier-tailed distributions can be better suited to datasets with challenging properties such as corrupted data and label imbalance.",
            "main_review": "### Strengths\n\n-\tTensor programs for expressing GPs using the NNGP formulation are a fairly recent development, and the work presented here is consequently a timely contribution to a topic that has accrued great interest within the community.\n-\tAlthough the core contribution feels fairly incremental in nature, both the execution and the corresponding analysis are non-trivial. Although I was not familiar enough with tensor programs to properly go through and understand all the derivations featured in the paper, the exposition appears to be quite thorough all throughout the paper, with sufficient detail also being provided in the supplementary material.\n-\tThe experiments are broad and varied, covering a wide selection of possible practical challenges (noisy data/unreliable labels, etc), where the proposed technique might be particularly effective.\n\n### Weaknesses\n\n-\tWhile well-written overall, I often found the paper to be quite difficult to follow properly. The paper contains a lot of set-up, whereas the paper’s own contributions are only presented from the very end of page 4 onwards. Even then, the majority of the paper’s contributions are all grouped together in a single ‘Results’ section that are only slightly disambiguated under three different theorems. Although the paper already contains a very extensive and thorough appendix, I do believe that the paper would benefit from some additional restructuring and refactoring.\n-\tIt would also be nice to expand on some of the related work (in particular the competing approaches highlighted for CNNs) in order to better emphasise how the approach being proposed in this paper is ‘simpler’ while being just as effective.\n-\tI have a few questions on the Experiments section:\n--\tIs there a particular reason for not showing RMSE results for the regression datasets? While the benefits of using a heavier-tailed distribution may indeed be more pronounced in the MNLL metrics, it would still be interesting to see whether this comes at the expense of possibly inferior mean predictions. \n--\tFor the results on the regression datasets shown in Table 1, the authors comment that the results for competing methods are copied from the work of Adlam et al (2020). However, given that there are no predefined train/test splits for these UCI datasets, how are you ensuring that the same splits are used as for this earlier work?\n--\tWould it be possible to compare against the other heavy-tailed stochastic processes described in the Related Work section? This should at least be possible for the image classification experiments right? There’s also a small error in the caption of Figure 1 which contains references to ‘top’ and ‘bottom’ figures.\n-\tThere are a few typos remaining in the paper, but these should be fairly easy to with a proper read-through of the paper. A few of the titles in the references also require capitalization for words such as Gaussian, etc.\n",
            "summary_of_the_review": "The contributions featured in this paper provide a valuable and non-trivial extension to the literature on NNGPs. Given that this is a fairly new area of research, such work is especially timely. Even so, I still believe that the paper’s writing could be heavily improved in order to make the exposition more clear. I also have some remaining concerns on the experimental evaluation that I would like to see addressed in either the rebuttal or a future revision of the paper before tending more heavily towards acceptance.\n\n** Rebuttal Update **\n\nRaised score to 6 following clarifications and additional experiment results provided by authors.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Building on the recent results on the correspondence between infinite-width neural networks and Gaussian processes (NNGPs), this paper proposes and studies a simple extension, looking at a scale mixture of such processes. The key idea is to introduce a scale prior on the parameters of the last layer, allowing construction of a richer classes of stochastic processes, particularly those with heavy tails. Some convergence results for these general processes are obtained by applying the tensor program and the Master Theorem, under various settings. Empirical results are also provided, showing the promise of the approach and robustness to out-of-distribution data.",
            "main_review": "Strengths:  \n- Interesting extension of NNGPs to consider limiting processes which can have heavy tails, backed by rigorous convergence results and empirical evaluations\n- The paper is mostly well presented and easy to follow\n\nWeaknesses:\n- There are empirical findings showing that the scale mixture of NNGPs leads to robustness to the out-of-distribution data, but no theoretical results are provided to justify these (would be nice to at least give some intuition)\n\nComments/questions:\n- It is not clear to me how and why the heavy tail nature of the NNGPs leads to the robustness (also, why not robustness to noisy labels, imbalance, etc.). What is the key intuition there?\n- As for the empirical results, the scale mixture of NNGPs seems to give mixed performance (both accuracy and robustness to various perturbations) for different datasets. How should one decide which model to use for a given task to achieve desirable accuracy and robustness? Are there some trade-offs?\n\nMinor comments:\n- It may be a good idea to define the inverse Gamma distribution introduced on page 4\n- There is a missing comma after the first equation in Theorem 3.2",
            "summary_of_the_review": "Overall, this is a well written paper that studies an interesting extension of NNGPs to a scale mixture version, which allows realization of a richer class of stochastic processes such as those with heavy tails. Rigorous results are provided and they seem to be technically correct (although I have not checked all the details). These are the key strengths of the paper and the main factors behind my rating (an acceptance). However, no intuition or theory is provided to justify the empirical findings for the robustness, which seems to be an important practical utility of the proposed approach. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes to extend the typical GP formulation as the limiting case of a NN with Gaussian weights when the number of hidden units tends to infinity. In particular, it considers a scale of mixture of Gaussians at the last layer for the prior. When this scale mixture of Gaussians is a Inv. Gamma distribution it is well known that the result is a Student-t process. The paper also gives some insights about the resulting process obtained by training the weights of the last layer and following a similar initialization. The resulting processes are heavy-tailed and more robust as shown in the experiments.\n",
            "main_review": "Font in tables is too small to be readable. Same for Fig. 1.\n\nThe paper in general clear and well written. The experiments are also extensive. The theorems seem correct although I did not check them thoroughly.\n\nMy main point of criticism of this paper is that there is little novelty in it. Although it provides interesting theoretical results, the main application, i.e., the use of Student's T process is already known in the machine learning community. So there are little new practical applications of this work.\n\nSumming up, I believe that although this paper is interesting it is not expected to have a big impact in the machine learning community.\n\n\n\n",
            "summary_of_the_review": "Nice theoretical results that lead to already known methods in the machine learning community which questions the novelty of the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}