{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper focuses on understanding how the angle between two inputs change as they are propagated in a randomly-initialized convolutional neural network layers. They demonstrate very different behavior in different settings and provide rigorous measure concentration results. The reviewers thought the paper is well written and easy to read with nice theoretical results. They did raise a variety of technical concerns that were mostly addressed by the authors rebuttal. My own reading of the paper is that this is a nice contribution. I therefore agree with the reviewers and recommend acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "\nThe authors consider angle propagation in randomly-initialized convolutional\nneural network layers -- given two input images that have cosine angle $\\rho$,\nwhat is the cosine angle of their feature embeddings when propagated through\nthe random layer? They show that the behavior is very different from the\nstandard feedforward case that has been discussed at length following the work\nof Daniely et al. (2016): for filters of small spatial support and e.g. ReLU\nactivations, the output cosine angle can be as small as the ReLU of the input\ncosine angle, or as large as a linear function of the input cosine angle,\ndepending on the patch structure of the input image. This has implications for\nunderstanding signal propagation and issues like dynamical isometry in\nconvolutional networks. The authors give some measure concentration results, in\nthe spirit of Daniely et al.'s results for feedforward networks, that specify\nthe behavior in the convolutional case, and present examples of different types\nof images (unstructured/gaussian; cartoons; CIFAR-10 data) where different\nangle propagation behaviors are observed.\n",
            "main_review": "\n### Strengths\n\n- The paper is well-written and enjoyable to read. There is a good mix of\n  theory and intuition. The mathematics is written clearly and precisely.\n- The basic observation around angle contraction in random convolution layers,\n  while possibly implicit in some previous works (see request for clarification\n  below), does not seem to have been presented systematically as the authors do\n  so here, and is quite an interesting difference relative to the feedforward\n  case. The way the authors study this issue, using tight lower and upper\n  bounds for the new map with the ReLU activation, as well as examples with\n  gaussian data, empirical data (Figure 1) and cartoon data (section 3.4), is\n  sufficiently systematic to give a good entry point to understand various\n  interesting aspects of this difference. I anticipate this will be useful for\n  follow-up work.\n- The issue is placed well in context via discussion of many related works\n  about dynamical isometry and more generally accelerating training of\n  deep networks.\n\n\n### Weaknesses\n- There is a large body of work on CNTKs that has appeared following the\n  initial interest in the NTK, and I think it would be helpful if the authors\n  spent some time in the related work section comparing to any relevant work in\n  this line -- it seems to me, although I could be mistaken, that some of the\n  formulas and concentration results the authors give in the paper may have\n  been derived previously in these contexts due to the natural interest in such\n  expressions for writing down CNTKs, and it is important to know of such\n  overlaps to assess the authors' work's novelty. For example, [1-4] below.\n- The concentration results are rather primitive: notably Theorem 2\n  requires a stack of i.i.d.\\ feature maps to be channel-averaged to obtain\n  concentration, whereas one would expect it is possible to have a certain\n  degree of concentration as long as the input size $n$ is sufficiently large\n  relative to the kernel size $r$. This weakness is due to the authors using\n  worst-case analysis of the convolutional structure in Theorem 2's proof -- a\n  union bound is taken to control the concentration of the individual\n  translates of the filter rather than something more sophisticated like\n  chaining/decoupling tools, which are standard in the sharp analysis of\n  RIP-type properties of convolutional operators (e.g. [5-6]). In this\n  connection, the content of Section 3.1 seems superfluous. There also exist in\n  the signal propagation literature significantly sharper analyses of the\n  concentration behavior of the ReLU's dual activation that are relevant in the\n  context of results like Corollary 1 applied to the ReLU [7, Lemma E.3]. All\n  this said, I do not believe this necessarily diminishes the authors' results,\n  as long as the previous point about comparisons to the literature is\n  satisfied -- if this work is deriving these kinds of results for the first\n  time in the literature, they of course do not need to be optimal -- but it is\n  necessary to acknowledge these limitations to accurately place the results in\n  context and make follow-up directions clear.\n- Proof of theorem 2: the argument around the Lipschitz property is not\n  correct: it is not true in general that a Lipschitz function of a\n  sub-gaussian random variable is sub-gaussian (consider a Rademacher random\n  vector). The argument should use the gaussian distribution of the random\n  variables (and Gauss-Lipschitz concentration e.g. from Vershynin ch. 5 should\n  be cited).\n\n[1] http://arxiv.org/abs/1905.12173\n\n[2] http://arxiv.org/abs/1904.11955 \n\n[3] http://arxiv.org/abs/2102.10032\n\n[4] http://arxiv.org/abs/1806.05393\n\n[5] http://dx.doi.org/10.1002/cpa.21504\n\n[6] http://arxiv.org/abs/1712.00716\n\n[7] https://openreview.net/forum?id=O-6Pm_d_Q-\n\n\n### Minor points:\n- I don't immediately understand the statement \"the unit vectors $x, y$ are\n  $1$-dimensional\" in Remark 2 -- is this thinking of $x$ and $y$ as being $1\n  \\times 1 \\times 2$ images? In the next sentence we have $x, y \\in \\mathbb{R}^2$\n  stated, i.e. not $1$-dimensional.\n- It would be helpful to have some visualizations of the different regimes of\n  behavior represented in the bounds of Theorem 3, as well as some more insight\n  into the examples of Remarks 2 and 3. For example, can we understand when\n  this bound is loose/tight based on the structure of the inputs across\n  patches? (The gaussian example recovers the FFNN case -- when there are\n  correlations across patches, does this lead us towards the other extremes?)\n- First paragraph on page 3: typo \"outout\" (output)\n- The last two paragraphs of the introduction (Section 1) are slightly hard to\n  appreciate to this reader because they are written in what seems to be a\n  deliberately technically imprecise way -- for example, it would be helpful to\n  specify what kinds of depth-width and initialization regimes are being\n  considered here (the discussion quickly jumps from angle propagation across\n  one layer to angle propagation throughout many layers -- to have the points\n  being made here have meaning, I believe it is necessary to be considering\n  \"edge of chaos\" initialization). I particularly do not understand the last\n  paragraph unless it is being assumed here that the depth is significantly\n  larger than the width (e.g. width fixed as depth diverges).  I think the\n  point made in the second to last paragraph is insightful and applies much\n  more generally than the \"diverging depth\" setting.\n- Corollary 1: the statement says the variance of the gaussian is $\\nu^2$, but\n  it seems from the value given for $K$ that a specific choice of $\\nu$ is\n  being used here.\n- Theorem 5: Above the $r$-boundary is defined as a set -- is the usage here\n  supposed to be the cardinality of the $r$-boundary?\n- section 4.1: \"curios\"\n",
            "summary_of_the_review": "This is an interesting paper that combines theory and empirical insights to\nhighlight differences between angle propagation in random CNNs and feedforward\nnetworks. The results should inspire interesting follow-up work in this\nsetting. I hope the authors will clarify the relationship to the theorems in\nthis work to any possible prior art in e.g. CNTK papers, as mentioned above.\nThe concentration results are basic, but probably enough as first-cuts (it\nseems important to acknowledge limitations, however).\n",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper shows (as clearly described in the introduction) two things: random filters have the distance preservation property. For random convolution + ReLU activation it is contracted (which is also expected, since ReLU throws away half of the entries).\nHowever, for images the authors try to explain why CNN+ReLU is isometric by considering images as objects with large monochromatic images. \n\nThe main motivation for such research in practice is initialization. A deep fully-connected network with ReLU may lead to collapse, whereas for CNNs it does not. The paper argues that the randomized theory presented here is a justification for such fact for images.",
            "main_review": "The paper is not a typical paper that we read today. The formulation is simple, and the paper is easy to read (in the main part, which does not contain proofs, only formulation).\n Some results are easy to prove (such as Lemma 3 and Theorem 1). Some results have doubtful meaning, such as concentration of measure result (see 22e) for example, which contains very unrealistic constants.\nHowever, the theorems seem to be correct. Also, the authors test their results on different datasets and get intriguing results that show that the lower bound seems to be better suited for random convolutional filter, applied to CIFAR-10 or ImageNet datasets.\n\nA possible addition would be to study how the isometry property \"suffers\" during the training, but since the authors limit themselves to the analysis of the initialization, that is not formally connected to the results. \n\nQuestions:\n\n- After the first CNN filter, the distribution might change. What is the effect of deeper layers on the isometry property?\n- Can such kind of studies provide practical recommendations for initialization?\n",
            "summary_of_the_review": "Good, self-consistent and easy to ready paper. Some proofs are not needed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the random initialized CNNs and analyzes the geometry preservation. For linear CNNs, the authors show the JL lemma type results hold. For  CNN+ ReLU, the output contracts and the level of contraction depend on the inputs. In numerical experiments, the authors verify the geometry of natural images is preserved, while for random gaussian correlated inputs, the contraction behavior appears. ",
            "main_review": "Strengths:\n\nThe paper is well-written. The introduction and related work section give plenty of background information and a notation table is also included to help the reader to access the technique details. I check the technical details and no major flaw is found.\n\n\nWeaknesses:\n\nThe technical strengths may not be strong enough. For the linear CNNs with random Gaussian weights, the underlying procedure is a sum of product between the inputs and random Gaussian random variables. It is expected that the JL lemma-type results will still be held. For the CNNs+ ReLU, since part of the information can be truncted by ReLU, the contraction behavior is also straightforward. If the authors think the proof contains nontrivial contributions, they should be highlighted. ",
            "summary_of_the_review": "My major concern is the technical significance. At the current stage, I think this paper may not be strong enough for ICLR.\n\n**** After seeing the author's comments. I agree the JL-type results for CNNs are different from those in the literature. I change my score accordingly.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}