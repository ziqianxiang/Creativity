{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper describes how to apply a combination of case-based reasoning and RL methods to improve the performance of agents in text-adventure games.  The reviewers unanimously recommend acceptance.  This work is both insightful and practical.  This is a valuable contribution.  Well done!"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an approach to combine case-based reasoning with reinforcement learning for text-based games. The method works by keeping track of states that received positive rewards in the game, and then having a retrieval mechanism to retrieve similar contexts at a new state in the game. The authors use a quantization technique for efficient storage and retrieval, and a fallback neural policy (trained with RL) in cases where none of the retrieved action templates result in an admissible action in the current state. The writing is quite clear and the experiments are mostly convincing, barring a few questions I have.",
            "main_review": "Strengths:\n- Solid approach combining CBR and RL\n- Experiments are thorough and convincing.\n- Writing is quite clear\n\nWeaknesses:\n- Some design choices may need more justification/analysis (see below, mainly points 3 and 4)\n\nComments:\n1. In the retriever module, how is the threshold $\\tau$ chosen?\n2. Algorithm 1 describes performing retrieval using multiple context selectors $\\mathcal{C}_t$. How are these specified and how are they different from each other? Do they store different types of contexts?\n3. The ‘Retain’ module seemed a bit ad-hoc to me since the module stores actions with positive rewards only. This goes against the key premise of performing good credit assignments in RL since the crucial action that led to this positive reward may occur 5 or 10 or even 20 steps before the transition with the positive reward. I suspect this is why storing previous actions helps empirically. So, to me, this design choice seems arbitrary and not well motivated. In fact, why not store negative actions as well or even use a prioritization scheme based on TD error similar to that in https://arxiv.org/abs/1511.05952 ? An ablation on this (even on a small subset of the games) may be helpful.\n4. The context discretization scheme in 4.2 reminds me of this paper (https://arxiv.org/abs/2103.13552) that used hash functions to represent the state. Would just hashing the state (e.g. with locality sensitive hashing (LSH) on fixed pre-trained BERT representations) with a random function work as well as the learned discretization scheme? Again, an ablation on just a subset of the games might be useful here. \n5. On the same point as above, the paper mentions that the discretization scheme helps tackle the issue of changing representations over the course of training. However, since the discretization is also a learned function, wouldn’t that also potentially change over time? It wasn’t clear how this solves the issue of the same context mapping to two different entries in the memory.\n6. In table 3, the term “win rate/win count” for the last row was confusing. I took it to be the win rate on the games themselves, but it seems to be a comparison across the different models. I suggest changing it to something clearer.\n7. It is also unclear how exactly the model chooses between two actions which have the same template, since they have the same relevance $\\delta$ value (which depends only on the template?). For example, in figure 3, I’m not sure how the agent picks `Put emerald in case` over `Put jewels in case`.",
            "summary_of_the_review": "I think this paper is exploring a novel direction, has good empirical results and would be a valuable addition to the literature. Addressing the points mentioned above would further strengthen the paper and I’m happy to raise my score if the authors can provide convincing responses.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper describes how to apply  a combination of case-based reasoning and RL methods to improve performance of agents on text-adventure game type tasks. It introduces a GNN representation of state and a vector-quantized encoding scheme so that contextual information about successful actions from the past can be retrieved and re-used. Experiments show a significant increase in performance, along with ablations showing value of the CBR  especially on OOD environments.Finally there is qualitative insights  of the memory representation showing how the CBR helps.",
            "main_review": "Using the Neurips rubric:\n\nOriginality: As the related work shows,  CBR to speed up RL has been tried before, however those methods are very different and not meant for the modern scale of deep RL and contextualized representations+retrieval methods. The actual details of the CBR model used in this paper, seeded graph attention and vector quantization to aid retrieval seem relevant at least in their application to doing CBR for RL. \nThe basic experiments are straightforward, applying combinations of CBR and sota RL methods on the 2 TAG domains. However, there are a couple of novelties in the ablations and a very good qualitative analysis to show where the gains from CBR are coming from (fig 3).\n\nQuality: I thought the model described in sec 4 was interesting and mostly well-motivated. Some minor questions:\n  a. comparing the 2 equations in seeded graph attention, it looks like the h's dependency on $\\alpha$ is quadratic, which seems unusual for attention?\nb. why sum for the final representation and not average? it could lead to wierd biases when the number of entities are different between states.\nc. The VQ approach is interesting, but I am a bit skeptical that it is necesary. Very efficient methods exist to do retrieval in large dim spaces, with some effort. So for practical applications, it might just be easier to do that. Can you share some quantitative measurements to show that this code splitting approach is necessary?\nd.  The claim that CBR lets you generalize OOD seems not supported and my intuition is almost the opposite. Can you elaborate?\ne. The CBR and neueral agent seem to be separately trained. Can they be jointly optimized or at least can the neural agent be trained with  some knowledge of when the CBR will over-ride it so that it's policy can adapt to that?\nf. Fig 4: it would be more interesting if this could show the \"counterfactual\" probabilties. since the neural agent always loses to the CBR in action selection, what if the neural agent had been allowed to execute, would it still have been successful?\ng. Is locality sensitive hashing another way to do efficient retrieval that can generalize?\n\nClarity: the paper is well written and all major parts are well-explained. A few details could use some clarity:\na.  when you say \"the final set of all retrieved actions and the corresponding relevance\", can you be more precise and define the tuple or whatever exactly the data structure is.\nb. sec 4.3: \"applied to the entities\": how does it choose which entities to use?\nc. Please elaborate on the Adolphs and Hofmann method, even though it's not an original contribution it is a core part of your system.\nd. The jump from 24% to 73% in win rate on Jericho seems really big ! You should highlight it more.",
            "summary_of_the_review": "Fairly well-thought out scalable approach to adding CBR to a tough RL setting which has got some attention recently. Strong results and good empirical analysis. It would be beneficial to get RL practitioners to seriously consider adding CBR type approaches as a new type of strategy for boosting Deep RL.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents, at its core, a case-based reasoning (CBR) centered approach to solving text-based games. The CBR process shown is a 4 step process consisting of: retrieve (useful past experiences), reuse (the past experiences in a meaningful manner), revise (modify the current policy to account for prior experiences), and retain (decide which experiences to keep). This method is applied to an A2C agent that uses a knowledge graph based state representation, though it can be used along with other types of text game agents too.\n",
            "main_review": "Strengths:\n\n- The core idea is well defined and motivated. \"Lets get agents to use previous useful experiences to better improve the current policy\"\n\n- The paper is well written overall I was mostly able to follow along, model design choices are mostly explained.\n\n- The paper compares to multiple existing methods and the line up of related work baselines strengthens the work.\n\nClarifications/Concerns/Weaknesses:\n\n- Section 3&4 in particular are a bit unclear and can be consolidated more. I think providing details of each portion under the relevant subsection of the 4 step CBR algorithm would make it a lot more clear.\n\n- The retrieve portion in particular seems very similar to many other retrieval methods recently seen in text based game literature and the wider NLP community. It would be nice to see a comparison/ablation in *just the retrieve portion* to not use the knowledge graph and only text on *Jericho* (one obvious way would be to frame it as a Machine Reading Comprehension task to retrieve passages as in Guo et al. 2020 https://arxiv.org/abs/2010.02386 [which is cited and compared to for TWC]). Without such experiments it is relatively unclear whether the portions shown in Sec 4.1 are aiding, and if so how.\n\n- The \"Baseline agent + CBR\" comparisons are good to have but would probably be more useful to have in Jericho than TWC. The main reason being that TWC is a singular \"home\" domain in which the commonsense knowledge (the text descriptions + genre knowledge) are more likely to be uniform throughout than with the varying genres like Jericho. This tests the limits of the CBR process and gives you the ability to analyze exactly in what portion of cases in certain types of knowledge transferable. In my opinion, such an analysis would likely prove to be the most valuable contribution of a work like this.",
            "summary_of_the_review": "The paper is well written and presents a useful paradigm for thinking about text game agents and the results are interesting. The main thing holding the paper back is the lack of analysis/ablations on certain portions of the proposed algorithm that make it unclear where the gain are coming from.\n\n\n====Rebuttal Update====\nI've read the rebuttal across the reviewers and am satisfied enough to increase my score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose to improve the existing policy-based RL algorithms for the text-based world environments by incorporating knowledge via a case-based reasoning module. This improves out of distribution performance. The key idea in Case-Based Reasoning is to collect interactions that led to positive rewards in the past and try to map a novel situation to one of these past interactions to decide on the action in the current situation. In particular, authors represent the state of a text-based game by a knowledge graph and use message propagation (focussed on the sub-set of the nodes) to get the most similar representation of a new context. ",
            "main_review": "Strengths:\n1. Authors show that combining CBR with existing RL methods improves agent performance significantly. \n2. Authors conduct detailed experiments (existing RL models + CBR) which show the effectiveness of the proposed approach. The proposed model is more sample efficient and generalizes better.    \n\nWeaknesses:\n1. The different components proposed in the paper, (e.g., CBR, KG) are not novel per se for the text-based RL agents. The paper has limited novelty. \n2. There has been recent work that has used transformer-based LMs for text-based RL agents [e.g., 1,2,3]. Since Transformers are trained on very large corpora, they have been shown to encode world knowledge and hence work well in text-based world settings when combined with an RL agent. The authors do not compare to these approaches. It would be nice to have a comparison with these knowledge-based approaches as well.   \n\n[1] Deep reinforcement learning with transformers for text adventure games, Y. Xu, et al., 2020: https://ieeexplore.ieee.org/document/9231622  \n[2] Pre-trained Language Models as Prior Knowledge for\nPlaying Text-based Games, Singh, et al., 2020: https://arxiv.org/abs/2107.08408\n[3] Stabilizing transformers for reinforcement learning. Parisotto, et al., 2020: https://arxiv.org/abs/1910.06764 \n\nSuggestion:\n\nAuthors say that they use A2C as the RL agent, however, they cite the paper that describes A3C, so it is not exactly clear if they are using a synchronous or asynchronous version of the Advantage Actor-Critic agent. More details on this would help. ",
            "summary_of_the_review": "The paper proposes the use of Case-Based Reasoning combined with Knowledge Graphs for improving RL agents for text-based environments. However, the approach has limited novelty as similar approaches have been proposed in the past. On the evaluation side, authors should also compare with other approaches (e.g., based on Transformers) that make use of external knowledge to generalize better. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}