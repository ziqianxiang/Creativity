{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "Somewhat borderline paper given the scores, but leaning on the side of accepting mostly because the positive (and weak positive) reviews are a little more persuasive. The negative review is a bit of an outlier; the main issues raised in the negative review are that the novelty is on the lower side or otherwise that the work is incremental. These complaints are largely not shared by the other reviewers, and furthermore seem not like deal-breakers. Still a borderline paper, but fairly safe to accept."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper addresses the problem of slow convergence in training of object detection transformer. The main contribution is to use trainable anchor boxes (4D) in a layer-by-layer manner. They also motivate their contribution by visualizing positional attentions and explaining the advantage of their proposed method in adapting to scale ratio of objects. They further propose to change the size of the positional attention by an additional temperature parameter, which they choose empirically for the dataset. They perform experiments to justify aspects of their claims, and provide ablations to prove their results being improved over the SOTA on the COCO '17 validation set.",
            "main_review": "## Main strengths:\n\n1) Providing detailed diagrams highlighting the differences between the proposed method and the baselines\n2) Performing an experiment explaining to rebut some potential doubts about sources of improvements\n3) Visualizing the attention between positional queries and positional keys, justifying the limitations of the previous works and their proposed method's advantage\n4) Improving benchmark results over SOTA\n\n## Main Weaknesses:\n\n5) Based on my understanding, the experiment visualized in figure 3, aims to justify that something is wrong with the way positional information is represented and it is not merely an issue with the optimization. The authors should generate training curves of a similar experiment, after making the proposed changes too. It would much better reveal the impact of their changes in the discussed matter and clear the doubt about the reasoning of the experiment, knowing how using \"well-trained\" queries of their proposed method compare to training them from scratch. furthermore, comparing the described training curves to those in figure 3 would much better illustrate the improvement in rate of convergence which is the main issue targeted by the proposed method. Drawing these curves as a result of multiple runs with an average curve and some measure of confidence could further support the claims based on these types of experiments.\n6) Hyperparameter study is limited to temperature and number of decoder layers. Effects of other hyperparameter choices are not clear to the reader. Even if their values are copied from the baseline methods, the proposed changes in the method could alter their behavior. Also, it seems like the provided ablations are compared using the the hyperparameter choices that work best with the proposed configuration. This could mean that those choices are biased towards the proposed method in the ablations. The hyperparameters for each individual configuration should be chosen independent of the others.\n7) While Deformable DETR (Zhu et al., 2021) is discussed in the introduction as the last paper in this line of research, its results are not compared in Table 2 and I am not sure the proposed method is necessarily better in terms of AP, and comparative in terms of GFLOPs and number of Parameters.\n8) Some of the compared methods had improved results with increased number of training epochs and other configuration of architectures such as backbones. It is not clear how would the proposed method compare to the SOTA in such settings.\n9) Qualitative results are quite limited in the paper. From the only provided figure (9), the main visible difference in the last two columns is how the books in the second row of images are detected. A comparison between the proposed method and the baselines highlighting a few examples that contribute to the improvements of the quantitative results could improve this aspect of the paper. Knowing the SOTA on this dataset is still under 50% in AP, visualizing some failure cases and trying to explain why the method failed could further improve the work.\n\n#### Minor points:\n10) There is also a paper called \" Dynamic DETR: End-to-End Object Detection With Dynamic Attention\" from ICCV'21 which better be included in the literature review and compared experiments of this work.\n11) Knowing the amount of GPU memory used could be important for reproducing the results.\n12) Only releasing the code would not suffice to support the statement on reproducibility. The mentioned points on hyperparameter studies and providing statistical analysis on the results rather than results from a single run, and providing details on the computational infrastructure used could further support this statement.",
            "summary_of_the_review": "The paper seems to be a step in the right direction to address the issues with detection transformers. The provided figures of architectures are well illustrative and helps readers to understand the differences. The performed experiments to some extent justify the decisions made in the design choices. However, there are missing complementary experiments that I feel are required to fully explain the results. The empirical approaches are lacking as also described in the main review which damages the credibility of some of the claims.\nTherefore, I am leaning towards a reject for this version of the work. But I do feel like the paper has the potential to be a great submission in a future revision, assuming the points of concerns are covered with additional experiments. I have provided more details on how to improve the paper. I look forward to see how the authors respond and address my concerns before I finalize my decision.\n\nEdit: I thank the authors for responding to my comments in details. \nI am glad to see that the authors found some of my comments useful and took steps to further improve their work.\nSome of my main concerns are resolved (5 and 9), while some other concerns are only partially addressed and they still remain a concern in my opinion (6, 7, and 8).\n\n- The authors have not provided even a hint of performing a somewhat systematic/objective hyperparameter search to convince the readers that the hyperparameters are not biased towards the proposed method in the ablation studies such as in table 3.\n- Some improved variants of the Deformable DETR paper including the two-stage Deformable DETR and results from the table 3 in that paper, seem to be still missing with appropriate comparisons in the updated draft.\n- The authors have provided new results using a couple of alternative architectures using their method which shows potential for further improvements. However, these are not provided side-by-side in comparison to other methods, So that the proposed method can be judged on its capacity for improvement compared to the alternatives discussed in the paper. I do understand that such experiments are time consuming, however, the paper having taken such a comparative approach to motivate its contributions, needs such thorough empirical comparisons to prove its claims on improvements.\n- It seems that the concerns of reviewer zkuH regarding contributions being marginally significant or novel, are not sufficiently resolved.\n\nTypos I noticed in the updated draft:\n- \"Some previous work also has similar analysis and confirmed this.\": \"work\" --> \"works\"\n- \"The modulated attentions can be regarded as helping perform soft ROI pooling.\": \"perform\" --> \"to perform\"\n\nOverall, the updated draft is improved and so I increase the recommendation score to 6.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces dynamic anchor boxes as a query formulation for detection transformer (DETR). The basic idea is to use explicit positional priors and scale priors from anchor boxes as the queries for training the decoder of DETR. Anchor boxes are also adjusted layer-by-layer to learn the optimal anchor setting. The proposed query formulation provides a better understanding of the query for vision transformer and is proven to outperform previous implicit and explicit query formulations.",
            "main_review": "Strengths:\n+ This paper is well organized, with clear comparison to prior works in tables and graphs, good problem analysis on the role of queries in DETR and conditional DETR, and informative figures.\n+ Experiments demonstrate the advantage of dynamic anchor queries over previous implicit and explicit query formulations including original DETR, conditional DETR and anchor DETR. \n+ The proposed query formulation introduces marginal computational overhead compared to other formulations in terms of GFLOPs.\n\nWeaknesses:\n- It seems the proposed query formulation works well with the Deformable Convolution Network (DCN) backbone but less competitive without the DCN backbone. For example, DAB-DETR-DC5-R50 outperforms Anchor DETR-DC5-R50 by 1.5 AP but DAB-DETR- R50 outperforms Anchor DETR-R50 by only 0.5 AP. In the ablation study, all experiments are conducted with the DCN. It would be more comprehensive to provide analysis or insights on why dynamic anchor queries cannot offer significant improvements without DCN.\n- GFLOPs is a partial indicator of computational efficiency. The authors might want to report the actual runtime speed (ms per image).\n- A minor reminder on citation correctness, e.g. Section 4.3 2nd line DETR should not cite Meng et al., 2021.\n",
            "summary_of_the_review": "This paper has the merits of good performance, clear presentation, and a neat idea. Although more analysis and experiments could further improve the manuscript, overall this is a good paper. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes DAB-DETR, a DETR variant that incorporates dynamic anchor boxes for object queries. Based on the analysis of the learned queries in DETR, the authors conclude the slow convergence of DETR is likely caused by the multiple mode property of the queries. To solve the problem, explicit positional priors (box coordinates) are used to improve query-to-feature similarity and accelerate model convergence. Experiments and analysis show that DAB-DETR achieves promising results on the object detection task.",
            "main_review": "### Pros\n\n- This paper provides a systematic comparison between several detection transformer variants, from the perspective of positional encoding and attention mechanism, as summarized in Table 1. \n- The experiment results are promising, and DAB-DETR achieves state-of-the-art performance on COCO 2017 dataset.\n\n### Cons\n\n- The novelty of this paper is very limited, the idea of using explicit positional priors to accelerate training and improve explainability has already been adopted in Deformable DETR, Conditional DETR, and Anchor DETR.\n- This is incremental work. Almost every part of the method can be found in existing work. For example, the iterative update of 4-D anchor boxes resembles the update of reference points in Deformable DETR, the decoupling of the content and positional queries resembles Conditional DETR, and the explicit anchor design resembles Anchor DETR. This paper only makes a combination of those methods, while offering little novel insights.\n- The observations in Section 3 are almost obvious, and the reviewer did not feel that new information was brought to the table after reading.\n- The conclusion in Section 3 is somewhat weak, as the authors described \"the multiple mode property of queries in DETR **is likely** the root cause for its slow training\". And the method is proposed based on this somewhat invalid conclusion, which makes the motivation less clear, and the method less convincing. Moreover, the observations in Figure 4 are not supported by quantitative experiments, and it is difficult to give conclusions from the qualitative visualization of 4 out of hundreds of anchors.",
            "summary_of_the_review": "This paper proposes a DETR variant that incorporates dynamic anchor boxes to accelerate model convergence. Though promising result has been achieved, the method lacks novelty and the motivation is not very clear. Thus, the paper is not ready for publication. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}