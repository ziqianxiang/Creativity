{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper deals with the important topic of active transfer learning. All reviewers agree that\nwhile the paper presents some shortcomings , it is considered to be a worth contribution."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an active learning method to efficiently select data in the target domain that is suitable for learning hypotheses together with data in the source domain in domain adaptation problems. \n\nThe authors first propose to evaluate the dissimilarity between the source and target domains by localized discrepancy, where the hypothesis set is restricted to includes only hypothesithes with sufficiently small errors with the label function of the source domain on the training data (the union set of the input of the source domain and the input of the target domain to be labeled). Then an upper bound of the expected risk for the target domain based on this discrepancy. The main theoretical contribution is that the authors have shown that the discrepancy term in this upper bound is further bounded from above by the sum of the distances between the inputs of the training data and the inputs of the target domain, and this is the background for the design of the active learning algorithm. \n\nIn domain adaptation, the goal is to solve the problem of minimizing the discrepancy between the two domains. From the statements of this theorem, it can be replaced by the problem of selecting K data in the target domain so as to minimize the sum of the distances between the inputs of the training data and the inputs of the target domain. The latter problem is equivalent to solving the K-medoids problem for clustering, and the authors' technical contribution is that they have proposed an accelerated algorithm for this problem. \n\nFinally, the authors conduct an evaluation experiment of the proposed method under various scenarios of domain shift using three types of benchmark data.\n",
            "main_review": "### strength \n\n- In the conventional active learning methods, it has been the mainstream to select the input of the target domain to be labeled based on the output of the domain classifier trained at the same time (in general, the relationship between this criterion and the expected risk of the target domain is unclear). On the other hand, this paper proposes an single-shot active learning method that chooses the data to be labeled in the target domain in such a way that the expected risk of the target domain is directly reduced. \n\n- The part corresponding to the discrepancy in the theoretically derived upper bound of the target risk does not depend on the complexity of the hypothesis set (strength of Lipschitz condition) and the shape of the loss function. This has the advantage that the algorithm for active learning can be constructed in the way of hypothesis set-free or loss function-free. Actually, the proposed algorithm can be interpreted as a clustering algorithm that minimizes the sum of the distances between the data.\n\n- The proposed method, Accelerated K-medoids, is not only a method for selecting K inputs to be labeled in the target domain, but also an algorithm for clustering. In fact, it is an improved version of Greedy K-medoids method in which the computational complexity with respect to sample size is improved from n^2 to n^3/2.\n\n\n### weakness\n- In the appendix, the authors compare the empirical computational time of the Greedy K-medoids and the Accelerated one, and show the effectiveness of the latter.　On the other hand, it is not specifically mentioned how these two algorithms relate in terms of the expected risk of the target domain. If the two algorithms produce identical output, it is better to explain that, and if not, the two algorithms should be compared not only in terms of computational time but also in terms of the expected risk of the target domain.\n\n\n### questions\n- In the results shown in Table 1, the accuracy of l → h (low to high) and h → l (high to low) seem to change significantly (the latter is worse) except for random and Kmedoids. What could be the reason for this? Also, can you argue that the proposed method gives more robust results even when this phenomenon (asymmetry) occurs using other methods?\n\n- For the Localized Discrepancy, how much can we expect the situation that \"the restricted hypothesis set H^K_{\\epsilon} is not empty and the tolerance \\epsilon is reasonably small\"?\n\n- Although this paper deals with 1-shot active learning, can I understand that if we iterate this method, we can also empirically approximate the risk in the target domain and evaluate the joint error \\eta_H in the risk upper bound?\n\n- In Sec 5.1, the author claim that \"We assume that the architecture and the resulting hyper-parameters will still be appropriate after adding the queried target data to the training set\" while could you give an example of possible situations where this assumption does not hold? I would like to know how reasonable this assumption is.",
            "summary_of_the_review": "This paper deals with the important topic of active transfer learning and is considered to be a contribution to both theory and experiment.　Although there are some shortcomings in the experiments and some questions, I can generally support the acceptance of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a k-medoid solution for active learning in the context of domain adaptation. The paper builds on top of Mansour et al. (2009) and looks at the discrepancy between source and target distributions. Looking at the whole hypothesis space is very conservative since this would include hypotheses that the learner would never consider as a labeling function. In order to deal with this problem, this paper only considers localized discrepancy (Zhang et al. 2020) where we only consider the hypotheses that are epsilon away from labeling function for source domain, i.e. we are only considering labeling functions that are epsilon away from the source labeling function. Generalization bounds are derived using Rademacher average and localized discrepancy for general loss functions. From these bounds, the paper shows that one can minimize the target risk by solving a k-medoid problem. \n",
            "main_review": "Pros:\n\n1) Overall, the paper addresses an important problem and formalizes the task of active learning for domain adaptation. \n2) It is interesting to see that the minimization of the target risk maps to the k-medoid problem.\n\nCons:\n\n1) One of the main concerns is that the final algorithm used is a classical clustering algorithm based on k-medoids. K-medoids is well studied and has been used in the active learning context before and several efficient implementations of the k-center have been studied. \n\n2) In addition to datasets such as UCI and superconductivity, it would be good to show results on standard datasets such as CIFAR10, CIFAR100, and MNIST by treating one as source and the other one as target?\n\n3) While the paper says that k-medoid can be solved using greedy and provides a bound of 1-1/e it does not explain how this is achievable. One possible interpretation is that k-medoid can be seen as the maximization of submodular functions and this leads to an approximation guarantee of 1-1/e. The bound (1-1/e)-approximation only applies to the original objective function of k-medoid where we use greedy to compute all the elements of the subset. Here we use all the points from the source set and only label K elements in the target set. The authors need to carefully provide all the details of this bound as it may not directly translate from the original problem. \n\n4) There is also a close connection to the use of MMD for subset selection shown in [Kim et al. 2016]. This paper looks at the MMD measure and shows that the selection of prototypes and criticisms can be mapped to the maximization of submodular functions and solved using greedy as well. Kim et al. Examples are not enough, learn to criticize! Criticism for Interpretability, 2016.\n\n5) I find this comparison with k-center a bit vague. The paper makes simplifying assumptions such as f=f_P = f_Q and \\epsilon = 0 and shows that the bound using k-medoids is tighter than k-center in equations (5) and (6).  While max_{x’\\in T} d(x’,L_k) is higher than (1/n)\\sum_{x’ \\in T}d(x’L_k), both k-center and k-medoids are solved with greedy algorithms with approximation guarantees, and it is not clear whether such a comparison can be treated as a formal result. \n\n6) It would be good to provide more intuition on the choice of weighted k-medoids algorithm with weights multiplied with the min d(x,x’)? Typically, when we do a weighted extension, we can consider two terms in many active learning objective functions where one term models the uncertainty that looks for points near the decision boundary and the other term models the uncertainty. The paper uses uncertainty weights loosely in equation (7) without sufficient justification as to why multiplying the weights would be better than using this as an additive term. \n",
            "summary_of_the_review": "While the formalizes and motivates the proposed solution, I don't see any novelty in the final algorithm used for domain adaptation. There are also many loose ends that I feel that the authors should address. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies the problem of active learning for domain adaptation. The paper has two main contributions. First, the problem of the selection of a query target sample set is studied theoretically. A generalization bound is presented, which is based on the recently proposed concept of localized discrepancy as opposed to rather classical discrepancy measures that can be too conservative. The second contribution of the paper is an algorithm for the batch selection of K target queries, which is motivated by the theoretical findings. ",
            "main_review": "Studying the domain adaptive active learning problem on both theoretical and methodological levels in a consistent way is a strong side of the paper. On the other hand, the novelty of the methodological contribution is limited as the proposed adaptation of the K-medoids method is simply put forward as a combination of a couple of baseline strategies from previous literature. Another aspect that could be improved is the presentation of the proposed algorithm: The discussion of the proposed method is given at a very high level, without explaining, motivating or describing the involved steps in much detail.\n\nMore detailed remarks:\n\n1. In Theorem 1:\n- Please define explicitly what the distance d(x',x) stands for. \n- \"We denote by M the bound of L\": Please define this mathematically.\n- I guess the term L_P(h,f) should have been L_P(h,f_P) rather.\n\n2. In the experimental results, there are some algorithms used in the comparisons in Figure 3, but skipped in the results of Figure 2 (like BADGE, CLUE, ...). Is there a reason for this (not suitable for regression tasks, etc.)?",
            "summary_of_the_review": "While the proposed methodology is of limited novelty and not described in a self-contained manner, when considered along with the proposed theoretical framework, the contributions of the paper may be worth sharing with the community. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies active learning for domain adaption for a set of Lipschitz functions. The paper proposes to use a localized discrepancy to restrict the relevant candidate hypotheses. Under Lipschitzness, the localized discrepancy is further relaxed into a distance measure over the X domain; the authors also design an accelerated K-medoid algorithm to minimize such distance. In special cases (under certain simplifying assumptions), theoretical guarantees presented in this paper show advantages over previous ones. The proposed algorithm also shows empirical advantages over existing ones.",
            "main_review": "Strengths: \n1. The authors provide a computable way to conduct batch active learning for domain adaptation, and the method shows empirical advantages over existing ones.\n2. The authors provided an accelerated version of the K medoids algorithm, and also analyzed its computational complexity.\n3. The paper is generally well-written, and the authors also provide visual insights to help understand the paper. \n\nWeaknesses:\n1. Since the localized discrepancy is proposed in Zhang et al 2020 (as noted by the authors in the paper as well), I think it's fair to say that, on the theoretical side, the main contributions are (1) the relaxation of the localized discrepancy under Lipschitzness and (2) the accelerated K-medoids algorithm (for the relaxation). That being said, the proposed algorithms only apply to restricted cases.\n2. Following the previous point, my biggest concern is how well the relaxation is, even under Lipschitzness? The authors didn't provide a thorough comparison between bounds in Eq (3) and Eq (4). As a result, the relaxation could be meaningless if Eq (4) is much larger than Eq (3) (say, in some cases).\n3. The algorithm is restricted to the batched active learning case, where the learner picks the queried data in a single batch. What would can in the more general sequential case?\n4. I'm guessing that the requirement $\\epsilon \\geq \\eta_H$ in the theoretical guarantees is to make $H^K_{\\epsilon}$ is not empty, is that right? But I wonder why choose $\\eta_H$ as the threshold value? Seems that we could have $H^K_{\\epsilon}$ being non-empty for an $\\epsilon$ smaller than $\\eta_H$ since the definition of $\\eta_H$ has a summation of two things.\n5. In Section 4.3 the authors wrote \"... the parameter $\\epsilon$ will be small because ... This would lead to $\\epsilon < \\eta_H$ and the bound would not be valid anymore\". Isn't it true that $\\eta_H$ would get smaller as well when $H$ has a great approximation power. Also, a related question is that I thought $\\epsilon$ is a parameter that the learner gets to choose, i.e., one can simply choose an $\\epsilon$ larger than $\\eta_H$ (if it's known). \n",
            "summary_of_the_review": "Based on the strengths and weaknesses listed above, I think overall the paper is marginally above the threshold. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}