{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper analyzes a variant of the Q-Learning algorithm with two modifications: Online Target Learning (OTL), and Reverse Experience Replay (REP). OTL is essentially the same as using the target network. REP is a new modification of ER, which instead of randomly selecting samples from the buffer, replays them in the reverse order.\n\nMost reviewers are positive about this paper, so I am going to recommend acceptance. There are, however, several concerns that have been raised by the reviewers. As the authors have not revised the paper during the discussion period, my acceptance recommendation is under the good faith expectation that the authors make a serious effort in improving their work based on the reviews. Some of the concerns are:\n\n- The intuition of why REP breaks the correlation is not clear enough. This has been brought up several times by the reviewers.\n- What are the technical differences in the analysis compared to previous work such as Zou et al., 2019?\n- The kappa appearing in Assumption 4, and showing up in the error bounds, can be dimension dependent. Please clarify this and its effect on the results.\n- Much of the paper is in the appendix. It helps if the authors can include more about the proof technique in the main body of the paper.\n- Describe the relation between the error in the value function vs. the performance of its greedy policy."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the convergence of Q-learning with two popular empirical heuristics (a) online target learning and (b) experience replay. The analysis in this paper is established upon the off-policy setting along with fast-mixing and minimum reachability assumptions. Main convergence results are provided for the linear MDP where all relevant quantities (transition, reward, and Q functions) can be represented as linear functions. The authors propose two variations of Q-learning, namely, Q-Rex and Q-RexDaRe. Convergence results are also given for two algorithms with the sample-optimality guarantee for Q-RexDaRe. ",
            "main_review": "I start by saying that I agree with the motivation of the paper to study practically successful algorithms. However, I am not very convinced with the significance of the results or techniques. \n\n- The paper does not study Q-learning as it is, but uses a fixed-policy to remove correlations between Q-estimate and actions. \n\n- Algorithm 1 seems like a minor variation of variance-reduction technique. I think that the formulation of (reverse) experience-replay or online target Q-learning should be more explicitly described before Algorithm 1. \n\n- Assumption 3 with Assumption 4 is quite strong and crucial for the analysis. With this assumption, I think the analysis should be similar to previous analysis on Q-learning or SARSA (e.g., Zou et al., 2019). \n\n- In Theorem 3, by the sample complexity do you mean $\\Theta(NKB)$? ",
            "summary_of_the_review": "Overall, I agree that it is important to study practically successful and well-used algorithms (even if they do not have any particular benefits from a theoretical standpoint). However, considering the theoretical nature of the paper, I think the contribution of this paper is marginal given a long line of work in the convergence analysis of practical RL algorithms. I lean toward rejections.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper provides sample complexity bounds for a Q-learning based algorithm with target Q and experience replay.\nContributions: The authors provide an algorithm with proven sample complexity for linear MDPs and the tabular setting bridging the current state of the art. The analysis relies on common heuristics of experience replay buffer and using a target network given these some theoretical grounding.",
            "main_review": "Strengths:\n1. The paper provides solid novel analysis combining the recently proposed reverse experience replay. \n2. I could not find no typos and the writing was good.\n\nWeaknesses:\n3. Much of the paper is in the appendix (all proofs, the experiments). On the other hand, the paper holds 2 options inside the algorithm (that doesn't seem to be part of the core message but more of a technical point) and 3 algorithms where only one is given and considered in the theory. This makes things unnecessarily complex in the paper itself, while actually missing some of the more interesting complexities of the proofs, or the empirical results.\n\n4. I'm missing some understanding regarding the reverse experience replay. Usually experience replay buffers are used to reduce correlations in the updates of the Q, but when samples are taken serially backwards - won't the correlations be very strong? The observation regarding super-martingale might important here - perhaps it would be better to state it in the paper itself in a more clear way (since this does seem like part of the core of the paper). \n\n5. As a follow-up - I understand the gaps are there to make the buffers independent (also could be made clearer in the paper). But why do we need the buffers to be independent? Is this just as a technicality to make the analysis easier?",
            "summary_of_the_review": "I think the paper provides interesting theoretical results and analysis. The idea of using a reverse experience replay buffer sounds interesting for practical use, though I'm not entirely sold there as mentioned in the main review. The work is not groundbreaking but is a solid advancement in its niche.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the sample efficiency of the Q-learning algorithm in both tabular and linear setting. They propose efficient algorithms which can provably converge to the optimal policy with near-optimal sample complexity bounds.",
            "main_review": "In this paper, the authors study the sample efficiency of the Q-learning algorithm, which is a popular approach with broad applications in RL. Based on the standard Q-learning approach, they propose several algorithms called Q-Rex and Q-RexDaRe. With the mixing-time assumption and access to an exploration policy with good coverage, they prove that their algorithms can converge to the optimal policy with near-optimal sample complexity bounds. Their results highlight the benefits of online target learning and experience replay. \n\nAssumption 3 ensures that the Markov chain induced by the policy $\\pi$ can quickly converge to a stationary distribution after $O(\\tau_{mix})$ steps, and Assumption 4 indicates that the stationary distribution has a good coverage in the feature space. These assumptions simplify the problem since the agent can collect i.i.d. samples from a stationary distribution with good coverage by executing the policy $\\pi$. I wonder whether there are concrete examples in which these assumptions hold, or whether these assumptions can be further weaken with other techniques. Besides, I remember that the lower bound proposed in Azar et al. (2013) is proved in the generative-model setting without such assumptions. In other words,  the lower bound in Azar et al. (2013) may not hold with further assumptions in this work, which means that the sample complexity of Q-RexDaRe may not be minimax optimal as claimed by the authors.\n\nThe sample complexity (regret) of Q-learning algorithm in the tabular setting has also been studied by Jin et al. [1]. They obtained the near-optimal sample complexity $O(H^4SA/\\epsilon^2)$ in the online-exploration setting without access to the exploration policy $\\pi$ or any generative model. More comparison between the tabular results in this work and that in [1] is necessary in the related work section.\n\nminor comments:\n1. The parameter $\\kappa$ may implicitly depends on the feature dimension in the linear setting. Therefore, the sample complexity bounds in the theorems still have implicit dependence on the dimension $d$.\n2. Several techniques in this work have also been used in the algorithmic design and complexity analysis of previous related results. For example, the design of gaps with length $u$ between buffers has been applied in [2] to obtain near-optimal regret in the infinite-horizon average-reward setting, and the benefits of experience replay has also been observed in [3] for policy optimization methods. It would be better if these relevant references are added in the paper.\n\n[1] Is Q-learning Provably Efficient?\n\n[2] Model-free Reinforcement Learning in Infinite-horizon Average-reward Markov Decision Processes\n\n[3] Cautiously Optimistic Policy Optimization and Exploration with Linear Function Approximation",
            "summary_of_the_review": "Overall, the paper is clear with solid theoretical bounds. Their results show that the Q-learning algorithm with online target learning and experience replay can provably converge to the optimal policy with near-optimal sample complexity. One concern is that the assumptions may simplify the analysis of the problem. Besides, more discussion on the related works needs to be added.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper connects Q-learning with online target learning and reverse experience replay and also obtain the result for linear MDPs. Those results are the first of its kind and are shown to be near-optimal in their respective regime. In particular, for asynchronous setting the Q-REX has order $\\frac{|\\mathcal{S}||\\mathcal{A}|}{\\epsilon^{2}(1-\\gamma)^{4}}$ and Q-REXDARE has order $\\frac{\\max \\left(\\bar{d}, \\frac{1}{\\epsilon^{2}}\\right)}{\\mu_{\\min }(1-\\gamma)^{3}}$. The latter one nearly matches the standard lower bound.",
            "main_review": "Strengths: This paper studies the sample complexity of asynchronous learning. Especially, the new algorithm Q-REX achieves the sample complexity $\\frac{|\\mathcal{S}||\\mathcal{A}|}{\\epsilon^{2}(1-\\gamma)^{4}}$ and Q-REXDARE achieves complexity $\\frac{\\max \\left(\\bar{d}, \\frac{1}{\\epsilon^{2}}\\right)}{\\mu_{\\min }(1-\\gamma)^{3}}$.\n\nWeaknesses:\n\n1. Although Q-learning is known to be sub-optimal, the variant of Q-learning are optimal in a couple of setting. For instance, [1] in online setting and [2] in finite horizon offline setting. Since Q-REXDARE is also a variant of q-learning, I think the improvement is a little bit limited comparing to [2] (which uses variance-reduction for Q-learning with $d_m\\approx \\mu_\\min$). I do notice the there is a theorem 1 which holds for linear MDPs, but I cannot see whether this is optimal. (By the way, the paper did not include the related literature [1],[2])\n\n[1] Zhang et al., Almost Optimal Model-Free Reinforcement Learning via Reference-Advantage Decomposition, NeurIPS 2020,\n[2] Yin et al. Near-optimal offline reinforcement learning via double variance reduction, NeurIPS 2021.\n\n2. Another major weakness is the current algorithm can only guarantee Q-function learning, which cannot guarantee $\\epsilon$-optimal policy with the same complexity since it is known $\\epsilon$-optimal Q-function learning can only imply $(1-\\gamma)^{-1}\\epsilon$-optimal policy. Therefore, the current result cannot guarantee $\\epsilon$-optimal policy with near-optimal sample complexity.\n\n\n",
            "summary_of_the_review": "Overall, due to the weaknesses I mentioned above, I think this paper does not cross the bar due to the high selective of ICLR. Since the theorems are correct from my point of view, I will choose weak reject.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}