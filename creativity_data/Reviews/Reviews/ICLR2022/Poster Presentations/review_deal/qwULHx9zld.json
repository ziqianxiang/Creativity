{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The reviewers overall were quite happy after the rebuttal phase, in which the authors considerably improved the presentation quality and addressed reviewer concerns, and recommended acceptance. The reviewers agreed that while the theory was short and relied on various possibly restrictive assumptions and maybe was largely an improvement in constant factors, it extended prior work (some of which was in ICLR) and was interesting and motivated the experiments which were notably faster than existing methods."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors present a resource-efficient approach for approximating kernels using random feature transformations. The random feature approach was introduced in a seminal paper by Rahimi and Recht, where each random feature is extracted from the input $x$ as $\\sigma(\\langle w, x\\rangle )$, for a random (appropriately distributed) vector $w$ and an activation function (e.g. sinusiod) $\\sigma$. In this paper, the goal is to save memory and time by restricting $w$ and $\\sigma$ to take values in {$-1,0,1$}, meaning that $\\sigma(\\langle w, x\\rangle )$: \n1. can be evaluated only with additions/subtractions (no multiplications) and\n2. only takes $1$ bit of storage. \n\nThe authors prove that under some conditions, their kernel estimate approaches the true kernel at the limit where the number of features and the number of data points are comparable and approach infinity. The assumptions are roughly:\n1. The data follows a Gaussian mixture model,\n2. the input features are approximately pairwise orthogonal,\n3. the entries of $w$ are chosen i.i.d. and have bounded fourth moment, and\n4. some boundedness assumptions on the generalized Gaussian moments of $\\sigma$.\n\nThe theoretical result is accompanied by numerical experiments on various tasks like kernel ridge regression and SVM on both real and synthetic datasets. The results demonstrate a significant advantage of the authors' approach compared to previous quantized random features approches, both in terms of runtime and memory, without a significant drop in accuracy.",
            "main_review": "I like the ternary quantization idea and it seems to work great in the practical experiments. It seems to give roughly a 2x speedup and 8x memory decrease compared to RFF and other kernel methods, which is quite impressive. The theoretical results are interesting but they work under quite restrictive assumptions.\n\nI have concerns on the quality of presentation of the theoretical results. In particular, I am sure the statement of Theorem 1 can be made clearer. For example, the quantities defined after (8) seem to be in somewhat random order which makes it hard to read. Also, it would be good to flesh out what is the main *technical* insight of Theorem 1 compared to previous work. More importantly, I was not able to follow the proof of Theorem 1. My suggestion is to add more text to explain what is happening and re-write equations like (19) in a way that is easier to parse. Also, I don't see how Corollary 1 follows from Theorem 1. E.g. where is (10) coming from? but also there is a lot of explanation missing here since the approximate kernels in Theorem 1 and Corollary 1 are very different.\n\nI would like to see a discussion about the assumption that the entries of $w$ are i.i.d. In particular, when (and why) should I expect this assumption to hold?\n\nI didn't see any discussion on the minimum number of random features $m$ needed. Is this not necessary in the analysis?\n\nWhat is the time to compute the random features? It seems like it has some non-trivial parts like computing $\\hat{s}^+$, $\\hat{s}^{-}$, so it would be good to have a short discussion on this (with some sample runtimes), and also explain a bit more how $\\hat{s}^{+}, \\hat{s}^{-}$ are computed.",
            "summary_of_the_review": "In summary, this paper is mostly well-written except for the theoretical parts which need some work. The idea is really interesting and relevant for resource-intensive applications. The experimental results look very promising. I am leaning towards acceptance, but I might change my rating based on if my concerns have been resolved.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a way to approximate common kernel matrices in a way that is (1) Sparse (2) Low Bit-Complexity (i.e. the nonzero entries have few bits) (3) efficient to compute.\n\nThe theoretical justification from this model comes from a random matrix theory (RMT) analysis of kernel matrices for datasets of vectors drawn iid from a fairly general gaussian mixture model. The RMT analysis shows that the true kernel matrix is asymptotically distributed as the approximation described above. No non-asymptotic results are given.\n\nThe introduction mentions deep networks, but this paper really has no strong connection to deep learning. View it as a kernel paper.\n\nA good amount of supporting experiments are given to demonstrate that the approximate kernel matrix has the same statistical performance as (eg) a Random Fourier Features approximation.",
            "main_review": "# Quality and Clarity\n\nThe paper is well written and clear. It was easy to follow. I'm not a fan of purely kernel oriented papers discussing deep learning in the introduction if that's not significant to the paper, but that's a personal subjective preference. Good marks for quality and clarity\n\n# Theory\n\nThe core theoretical claim is compelling in its generality and formality. While it wouldn't be enough to carry the paper on its own, it serves as a nice motivation for the TRF algorithm proposed. There's really only one theoretical claim in the paper, but that claim is all they need to show. It's well stated, clear, and to the point. Elegant.\n\nAmongst the theory, the only clarification I would like to see is why the estimator $\\hat\\tau = \\frac1n \\sum_{i=1}^n \\|x_i\\|^2$ is asymptotically correct. I assume this follows from some simple properties of the Gaussian distribution, but it would be nice to see at least this discussed in the paragraph before _Algorithm 1_.\n\nAs a side note: while I didn't review the full proof of Theorem 1, glancing at the appendix, the proof seems surprisingly (in a good way) simple and approachable for a RMT proof. Props.\n\n# Experiments\n\nThe theory in this paper serves as a motivation for the TRF algorithm which approximate the kernel matrix. Unlike most work on random features, this paper's theory requires data to be drawn from a gaussian mixture model (GMM). Since most theory of kernel approximations don't require this assumption, in my view, this paper lives and dies by its experiments: is the GMM essential to the proposed approach, or is it a mathematical simplification to make the math more tractable and motivation a pragmatic algorithm?\n\nThe experiments are largely convincing. There's several points I would very much like to see (frankly I expect to see) fixed up in a camera-ready version. But overall, the experiments are compelling, so I think the overall paper works well. This suggests the GMM assumption in the theory was just a simplification to make the math tractable.\n\nOverall, the experiments compare three statistics: running time, space complexity, and the statistical performance of the resulting kernel matrix. The proposed TRF algorithm is compared usually against classical Random Fourier Features (RFF), and sometimes against a few other kernel approximation algorithms.\n\nThe experiments suggest that TRF is notably faster and uses less space than RFF. Further, they can tune a sparsity parameter to tradeoff the accuracy of the kernel matrix with the space of the kernel matrix. Some interesting takeaways from these experiments:\n- A very sparse kernel sometimes does not impact accuracy a lot, and sometimes is very important (figures 5, 6, and 7 show sparsity control accuracy for only moderate amounts of regularization). Is there a rational about _when_ sparsity impacts accuracy a lot?\n- Sparsity helps speed up computation, but not a huge amount. Going from a $90\\%$ nonzero matrix to a $10\\%$ nonzero matrix can shave off $\\frac13$ of the computation time. It's a good boost, but maybe bad value if this can increase error by $50\\%$?\n    - The $50\\%$ number comes from Figure 5, $\\gamma\\approx5$, where the triangle has MSE $0.5$ and the $\\otimes$ has error $0.75$.\n- The running time improvements are especially encouraging across the board.\n\nThese experiments are strong enough (suggesting that the theory -- as they conjecture -- is more general than the GMM setting) that I support clearly accepting the paper.\n\nThat said, here are the changes I would like/expect to see in an experiment driven paper like this:\n1. Better baselines. We see most plots use classical RFF as a baseline, but this doesn't always make sense. In Figure 3, we see the $\\varepsilon=0.1$ data fall below the baseline MSE, suggesting that TRF outperforms RFF. That makes for a weird baseline. I would like to see the true optimal MSE, computed without any random features at all. I would like to see this on all the statistical efficiency (MSE and accuracy) plots.\n    - This will add a good deal more of valuable information: like in Figure 3 on the left plot, we see a gap between all TRF models and the RFF model. Is the RFF model close to the true baseline, or do TRF and RFF both have a large gap from the baseline? I understand there is some approximation error, but I don't understand if there's a lot of approximation error.\n1. Confidence intervals. TRF and RFF are both randomized algorithms. RFF has many well studied concentration inequalities, so I'm reasonably confident it's well concentrated. I don't know that about TRF though. What do the $25^{th}$ and $75^{th}$ quantiles look like? Also, how many repetitions did you run these algorithms for?\n1. For Figures 5, 6, and 7, where there's a lot of series of data, these plots are too hard to read. I would recommend reducing the number of series (i.e. drop $\\varepsilon=0.3,0.7$) to make the lines more legible when the confidence intervals are added, and to color code the series instead of using different markers.\n\n---\n# Extra Tidbits\n\n## Some Technical Questions I Have\n1. In the experiments, you mention using 32 bit floats for Nystrom approximation. To reduce space complexity, can you just perform the same computations with 16 bit floats or 8 bit floats. Does this have a disastrous statistical impact, or could we close the gap between TRF and Nystrom a bit, very easily?\n1. Are you aware of Michael Mahoney's work on Random Matrix Theory for deep learning? I read one or two of those papers a while back, and they might also fit into this world of RMT to understand efficient estimators (e.g. in the end of your Section 1.3)? Not totally sure, and I don't want to force you to add in a reference to an unrelated paper. Just want to bring up a possibly interesting connection existing in the literature.\n1. When does TRF give better statistical performance than RFF? (Figure 3 left image, large $\\gamma$ and small $\\varepsilon$)\n\n## Typos and small recommended edits\n1. [Page 2 footnote] \"is equivalent to _centering_ the data\" not \"center\"\n1. [Figure 4] Replace \"$0.5 \\cdot 10^{-2}$\" with \"$0.05$\"\n1. [Figures 11-15] Mention in the captions that the y-scale is zoomed in a lot. Explicit is better than implict.\n1. [Figure 12] Add the dataset name to the caption. I know it's mentioned in the text of the appendix, but it should be here too.\n",
            "summary_of_the_review": "The theory is a compelling motivation for a proposed matrix approximation algorithm that seems to experimentally work well.\n\nThey should clean up their experiments a little, but this is overall encouraging as a pragmatic way to reduce the time and space complexity of real kernel algorithms by large constant factors.\n\nI recommend this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concern.",
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "With Gaussian mixture assumption for input data, this paper proposed a sparse random features approach where the approximated kernel is independent of the iid weights and depends on Gaussian moments of the activation functions. This paper provided a theoretical guarantee for the asymptotical equivalence with the centered kernel. And then, the authors derived the computationally efficient random features and devised a simple algorithm. Finally, they validate the accuracy and efficiency of the proposed random features by several experiments.",
            "main_review": "Strengths:\n1)\tThis paper provided self-contained theoretical guarantees for the asymptotic equivalent of the primal kernel and a delicate form of ternary random features. The asymptotic results are interesting, and the proofs seems to be correct.\n2)\tThis paper designed a simple and efficient framework to construct ternary random features. Due to the sparsity of ternary random features, the algorithm is benefit from both high computational efficiency and lower storage complexity.\n3)\tThis paper also provided experimental validations by comparing the training time, memory costs and MSE with SOTA methods. The empirical results coincided with the theoretical finding, that the proposed approach characterized both computational and storage gains.\n4)\tThe writing of this paper is clear, and most parts are easy to follow.\n\nWeakness:\n1)\tMy main concern is that the Gaussian mixture data assumption may be too strong. As shown in Eq. (7), this paper assumed the inputs are under multivariable Gaussian distribution. Many real-world datasets break this condition, such as long-tailed distribution. Moreover, the classic random features, for example random Fourier features, have no restriction on data distribution. Besides the Gaussian data assumption, Assumption 1 (iii) also seems to be strict. The authors are expected to provide more examples to illustrate the applicability of these assumptions.\n2)\tThe presented asymptotic theory requires the dimension of input space to approach infinity $p \\to \\infty$ is unfamiliar in practical. Because the dimension of input space is fixed, I wonder that is there still a good approximation between $K$ and $\\tilde K$ if $p$ is small? For a given task with a fixed $p$, is there a natural gap between $K$ and $\\tilde K$?\n3)\tIt seems both Theorem 1 and Corollary 1 are independent from the required number of random features $m$. In the existing random features literature, $m$ is crucial to the approximation ability and generalization ability. In general case, $m=O(\\sqrt{n})$ random features can guarantee the similar generalization ability (Rudi and Rosasco, 2017). The authors may illustrate how the number of ternary random features influence the approximation or generalization.\n4)\tThe kernel hyperparameters usually determine the performance of kernel methods, but the proposed random features approach seems to be independent from kernel hyperparameters and only depend on the kernel type. Can TRF approximate any kernel with different hyperparameters? How does TRF remove the influence of kernel hyperparameters?\n\nRudi A, Rosasco L. Generalization Properties of Learning with Random Features[C]//NIPS. 2017: 3215-3225.\n",
            "summary_of_the_review": "This paper provided the sparse random features with theoretical guarantees, efficient algorithm and sufficient experimental validations. However, this paper assumed the input data with Gaussian mixture that may limit its applicability. Meanwhile, the influence of the number of random features has not been explored well.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies random features method, which randomly projects the data onto a low-dimensional space, while at the mean time approximates the original kernel structure. \nSome contributions of this paper:\n- The authors show that, under the Gaussian mixture model, in the high-dimensional limit, in the spectrum sense, the random features-type kernel (defined in equation (1) of the paper) only depends on the first two generalized Gaussian moments of the activation function (with the assumption that the random projection weights are i.i.d. from some mean 0 and var 1 distribution).\n- Based on the above result, the authors propose a special choice of the random weights and the activation function, that could be efficiently computed and needs less memory to store.\n- The authors provide empirical evidence showing that the proposed method performs as well as other random features methods, and with advantage in computation and storage.",
            "main_review": "This paper is well organized. I like the idea of the paper to exploit the theoretic result (in the high dimensional limit the spectrum of the kernel only depends on the first two generalized Gaussian moments of the activation function) by coming up with a sparse choice of the activation function and projection weight's distribution, that could make the random features computationally efficiently and easier to store.\n\nThe main concern is the importance of this finding (have a constant scale improvement in computation time and storage memory). I believe the proof technique of the main theorem is not original, so the contribution mainly come from the gains in computation and storage. I feel the authors should spend more space to argue that.\n\nSome minor issues:\n- In page 7 the authors argue that, the computation of the proposed random feature requires no multiplication and only $O(\\epsilon mnp)$ additions. Could there be more explanations around this claim? For example, (1) in the case $\\epsilon != 0$, why there is no multiplication needed, (2) in the case $\\epsilon = 0$, does the proposed random feature requires no multiplication and no additions? Or should the needed additions be $O((1-\\epsilon) mnp)$ instead since $1-\\epsilon$ proportion of the weights are non-zero?\n\n-------------- After revision ---------------\n\nThanks the authors' response. After reading the response and other reviewers' comments, I agree that the constant scale improvement in computation time and storage is a good contribution, and I raise my score to 6.",
            "summary_of_the_review": "My biggest concern is how important the main contribution is (have a constant scale improvement in computation time and storage memory). To be honest I'm not sure how to measure that, and thus will lower my confidence score.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}