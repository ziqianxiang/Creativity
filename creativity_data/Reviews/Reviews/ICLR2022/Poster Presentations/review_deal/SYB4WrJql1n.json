{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "Dear Authors,\n\nThe paper was received nicely and discussed during the rebuttal period. The current consensus suggests the paper be accepted, but could have another round of revisions before it gets published\n\n- Definition of sparsity within theoretical results + clarity of results. This seems to be the main concern by one of the reviewers. \n- The reviewers acknowledged that some of the concerns raised could be found somewhere in the appendix, which raises further the concern of the presentation of the results: reviewers suggest a more focused and proper dissemination of the results (main theorems in main text + explanation of the results obtained, etc), which requires another round of revisions and reviewing. \n\nBest AC"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper gives proof of the existence of universal lottery tickets. Specifically, a definition of universal lottery tickets is proposed, and then proved based on functions approximation results for polynomials, Fourier series using neural networks. \n",
            "main_review": "I think this paper provides some interesting ideas on how to study the universal lottery ticket hypothesis. However, I think the paper may have some significant limitations.\n\nThe most important issue I find in this paper is that the results are not rigorous enough and the sparsity level of the lottery ticket is not addressed properly. This paper only has an informal theorem in the main paper, and has more detailed results in the appendix. However, after going over them, it is still not very clear to me what exactly is the sparsity level of the universal lottery ticket that is proven to exist. This is very important, as without guarantees on sufficient sparsity, one can just set the mask $B$ in Definition 2 to be all ones to use the original network as a lottery ticket of itself. Moreover, the authors set a sparsity level of 0.5 in the experiments. I feel that this may not be sparse enough. When the sparsity is just at a constant level, it seems that we can simply randomly delete a constant proportion of the neurons in each layer of the network, and then the universality of the lottery ticket can still be proved based on the universality of random feature models.\n\nTheorem 1 is also a bit confusing. According to Definition 1, if $\\lambda f_{\\epsilon}$ is strongly universal, isn’t $f_{\\epsilon}$ also strongly universal? If this is the case, it seems pointless to introduce $\\lambda$. \n\nThe result of this paper is also a bit incremental, as it is mainly a combination of known results on using polynomial/Fourier bases to approximate smooth functions and using neural networks to approximate polynomial/Fourier bases.\n\nIn terms of the presentation of the results, the authors may consider presenting formal theorems in the main paper, and add discussions to justify the results.",
            "summary_of_the_review": "For the reasons listed above, I think the current paper needs some further improvement. Therefore I would like to recommend rejection.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proves that large enough randomly initialized ReLU networks can contain subnetworks, which can act as universal lottery tickets, that is, only by tuning the last layer of these subnetworks, they can approximate any (reasonably well-behaved) function. The paper does this by showing that large enough randomly initialized ReLU networks contain subnetworks such that their last hidden layer can act as the basis for a class of functions, and hence the output layer can simply take a linear combination of the neurons of the last hidden layer to approximate any function in that class. The paper proves this for polynomial and Fourier basis. Another key contribution of this paper is that it demonstrates that large depth can be leveraged to reduce the width needed for the strong lottery ticket hypothesis to hold.",
            "main_review": "As written in the summary above, this paper provides some novel results and intuitive ideas to show the existence of universal lottery tickets - lottery tickets which can be slightly tuned to approximate any function in a function class. However, there are two main concerns that I have:\n\n1. Looking at the theorem statements in the appendix, the parameter $N$ in Theorem 1 might be of the order $dk/\\epsilon$. In that case, the required width $n_{l,0}$ becomes super linear in $d$ and $k$, and no-longer remains logarithmic in $\\epsilon$. This would be bad, particularly since modern ML applications have huge dimension $d$.\n\n2. The paper only talks about approximating the functions in the basis $\\mathcal{B}$, whereas the more interesting case is approximating classes of functions such as Lipschitz-continuous functions. However, I think, applying this paper's results directly for that setting would lead to the classic 'curse of dimensionality', where the basis would need to contain $k=(h(1/\\epsilon))^d$ functions, where $h$ is some monotonically increasing function. While this might be unavoidable, this does put the point of universal lottery tickets into question, that is, can there exist practical, small-sized universal lottery tickets in high dimensions? \n\nNOTE: I might be missing something important here in the two points above, so the authors should feel free to correct me.\n\nOther than this, I think the main paper (first 9 pages) is not able to explain a lot of important aspects and I had to go through the appendix to understand them. One suggestion could be to just focus on one of the bases (say polynomial or fourier) in the main paper, and defer others to the appendix. This would give the readers a more direct understanding of how the construction works.",
            "summary_of_the_review": "The paper provides some novel results and intuitive ideas to show the existence of universal lottery tickets, and shows that depth can be used to reduce the width required for strong lottery ticket hypothesis. However, I have some concerns regarding the required width of the network, which might limit the overall practicality of the results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper develops a theory to explain the lottery ticket hypothesis based on extensions of subset-sum results and a strategy to leverage higher amounts of depth. The developed proof could be of independent interest since it leverages the power of depth and improves the existing bounds.",
            "main_review": "Overall this paper is technically sound and well organized. Besides, my comments are as follows:\n\n---\n\nBy looking at Theorem 1, it seems that the major contribution compared to existing works (in terms of the theoretical results) is that this paper does not require the mother network to have depth $2L$ to approximate the target network with depth $L$, but could be of any depth, at the price of using more neurons per-layer. Could you elaborate on this? It would be better to clarify this technical difference in the introduction.\n\n---\n\nBesides, theorem 1 states that the universal approximation is taken with respect to the function family $F(B)$. But as claimed by the authors, functions in $F(B)$ are not exactly the target neural network function $f$. There seemly has a gap since the lottery ticket hypothesis states that a larger network with pruning can approximate a smaller network. Perhaps the authors state this approximation results with respect to the target network somewhere, but it could be much better if you can directly highlight that as a theorem or corollary.\n\n---\n\nCould you also specifically show the case when the mother network has a smaller depth in the main section? This case is rather interesting to me since existing works cannot cover it.\n\n---\n\nI do not quite get the illustration in Figure 1. It seems that the authors want to show how to develop an architecture to approximate a monomial. However, as stated on page 4, g(x) and h(x) are exponential and logarithmic functions, which I am not clear how to use FC-net and Conv-net to approximate (where we only have linear transformation and ReLU activation).\n",
            "summary_of_the_review": "The authors may need to carefully address my concerns and questions, or I am not sure whether this paper is of good quality or sufficient importance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the existence of universal lottery tickets, a stronger version of lottery tickets that allows transfer learning.\nThe key idea is to show that, by pruning a network, we are able to approximate a finite set of universal base functions.\n",
            "main_review": "Overall, I think the paper is well written and studies an interesting direction. However, my main concern is the significance of the result.\n\nAs shown in Theorem 1, the number of neurons is linearly proportional to the number of base functions |B|. Such linear seems to make the result a bit trivial than expected. Specifically, I feel the theory is essentially doing the following thing:\n1. Given a set of base functions, for each function, we prune a randomly initialized network for approximation.\n2. We then concatenate all the randomly pruned networks together (allowing no interaction between those networks in each layer), which gives the universal lottery ticket network.\n\nI didn’t dig into the details of the proof but I think following the above strategy we will end up with the same rate shown in this paper with a linear dependency on k. \nOverall, it seems this paper utilizes the above way for construction with additional effort to deal with the scaling issue, which is not difficult. ",
            "summary_of_the_review": "Overall, I think this paper studies an interesting direction. However, due to some concerns on the significance of the result, I am not sure whether the result is above the acceptance bar of ICLR. I currently lean to give a weak rejection but I am happy to adjust the score based on the authors' response.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}