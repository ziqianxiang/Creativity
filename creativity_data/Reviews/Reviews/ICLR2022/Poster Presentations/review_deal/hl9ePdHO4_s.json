{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The manuscript develops a new and simple graph neural network architecture. The proposal make use of only O(V) (number of vertices) rather than O(E) (number of edges, meaning that it may be useful for scaling to larger problems. The didactic figures are especially clear, and as is shown in Fig 1 the proposed architecture passes messages based only on the source vertex rather than based on source and target. This challenges common ideas in the field that passed messages ought to reflect a function of both source and target. In spite of this introduced simplification, the architecture performed better than or as well as a set of strong baselines on a set of 6 datasets. The manuscript also examines latency and memory consumption, showing that the methods comes out favourably in this regard.\nOne of the reviewers worries that the paper does not directly provide a solution to scaling network training to very large graphs; they note that several of the datasets that are examined do not contain large graphs. This is true, but the paper does not overclaim in this regard, and I agree with the majority of reviewers that the manuscript is worth publishing on the basis of having developed a well-performing approach that challenges the accepted assumptions in the field. While it may not be a direct solution, the counterintuitive results may help point the direction toward development of simple, effective approaches that do scale up."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper claimed they designed a new GNN architecture that achieves state-of-the-art performance with lower memory consumption and latency. More specifically, the proposed model uses memory proportional to the number of vertices in the graph $O(V)$, in contrast to competing methods which require memory proportional to the number of edges $O(E)$. The paper claimed that the new architecture enabled each vertex to have its own weight matrix, thus following a novel adaptive filtering approach. The experiments found that the proposed efficient model could achieve higher accuracy than competing approaches across six large and varied datasets against strong baselines. Moreover, the experiments demonstrated that the proposed method achieves lower latency and memory consumption for the same accuracy compared to competing approaches.",
            "main_review": "### Strengths:\n1. This paper applied the idea of \"basis weights\" and thus effectively uses different weight matrices for different nodes.\n2. This paper has provided a long section to help interpret the proposed architecture and compared it with GCN, GAT, and PNA.\n\n### Questions:\n1. Since it is noted that different aggregators might result in outputs with different means and variances, for the experiments in Table 1, did you normalize the combination weights $w$ or the different aggregated outputs?\n2. I am curious how you implemented the \"weighting of bases\" per node, i.e., multiplication between the combination weighting coefficients $w$ (different for different nodes) and the aggregated outputs corresponding to different basis weight matrices? Is this step before or after the aggregation? Although the theoretical complexity of this step is $O(V)$, I think the choice of actual implementation might significantly affect the practical time efficiency.\n\n### Weaknesses:\n1. The major weakness of this paper, in my opinion, is that this paper seems to have misunderstood the real bottleneck to scale-up GNNs, and the proposed architecture may not be suitable for large graphs (i.e., size over a million).\n   - The paper has compared with many existing approaches to solve the scalability issues of GNNs on large graphs. However, this paper did not tackle this problem. Firstly, the theoretical complexities of the proposed architecture are still $O(V)$. Thus on a graph with more than a million nodes (which happens in some node classification or link prediction benchmarks), the memory consumption is still too large for conventional GPUs. Secondly, the main experiments (Table 1) are conducted on four graph regression/classification benchmarks and only one node classification benchmark. However, the graphs in the four graph-level benchmarks are small. The experimental exploration and evidence on large graphs are insufficient.\n   - For graph sampling approaches, the paper claimed that `We evaluated a variety of sampling approaches and observed that even modest sampling levels, which provide little benefit to memory or latency, cause model performance to decline noticeably.` The corresponding experimental results are shown in Table 8 in the appendix. However, all four benchmarks used in Table 8 are graph-level tasks (graph regression or classification), where the sizes of input graphs are small. For example, ZINC and MolHIV are molecule graphs, and CIFAR consists of super-pixel sampled graphs whose sizes are at most 150 (Dwivedi et al., 2020). Applying sampling strategies to these small input graphs is not reasonable, and it is not strange to observe significant performance degradation. Actually, the performance of some sampling algorithms on relatively large graphs for node classification is strong, e.g., GraphSAINT (SAGE aggregator) can outperform the \"Full-batch\" GraphSAGE on *ogbn-products* (see [leaderboard](https://ogb.stanford.edu/docs/leader_nodeprop/)). The quoted sentence above is not appropriate.\n   - To sum up, I think the architecture proposed in the paper does not solve the scalability problem on large graphs. Thus the authors should make this point clearer in the related work and experiment sections.\n2. Another potential problem, in my opinion, is the spectral interpretation in Section 4.2. The paper claims that `Our approach corresponds to learning multiple filters and computing a linear combination of the resulting filters with weights depending on the attributes of each node locally.` The paper compared two formulas: (1) the ideal linear combination of filters: $y=\\sum_{b=1}^Bw_b\\odot g_{\\theta_b}(\\mathbf{L})\\mathbf{X}$; and (2) the filtering of proposed model (correspond to EGC-S, Eq.(2)) $y=\\sum_{b=1}^Bw_b\\odot (\\tilde{D}^{-\\frac12}\\tilde{A}\\tilde{D}^{-\\frac12})\\mathbf{X}\\Theta_b$.  However, I think the latter formula is not a combination of multiple filters. There is only one filter $(\\tilde{D}^{-\\frac12}\\tilde{A}\\tilde{D}^{-\\frac12})$ (which is left multiplied to $\\mathbf{X}$) and the weight matrix $\\Theta_b$ right multiplied to $\\mathbf{X}$ should not change the relative magnitude of signals of different frequencies. It is not the same if changing $g(\\mathbf{L})$ in the former formula, where the matrix left multiplied to $\\mathbf{X}$ changes.",
            "summary_of_the_review": "This paper proposed a novel architecture to linearly combine the aggregated outputs using different weight matrices for each node. The model is memory efficient since the memory consumption is $O(V)$ instead of $O(E)$. However, the proposed method did not solve the scalability problem of GNNs when applied to considerably large graphs. And it is unclear when the provided memory efficiency is necessary, and thus the proposed method is the first choice. In terms of performance, the improvements compared with PNA are also not very convincing. The significance of this paper might be limited if the efficiency/performance improvements are marginal. I would encourage the author to explore the theoretical understanding of the proposed architecture further. Currently, section 4 is well-written, but the conclusions are limited and may have some flaws. In general, I could not recommend the current manuscript for acceptance.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces Adaptive Filters that enable some of the benefits of Message Passing architectures, but while maintaining a memory consumption that scales with the number of nodes.\n\nThe authors claim that this architecture is not only better performing, and use less memory, but more efficient for GPUs through the use of sparse matrix multiplication.\n\nThe idea of the model is that you have a number of filters (MLPs or linear layers) applied to each sending node latent, a \"filter\". These are then weighted and summed by a weighting vector calculated as a function of the receiving node. Since there are no functions that take as input both sending, receiving or edge inputs, the memory will scale with the number of nodes. You, in essence, get an efficient pseudo-attention mechanism. \n",
            "main_review": "Strengths:\n- The paper conducts a thorough analysis and comparison of alternative approaches and describes how some of the motivations behind the alternatives can be gained without the memory costs.\n- The paper conducts a thorough analysis on latency, which I appreciate.\n- I like that the authors consider hardware - in many cases, more efficient algorithms in FLOP terms are less efficient because they are not parallelisable. This is an important point and I encourage work in the GNN literature that addresses this.\n- The results are better than the baselines, which are over a number of different graph networks. I think the model description is quite general.\n\nWeaknesses:\n- To state the obvious, such a network is only really applicable to data where you do not need to consider edge features explicitly. While this is fine in principle, the paper doesn't really make this distinction and so the claim is probably broader than the evidence supports.\n- The text seems to imply that the computational efficiencies derive from the use of sparse matrix multiplies. But, many new accelerators do not support sparse matmuls well at all, instead are very strong on dense compute. This distinction should be clearer in the text - this is a *GPU* optimised GNN, not an accelerator optimised GNN (perhaps a more parallel GNN would count for something more general purpose).\n- I do not think these results are state of the art - simply that they are better than the baselines reported. Please remove the phrase state of the art.\n- Whilst I understand that this method is different from the literature, it is *very* related to concepts such as attention. Thus, I think the impact may be limited.\n\nSome nits: It would be nice to have more detail on the OGB and Zinc results - perhaps explain why this model does not perform as well as MolHIV.\n\n\n\n",
            "summary_of_the_review": "I think this is a thorough piece of work, but I am concerned that the impact is limited for the following reasons:\n- I am not convinced that the results presented reflect the current state of of O(V) scaling GNNs. You cite \"Training graph neural networks with 1000 layers\" which I believe scales O(V), and I think achieves better results on OGBN-ARXIV. \n- The ideas seem very related to existing work. This isn't to suggest that the ideas are identical, more that the contribution appears to be incremental.\n\nThat being said, I think the analysis is beneficial to the community so give it a weak accept.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose a different formulation for a graph neural network, focused on achieving accuracy equal to anisotropic GNNs without anisotropic mechanisms.",
            "main_review": "This paper is a mixed bag. On one hand, I appreciate the authors' vision: take some things we've learned about GNNs (heads, aggregators), revisit first principles (isotropic GNNs), try to come up with something new that blends them. In addition, I appreciate their take on computational efficiency: they correctly seize upon the notion that a well-understood algorithmic kernel can have meaningful computational impact, and they build on its advantages (e.g., section 3.2-aggregator fusion). On the other hand, many of the conclusions leave something to be desired. The analyses and descriptions sometimes gloss over important factors, and I wish the results delivered a stronger takeaway to match the ambition. For all the talk about computation and memory efficiency, the actual numbers are not all that substantial.\n\nStrengths:\n\n- The authors' direct challenge to accepted folk knowledge about anisotropic GNNs is healthy and welcome. As models evolve over time, it is important for the community to revisit fundamental positions on model components. The paper opens a lot of areas for other researchers to build off it, which I see as a major strength.\n- The proposed model is clearly explained and fairly easy to follow. The intrepretations are useful and provide good intuition for how to place their proposed model in the context of other current work.\n- The appendices actually provide a lot of useful content, sometimes more than the paper itself. The rationale in appendix C, for instance, was far clearer than the paper's 'aggregator fusion' section in 3.2 (even without the algorithm).\n\nWeaknesses:\n\n- Some of the evaluation results are weak. The accuracy-normalized results in seem to demonstrate that EGC-S  is not all that much faster or smaller than GCN. It also seems to indicate that parameter-normalized results do not correlate particularly well with memory footprint across models, which weakens the argument for O(V) vs. O(E).\n- Despite a nominally primary argument of the paper being about memory efficiency and computational performance, very little quantitative results support that argument. I agree that there is definitely a lot of potential for this approach to be efficient, but I wish the paper had strong support for it.\n\n- The authors fail to disambiguate different types of memory effects. For instance, while high-watermark memory footprint (which is mostly what is measured in their experiments) is important, they overlook the effects of parallelism. This leads them, amongst other things, to dismiss sampling methods almost out of hand. The memory behavior of sampled methods is vastly different than non-sampled GNNs when dealing with distributed execution or on graphs that are substantially larger than single-device memory capacity.  Or the focus on O(V)/O(E), which glosses over the fact the average degree is small (between 2 and 13 on their chosen OGB datasets) and ignores other 'constants' on the same order. This is technically correct but misleading. None of these invalidate the authors' methods, but it's somewhat disingenuous.\n\nAlso, it very much felt like parts of the paper were aimed at different goals. The intro and background lean heavily into hardware efficiency, while other parts of the paper (S3 outside of the last paragraph; S5.1-5.3) seem to ignore it.",
            "summary_of_the_review": "The authors take a different tact with their proposed GNN, which is welcome. They explain it clearly, and the paper's writing and organization are solid. The results support their claim that they can achieve competitive accuracy, but the quantitative computation and memory results are somewhat underwhelming. In my view, the contribution of this paper is its approach and challenge to convention, which feels good enough to publish, even if it left me wanting more substantive.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper investigates whether incorporating anisotropy (treating neighbors differently using latent functions and thus resulting in $\\mathcal{O}(E)$ memory cost) is necessary for boosting GCN's accuracy. They argue that the proposed EGC with $\\mathcal{O}(V)$ memory requirement can achieve higher accuracy than prior anisotropy-based works (e.g., GAT) using adaptive filters and thus achieve (1) better accuracy than vanilla GCN; (2) and less memory cost and latency than anisotropy-based methods.",
            "main_review": "I am convinced by the introduction part. The hardware efficiency and memory cost are arguably two bottlenecks in the GCN inference and training and they are often correlated.\n\nStrengths:\n\n* The recap of algorithm-hardware co-design is clean and to the point and should promote the understanding of hardware acceleration for the GCN community, potential reference also includes GCoD (HPCA'22) and G-CoS (ICCAD'21).\n\n* The comparison table give the readers high-level information about the propagation differences among various GCN methods and their corresponding memory requirements.\n\n* The adaptive filter is a kind of meta-learning approach that allows different basis filters to explore different latent spaces and finally weighted summed. I suppose the experiments are a fair comparison with a similar number of parameters as Fig. 3 shows.\n\n* A clean codebase is provided.\n\nQuestions:\n\n* I have one question about the sampling-based methods. As Table 2 shows, GraphsSAGE achieves the lowest GPU training and inference latency and also the least peak training memory, which seems to contradict the argument in Sec. 2 that sampling-based methods are often ineffective. In addition, why the inference time of GraphSAGE is also lower than GCN since we will not sample the subgraphs during inference? I am also wondering whether EGC can also be implemented in a sampling-based way?\n\n* Could you elaborate more about the potential impact on the GCN hardware acceleration? Will EGC-S or EGC-M propose unique challenges/opportunities that commercial devices (CPU or GPU) cannot efficiently handle while needing further customized hardware architecture for leveraging their full potential?",
            "summary_of_the_review": "I think the introduction and related works description clearly recap the background of both GCN algorithm and potential algorithm-accelerator co-design, and also the current dilemma in the GCN community, that is, the more accurate but less time/memory efficient anisotropy approaches VS. less accurate but more time/memory efficient vanilla GCN approaches. The proposed EGC can alleviate such a dilemma by achieving both higher accuracy and efficiency.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}