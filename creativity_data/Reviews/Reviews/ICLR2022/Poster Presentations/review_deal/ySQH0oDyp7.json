{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a simple, theoretically motivated approach for post-training quantization. The authors justify its effectiveness with both a sound theoretical analysis, and strong empirical results across many tasks and models, including a state-of-the-art result for 2-bit quantized weights/activations. All reviewers agreed the paper is worth accepting, with 3/4 rating it as a clear accept following the discussion period, and the fourth reviewer not giving strong reasons not to accept."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors conduct theoretical studies on how activation quantization affects weight tuning, and their conclusion is that involving activation quantization into the reconstruction helps the flatness of model on calibration data and dropping partial quantization contributes to the flatness on test data. They present both empirical and theoretical findings, and propose the QDROP algorithm to exploit this phenomenon. QDROP randomly drops quantization during post-training quantization (PTQ) reconstruction to pursue the flatness from a general perspective. The authors also claim a new state of the art for PTQ on various tasks including image classification, object detection for computer vision, and text classification and question answering for natural language processing.",
            "main_review": "The paper presents a strikingly simple approach to activation quantization. It boils down to a per layer modified dropout of the quantized activation, where either the network backdrops through the quantized activation or the full precision activation. Since approach is developed and implemented via layer wise fine-tuning, we could say that the approach is \"greedy\" but with the benefit of being palpably easy to implement in practice. \n\nAlthough the empirical results provided by the authors show that in practice the approach works well enough, I would like to see the authors to push the envelope further and try to use 1st or 2nd order gradient information (i.e. gradient or hessian) to better inform when applying their dropout is actually necessary (and to be even more clear, I'm thinking things like the Hessian criteria presented in HAWQ-v2 which the authors reference in their argumentation in favor of flatter models in Section 2 or the Gradient loss presented by Lee et al. in \"Data-free mixed-precision quantization using novel sensitivity metric\" https://arxiv.org/abs/2103.10051).\n\nI would also like to author to explain how their approach relates and differs to the one introduced by Fan et al. at ICLR21 in \"TRAINING WITH QUANTIZATION NOISE FOR EXTREME MODEL COMPRESSION\" since both approaches seemed to boil down fine-tuning the model under a noise quantization modeling scheme. I don't necessarily need a numerical comparison, but simply a brief mention in the related work section would be sufficient to better inform the reader of the potential connection between these 2 works.\n\nOverall, I'm satisfied with the paper, and I think it's in a good enough shape to be published at ICLR.\n",
            "summary_of_the_review": "The paper presents a simple approach to post training activation quantization that should be easy to implement and test by other researchers. The authors present both a mathematical treatment and empirical results to back up their claims and I wasn't able to find any glaring error. Hence, I think the paper is good and should be accepted. I expect their code to be jointly published with their paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors proposed a random dropping quantization method at the post-training stage\nto achieve a low bit quantization network.\nBy observing the performance on partial activation quantization, \nthe authors analyze the influence of incorporating activation quantization into weight tuning.\nExperimental results show that their QDROP methods have benefits on several scenarios, such as detection and NLP tasks.",
            "main_review": "Strengths:\n\n1. Do exploration on activation quantization and transforming the activation quantization into weight perturbations is a good idea.\n\n2. Experimental results on many tasks and models show the QDROP's advantages.\nAblation studies also support the analysis.\n\n3. This paper is generally well organized.\n\nWeakness:\n\n1. The terms of 'flatness' and 'sharpness' are not well defined, it might make readers confused on these and hard to follow.\nCan the authors provide more detailed explanations on these two terms? \n\n2. About Figure 3, the reviewer did not find any related explanation about these figures.\n \n3. The reviewer expected a discussion on how does activation quantization involves with smoother loss surface,\nwhile it looks missing in this work.\n\nMinor issues:\n\n1. Impact of dropping probability on ImageNet is performed on two network structures. \nCan the authors provide more experiments on other networks? That will be more convincing.\n\n2. On page 3, it should be H^{w} in the line below the equation (3)?",
            "summary_of_the_review": "Overall, I think this will be an instructive work if the authors can tackle the reviewer's concerns. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper aims to analyze how activation quantization affects the PTQ process and provides some theoretical analysis and experimental results on the effects of activation quantization. The authors argue that previous studies only model the weight quantization as perturbation while ignoring activation quantization, causing a sub-optimal solution by missing out on the main factor in the performance degradation of quantized models in low-precision PTQ environments. Based on the empirical and theoretical analyses, this paper proposes QDrop to pursue flatness and demonstrate that partial activation quantization is more beneficial.\n\nContributions: \n1. Observed the benefits of activation quantization in low precision PTQ.\n2. Conducted theoretical studies on how activation quantization affects weight tuning.\n3. Presented QDrop by showing that both integrating activation quantization into PTQ reconstruction and dropping partial activation quantization may help the flatness of the model, which is vital to the final accuracy.\n4. Established a new SOTA for W2/A2 PTQ with QDrop.",
            "main_review": "This paper is well-organized, and its goal is well-oriented by providing a good motivational experiment. Theoretical analyses are well-balanced with empirical observations to offer a valuable insight that the flatness of the model is crucial to the extremely low-bit PTQ. However, there seem to be some glitches in formulating and presenting a theoretical framework. The novelty of the QDrop algorithm itself is moderate and simple, but the intuition behind it is critical. Experimental results look promising. Here are a few concerns and questions: \n\n1. An essential basis of the given theoretical framework is an assumption that the activation quantization could be modeled as injecting a multiplicative form of perturbation, say a^ = a + e = a + au = a(1+u) = as, where s is a scaling factor. Is this assumption reasonable in an actual situation? If this assumption is not available, is the conclusion still valid? Is it possible to be represented by any generalized form of perturbation?\n\n2. An operator used in the vector/matrix multiplication should be defined. Is it element-wise multiplication or ordinary matrix-vector multiplication? Does it have a commutative property? For example, in Eq.(5) and the equation above Eq.(5), could you check the dimension of vectors and matrices? Could you provide any proof of Eq.(5)? It seems to be crucial to prove the following Lemma 1 and Theorem 1. The proof of Sec. 4.2 provided in Appendix A is a bit confusing to the reviewer. Could you clean up the critical statements by setting up Corollary or Lemma ones?\n\n3. Several vague and confusing terminologies were used in Corollary 1. For example, the trained quantized weights are usually(?) flatter on some(?) directions and more robust under some(?) perturbations for calibration data. Is there any other case that the trained quantized weights are not flattering? Again, technical terms should be used to give some intuitions. \n\n4. In Section 4.3, the authors claim that it is highly possible(??) that Case 2 causes overfitting due to the mismatch on calibration and test data. However, Case 2 and Case 3 seem to perform activation quantization based on Eq.(7) using the same calibration data. Could you prove that the relatively low performance of Case 2 compared with Case 3 is due to the overfitting? Their performance gap is just 2% in most of the models in Table 1. If overfitting occurs during the weight tuning process, it seems to be suitable to arise in Case 1, which does not perform activation quantization. There should be a supplementary explanation for this by comparing the results before and after weight tuning.\n\n5. When equivalent compression levels are applied by quantization, it seems more evident that the proposed method can improve generalization performance while preventing expected compression losses. Could you evaluate the average precision(bit-width) of QDrop in the experiments? If the average precision is not critical to this case, could you explain the reason?\n\n6. It is generally known that the performance of QAT is relatively good compared to PTQ. Are some ablation studies necessary to provide supplementary experiments with a QAT setting in Appendix B?\n\nMinor Comments: \n1. (Singh et al., 2019) is wrongly cited on page 2. Please refer to (Nagel et al., 2019) and (Banner et al., 2019) instead. \n2. There are several typos: (1) are->is on page 2, (2) for each architecture, we ~~ on page 7, (3) origin->original on page 9, (4) Fig. 5.3 should be replaced with Table 6 on page 9, (5) Table ?? on page 15.\n3. In Table 2 and Table 6, please unify the same terms, say No Drop vs. No QDROP. Does it correspond to Case 2?\n4. The caption title is missing in the table on page 15. In the table, what is SWA_20? and 32/4 is right in OMSE? It seems that the order of rows in OMSE should be sorted in this case.\n",
            "summary_of_the_review": "To sum up, the paper is well-written and generally interesting even though there are a few glitches. Therefore, I may change my rating according to the authors' rebuttal to resolve the above concerns and questions. \n\n***** Post-Rebuttal Comments *****\nI appreciate the authors for their detailed response. The newly updated manuscript addresses most of my concerns. Therefore, I am happy to raise my rating up and genuinely recommend this paper to be accepted. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes the post-training quantization for extremely low-bit neural networks. By considering the activation quantization during reconstruction, the presented QDrop randomly drops the quantization of activations with higher loss flatness that adapts the activations with various activation bitwidths well. Experimental results have demonstrated the superiority of the presented method.",
            "main_review": "Post-rebuttal\nThe response well address my concerns, and I would like to increase the score.\n-----------------------------------------\nPros:\n1. The idea of considering the activation quantization during reconstruction of PTQ is significant. As the authors demonstrate in Table that Case 1 underperforms Case 2 obviously, assigning more optimal weight calibration is highly demanded.\n\n2. The observation of loss landscape flatness is also beneficial to the design of PTQ. Although the property of loss landscape of neural networks have been widely studied, the importance to PTQ remains to be explored.\n\n3. The excremental results clearly show the presented methods outperform the SOTA especially in lightweight architectures such as MobileNet-V2, and the accuracy of 4-bit PTQ seems to be promising in large-scale dataset such as ImageNet.\n\nCons:\n1. I think the core problem of this paper is the technical soundness of Eq. (7). I do not think the approximation in Eq. (14) and (15) is accurate Since the summation of the production in (19) is assumed to be exchangeable, is this always true? Otherwise, the form of Eq. (7) will be very complex.\n\n2. The writing needs to be improved. The notations should be well defined before use. For example, what do $u(x)$ and $V(x)$ exactly mean? The authors only say  $1+u(x)$ represent the noise and this is not the formal definition.\nMeanwhile, the connection between Eq. (5) and (6) should be specified.\n\n3. What is the relationship between Corollary 1 and Eq (8)?\n\n4. Figure 3 is not explained in the mainbody. What information can this figure convey? Meanwhile, why case 1 and case 3 seems to be similar?\n\n5. The connection between flatness and randomly activation quantization dropping needs to be explained, i.e., the physical meaning of Eq. (9).",
            "summary_of_the_review": "Please see the Cons in the main review.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}