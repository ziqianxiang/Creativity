{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "Most of the reviewers think this paper is clearly a valuable addition to ICLR based on the convincing theoretical analysis and extensive experimental results. Please refer to reviewers's review for more detailed discussions of the pros and cons of the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors proposed a new offline RL method called VEM that combines value expectile learning and implicit planning, with the goal of reducing extrapolation error in the action space. The proposed value expectile learning is an interpolation between Bellman expectation operator and Bellman optimality operator. By adjusting the hyperparemeter, one can adjust the tradeoff between sticking to behavior policy or deviate from it to potentially obtain better performance. On standard offline RL benchmarks, the authors demonstrate that the proposed method achieve SOTA performance. ",
            "main_review": "## Pros\n* The idea of using expectile Bellman operator to interpolate between expected Bellman operator and Bellman optimality operator is interesting.\n* The authors has conducted experiments in more than 20 environments and the proposed method's performance either surpasses SOTA methods or on-par with them.\n\n-----\n## Cons/questions/suggestions [C/Q/S]\n* [C1]  The clarity of the manuscript could be significantly improved as some of the discussions/explanations are either not precise or consistent with each other. __Example 1__ On page 3, the authors wrote \"The value estimation learned by EVL, on the contrary, achieves a trade-off between learning optimal policy and behavior cloning and can be close to the optimal value with proper chosen ... The estimation error can be significant when the dataset is small, and EVL needs a smaller to be more conservative and closer to behavior\". When the dataset is small, I understand that one want to decrease the $\\tau$ to be more conservative. But it's not clear to me whether the smaller the better in this case. Also, when the authors say EVCL needs to be closer to behavior cloning, does that mean we should just choose $\\tau = 0.5$ in this case? __Example 2__ The optimal values for $\\lambda$ are mostly below 0.5 as seen in table 3. Based on the reasoning given by authors, this seems to indicate that these datasets used in the experiments are \"small\" because we require small $\\lambda$ to obtain good performance. However, the datasets in D4RL are quite large and often much bigger than most real datasets. I feel these results are inconsistent with the authors' discussion in the aforementioned example . __Example 3__ On page 3, the authors wrote \" VEM uses expectile V -learning (EVL) to learn V -functions while avoiding extrapolation error in the action space.\" This claim seems to be too strong for me. As \"avoid\" indicates completely removing the extrapolation error, while EVL actually just serves to reduce the extrapolation error. I think this claim should be more precise.  __Example 4__. On page 3, the authors wrote \"in real-world problems, the dynamics are often nearly deterministic\". For robotics applications, this might be true. However, for a lot of other real-life applications, for example, healthcare, industrial control, autonomous driving, recommender systems, etc, I don't think the dynamics is anywhere near deterministic. \n* [C2] The authors emphasize that EVL could fundamentally avoid unseen actions. I don't see why this is the case. I feel the policy network learned through advantage-weighted loss (eqn.7) can defintiley give out-of-distribution actions when taking the argmax due to inaccurate advantage estimations. Please clarify on this.\n* [Q1] In Figure 10 (b - e), the value estimation error never decreased, and the optimal estimations were obtained at 0th step. This seems to indicate that the value networks failed at learning in the corresponding environments. However, in table 1, good performances are reported for these environments. Could the authors explain why this is the case? \n* [Q2] AWR is a value-based offline RL method and is probably the most relevant baseline. I'm wondering whether the authors could provide some explanations on why AWR failed at four out of six dataset types for antmaze while VEM performs well for all six.\n* [S1] For equation 2, the authors mention that when $\\tau\\rightarrow 1$, the Bellman expectile operator approaches the Bellman optimality operator. However, the lemma 3 for this was not mentioned until page 5. I suggest adding a sentence referring readers to lemma 3 for this important observation.\n* [S2] When $\\lambda=0.5$, VEM is essentially behavior cloning + implicit planning. I think this baseline should be listed separately in table 1 to help readers see the importance of introduced flexibility for value learning with the Bellman expectile operator.\n\n----\n## Minor comments \n* [Typo] Page 8, \"Therefore, the generative model in BCQ cannot __guarantees__ completely\" -> guarantee\n* [Plot] Figure 4 is quite hard to read. The authors could probably just get rid of the floor pattern and make it a different color that has higher contrast with the value pixels.",
            "summary_of_the_review": "Though the idea of balancing between behavior cloning and optimal value learning through the Bellman expectile operator is interesting, the clarity of the current manuscript still require some improvement. If the authors could address my concerns and questions, I'd be happy to adjust the score.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work proposed a new offline reinforcement learning framework, where the value function is employed to do update. Specifically, the proposed algorithm used optimal value learning and behavior cloning. Theoretical guarantees about the convergence of the proposed algorithm are provided. Besides, experiments on D4RL tasks are provided to show the effectiveness of the proposed method.",
            "main_review": "Strengths:\nThis work is well organized, the major idea is clear and valid. Besides, the proposed method is efficient and extensive experiments are provided to prove the effectiveness.\n\nWeaknesses:\nThe proposed framework seems to be similar to work [1], can you compare in detail the difference and the superiority of the proposed method? \n\nBesides, the major idea of the proposed method is also similar to the work [2]. Though work [2] is not designed for offline RL, can you also compare the major idea of the proposed method and this work?\n\n[1]https://arxiv.org/pdf/2106.08909.pdf\n[2] https://arxiv.org/pdf/2101.08152.pdf\n\nThe most recent baseline methods used are proposed in 2020, can you compare with the methods proposed in 2021, like [3]?\n[3] https://arxiv.org/pdf/2106.08909.pdf\n",
            "summary_of_the_review": "Overall, I think this is a good paper. But I am a little concerned about the novelty of this work. So I will give weak acceptance.\n\n_____________________________________________________________\n\nI have read the author's responses. My major concern has been solved. I decide to raise my score to 8.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose to use the expectile operator as a smooth interpolation between behavior cloning and optimal value learning in offline RL. Based on this operator, the authors develop a new offline method called Value-based Episodic Memory. The authors provide theoretical analysis and empirical results for the developed method. ",
            "main_review": "The main contribution of this paper is the introduction of the expectile operator as a smooth interpolation between behavior cloning and value learning in offline RL. Basically, the interpolation is controlled by a parameter $\\tau$, so that when $\\tau = 1/2$, the operator is reduced to taking expectation, and when $\\tau = 1$, the operator is equivalent to the Bellman optimality operator. The authors argue that such an operator is useful in offline RL, in which case learning algorithms need to carefully balance behavior cloning and value learning to avoid extrapolation error. The authors also prove nice properties of the introduced operator, and provide empirical results to justify the effectiveness of the proposed approach. \n\nAlthough this paper introduces an interesting idea, I still have the following concerns.\n\n(i) the Bellman expectile operator is not well-defined. It seems to me that when $\\tau = 1$, any value $v$ that is sufficiently large would achieve the same minimum value ($0$). In this case, how should we define the Bellman expectile operator? \n(ii) The authors seem to assume that the reader is familiar with prior work on value-based offline RL. For example, in Section 2.2, $R_t$ is not defined. The algorithm in Section 3.2 is pretty hard to understand without background on episodic memory-based methods. The authors should at least give an overview on episodic memory-based methods before diving into their new methods. \n(iii) It is unclear to me how one should choose $\\tau$ in practice. Note that in the offline setting, one cannot simply try different $\\tau$ and pick the one with the best performance, since in the offline RL setting, it is assumed that the agent does not have access to online samples. How is $\\tau$ chosen in the empirical evaluation? The paper can be greatly improved the authors could give some guideline on how to pick $\\tau$ in practice. \n\nDue to the above concerns, my current recommendation is a \"weak reject\". However, I am open to raise my score if the authors could resolve my concerns described above. \n\n",
            "summary_of_the_review": "See above. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors intend to derive offline reinforcement learning policies by learning the V-function, instead of the Q-function, so as to balance the imitation learning and optimal value learning. To achieve this goal, the author proposes Expectile V-learning (EVL) to smoothly interpolate between the Bellman expectation operator and optimality operator. Based on the learned value, the authors propose Value-based Episodic Memory (VEM) to approximate the optimal value with offline data and conduct implicit memory-based planning to further enhance advantage estimation. The authors design theoretical analysis, a toy example, and empirical experiments to validate the proposed methods.",
            "main_review": "Advantages:\n1. This paper provides a new perspective to evaluate actions out of the dataset's support. Traditional attempts focused on Q-based methods which require additional constraints or penalties for actions out of the dataset. This paper learns bootstrapped V-values while being completely confined within the dataset without any regularization.\n2. The Value-based Episodic Memory is simple yet efficient. The value-based planning to conduct bootstrapping is efficient. The adaption of episodic memory-based methods is appropriate. The adaption of return-based offline RL methods is effective.\n3. The theoretical analysis is convincing and enhanced the persuasion of the claims.\n4. The experiments are extensive and supportive. The critical experimental parameters are provided, and thus there should be no issues with the repeatability of the experiments. \n\nDisadvantages:\n1. There are quite a few contributions of this paper, but none of them are significant. The first contribution (the most important one), using V-function to substitute Q-function, doesn't show distinct advantages intuitively. Why the V-function is better than Q-function? Simply by adding no additional constraint or penalty for actions out of the dataset? If so, what are the basic reasons? Is it because the constraint or penalty in Q-function difficult to tackle or not reasonable? The contribution of balance between imitation learning and optimal value learning is good, but it is also trivial by just combining two losses together. The implicit memory-based planning and generalized advantage-weighted learning are direct adaptions from existing work.\n2. The Equation formations should be unified. For example, Equation (2) should include (s,a) in the Dirac function.",
            "summary_of_the_review": "The Expectile V-learning (EVL) is new, the Value-based Episodic Memory is simple yet efficient, the theoretical analysis is convincing, and the experiments are supportive. However, my major concern is that none of the contributions are significant. I am looking forward to the authors' response, especially justifications on the novelty and effectiveness of  EVL.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes to reduce the overestimation in offline reinforcement learning by (i) learning $V(s)$ function instead of $Q(s,a)$ (and then using AWR to get the policy) to be within support of data and (ii) using expectile learning to train value function that interpolates between optimal value learning and BC. Furthermore, to obtain better advantage estimation for AWR during policy learning, the paper proposes to compare best return along the trajectory with value estimates and take the maximum between the two (i.e. $R_t = r_t + \\max(R_{t+1}, V(s_{t+1}))$). Finally, the paper provides theoretical guarantees for the proposed method and shows improved performance on a subset of D4RL tasks. ",
            "main_review": "Strengths:\n1. The paper proposes a principled way to prevent overestimation in value function in offline RL by using expectile value function learning.\n2. It proposes using episodic memory to obtain better estimates of returns for AWR during policy learning phase\n3. The paper provides theoritical guarantees for the method and shows improved performance on subset of D4RL tasks.\n\nConcerns:\nThe proposed method feels like implicit/non-parameteric offline model based RL given it uses implicit planning to obtain better targets during policy learning phase. Hence, my main concern is that authors should compare their proposed method to model based offline RL methods like MoREL (kidambi et al., 2020), Combo (Yu et al., 2021).\n\nReferences:\n1. MOReL: Model-Based Offline Reinforcement Learning. Kidami et al., 2020.\n2. COMBO: Conservative Offline Model-Based Policy Optimization. Yu et al., 2021.",
            "summary_of_the_review": "Weighing the strengths and concerns of the paper, I am recommending weak accept as of now.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a new offline RL algorithm that leverages expectile regression for value learning and performs AWR-style policy learning with the value function learned with the expectile loss and memory-based planning. The proposed method, dubbed VEM, is able to interpolate between learning optimal Bellman operators and behavior cloning, preventing overestimation in Bellman backups. VEM is shown to converge and the episodic memory-based planning module is able to both theoretically improve the convergence rate and also empirically improve the performance. VEM achieves the best or comparable performance on most of the D4RL tasks.",
            "main_review": "Pros: \n1. The paper is clearly written and easy to understand.\n2. The method is technically sound. The authors provide theoretical guarantees of the convergence of the VEM and also show that the memory-based planning module improves the convergence rate.\n3. The empirical results of VEM show that it can outperform prior methods in many of the D4RL tasks.\n\nCons:\n1. I'm not sure why the authors didn't evaluate VEM on the full set of D4RL mujoco datasets, e.g. medium-replay, medium-expert and expert datasets. VEM is also not evaluated on the kitchen dataset. Including these would give a more clear sense of how VEM compares to prior methods.\n2. Several important baselines are missing such as [1,2,3,4,5,6]. These methods along with CQL seem to be better than VEM on most of the mujoco tasks while [3] seems to obtain strong adroit results and [6] obtains good antmaze results. The authors should perform a thorough comparison between VEM and these approaches.\n3. VEM seems to require per-task tuning with online rollouts as the authors show different $\\tau$ values on different tasks/datasets. This could be problematic since it is typically impractical and unsafe for offline RL to evaluate the policy online and per-task tuning would make it even worse.\n4. The memory-based planning module seems a bit orthogonal to the main approach, which is the expectile regression. It is directly adapted from prior work and added on top of the method. It would be interesting to see how other offline RL methods perform with the memory-based planning module.\n\n[1] Fujimoto, Scott, and Shixiang Shane Gu. \"A Minimalist Approach to Offline Reinforcement Learning.\" arXiv preprint arXiv:2106.06860 (2021).\n\n[2] Kostrikov, Ilya, et al. \"Offline reinforcement learning with fisher divergence critic regularization.\" International Conference on Machine Learning. PMLR, 2021.\n\n[3] Wu, Yue, et al. \"Uncertainty Weighted Actor-Critic for Offline Reinforcement Learning.\" arXiv preprint arXiv:2105.08140 (2021).\n\n[4] Brandfonbrener, David, et al. \"Offline RL Without Off-Policy Evaluation.\" arXiv preprint arXiv:2106.08909 (2021).\n\n[5] Chen, Lili, et al. \"Decision transformer: Reinforcement learning via sequence modeling.\" arXiv preprint arXiv:2106.01345 (2021).\n\n[6] Ajay, Anurag, et al. \"Opal: Offline primitive discovery for accelerating offline reinforcement learning.\" arXiv preprint arXiv:2010.13611 (2020).",
            "summary_of_the_review": "Given the comments in the previous section, I think it would be good if the authors can address the raised issues. Based on its current status, I would vote for a weak reject.\n\n_____________________________\npost rebuttal update: I've read the response and most of my concerns are addressed. I will raise my score to a 6.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}