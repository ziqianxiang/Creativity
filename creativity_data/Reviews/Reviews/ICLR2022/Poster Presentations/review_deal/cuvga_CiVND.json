{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper observes that the number of redundant parameters is a function of the training procedure and proposes a training strategy that encourages all parameters in the model to be trained sufficiently and become useful. The method adaptively adjusts the learning rate for each individual parameter according to its sensitivity (a proxy for the parameter's contribution to the model performance). The approach encourages the use of under fitted parameters while preventing overfitting in the well-fitted ones. Experimental results are presented covering a wide range of tasks and in combination with several optimisers, showing improvements in model generalization.\n\nThe paper is very well written and easy to follow (as mentioned by Reviewers NSqH, 4pzE and sSHP). \n\nThe authors provided a strong rebuttal including new experiments, like training using CNN based architectures (as requested by Reviewers sSHP and MzBV). Reviewer sSHP requested these results to be reported with STD, the AC encourages the authors to do so for the camera ready.\n\nReviewer MzBV points out that the paper could be improved by giving a motivation of the update rule and proving convergence. However, still recommends accepting the paper due to the novelty in the idea of not taking redundant parameters as something inevitable and devising an effective strategy to improve it. This idea was also appreciated by the other reviewers. While the AC agrees that adding these points would improve the work, it takes as valid the point made by the authors. Namely, that the intuition behind the update rule is quite clear, and many other reasonable variants were ablated (in Appendix A.4.4). Furthermore, the empirical evidence shows that the method improves generalization.\n\nReviewer NSqH points out that while SAGE improves the model’s generalization performance for lightly compressed models, its performance becomes more susceptible to pruning when the model is compressed heavier. While the authors responded with good points, the AC encourages them to follow the reviewer’s advice and incorporate further experiments studying this issue (e.g. other datasets).\n\nIn sum, the paper proposes a simple and effective method that is able to improve generalization of large scale models. All four reviewers recommend accepting the paper. The AC agrees and encourages the authors to incorporate the requests mentioned above."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper focuses on the parameter redundancy issue in large transformer architectures. Instead of pruning redundant parameters, it strengthens training them to make them contribute better performance. To this end, it proposes an adaptive learning rate algorithm SAGE, which automatically scales the learning rate for each parameter based on its sensitivity. The sensitivity is approximated by the dot product between parameters and their gradients. An exponential moving average is used to track the sensitivity scores to reduce uncertainty in mini-batch training. The algorithm is applied to fine-tuning pre-trained transformer models in benchmarks for natural language understanding (NLU), neural machine translation (NMT), and image classification.",
            "main_review": "Strengths\n1. The paper is well-written and easy to follow. The method section provides helpful intuitions behind the algorithm design.\n2. The idea is interesting since it explores a different direction in dealing with redundant parameters.\n3. Experiments use multiple benchmarks, including both language and vision, and results show noticeable performance improvements.\n4. SAGE is orthogonal to the existing adaptive gradient methods. Jointly using them can bring more gains.\n\nWeaknesses\n1. Table 1 shows that models with different percentages of redundant parameters have similar performance, implying that performance may not be proportional to \"well-trained\" parameters. It means that making more parameters \"well-trained\" does not necessarily improve the performance. This seems not totally in line with the paper's motivation.\n2. Figure 5 is not straightforward to visualize the difference. The curve is drawn only on the right subfigure.\n3. Sufficiently training all parameters seems a double-edged sword. According to Figure 3, models trained with SAGE are susceptible to parameter pruning. In general, we want efficient and compact models in deployment. Will SAGE be harmful to developing efficient deployment models?",
            "summary_of_the_review": "This paper proposes SAGE, an adaptive learning rate schedule to train redundant parameters more sufficiently to improve model generalization. SAGE, together with existing adaptive optimizers, shows effectiveness in a wide range of downstream tasks. The main concern is whether SAGE has adverse effects in getting efficient deployment models.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper argues that redundant or \"useless\" parameters are not an axiom we should take for granted, but rather a symptom of current optimization settings. The authors propose an adaptive learning-rate schedule that specifically aims to eliminate redundant parameters. Through extensive experiments on fine-tuning transformers, they show that indeed this method (SAGE) does reduce redundancy and also slightly improves results.",
            "main_review": "I think that overall the paper is good as it raises and studies an important point: redundancy in parameters is not an axiom we have to accept.\nHowever, after this introduction and motivation, it reads more like a typical optimizer paper introducing a new optimizer based on hand-wavy intuitions. This is unfortunate and not rigorous.\n\nWhat follows are detailed comments:\n1) The use of Taylor approximation is only good close to the operating point for nonlinear functions. However Theta_j,-j may be very large, and thus the approximation may be very bad.\n2) I do not agree that the memory and computational costs of SAGE are \"marginal\". The EMA \"I-hat\" is a full copy of the model since we need one EMA per parameter, especially for large models this is substantial overhead. I would further appreciate timings to verify that indeed the computational overhead is \"marginal\".\n3) It is not clear to me in what scale the quantities U and I-hat are, and hence, the multiplier to the learning-rate. Because of this, it is not clear how one needs to change the original learning-rate \"eta\" when one turns on SAGE.\n4) I am confused by Fig5, it does not look like the two-moon dataset at all to me? Please image-search \"two moon dataset\" and compare.\n\nminor nitpicks:\n5) Fig1: Percent should be [0,100].\n6) I do not agree that one can conclude from the results that SAGE is more effective for small datasets than for large ones, since the \"points\" betwen the datasets/tasks do not live on the same scale.\n7) All experimental evaluation is about fine-tuning of already pre-trained transformer models. This should be more accurately reflected in the title, for example by replacing the word \"training\" by \"fine-tuning\". It would further be interesting to see how well it works when training from scratch; even if it does not work, it is still a valuable fine-tuning method, and stating this may save a lot of people a lot of resources and time.\n8) In Figure 6, since we have per-parameter learning-rates, which one is shown for SAGE?\n9) Typo: \"smoothier\" -> \"smoother\"",
            "summary_of_the_review": "For an \"optimizer\" paper, it is weak: hand-wavy motivation, no real derivation of update rule or even convergence proofs, and experiments only in one very specific domain: transfer of pre-trained transformer models. I am not convinced that it will be generally useful at all.\n\nHowever, the paper raises a very important point: we should not take redundant parameters as a necessity, and it does propose a method to avoid them. I find this point important enough to still suggest acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a method for scaling the learning rate during training in order to encourage all parameters in a neural net to be fully used. Specifically, the updates for parameters are scaled in inverse proportion to how much they affect the loss (their sensitivity); the scaling factor also depends on how stable the estimate of sensitivity is, so that high-sensitivity parameters whose role changes rapidly aren't down-weighted as much. This technique is shown to improve the performance of Transformer models on several different problems, even when used in combination with other adaptive learning rate methods (Adam, Adamax). Analysis experiments verify that the method has the intended effect: compared to the baseline, more parameters have higher sensitivity, and pruning is less effective.\n",
            "main_review": "Strengths:\n\n- The idea is novel, straightforward, well motivated, and easy to implement. Computational cost is low.\n- Potential for large impact on model training best practice.\n- The experiments are very thorough, demonstrating gains across different settings, and showing that the technique achieves its goal.\n- The paper is extremely clearly and carefully written.\n\nWeaknesses:\n\n- The scaling formula seems somewhat ad hoc and hard to characterize. In particular, if the sensitivity of some parameter spikes on a given iteration, it will get a larger update than another parameter with the same moving-average sensitivity that has not spiked.\n- There are some hints that the gains might be partly related to regularization (eg, better results on IWSLT than WMT). It would have been good to test the combination with dropout or something similar.\n- The hyper-parameters for SAGE are exhaustively tuned, more so than for the baseline adaptive optimizers, so there’s a potential for bias. Figure 7 counters this possibility, but only by showing heatmaps, so it's hard to gauge the actual numbers.\n\nDetails:\n\n- Table 3 should show results for your implementation of Adamax that’s comparable to Adamax-SAGE, in addition to the Devlin et al numbers.\n- Figure 2: Consider flipping these plots to the standard convention, and stacking them.\n- Figure 4: Why do we care that local temporal variation is lower in SAGE than the baselines?",
            "summary_of_the_review": "A simple adaptive learning-rate formula that seems to work really well, even on top of other optimizers, as demonstrated by very thorough experiments. Downsides are that the formula isn't particularly well justified, and there are a few potential experimental weaknesses.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes the SAGE method. The idea is that neural networks have redundant parameters. Some approaches will prune these parameters which has the effect of not decreasing the performance but to decrease the number of parameters. The paper studies if it is possible to learn better these parameters in order to make them more useful for the network. The paper proposes a method that will allow to train differently the parameters of a network in order to have a better use of the weights. The paper is evaluated in NLP and image classification with transformers.\n",
            "main_review": "Strengths:\n\n- **Writing:** The paper is well written and easy to follow.\n- **Tasks:** The paper evaluates its method with different optimisers and on different tasks in NLP and image classification.\n- **Results:**  The results are quite good; the proposed method surpasses the baseline each time.\n\nWeakness:\n\n- **Architecture:**  The method is evaluated only with transformer architectures. In the context of image classification it would be interesting to evaluate it with CNN and others architecture than ViT.\n\n- **Optimisation:** In image classification, the pre-training procedure used is quite sub-optimal since the paper of Dosovitsky et al. [1] many improvements have been proposed such as the DeiT approach [2]. It would be interesting to see if the proposed method still works when the model is trained with more regularisation and data-augmentation which may also lead to a better use of weights.\n\n- **Image Classification:** Having only results of models pre-trained on ImageNet-21k and fine-tuned with the proposed method on downstream tasks is not very usual in image classification. It would be interesting to have results on ImageNet only where the SAGE method is used during the training.\n\n[1] Dosovitskiy et al, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, ICLR 2021\n\n[2] Touvron et al, Training data-efficient image transformers & distillation through attention, ICML 2021",
            "summary_of_the_review": "The idea of the paper is interesting and the proposed method seems effective. Nevertheless, more complete experiments in image classification would allow to better evaluate the interest of the proposed method. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}