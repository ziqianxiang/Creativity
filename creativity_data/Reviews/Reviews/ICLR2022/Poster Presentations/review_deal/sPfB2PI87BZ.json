{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper considers the generalized target shift setting for domain adaptation and proposes an optimal transport map-based approach to it. The considered setting for domain adaptation is rather general and of practical use. The proposed method seems sensible, as supported by the theoretical identifiability and empirical results. \n\nIt is worth noting that the way to cite previous work seems to be improved. For instance, in the first paragraph of Introduction, the authors reviewed various settings for domain adaptation. For model shift, the authors cited previous work. However, when discussing covariate shift, target shift, and generalized target shift, the authors did not cite the original work that provides the categorization. For completeness, the authors may want to consider including the setting of conditional shift as well, which has received a number of applications in domain adaptation in computer vision. I believe the categorization of target shift, conditional shift, and generalized target shift was provided by Zhang et al. (2013). This work should also be cited when the authors give the problem definition in Section 2.1. The quality of the paper will be even better if the authors cite previous work in all the right places--this may also make the authors' contribution clearer."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a method for unsupervised domain adaptation under conditional and label shift, one of the most challening versions of DA. At the core of this method is an optimal transport problem that seeks a mapping between source and target domains *and* a class proportion vector that minimize a total transportation cost between the domains. Based on this formulation, and a set of 4 sensible assumptions, the paper provides theoretical results in the form of unicity of the solution and classic generalization bound on the target domain risk. The implemented algorithm is inspired (though not identical) to this objective, which is extended with a few other terms: a constraint on the the label proportions based on a confusion matrix, a classification loss for the mapped source samples, and a relaxed version of the OT's pushforward constraint, and finally, two objectives that seek to improve the discriminativity of target representations. The paper then proceeds to validate the propose method in a wide array of benchmark DA datasets, with various levels of class unbalance, whereby it shows that the method repeatedly and significantly outperforms SOTA UDA alternatives. ",
            "main_review": "Strengths:\n* The core of the approach is well motivated and theoretically appealing\n* Unlike many other deep learning DA papers, this one clearly puts forward assumptions and bases theoretical results on them\n* The domain generalization bound provides interesting insights regarding class proportion and imbalance\n* The experimental section is quite thorough, provides meaningful error estimates, various ablations, and achieves impressive SOTA results\n* The paper is overall very well written, and is quite fun to read\n\nWeaknesses:\n* Although the risk bound motivates Wasserstein distance minimization between latent marginals and class-weighted conditional distributions, the actual optimization objective used combines a plethora of terms, so it's not clear how relevant the theoertical results are for the actual method proposed\n* There is not (theoretical or empirical) runtime/complexity analysis \n* The intuition/motivation for some of the assumptions could use a bit more detail. Some of them are taken ipso facto from prior work, without much justification. It would be useful to have it here (even if in the appendix). This is especially true for Assumption 2, as per question below. \n* One aspect that is not discussed enough is the effect of the latent encoder g in the overall performance of the method, e.g., what happens if g is not rich enough (e.g., initial layers of a NNEt $f\\circ g(x)$, for which $g$ is shallow and $f$ is deep). Moreover, given two possible encoders g1, g2, how should one choose between them? is there a way to easily verify which one leads to the 4 Assumptions being satisfied? Should one try to estiamte the bound for both of these and choose the one with the tighter one?\n\nQuestions/Comments/Suggestions:\n* Objective OT is *almost* the Monge OT problem between $p_T(Z)$ and $p_S(Z)$ (by moving sum inside and using law of total prob), except for the fact that the pushfoward constraint is not exactly $\\phi_{\\sharp} \np_S(Z) = p_T(Z)$, but rather one that uses the class proportions $p_N$ rather than $p_S$. I was hoping to get more clarity on the reason for this discrepancy in the paper or Appendix, but it seems it isn't really discussed anywhere. \n* Assumption 2 feels potentially very restrictive: it says that the $\\phi$ must exactly preserve mass across clusters. How realistic is this? Furthermore, does the j have to uniquely assigned to a k? Or can two source clusters push their mas to a target cluster? If not, this seems to immediately rule out DA between datasets where the target domain has fewer labels that source domain (which, fine, is not that common in classic DA, but is very common in large-scale image pretraining/fine-tuning). But beyond that, \n* Is the bound computable? Given g, $f_N$, $p_N$, (C) and (A) seem straightforward to estimate from samples. What about (L)? The second distribution relies on $p_T$ and $p_T(Z|Y=k)$ which are not directly available in UDA. \n* In related work, OT approaches for UDA: it is stated that Rakotomamonjy et al. (2020) solves GeTarS with OT, buth then two lines later the paper claims OSTAR is the first one to to this. Is the *mapping* the keyword here? If so, would be nice to empahsize that this is the main difference between this paper and Rakotomamonjy et al. (2020). \n\nTypos:\n* It seems that Asssumption 4 should read $p_T(Z| Y=i)$ (not k).\n* Pg 8 conforts->supports/confirms?",
            "summary_of_the_review": "This paper makes a solid contribution towards the challening problem of unsupervised domain adaptation under label and conditional shift. Based on a sound and (to the best of my knowledge) novel variant of the parametric OT formulation, the method proposed achieves remarkable results across a wide range of benchmark tasks. All things considered, I think this paper would be a good contribution to ICLR and would be relevant to the OT and DA communities. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors unsupervised domain adaptation (UDA) where both the label conditional and marginal distributions are different between the source and target domains (GeTarS). The authors propose an approach, OSTAR, to align pretrained representations under GeTarS. A new theoretical bound under some assumptions is derived. Experimental studies also show the effectiveness of OSTAR. ",
            "main_review": "Pros:\n(1)\tThe proposed OSTAR well handles the GeTarS case and improves the target discriminativity. \n(2)\tThe paper gives both the theoretical and empirical analyses for the proposed OSTAR.\n(3)\tThe experiments show improvements over existing baselines. \n\nHere are some comments/questions for improvements:\n(1)\tIn introduction, the paper gives two limitations of existing works, discriminativity and strong assumptions. Please clarify (1) what’s the advantage of OSTAR compared with those UDA methods that explicitly preserve the data discriminativity in domain-invariant feature learning, e.g., [ref1] and [ref2]; and (2) please clearly state the strong assumptions or indicates the related works.\n\n[ref1] Structure preservation and distribution alignment in discriminative transfer subspace learning\n\n[ref2] Discriminative Transfer Feature and Label Consistency for Cross-Domain Image Classification\n\n(2)\t As $g$ is learned by eq (3) and fixed afterwards. It is unclear how to guarantee the target discriminativity is preserved when learning $g$. \n\n(3)\tRegarding eq (OT), please also give reference for “least action principle measured by monge transport cost”.\n\n(4)\tAccording to eq (9), the constraints in eq (OT) should be $p_N^\\phi(Z) = p_S(Z)$? If so, It is unclear how to link $p_T(Z)$ with $p_N^\\phi(Z)$. \n\n(5)\tI understand that each of assumption 1-4 is less restrictive than the corresponding one in the related work. However, proposition 1 requires $Z$ satisfying all the four assumptions. In this sense, it is unclear how significant the proposition 1 is in terms of the flexibility.\n\n(6)\tSome important references are missing, e.g., [ref3] and [ref4]. \n\n[ref3] Flexible transfer learning under support and model shift\n\n[ref4] Generalization Bounds for Transfer Learning under Model Shift\n\n(7)\tIn the overall objective, why only $\\mathcal{L}^g_{OT}$ is weighted by an importance factor? How about the term $\\mathcal{L}^g_{wd}$ is also weighted? Similarly, for each term in eqs. (SS) and (SSg), how about they are weighted?\n\n(8)\tRegarding $\\lambda_{OT}$, the authors only conduct 0 and 0.01 cases. How about the other values? More general, It is interesting to see the sensitivity analyses on such an importance factor. \n\n(9)\tFor table 1, it is better to show the average results for each dataset. \n",
            "summary_of_the_review": "Overall, the paper is well-written and makes some contributions. For more detailed comments, see above. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes an approach for Generalized Target Shift (GeTarS) where both conditional and label shift are present in the target domain. The proposed approach, Optimal Sample Transformation and Reweight (OSTAR), uses optimal transport to transform the latent space and uses information maximization to refine the classifier's decision boundaries. Theoretical analysis shows the proposed approach minimizes the the components in the target risk's upper bound. Empirical studies are carried out on three datasets and ablation studies were carried out to analyze the impact of MI.",
            "main_review": "Strength:\n1. The paper is well-motivated from the Generalized Target Shift setup and uses optimal transport to map the source distribution to the target.\n2. The proposed approach is supported by theoretical analysis that minimizes the upper bound of the target risk.\n3. The proposed approach demonstrates sota results over the baselines on three datasets.\n\nWeaknesses:\n1. The theory and implementation are disconnected with the use of information maximization. The main argument is the use of optimal transport for joint alignment and classification, and it is not clear why it requires information maximization if the joint alignment and classification are successful. Because the information maximization (IM) approach is proposed in the context of source-free transfer, it is not clear whether the use of IM hampers the learning of optimal transport.\n2. In the experiments, the IM component in SS and SSg substantially improves (+9%) the results of joint alignment and classification, while the main focus of this paper is joint alignment and classification. But in the main empirical results in Tab. 1, only results of OSTAR are presented. It will be more informative and convincing to also include the results of CAL alone to help fully understand the impact of optimal transport.",
            "summary_of_the_review": "The idea of using optimal transport to address the problem of generalized target shift is interesting, but the main argument of this paper, optimal transport and its theoretical analysis, is not sufficiently evaluated and is confounded by the additional component information maximization. Based on the empirical studies, it appears information maximization has substantially more contribution than optimal transport, and the outcome of optimal-transport (CAL) is missing in the main results (Tab. 1).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}