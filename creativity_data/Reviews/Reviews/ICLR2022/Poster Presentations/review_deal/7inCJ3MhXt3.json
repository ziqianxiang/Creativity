{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper studies the design and analysis of contextual bandits algorithms, combining the ideas of neural network models (Zhou et al, 2020 and Zhang et al, 2020) and reward perturbations (Kveton et al, 2019, 2020); this has the computational advantage of avoiding inverting large covariance matrices, as is done in the other neural contextual bandits algorithms. Although the reviewers think that the papers need to do a better job in highlighting differences and extra challenges in the current work compared to prior works, they also acknowledge that this paper is the first that combines the above two ideas. \n\nThe reviewers also acknowledge that the additional experiments in the rebuttal period help clear the concern the reviewers have about why all regret curves look linear. However they also pointed out, that comparison with the FALCON+ algorithm (Foster et al, 2020) may be slightly unfair, as the algorithm retrains the neural network after every new iteration. Overall, the reviewers think that the pros outweight the cons, and they lean towards acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper investigates neural contextual bandit problem. A new, computationally tractable and efficient algorithm (Neural bandit with perturbed reward, NPR) is proposed and proved to suffer a nearly optimal minimax regret upper bound.",
            "main_review": "Strengths:\n* Unlike the algorithms in existing literatures which are either computationally intractable or has no theoretical regret upper bound using approximate computations, NPR is computationally tractable and has nearly optimal regret upper bound.\n* Extensive experiments are conducted to compare the practical performance between the proposed algorithm and the ones in existing literatures.\n\nWeaknesses\n* NPR is technically sound but hard to be applied to practical applications due to that 1) During each time step, the algorithm has to use all of the previous rewards. When the time horizon T is large, the computation becomes expensive. 2) The model studied is a K-armed setting where during each time step, the learner are facing the *same* set of arms. For practical applications, it is more reasonable to assume the arms set faced during each time step is different.\n* The originality of the techniques used to prove the regret upper bound is limited. If I read the paper correctly, it seems the key lemma (Lemma 4.1) is from Jacot et al., 2018. It might be better if the authors could spend one paragraph to discuss the technique originality.\n* The experiment setup seems to be different from the one discussed in this paper.",
            "summary_of_the_review": "Personally, I like the idea using perturbed rewards to implicitly let the neural network to explore. However the proposed algorithm is still hard to be applied to practical applications. \n\nDetailed comments:\n\nEquation 3.2: parameter $\\lambda$ is missing?\n\nAfter Lemma 4.3: ... the the neural network ...  the extra the should be removed\n\nLemma 4.4: it might be better if the authors could define $E_{t, 3}$ in the beginning of Lemma 4.4.\n\nAfter Lemma 4.6: please resolve the cross reference error\n\nExperiment 5.1: It is confused to me that `At each round t, only a subset of k arms out of the total K arms are sampled without replacement and discosed to all algorithms for selection.` Could the authors help explain why the algorithms are restricted to pull $k < K$ arms which seems to be different from the one discussed in this paper?\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper addresses the reduction of computational cost of the neural contextual bandit problem which uses the deep neural networks to model the reward function. Existing works in this field often require the exploration of the entire neural network parameter space\nand thus, the computation cost is expensive, especially when the network is large. Their main contribution is to perturb the rewards when updating the network to eliminate the need for exploration. They showed that this idea works well in experiments while still maintaining the same regret bound rate as that of previous works in the NTK regime.",
            "main_review": "The idea of using the reward perturbation to avoid the exploration of the entire neural network is sharp. This allows their algorithm to significantly improve over NeuralUCB and NeuralTS in practice.   \n\nHowever, using the reward perturbation has been exploited in previous bandit models as in generalized linear bandits. This is also mentioned by the authors in the related works. Applying this idea to the neural bandits is straightforward. In an over-parameterization regime, the neural network behaves as the linear model. So I do not see technical challenges here because the main challenges of the neural bandits have been solved in both NeuralUCB and NeuralTS.  The used techniques of this paper is an adaptation of  NeuralUCB, NeuralTS to the situation with perturbed rewards. Indeed, Lemma 4.1 is similar to Lemma 5.1 in the NeuralUCB paper. Events $E_{t,1}$, $E_{t,2}$ are similar to the events $\\mathcal E^{\\mu}$ and $\\mathcal E^{\\sigma}$ defined in the neuralTS paper. Lemma 4.2 and Lemma 4.3 are similar to Lemma 4.3 and Lemma 4.2 in the NeuralTS paper, etc. \n\nIf the authors can show me the novelty in technical proofs compared to previous works, I will raise my score.\n \n\n ",
            "summary_of_the_review": "This is a good paper. It brings a solution that is much more practical than NeuralUCB and NeuralTS.  The experimental results are convincing. However, I think that the novelty is not enough. I am leaning forward to rejecting it.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies neural contextual bandits in the realizable setting. It proposes an algorithm that trains the neural network that maps arm contexts to rewards, with perturbed rewards. The paper proves that under NTK analysis the learnt function provides an optimistic estimate of the reward of each arms and therefore sub-linear regret guarantees can be derived. Crucially the dimension term in the regret only depends on the effective dimension of the NTK. Some experiments are performed on real and synthetic datasets where the proposed algorithm performs at par with neuralTS and neuralUCB algorithms while cutting down the running time by a large factor. Thus this can indicate the practicality of the proposed algorithm.",
            "main_review": "__Pros.__\n\n1. The algorithm is practical as the neural network training is unaffected by the exploration strategy. This is a big advantage over NeuralTS and NeuralUCB.\n\n2. The paper uses the same idea as [Kveton et al 2020] however it rigorously proves regret guarantees for the neural network function class under the NTK setting.\n\n3. The regret guarantee only scales with the effective dimension (Valko et al 2013) of the NTK.\n\n4. The experiments show some promise in real world datasets especially because of the predictably better running times than NeuralUCB.\n\n__Cons.__\n\nI have some general clarifying questions and some comments, which will hopefully improve the quality of the paper.\n\n1. My biggest concern is that the paper fails to discus and compare with the line of work in FALCON (https://arxiv.org/pdf/2003.12699.pdf). Both the papers lie in the realizable setting. FALCON also does not need to modify the training of the neural network in any way and in fact needs to train the model even less frequently based on an epoch schedule. Further I think the guarantees of FALCON will probably hold with a similar regret term; basically the generalization error from the NTK will appear in Assumption 2 and Theorem 2 in https://arxiv.org/pdf/2003.12699.pdf. I think the paper should definitely have a discussion with this. Besides the paper should also compare with this algorithm in the experiments as it is easy to implement.\n\n2. The $\\lambda$ term is missing from Equation 3.2. In algorithm 1 how is $\\nu$ set -- can the authors please point me to wherever this is defined in the paper? What setting of $\\nu$ is used in practice in the experiments? Also, in practice is the neural network trained per time-step as in the algorithm pseudocode? That would be pretty prohibitive in practice. \n\n3. (minor) In the def of $\\bar{b}_t$ in page 4 the expectation of $\\gamma_s^{t-1}$ is zero, right?  Then is this just for the sake of readability?\n\n4. The reference to definition of effective dimension is broken in page 5.\n\n5. In the synthetic experiments in Figure 1 it does seem that all algorithms kind of have a linear regret scaling. It would be great if we can add a full information algorithm (algorithm that sees reward from all arms chooses greedily). Such an oracle algorithm would show the lower bound on the achievable regret. I could find that the real experiments in Figure 2 are averaged over 10 runs, is such an averaging performed in the simulated setting. If so, then please also plot the confidence bars. \n\nAlso subsampling k arms out of a total of 100 arms each time is slightly non-standard. It would be good if the algorithms are just compared over a fixed set of 10, 20, 50 and 100 arms, that are decided in the beginning and held fixed over the rounds.\n\n6. In all experiments, as before it would be great if the following baselines (or moonshots) can be added\n-- FALCON\n-- Greedy (bandit feedback plus choosing the arms greedily)\n-- Full information (full information feedback and the arms are chosen greedily)\n",
            "summary_of_the_review": "I think the pros above outweigh the cons slightly in my mind. I am happy to raise the score if the authors can answer 1 and 6 among the cons.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper considers a version of the stochastic contextual bandit where the underlying reward function is modelled by a deep neural network. This framework is useful for a number of complex problems where simpler parametric models of the reward are insufficient. The authors propose an algorithm inspired by the perturbed reward approaches of Kveton et al (2019a,b,2020) which ensures an exploration-exploitation balance by adding additional noise to the observed rewards. This approach avoids the inversion of large covariance matrices which existing approaches (NeuralUCB and NeuralTS) necessitate, and results in a computational speed-up while maintaining near-optimal regret guarantees.",
            "main_review": "PROS:\n\n•\tThe paper is very nicely written. I found it very easy to follow even the more complex aspects and the introduction does a good job of motivating the work and providing a useful history of the relevant literature.\n\n•\tThe central idea of the paper is a useful and sensible combination and extension of existing techniques. It seems to work effectively, in addition to having desirable theoretical properties, and should be interesting to the ICLR community.\n\nMAJOR COMMENTS ON CONS:\n\n•\tWhat can be said in terms of order results on the computational cost of NPR versus NeuralUCB/NeuralTS? The empirical comparison is useful, but to have both would solidify the core argument for the usefulness of NPR and be quite informative.\n\n•\tThe biggest issue I can see is that the experimental results are not as informative as they could be. Firstly, they do not give a sense of the distribution of regret over different runs, and second, they do not (in the fully synthetic scenario) seem to be run for long enough to show that the algorithms have converged. All the regret plots look linear but with differing gradients in fig 1. Can you remedy this?\n\n•\tThe use of grid-search to select $\\eta$ perhaps needs some commentary as the theoretical guarantees section talks about a specific value $\\eta$ for the theoretical guarantees to hold. The diagonalised algorithms are criticised for losing theoretical guarantees, but does this optimisation of $\\eta$ not affect the theoretical guarantees of your approach?\n\nMINOR COMMENTS ON CONS\n\n•\tThe definition of $f(x,\\theta)$ may be better in an equation display as the page break in the middle of it is unfortunate.\n\n•\tMiddle of page 3: should be “When all the $K$ arms have been pulled once” not “for once”. \n\n•\tIf I were aiming to improve the commentary on the theory, I would perhaps add a paragraph to the start of Section 4 to highlight what the main challenges/novelties in the theory are – as I understand this it is the design of the noise and of the step-size in the gradient descent? This would help the reader get a clear picture of what is more/less standard going in to this section.\n\n•\tThe regret bound achieved in Theorem 4.7 is (understandably) quite complex and not directly compared to the regret of the competitor algorithms. I think there would be value in giving some commentary afterwards to highlight the dependence on the main terms such as T and K and \\tilde{d} and how this compares to NeuralUCB/TS.\n",
            "summary_of_the_review": "I’m very positive about the idea behind this paper: to use combine perturbed rewards with neural contextual bandits and think that this work is timely and potentially very impactful. I have a few reservations about the extent to which experiments support the main claims of the paper, and as such have chosen a borderline score which I could increase if my concerns are addressed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}