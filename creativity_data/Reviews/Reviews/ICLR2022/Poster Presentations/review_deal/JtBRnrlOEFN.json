{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper introduces a soft gradient-based subword tokenization module (GBST) that learns latent subword representations from characters. GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network. The resulting model was tested on GLUE, and several cross-lingual tasks. The performance is competitive with ByteT5 and often similar to subword models while being more efficient in FLOPs and RAM.\n\nReviewers are mixed on this. The negative reviewer points to how this not being a real tokenizer and does not produce a tokenization, that experiments that use the base model do not address bigger scales, and that there is a lack of code which is important for this kind of work, and the resulting accuracy gains are not significant and the method being not interpretable. The positive reviewers like the extensive experiments, the efficiency improvements and flexibility / simplicity of the GBST module. The authors seemed to have addressed most of the reviewer issues by providing larger scale experiments and code. I believe the results are fairly strong, since one would not expect a big performance difference in a learned tokenization method, but rather efficiency or flexibility gains. The paper is generally well-written though details about the convolution should be included in the text (and not just the code).\n\nRecommending accept."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose a soft gradient-based subword tokenization (GBST) module with the aim of improving tokenizer-free end-to-end training of language models. The GBST module takes a byte-level sequence and computes all possible subword representation (up to a length) in a convolution-like manner, the results are then pooled, scored, and weighted with other add-ons including 1D-convolutions and an attention-like position-wise score. The authors use the GBST module followed by an encoder-decoder Transformer stack similar to that of T5 and call it Charformer. The authors conducted experiments on a range of NLP classification tasks under monolingual and multilingual settings. The trend seems consistent that subword-level models > Charformer ≈ byte-level T5 on monolingual and multilingual clean data, Charformer ≈ byte-level T5 > subword-level T5 for monolingual noisy data. In terms of efficiency, the authors show evidence that Charformer uses fewer parameters and FLOPS than subword T5 and proceeds more steps per second than byte-level T5.",
            "main_review": "- (Unclear) How is the proposed GBST different from convolution, despite the scoring part?\n- (Pro) Extensive evaluation results across many NLP classification tasks in mono- and multi-lingual settings.\n- (Pro) Charformer seems to achieve similar predictive performance to Byte-level T5 while being more computationally efficient (Table 6). \n- (Mixed) Charformer achieves worse predictive performance than subword models BERT and T5, which seems to suggest the proposed GBST gives little lift from character-level to subword-level. However, the Charformer models presented are smaller in size or faster in speed, or more efficient in FLOPS. Thus, hopefully, it will provide people with more choices for their use cases.\n- (Mixed) Experimental results do not show strong improvements on accuracy metrics. Table 1-3 shows that Charformer is generally on par with Byte-level T5. Sometimes Charformer wins; other times Byte-level T5 wins. Either with small differences. In the multilingual case (Table 4), we do see Charformer consistently outperforms Byte-level T5 with a small margin. In all cases, Charformer is significantly worse than subword models. The rescaled version Charformer_S is able to achieve comparable and sometimes better accuracy than subword models. However, the re-scaling is not tied to Charformer and can be applied to other character-level models as well. It is not clear if the gain is because of the proposed GBST or solely due to the re-scaling.\n- (Con) It is not clear if the proposed GBST module learns meaningful subword tokenization. Better qualitative evidence, e.g. examples that highly scored subwords align with human intuition, or quantitative evidence, e.g. how the learned tokenized subwords align with established methods, or how they are better at solving lexical tasks, would help.\n- (Con) Many technical details are unclear/confusing. It would help if the authors can provide more and clearer technical details. If limited by pages, the author can point readers to appendices. Make sure the main text still contains the necessary details when moving things to Appendices.\n- (Con) No code provided. Providing code will greatly help reproducibility and help clarify many technical details not fully described in the paper.\n\n### Questions to the Authors\n- How is the proposed GBST different from convolution, despite the scoring part?\n- Can the authors help clarify the design of GBST and implementation of Charformer? For example, what is the Transfomer stack used by Charformer? It seems like T5, but I don't find it mentioned anywhere. See other localized points below.\n- All the tasks in the paper seem to be classification tasks. Is Charformer encoder only? If so, how do the authors ensure a fair comparison with encoder-decoder models, e.g. T5? If not, is the decoder part of Charformer character-based, or it also uses some notion of subwords? \n\n### Localized Points\n- Eq (1). What is used as $F$ in the experiments?\n- In \"Considering Offsets\". How is 1D convolution applied to X? How does it save computation?\n- Figure 2. How to interpret this heatmap for block size > 1? How are they aligned? For example, we see a high score for block size = 3 at \"k\", is it for subword \"tok\" or subword \"ken\"?",
            "summary_of_the_review": "A plausible proposal for the promising direction of learning subword tokenization end-to-end. However, some important design and implementation details are missing or confusing which are not helped by the lack of source code. The extensive experimental results themselves would benefit the community. However, they don't seem to strongly support the advantage of the proposed module in learning subword representation either in terms of end-task accuracy or explainability. If the authors could clarify what they did exactly with their GBST module and thus their novelty, I would be inclined to accept for the merit of extensive evaluation results and one more alternative that lies between subword-level and plain-byte-level models.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper aims to remove the reliance of NLP models to external tokenizers. It proposes a gradient-based subword tokenization module that can be used in any neural model. The module scores predefined candidate blocks in a soft way for each position and then it weight averages them to obtain a mixture of subword representations. The resulting sequence is then downsampled in a fixed way to make processing easier. The proposed method integrated in a deep narrow transformer model performs on par and slightly better than character-level baselines on monolingual and multilingual classification respectively. In some settings it also outperforms strong subword-level baselines.",
            "main_review": "Strengths\n1. Avoiding the reliance of NLP models on tokenizers is an important problem. Tokenizers influence the input structure and the design of the model. The input structure is not amenable to graceful modifications after the initial training which can be limiting. \n2. The idea to calculate position representations based on a weighted average of subword blocks is interesting. This way the representation captures information from nearby n-grams. \n3. The experimentation has some competitive results and focuses on both performance and speed aspects. \n\nWeaknesses\n1. My main concern is that the proposed method does not seem to be satisfying the criteria for a proper tokenization method. \n     * First, it does not learn to segment the input to different chunks to be fed in the contextualizer but rather downsamples the sequence in a fixed way. The latent subword representations could be captured by combing multiple convolutions and a simple pooling function per position, which makes the method seem less novel.  \n     * Second, the soft selection at each position is performed over blocks of different size that correspond to that position and does not take into account global information e.g. by computing scores of all possible segmentations to maximize for the most likely one.  \n2. The current framing would suggest comparison to existing tokenization methods like BPE or recent  optimized ones to the downstream task [1,2] in a controlled setting (e.g. similar vocabulary size and contextualizer design), but this was not explored at all in the evaluation. In addition, there is no proper evaluation of the learned segmentations other than the visualization of the learned weights for a single example. \n3. The results are not that good compared to pure character-level models on average which makes results less exciting. Also, one of the claimed benefits of the proposed method is that is faster is mainly due to the fixed downsampling which is not something new and applies to other character-level baselines. \n4. Studies that focus on tokenization like the ones cited below evaluate on tasks where tokenization is important for reaching state-of-the-art performance like machine translation. It might be worth evaluating there.  \n\nQuestion:\n* In section 8.2 about monolingual datasets, was the downsampling rate optimized only for the proposed model?\n\n[1] https://aclanthology.org/2020.findings-emnlp.120.pdf\n\n[2] https://arxiv.org/pdf/2012.15671.pdf\n",
            "summary_of_the_review": "The paper focuses on an interesting problem and has many experiments with state-of-the-art results in certain settings. My main concerns are regarding its novelty, framing, and evaluation.  Even though some of the results are competitive, the experiments do not provide enough evidence that the learned  tokenization is crucial for achieving the results. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper focus on a gradient-based subword tokenization (GBST) method for byte-level transformers. The key idea is to use a linear transformation (with trainable parameters) + softmax to compute embedding for each character based on several n-grams in its neighborhood, and then use average pooling to downsample and decrease the sequence length.",
            "main_review": "Strength\n\n1. The GBST module is simpler and faster. It has on-par performance but is 35%-80% faster with less memory usage than current SOTA byte-level transformer ByT5. Comparing with the lighter ByT5+CANINE model, it has similar speed and memory usage, but more straight forward and has better accuracy. It also has on par performance and less computation costs in most tasks comparing with T5/mT5.\n2. The re-scaling of the model (SBase) achieves better performance with ~20% less computation. Though this idea to have deeper encoder is from previous work, it’s still informative to have the experiment.\n3. The paper is very well written and has enough experiments to show the model’s performance in accuracy and capability in different tasks. \n\nWeakness\n\n1. (minor) The technical novelty of this paper is only GBST, a dense layer over n-grams, while training setting follows ByT5, and the best SBase model is also an implementation of the deep encoder concept.\n2. The scalability of the model is not shown. All the experiment focusing on models (other than mT5) with a parameter scale of 200M. But the paper ByT5’s smallest model has 300M parameters and performs much better than with 200M parameters (comparing Table 4 of ByT5 and Table 4 of this paper).\n3. (minor) T5 seems much better than SBase under Zero-Shot settings, though it takes more computation. \n4. From the last sentence of 3.1 Baselines, it seems that the ByT5 model used for comparison is unscaled, and I couldn’t find in the paper about structure of this ByT5 (sorry if I missed that). This is important since in the ByT5 paper all the model variants have much heavier weight in encoders, and there is also an ablation study to prove that heavier encoder can improve performance a lot. In this paper, the scaled Charformer-SBase is also much better than the unscaled Charformer-Base. Thus it’s a bit strange that the best settings of both models are not used for comparison.\n\n",
            "summary_of_the_review": "To summarize,\n\nPro\n\n1. Excellent paper writing.\n2. Extensive experiments.\n3. Simple but effective token-free module.\n\nCon\n\n1. Lack of experiment on different sizes of the model, especially performance when with a larger scale.\n2. The comparison seems not between the best setting (deep encoder, shallow decoder) of both baseline and this model.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work first implements a byte-level (character) tokenizer, which is learned as part of the model rather than as a preprocessing task. To limit the computational burden of character level encoding on the downstream architecture, the method creates byte n-grams and combines them via a scoring network as outlined by the authors. Common Transformer architecture can use the tokenizer and even be narrowed. Finally the model is evaluated using multiple NLU and noisy NLP tasks. As a result, the approach achieves comparable end-task performance to sub-word and other byte-level Transformers, while improving parameter efficiency as well as ease of use for non English NLP tasks.",
            "main_review": "Strengths:\n+ The work makes tokenization more adaptable by learning sub-tokens on-the-fly.\n+ Training speed and parameter efficiency are improved over more rigid, sub-word tokenization based Transformers in many cases.\n+ The tokenization method is based on a weighting of pooled byte n-gram embeddings. Score calibration and downsampling are technically sensible and the authors make a noticeable effort to ease optimization and not introduce foreseeably brittle complexities like extra hyperparameters.\n+ The n-gram weighting approach allows the sub-token weightings to remain somewhat interpretable.\n+ The downsampling operation allows an easy way of saving later transformer stack parameter size, which is shown to increase learning speed and reduce parameters.\n+ Potential limitations that require future work are mentioned proactively.\n+ Instances of reasoning about technical choices, e.g. \"narrower encoders\", provide instructive details and help make the foci of the experiments more deducible.\n+ Experiments are intentionally controlled (deconflated), to produce more robust insights.\n+ Table 2 uses AUC-PR rather than the original works AUC-ROC as performance measure, which can be considered an improvement.",
            "summary_of_the_review": "This work first implements an end-to-end trainable byte-level (character) tokenizer, while limiting computational overhead via a learned token weighting. It is compatible with common Transformer stacks, but can reduce their width for computational gains, without sacrificing performance, and in some places gaining performance. The evaluation on multiple NLU and noisy NLP tasks demonstrates only slighly less to slightly stronger performance compared to byte and sub-word tokenization models. Ease of use, being conceivably language agnostic, training more efficiently and allowing easy reimplementation (pytorch), make this work a welcome contribution to the field.\n\nTherefore, I recommend accepting this work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This work aims to let the model learn the subword representations by itself from character-level embeddings of the input without tokenizing. A gradient-based subword tokenization module (GBST) is designed for replacement of the general tokenization. The proposed CHARFORMER, a re-scaled Transformer architecture (transformer with GBST) that integrates gradient-based subword tokenization, outperforms several byte-level baselines and performs on par with some subword-level models on three kinds of benchmarks, including GLUE, multilingual, and noisy text datasets. Moreover, CHARFORMER has the advantages of less memory usage, faster speed compared to the models with similar parameter size.",
            "main_review": "Strengths:\n\n1. The paper proposes a novel gradient-based tokenization module to address the issue of rigid subword tokenization algorithms and attains competitive performance. The proposed method saves engineering efforts for tokenization, which would be easy to generalize to different tasks. \n\n2. It is interesting to find that this module can deal with multilingual and noisy datasets to some extent, which gives credit to learning subword representations from characters. \n\n3. This proposed token-free module (GBST) has less computational cost than using the general methods of tokenization and can be well extended to other models.\n\nWeaknesses:\n\n1. The performance is not surprising, though the model indeed saves model size. It is not clear if the method works for other languages. For example, character-based models tend to achieve similar performance compared with word-based ones. Would this method has advantage over the character-based baseline then?\n\n2. The evaluations are based on base models. Since this model saves model size, would it achieve better performance compared with the same size (e.g., 200M like Byte-level T5 Base in Table 1)? Do you try large models? Besides, it would be impactful if this paper showed that GBST module could actually generalize in other kinds of models. \n\nMinor Comments:\n\n1. I wonder if average pooling is good way to down sample though it is really fast, but how about MLP? \n\n2. In Eq.3, the indices should be 0 to M-1 or 1 to M (total number of block size is M), i.e. $P_i = \\softmax([p_{0, i},  p_{1, i},  \\dots, p_{M,i}]$ -> $P_i = \\softmax([p_{1, i},  p_{2, i},  \\dots, p_{M,i}]$\n\n3. At Figure 1 (b), it seems the annotation `P5:8` in the fourth line is missed.\n\n4. In section 2.1.2, block sizes 1 < b < M should be 1 <= b <= M\n\n5. In section 2.1.5, “latent subwords $\\hat{X} = [\\hat{X}_i, \\dots, \\hat{X}_M]$  should be “latent subwords $\\hat{X} = [\\hat{X}_i, \\dots, \\hat{X}_L]$\n",
            "summary_of_the_review": "This work is well-motivated. The overall design of the model is technically sound, and it provides moderate improvements on several datasets, while being memory efficient, faster, with fewer parameters. It is not clear if the advance of the method can generalize to other languages or larger models.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}