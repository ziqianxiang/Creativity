{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper show that in several different neural network  architectures, recurrent  \nnetworks that share parameters over iterations have comparable \nperformance and similar features to feed-forward networks of the same\n\"effective depth\".  \n\nReviewers initially had some reservations about novelty and \ngeneralizability to deeper SOTA networks.  These were successfully\naddressed by the authors and all reviewers feel the paper is above the\nbar due to the importance of the area, and that this paper brings\ntogether many important insights that, while many may have been known \nbefore, had not previously been all brought together before.  The maze \ntask was also considered a useful task for the field.  I agree that\nthe paper makes a worthwhile contribution and am in favor of \nacceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors explore the similarity between neural networks that accrue information processing via depth and via recurrent iterations. They have conducted several interesting qualitative and quantitative analyses to study the above question and conclude that recurrent processing mimics depth in the experiments they have conducted and draw links to relevant work in computational and experimental neuroscience that also study the role of recurrence in information processing in the cortex.",
            "main_review": "Pros:\n- I believe that the authors are studying an interesting, although not new, question about the similarity between recurrence and depth in information processing in neural networks. There has been immense progress in the field of AI (particularly computer vision) with feedforward models and it is puzzling why recurrent models haven't caught up (despite being computationally sophisticated and present in abundance in the cortex). This work is sparking further interest in studying the relationship between depth and recurrence.\n- The authors conduct extensive quantitative and qualitative analyses with several model families and diverse effective depth ranges to test their hypothesis that recurrence mimics depth. \n- The newly proposed maze challenge is very interesting and connects well to prior work in Vision science and psychophysics that study the influence of global cues in visual processing. \n- I find Section 5 on recurrent models reusing features to be interesting, but the clarity of writing in this section could be improved for better understanding on the readers' end.\n\nCons:\nI have the following concerns about this work:\n1. How are the findings in this work different from Liao and Poggio 2016 [1]? For e.g. Figure 3 in their paper conducts the same analysis that is conducted by Section 3 (albeit with a wider range of datasets). While the observed results confirm that deep architectures and recurrent architectures achieve similar classification accuracy, I didn't find this section particularly novel.\n2. The studied recurrent models are very simplistic in nature (stack of convolutions with weight sharing) and do not incorporate several crucial components that make recurrence interesting (both in ANNs and in the brain) such as gating, bypassing, temporal decay, etc. \n3. There isn't enough details about the recurrent architectures leading to following unresolved questions: Exactly how many layers are recurrent in each of these architectures? How many steps of recurrence is implemented in these layers?\n4. The maze solving task is interesting, however, could one explain the results merely based on the effective receptive field size growth with recurrence and depth? The authors say that RF size cannot explain the full effect as the accuracy increases beyond the point where models achieve enough RF size to capture the full maze, however this is confounded with increase in # parameters and capacity as the effective depth is increased. The authors themselves report results from a dilated convolution experiment showing superior performance with larger receptive fields at shallow network depths. Hence, I strongly feel that the similarity observed can be largely reduced to both recurrent and feedforward networks increasing their effective receptive field size with more depth / iterations.\n5. I feel that the paper makes quite a few very strong claims in the Discussion section that aren't fully tested by the Methods. I feel that this section needs to be toned down further to promote more exploration into these areas through future experiments.\n\nReferences\n[1] Liao, Q., & Poggio, T. (2016). Bridging the gaps between residual learning, recurrent neural networks and visual cortex. arXiv preprint arXiv:1604.03640.",
            "summary_of_the_review": "I find the question approached by the authors to be an interesting (although not novel) one and requires studying from various angles in order to shed more light on how recurrence and depth are related. The authors have made a nice effort to perform various analyses (both quantitative and qualitative) to study their hypothesis that recurrence mimics depth of processing. However, I find part of this study (Section 3) to be quite similar to prior work in this area although scaled up with more datasets. In Section 4, I find contradictory claims such as the authors claiming that performance rise not fully explained by receptive field size, and contradicting this statement with models containing dilated convolutions (exponential receptive field growth with depth) that produce large performance gains. The recurrent models studied are quite simplistic and lack several interesting components that have contributed to the expressivity of recurrent processing in the past (such as gating, bypassing, nonlinearities, temporal decay etc.). \n\nAt the current stage, I feel that the readers do not gain much new information about the relationship between depth and recurrence beyond what has been established by prior work. I suggest the authors to experiment with more sophisticated recurrent networks and explore more novel analyses (such as the maze challenge, which I find to be really interesting but needs to be scaled up to explain further similarities beyond receptive field size growth) to understand the relationship between depth and recurrence. This is what I feel is required to truly expand our understanding of (whether, and under what conditions) recurrence mimics depth, and under what conditions / tasks the former is more expressive than the latter and vice versa.\n\n======= Update after author response =======\n\nI have read the authors' response and updated manuscript, after which I am increasing my score to 6. I believe that the updated manuscript's contributions are clear and builds on the interesting work from Liao and Poggio, 2016, adding further information with novel analyses and a new dataset (Maze challenge). I believe this work will spark further discussion and exploration of the similarity between recurrence and depth.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this work, the authors compare and contrast models wherein layer depth is replaced with equivalent number of recurrent time steps. In particular, they explore feed-forward, CNN and residual deep network wherein the intermediate layers are replaced by an equivalent recurrent block. For each architecture, the authors compare accuracy of the stacked and recurrent variants for image classification on 4 datasets- CIFAR-10, EMNIST, SVHN and ImageNet and a novel Maze task wherein the network has to assign binary labels to each image pixel (0= not a valid point in path, 1=valid point in path to solve the maze). Overall, the authors observe that across all models and tasks, the recurrent and stacked networks achieve similar performance. They also verify this with batch normalization and dilated CNNs in some tasks. Next, they investigate the number of positive activations at each recurrent step to find that filters are active/recycled across recurrent depth. They also analyze the linear classification accuracy at different depths of the stacked and recurrent CIFAR-10 feedforward model, finding comparable accuracy at each depth. Finally, they visualize filter activations at different depths for both CIFAR-10 & ImageNet, finding comparable feature maps in both architectures.",
            "main_review": "#############\n\nUPDATED REVIEW:\n\nThank you answering all my questions and apologies, for the delayed response.\n\nUpon reading the rebuttal and revised draft, I believe the authors have satisfactorily addressed my concerns and hence, I am increasing my score by 1 point. I would be interested, however, to see how the hierarchical representation plays out in the future with different architectures and more complex tasks!\n\n#############\n\nStrengths: The paper asks an interesting question on whether depth in artificial neural networks can be replaced with recurrence that would use much fewer parameters. The analyses on qualitatively visualizing the learned filters was exciting and insightful in how these models represent information/learn hierarchical structure even with recurrence.\n\nWeaknesses: My comments and questions are listed below.\n\nImage classification task:\nI am not sure I find this result particularly surprising. In both MLP and R-MLP, there are additional non-linear transformation on the model inputs and this leads to some patterns in accuracy. Granted that the “inputs” to the middle layer are recurrent in one case and not the other, which has implications on the number of model parameters. But in several cases:\n\n1. Are the differences between the effective depth across a model architecture statistically signifiant to begin with? If not, I am not sure it is fair to claim that the recurrent versions mimic the non-monotonic patterns of depth. Instead, they just achieve similar accuracy to the original model \n\nThe above comment is in reference to:\n> First, the trend in performance as a function of effective depth is not always monotonic and yet the effects of adding iterations to recurrent models closely match the trends when adding unique layers to feed-forward networks. \n\n> In Figure 2, the striking result is how closely the blue and orange markers follow each other. \n\nFor example, in the MLP based models, it is not clear if the differences across depth are significantly different from each other.\n\n2. The biggest differences in accuracy seem to arise from different architectures themselves and not the absence or presence of recurrence. This further suggests that the types of functions approximated by the model (either as stacked non-linearities or recurrent ones) is more important.\n\n3. For ImageNet, we observe a monotonically increasing accuracy curve as the number of non-linearities in the model increase (irrespective of how they increased). What conclusions do the authors draw from this aside from the following?\n> These experiments show that the effect of added depth in these settings is consistent whether this depth is achieved by iterating a recurrent module or adding distinct layers. \n\nWhile the results are demonstrative that using recurrence (and hence, fewer parameters) one can achieve similar task performance on image classification, without a theoretical proof, I do not think they indicate that the models learn similar functions (I understand the authors never claim this but perhaps I'm missing the bigger/less obv picture painted by the generalization performance result).\n\nMaze task: The dataset itself is very interesting! As a minor point, the authors mention that fewer than 0.5% of the mazes are duplicated within each of the training sets. What is this statistic for the train-test overlap? I also found the linear probing analyses in section 5.1 to be demonstrative of the increasingly separable features learned as (pseudo-) depth increases.\n\nMy comments and questions in the image classification task apply here as well.\n\nTo me the most exciting part of the paper was the analysis in section 5.2. While it seems obvious to me that accuracy improves as depth increases (whether or not the parameters are reused), the finding that qualitatively these models learn similar features despite using different mechanisms of weight sharing is insightful. It suggests that the information captured by neural networks is possibly not in the parameters themselves but the states of the model. However, I would be interested in learning the authors' thoughts on whether this is expected given the increasingly non-linear nature of the input to the model. Further, can the authors comment on the lack of alignment in the recurrent and feed-forward columns visualized in Fig. 12?\n\nThe paper would greatly benefit from adding theoretical proofs to suggest that these models are perhaps approximating similar functions or adding a wider suite of complex tasks & architectures. See for example this paper that theoretically and empirically proves recurrence and depth are equivalent in RNNs: https://arxiv.org/pdf/1909.00021.pdf\n\nIn its current form, I am not convinced the generalization results are surprising/insightful given that both version of the models have access to stacked non-linearities but I encourage the authors to explore and emphasize on the hierarchical structure emulated by recurrence in the future.\n> we find that recurrent networks can emulate both the generalization performance and the hierarchical structure of deep feed-forward networks\n\nOther questions:\n- What are the results for classifiability of features with other architectures? (table 3)\n- How was hyper-parameter tuning done? What are the train-val-test splits for different datasets? How much time did each model take to train and test? Was there signifiant differences in optimizing one over the other?",
            "summary_of_the_review": "The paper has interesting results on how hierarchical representations emerge in recurrent networks akin to stacked networks, despite the former sharing parameters across its \"depth\". However, the similarity in task accuracy between recurrent and stacked networks with similar number of non-linearities is not terribly surprising. It also doesn't shed light on whether these models approximate similar functions on whether this finding can be generalized to other task paradigms. I believe this is a promising research direction and encourage the authors to analyze the similarities and differences in the information learned by the two paradigms in more depth.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work examines what constitutes depth for deep networks by examining inference for networks with and without distinct parameters at each layer. Sharing weights across layers defines networks that are recurrent in depth, and still compose the same number of nonlinearities, but reduce the number of parameters. Recurrent architectures of this kind challenge a common intuition about hierarchy in deep representations and so deserve analysis. The experiments in this work carry out controlled comparisons between pairs of networks with shared/unshared parameters, and measure: (1) task accuracy, (2) depth vs. receptive field, (3) activation statistics across layers, (4) discriminability of features across layers, and (5) visualization of filter responses. The tasks studied are image classification, with standard datasets like ImageNet and CIFAR-10, and a custom task concerning images of 2D mazes. The architectures control for differences between the feedforward/unshared and recurrent/shared models, but exclude standard architectures as references. The networks in these experiments are relatively less deep (at 19 or fewer layers), and differ from standard networks (like a ResNet-50) in their lack of pooling and fixed channel dimensions. As suggested by the title, the results for a given depth are indeed quite similar with or without recurrence, at least in most cases for the networks and tasks studied. In this scope, the work convincingly shows that distinct parameters are not necessary, but that does not necessarily generalize to more standard deep networks for vision, nor to other modalities of data, nor to other prediction tasks.",
            "main_review": "Strengths\n- **Intriguing Topic**. Deep learning papers, lectures, and even books at this point all underline the importance of parameters in the success of deep networks as compositions of learned and nonlinear functions. Scrutinizing this more closely, and questioning if its the sheer number of parameters that matters, is of definite interest. Existing work provides evidence that recurrence across depth/shared weights can suffice, but a paper that thoroughly studies shared/unshared layers could draw more attention to the topic. This is true whether the results reinforce or erode the existing intuitions!\n- **Variety of Measures**. While task accuracy is studied first, and should be, further experiments examine processing through other measurements. For example, depth is contrasted with receptive field size (for mazes, Sec. 4.2), activation magnitudes are compared across layers (Figure 6), and discriminability is checked layer-by-layer (Table 3). Visualization is also used to qualitatively compare filter responses across recurrent and feedforward models (Figures 7 & 8).\n- **Clear Writing**. The introduction is direct and concise in justifying the research question of whether and how models with shared weights differ from models with distinct weights. The setup (Sec. 2) defines its terms and goals, with sufficient detail on the architectures studied to reproduce the experiments. The figures and tables are legible and appropriately captioned.\n\nWeaknesses\n- **Missing Related Work**. The related work is incomplete and does not sufficiently credit existing demonstrations of networks that are recurrent in depth. [A] analyses the effects of sharing weights across depth for convolution and contrasts it with other design choices. [B] defines and experiments on partially recurrent residual networks, and addresses the same point about normalization as Section 3.1 in this work. [C] explores aggressive sharing of weights across depths in residual networks and compares with standard architectures at common scales of model depths and parameters. More afield, but not unrelated, are methods that unroll an optimization process and show that sharing weights can be effective, such as [D]. This work could provide more in-depth analysis, or more up-to-date analysis w.r.t. the models considered, but prior work must be credited, above all in case the earlier work can inspire more analysis.\n- **Narrow Experimental Scope**. The experiments are restricted to relatively shallow nets (with depth of 19 or less) in the visual domain. The chosen architectural constraints, like no intermediate pooling or change in channel dimensions, do conflict with standard networks. Experiments that control for these are valuable, and are included in this work, but the experiments should also examine standard architectures. At the current scope, I am concerned this work runs the risk of confusing matters, while a broader look would be more clarifying for the community. To start, experiments could cover deeper nets, and then go on to look at other tasks like sequence modeling (as done by implicit nets, which also share weights).\n- **Technical Redundancy**. Recurrent nets of this type have already been defined and experimented on, as done by [A, C] and partially [B]. The prior existence of such networks leaves this work without technical novelty. To be clear this is not a fatal flaw, but it does squarely place the value of this work in the breadth and depth of its empirical results.\n- **Task Choice**. The maze task is non-standard, and therefore less informative than any other established task, although it does make some sense. The analysis does support its soundness by showing that there is a distribution of difficulties, that depth helps across them, and that receptive field size does not fully explain away the results. However, all of this could be spared by adopting a task like semantic segmentation on PASCAL VOC or Cityscapes, which are both well-known, and even image-to-image tasks not unlike the maze task.\n\nFor Rebuttal\n- **Related Work**. Please position this work relative to the papers raised in this review, especially [A, B]. Please also situate the contributions of this work w.r.t. the implicit models in work on DEQs and NODEs that show such models can rival explicit models, and even include purely recurrent models (as ablations of their inference optimization) as baselines as in Bai et al. 2020.\n- **More Depth**. Please experiment at depth=50 for the results reported in Figure 2. Please likewise experiment with a standard ResNet-50 architecture and compare it to a recurrent analogue, such as a net that shares the parameters across blocks in each stage (but not the normalization layers, as higlighted by Section 3.1).\n- **Reuse Analysis**. Please discuss other options for measuring reuse, and furthermore interventions to check the analysis. For instance, what if channels are masked out (that is, multiplied by zero) at their least active layer? Severe loss in accuracy from this masking would provide stronger evidence that the reuse is happening and necessary. In the same vein, what does Figure 6 look like for an untrained network? If it differs, this would support the use of the proposed analysis by counting.\n\nMiscellaneous Feedback\n- [clarity] Please define \"depth\" up front, in the introduction. The definition at the beginning of Section 2 is good, but a one sentence summary in the first paragraph or at least on the first page would orient the reader more immediately.\n- [format] Please consider sharing the y axes across the plots in Figure 2 to consistently convey the size of the differences.\n- [method] For normalization (Table 1), consider sharing the parameters—but not the statistics—of the batch norm layers. Sharing the affine parameters would make the model even more recurrent, while still allowing for different statistics across recurrent iterations. In the same vein, consider a comparison with group norm, which is known to work with ResNets, but unlike batch norm has purely local state.\n- [clarity] The neuroscience reference in the discussion comes out of nowhere, and would be more illuminating if its content was concretely connected to the architectural choices and experimental results in this paper.\n- [related work] As a possible counterpoint to theory of layer specialization, you may be interested in reading https://arxiv.org/abs/1605.06431 for its experiments that question the importance of any one layer in a residual network.\n\nReferences\n- A. Understanding Deep Architectures using a Recursive Convolutional Network. Eigen et al. arxiv'13 and ICLRW'14.\n- B. Residual Connections Encourage Iterative Inference. Jastrzębski et al. ICLR'18\n- C. ShaResNet: reducing residual network parameter number by sharing weights. Boulch. arxiv'17 and PRL'18\n- D. Conditional random fields as recurrent neural networks. Zheng et al. CVPR'15",
            "summary_of_the_review": "It is nice to see this work take a closer look at what exactly differs or not across layers with distinct or shared parameters. This is an important topic to scrutinize since deep models are now so common while sharing weights across depth is still rather rare. The intuitive or \"folkloric\" understanding of deep networks as strictly hierarchical is not beyond empirical analysis, and so in general I welcome this effort. Nevertheless there is more to be done for this work to be sufficiently thorough and informative. Putting aside the missing related work, and considering only the content in the paper, the analysis done here is quite narrow. The only modality considered is vision, the only mainstream task is classification, and standard/unshared models are only studied where they most closely align to recurrent models. Inference is studied in several aspects, like accuracy, feature visualization, and layer-wise prediction, but optimization is not addressed at all. That is, does a recurrent model train differently than its paired feedforward model, even if they ultimately converge to similar accuracy? In summary, I encourage the authors to continue in this direction but with a wider lens, and re-submit with more coverage of different data and tasks, and most of all broaden the bounds of which shared vs. unshared architectures they consider (for instance, deeper nets like ResNet-50 due to its popularity, and perhaps hybrid architectures that still have pooling but share all weights within a \"stage\" at a given resolution, and so on.).\n\n**Final Review** The author response addressed the points requested for rebuttal: inclusion of missing related work, an experiment at greater depth with greater resemblance to a standard ImageNet classifier, and more justification of the layer-wise activation analysis. While I still find the technical and empirical novelty of this work to be lacking, as much or all of it can be reconstructed from the related work, I can see value in the unified experimental setup and the juxtaposition of different results. As this paper could bring more attention to recurrence and depth, as earlier work seems not to have done, it could inform the community. I have therefore raised my score to 6.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper shows that sharing the weights between inner layers of image classification networks has minimal effect on their performance: they achieve similar accuracy with significantly fewer parameters. This questions the commonly accepted view that later layers of CNNs function as more and more specialised filters. It also suggests that instead of the number of weights, the amount of computation might be the main reason behind the good performance of the deep models. Interestingly the authors use feature visualisation to show that although the weights are shared, the network still learns hierarchical features.",
            "main_review": "# Strong points\n\n-   The paper is clear and easy to understand\n-   The authors conclude multiple experiments to support their claims\n-   They use multiple datasets, including ImageNet\n-   They use multiple network families, including MLPs, CNNs and ResNets\n-   They also use linear probing of layer-wise representation power, which support their claims\n-   Feature visualisation also supports their finding\n-   The correspondence of feature visualisations, linear predictive power and general trends is very surprising.\n\n# Weaknesses\n\n-   The minimal effect of weight sharing was already shown in some cases [1,2], although this paper analyses it much more thoroughly and shows a broader picture of the effect    \n-   There are some indications that the similarity would break down with deeper networks.\n-   The analysis of feature reuse can be because of different effects (Section 5, first part).\n-   For filter visualisations, it is unclear whether the same neurons are responsible for high and low-level features\n\nPlease find more details below.\n\n# Questions/suggestions\n-   For the maze problem, it is unclear how the performance is measured. Is it pixel-wise, or the whole maze should be correct for the sample to be considered correct?\n-   For the ResNet architectures, the parameters of the batch norm are not shared between the layer. But multiple papers are suggesting that these parameters can have a significant effect on the behaviour of the network [3,4,5]. Therefore, it would be worth considering making a partially shared variant of the batch norm that shares the affine parameters but not the statistics.\n-   The analysis of the activation strengths (Section 5, first part) might not indicate what one might think. If there are biases in the network, the outputs of specific neurons can be high even when “they are not doing anything”. Also, it is questionable whether having high activation most of the time means that they are actually doing computation most of the time. The actual information might be carried on top of a high bias. That said, I don’t have a good suggestion on how this can be adequately measured. Maybe measuring variance between the different samples might be a better metric. Probably visualisation of the same unit in different depths would be informative. Maybe soften the claim in a bit when discussing.\n-   For feature visualisations, it is unclear whether the low-level and high-level features shown in the figures come from the same or different neurons for feature visualisations. How does the presence of the high and low-level features change through the layers? Figure 12 shows high-level features together with low-level ones already at depth 11. It would be nice to visualise the same neuron over “depth”.    \n-   It would be nice to cite [6] when discussing feature visualisations in Section 5.2. It is an outstanding analysis of how networks work.\n    \n# Additional suggestions that could make the paper better\n-   In Figure 2, the gap starts growing noticeably with deeper networks (19 layers on CIFAR-10, EMNIST, ImageNet). It seems like the RNNs overfit sooner than the CNNs (CIFAR-10, EMNIST), which is the opposite that I would expect. It would be worth investigating this effect more.\n-   The ImageNet experiment uses a very small ResNet and with 19 layers. Together with the previous point, I would be curious about the gap with a more realistically-sized ResNet.\n\n[1] Liao et al: Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex (2016)  \n[2] Alexandre Boulch: ShaResNet: reducing residual network parameter number by sharing weights (2017)  \n[3] Revisiting Batch Normalization For Practical Domain Adaptation (2017)\n[4] Frankle at al.: Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs (2020)  \n[5] Kanavati et al.: Partial transfusion: on the expressive influence of trainable batch norm parameters for transfer learning (2021)  \n[6] Cammarata et al.: Thread: Circuits (2020)  ",
            "summary_of_the_review": "I recommend accepting the paper because it highlights the importance of computation depth instead of the number of parameters (which is commonly assumed to be a good indicator of the predictive power). This might motivate further research in models with increased computation depth. Despite the weaknesses, the authors presented enough evidence for supporting the main messages of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}