{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper investigates how well properties invariant to changes such as lightening and background learned in the major class can be transferred to the minor class. In this paper, the authors reveal that invariances do not transfer well to small classes, and suggest that resolving this phenomenon can help increase the performance on imbalanced datasets. From this point of view, the authors propose a generative model-based augmentation technique.\n\nThree reviewers suggested acceptance, and one reviewer judged borderline reject. It seems true that the method is not novel enough, but it is solid and well motivated. In particular, the finding of the paper is interesting and the design of the experiment is well done, so I think that it will have a great influence on research in this field in the future. As the negative reviewer mentioned, the lack of large-scale experiments is a major weakness of this paper. I strongly encourage the final version to supplement the promises made to the reviewer, including adding iNaturalist experiments."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper investigates if invariances learned by the model transfer across classes. Focussing on class-agnostic nuisance parameters, the interplay between per-class size and invariant representations has been explored. It is suggested that while networks can become invariant to these parameters for classes with many examples, it is unclear if this is also the case for classes with fewer examples. The paper shows this is not true i.e. invariances do not transfer well to small classes, and suggest that improving this can help increase performance on imbalanced datasets. And so, the paper proposes a two step solution to this problem. First, an image-conditional generative model is learned which learns to transform the image such that only the nuisance parameter changes. Secondly, this model for nuisance parameters is used for data augmentation. Using this approach, the authors are able to achieve a significant improvement on standard long-tail datasets.",
            "main_review": "Strengths:\n\n1. An extremely important problem: transferring invariances across classes opens up many avenues. For instance, to become invariant to any new transformation it would be sufficient to collect diverse data with only a few classes. This can help reduce resource required resources significantly. \n\n2. The approach of modeling nuisance parameters using generative model and then propagating them to low frequency classes is very interesting.\n\n3. The evaluation and experiment design is solid and rigorous.\n\n4. Paper is very well written. It is easy to follow and understand. \n\nMinor Weaknesses:\n\n1. I am curious how GIT compares to other methods attempting to specifically enforce invariance. One strength of GIT is that it doesn't require knowing the transformation. However, in the case of dilation/erosion and background intensity showed here, the transformation is known. So, it is possible to get an exact value for an upper bound of what GIT could have achieved. It would be good to include this upper bound to know how well GIT performs.\n\n2. Equation (1) is missing the superscript (i) for the image x. \n\n3. Missing literature: Some recent literature on the role of dataset size/diversity on invariances is missing which would be good to comment and connect to [1,2].\n\n4. I wonder where the most important contributions of the paper are. To me, it seems that bulk of the experiments are on quantifying their approach as a solution for class imbalance. So, I would suggest that the title and introduction should be adapted to focus more on this aspect. However, if instead the authors believe the main contribution to be the evaluation and solution for transfer of invariances, numbers mentioned in point 1 above should be mentioned.\n\nReferences\n\n1. Madan, S., Henry, T., Dozier, J., Ho, H., Bhandari, N., Sasaki, T., Durand, F., Pfister, H. and Boix, X., 2020. When and how do CNNs generalize to out-of-distribution category-viewpoint combinations? arXiv preprint arXiv:2007.08032.\n\n2. Yang, G.R., Joglekar, M.R., Song, H.F., Newsome, W.T. and Wang, X.J., 2019. Task representations in neural networks trained to perform many cognitive tasks. Nature neuroscience, 22(2), pp.297-306\n\n3. Achille, A. and Soatto, S., 2018. Emergence of invariance and disentanglement in deep representations. The Journal of Machine Learning Research, 19(1), pp.1947-1980.",
            "summary_of_the_review": "The paper investigates an extremely important problem: class-imbalance, and shows that part of the poor performance on classes with less samples stems from poor transfer of invariance to these classes despite good invariance building up for larger classes. Their proposed solution is thus very well motivated, and is a very nifty idea to model nuisance parameters. All in all, this is a solid applications paper in my opinion, and I think some minor changes in writing to focus more on fixing class-imbalance would make it a good paper for the computer vision community.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper investigates if robustness (or \"invariance\") to nuisance transformations which do not change the class label such as lighting, rotation etc are learned across all classes or if such robustness is sensitive to the class size. The paper demonstrates that such invariances seem not to transfer across classes: ie the classes with fewer examples suffer more. THe paper proposes a generative model (GIT) to \"augment\" the less frequent classes which, to some degree, remedies this problem.\n",
            "main_review": "Strengths\n\n+ Very interesting and fundamental research question \n+ Insightful analysis\n+ Well written, clear, and easy to understand\n+ The GIT solution improves\n\n\nWeaknesses\n\n- I find the empirical risk minimization setting strange: the paper assumes a different distribution of the training data from the test data.\n- I am not convinced of the main result in fig 1 (It could \"just\" be the class frequency; and not the transformation). See my detailed review below.\n- The abstract claims an \"explanation\" but this is actually what I miss. Why is this effect is happening? Sure, a generative model can remedy the symptoms to some degree, but what is actually the cause of these symptions?\n- Paper is not self-contained (I need the appendix for essential parts, thus, the appendix becomes part of the page limit?). See my detailed review below.\n- I miss an analysis on GIS for \"rotation\" (fig 1 vs fig 3)\n- I find it strange that the effect only seems to hold for supervised learning. So, if the class labels are removed but the dataset is the same, does the effect no longer occur? (ie: the GIS is trained unsupervised and seems to not have trouble in learning these general nuisance effects..)\n\nDetailed review:\n\n- Minor detail in general: The paper is fond of using \"very\" very much. A small tip is to try to avoid \"very\" it achieves the opposite of what the writing aims to achieve: http://www.writerswrite.co.za/45-ways-to-avoid-using-the-word-very/\n\n- Minor detail: Abstract: \"In order to\" can nearly always be replaced by \"To\".\n\n- Abstract: \"much less invariant on smaller classes\", what does this mean? Isn't invariance to a transformation T a binary property? ie: T(f(x)) = f(x) or is some (relative?) distance/similarity measure implied? I also assume its not about the classes, but about the nuisance transformation T in those classes? Perhaps the word \"robust\" is better?\n\nPage 1: \"where withholding explicit class information\" so, is it then about the class information? Or about the occurrence frequency of the appearance of a class? Ie: this statement makes me wonder if class labels play a role here at all? Ie: in unsupervised settings there can also be an imbalance in the 'semantics' as measured by the nr of samples (but no explicit class labels are used). Basically: Why does this effect hold when using labels, and why does this effect not hold without using labels (but using the same dataset). It seems to me that the generative approach proposed later by this paper does not suffer from this effect?\n\nFig 1: How can we be sure that this effect is due only to the transformation and to nothing else than the transformation? I could pose an alternative explanation to this graph which is: Its generally more difficult to learn the classes that have fewer examples. This would also explain the shape of these curves, without relying on transformations. Ie: I miss a figure where there is no transformation at all, to see the baseline effect of having fewer examples (perhaps this can be achieved by measuring the eKLD for different initialized runs of the same model trained on untransformed data only? This would give an estimate of P_w || P_w ). I assume that this \"no transformation baseline\" would follow the same trend for ERM; yet perhaps be completely uniform for CE+DRS (?) So, generally, for this figure, I wonder how to investigate the exclusive effect of the transformation without measuring the confounded effect that \"classes with fewer samples are more difficult\".\n\nPage 2, minor detail: \"in long-tailed and class-imbalanced settings\", Its a bit unclear to me what the difference between these two setting is (?).\n\nPage 2: \"we find that combining resampling methods with GIT further improves\", it seems to me that GIT itself is another version of re-sampling? Perhaps a more specific method geared towards invariance?\n\nEq 1: Minor detail: shouldn't \"x\" be \"x^{i}\" ?\n\nPage 3: \"want our model to perform well across all classes\". This is strange to me. In ERM a strong assumption is that P_train is distributed identically as P_test. In the setting where performing well over all classes is required, thus changes P_test from P_train. Does it make sense at all to use ERM then? Put in another way: if some classes are rare in the considered problem setting (which is why they are rare in P_train) then why is it to be expected that suddenly they do occur frequently when deploying the model? (not rare in P_test) ? Ie: optimizing *average* risk seems not the final goal here?\n\nPage 4: About the conclusions about fig 1: see my earlier remark (how can we be sure that this effect is just because of the transformations, and not due to the small nr of samples?)\n\nPage 5: I'm a bit confused what conclusion is now given by \"Hence modeling the transformations directly is the more natural choice on imbalanced datasets\". This paragraph seems to argue that a generative model does *not* suffer from class frequencies? Is this the goal of the paragraph? And if so, is that claim then true?\n\nPage 6-7: All classifier/architecture information is missing. A paper should be self-contained. If it is mandatory to read the appendix to understand the paper then the appendix is an essential part of the paper and thus the paper is over the page limit.\n\nFig 3: Where is the \"rotation\" experiment from Fig 1? The paper is incomplete. (If the appendix contains such essential results, then, in my opinion, the appendix becomes part of the main submission and should adhere to the same page limits that hold for other authors)\n\n\n",
            "summary_of_the_review": "I'm sorry if my review reads negative, but I really like the paper!  I think the question it asks is insightful, fundamental and important. \nYet, I am not fully convinced by how this question is then investigated (see my detailed review below).\nI find the \"GIS\" part not so interesting, although it indeed remedies the symptoms and gives \"bold numbers\". I would be much more interested in *why* this happens. And if, for example, lighting plays a role, then why dont the first layers (which presumably are shared between classes) to some degree deal with this?\nIf the authors can somehow convince me, either with some new insights, or by new argumentation, I would be very happy to upgrade my score.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper works on long-tailed or class-imbalanced learning. The authors found that the learned classifier in such a setting cannot effectively transfer the class-agnostic (in)variance in a dataset from the head classes to the tail classes, which causes poor classification performance for the tail classes. The authors thus proposed to learn such class-agnostic (in)variance via a generative model, and then use it to augment the training data of minor classes. The experimental results on several small-scale datasets demonstrate the effectiveness of the proposed method in improving long-tailed or class-imbalanced classification.",
            "main_review": "=== Strengnth ===\n1. The paper is mostly well-motivated, well-written, and very easy to read.\n\n2. The question raised by the authors in Section 1 is interesting. The experiments and metrics in Section 3 are inspiring and well-designed. They point out one direction for future research.\n\n3. Overall, I think the direction the authors proposed to approach imbalanced classification is quite interesting. Generative models have not been widely used in this task. The authors also make \"accurate\" claims: the proposed method is mainly to deal with task/dataset-specific (but class-agnostic) variation.\n\n=== Weakness ===\n1. The technical part of the paper (GIT) is not well-written, described, and justified. The authors only simply mentioned that they apply MUNIT but didn't have more discussion or provide formulations. For example, why is MUNIT the appropriate method to be used? Why can we learn anything meaningful (i.e., the class-agnostic variance) by turning MUNIT to learn the mapping between the \"same\" domains? Are there related works that learn to change the input image (e.g., style transfer) and can be applied here? Finally, the authors should have mentioned in section 1 that the generative model is conditioned on the input image. \n\n2. The experimental results are not enough. The authors only use small-scale datasets, but not large-scale ones like iNaturalist or mini/tiny-imagenet. The smaller improvement on CIFAR than on characters/traffic signs also raises a question --- could the proposed method be applied to natural, more complicated images/objects? Besides, the authors only compare to CB, LDAM, RS/RW, which are a bit outdated (though they are proposed in 2019). Finally, the proposed method seems to be not stable (according to Table 5).\n\n3. As the authors argue that the proposed method is not merely data augmentation, I would like the authors to empirically compare it to existing sophisticated data augmentation methods, e.g., Zoph. et al. Rethinking Pre-training and Self-training, NeurIPS 2020.\n\n4. From the paper description and the qualitative results (Fig 2 and Fig 4), it seems that the proposed method can only make the class-agnostic background, color, lighting, and dilation/erosion changes. However, in many large-scale datasets, there exists class-specific or semantically-related variation: for example, the appearance changes of cats might be similar to lions than to buses. Also, in many large-scale datasets, many tail classes seem to suffer from insufficient observations of their appearance changes (like from different viewpoints, insufficient instances (e.g., bag/airplane styles)). These can be found in Fig 7. It seems that the proposed method cannot effectively resolve these variances (see Fig 4). \n\n5. Some highly relevant works are missing (not cited and discussed). \n\nTransferring data variance from many-shot to few-shot data (e.g., to generate more data)\n\n[a] B. Hariharan et al. Low-shot visual recognition by shrinking and hallucinating features. In ICCV, 2017\n\n[b] X. Yin et al. Feature transfer learning for face recognition with under-represented data. In CVPR, 2019\n\n[c] Y. Wang. Low-Shot Learning from Imaginary Data. In CVPR, 2018\n\nLearning class-agnostic information\n\n[d] Y. Yang. Rethinking the Value of Labels for Improving Class-Imbalanced Learning, NeurIPS, 2020\n\nStyle transfer or image-to-image translation should be discussed in related works.\n\nFinally, based on the proposed method, I was wondering if the method proposed by Mariani et al. (2018) is applicable. If so, it should be compared.\n\n=== Minor weakness/questions ===\n1. The paper could cite and reference more existing works. For example, in Section 1, there are insufficient references (which dataset, which augmentation method?). \n\n2. When DR or DS is used, do the authors only apply the proposed augmentation to the final learning steps or the whole learning steps? As the proposed method does not simply over-or under-sample the examples in the original dataset, I think it could be applied to the entire learning process.",
            "summary_of_the_review": "Overall, I enjoy reading the paper as it is well-motivated and well-written. The experiments designed in section 3 are quite interesting. However, the proposed method in section 4 is not well-described and justified; the experimental comparison in section 5 is not sufficient. I thus give a score \"5\" for now. More accurately, my score is between \"3\" and \"5\".",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "=======================\nSummary: \nThis paper studies the problem of how well do neural networks transfer class-agnostic invariances from head classes to tail classes. They found that even a given transformation is class-agnostic, the DNN models still cannot disentangle it from class-specific features, which could partially explain the worse performances on rare classes. To solve this problem, they introduce a GIT method that uses a generative model to augment the tail categories with more diverse samples under certain transformations. It can empirically improve the long-tailed performances and transfer the knowledge of class-agnostic transformations from head to tail.\n",
            "main_review": "===================\nStrengths:\n+This paper provides a new perspective to understand the long-tailed problem of DNN models. They found that the class-agnostic invariances cannot automatically transfer from the head to tail classes, which means the models are simply memorizing the data without understanding it.  Since humans are able to disentangle the class-specific contents from class-agnostic transformations, they can easily learn new contents (categories) through a limited number of observations. Therefore, this paper pinpoints that the key to solving the long-tailed problem might be disentangling the class-specific features from class-shared features.\n\n===================\nWeaknesses:\n- Despite the good motivation of this paper, the technical details of the paper are not satisfactory. For example, I think the authors confused the classifier with the entire model. A DNN classification model is usually composed of a backbone (like ResNet) and a classifier (e.g, Linear classifier or Cosine classifier). In algorithm 1, you only mentioned the classifier updating, but I guess it means the entire model here. Such a description is confusing in the study of long-tailed classification (LTC) because a commonly used strategy in LTC is called decoupling[1], which learns the backbone and the classifier separately.\n- Second, according to Figure 3, the proposed method actually hurt the invariance of head categories while the CE+DRS doesn't, which means GIT is not a good method to transfer the learned class-agnostic invariances from head to tail classes. Unlike the head-tail trade-off in the accuracy, I don't think the learned invariances should be forgotten. The eKLD of the CE+DRS also proves that the trade-off shouldn't be existing in the given invariance metric.\n- Besides, I don't think the GIT is necessary to increase the invariances of tail classes and the current ablation study is too simple. For example, how about using RandAug[2] for tail classes during training? It's much simpler than learning a generative model and I'm pretty sure that it can increase the invariance.\n\n[1] Decoupling Representation and Classifier for Long-Tailed Recognition\n[2] RandAugment: Practical automated data augmentation with a reduced search space",
            "summary_of_the_review": "===================\nJustification:\nThis paper provides a new perspective to understand the long-tailed problem of DNN models, yet, the technical details and the solutions of the paper are not satisfactory",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}