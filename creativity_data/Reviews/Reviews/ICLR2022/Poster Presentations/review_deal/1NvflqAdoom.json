{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The authors make a case for a phenomenon of deep network training\nthat they call the \"silent alignment effect\": that, while the training\nerror is still large, the NTK associated with the network aligns its eigenvectors\nwith key directions in \"feature space\".  They support this with non-rigorous theoretical\nanalysis of linear networks, and extensive experiments with real networks on real data.\nThe consensus view was that this paper provides novel and useful insight into training\ndynamics, in particular regarding feature learning."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Kernel regimes have been a major workhorse in studying the training dynamics of deep neural networks. Previous work has shown that networks with large initialization scale and trained with gradient flow will converge to the kernel regression solution under a data-independent kernel, known as the NTK. However, it is now known that the practical training of neural networks often operates in a different regime called the \"rich\" regime, which makes it more difficult to analyze. \n\nThis paper bridges the gap between the two by pointing out that the rich regime can be split into two phases. The first phases is aligning the kernel spectrum with the training labels, which constructs a data-dependent kernel. The second phase only increases the scale of this kernel. This behavior is analytically shown on (deep) linear networks by writing out the dynamics of the NTK. Therefore, after the first phase, the neural network training will converge to the kernel regression solution of the adapted NTK. The same behavior, although not theoretically shown, are empirically verified on nonlinear ReLU nets.",
            "main_review": "## Summary\n\nKernel regimes have been a major workhorse in studying the training dynamics of deep neural networks. Previous work has shown that networks with large initialization scale and trained with gradient flow will converge to the kernel regression solution under a data-independent kernel, known as the NTK. However, it is now known that the practical training of neural networks often operates in a different regime called the \"rich\" regime, which makes it more difficult to analyze. \n\nThis paper bridges the gap between the two by pointing out that the rich regime can be split into two phases. The first phases is aligning the kernel spectrum with the training labels, which constructs a data-dependent kernel. The second phase only increases the scale of this kernel. This behavior is analytically shown on (deep) linear networks by writing out the dynamics of the NTK. Therefore, after the first phase, the neural network training will converge to the kernel regression solution of the adapted NTK. The same behavior, although not theoretically shown, are empirically verified on nonlinear ReLU nets.\n\n## Strengths\n\n* From the claims and the cited references this seems to be the first work that figures out the so-called \"silent alignment\" effect. The effect is analyzed on (deep) linear networks and is a bit surprising that how well it aligned with empirical evaluations. \n* Combining the conservation law shown in Du et al. (2018) with early dynamics from small initialization seems to be the main \"magic\" that enables the analysis. The proof technique can itself be interesting to the community.\n* I checked the derivation of major claims made in the paper and they looked correct to me.\n\n## Weaknesses\n\n* The analysis was done on (deep) linear networks. The justification on ReLU nets is through a combination of the observation of Chizat et al. (2019) and empirical evidence. It is unclear from the text if this assumption is widely tested on a variety of ReLU nets or just the single example of 1K whitened CIFAR images used in the paper. It would be nice to see if the silent alignment behavior is consistent across shallow/deep, narrow/wide nonlinear nets.  \n\n* The set-up of the simulation is a bit unclear to me. From the figures and captions I cannot see what the time span is for values like (t=1000). Does the results hold if the training spans as long as what people do in practice? The behavior might also change if the dataset contains way more patterns than the 1000 image experiments presented in the paper.\n\n## Overall suggestions\n\nI vote for acceptance of the paper. I should note that the submission should be best reviewed by deep learning theory people, since I am not particularly familiar with the latest literature that focuses on learning dynamics beyond the lazy regime. My assessment is based on the technical correctness and proper justification of the claims but may have missed some important related work that lowers the significance of contribution. \n\n## Questions \n\n* Figure 1b/1c: does the individual points correspond to training data or test data?\n* Section 3.1: I don't understand what you mean by \"In this setting the overall scale of the weights and kernel, evolves slowly since the network is initialized close to the saddle point at W, a=0\".\n* I don't see how the left hand side of (11) is approximated by the right.\n\n## minor points\n\n* Section 2: \"However, we analytically show in sections 3 and 4 analytically\"\n* Missing period on page 5, last paragraph.",
            "summary_of_the_review": "I vote for acceptance of the paper. It provides a new perspective of the \"rich regime\" of neural network training. The claims are backed by analysis on (deep) linear networks and align well with empirical evaluation. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper identifies a phenomenon called the \"silent alignment effect,\" which happens in linear networks as well as ReLU networks on whitened data, with sufficiently small initialization of the network weights. In such a regime, the NTK of the network changes its eigenvectors to align with the problem structure at the beginning while remaining small in scale; then, the kernel essentially stops changing its eigenstructure and just grows in scale, eventually leading to a kernel regression solution wrt the NTK after the initial phase. The paper also demonstrates that non-whitened data can weaken this effect, for which the NTK needs to evolve later in training.",
            "main_review": "Strengths:\n1. Understanding the behavior of neural networks outside the NTK regime is a major question in the theory of deep learning. This paper gives a precise characterization of the kernel evolution in certain settings, in which the learning can be cleanly divided into two phases. The end result is that the final predictor is equivalent to a kernel regressor wrt a data-dependent kernel.\n2. The paper also investigates how varying certain design choices (e.g. depth, relative learning rates, covariance of data) affects the silent alignment effect.\n\nWeaknesses:\n1. There appears to be a small mistake in the treatment of deep linear networks. Since different layers have different learning rates, the corresponding NTK should also be a weighted sum of the contributions from all layers, instead of an unweighted sum (Eq. (36)).\n2. The analysis in the paper is not completely rigorous because of all the approximations used, and many times equations are used for approximate equalities. It would be much better if the approximations can be controlled explicitly and rigorously.\n3. Figure 5b, 5c: why does silent alignment not clearly happen for deeper networks (i.e. alignment keeps improving while the loss is decreasing)? Does the theory predict this?\n4. Overall speaking, the results are quite restricted since they are only for linear networks and whitened data. It would be very interesting if some characterizations can be given for the final NTK in non-linear networks.\n\nMinor points:\n1. page 5 line 5: \"achieved low loss\" --> \"achieved high loss\"?\n2. Eq. (10): $K_0(x, x')$ missing? Approximation error missing?\n3. Figure 5 caption: $10^{-2}, 10^{-1}, 10^{-2}$?",
            "summary_of_the_review": "This paper makes some interesting and novel contributions towards understanding the learning dynamics of neural networks through the lens of NTK evolvement. On the other hand, the setting where a complete answer can be provided is restricted, and the analysis in the paper is not rigorous.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper the authors show that in some cases, a neural network, trained in the rich regime, can behave like a kernel predictor where the kernel is the neural tangent kernel at the end of training. Specifically, this happens when the kernel changes before the loss significantly decreases, but then stays fixed up to changes in scale. The authors call it “silent alignment”, and show that this happens for deep linear networks with small initialization and whitened data. Some experiments show that silent alignment can happen also for non-linear networks (e.g. with ReLU activations).",
            "main_review": "Strengths:\n-\tThe authors identify an interesting silent alignment effect, where the trained network acts as kernel predictor, and provide a thorough analysis for 2-layers linear networks and deep linear networks.\n-\tThe authors demonstrate the effect empirically.\n\nWeaknesses:\n-\tI think the authors should contrast their results with recent results on implicit bias of linear networks with small initialization. Specifically, it is shown in [1] (theorem 7 therein) and [2] (theorem 6.3 therein, for the case of 2 layers) that linear fully connected networks with small initialization converge to minimum $\\ell_2$-norm solution (i.e. kernel solution with linear kernel). Note that this is correct also for anisotropic data.\n-\tThe effect of depth is not so clear from the simulations. It can be useful to show on the same plot the kernel alignment for a fixed initialization scale but different depths.\n\nQuestion:\n-\tPlease explain why in Eq. (8) the error is $O(\\sigma^2)$.\n-\tPlease explain the transition in Eq. (15), why the integral starts from 0 and not from $\\tau$ ?\n\nMinor:\n-\tThere is a typo in the definition of $K_{c,c'}$.\n-\tIn figure 5, only one $\\sigma$ is used.\n\n\n[1] Yun et. al. A unifying view on implicit bias in training linear neural networks. ICLR 2021\n\n[2] Azulay et. al. On the Implicit Bias of Initialization Shape: Beyond Infinitesimal Mirror Descent. ICML 2021\n",
            "summary_of_the_review": "Overall the paper is interesting but the relation to implicit bias results is not discussed. I will be happy to adjust my score if the authors can address my questions, and also compare the results to known implicit bias results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}