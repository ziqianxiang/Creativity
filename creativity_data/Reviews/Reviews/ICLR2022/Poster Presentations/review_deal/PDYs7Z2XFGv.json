{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper presents a simple and effective solution to tune the receptive field of CNNs for 1D time series classification. The reviewers think the idea is original and elegant but would appreciate more theoretical insights into the solution."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes an original trick to tune the receptive field kernels for 1D ConvNets to tackle time series problems. ",
            "main_review": "The strengths of the papers are as follow :\n\n1- The paper is well structured and easy to follow\n\n2- Tables and figures are very clear.\n\n3- The authors have conducted extensive experiments to support the claim. Moreover, the results are discussed and compared in a consistent manner.\n\n4- The originality of the paper is related to the introduction of a concept from the theory of numbers, namely Goldbach conjecture allowing to cover the receptive fields at different scales\n\n5- The claim is validated on univariate and multivariate time series considering  different tasks.\n\nThe weaknesses of the paper :\n\n1- The proposed receptive field module could have been studied for time series vision tasks, namely video classification for instance\n\n2- The work lacks of theoretical study. The latter is very important to study the stability of the representations. Moreover, Goldbach conjecture is not well motivated and analyzed.\n\n3-  The proposed module could be studied under different neural network architectures to assess its genericity, including different tasks and domain.\n",
            "summary_of_the_review": "Following the aforementioned consideration, l recommend to accept the paper \"6: marginally above the acceptance threshold\"",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents an Omni-Scale block (OS-block) to efficiently determine the kernel sizes for 1D-CNNs. The proposed method achieves similar performance as models with optimized RF sizes.",
            "main_review": "While the proposed method yields a simple and universal rule for kernel size tuning, and has shown to perform as best as the state-of-the-art methods in the literature, there are some major points that are neglected in the current version of the manuscript:\n\n1) The motivation and justification of using the Goldbach’s conjecture is missing. According to the conjecture, any even number such as \"e\" can be written as the sum of two prime numbers such as \"p1\" and \"p2\". Authors have used this decomposition and prime numbers p1 and p2 for the kernel size of their OS-block. However, any even number \"e\" can also be written as the sum of many other pairs of even numbers such as \"e1\" and ''e2\" or odd numbers such as \"o1\" and ''o2\". Why not using these (e1, e2) pairs or (o1, o2) pairs for the kernel size of the OS-block? What advantage does (p1, p2) have over (e1, e2) or (o1, o2)?\n\n2) As shown in Figure 2, model's performance has a positive correlation with the single RF size. Furthermore, authors have claimed in the beginning of Section 2 that the performance of 1D-CNN is not sensitive to the specific kernel size configuration that leads to a specific RF size. If that is the case, then using the Goldbach’s conjecture makes even less sense since any other decomposition of the even number \"e\" could perform the same according to this claim.",
            "summary_of_the_review": "The main novelty and contribution of the paper lies in using the Goldback's conjecture to design the OS-block for an efficient kernel size design. However, this key factors suffer from the above two points and the manuscript needs to be revised to address these issue. Authors can either theoretically show the benefit of prime decomposition or experimentally verify the efficacy of the prime decomposition of the even number \"e\" over other trivial decomposition of \"e\" such as (e1, e2) or (o1, o2). ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "\nThe paper is on the receptive field of CNNs for 1D time series classification. It proposes an elegant decomposition of receptive fields based on the Goldbach conjecture that any number can be represented by a sum of primes. The paper thus puts RFs with prime numbers in CNN layers, and by having multiple layers, these RFs will sum, and thus allowing the proposed network to cover all RF sizes. Experiments on several datasets shows that the method gives good results.",
            "main_review": "Strengths\n\n+ Elegant solution to design an architecture to cover all RFs\n+ Good results\n\nWeaknesses\n\n- No related work on adaptive RFs is mentioned at all. See my detailed comments below.\n- The paper is sometimes difficult to understand. See my detailed comments below.\n\n\nAll these following citations about adaptive receptive fields in CNNs are missed:\n\n- Han et al. Optimizing filter size in convolutional neural networks for facial action unit recognition. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018\n\n- Tabernik et al. Spatially-adaptive filter units for compact and efficient deep neural networks. International Journal of Computer Vision (IJCV), 2020.\n\n- Zhitong Xiong et al. Variational context-deformable convnets for indoor scene parsing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\n\n- Pintea et al. Resolution learning in deep convolutional networks using scale-space theory in Transactions on Image Processing (TIP), 2021\n\n- Tomen et al. Deep Continuous Networks in International Conference on Machine Learning (ICML), 2021.\n\nAdaptive receptive fields are quite related to this paper; and I expect them to be included and their relation to this ICLR submission explained and motivated. In addition I would like to see the paper state if these aproaches are, or are not, applicable here.\n\n\nDetailed comments:\n\nFig 1: I like the figure. Yet, a \"win\" is based on the very best score, and thus quite sensitive to small changes; it could be that a method is only slightly worse in which case it would not \"win\". It would be a stronger plot to also show the average accuracy (or the average rank) for each Receptive field size on all datasets.\n\nPage 2: \"Firsty, we find that\".. I do not understand what is meant here.\n\nPage 3: It would be good to remind the reader what the term \"OS block\" means, to make the method section more modular and better readable as a stand-alone text.\n\nPage 3: Section 3.2: At this point in the text I'm confused how these kernel sizes are used in a \"multi-kernel structure\" as this is not explained.\n\nFig 3: \"medium\" = \"middle\"\n\nPage 4: Section 3.2: I'm still confused here about \"because each layer of the OS-block has more than one convolution kernel\"; how does that work? Fig 3 seems to hint as something, but is not sufficiently exlained.\n\nEq 2: I do not understand why a specific RF size is now coupled to it's specific position in the layers (ie: I do not understand why Godlbach's conjecture is needed to uniquely define unique RF size paths through the layers).\n\nSection 3.3. I do not understand what is meant by \"For example, suppose we use 2 as the common ratio to cover RF size 15\". \n\nFig 5: There is no need to sort the datasets alphabetically. Please sort the datasets on OS-CNN accuracy: it will probably make the graph much easier to understand. (indeed, in appendix A there seem to be such plots, but I do not understand the diffences between all those plotted variants; I would just sort fig 5 directly on the accuracy of OS-CNN).\n\nFig 6: the graphs are small, and difficult to see the differences.\n\n",
            "summary_of_the_review": "I like the elegant decomposition to cover all RFs. Yet, much related work is missed, and clarity can be improved.\n\nThese issues seem fixable.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I dont think the missed paper are left out on purpose; thus i see no ethical concerns.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}