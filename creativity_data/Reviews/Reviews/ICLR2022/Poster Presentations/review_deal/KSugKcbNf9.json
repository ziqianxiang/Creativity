{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper presents a method for using transformer models to perform approximate Bayesian inference, in the sense of approximating the posterior predictive distribution for a test example.  This seems similar to doing amortized variational inference using a transformer model.  The reviewers all found the paper to be clearly written, interesting, novel and compelling.  Two of the reviewers found the results \"impressive\".  There is some concern of over-claiming (is it really Bayesian?, are the authors making too broad statements based on very simple case studies?).  The presented method is also not scalable O(n^2), so the setting is restricted to very small datasets and models. \n However, the reviewers didn't seem especially concerned by this.  The reviews were mixed but leaning positive (8, 6, 5) and the positive reviews are more substantial.  Therefore the recommendation is to accept, but please incorporate the reviewer feedback and additional discussion about related methods (discussion below) into the camera ready."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper describes a novel way to do bayesian inference with deep learning models by employing a meta-learning approach. This is done by creating a synthetic prior over datasets, training a deep learning model on samples from this prior, which results in an approximation of the posterior predictive distribution over the datasets. The authors demonstrate their approach on two synthetic tasks: approximating gaussian processes and bayesian neural networks, then on two real world tasks: the OpenML tabular datasets and the few-shot learning Omniglot dataset.",
            "main_review": "Strong points:\n\n- The meta-learning algorithm itself, which proposes to draw datasets from synthetic priors, is an interesting and novel approach. It requires a leap of faith to attempt to transfer knowledge from synthetic datasets, such as ones generated from the BNN, to real-world datasets such as the tabular ones. It is surprising and exciting that this approach worked well and beat all the baselines.\n\n- The novel set-valued training approach which allows to feed entire datasets as samples, and the appropriately modified Transformer model. It fits very well with the meta-learning approach, and allows for fast inference.\n\n- The experiments are convincing and quantitative. In particular, the authors compare their approach to very solid baselines with appropriate tuning.\n\nWeak points:\n\nMostly, I would like to see a bit more details and explanations, as outlined in the questions and comments below:\n\nSection 2:\nStarting with ‘in this work’, this is not background anymore, this is your method. I would rework that into section 3.\n\nSection 4: \nHow do you choose the size of the datasets, N? Is this a hyperparameter of sorts? Is it better to have datasets of say, 100 samples each, or 10k samples?\n\nIn section 5.1, am I correct in my understanding that what you’re doing is:\n1- Choosing a given gaussian process.\n2- Sampling from the GP.\n3- Grouping the samples into buckets of size N: these are the datasets used for training your PFN.\n4- Training the PFN on the datasets, and in figure 4)a) you vary the number of datasets you use (from 500K to 4M)\n5- Using that for inference on new, unseen datasets generated from the GP (1000 datasets, according to appendix F). \n\nIf this is correct, then I don’t understand how this is different from traditional supervised learning, in which you directly train on all the datapoints without grouping into datasets. Indeed, I understand the point of meta-learning as being that the datasets are coming from different data generating processes. Section 5.2 and 5.3 make sense to me, as the data generating process changes between sample datasets due to the sampling of GP parameters.\n\nI don’t understand what does ‘number of data points’ refer to in Figure 4? Is the size of the dataset, N? \n\nSection 5.3: \n\nIn figure 5, you show the performance of a PFN with 4M datasets. However, in the text, you mention sampling K=100 BNNs with m=200 and n=100. Are those just the evaluation datasets?  \n\nSection 6: \n\nAm I correct in my understanding that, for this section, the data priors for the PFN are just the synthetic datasets generated in sections 5.2 and 5.3 (plus the architecture prior)? As in, there is no PFN training on the tabular datasets, only bayesian inference? \n\nIf it is the case, that is very impressive, and I wonder why does it work so well? In particular, why does the scale and nature of features (e.g. categorical) not impact the appropriateness of the data priors you used? Why did you not fine-tune on the tabular datasets, just like you did in section 7 for Omniglot? I’m surprised because seemingly the tasks in the tabular datasets are very different from the GP or BNN priors, since the data generating process is intuitively very different.\n",
            "summary_of_the_review": "Overall, I vote for accepting. I think the approach is very novel and the results surprisingly good – especially for the tabular data with BNN priors. This method has a lot of potential for bridging the gap between deep learning models and small data regimes, all the while adopting a Bayesian perspective. It also opens the door to very interesting future research, for instance on the question of design of priors for a particular problem. \nMy main concern is with regards to the clarity of the paper, as there is a lot of prior knowledge assumed from the reader and sometimes a lack of details and explanation. Hopefully my comments will be addressed during the rebuttal period.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents the posterior inference framework named Prior-Data Fitted Networks. The theoretical background is to approximate the posterior predictive distribution (PPD) with a proposal distribution and optimize the KL divergence between the proposal and the PPD. The architecture used as approximation networks is adapting from a transformer encoder, by tweaking the attention mask to output the estimation of $y$ for queries alone. In addition, the authors mainly discussed two application cases, Gaussian process (GP) and Bayesian neural networks (BNN). In the experiment, the paper demonstrates the proposed method can be successful in few-shot learning.",
            "main_review": "Strengths:\n1. The proposed framework is clear and sound, and it works well for some well known models (e.g., GP and BNN).\n\nWeaknesses:\n1. According to Eq(2) or Algorithm 1, the dataset $D$ can be drawn from a much larger dataset or distribution $p(\\mathcal{D})$. In practical, the sampled data size of $D$ should not be computationally reasonable, which means that $D$ may not represent $p(\\mathcal{D})$ very well. I'm wondering the scalability of this approach. In other works, for more stochastic case ($|D|$ is far smaller than $\\mathcal{D}$ or a much larger model with more number of parameters), how about the performance of the proposed method? In the experiment, only small datasets are discussed. \n2. In this paper, the authors emphasize that the transformer encoder can be adapted to fulfill the suggested PPD approximation. So does the title. However, I didn't see the necessity or justification to use transformer as the tool for PPD approximation. By observing Fig 2(a), I think any graph neural networks can achieve the same purpose. ",
            "summary_of_the_review": "In summary, the paper is clearly written and proposes an interesting method for approximation of posterior predictive distribution. The author has validated their approach in the classification task with small datasets. But I would like to see the generalization on other tasks and data/model scalability. In addition, I have concerns or unclear intuition for adopting transformer encoder architecture.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this paper the authors show a simple architecture (simple on top of transformer pieces) that can learn to perform supervised learning on new datasets given collections of existing datasets. The model is trained on a collection of supervised learning datasets, and, at test time a novel training dataset and the corresponding test inputs are given and a single forward pass through the network produces the predictive distributions for those test inputs. Several different types of data are tested with close to state-of-the-art results provided, but in much less time.",
            "main_review": "I found the results of this paper pretty impressive, since it shows good performance across many different modalities of data and it is able to generalize well to new datasets. However, I find the main claim of the paper, that \"Bayesian inference\" is happening, unsubstantiated.\n\nIt is true that Fig. 3 shows that the posterior GP and the posterior obtained from the PFN are well matched. This proves that the Bayesian posterior is being recovered _for this particular case_. This is a particularly simple case, and similar results could be obtained simply taking the training data (which is 1D and confined to a small space) and plotting all the training trajectories weighted by how close they are to the two training points when traversing that particular x coordinate. The results would be very similar to those in Fig 3. As we get to more complicated cases, we see that the results from PFN and GPs start to diverge. In Fig 4.b, for 5 data points, we see that the MCMC solution and the PFN solution are different. The fact that PFN performs better in terms of the NLL means that it is a better model for the data, but not necessarily that it's closer to the true posterior. In fact, the MCMC solution, if run long enough, should be the golden standard.\n\nThus, the provided results show that PFNs perform very well in terms of NLL which means that they are well calibrated, but saying that they are doing Bayesian inference could be a stretch, since outside of the GP with fixed-hyperparameters we have no verification for that (in fact, it seems that either MCMC was not run to covergence, or PFN does not match Bayesian inference).\n\nMinor:\n\nTheorem 1 and the following Corollaries are trivial and that space could probably given a better use. The so-called \"Prior data NLL\" is just a standard leave-one-out NLL. The connection between maximum likelihood and minimum KL are also trivial and well-known.\n\nComparisons are missing the training time of the PFN. Although I understand that that time can be amortized, it would be good to have a sense for it in each of the experiments.\n\nHow does this process scale with the size of the dataset? Can we use it for GPs with 100.000 training samples for instance?\n\n\"we sample the N inputs xi uniformly at random from the unit-cube\"-> isn't x_i one dimensional? If so, it's not a unit \"cube\". What's the value of N?\n",
            "summary_of_the_review": "Interesting paper and strong results. Probably mainly useful for fast inference in small datasets. The claim of doing Bayesian inference is not fully substantiated beyond 1D GPs.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}