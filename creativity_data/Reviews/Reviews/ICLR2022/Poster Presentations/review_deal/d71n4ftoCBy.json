{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "Dear Authors,\n\nThe response you have provided, based on the main concerns of reviewers, have answered most of the questions raised. \nAs far as I understand from the added experiments you have provided, the proposed methodology shows resilience in being at least as good as state of the art approaches, while at the same time it is a mathematically interesting approach. \n\nYour response has covered concerns like comparison to other communication techniques (the comparison list is not complete, but yet your effort is appreciated), adding discussion on the rank parameter, add comments on expressiveness and the connection with low-rank parameterization, etc. \n\nThese efforts cannot be overlooked, and for that reason I suggest acceptance (poster).\n\nBest\n\nAC"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces FedPara, a low-rank based method for achieving communication efficient federated learning. On the client-side, low rank factorized models are trained. Different from the traditional approaches to finding low rank factorized networks, FedPara introduces a novel low rank factorization strategy, which attains a higher maximal rank for the weight matrices. The paper also identifies that FedPara can be used to enhance personalized federated learning (by introducing a variant of FedPara, i.e., pFedPara). Both theoretical analysis and experimental results are provided to show the effectiveness of FedPara. ",
            "main_review": "======================= Post Rebuttal ======================\n\nAfter reading the authors' response. Part of my concerns are addressed. Thus\nI would like to increase my overall evaluation to 6. And I believe that this work\ncan make a decent contribution to the efficient FL training area.\n\n========================================================\n\n\nThe paper proposes to train low rank factorized model on the client side to improve the communication efficiency of federated learning while introducing a novel low rank factorization method. \n\nPros:\n1. The paper is well written, and the research direction of enhancing the communication efficiency of federated learning is potentially impactful. \n2. The proposed low rank factorization method is novel. \n3. Both theoretical analysis and empirical results are provided to justify the effectiveness of FedPara. \n\nCons: \n1. The main concern I have for the current version of the draft is that the proposed FedPara method is only compared against the traditional low rank based model factorization method, e.g., vanilla low rank model or Pufferfish. It is not clear if FedPara is also better compared against other communication efficient federated learning methods, e.g., [1, 2] or recently proposed low rank based communication efficient FL method [3]. \n2. By looking at Algorithms 1 and 2, it is not clear which weights are kept on the client side and which weights are kept on the server side. It says “Optimize(W)”, does it mean only $X_1, Y_1$ and $X_2, Y_2$ are optimized? Also for aggregate $X_1, Y_1$, and $X_2, Y_2$, does it require to transform them back to W before aggregation across clients’ updates? The authors are supposed to clarify this. \n3. How to choose $\\gamma$ is not clear, a fixed rank ratio seems to make sense. But it may also be possible that different layers can contain different redundancy. Is there a chance to adaptively choose $\\gamma$ for different layers? \n4. It is recently observed that the model weights of the Transformer are not low rank. Does it mean FedPara can not be applied over the Transformer architectures? \n\n[1] https://arxiv.org/pdf/1610.05492.pdf\n\n[2] http://proceedings.mlr.press/v108/reisizadeh20a/reisizadeh20a.pdf\n\n[3] https://arxiv.org/pdf/2104.12416.pdf\n\n[4] https://openreview.net/forum?id=_sSHg203jSu",
            "summary_of_the_review": "Overall I think the paper introduces a valid research idea on improving the communication efficiency of federated learning. However, several concerns exist (please refer to “Cons” in “Main Review”). If my major concerns are addressed, I will consider increasing my overall evaluation score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a communication-efficient parameterization for federated learning by using low-rank weights. Several numerical experiments are conducted to demonstrate the effectiveness of the proposed FedPara and pFedPara.",
            "main_review": "======================= After Rebuttal ======================\n\nI have read the authors' responses. Part of my concerns is alleviated. I would like to increase my overall evaluation to 6. \n\n========================================================\n\nStrengths:\nThis paper is well organized and easy to follow. The authors provided comprehensive experiments.\n\nQuestions:\n1. As for Figure 1, does the low-rank parameterization $W=XY^{\\rm T}$ always leads to the Hadamard product of two low-rank inner matrices $W=W_1\\circ W_2=(X_1Y_1^{\\rm T})\\circ(X_2Y_2^{\\rm T})$? What if the low-rank weight matrix $W$ cannot be decomposed into the Hadamard product of two low-rank inner matrices?\n2. Proposition 1 only provides the upper bound for the rank(W). It does not necessarily mean that $rank(W)=r_1r_2$. How could the constructed matrix always span a full-rank matrix when $r_1r_2\\ge \\min(m,n)$?\n3. In the numerical results with varying $\\gamma\\in[0.1,0.9]$, what is the corresponding number of parameters?\n4. The accuracy differences between the low-rank method and FedPara in the non-IID case are smaller than the differences in the IID case. Is that any explanation for this phenomenon?\n5. It would be better to move the details of proof to the appendix and add more discussions in the main manuscript.",
            "summary_of_the_review": "The paper is mostly sound, while the contributions are marginally significant.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a communication-efficient parameterization methodology for federated learning tasks through the Hadamard product of low-rank weights. Such parameterization has better model expressiveness in terms of minimal parameters to achieve a maximal rank, compared with conventional low-rank approaches. Using such parameterization, the work designs the corresponding communication-efficient federated learning framework which only uploads a low-rank part of parameters of each level to the server. The work presents empirical studies of the proposed algorithm and validates its effectiveness in terms of accuracy vs communication costs.",
            "main_review": "The paper proposes a low-rank parameterization method to help improve the communication efficiency of federated learning. The work provides theoretical justification of the effectiveness in terms of the maximal rank the parameterization can express. The work shows extensive empirical studies of the proposed method on multiple FL datasets. The results indicate the superiority of the proposed method in terms of model accuracy and communication cost.\n\nQuestions and comments:\n1. In terms of expressiveness, what is the key limitation of the Hadamard product parameterization compared with general unconstrained parameterization?\n2. The paper does not discuss any related work about the Hadamard product parameterization. Is this a new idea? Or what is the difference compared to other literature involving Hadamard product parameterization?\n3. In terms of computational cost, how does training the Hadamard product parameterization compare to previous low-rank approaches?",
            "summary_of_the_review": "The work proposes an effective low-rank parameterization that shows the superiority of communication efficiency in federated learning tasks. The idea is relatively interesting to the reviewer. The overall presentation of the idea and empirical results is clear.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper introduces FedPara, which is a low-rank parametrization method for the neural networks aiming to reduce the total number of communication bits while preserving the model accuracy in the federated learning scenario. Besides FedPara, which is designed for federated learning applications, the paper also generalizes FedPara into pFedPara, which is designed for personalized federated learning.\n\nThe novelty of this paper includes using the Hadamard product in the low-rank approximation. Compared with directly using low-rank approximation, using Hadamard produce can increase the expressiveness of the parametrization.\n\nThe paper also includes empirical results to show the effectiveness of FedPara/pFedPara in the federated learning setting.",
            "main_review": "I am not very familiar with reparametrization, and I am not able to comment on the significance of this paper.\n\nI think that the idea of this paper is interesting, using the Hadamard product of two low-rank matrices may only increase the model size by a factor of 2, but may drastically increase the expressiveness of the parametrization.\n\nAs for the experiments, I think the experiment design is generally enough. The experiment compares FedPara with original training methods with no reparametrization, and low-rank reparametrization without the Hadamard product on different machine learning models, e.g. VGG and LSTM.\n\nBelow are some suggestions and questions:\n1. In the experiment, the authors compare FedPara with original low-rank reparametrization and without reparametrization, and the optimization algorithms are chosen to be the same, e.g. FedAvg, SCAFFOLD, etc. I wonder what are the results between FedPara and gradient compression algorithms, for example, quantization. Or if the result can become better if using gradient compression schemes in FedPara instead of local steps. It may also be good to add some experiment results with compression.\n\n2. FedPara converges much faster than the original models in terms of the communication bits. How about the final accuracy? What is the tradeoff between communication efficiency and the testing accuracy?\n\n3. Although the low-rank constraints reduce the number of communication bits during the training procedures, it may increase the local computational cost since the gradient of the new model may become more complicated. In your experiment, what is the local training cost, e.g. the local training time between two communications, or the number of flops for one backpropagation?",
            "summary_of_the_review": "I am not very familiar with reparametrization and its corresponding related works, and I am not able to comment on the significance. However, I think that the technique used in this paper is interesting and simple, which can be applied to real applications. The experiments shown in the paper are enough. Some questions and small suggestions are listed in the main review part.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}