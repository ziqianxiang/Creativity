{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper presents some efficiency improvements over existing methods to compute matrix square root and its gradient.  Reviewers find that the novelty over existing methods is sufficient, and that the improvements are valuable.\n\nI propose a poster despite the relatively high numerical scores, because the group of practitioners who will use the result is somewhat niche -- the reviewers are of course selected from this group and hence value the paper more highly.\n\nIn addition the real-world speedups are modest, but it is nevertheless important to document this approach."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The work proposes a fast method to solve the matrix square root. The proposed approach is differentiable and is faster than SVD and NS iteration. Thus, the proposed method is very suitable for optimizing deep neural networks that involves matrix square root computation. Its forward pass uses matrix taylor polynomial or matrix pade approximants. Its backward propagation is done by solving continuous-time Lyapunov equation. Various experiments demonstrate its better performance in both accuracy and speed.\n",
            "main_review": "Strength:\n\n+ The proposed method is differentiable and can be applied to a variety of deep learning tasks.\n\n+ MTA and MPA tradeoff between speed and accuracy, which provide users flexible options. \n\n+ The writing is clear and easy to follow.\n\n\nWeaknesses and questions:\n\n- The novelty of the proposed method is limited and the theory behind it is well-known.\n\n- The speedup by the proposed method is not very significant compared to NS iteration for large matrix dimension, see Figure 2(b). \nMPA requires either matrix inverse or solving a linear system of equations, which should be more computationally expensive than matrix multiplication. This may explain that when increasing matrix dimension, in Figure 2(b) the runtime of MPA becomes closer to NS iteration. My question is that when further increasing the matrix dimension (>200), will MPA-Lya be slower than NS?\n\n- Most of the experiments are on matrices of small dimensions. I wonder what the maximal dimension the proposed method may handle (probably comparable to that of NS)? Of course it should also depend on the space complexity.\n",
            "summary_of_the_review": "In summary, I believe that the proposed method can be useful for a variety of deep learning tasks, although clarification of scalability of the proposed method on large matrices is expected.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper aims to solve an important problem: how to efficiently compute matrix square root in both forward and backward propagations. To this end, the authors proposed to exploit Matrix Taylor Polynomial and Matrix Pade Approximation to compute matrix square root in forward propagations, while approximate Lyapunov equation based on Newton Schultz iteration is developed for backward propagation. From the perspective of computation complexity, the proposed method requires less matrix multiplications than one based on Newton Schultz iteration. Experimental results on the decorrelated batch normalization and second-order vision transformer show the proposed methods are faster than existing methods, including SVD based ones and NS based ones. ",
            "main_review": "Strengths:\n1.\tThis paper starts the point of matrix multiplication times in computation of matrix square root, and proposes some strategies to reduce number of matrix multiplications both forward and backward propagations, aiming to improve the efficiency. The motivation is clear and reasonable. \n2.\tThe idea of solving the Lyapunov equation based on matrix sign function and Newton-Schulz iteration seems interesting, which avoids huge memory consumption of the Kronecker product in the original closed-form solution and requirement of Schulz decomposition in Bartels-Stewart algorithm.\n3.\tThe experiments are conducted on two real-world applications, including decorrelated batch normalization and second-order vision transformer. The speed-up over Newton Schultz iteration is ok.\n4.\tThe paper is well-written and easy to read.\n\nWeaknesses:\n1.\tIn this paper, Matrix Taylor Polynomial or Matrix Pade Approximation are used for forward propagations, while approximate Lyapunov equation is used for backward propagation. Although Table 1 and Table 2 show MTP/MPA and Lya require less matrix multiplication than NS iteration, which one is most important for fast matrix square root? To verify it, MTP/MPA+NS based BP and NS baed FP + Lya are suggested to be compared in terms of accuracy and running time. \n2.\tFor previous SVD-based and NS-based methods, computation processes for forward and backward propagations are consistent. However, this work adopts Matrix Taylor Polynomial or Matrix Pade Approximation for forward propagations and uses approximate Lyapunov equation for backward propagation, leading variance in forward and backward propagations. The authors would better make some discussions about this issue.\n3.\tThe authors claimed BP of MPA is both time and memory-consuming. [r1] tries to respectively use SVD and MTP/MPA as forward and backward propagations, where the authors show BP of MPA is efficient (as shown in Table 6). The authors would better make some discussions about it.   \n\n[r1] Why Approximate Matrix Square Root Outperforms Accurate SVD in Global Covariance Pooling? ICCV, 2021.\n\nOther comments:\n1.\tIt is clear that MPA involves matrix inverse, which is very GPU-unfriendly.  As stated in the paper: \"Moreover, we note that the matrix inverse can be avoided, as Eq. (13) can be more efficiently and numerically stably computed by solving the linear system\". The authors would better provide more detailed computation and analysis.\n2.\tHow to compute the coefficients of $p_{m}$ and $q_{n}$ for the Matrix Pade Approximation in equation (12)? Do forward operations of MPA in Table 1 contain computation of coefficients $p_{m}$ and $q_{n}$?\n3.\tDoes Equation (2) lack a (·)_{sym} operation for $/frac{l}{U}$? \n4.\tIs equation (11) missing a term z^{k} in the left side?\n5.\tI am not sure why sign(B) in equation (21) can be calculated as identity matrix?\n6.\tP_{M} and Q_{N} are used to approximate the Taylor series. If I am not misunderstanding, does I- Q_{N}^{-1}P_{M} replace Q_{N}^{-1}P_{M} in Eqn. (13)? and do the terms I-X replace X in Eqn. (12)?\n",
            "summary_of_the_review": "This paper proposes some strategies to reduce number of matrix multiplications both forward and backward propagations, aiming to improve the efficiency of computation of matrix square root. Particularly, the idea of approximate Lyapunov equation based on matrix sign function and Newton-Schulz iteration seems interesting. The more experimental comparisons and discussions could be strengthened. Overall, I like this work.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper addresses the problem of computing the matrix square root of a positive semidefinite matrix and computing the gradient of the matrix square root, so they can be used as a forward pass and a backward pass in a deep learning framework. \n\nThe paper has two separate contributions. In the forward pass, two approximation methods are introduced, Matrix Taylor Polynomial (MTP) and Matrix Pade Approximant (MPA). These are straight-forward generalisations of the scalar power series of, and the scalar Pade approximant to, the function $(1-z)^{1/2}$, to matrices.\n\nIn the backward pass, computing the gradient is viewed as a special case of the Sylvester equation, also known as the continuous Lyapunov equation, and is solved using a Newton-Schulz iteration first introduced by Benner (2006).\n\nNumerical tests were conducted to figure the speed and error of forward and backward passes as functions of batch size and matrix dimension.\n\nExperiments on ZCA whitening and covariance pooling in recent deep learning approaches were presented.",
            "main_review": "I. Theoretical contributions\n\nI.I. Forward pass: MTP and MPA approximation approaches\n\nIn terms of math, the formulae are mostly correct except for minor issues. In eq (7), the notion $(1/2 \\quad k)^T$ should be further explained. In eq (11), $z^m$ is missing in the numerator and $z^n$ is missing in the denominator.\n\nThe approximation approaches were motivated by the slowness of SVD in GPU computing. We would like iterative approximations for speed. This is fair. However, while Jacobi-based SVD approaches abandon parallelism and a bit of speed to preserve stability, and orthogonality is crucial in it, the proposed approximation approaches, in my view, seem to trade stability for speed. \n\nThe proposed MTP based on Taylor expansion is fast. The authors thoroughly explained a stability issue MTP, which I find fair and honest. The explanation is used to motivate the invention of the second approach, MPA. Although it looks like MPA has addressed the stability issue in MTP with a bit of speed compromise, I think the authors have overlooked another stability issue with Pade approximants. Crucial to any Pade approximation approach is the idea that a function is approximated by the ratio of two low-degree polynomials. The problem though is that Pade approximants in scalar forms can have defects, i.e. small regions where the numerator and the denominator approaches zero. Defects can make the approximation unstable very quickly. See for example:\n\n[1] Baker, Defects and the Convergence of Padé Approximants, In Mathematics, 2000.\n\nOne can generalise the issue when scalar serieses are replaced by matrix serieses. In eq (13), it looks like solving a linear system is key to MPA. However, what if $det(Q_N) \\rightarrow 0$ and $det(P_N) \\rightarrow 0$ in some region in the matrix space? This case would be like the matrix version of scalar defects.\n\nI do not think the provided numeric tests are sufficient to spot the defects, because the defected regions can be small. Unfortunately, there is no treatment of Pade approximant defects in the paper. \n\nI.II. Backward pass: viewing matrix square root gradient as a continuous Lyapunov equation and using the matrix sign function to solve\n\nThere is some novelty in this section but I think the novelty is not much. The idea of viewing the matrix square root gradient as a Sylvester equation is interesting but it has been proposed before (i.e. Lin-Maji (2017) pointed out in the paper). There, Lin-Maji did not use any iterative approach but instead chose to solve directly the problem using the Bartels-Stewartz (1972) algorithm. Here, the authors proposed to solve the Lyapunov equation using the matrix sign function and Newton-Schulz iteration. However, if one looks at Benner et al (2006) section 2.3 then it is clear that the proposed solution here is the same as Benner et al (2006) section 2.3 with matrices A, B, C in the Sylvester equation being replaced by $A^{1/2}$, $A^{1/2}$ and $\\frac{\\partial l}{\\partial A^{1/2}}$.\n\nII. Literature Review\n\nThe review of relevant work is sufficient. However, grouping together the work of Huang et al (2019) with those of Lin-Maji (2017) and Li (2018) is, in my view, not appropriate. While the latter two papers use the Newton-Schulz iteration explained in this paper, which is Higham (2008)'s approach, Huang et al (2019) has a different kind of Newton-Schulz iteration, one which deals with the matrix square root inverse rather than the matrix square root, a direct application of [2] below. Their NS iteration is more closely related to Benner et al (2006) and the one proposed in the backward pass in here. I think they deserve a separate treatment.\n\n[2] Dario A. Bini, Nicholas J. Higham, and Beatrice Meini. Algorithms for the matrix pth root. Numerical Algorithms, 39(4):349–378, Aug 2005.\n\nIII. Experiments\n\nThe numeric tests in section 4.1 are sufficient for speed comparisons but are a bit lacking for accuracy comparisons. Fig 2c is the only test related to accuracy on general settings. While it reveals that on average, MPA is better than MTP and Higham (2008), there is no treatment about special cases where MTP or MPA may potentially fail. For example, a figure revealing when MTP fails can support the claimed \"hump phenomenon\" in sec 3.1.2, making a stronger point. It would be good to have some experiments on potential defects of MPA. It would be good to have some accuracy test for the backward pass as well, which is currently lacking.\n\nSection 4.3 is adequate, showing the proposed approaches inbetween two extrema, one with full SVD (i.e. SVD-Taylor) and the other with only the principle eigenpairs (PI). However, in section 4.2 I think a better candidate to compare against is not Higham (2008) but rather Bini et al (2000) ([2] above) that Huang et al (2019) have used. They directly targeted $A^{-1/2}$ which is appropriate for ZCA whitening. If there were space, I would recommend treating the inverse matrix square root problem as a separate, but related problem. However, if the scope of the paper is just matrix square root then presenting ZCA whitening is still good but it should be stated in the paper that you are handicapped against state-of-the-art by one extra linear system.\n",
            "summary_of_the_review": "The paper has two separate contributions, one in forward pass and one in backward pass in dealing with (positive-semidefinite) matrix square root in deep learning.\n\nIn the forward pass, two approximation approaches MTP and MPA both seem to trade stability for speed. MTP is the fastest but also the least accurate. MPA is slightly slower but on average is more accurate. However, the current paper is not complete in the sense that it lacks treatment of \"defects\" (regions where the numerator and the denominator approach zero) in Pade approximant approaches. Therefore, the stability and convergence of MPA is somewhat concerning.\n\nIn the backward pass, viewing the computation of the gradient of the matrix square root as solving a Sylvester equation is interesting. However, experiments in the generic settings lack accuracy tests to reveal how close these Newton-Schulz-based approaches are to the real gradient.\n\nThere is a bit of concern when in section 4.2 the proposed approaches are not compared against the closest possible candidate (Huang et al (2019)) for the ZCA whitening problem.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}