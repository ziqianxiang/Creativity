{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The Authors propose a neural-network based approach for the phase retrieval problem. Solving the phase retrieval problem is key for important application areas such as crystallography or radioastronomy.\n\nAfter adding more baselines and other changes, 3 out of 4 reviewers recommended acceptance. Reviewer kQWk recommended rejection mostly based on the fact that the paper is quite narrow in scope.\n\nReviewer kQWk is right that the topic might not appeal to most of the ICLR community. It is worth noting that the main contribution of the paper is not about neural networks but rather about connecting phase retrieval with Blaschke products. As it stands, it seems that after making this connection, any non-linear approximator could do well.\n\nHaving said that, this is an important application area and the progress is welcomed. Hopefully, it will draw inspire more research in this area.\n\nCurrently, the key issue of the paper is that it is very challenging to understand for people without background in the phase retrieval area or complex analysis. To make this paper more valuable for the ICLR community, I would strongly encourage the Authors to devote at least a page to explanation of what is the phase retrieval problem, and the intuition behind the solution. Perhaps [1] could serve as an inspiration.\n\n[1] Phase retrieval in crystallography and optics, R. P. Millane"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces a new neural network architecture, called the BPNN, to recover phase information in physical problems where one only has access to the magnitude of a function on the complex line. This is a common problem in experimental physics and scattering where phase information can be lost, but is nonetheless important. To recover the phase in this setting the authors propose approximating the phase using Blaschke Products, which can approximate any continuous function on the imaginary line whose magnitude is one. Specifically, the authors train a fully-connected neural network to compute the coefficients of a rational approximation to the Blaschke product. They demonstrate the success of their method on a number of example problems and include code to reproduce their results.\n",
            "main_review": "I liked a number of aspects of this paper. I thought the problem was well-motivated and the authors did a good job of explaining the underlying theory. Although I am not an expert on phase retrieval, the proposed method seems pretty elegant and clever. The experiments also seemed relatively thorough and the authors definitely tried their method on a number of different problems.\n\nHaving said this, there are a number of ways in which I think the exposition could be improved (it is also possible that I am missing some details).\n1. Reading the paper, my main question was what the fully-connected neural network brings to the table? If I understand correctly, it seems like the authors could have just regressed the coefficients of the rational approximation directly. I’d appreciate some clarity on why this couldn’t be done or why it wasn’t included as a baseline.\n2. I think I’m missing something about the training procedure. In particular, the equation for the backward pass features f(j\\omega_i), which presumably includes phase information. However, I thought the premise was that only |f(j\\omega_i)| was available. Can the authors clarify that this equation is really correct as written?\n3. I think the section on the piecewise BPNN could use some elaboration since it wasn’t totally clear what was happening here.\n4. I, personally, could have used a refresher about why the function values are only available on the imaginary line. \n5. While it is nice that the authors included code (which did help me understand exactly what was happening) I think the authors should clean it up a bit and add some comments before the conference.\n6. There were a number of places in the paper where the authors used terminology such as “it is easy to see that…” Typically I don’t find this style of exposition especially helpful. In particular, I think a little bit more detail on the rational approximation to B_k(j\\omega) could have been nice.\n\nDespite these issues, I do think this paper seems interesting and potentially useful. I also think the trick of using Blaschke products is something I hadn’t seen before and other researchers might find it interesting. So I do support publication. ",
            "summary_of_the_review": "A clever paper using a relatively uncommon mathematical trick to make progress on a niche, but important problem. While the paper could be improved, I do think the ideas here could be interesting and useful to the community. The experiments seem relatively compelling and code is included with the submission.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "**Summary**\n\nThe paper proposes a new method, BPNN, for phase retrieval, where the phase of some holomorphic or meromorphic function is predicted from measurements of its magnitude. The authors show that the unknown function can be expressed in terms of Blaschke products. Given some tolerance epsilon to the unknown function, the Blaschke product amounts to a rational function. A neural network is then used to predict the coefficients of this rational function and is trained using L2 loss. Finally, BPNN is compared against using a neural network to directly approximate the unknown function.",
            "main_review": "**Strong Points**\n\n- Imposing priors on neural networks based on the physics of the problem is very important. Using Blaschke products to do this is an interesting idea.\n\n\n**Weak Points**\n\n- No comparison to existing methods in phase retrieval.\n- No ablation study.\n\n\n**Questions**\n\n- How does the size of the Blaschke product in Theorem 1 depend on epsilon? How do you choose the degree of the rational function?\n- Why is a neural network used to predict the coefficients for the rational function? How does this compare to simply fitting the ration function directly?\n- In Section 4.1.1, p(z) is “randomly selected.” What does this mean? What is the distribution over p(z)?\n\n\n**Additional Feedback**\n\n- Section 4.2: “Each NNs are trained with 6000…” This is still no guarantee of convergence. Other metrics, like gradient norm should be monitored for this.\n- There are many small grammatical errors. \n",
            "summary_of_the_review": "**Recommendation**\n\nMy sense is that in its current form the paper should be rejected. Especially given the lack of comparison to existing methods. I am not an expert on phase retrieval, so I am happy for the other reviewers and rebuttal phase to change my mind.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors introduce a neural network architecture based on Blaschke products for phase retrieval applications.",
            "main_review": "I would suggest to conduct a few additional experiments to better analyze the model behavior. First, a comparison against other baselines than the neural network model should be performed if feasible, e.g. compressed sensing based methods or even the Kramers and Kronig formula. While the latter might not be a good choice due to the problems with the Riemann sum approximation for few datapoints, it could e.g. still provide a more complete picture.  It would further be interesting to see, how the methods behave for larger training/test set splits (e.g 50:50). Similarly, the influence of different possible configurations of BPNNs is underexplored. How do they behave with respect to the number of roots used? What is the influence of different window choices on the piece-wise BPNN implementation? Regarding the results section in general, the paragraph on the MMA experiments (page 8) is hard to follow and should be reworked. In general, including only the learning curves of a few representative NNs in the figures should be sufficient, the rest could e.g. be moved to an appendix.\n\nThe section regarding model architecture would profit from additional information as well. It is, for example, unclear what activation functions were used in the BPNNs and fully connected NNs or what kind of optimizer was used for training the models.\n\nFinally, there is a slight confusion regarding the number of frequencies used in the metamaterial absorber dataset. Table 1 reports 256, while the text states that 1000 frequency points are used. ",
            "summary_of_the_review": "In general, the idea to use neural networks to parametrize Blaschke polynomials for predicting phase functions is well motivated and shows promise based on the experimental results. The architecture and theory are presented in a concise fashion. However, the evaluation of the model is quite minimal and could profit from a more in-depth analysis in the form of additional experiments. Nevertheless, I tend towards accepting the work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In complex analysis, the Blaschke product is an important family of bounded analytic functions in the open unit disk that is constructed to have roots at prescribed complex numbers. Finite Blaschke products are classically used for constructing rational functions with prescribed poles or zeros, which are then used in the approximation theory, differential equations, dynamical systems, and harmonic analysis.  Here, the authors use finite Blaschke products for phase retrieval of holomorphic and meromorphic functions. By looking at the argument of the approximant, the authors are able to estimate the phase at a set of frequencies. The authors use an L2 loss function to learn the parameters in the Blaschke product.",
            "main_review": "I am not aware of a publication that considers computing Blaschke products. Therefore, if this is a good idea compared to previous approaches, then the manuscript is valuable and novel. However, while the paper compares against different neural network structures and setups, it does not compare to standard approaches for rational approximation or phase retrieval that do not involve neural networks.  In particular, I find that there is a significant lack of motivation of why one would use a neural network to compute a Blaschke product, as opposed to using existing data-driven methods for approximating functions by rational functions or exponential sums. For example, for data-driven methods for approximating functions by rationals we have the AAA method (“The AAA algorithm for rational approximation” by Nakatsukasa, Sete, & Trefethen) as well as data-driven methods for approximating by exponential sums, see Prony’s method and MUSIC.  \n\nWhat is the main motivation for using piecewise BPNN? Is that so the Blaschke products can be kept low-degree?  A potential weakness is that the Blaschke products have a fixed degree as the structure of the neural network is fixed.  In data-driven methods for approximating with rational functions, it is found to be essential to adaptively select the degree of the rational function; otherwise, there are numerical stability issues and spurious poles. Can the authors comment on this? \n\nI am also confused by the numerical results. What is the y-axis in the MSE plots?  Is it the logarithm of the MSE in base 10 or the natural log? ",
            "summary_of_the_review": "I believe this manuscript has a promising idea of using neural networks to learn the parameters inside of a Blaschke product. Since this is fundamentally a technique for approximating functions of one complex dimension, the application to phase retrieval seems very natural. I think the authors have a half-baked idea, though. In my opinion, they fail to fully compare or motivate their choices. In particular, I wonder how the author's approach compares to other rational function approximation schemes that do not use neural networks such as the AAA algorithm. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}