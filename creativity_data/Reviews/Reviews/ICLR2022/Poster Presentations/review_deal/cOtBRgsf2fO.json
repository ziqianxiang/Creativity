{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "All reviewers concur on the fact that the paper contains solid ideas. The discussion helped clarify the case of class-imbalance and no major concerns remained after discussion phase. I thank the authors for the additional details on execution time / complexity.\n\nOn a separate note and perhaps to dig further in the paper's ideas,\n\n1- the validity of the Gaussian assumption carried in the paper was raised (e.g. ercK), but I would like to point out that Theorem 2 can also be derived for general exponential families given the objective in (2), with perhaps a reformulation of the trace constraint (still, this would imply the knowledge of the exponential family for the KL divergence to simplify).\n\n2- when it comes to protecting labels, the authors might want to have a look at the rich literature on learning from label proportions, which shows that the knowledge of the class is not necessary to learn a supervised model (see for example Patrini et al, NeurIPS / NIPS 2014). Thus, protecting the class could in fact be more achievable than by just considering that learning “needs observed classes”."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper formulates a threat model on two-party split learning (parties have different features, with one party holding the labels) for binary classification, and provides insights about how simple functions on the gradients can be used to extract confidential label information. The authors then proceed with defenses to these attacks, based on random perturbations of the communicated gradients from one party to another. In doing so, the authors motivate a new measure for privacy termed the leak AUC, based on the AUC of the ROC curve.",
            "main_review": "Strengths:\n\n* The paper studies an interesting problem. \n\n* The paper is well written and organized. \n\nWeaknesses: \n\n* The setup for attack models (e.g., observations in attacks 1 and 2 in Section 3.3) seem very restrictive, and may only work for a specific set of binary classification problems with highly imbalanced classes, such as the disease prediction example given in the paper. What if the labels are more homogeneous, what would be the impact on observation 1? \n\n* The privacy objective is defined as a function of KL divergence with respect to any underlying probability distribution on the perturbed gradients. This appears as a rather difficult objective to achieve in general. In fact, the paper then assumes a Gaussian distribution as the underlying distribution (and the perturbations) for each of the classes while evaluating the objective function. This assumption is not clear. The first thought that comes to mind is the central limit theorem, but then, as mentioned in the paper, one class may have very few samples (e.g., the disease prediction example). Moreover, it appears that the empirical mean of each class is treated as the true mean. However, the validity of this assumption would also depend on the number of samples collected for each class, which could be highly imbalanced between the two classes according to the motivated problem setup. This, combined with the fact that the max_norm heuristic seems to perform as well as Marvell, seems to require a stronger motivation for Marvell. \n \n\n\n\n\n",
            "summary_of_the_review": "Understanding the label leakage problem and protection against it under a split learning setup has the potential to start an intriguing line of work. The main drawback of this work is the restricted setup on which the attack/defense models are built on.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors in the paper consider stealing the private label information from the party that does not know the label of training data during split training for binary classification. Specifically, the attack methods are based on differences of gradient information between positive and negative examples and the defense method MARVELL is based on optimal random perturbation solved by an optimization problem. Experimental results suggest that the proposed method could effectively defend against the label stealing attacks.",
            "main_review": "The paper considers a novel setting in split learning: stealing binary labels during training and the setting could not be solved directly by differential privacy. To begin with, the authors first explore the possibility of stealing label information via some heuristic observations of the gradient information of positive/negative examples. The effectiveness of the proposed attack is shown in the experiments. The authors then propose an optimized random perturbation approach to mitigate the attack and the defense is shown to achieve the best trade-offs among the baselines. The paper is well-written and clearly organized. Overall, I think the paper makes solid contributions. \n\nMy biggest concern is that the paper is limited to (class-unbalanced) binary classification. I carefully check the proposed attacks, but find it difficult to extend the multi-class classification. For example, observations 1-4 are not necessarily extended to multi-class classification. \n\nBesides, since the proposed defense MARVELL at each update step requires solving an optimization problem, the (time) efficiency of MARVELL should also be discussed and demonstrated in the experiments. I am also curious about how the locations of the cut layer could affect the performance of the proposed attacks and defense (not limited to the first layer). \n",
            "summary_of_the_review": "1. The paper makes solid contributions in privacy-preserving machine learning.\n2. Time efficiency of the proposed methods should be discussed and more ablation study can be done.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "## Summary of Contributions\n\nThis paper studies the multiparty setting where there are two parties: the first party (aka non-label party) holds the feature vectors $X_i$ of user $i$ whereas the second party (aka label party) holds the corresponding labels $y_i \\in \\{0, 1\\}$. Together they wish to jointly train a model in such a way that the non-label party does not learn of the users' labels. This is especially relevant in online advertisement (where the label party corresponds to the advertiser who knows whether each user converts and the non-label party is e.g. the publisher who shows the ads) and in medical applications (where the label party is the hospital).\n\nThe paper focuses on *split training* which is a setting where the model is divided among two parties with the first layers (denoted as a function $f$) is with the first party and the remaining layers (denoted as a function $h$) are with the second party. Here the training is performed as follows. First, the non-label party computes $f(X)$ (referred to as the \"cut layer\") and send this to the label party. The label party then compute the gradient w.r.t. $h$ and update its parameters and furthermore it computes the gradient $g$ w.r.t. the cut layer $f(X)$ and sends it back to the non-label party. The non-label party can then use backpropagation starting with $g$ to update its own parameters. The main concern tackled in this paper is that, in such a scheme, the gradient $g$ sent back to the non-label party can leak the information about the label. Specifically, the authors observe and experimentally show that the following two attacks are very effective in predicting the labels given the intermediate gradient $g$:\n- Consider the norm $\\\\|g\\\\|_2$ and make prediction based on whether it exceeds a certain threshold.\n- Consider the cosine similarity between $g$ and another (fixed) gradient, and then threshold.\nIn fact, the two above attacks get near-perfect predictions in some examples. The exact measure they use to evaluate their model is the area-under-curve (AUC) of the (false positive rate, true positive rate) curve.\n\nThe authors then propose to add noise in order to mitigate such attacks. The baseline here would be to add isotopic Gaussian noise (with a certain scale). The authors then formulate an optimization problem aiming to achieve better privacy-utility tradeoff. Roughly speaking, this is an optimization problem of the form \"minimize AUC\" such that \"utility is at least ...\". It is hard to make this precise (and it is probably inefficient), so the authors use certain proxy for both the objective and the constraint. For the objective, the authors show that AUC is upper bound by a certain quantity involving the KL divergence of the distributions of the gradients from the two labels. For the utility constraint, the authors instead try to restrict the amount of the noise added by having an upper bound on the trace of their covariance matrices. Furthermore, the authors show that when the (unperturbed) gradient distributions are assumed to be Gaussians and only Gaussian noises are considered, then the optimization problem simplifies greatly to just a single constant-size optimization problem (Theorem 2). This final form constitutes their so-called Marvell algorithm. Finally, the paper concludes by several experimental results showing that Marvell is very effective against protecting the two aforementioned attacks in real-world datasets and models, while preserving reasonable level of privacy.",
            "main_review": "## Strengths\n\n- Label privacy is an overlooked problem that has many practical applications. This paper provides a formulation for this problem together with a fairly strong attacks and a promising mitigation.\n\n## Weaknesses\n\n- I think the current split learning setting seems strange: if our goal is strictly to protect just the labels and not the feature vectors, it would also be fine to the non-label party to send the features directly to the label party; then, the latter can train the entire model by itself without leaking any label at all. This seems much better for label-privacy and also for communication complexity.\n\n## Comments for Authors\n\n- As stated above, it would be good to justify the current split learning setting more. For example, is the goal here to also protect the X? If so, then shouldn't we also employ some method for that? And how would such a method, in conjunction with those presented in the paper, effect the utility?\n\n- It is stated (on pages 2 and 3) that \"DP and its variants are not directly applicable metric in our setting\". I do not think this statement is true. In fact, *local* DP would still be applicable to non-aggregate data. The paper [Ghazi et al. 2021] you cited also yield label DP in the local setting without aggregation (by flipping the labels). Similarly, the isotropic Gaussian baseline discussed in the paper also achieve some level of local DP. I think this point has to be clarified in the revised version.\n",
            "summary_of_the_review": "## Recommendation\n\nI think formalizing attack vectors and proposing mitigations for label privacy is an important contribution that may have a large impact. On the other hand, the current formulation/attack/mitigation is tailored towards the specific split learning setting which (as stated above) might require more motivation. Taken both into account, I'm giving a weak accept for now.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}