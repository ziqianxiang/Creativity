{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This submission proposes a method for learning convolutional filters with trainable size, that builds on top of multiplicative filter networks.  Anti-aliasing is achieved by parametrization with anisotropic Gabor filters.  The reviewers were unanimous in their opinion that the paper is suitable for acceptance to ICLR.  The authors are encouraged to make use of the extensive reviewer discussion in improving the final version of the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a method for learning convolutional filters with trainable size, that builds on top of multiplicative filter networks. The learnable convolutions are called FlexConv, and a network that deploys them is called FlexNet. In order to control the aliasing eventually introduced when learning the convolutional filters, anisotropic Gabor kernels are used within the multiplicative filter networks.",
            "main_review": "### Strengths \n1. Nice ideas and well-argumented design\n2. A mechanism to control aliasing\n3. Results above those of existing works\n\n__Idea.__ The approach is based on the idea of allowing the learning of filters of different size within a convolutional network, and it is supported by literature and empirical studies. The use of continuous convolutions via implicit representations is becoming of interest for the community. This work has thus relevance and places itself well within the current developments in representation learning.\n\n__Anti-aliasing.__ The observation that other implicit representations (e.g. SIREN) suffer from aliasing when kernels learned for a given resolution are applied to higher resolution data stimulated the authors to investigate the possibility of learning an anti-aliased implicit representation network. The use of anisotropic Gabor functions as kernels for the implicit representation networks (which can be considered, in my opinion, the true contribution of this work) allows for controlling the maximum learnable frequency.\n\n__Results.__ The results obtained are higher than those reported by other existing works. This places this paper as state-of-the-art for learnable kernel networks.\n\n\n### Weaknesses\n1. Contributions are not made crystal clear. Novelty seems oversold.\n2. The work seems a marginal improvement on CKConv, with the difference that an anisotropic Gabor kernel is used to modulate the implicit network representations.\n3. The computational impact of the learnable kernels is not discussed enough. Application to 'normal-size' images is \n4. Some comparison statements are questionable.\n\n__Contributions.__ In my opinion, the authors make overstatements about their contributions. FlexConvs seems like a re-branding of CKConvs, while FlexNets are networks that deploy FlexConvs but are presented as a contribution themselves. The true contribution of this paper, and the main part where novelty is presented, can be considered the use of anisotropic Gabor functions as kernels of the implicit representation networks. The Gabor functions indeed allow to control the extension of the Gaussian masks that determine the size of the kernels, and also constrain the maximum learnable frequency for the kernels in order to avoid aliasing in the networks. \n\n__CKConv.__ This work seems to build a lot on top of CKConvs (also under review @ICRL22 - here results of CKConv are reported, not likewise for the CKConv paper), but fails to highlight what the real contribution is (see point above). \n\n__Computational impact.__ The computational impact of using MAGNets is only briefly addressed in this paper. An obvious observation is that bigger kernels require more computations because of the increased number of convolutional weights. This also relates to the increased size of the images in the case a network is applied to higher-resolution images (the case of a network learned on CIFAR 16x16 and then upscaled to CIFAR 32x32). In this work the maximum threated resolution is indeed 32x32, also in the case of ImageNet, which is rescaled. An actual discussion and experiments about the learning or application (by upscaling) of the proposed approach to REAL higher-resolution images (e.g. the original ImageNet) are missing. This limits, in my opinion, the transferrability of this work to real-case uses. \n\n__Questionable statements.__ Some statements about results are questionable or not precise. For instance, the authors claim that the proposed FlexNet achieves comparable results to much deeper ResNets. Actually, the ResNet and the FlexNet concerned by this statement have roughly the same number of parameters, which may influence more directly the learning capabilities of the networks (and thus the results). The ResNet has  blocks (although more convolutional layers), while the FlexNet has 7 FlexConv Blocks. Should it then be considered deeper than the ResNet? I think that the authors should not bring the depth as an argument at their advantage, but rather focus on comparing the networks in terms of the learnable parameters.\n\n\nOn a general note, the paper introduces nice ideas, but it should be better written to highlight the contributions and what is taken from the state-of-the-art.\n\n=========================================================================\nAfter reading the response to reviews and the revision of the paper, I had my concerns clarified. I identify the novelty and amount of contributions provided by the authors in this paper, and recognize their value for further development of CNNs. Although relation with training at low resolution and testing at higher resolution is 'only' drafted in this paper, one can deal with the fact that this paper is a good seed for further experiments in future works. Thus I am inclined to raise my score.\n\n",
            "summary_of_the_review": "The paper introduces some new nice idea, but it seems overstating some of the contributions. The experimental analysis is extensive but on very small data set of low-resolution, while actual problems encountered to scale this method up to more 'real-case' resolutions are not discussed appropriately.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose FlexConv, a novel convolutional operation with which high bandwidth convolutional kernels of learnable kernel\nsize can be learned at a fixed parameter cost. The FlexNet obtains state-of-the-art across several sequential datasets, and matchs recent works with learnable kernel size on CIFAR-10 with less compute.",
            "main_review": "\n(1) Section 4.1 is not very clear, how the target image is fitted by different kernels? What is an AlexNet kernel, 3x3, 7x7? Please give more descriptions of the experiment.\n\n(2) In Table 3, it would be better to show the time consumption of ResNet-44 or its reproduction since FlexNet has additional operations that will reduce the inference speed, which is much more crucial than the model size.\n\n(3)  This paper and its related work focus on kernel sizes, dilations, etc., which may obtain greater improvement on high-resolution images. However, it is not clear whether FlexNet performs well on high-resolution image tasks, such as ImageNet or MS-coco in the cited work [1].\nThe authors are expected to demonstrate the superiority of FlexNet on typical and practical tasks (classification, generalization, detection), which would draw more attention from the CNN community.\n\n(4) Why \"alias-free\" is important? The authors claim that FlexNets can be deployed at higher resolutions than those seen during training. But the experiments only show the improvement on accuracy. It would be more clear if authors can show the superiority on other aspects, for example, vanilla nets are trained on high resolutions while FlexNet are trained on lower resolutions, they have comparable classification accuracy but the latter are more efficient in terms of time/computation cost.\n\n[1] Dai, Jifeng, et al. \"Deformable convolutional networks.\" Proceedings of the IEEE international conference on computer vision. 2017.\n\n==========================================================================\n\nI have read the authors' rebuttal, some of the concerns have been addressed, others still remain.\nThe authors show that the reproduction of the ResNet-44 is 22s per epoch during training, which is much faster than FlexNet series with similar # parameters and accuracy, which almost can be extended to the inference stage.\nIn the reply to point 4, the author also notes the computational efficiency of alias-free models. \nHowever, a direct comparison between FlexNets and classical models (ResNet, DenseNet, etc) with regard to the accuracy, training time, model size, inference time, etc, is still lacking.\nAside from ablation study in the settings of FlexNet models, the authors are encouraged to provide more evidence such that the community is willing to discard common practices and adopt your methods on some occasions.\nIn summary, the reviewer will keep the score.\n\n",
            "summary_of_the_review": "This is a very interesting study, the method and theoretical analysis seem sold, but the motivation and comparative experiments should be improved.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a new convolutional operation with varying-sized kernels. Flexconv constructs their convolutional kernels with the product between the CKConv and Gaussian masks so that the size of the kernels can be learned during training. Also, in the network of Flexconv, MAGNets allow the Flexconv to be used at higher resolutions without aliasing which are unseen resolution. The experiment on several sequential datasets showed that the classification performances from both TCN and ResNet based on FlexConv outperform that of state-of-the-art networks.",
            "main_review": "\nPros.\n1.\tThis paper introduced a new parameterization method of CNN kernels by multiplying continuous kernels and with sigma-learnable Gaussian masks. Especially, multiplication with learnable Gaussian is first demonstrated.\n2.\tFlexconv operation is robust to fit high frequency signals in large kernels by MagNets.\n3.\tAccording to Table 4, the proposed network shows good performance on higher resolution images without additional training.\n4.\tThis work achieves state-of-the-art performance on several sequential datasets. \n\nCons.\n1.\t“Large kernels” and “small kernels” are too obscure.\n2.\tAlthough the meaning of “bandwidth” seems to be important, the descriptions are not enough to understand. It confuses the original meaning of bandwidth; originally, bandwidth in signal processing is a term for finite support in frequency domain.\n3.\tIn table 4, the two scales of 16 pixels and 32 pixels are inefficient to explain the performance of FlexConv. This is exaggerated without further scale experiments.\n4.\tThe authors claimed that the cropping operation in Flexconv improves computational efficiency. But there is no ablation study about the cropping operations, reviewer has no clues about the relationship between cropping size and computational efficiency.\n\nMinor.\n\nIn page 20, there are type such as ‘??’.\n",
            "summary_of_the_review": "The way of representing and analyzing the results is a bit overclaimed without serious supporting experiments. However, their method demonstrated state-of-the-art performances compared with recent approaches. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a novel convolutional operation named FlexConv, to produce high bandwidth convolutional kernels with learnable kernel size at a fixed parameter cost. It is able to generate kernels with large kernel size and model long-term dependencies among elements in a sequence or an image. State-of-the-art performance on both sequential datasets and image datasets demonstrates the effectiveness of the proposed method. In addition, a novel kernel parameterization method is proposed to control the frequency of the generated kernels and avoid aliasing, and hence can well generalize to higher resolution cases which are never seen during training.",
            "main_review": "Pros  \nThe FlexConv is proposed to produce learn kernels with various kernel sizes.\nAn effective regularization is proposed to address the aliasing issue and generalization when dealing with higher resolution at the inference stage.\n\nCons\n1. Incremental novelty compared to Multiplicative Filter Networks (Fathony et al., 2021). Anisotropic gabor functions are used to replace the original isotropic gabor functions in MFNs.\nIn addition, there is no explanation on why anisotropic gabor functions are used here and why it is better.\n2. The idea of using gaussian mask to control the effective receptive field has also been proposed in Chapter 3.3 in [a]. The author even discussed different parametric kernel functions in that thesis.\n[a] Duc Tam Nguyen, Robust Deep Learning for Computer Vision to counteract Data Scarcity and Label Noise, Doctoral Thesis, 2020.\n3. There is no explanation on how the diversity in the initialization of $\\gamma^{(l)}_X$ and $\\gamma^{(l)}_Y$ relates to the accuracy.\n4. There is no explanation on how equation 9 is derived.\n5. Why not put relative change instead of absolute number in the left table of Figure 5, but use the absolute number in the right table?\n6. Lack comparison with other methods on image benchmark datasets. Why only compare with ResNet44 instead of more popular ones such as ResNet50, ResNet 18, ResNet 101?\n7. Lack comparison between a ResNet and the one whose all normal convolutional layers are replaced by FlexConv Layers.\n8. The result of figure 6 is a little different from intuition and classic convolutional neural networks such as AlexNet and ResNet. Any explanation on this?\n\n--------------------------------------------------------------  \n\nThe explanation of authors has clarified some of my concerns but there are still some issues remained. Therefore I only raise my ratings to '6: marginally above the acceptance threshold'.",
            "summary_of_the_review": "The novelty of the paper is incremental and further explanation and experiment are required to demonstrate its effectiveness.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}