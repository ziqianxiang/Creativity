{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "To address the problem of unauthorized use of data, methods are proposed to make data unlearnable for deep learning models by adding a type of error-minimizing noise. Based on th fact that the conferred unlearnability is found fragile to adversarial training, the authors design new methods to generate robust unlearnable examples that are protected from adversarial training. In addition, considering the vulnerability of error-minimizing noise in adversarial training, robust error-minimizing noise is then introduced to reduce the adversarial training loss.\nThe authors have tried to respond to reviewers' comments along with adding more experiments.\nOverall, this manuscript finally gets three positive reviews and one negative review, where the possible vulnerability or robustness of error-minimizing noise against (simple) image processing operations was not verified.\nIn comparison with other manuscripts I'm handling that got consistent positive comments, this manuscript is still recommended to be accepted (poster) with a further study of robustness under simple image transformations in the final version."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Based on a previous work (Huang et al. 2021), this paper proposed a method, called REM, that generates makes a dataset \"unlearnable\" to adversarial training. Experiments are conducted to validate the effectiveness of REM under various settings, including (1) different adversarial training perturbation radiuses, (2) different proportions of unlearnable data, (3) different model architectures, on different common datasets, including CIFAR-10, CIFAR-100, and ImageNet.",
            "main_review": "This paper is well-written, and the experiments are extensive. However, the proposed REM has the following issues:\n\n* Unrealistic assumptions. The authors assume that a \"data protector\" knows the hyperparameters (such as the loss function (l_{1}, l_{2}, or l_{inf}) and perturbation budget (rho_{a})) of adversarial training to be used by a data consumer, which is unlikely to hold in practice. Although the author used a large perturbation budget to evaluate REM, the budget may differ across different applications for many reasons (e.g., due to different degradation in clean performance introduced by adversarial training). Also, recent studies (e.g., (Tramèr and Boneh 2019)) have demonstrated that adversarial training with a single type of perturbation cannot provide well defense against other types of adversarial attacks, yet adversarial training with multiple perturbation types is still an active research problem (Maini, Wong, and Kolter 2020; Madaan, Shin, and Hwang 2020; Zhang, Zhang, and Wang 2021; Stutz, Hein, and Schiele 2020). Furthermore, there is a recent study (Yuan and Wu, \"Neural Tangent Generalization Attacks,\" 2021) that makes a dataset unlearnable using blackbox settings. The authors should 1) cite the work (Yuan et al. 2021) and discuss whether the problem targeted by this paper still exists for blackbox methods, and 2) discuss in more detail how REM perform against adversarial training with multiple perturbations, and 3) discuss how to choose the perturbation budget when running REM for different applications in practice.\n \n* Time complexity. The REM finds unlearnable examples under adversarial training by approximating a min-min-max problem, which can be very time-consuming and limits the practicability of REM. How long does take to make a full MNIST, CIFAR10, CIFAR100, and ImageNet unlearnable, respectively?\n\nFurthermore, exiting experiments lack some critical information and more should be conducted:\n\n* None of an experiment shows how training and test accuracy evolve as the training epoch increases. There’s no evidence to prove that \"the poor accuracy is not come from overfitting on the training data.\"\n\nTypos:\n1. Page 6 line 2: the loss has a typo  \n2. Table 7: \"AP\" should be \"TAP\"\n\nEdit after rebuttal:\n\nThe authors have partially addressed my concerns, so I am willing to raise my score provided that the authors agree to further improve the paper in the following aspects:\n1. Show the actual time cost of your approach and different baselines (including the new NTGA). Please describe the hardware (CPU/GPU) and settings you use in detail. \n2. Discuss in more detail how to set the epsilon in practical situations, especially for the black-box settings.  \n",
            "summary_of_the_review": "Overall, this paper studies an important and interesting problem. The paper is well-written and the experiments are extensive. However, the assumption used by the proposed REM is not realistic, and the authors missed an existing blackbox method that could entirely destroy the motivation of this work. Furthermore, some critical information, such as runtime, is missing from the current experiments. Therefore, I give a vote for rejection before the above concerns can be addressed.  \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes a min-min-max formulation to generate robust unlearnable examples in order to protect data privacy in adversarial learning. The basic idea of this paper builds upon a previous paper \"Unlearnable examples: Making personal data unexploitable\" where the error-minimization noise is generated to reduce the training loss such that the model performs badly on the clean data. This paper finds that such unlearnability is fragile in adversarial training and fragile to minor data augmentations. Therefore, it proposes to generate more robust unlearnable samples by considering adversarial loss and data augmentation in the noise generation process, and it shows the generated noise can effectively reduce the quality of the trained model through extensive empirical study.\n\n",
            "main_review": "## Strengths\n1. The paper is clearly written and is easy to follow. \n\n2. The paper considers the adversarial learning setting, which might be useful in practice. The proposed min-min-max framework is reasonable in principle.\n\n3. Extensive experiments are shown to demonstrate the effectiveness of the generated robust error-minimization noise in terms of reducing test performance (but it doesn't show how data privacy can be protected as discussed in the weakness).\n\n## Weaknesses\n1. The paper majorly builds upon the previous work on unlearnable examples which only consider normal training. Although this work improves the reliability of the noise generation by replacing the original loss with adversarial loss and considering the expectation of data augmentation, the proposed method is quite straightforward and a bit trivial. Therefore, the novelty of the paper is a bit limited. \n\n2. The paper aims to protect data privacy but there is a lack of measurement of how privacy is protected. In all experiments, the paper focuses on showing how effectively the manipulated data degrades the test performance of the trained model. As far as I am concerned, this goal is the same as data poison attacks. However, it is unclear why and how the test performance relates to data privacy. From my understanding, the fact that a model performs badly on some samples doesn't necessarily imply the data privacy of those samples is protected. A thorough discussion on this issue will be necessary since protecting data privacy is the major goal of this work. Otherwise, it might be more appropriate to put it in the context of data poison attacks.\n\n3. The paper claims that the unlearnability conferred by the noise generated via Eq. (4) is fragile to minor data transformation and proposes to adopt the expectation-over transformation technique (EOT) into the generation process. However, the paper doesn't present evidence to support this claim. Moreover, it is expected to include more ablation studies about the effect of the EOT technique in both the normal setting and the adversarial setting. \n\n4. There is a lack of discussion of the data poison percentage that can effectively reduce the model performance.\n\n",
            "summary_of_the_review": "Overall, the paper has shown that the proposed method can successfully reduce the test performance of the model (adversarially) trained by generating robust-error-minimization noise. However, the novelty is a bit limited and more justification of the data privacy protection is necessary. Therefore, I would like to recommend rejection. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The paper studies how to prevent misuse of people's data. There is no ethics concern.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Error minimization noise was first proposed so that when a dataset is released to the public, conventional empirical risk minimization cannot learn a good model from the perturbed dataset. However, error minimizing noise is only effective to the extent that adversarial training is not used. Adversarial training was identified as an effective method of overcoming the error minimization noise in prior work. In this paper, the authors proposed a variant of the error minimization noise, which continues to be robust even in the presence of adversarial training. Specifically, they generat the adversarial noise with a similar method as the original error minimization paper, but calculate the gradients with respect to the attacked input as opposed to the clean input. ",
            "main_review": "Strength\n- The paper is well motivated as being able to securely release the dataset to the public is an important problem that people care about\n- The authors proposed a sensible remedy (robust error minimization noise) for the weakness identified by the prior authors.\n- The authors are thorough with the experiments to illustrate the effectiveness of the proposed approach compared to the prior methods\n- The authors have provided sufficient details for the experiments to make me feel comfortable reproducing their results\n\nWeakness\n- The proposed method seems quite straightforward. While the method is effective, the results are not that surprising.\n\nQuestions\n- In table 1, what are the differences between REM with radius 0 and EM? It seems like the numbers between the two columns are quite different\n\n",
            "summary_of_the_review": "While the novelty of the method is weaker, I do find the study of error minimizing noise to be important. The proposed method is sensible, and the experiments are quite thorough to convince me of the effectiveness of the proposed method. Overall, I like the submission, and would recommend weak accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies how to make data unlearnable towards adversarial training. In prior works, including dataset poisoning, no methods can prevent data from being learned in adversarial training. This paper proposed a robust error-minimizing noise to solve this task. ",
            "main_review": "Overall, I lean towards recommending accepting this paper. \n\n---\n\nStrengths:\n- The proposed robust unlearnable example is technically sound and empirically demonstrated it can be effective when used against adversarial training. Experiments are comprehensive. \n- As suggested in the paper, none of the existing work can effectively be used to protect the data against adversarial training, this work demonstrated adversarial training could also be vulnerable to training-time perturbation/data poisoning. \n- Paper's presentation is easy to follow and most related works have been discussed.\n\n---\n\nQuestion/weakness: \n- How important is the EOT for the robust error-minimizing noise? An ablation study would be very helpful. Why is the REM on ImageNet can show the content of the image while not on CIFAR-10? Is this because of different data augmentations between CIFAR/ImageNet for EOT? \n- Is the proposed method only effective if the epsilon for the defensive perturbation is larger than epsilon for the adversarial training? For table 1, if adversarial training uses larger epsilon, at what point does the REM become ineffective?\n- What is extra overhead (time) compared to EM/TAP?\n- Need some clarifications for Algorithm 1. Line 10-13 finds the adversarial perturbation for J steps, and line 14-15 finds the unlearnable perturbation for a single step, is that right? For the adversarial perturbation, then the maximum change is 4/255, but for a single step of the unlearnable perturbation, the step size is $\\alpha_u$, wouldn't the adversarial perturbation always overwrite the unlearnable perturbation? ",
            "summary_of_the_review": "Existing works have demonstrated adversarial training (AT) can remove the effect of both data poisoning using targeted adversarial attacks and unlearnable examples. This paper demonstrated an effective method that can prevent the data from being used for AT. Despite the potential limitation on the perturbation radius difference between AT and the unlearnable, this work reveals a weakness of adversarial training. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}