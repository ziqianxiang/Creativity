{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper proposes a methodology for alternatively growing and pruning a subset of layers within a network in order to eventually produce a trained, sparse model.  After discussion, all reviewers favor accept.  Empirical performance of the sparse models appears strong, but requires significant computational expense during training to achieve."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a scheduled grow-and-prune approach to produce a sparse CNN model. In particular, a sparse model is achieved by repeatedly growing and prune subsets of layers during network training. The main difference between this approach and other approaches that do pruning during the training time is that this approach guarantees that all the weights are explored when a pruning decision is made.\n\nIn the paper, two variations of the approaches are discussed: C-GaP and P-GaP. The theoretical convergence of the C-GaP is analyzed.\nExperimental results on various datasets and tasks are presented, which shows improvement compared to baseline approaches. Additionally, key variables such as partition number are explored and empirical results are provided in the supplementary results. ",
            "main_review": "Strengthes:\n1. The empirical results demonstrate the intuition that exploring all the weights could produce a better pruning result compared to a random selection of weights that an algorithm would explore. I am curious when a random selection algorithm is used, if we apply heuristics such as recording the selected weight to make sure all the weights are explored, would that lead to a similar result?\n2. A key variable of the proposed approach is the number of partition. Some empirical studies are done and presented in the supplementary results and a four-partition GaP is suggested. I wonder when the network size is big, whether the grow-and-prune process is slow within each partition?\n\nWeakness:\n1. The approach has several key variables and those variables are likely to impact the results. This makes it not easy for later studies to build upon this work.\n2. As presented in Supplementary results A2, Table 8, it seems that partition strategy doesn’t matter much. Does that mean any approach that could guarantee a complete transverse of the weights, it would produce a better pruning result? \n",
            "summary_of_the_review": "Empirical results and conclusions that are presented in this paper are interesting. Even if some of the theoretical analyses were made, the experiments are mostly empirical. Therefore, I believe the paper is useful for researchers in this area to read, but this approach doesn't make a breakthrough. Therefore, I rate it 6.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a new framework for model pruning. Unlike most previous pruning works, this framework does not require pre-training a dense model initially. They have proposed a schedule grow-and prune strategy to fulfill such a goal where the whole network has been split into multiple partitions. There will be an alternation of pruning and growing during the pruning, which ensures that all network parameters are updated in certain loops. Moreover, the authors provide theoretical insight behind such a design that all the weights trained per round are crucial for convergence. To accelerate the pruning process, this paper presents a parallel version that the growing and pruning of multiple partitions would be computed simultaneously. The accuracy and efficiency of the proposed framework have been verified on multiple tasks, including 2D images classification/detection, 3D detection, and text understanding.",
            "main_review": "Strengths:\n\n+ The key contribution of this paper is the grow-and-prune schedule that does not need the dense pre-training initially. Such a design can help to reduce the time/computation cost during training.\n\n+ Such a proposed design has ensured that all the weights could be updated during training, which is essential to the model convergence. Moreover, the authors provide the theoretical analysis, and it makes sense intuitively.\n\n+ There are rich and solid experiments provided in this paper. It has been verified on both 2D/3D vision and NLP tasks which are quite strong to show the effectiveness of the proposed method. And there is a detailed ablation study in the appendix, which is also helpful.\n\n+ This paper is written with clear logic and is easy to follow. The discussion of related works is comprehensive to include most of the recent and classic works.\n\nConcerns/Questions:\n\n1.\tTo verify the major claim in training efficiency, some of the important baselines are missing. It should be compared with the dense pre-trained model pruning in the time cost. For instance, the baseline’s total time should include pre-training time + pruning time + finetuning time. And please compare yours (pruning time + finetuning time) with it. Moreover, the performance of dense pre-trained model pruning is better provided for comparison.\n2.\tSince it has been claimed that the superiority of the proposed method comes from updating all the weights, I am not quite convinced about this point. As shown in Fig.2, it seems that the random mask exploration has not been limited within certain regions, which indicates that any parameters of the network can be updated. Although such a random mask exploration has not guaranteed that all the weights will be updated within some rounds, it looks like all of them will be updated in the end if there are enough epochs for exploration. Does this randomness destroy the convergence? Could you please explain more on this point?\n3.\tThe partitions of parameters seem tricky in both partitions number and boundaries. Could you do more ablations on such two factors (and ResNet-50 may be a good choice for this ablation study)? And it will be better to share some high-level instructions on how to set such two hyper-parameters.\n4.\tWhat is the reason for the performance divergence between P-GaP and C-GaP? And how much time/computation costs have been saved comparing P-GaP with C-GaP.\n5.\tAblation study on different sparsity (0%:10%:90%) may also be necessary. Any datasets are welcome.\n6.\tThe last “s” of “previous works” of Remark 3 in Page 8 is red.\n7.\tPlease make sure all the references are in the same format.",
            "summary_of_the_review": "I recommend borderline reject due to the missing of important baselines and I will consider changing my rating after seeing strong rebuttal.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This study takes use of pruning-rewiring-pruning method to get a high unstructured sparse model. Broadly, this is one dropout regularization method. With step-by-step growing and pruning, most of the weights are close to $0$ and will be pruned in the next step. Experiments show that this training method can improve the sparsity and performance for a wide range of architectures and applications,",
            "main_review": "Strengths:   \n1. This study provides one model compression method. With theoretical analysis, authors proved the correctness of this method.    \n2. This method shows preferable performance in constrast to prevent methods in various experiments.     \n   \nWeakness:  \n1. The creativety of this method is somewhat weak. The pruning-rewiring-pruning method have already been used in several papers, such as \"DSD: Dense-Sparse-Dense Training for Deep Neural Networks\" and some papers in the references.    \n2. Batch normalization layer is used to compute the statistics in current network. Wheter this kind of layer should be managed specially?   \n3. In experiments, the epochs of this method is much larger, even $10$ times more than other methods, which has negative societal impact and will limit its  applications.  \n4. The sparsity is very high, under 80\\% -90 \\% sparsity, the model maintains high accuracy.  However, the FLOPs has minor relations with inference speed. Unstructured pruning methods have limited befits to the memory even considering the storage mode of sparse matrix. So, what's the real gain by this method?",
            "summary_of_the_review": "This study provides one new training method of sparse model by scheduled grow-and-prune (GaP) methodology. The authors provides  a series of experiments. However, this study requires additional comparions, particularly some moden pruning methods under same experiment settings, such as SCOP, HRank, etc. Alternatively, the authors should include more information about the gains of this method.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Note: I have previously reviewed this paper for NeurIPS 2021 and I will be reusing my review from then and add the additional comments along the way based on how the paper has been updated along with the promised updates from the previous revisions. \n\nThis paper proposes a way to obtain sparse neural networks by repeatedly growing and pruning parts of the model in a scheduled fashion. Figure 1 gives a good idea of the whole technique. It basically divides the whole network into K parts and only one of these K parts is dense while the rest are sparse. The dense part keeps cycling ensuring over-parameterized learning when needed. The paper proposes that these scheduled routines help in improving the accuracy of the sparse neural network models compared to other sparsity-inducing techniques.\n\nThe paper has extensive experimental evaluation on tasks like classification, object detection, 3D object part segmentation, and Transformers. The authors also try to provide some theoretical explanations to these proposed ideas.\n\nThe paper has updated visualizations to make the algorithm much more clearer from the earlier version which is commendable. ",
            "main_review": "I will go sequentially starting with general comments followed by strengths and weaknesses.\n\nThe paper is hard to read and would benefit from a revision from a writing perspective. The ideas can be further simplified while explanation (the verbatim has changed a bit from the NeurIPS version but it can still be implied). The illustrations are very helpful, but the text and the language often made me feel lost (the only reason, I am comfortable right now is due to the conversation with reviewers that made me understand more). I would strongly recommend the authors revise the write-up for the next version (again).\n\nStrengths:\n\n1) Figure 1 and Figure 2 are very very clear and basically give the idea of what is being done in the paper completely.\n2) The motivation for GaP is well explained.\n3) The pseudo code and method section is clear.\n4) The idea is simple yet seems to be effective. It is sort of a greedy way (even though scheduled) to sparsify the network, where the dense portion is like dense gradient update of multiple methods in the literature (eg., RigL) but also the classical iterative hard thresholding. The idea makes sense, fits well into the greedy growing NAS literature.\n5) Parallel Gap is a nice engineering solution to make use of parallelization of GPUs and shows that the costs saved are worth it. I really like the new figure illustrating P-GaP, this helps more people understand its potential.\n6) The experimental results are good and move past the usual image classification benchmarks to object detection, 3D object part segmentation, and transformers.\n7) The ablations on the # partitions are interesting.\n8) The related work did improve from the previous version, I appreciate the authors for taking time and reviewing more papers to put things in perspective. \n9) I am extremely happy to see the training costs as part of the tables. This was very much lacking in the previous revisions and puts things into perspective. The cost of training is a trade-off in this case which is at least presented to the readers. I have some issues with the costs involved which I will mention in the weaknesses, but this is very promising. \n\nI am not an expert on theory so I will defer to other reviewers on that aspect. However, I don't think theory is needed to justify this paper, if this is a theory for the sake of theory, I would suggest moving it to the appendix and use the space to make compelling cases and use cases for the algorithm itself. \n\nWeaknesses:\n1) The major concern, similar to last time, is the computational costs involved. Is the trade-off worth it? I can see this paper as just pushing the limits on the inference end, getting the best models that are sparse, but the gain of 2% with say 800 additional epochs is something I am not sure how to weigh.\n2) Table 1 is not informative at all, and space could be used for something else. This is a very subjective table, I have no idea what better or good means here. I don't think it is worthy of the space. \n3) The non-uniform sparsity scheme for GaP is missing (again). This was explained to be done using global sparsity (in the rebuttal), but it is not present in the paper. \n4) While I agree FLOPs definition in this paper is correct, taking a line to explain why previous papers were reporting it the incorrect way would be great. \n\nOverall, the paper makes sense and has improved to an extent from the previous version as I mentioned above. However, I would like to have a discussion with the authors about the questions I mentioned above. I was also promised a \"limitations and societal impact\" section trying to explain the trade-offs and issues which was not delivered. I am looking forward to it. \n\nI think the paper is publishable but can be made stronger after the rebuttal.\n",
            "summary_of_the_review": "I have reviewed this paper for NeurIPS 2021 and gave it a score of 6. But the paper had a bunch of issues making it not ready for publication. \n\nThis time, the authors tried to put things in perspective. The empirical performance of the method is strong but I would like to see the limitations of the method along with a discussion of trade-offs. \n\nI am giving it a score of 6 but will be willing to increase it to 8 if the authors respond back with valid explanations for weaknesses. \n\n---------------------------------------------\nAfter rebuttal:\n\nI am convinced about the contributions of the paper and I would be arguing for the acceptance of the same during the internal discussions. Increasing score to an 8.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}