{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a policy gradient algorithm based on the Bregman divergence and momentum method. While one reviewer was initially concerned about the technical novelty of the paper given some existing works, after the author's response and paper revision, the reviewers are all convinced and have reached a consensus to accept this paper. Thus I recommend acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work proposes a Bregman gradient policy optimization framework for RL. Two specific algorithms are proposed, which are BGPO and VR-BGPO, where VR-BGPO is an accelerated version of BGPO. The authors provide the convergence rates results for these two algorithms and show their efficiency through multiple numerical simulations.",
            "main_review": "In general, the paper is well written with some minor grammar issues. The developed BGPO and VR-BGPO are simple and easy to implement. The derived convergence results match the state-of-the-art one. Numerical results illustrate the effectiveness of the proposed algorithms. However, we still have some concerns as follows:\n\n1) My understanding is that the proposed algorithm is pretty general. Why does the work only focus on policy gradient rather than general gradient? What is the unique property that has been used here to guarantee theoretical results compared to the traditional stochastic gradient methods?\n\n2) Why do the compared algorithms shown in Table 3 need a large batch size (i.e., $\\epsilon$ dependent) while the proposed one only requires constant batch size?\n\n3) I agree that VR-BGPO is better than BGPO. But BGPO is not simulated in Figure 3?  \n\nSome other minor issues:\n\nPGT is not explicitly definied.",
            "summary_of_the_review": "Overall, I believe that there are some merits of this work regarding the simplicity of the implementation of the algorithms, theoretical justification of the convergence rates, and numerical instances of verifying the efficiency of BGPO and VR-BGPO.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "na",
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the convergence of policy gradient algorithms with constraints. They modify the vanilla policy gradient with Bregman divergence as a regularizer. The authors also propose a new variance reduced policy gradient methods based on the STORM estimator in nonconvex optimization. \n\n\n",
            "main_review": "My concern about this paper mainly lies in its lack of novelty both in the algorithm design and the theoretical analysis. First of all, mirro policy gradient with Bregman divergence regularizer was already proposed in other papers. For example, the algorithm proposed in the current work is almost the same as that in the following paper:\nYang L, Zheng G, Zhang H, Zhang Y, Zheng Q, Wen J, Pan G. Policy optimization with stochastic mirror descent. arXiv preprint arXiv:1906.10462. 2019 Jun 25.\n\nThe difference of the current paper with the above one lies in the variance reduction techniques. Yang et al uses the SPIDER estimator, while the current paper uses the STORM estimator proposed in Cutkosky and Orabona (2019). Moreover, even the adaptation of STORM from nonconvex optimization to policy optimization was already studied in a recent paper: Feihu Huang, Shangqian Gao, Jian Pei, and Heng Huang. Momentum-based policy gradient methods (2020). Therefore, combining these methods with a mirror descent update do not seem to have enough contributions for the publishment of this paper. \n\n\n==after the author's response==  \nI thank the authors for their detailed response on the differences of the submission from closely related work. Given these comments and discussions, I am willing to raise my score to accept this paper.",
            "summary_of_the_review": "This paper has limited novelty in both algorithm design and theoretical analysis. It is also highly similar to existing papers from several perspectives.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors consider the optimization problem of an MDP. They designed two policy gradient algorithms based on the mirror descent method, which are named BGPO and VR-BGPO. The BGPO algorithm is a momentum mirror descent method that finds an $\\epsilon$-stationary policy with $O(\\epsilon^{-4})$ samples. The VR-BGPO algorithm is a STORM-type variance reduced mirror descent method that finds an $\\epsilon$-stationary policy with $O(\\epsilon^{-3})$ samples. The analysis is nicely organized and the authors also provide a couple of experiments to verify their theoretical findings.  ",
            "main_review": "The paper discusses the momentum and STORM version of the mirror descent PG method, which is a relatively new result. The analysis and the presentation of the results are clear and well-organized. Although the result can be expected since both STORM and mirror descent, as well as their numerous variants, are well-studied, the discovery of this paper does imply the convergence of several important special cases of the PG method, including the NPG and Super-Adam version of PG. So I think this is a good paper. ",
            "summary_of_the_review": "The main comments are provided in the main review part. Here I will add a few comments. \n\n1. The authors have used \"mirror descend\" several times in the paper, please unify terminology and use \"mirror descent\".\n\n2. Because the second algorithm proposed by the author is a STORM type variance reduced mirror descent PG method, the authors should also mention the work by Yuhao Ding Junzi Zhang & Javad Lavaei: \\emph{On the Global Convergence of Momentum-based Policy Gradient}. The algorithms of this paper also apply the STORM technique.  For the same reason, the following work by Nhan H. Pham et al. \\emph{A Hybrid Proximal Stochastic Policy Gradient Algorithm} should also be cited. \n\n3. Regarding equation (4), the author should mention that the objective function with a horizon $H$, there is a truncation error of $\\frac{\\gamma^H}{1-\\gamma}$ compared to the original infinite-horizon MDP. \n\n4. Regarding Assumption 2 and Assumption 4, the author should mention that both of them are satisfied automatically given Assumption 1 and the fact that all the rewards $r(s,a)$ are bounded. \n\n\n5. Regarding Assumption 3, the authors should also mention that the importance weights can be bounded theoretically (instead of an uncheckable assumption) by using the truncated gradient step instead of a gradient step. See the work of Junyu Zhang et al. \\emph{On the convergence and sample efficiency of variance-reduced policy gradient method}. The importance weights actually as some nice properties,   for soft-max policy it is bounded by $e^{c||\\theta_1-\\theta_2||}$, in Gaussian policy it is bounded as $e^{c||\\theta_1-\\theta_2||^2}$. So a simple truncated update is enough to control this value. \n\n6. There are still a few typos/gramma errors in the paper, for example, in the first row of contribution (a), the \"based the\" should be \"based on the\". The authors should carefully check the spelling and gramma in the revision. \n\n7. Regarding the experiments, the authors compared with several different algorithms. However, there is a small issue w.r.t. the selected algorithms. \n\nVR-BGPO: variance reduction (VR) + mirror descent (MD)\n\nVR-MDPO: VR + MD\n\nMDPO: VR only\n\nTPO & PPO: no VR, no MD\n\nFrom these comparisons, I'm not able to see if the MD really works. The authors should also compare with algorithms such as Prox-HSPGA, (A hybrid stochastic policy gradient algorithm for reinforcement learning) which applies STORM technique while not using mirror descent. \n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics issue.",
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}