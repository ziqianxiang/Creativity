{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper tackles the problem of how to utilize a network from the source domain to benefit target domain training in terms of sample/training efficiency. In contrast to prior methods (e.g. that perform fine-tuning or distillation), this paper poses it as a bandit problem that decides how to wire intermediate representations of the source model into the target model as well as what aggregation function to use. An alternating/mixed discrete-continuous optimization is proposed to perform this decision-making, and results are shown across a mix of source-target pairs and network architectures. \n\n  The reviewers overall found the method interesting and paper topic both interesting and extremely practically useful, presenting an opportunity to save significant energy, compute, and labeling requirements when training on target domains. The results also show very significant improvements, on the conditions tried. However, a number of concerns were raised including comparison to simple same-architecture fine-tuning (3TiT), comparison benchmarks e.g. VTAB and newer methods in the area (u325, 7soo, 3TiT), need for the adversarial bandit formulation (3TiT, 7soo, 8d9w), and the added storage/inference costs required (3TiT). \n\n  Based on these reviews, the authors provided a thorough rebuttal, additional baselines and experiments demonstrating the efficacy of the method (especially the full version) over both reasonable simple baselines (same architecture fine-tuning) as well as simpler versions (fixed aggregation), and time/inference-time matched performances. Importantly, the advantage of the full method comes out a lot more in the new experiments. Overall, through the rebuttal process the paper has been made much stronger. \n\n  Given that the paper provides a nice principled approach to the problem and now has strong compelling results, I recommend acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes an algorithm to transfer a \"source\" pre-trained deep model into a new one for a given target task. The idea is to wire in a certain way the intermediate representations of the source model into those of the \"target\" one. The algorithm considers a (small) discrete space of potential such ways to connect both models (location, source processing, and type of merging), and applies a bandit algorithm to sequentially spend training budget under one of the configurations. Experiments are provided that show big gains with respect to standard approaches (like finetuning the source model).",
            "main_review": "Transfer learning is becoming an important tool for applied machine learning. On one hand, it can help to save lots of energy and compute by training once and benefitting many times from it. On the other, it can unlock the use of small datasets that, in isolation, are hard to crack. Accordingly, any development in this front can have significant consequences in the practical use and adoption of modern deep learning (or ML, more broadly). So far, the golden standard is to finetune the model on the new dataset (maybe after distilling to the target architecture if needed, and possibly by applying some additional regularization to avoid overfitting or diverging too much from the original model).\n\nThis paper proposes an alternative. There are many ways to use the source predictions and representations in a potentially useful downstream manner. Rather than choosing one, and just committing to it in all cases (e.g., full finetuning), they propose an adaptive mechanism that operates in two dimensions. First, it essentially considers a set of architectures (one for each potential source-target plugging). Second, for a given wiring, it trains part of it (all except the original source network which remains frozen) on the downstream data. The former is a discrete optimization problem, the latter is a continuous one. A bandit algorithm (Algorithm 1) and gradient descent (Algorithm 2) are used respectively, in an alternating fashion. \n\nIn my opinion, the idea is quite natural. There are some subtleties, though. Algorithm 2 changes the \"environment\" by training, so it's a non-stationary problem. Also, rather than considering the joint (or say, Cartesian product) action space, the algorithm solves one bandit problem per target layer as an independent instance. The reward designed by the algorithm (given in Algorithm 3, line 2) assumes no \"decision\" is applied in the other bandits problems, and tries to find the best action for the layer while assuming there won't be any wiring in the remaining target layers (if I understand correctly).\n\nI find the experimental section to be a bit limited.\n\nFirst, the simplest baseline I can think of (source and target identical architectures, and finetune source into target) is not there: ResNet X to ResNet X. There's a comparison (\"Scratch (finetune)\") where a smaller source network (RN18) is used to help the target (RN18), but still compared with Auto-Transfer on a RN36 source network, so not exactly apples to apples. It would be extremely informative to see this comparison as I think it is the cleanest one. Moreover, the direct finetuning should be much faster; can we also see a time-matched comparison (i.e. train for the same amount of time the RN-X to RN-X finetune transfer and the Auto-Transfer approach)? I'll raise my score if this is added (and given the numbers in Table 1, I'd be confident this should still work well for Auto-Transfer).\n\nThe numbers in Table 1 are really impressive: Auto-Transfer seems to offer massive gains. There's only 4 datasets though, considering something like VTAB [1] could improve the work (it contains a large number of diverse downstream vision tasks). Also, it'd be interesting to see a more detailed ablation to understand which aspects of the algorithm really explain the gains. Is the bandit algorithm fundamentally important? Can we use a simple UCB? Or even an epsilon greedy approach based on the average reward? Or is it mostly about the action set --some of this is shown in the full/fixed/route comparison? For example, rather than applying the (somewhat complex) Auto-Transfer algorithm every time we want to transfer, a more desirable outcome would be to discover specific combinations (i.e. actions in the action set) --that is, transfer design principles-- that work well across many different setups. Maybe we can look at the finally selected ones for the datasets in Table 1, find a simple common pattern (i.e. one choice of the action set), and train with this fixed choice, and see how it does? In those cases, maybe rather than the algorithm, the contribution could also include some high level wiring tips (this may seem at first less appealing as it's tailored towards one specific architecture: resnet, transformer, etc) but it could have a much bigger impact.\n\nA potential explanation to beating simple finetuning may have to do with the intrinsic memory that this algorithm provides. In some sense, by paying in terms of inference cost and storage, we can't forget the original representations. The final models have very different size/dimension/power (if you keep the source model or not around). \n\nThis leads to another big question: inference. Regardless of the cost and complexity of finetuning, the proposed approach requires that during inference we'll need to apply both networks (source and target). Moreover, in general sequential processing is needed: you have to run source first, as its last layer representation could be wired to the first target layer one. Can you provide results where the inference time is matched (ignoring complexity)? In other words, compare a larger model directly finetuned to, with two smaller source and target networks trained via Auto-Transfer, so that total inference flops or runtime in both cases are identical. At the end of the day, these considerations probably matter a lot in real practical systems. Looking at Figure 2, I could be optimistic about these cost-matched comparisons, if one model is able to unlock much higher performance values, then it seems no matter how much time you devote to the suboptimal one, it may not be able to catch up.\n\n\n[1] = A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark",
            "summary_of_the_review": "The paper proposes a sequential decision making idea to do transfer learning using and keeping two networks: pre-trained source and from-scratch target. The results that are presented suggest Auto-Transfer offers large gains, but the scope of the experimental section is a bit limited.\n\n--------------\n\nAfter reading the detailed reply by the authors, I decided to raise my score: 5 --> 6.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work addresses the challenge of automatic knowledge transfer between different networks. The authors propose to use an adversarial  multi-armed bandit to decide where, what and how to combine outputs from different layers of the two networks, improving on the recent line of work that achieves transfer by enforcing closeness between the representations obtained by the two networks.\n\nFor each layer, the proposed method introduces some additional parameters in the form of intermediate representations and parametrized ways to combine such intermediate representations with the source network's output.  The multi-armed bandit is trained to choose which intermediate representation and which aggregating function to use. The reward is determined based on a hold out set. The authors compare the proposed approach with some previous work, showing improvements, and perform a qualitative analysis of the results.",
            "main_review": "Strengths:\n* I like the principled approach to letting the model figure out what information to transfer. The additional inductive bias of constraining the information flow could be useful to directly inspect what information is being transferred between the networks. \n\nWeaknesses:\n* Evaluation setup: is there a standard benchmark for this task? And more recent baselines? If your interest in this setup is driven by addressing scenarios with limited amounts of labels, more careful evaluation of this scenario would be meaningful to add.\n* Ablation studies: it would be interesting to investigate deeper the effect of choosing adversarial bandits in this context. The Appendix reports an ablation study with a fixed routing choice, but other baselines could be interesting too. What's the effect of the frequency of updates of the bandit? How do different parts of the proposed method impact the overall effectiveness? How does the choice of number of representations influence the performance? Are these learned intermediate representations meaningful?\n* The authors mention the ability to decide whether to overwrite the target's network own information as an advantage of their method. Are there ways to test this hypothesis? It seems like giving the option to let the network choose the aggregation operation resulted in always the same choice, \n\nMinor:\n* Some experimental details are missing, for example how many random initializations where tried to compute the variance over the reported accuracy?\n* Time improvements: how do the results depend on the complexity of the routing problem?\n",
            "summary_of_the_review": "While the authors address an important problem with an interesting technique, a more principled evaluation would be necessary for this work to be complete. The task should be better contextualized within the existing literature, and the proposed method evaluated more throughly. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "To transfer knowledge between heterogeneous source and target networks and tasks, this paper proposes a novel adversarial multi-armed bandit approach (AMAB) which automatically learns to route source representations to appropriate target representations. It combines feature representations received from the source network with the target network-generated feature representations via various aggregation operations. The work is interesting and makes sense to some extent.\n",
            "main_review": "\n1. The method may be not novelty enough. It tries to introduce reinforcement learning into transfer learning to route transferable representations. The reinforcement learning part utilizes a traditional bandit-based method while routing choices are common choices that have been used in existing work widely.\n2. It is better to add more latest state-of-the-art comparison methods. This paper only compares with methods used in L2T-ww. However, L2T-ww is a paper proposed in 2019, which cannot represent the best results now. Some more work, such as SNOW[1], has been proposed for knowledge transfer. \n3. Since the paper is similar to L2T-ww, a method proposed in 2019, it is better to compare all results showed in L2T-ww. And more new benchmarks are needed.\n\nOverall, I recommend rejecting the current version of the paper.\n\n[1] Yoo C, Kang B, Cho M. SNOW: Subscribing to Knowledge via Channel Pooling for Transfer & Lifelong Learning of Convolutional Neural Networks[C]//International Conference on Learning Representations. 2020.",
            "summary_of_the_review": "See main review.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper presents a new approach to perform transfer learning between heterogeneous architectures and tasks by introducing a new algorithm based on adversarial multi-armed bandit, which automatically learns a mapping between source and target representations, as well as the way to combine such representations. The approach is evaluated in multiple vision tasks, starting from models pre-trained on ImageNet, producing satisfactory results when compared to the state-of-the-art.",
            "main_review": "Pros: \n\n- The proposed approach does a good job at transferring representations between heterogeneous CNNs, showing good results in several image classification datasets and improving upon state-of-the-art solutions.\n\n- The algorithm is relatively easy to implement and results should be reproducible. Code is provided.\n \n##########################################################################\nCons: \n \n \n- There seems to be a disconnect between the general idea presented in the paper and the “flavor” of the approach that yields the best results. In practice, the idea of using multi-arm bandits to learn features mapping along with a set of transformation and combination operators seems to be reduced to learning the feature mapping fixing those operators. If this is the case, the narrative of the paper should reflect the finding (from the beginning of the manuscript). If this is not the case, empirical evidence should support it.\nThis observation affects the consistency of the paper and triggers multiple questions that need to be addressed during the rebuttal period.\n\n##########################################################################\nQuestions during rebuttal period: \nWhy did authors choose a normalized linear operator for the transformation of the source features (i.e., Tj = BN(Conv(·)) )? Were other operators analyzed and discarded? Is the idea just to adjust channel dimensions and address the covariant shift? \t\n\t\t\t\nFrom table 1, it seems that fixing the combination operator to be a weighted addition and the aggregator operator to be a bilinear interpolation yields the best results. The “full” configuration, in which the multi-arm bandits’ algorithm has to select these operators comes in second place. Shouldn’t this observation deeply affect the original hypothesis of this work as well as the global structure of the paper?  \n\nAs a follow-up to the previous question, could the authors provide additional details on the “fixed configuration”? From the text, it is clear that the feature maps are done manually, but is the combination operator set to a weighted addition, and is the aggregator operator set to a bilinear interpolation? After observing how the \n“route configuration” outperforms the “full configuration” I wonder if the “fixed configuration” could be further improved by fixing those operators to a weighted addition and a bilinear interpolation. Could authors please report a configuration with these details? \t\t \t ",
            "summary_of_the_review": "I am currently indecisive, leaning towards rejection. The paper presents a nice narrative about the adversarial multi-armed bandit approach, highlighting the benefits of the routing of representations in combination with the dynamic selection of operators to combine and transform features. However, experiments seem to hint at a different reality, where fixing these operators is indeed more beneficial.  Based on this, I consider that authors need to present stronger evidence that supports the value of the approach, as it is currently presented in the paper, or they need to update the narrative of the paper to account for the empirical findings.\n \nPlease, consider my questions for rebuttal.\n\nPost-rebuttal review:\n\nAfter the evidence provided by the authors, I have decided to increase the valuation to a 6: marginally accepted. The new tables are able to defend the value of the main idea, but still, there are some doubts about the potential of \"full\" vs. \"route\" in different problems/datasets. This is the main reason for not assigning a higher score.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "details_of_ethics_concerns": "As described by the authors, it is important to be alert about the possible biases transferred from the source domain/model into future target models. This is probably true for every single domain transfer method. I believe this is not a blocker for publication, and authors clearly reflect this in their manuscripts, still, it is good to keep it in mind.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}