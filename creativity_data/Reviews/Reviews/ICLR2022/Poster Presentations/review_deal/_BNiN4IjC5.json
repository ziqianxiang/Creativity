{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper suggests using a conditional prior in conditional diffusion-based generative models. Typically, only the score function estimator is provided with the conditioning signal, and the prior is an unconditional standard Gaussian distribution. It is shown that making the prior conditional improves results on speech generation tasks.\n\nSeveral reviewers initially recommended rejection, but after extensive discussion and interaction with the authors, all reviewers have given this work a \"borderline accept\" rating. \n\nCriticisms included that the idea is too simple or obvious to warrant an ICLR paper. I am inclined to disagree: simple ideas that work are often the ones that persist and see rapid adoption (dropout regularisation is my favourite example). Like the authors, I believe simplicity is an advantage in this respect, rather than a disadvantage. Of course, simple ideas do require extensive and convincing empirical validation to be worth publishing at ICLR. After the authors' updates, I believe the work meets this bar.\n\nAnother issue raised by several reviewers is the limited theoretical justification for the approach. However, combined with the simplicity of the method, I believe the empirical results of the revised version sufficiently justify the approach on their own. Nevertheless, I would recommend that the authors consider further how they could address this issue in the final version of their manuscript, as they have already begun to do during the discussion phase.\n\nAnother way to strengthen the paper further would be to demonstrate how the generic approach can be applied in a different domain (e.g. conditional image generation), but I do not consider this addition necessary for the work to warrant publication.\n\nIn light of this, I am recommending acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper authors propose to use informative prior for conditional diffusion model when applied to neural  vocoding task.  ",
            "main_review": "\nMost of the theory explanations focus on known \\mu and \\Sigma, but I see very little about how these parameters are estimated and pros and cons about  different estimation techniques. \n\nRelating to point above, can you clarify where is the \\mu coming from in case? Noting that original paper had prior as standar normal, you have \\mu also. I understand that noise processis zero-mean, but \\mu appears in your ELBO. Can you clarify this point. \n\nIn 4.1: Why non-clipping variance causes instability in training? Is there a theoretical explanation to it or is it only empirical observation? Relating to  the accompanied footnote, did you try to see if different corpora would have different clipping threshold? \n\nIn the Table 1, is GT a the clean, i.e. non-vocoded sample? At least it is supposed to be. And in the same vein, why is 500k training steps better for the proposed than 1M training steps? Can you check statistical significance of this, as if result is significant, then there should be a reason for it. \n\nIntro: 2nd para: \"procedurally destroys signal into\" ??\nIn Algorithm 2, line after If-statement seems to have wrong index x_{t-1} to x_t. \nNotation seems to be a bit mismatched in style. I would like to see consistent style, for example all vectors to be bolded non-caps and all matrices to be bolded and capitalized. Then it is easier for the reader to follow the story. \n\nReview has been updated based on author rebuttal and discussion. ",
            "summary_of_the_review": "Paper provides quite nice theoretical justifications for their decision to use informative prior instead of classic non-informative one. However, I feel that modification itself is too simple to warrant an ICLR paper. I hope authors can provide a nice rebuttal to this point in assessing the level of contribution of this paper. \n\nThe other point that lowers my score is that the final selection of noise variance is based on ad-hoc technique and not justified theoretically, whereas other parts of the paper are.  ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents an extension to conditional denoising diffusion models for text-to-speech (TTS). The initial noise in the inference procedure is not sampled from a standard Gaussian but from a data-dependent conditional non-standard Gaussian, which is called PriorGrad. The motivation is to increase inference speed and training convergence. This method is applied to DiffWave used as a vocoder, and also applied to DiffWave used as an acoustic model (phonemes to mel spect).\n",
            "main_review": "Strengths:\n- Novel idea to change the initial sampling distribution to improve inference speed and training convergence.\n- Tested for both vocoder and acoustic model.\n\nWeaknesses:\n\n- Mathematical justification of such prior is lacking.\n- Same related work is missing.\n- Experimental evaluation lacks comparison to alternative models, both for vocoder and acoustic model.\n- Baseline acoustic model is non-standard, which makes it even more relevant to compare to other alternative models.\n- Acoustic model comparison uses non-optimal vocoder (PWG).\n- No discussion and numbers on the inference speed and real-time-factor (RTF), neither for vocoder or acoustic model.\n- Code not published.\n\nDetails:\n\nIs prior grad mathematically motivated?\n\"The (DDPM) framework assumes the prior noise as a standard Gaussian distribution\"\nNo, it is not an assumption. It is a mathematical consequence of the definitions of the framework.\n\nMost of the math theory in the paper just shows that convergence rate and training behavior might be better for the proposed method. But it does not give a justification why the data-dependent prior is mathematical sound within the framework.\n\nRelated work section seems good, at least related diffusion (or grad-based) models, related prior modeling.It misses many other state-of-the-art vocoder models (MelGAN, UnivNet).\n\nGAN-based vocoders are currently among the best vocoders, such as:\n\n- MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis (https://arxiv.org/abs/1910.06711)- Multi-band MelGAN: Faster Waveform Generation for High-Quality Text-to-Speech (https://arxiv.org/abs/2005.05106): MelGAN 3.87 MOS, proposed MB-MelGAN 4.22 MOS, Recording 4.58 MOS.\n- HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis (https://arxiv.org/abs/2010.05646): proposed HiFi-GAN V1 4.36 MOS, Recording 4.45 MOS, MelGAN 3.79 MOS, WaveGlow 3.81 MOS, WaveNet (MoL) 4.02 MOS.\n- UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation (https://arxiv.org/abs/2106.07889): MelGAN: 3.56 MOS, Parallel WaveGAN 3.07 MOS, HiFi-GAN 3.89 MOS, proposed UnivNet-c32 3.93 MOS, Recordings 4.16 MOS.\n\nThose are not compared here.\n\nThen there is WaveGlow, WaveFlow and similar vocoder models which are also not compared.\n\nSo experimental validation on vocoder experiments is weak.\n\nExperimental validation on acoustic model is weak as well. Custom-implemented baseline with non-optimal vocoder (PWG). Why not test against other acoustic models such as Diff-TTS or Grad-TTS or FastSpeech 2? Why not use a better vocoder?\n\nVocoder experiments: LJSpeech. PriorGrad based on DiffWave. T=50 steps for training and inference, for both DiffWave and PriorGrad.\nIt would be interesting to study the influence of T on both models. Maybe DiffWave with T=100 compensates the effect. Or PriorGrad with T=25. Just some variation on T for both models. You can argue that using the conditional data-dependent prior is an unfair comparison when using the same T.\n\nAcoustic model (TTS) experiment with fixed vocoder:\nTable 4 should make more clear what \"baseline\" is. From the text, it seems to be DiffWave, just as before.\nThis baseline is also some custom implementation.\nWhy not use existing implementation, such as Grad-TTS?\n\nSpeed (RTF) not evaluated? Not even discussed?\n\nCode?\n",
            "summary_of_the_review": "The basic idea of using a better initial prior distribution to improve inference speed and training convergence is nice, although it lacks some mathematical justification. The experimental section is lacking too much.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work builds on denoising diffusion probabilistic models (DDPM), and argues for to modify the forward and backward diffusion processes such that instead of using an uninformative prior $p(\\mathbf{x}_T) = \\mathcal{N}(0,\\mathbf{I})$, they use a data-dependent prior $p(\\mathbf{x}_T) = \\mathcal{N}(\\mu, \\Sigma)$, where the mean and covariance are derived from training data, in a \"pre-processing\" step.\n\nThe authors show that, under restrictive conditions, the ELBO obtained with the proposed prior is smaller than that obtained with the uninformative prior. Under similar restrictive conditions, the authors argue that convergence rate of the parameter optimization is better conditioned, and as a consequence faster.\n\nThis work illustrates the benefits of the proposed method using two applications, one to a vocoder, and one to an acoustic model. Experiments rely on several very recent related work, especially concerning technical details of the architectures, and data pre-processing choices. Overall, two methods are compared, a baseline DDPM and the proposed DDPM that uses training data to build a better prior.",
            "main_review": "This paper is well written and the key idea is described in a simple way. The main motivation stems from the question whether it is possible to induce diffusion probabilistic models (DDPM) to use a more informative prior, rather than the typical isotropic unit Gaussian, *without* additional computational or parameter complexity.\n\nAs a side comment, I would like to clarify that while it is true that the DDPM as formulated in this work bears no overheads, the task of estimating mean and covariances of the data (especially for \"complex\" data such as the one used in the experiments) comes with its own cost and design decisions, which should not underestimated.\n\nThe background section is typical for a DDPM paper (using the discrete-time Markov chain formulation). Although this is nitpicking, I think some clarifications are in order: the ELBO from eq (3), which is derived as in Ho et. al. 2020, is studied as three separate components, and the focus is on the second one, which is labeled $L_{t-1}$ in Ho et al. 2020. Then, eq (6) is not the ELBO, but only the $L_{t-1}$ term.\nAlso, since this work argues for taking into account an informative prior where $\\mu$ and $\\Sigma$ are instrumental, I would argue that the constant $C$ that appears in eq (6) should be developed, as there are terms that depend on it. This could help the reader gain a better understanding of the proposed method (and the limitations of proposition 1 and 2).\n\nThe presented method is discussed in an intuitive manner at first, using Fig 2. Nevertheless, after some thoughts, I think this figure can be misleading (even if it is just an illustration). In a two-dimensional toy example, $p(\\mathbf{x}_T) = \\mathcal{N}(\\mu, \\Sigma)$ could be \"closer\" to the true data distribution, actually, on top of it. This is not necessarily true for high-dimensional spaces though.\nMore to the point, since you \"simulate\" the transition from $\\mathbf{x}_0$ to $\\mathbf{x}_T$ in the forward process for a finite $T$, then the process does not end up in $ \\mathcal{N}(0,\\mathbf{I})$ (for the un-informative baseline) nor in $ \\mathcal{N}(\\mu, \\Sigma)$ (for the data-dependent prior).\nThe backward process starts from the prior, and again, since it is \"simulated\" for a finite amount of time $T$, it will not reach $\\mathbf{x}_0$.\nThis kind of illustration could be helpful to better understand the claim of proposition 2.\n\nAlso, note that in proposition 1, the quantity under study is not the ELBO, but $L_{t-1}$. This is also where the constant $C$ is \"hiding\" a dependence on $\\mu$ and $\\Sigma$.\n\nProposition 2 states that the use of a more informative prior is better in terms of a tighter bound. This seems intuitive at first, as there are more degrees of freedom than using zero mean and unit covariance. However, the proposition holds when the function that learns the denoising effect of the backward process is linear, which is not true in practice. Do you have any intuition if the proposition holds also in this more general case? Are there any other arguments that can be used to show that indeed the ELBO is smaller?\n\nThe observation that the Hessian of the $L_{t-1}$ term is better conditioned when using an informative prior w.r.t. the case of a unit Gaussian prior is also based on the assumption of linearity of the denoising function. However, results in Fig 4 do not show that the rate of convergence is improved in a practical endeavor. There, we see that the considered error metric is better for the informative prior case, but the rates of the two curves appear to be similar, just shifted. Do you have any comments on this effect?\n\nAlthough the application domain used to evaluate the proposed method falls outside my competence, I appreciate the efforts and the timeliness of the architectures and design choices made in the execution of the experiments, which I find somehow compelling to corroborate that the paper presents a valid idea. That being said, the evaluation only compares the non-informative prior baseline against the informative variant. Wouldn't it be interesting to compare additional methods from the state of the art, which the authors appear to know very well? I particularly liked the idea of studying \"parameter efficiency\", and I wonder whether the efficiency of an informative prior carries on when comparing to different approaches.\n\nTo conclude my comments, I think this paper introduces a relevant idea which shows potential in applications. I am less excited about the methodological sections of this paper, which are not strong and precise enough to be convincing.\n",
            "summary_of_the_review": "Interesting and relevant idea, to improve the behavior of diffusion models. Experimental results indicate the idea has a positive effect on metrics used in specific application domains, including training performance and parameter efficiency.\nHowever, the theoretical justification of the proposed method is somehow weak, and does not provide sufficiently compelling arguments on the observed behavior of the proposed method in practice.\n\n\n*** Post-rebuttal / discussions remarks ***\n\nThanks for the useful discussions and for the additional work to improve the paper. I have increased my ranking for the paper from 5 to 6.\nThe main reason why I didn't rank the paper higher is that the theoretical contribution of this work, despite being very promising, is still not mature and sufficiently mathematically grounded.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to use a data-dependent, adaptive prior for the noise used in conditional DDPMs. In particular, it proposes to move from the standard to the non-standard Gaussian prior. For that, the ELBO loss is reformulated by taking into account adaptive means and variances and the sampling procedure is also adapted. Statistics (directly or indirectly derived) from conditioning signals are used to compute time-wise and instance-based means/standard deviations. An empirical evaluation is conducted on two conditional audio generation tasks: vocoding and acoustic modeling, showing some favorable results.",
            "main_review": "Strengths:\n- Time-wise and instance-based means and variances provide additional information that models are able to exploit, yielding improved results in terms of convergence speed and generation quality.\n- Results are reported for both objective and subjective assessments. Baselines are clear and appropriate.\n- As far as I can judge, I think the paper is technically correct.\n- Well-written paper, except from some minor typos (see below).\n\nWeaknesses:\n- Obvious adaptation from standard to non-standard Gaussian prior.\n- Potential lack of generality to other conditional audio tasks or other domains such as image.\n- Non-trivial and case-specific methodology to compute means and variances for the non-standard Gaussian priors.\n- I don't think one of the three main claims is well supported by the experiments (see below).\n\nExtended review:\n1. I have the impression that the title, parts of the text, and even the abstract, try to encompass a general application of the methodology. However, results are only provided for two specific speech processing tasks. The title, for instance, does not even mention speech, and the abstract mentions audio when only speech is considered. While in principle the proposed methodology could be applied to other tasks and domains, the benefit of it is not shown for those.\n1. The procedure to compute instance- and time-specific means and variances for the prior seems non-straightforward and highly dependent on the type of conditioning signal, further complicating the generalization of the approach. The authors even state that with other conditioning signals for the considered task (V/U segments and phoneme-level statistics) they did not obtain any improvements. This also raises some doubts beyond generalization. In particular, regarding how hard is to adapt the approach to other tasks/domains or even conditioning signals, or how limited is the proposed idea. Even in the acoustic modeling setting the approach needs to rely on mel band energies (leaving the reader suspicious as if this is the only information that works).\n1. There is some unclear formulation of the approach. Expanding the last point to focus more on the considered case: How is frame-level energy normalized? Which is the exact formula? How do different procedures affect final performance? Why there should be a range of $(0,1]$ (later the paper mentions it is not really $(0,1]$ but clipped to some empirically-chosen value...)?\n1. There is at least concurrent work (https://arxiv.org/abs/2110.05948) investigating the effect of using other priors beyond the standard Gaussian. Perhaps contribution 1 could be softened in this regard.\n1. No details on how MOS subjective tests were performed. This can be a serious issue.\n1. In my opinion, the \"tolerance to a reduction in network capacity\" claim is not supported by the results. First of all, it is debatable whether reducing the width of convolutional layers is sound or meaningful in this setting (Sec 4.3, \"parameter efficiency\"). But, more importantly, the relative change in MOS in Table 2 does not support the claim (3.9/4.06=0.961 which is extremely close to 4.02/4.12=0.975; that is, both DiffWave and PriorGrad present almost the same relative reduction in performance (4% vs 3%). This complexity-leverage claim again does not hold with results in Table 4 when we think of relative terms.\n\nMinor considerations:\n- Talking about \"inefficiency\" in conditional speech synthesis when for instance WaveGrad shows that with only 6 steps one can achieve almost perfect accuracy is, at least, misleading.\n- \"Given the above investigation\" --> Given the previous reasoning?\n- \"our method can provide a better trajectory...\" --> Claim not shown either theoretically or empirically.\n- In Algorithm 2, it is perhaps unclear that $\\sigma_t$ corresponds to a noise source or that it includes it (the authors use $\\epsilon$ for Algorithm 1).\n- I think Figs 3 and 5 are unnecessary.\n- \"by comparing the baseline\" --> by comparing to the baseline\n- References Jeong et al and Kawar et al have no venue or arxiv or paper link.\n\n",
            "summary_of_the_review": "The paper is technically correct and shows some improvements in the empirical evaluation. However, I think that moving from standard to non-standard Gaussian priors is rather obvious. In addition, I am concerned about the generality of the approach, as it only focuses on two specific speech generation tasks and deriving appropriate means and variances from certain conditioning signals can be problematic or may not yield to an improvement. One of the three main claims is not well supported.\n\n[Update: After authors' rebuttal I'm changing my score from 5 to 6]",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}