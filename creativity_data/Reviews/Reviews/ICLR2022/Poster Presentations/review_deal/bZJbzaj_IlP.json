{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper contributes a theoretical understanding of training over-parametrized deep neural networks with rectified linear unit (ReLU) activations using gradient descent with respect to square loss in the neural tangent kernel (NTK) regime. Authors consider a non-parametric regression framework wherein the labels are generated using a ground truth function, which is assumed to be in the RKHS associated with the NTK, perturbed with noise. Authors show that gradient descent based training without early stopping fails whereas \\ell-2 regularized gradient descent achieved minimax optimal convergence rate. The paper is clearly written and the results are solid. Overall, a good paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors analyze the convergence rate of both the unregularized and the L2 regularized gradient descent for a regression problem. Under a positivity assumption of NTK,  this paper shows that without early stopping, the vanilla GD may fail. This can be solved by using L2 regularization and could achieve a better convergence rate.",
            "main_review": "Overall I think this paper is well organized and well written. The proof of this paper seems solid. However, I still have the following concerns:\n\n1.  Theorem 3.6 suggests that GD fails if the DNN is trained for either too short or too long. Could you add a comment on whether early stopping is hard to implement in this scenario? \n\n2. Does the regularization sensitive to the hyperparameter $\\mu$? In section 3.3., the authors directly choose $mu = \\Theta(n^{\\frac{d-1}{2d-1}})$. What if we choose $\\mu$ to be larger or smaller than a $log(n)$ factor?\n\n3. Typo: Theorem 3.6 this should be $O((\\frac{\\lambda_{0}\\delta}{n})^{2/3})$",
            "summary_of_the_review": "Overall I think this paper is well written, and the proof seems solid. Given Hu et al. [2021], the results of this paper are not that surprising. But the proof technique is different. I still have some concerns. So currently, I would like to suggest a rejection, but I am open to discussion and willing to change my score.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper considers fully connected DNNs with finite equal width $m$ in each layer. Gradient descent is used to minimize the unregularized or L2-norm regularized training error. For unregularized situations, this paper proves that the training error decreases to zero at a linear rate and the generalization error is bounded away from zero by some constant factor if the training process is too long or too short when there exists noise. For the L2-norm regularized situation, this paper provides an upper bound on the training error that consists of two terms, one converges linearly to 0, the other is a constant. Additionally, this paper also proves that the training dynamics of the regularized neural network can approximate the corresponding kernel ridge regression. For the generalization performance of the regularized situation, this paper proves that with enough large width, the prediction risk decreases with the number of training samples $n$ at least at the speed of $n^{-d/(2d-1)}$, which is a little bit faster than the existing results $n^{-1/2}$. Numerical results are provided to support those theoretical results.",
            "main_review": "This paper is well written and shows some novel results. Although I didn't go over all the proof, those theoretical seems correct to me. I also appreciate that the authors provide a detailed comparison with [Hu et. al, 2021] in order to highlight the contribution of this paper. The following are some detailed comments.\n\n1. The condition about $\\mu$ in Eq.~(12) seems a bit strange to me. It basically means that $\\mu$ should increase with $n$ at a certain speed, which is kind of a strong restriction on the regularized factor. It would be nice to see some explanation on why such requirement is needed and how the performance changes when such requirement is not satisfied.\n\n2. I notice that the noise considered in this paper follows i.i.d. Gaussian. I wonder whether the stated theoretical results (e.g., generalization error of unregularized case is bounded away from zero by a constant) are likely to hold for other types of noise.\n\n3. The presentation of Fig. 1(c) is not very clear. Fig. 1(c) is used to illustrate that the prediction risk goes to zero as the number of training samples $n$ increases. However, it is not clear with the x-axis being \"Epochs\". I suggest plotting a figure with the x-axis being the number of training samples $n$.",
            "summary_of_the_review": "This paper is a good paper with sufficient contribution. Some minor places could be stated more clearly.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proves a bound on the excess risk in a nonparametric regression setting, where the estimator is a deep ReLU network trained by GD with L2 regularization, and the regression function belongs to the RKHS induced by the Neural Tangent Kernel (NTK) related to a deep network.\nIn particular, this work considers presence of label noise and argues that without an explicit regularization such learning procedure is not consistent in general as the network reaches interpolating regime. The paper proposes to use a relative L2 regularization (relative to the initialization) and shows that with an appropriate tuning of parameters the procedure is consistent and yields an optimal rate for regression on RKHS. The proof is heavily based on the NTK machinery, and in particular on the \"coupling\" between predictions of the network and a ridge regressor given the NTK kernel matrix.",
            "main_review": "The paper addresses interesting and relevant topic. Namely, it is one of few papers which thinks of a neural network learning as a nonparametric problem. This is justifiable as learning is always overparameterized. The question is whether such learning procedure can recover complex regression functions in a presence of label noise, as typically considered in the nonparametric literature. To this end, the paper embraces an NTK point of view and considers regression functions which belong to a corresponding Hilbert space. The paper gives a positive answer to this question and moreover shows that this can be done with an optimal rate. The proof idea is also straightforward: It is well-known that with a right tuning, ridge regression enjoys an optimal rate and so all that is left to do is to establish that deep ReLU networks trained by GD (with regularization) \"track\" predictions of the ridge regressor: This is done in Theorem 3.8.\n\n\nThe paper can be considered as an extension of Hu et al. 2021, who considered a very similar setting albeit with a shallow network.\n\nTechnical details:\n* It is known that interpolants are not consistent in general, e.g. see [1]. It is not clear what theorem 3.6 adds to that.\n* The paper highlights that one of the main technical novelties is a control of the distance parameters travel from initialization, which is $\\\\|W^{(k)} - W^{(0)}\\\\| \\\\leq \\tilde{O}_P(1/\\\\sqrt{m})$ vs $\\\\|W^{(k)} - W^{(0)}\\\\| \\\\leq \\\\tilde{O}_P(1)$ (where $m$ is the width).\nI guess this should be $\\\\|W^{(k)} - W^{(0)}\\\\| \\\\leq \\tilde{O}_P(n/(\\\\sqrt{m} \\\\lambda_0))$ vs $\\\\|W^{(k)} - W^{(0)}\\\\| \\\\leq \\\\tilde{O}_P(n/ \\\\lambda_0)$ (sample size and the eigenvalue are relevant here)? It seems that $1/\\\\sqrt{m}$ comes from the factor $\\\\sqrt{m}$ which multiplies the neural network, this makes its way to Part 5 of the proof of Theorem 3.8 (explanation on page 3 seems to be a bit unclear about that).\nOne detail which is unclear, and would be nice if authors could clarify, how is the factor $\\\\sqrt{m}$ \"balanced-out\" in the part of the proof where you need to track the ridge regressor (second result of Theorem 3.8): It seems to be important because both predictors have to be of the same scale. Is it done by having an extremely small step size of order $1/m$? In most NTK-style proofs this factor is not present.\n* Remark 3.5. $\\lambda_0 = \\Omega(\\phi n^{-2})$ is either extremely loose, or there's a mismatch in normalization. Lower bounds on $\\lambda_0 = \\Omega(d)$ are known under certain assumptions on the input distribution. See, e.g. [4].\n* It is not very surprising that networks can indeed learn regression functions in RKHS of NTK, because such regression functions are essentially by design what networks learn in an extremely overparameterized regime. However, a somewhat more interesting detail is what kind of functions are representable there? In other words, is this RKHS small?\nThere are some works discussing this and it would be interesting if the paper could put them in perspective.\n\n\nSome recent relevant references about nonparametric regression with GD+neural networks not present in the work: [1,2,3,4].\n\n\n\n\n\n\n\n\n\n[1] M. Kohler and A. Krzyzak. Over-parametrized deep neural networks do not generalize well.arXivpreprint arXiv:1912.03925, 2019.\n\n[2] Z. Ji, J. D. Li, and M. Telgarsky. Early-stopped neural networks are consistent, NeurIPS 2021.\n\n[3] I. Kuzborskij and C. Szepesvari. Nonparametric Regression with Shallow Overparameterized Neural Networks Trained by GD with Early Stopping, COLT 2021.\n\n[4] P. L. Bartlett, A. Montanari, and A. Rakhlin. Deep learning: a statistical viewpoint. Acta Numerica, 2021.",
            "summary_of_the_review": "The paper proves a bound on the excess risk in a nonparametric regression setting, where the estimator is a deep ReLU network trained by GD with L2 regularization, and the regression function belongs to the RKHS induced by the Neural Tangent Kernel (NTK) related to a deep network. This is an incremental contribution w.r.t. Hu et al. 2021. Some technical parts necessary for the proof come from cited works. In this respect this work is also quite incremental. However, I believe that contribution points in a relevant direction of understanding neural nets from nonparametric point of view. I'd be happy to raise the score if authors clarify some details.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper made technical contribution along the recent line of theoretical work on DNN: It proves that when data is generated from a nonparametric regression model with i.i.d. Gaussian noise, training (over-parametrized, i.e., wide) ReLU DNN with a regularization term with GD approximately converges, and furthermore that the generalization error decays in order faster than O(1/n^{1/2}), where n is sample size.",
            "main_review": "Strengths: \n- Theoretical understanding of neural networks, and in particular, the understanding of multi-layer NN, is an important direction of research for advancing deep learning; so while I'm not following the latest literature, I believe this paper has merit along this ongoing line of research; specifically, the line of research leveraging the NTK (Neural Tangent Kernel, i.e., linearized) representation of a multi-layer NN\n- The paper has mainly made technically incremental improvements along existing works, but they have made clear what is their unique contribution and has made good effort in citing relevant work and making comparisons.\n\nWeaknesses:\n- While I would like to trust the technical quality of the current submission, I feel the authors can spend more time on this: \n   - perhaps the authors can summarize the intuition behind each theorem/proof;\n   - perhaps the authors should re-check correctness of their statements/results: In all of Theorem 3.4, Theorem 3.6, Theorem 3.8, and Theorem 3.9, the network width m has a dependence on itself (log m); is this a typo?\n- The practical interpretability of their result is limited: The bound on the network width seems unrealistically high (e.g., n^24, L^20); how does it compare to existing work?\n- Experiments are not very convincing: in the middle plot of Figure 1, it seems that the best un-regularized generalization prediction risk is lower than (or at least on par with) the best regularized one, and that the best result of un-regularized generalization error is achieved with very few epochs. How does this experiment support the result of Theorem 3.6?",
            "summary_of_the_review": "While the contribution of the paper seems a bit incremental and I am not a hundred percent confident about the technical quality of the paper, I think the paper works on an important topic and has merit to be accepted. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}