{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper proposes a novel curriculum learning method for RL based on the concept of boosting. The proposed method builds on the curriculum value-based RL framework and uses boosting to reuse action-values from previous tasks when solving the current task. The method is analyzed theoretically in terms of approximation accuracy and convergence. Moreover, extensive experiments demonstrate the effectiveness of the method. The reviewers acknowledged the importance of the studied problem setting and generally appreciated the results. I want to thank the authors for their detailed responses that helped in answering some of the reviewers' questions and increased their overall assessment of the paper. At the end of the discussion phase, there was a clear consensus that the paper should be accepted. The reviewers have provided detailed feedback in their reviews, and we strongly encourage the authors to incorporate this feedback when preparing a revised version of the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies a fundamental issue in curriculum design for a complex reinforcement learning problem (target task). When the target task is complex to solve with direct training on it, the learner can be trained over a sequence of tasks with increasing difficulty. Here, the tasks in the sequence vary in either the transition dynamics or reward function. The central question of the paper: developing a principled method for transferring the knowledge (Q-value function or residuals) gained in the current task of the curricula to the next task. The authors propose to use the sum of residuals trained on each task for knowledge transfer and develop a novel curriculum RL method called Boosted Curriculum Reinforcement learning. By leveraging techniques from approximate value iteration literature, they theoretically justify their choice for knowledge transfer and their algorithmic proposal. The authors also conduct interesting empirical investigations on fleshing out the importance of boosting and the curriculum components of their algorithm.",
            "main_review": "The paper makes an interesting progress/contribution towards applying reinforcement learning methods for real-world challenges (hard exploration problems). The proposed algorithm is novel and justified both theoretically and empirically. \n\nI haven’t carefully checked the theoretical analysis (Section 5) and the corresponding proofs.\n\nThe paper is overall well written and easy to follow. However, the related work discussion seems limited: missing references for the works from (i) curriculum design via environment design, and  (ii) knowledge transfer via reward shaping literature. \n\n----\n\nIn the introduction, the authors state that the existing (value-based) curriculum RL methods are constrained to use the same function approximator throughout all the tasks; this is because the trained Q-value function of the current task in the sequence is used as the initialization for the next task. However, as in [Brys et al. 2015], one can utilize potential-based reward shaping techniques (a principled approach) to transfer knowledge between tasks. In this case: (i) the next task in the sequence will receive a dense/informative reward function that will ease the learning while maintaining the policy invariance property, and (ii) also, has the flexibility of choosing different functions approximators for each task in the sequence. Thus, it is important to both conceptually and empirically compare your proposed curriculum method with the curriculum methods with knowledge transfer based on reward shaping methods (e.g., [Brys et al. 2015]). \n\n[Brys et al. 2015] Brys et al. Policy Transfer using Reward Shaping. AAMAS, 2015. \n\n----\n\nI had a few confusions related to the experiments (Section 6). Please clarify them. \n\nI am a bit confused about the methods compared (mainly FQI and B-FQI) in the experiments section:\n(i) BC-FQI is a method clear: FQI blended with Algorithm 1 (boosting + curriculum)\n(ii) C-FQI: FQI trained over the curricula of tasks; here, the knowledge transferred between tasks is the trained Q-value function (as in existing value-based curriculum RL methods). Is this right?\n(iii) FQI: trained only on the data from the target task or random mixture of data from all the tasks? The FQI (red color) curve in Figure 1 only corresponds to convergence/performance of FQI on the target task T3; or it is region-wise segmented for T1, T2, T3?\n(iv) B-FQI: same confusion as in FQI. If it is trained on the target task only, what is the boosting component here? \n\nA clear description of each method (or even pseudocode in the appendix) compared would be helpful for the reader. \n\nIn summary, \n1/ What is the difference between the curriculum methods? Are the non-curriculum methods only trained on the target task or they are also trained on the same pool of tasks as curriculum methods but presented in a random ordering? \n\n2/ If the non-curriculum methods are trained only on the single target task, what is the role of boosting here, i.e., how is the sum of residuals of different tasks applicable here? \n\n3/ “The boosted methods that do not use a curriculum will introduce the additional approximators at the same iteration at which the curricula switches between tasks.” -- this statement is not clear to me. Please clarify. \n\n----\n\nA question/suggestion/comment regarding the Maze experiment:\n“It obtains a dense reward based on the distance to the target ...”\nIsn’t a dense reward already ease the learning process of the RL agent? Of course, here, the size of the two walls will alter the hardness of the environment (by changing the transition dynamics). Still, considering the target task with a sparse reward that obtains a reward only when it reaches the goal would make it a complex problem to solve. Have you tested the sequence of tasks with such sparse rewards? \n\nThe hardness of the sequence of tasks can be varied by: (i) fixing the reward function, and varying the transition dynamics of each task, (ii) fixing the transition dynamics, and varying the reward function of each task, and/or (iii) varying both reward and dynamics. I see that both Car-on-Hill and Maze experiments fall under case (i). Have you tested the case (ii)?\n\nA simple experiment to cover case(ii): Consider a (stochastic) chain with length L (very large), and the goal is at the end of the chain. Here (keeping L fixed), one can vary the hardness of the tasks by defining reward functions with different granularity: sparse goal-oriented reward (target task), landmark-based rewards (intermediate tasks), and optimal myopic rewards obtained via potential-based reward shaping (easiest task).      \n\nTo cover case(i) with the chain example, fix the reward as a sparse goal-oriented reward, and gradually increase the length of the chain.\n",
            "summary_of_the_review": "The paper proposes an interesting and novel algorithm for curriculum RL. The paper is overall well written. However, the conceptual and empirical comparison to knowledge transfer via reward shaping methods is missing. There were some clarity issues in the experiments. If these two issues are addressed, I am marginally inclined towards acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper tackles the problem of curriculum learning and presents a new technique (boosted CRL) that provides a tighter bound on the approximation error of the action-value function than standard curriculum learning. The authors show both theoretically and empirically the effectiveness of the approach over standard CRL. The presented methodology learns a function approximation based on the sum of `residuals'. The evaluation was done on 3 environments that had scope for the generation of a curriculum by varying the reward and exploration factors -- car on hill, maze, linear system control. Results show that BCRL performs significantly better than CRL in all environments.",
            "main_review": "**Pros**\n* Writing and Presentation: The paper was clearly written and the graphs were well annotated.\n* BCRL Framework: The BCRL framework was interesting (in particular the idea of boosting using residuals). Some of the presented theorems were a bit mathematically involved, but the presentation of the ideas systematically helped parse the key ideas in the proof. \n\n**Cons**\n* Experiments and evaluation: While the setup is largely well explained, it would have been clear if at least one paragraph explained the details of the Linear System Control environment. From the main paper, it was not clear what was meant by the LSPI applied to LQR -- some of the details from the appendix can be brought to the main paper. Furthermore, the reasoning given for the decline in the performance of CRL in the environment needs more clarification/insights. I was not entirely convinced by the reasoning provided (on unlearning of Q-function when the task was switched to the target task).\n\n* Clarification question: Am I correct in understanding that, the proof (beginning with Proposition 1) assumes that convergence to the optimal state-action function will take place in a finite number of iterations? Extending the ideas presented, can it be said that the subsequent approximation error of the residuals becomes smaller as the number of tasks in the curriculum increase, given that for a task the error of BCRL is bounded?\n\n* Clarification question: The terms in Eq (11) can be made more intuitive in the paragraph explaining them. This was slightly difficult to parse.\n\n* The limitations of the approach could have been addressed better.\n\n**Additional Comments**:\n\nCan the presented methodology be extended to account for the increasing task complexity in the curriculum and weigh the importance of each task gradually presented in learning the state-action value? \n\n**General Summary**:\n\nOverall I thought the paper presented the key ideas in a clean way. The mathematical framework is very interested and opens avenues for future research in the area -- both practical and theoretical. The experimental setup for the Linear Control System could have been better explained.\n\n*Originality*:  Moderate\n\n*Clarity*: Good\n\n*Quality*: Good\n\n*Significance*: Moderate to High",
            "summary_of_the_review": "Overall I liked the ideas presented in the paper and think it provides a good starting point for analyzing theoretically, the non-trivial task of curriculum learning in RL. The idea of using the sum of residuals to model the state-action function approximation was interesting. I am inclined to accept the paper because of its promise and utility in inspiring similar approaches to curriculum learning (which provide a framework for theoretical analysis), though the experimental set-up could have been made clearer and limitations of the approach be made explicit.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new curriculum for RL method. The basic idea is to \"learn\" a residual for each task, modeling in this way how the tasks differ. The research topic is highly relevant and contemporary and the theoretical analysis of the method is interesting. The major drawback is the empirical evaluation. ",
            "main_review": "The authors propose a Curriculum Learning method to \"learn\" a residual for each task, instead of trying to learn from scratch each of them. \nThe proposed method is interesting and creative, and seems to be beneficial to the learning process in the empirical evaluation.\n\nMy main concern is the omission of the many related works. Although the authors evaluate the algorithm only in simple environments (which means it's relatively easy to implement other baseline methods), they do not compare against the results of any related methods from [1] or [2], for example (the former is.cited in the manuscript).\n\n[1] Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, and Peter Stone. Curriculum learning for reinforcement learning domains: A framework and survey. Journal of Machine Learning Research, 21(181):1–50, 2020.\n\n[2] Silva, Felipe Leno, and Anna Helena Reali Costa. \"A survey on transfer learning for multiagent reinforcement learning systems.\" Journal of Artificial Intelligence Research 64 (2019): 645-703.\n\nFurthermore, some sentences show lack of familiarity with the literature, for example: \"[...]contrary to curriculum RL which uses the same functional space \" (not all Curriculum Learning methods do that). Therefore, I would really like to see a performance comparison between the proposed method and state-of-the-art Curriculum Methods, instead of just against \"baselines\".\n\n\n----\nminor\n----\n\nShouldn't it be \"C-FQI\" in the legend of Figure 1?\n\n\n\n\n\n-----\nPost-rebuttal\n-----\nThe authors tried to address some of my concerns, in special adding some text in an appendix commenting on the distinction of their paper from TL. While I appreciate the (necessary) effort, I don't see the reason for adding this as an appendix, and think it would have been better to incorporate it to the main text even if it would require more effort to make sure space limitations are not exceeded. My evaluation was already positive, and the author response and other reviewers' evaluation had nothing that would make me lower my grades. Therefore, my evaluation is maintained .",
            "summary_of_the_review": "Paper investigating a relevant research topic. The manuscript contributes interesting ideas and the proposed method seems to have good performance. However, the lack of comparison against state-of-the-art methods makes it hard to assess how good the method is compared to other related works.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed a curriculum value-based reinforcement learning algorithm. Inspired by boosting methods, the algorithm learns on the Bellman residuals between the value function of the current task and that of the previous task. This paper provides analysis on the convergence of the algorithm and some finite-sample guarantee on the output value function. The method is tested on four different problems and is compared with two 3 baseline methods.",
            "main_review": "Strengths:\nThe boosting method is an interesting and promising way to adapt the change of task difficulty. As far as I know, this is the first paper that applies boosting to the RL curriculum learning. The experiment is solid and results are impressive. \n\nWeakness:\nMy concern is on the theoretical analysis. I found the formulation and analysis are poorly written. Many notations are not clear. The proof is either trivial or a direct application of previous results.\n\nHere are the detailed concerns. \n\n1. Can authors discuss how strong the assumption on the operator $S$ in Theorem 2 is? It seems a very strong assumption since it holds uniformly for all $y \\in \\mathcal{B}(\\mathcal{S} \\times \\mathcal{A})$.\n\n2. It is weird that the author states that the approximation error comes with two aspects and the second one depends on the nonlinear operator $S$, while the definition of approximation error in Equation (1) is independent of $S$.\n\n3. I think Theorem 1 is too complicated to interpret. For example, what is the behavior of the R.H.S in Theorem 1? The authors should elaborate more on the intuition of Theorem 1 (discussing the meaning of each term).  If the details of Theorem 1 is not important, the author may present only an informal version of Theorem 1 that highlights the high-level idea.\n\n4. In the definition of the nonlinear operator $S$, it is not clear what is $\\mu$-norm. If I am correct, $\\mu$ is the probability measure over $\\mathcal{S} \\times \\mathcal{A}$ and the empirical version $\\hat S$ is the empirical error evaluated on the dataset $\\mathcal{D}_t$. It is also not clear what $\\mu$ means in Algorithm 1.\n\n5. The notation is conflicting at the start of Section 4.1. It seems $Q_t^0$ represents the starting of task $t$. That means when task changes, $k$ should be reset to 0. However, the author also writes that $Q_{t+1}^{k+1} \\approx T_{t+1}^{*} Q_{t}^{k}$.\n\n6. I am assuming that Theorem 2 is analyzing the $Q_t^k$ using the accurate $S$ instead of the empirical version. The authors should make it clear about this point.\n\n7. It seems there are two versions of $Q_{t}^k$ in the paper, one generated from the optimal Bellman operator and the other one from the empirical one. The author should use different notations for them. For example, in Equation (8), the approximation error should be using the empirical one if I understand it correctly.\n\n8. Essentially, Theorem 4 concerns only the error of the last task. There should be more discussions on $\\epsilon_i^0$, which is also some approximation using finite samples. I think a more interesting direction is that when the complexity of the task is actually increasing, when the algorithm without boosting will suffer a high approximation error if they only use simple models. If they use complex models, then the VC-dimension could be high even for the simpler tasks, which slows down the training. ",
            "summary_of_the_review": "I recommend this is a borderline paper due to the issues I raised on the theoretical part and the notations.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}