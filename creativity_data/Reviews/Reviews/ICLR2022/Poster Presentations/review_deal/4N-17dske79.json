{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The authors propose a method for associative learning as an alternative to back propagation based learning. The idea is to interesting. The coupling between layers are broken down into local loss functions that can be updated independently. The targets are projected to previous layers and the information is preserved using an auto-encoder loss function. The projections from the target side are then compared with the projections from input side using a bridge function and a metric loss. The method is evaluated on text and image classification tasks. The results suggest that this is a promising alternative to back propagation based learning.\n\nPros\n+ A novel idea that seems promising\n+ Evaluated on text and image classification tasks and demonstrated utility\n\nCons\n- The impact of the number of additional parameters and the computation is not clarified (even though epoch's are lower)\n\nThe authors utilized the discussion period very well, running additional experiments that were suggested (especially ablation studies). They  also clarified all the questions that were raised. In all, the paper has improved substantially from the robust discussion."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper is a continuation of an original associated learning paper by Kao&Chen 2021. It attempts to propoose new learning approach associated learning as an alternative way to back-propagation. On top of the original paper, it discovers more interesting properties and extend AL to CNN, LSTM and transformers (though lacking sufficient details).",
            "main_review": "The authors have resolved my concerns on the technical details of how AL is applied on RNNs and Transformers.\n\n================================\n\nThe paper is well written. Experiments show that in text classification and image classification, the proposed method outperform BP in some basic architecture setting.\n\nHere are my concerns:\n\n- It is unclear how the AL is applied on RNNs and Transformers. Section 2.1.1 only very briefly described them, but I could not figure out some of the details. For example, how is the temporal data processed in LSTM?\n\n-In CNN, when flatting the hidden representation, it also lost the spatial information in feature maps. Furthermore, how to convert si to si' if ti is also a 3d feature map when the spatial information is lost.\n\n- From the description in Section 2, it seems that AL introduces around double the parameters for a given neural network. What is the impact of the increased parameters in computation cost?\n\n- The experiments uses relatively simple network architecture for text classification. Does the same benefits carry over to large transformer models, and still beats currently popular models like BERT?\n\n- The architecture information on CNN in section 3.3 is missing.\n\n- If my understanding is correct, the proposed architecture would not work in sequence generation task like LSTM and transformers could do. Right?",
            "summary_of_the_review": "In summary, I think though the paper proposes AL framework as an alternative to BP, it is actually a simple extension to a previous work, and does not proposes substantially new ideas. Some details are missing, and experiments are not extensive enough to cover state-of-the-art architectures.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Associated Learning puts forth a template that can be applied to almost any network to achieve faster training and inference. They apply their template to several existing deep learning models and perform experiments that show they can achieve comparable if not better results with less training time. ",
            "main_review": "The paper clearly lays out the advantages of associated learning: faster inference, dynamic layer accumulation, and pipeline. The paper is clearly written with good figures. The experiments appear to be easy reproducible, too. The decrease in epochs needed for LSTMs is particularly impressive.\n\nI found the biological basis a little lacking. Perhaps, some type of curriculum learning or more exploration on what the various shortcuts are doing could make this argument stronger. The related works section neglects to mention other gradient-isolated methods like https://arxiv.org/abs/1905.11786. I think in some ways this work can be seen as encoder-decoder with additional regularization, too?",
            "summary_of_the_review": "I would recommend this paper to be accepted. While there are several issues, the empirical results are strong (particularly the LSTM reduction in epochs). I think is a lot more to explore with the dynamic layer accumulation and gradient isolation, too, that would be interesting to other researchers.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies and benchmarks an alternative to back-prop named associated learning. They analyze the pros and cons.",
            "main_review": "Thank you for this read. The results and the methodology are definitely compelling. Why I cannot accept the manuscript as is, is that:\n· The motivation is not clear enough. It is clear wrt why BP is not ideal. But it is not clear how you landed on this method specifically, as compared to many other attempts on finding more optimal neural network optimization methodologies.\n· Section 2 is very difficult to follow. I would spend some more effort explaining how your method works in the manuscript.\n· It would be nice to include an algorithm of how to implement AL.\n\nA selection of minor comments:\n· Some typos throughout the manuscript, e.g., in abstract \"associate\" and paragraph 4 in the introduction \"in Section 4 We\".\n· Notation must be introduced, e.g., f, y, etc. in Section 2 are not introduced properly in relation to Figure 1.\n· It is difficult to follow the difference in notation when using h, b, and f. I recommend you spend some more time on making this very clear to the reader. \n· I find the Table 2 epochs for AG News difficult to follow. There is a clear pattern that AL is faster, but then things changes radically for AG News? Would be nice with some further analysis into this.",
            "summary_of_the_review": "With some more clarification on how you ended up with this methodology and a clear algorithm for how to implement AL, the reviewer would be happy to accept the manuscript.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes associated learning (AL) for CNN, RNN, and transformer. Different from back-propagation (BP), AL decomposes BP’s global end-to-end training strategy into several small local optimization targets such that each sub-networks has an isolated gradient flow. To achieve this, the paper proposes to map input $x$ and output $y$ into intermediate AL layers and performs metric learning (e.g., $t_1=b_1(s_1)$) and auto-encoder learning ($t_1=t_1^{‘}$), as shown in Figure 2. Moreover, Each AL layer can be optimized locally. The idea is interesting. The experiments demonstrate the effectiveness on (IMDB Review, AG’s News corpus, DBpedia Ontology, the Stanford Sentiment Treebank, CIFAR10, and Fashion-MNIST.",
            "main_review": "First, as in Figure 2, the paper proposes to map input x and output y into a latent space for metric learning ($f(x)=g(y)$) and auto-encoder learning ($y=h(g(y))$) are also investigated in multi-label classification[r1,r2], which are not discussed in this paper. In my opinion, the main difference is the design of multiple latent spaces compared with these multi-label classification methods.\n\n\nSecond, in traditional machine learning, we often map a high dimensional space to a low dimensional space for metric learning. It is unclear why maps the target $y$ to the intermediate layers in this paper. Given a high dimensional space (e.g., images), the inference model extracts useful features and filters unrelated features for metric learning. \n\nHowever, in this paper, I find that the authors conduct experiments on some single label classification (e.g., CIFAR10 and Fashion-MNIST). In this case, $y$ is a scalar or one-hot vector, I am curious about the exact form of $g_1$, $g_2$, $g_3$ in Figure 2. Does the proposed method map a low dimensional latent space to a high one? What is the motivation for expanding representation space? If $g_1(y)$ and $g_2g_1(x)$ are still in a low dimensional space or $g_i$ are very simple, do we really need inverse transformations from $Y$ to AL layers? In this case, we can simply fuse different AL layers to top layers for metric learning. For example, we can move $y$ after  $t_3$ and remove $h_1$, $h_2$, $h_3$ in Figure 2. Since $y$ is a specific label, it is unclear why we need to map to a high dimensional space.\n\nThe design of multi-label classification is reasonable to me because the target $y$ is complex (e.g., the multiple label vectors could miss some labels) and the multi-labels could be in a high dimensional space. In this case, one can map high dimensional space $X$ and $Y$ into a low dimensional latent space for metric learning.\n\n[r1] Learning Deep Latent Spaces for Multi-Label Classification, AAAI 2017.\n[r2] Multi-label Classification via Feature-aware Implicit Label Space Encoding, 2014.\n...\n\nThird, it would be better to set a baseline by moving $y$ after  $t_3$ and removing $h_1$, $h_2$, $h_3$ in Figure 2 for comparison.\n\nFourth, the architecture of AL layers is similar to Ladder networks. It is suggested to analyze the differences.\n\n[r3] Semi-Supervised Learning with Ladder Networks, 2015.",
            "summary_of_the_review": "(1) The proposed method can be optimized locally, and achieve competitive results. The proposed framework can be used for CNNs, RNNs, and transformers. The idea is interesting.\n\n(2) More analyses about the motivation and the necessity of inverse transformation for Y to latent space are needed.\n\n(3) The analyses and discussions about related works, such as multi-label classification and ladder networks, are missed.\n\n(4) Some experiments are suggested to support the authors' opinions (e.g. (2) and (3)) if possible.\n\n\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}