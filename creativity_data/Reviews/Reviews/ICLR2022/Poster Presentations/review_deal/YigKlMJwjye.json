{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper offers an alternative formulation of demographic parity, named GDP, which makes it amenable to easier computation when the sensitive attribute is continuous. Analytically, the paper relates GDP to other notions, offers ways to estimate GDP from data, and establishes the convergence of these estimators. Experimentally, the paper adds the estimated GDP as a learning regularizer and establishes the accuracy-fairness tradeoff that results by using this method versus others.\n\nThe need to handle continuous sensitive attributes is well-motivated since they are ubiquitous. The direction of the paper is thus very pertinent. The experimental exploration of the paper is also strong, though reviewers initially raised questions of clarity of the relationship of GDP with adversarial debiasing. These are mostly addressed by the authors. One weakness of the paper that largely remains is whether the paper offers new conceptual insights. Indeed, demographic parity is simply a notion of independence between an algorithm’s output and sensitive attributes. Other independence metrics are dismissed in the paper as unreliable to compute. However, one reviewer correctly raises the concern that *under similar regularity conditions* to the ones establishing the convergence of the kernel GDP estimator, it is also possible to establish convergence of other independence metrics, that would equally capture demographic parity. Another reviewer also points out that such convergence would follow using standard non-parametric statistics techniques. Smoothed estimators of mutual-information are indeed available in the literature, with convergence guarantees even in the high-dimensional regime. The authors do not satisfactorily address this, casting doubt on the overall significance of the contribution.\n\nThat said, given the strong motivation behind the paper and the overall promise of the methodology, it may be worth sharing with the community. The authors are urged to address the above. Additionally, they are urged to be transparent about what the theory offers and what it doesn’t. For instance, the convergence results of GDP only tell us that we can use these estimators to audit the fairness of existing models. In other words, although the paper is touted as showing that GDP can be successfully used for learning, the evidence there is purely empirical: there is no learning guarantee simultaneously on the accuracy and fairness of GDP-penalized risk minimization."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a generalized demographic parity for group fairness which is computationally feasible for both continuous and discrete protected attributes. Two estimations, histogram and kernel, are proposed for efficient estimation and the kernel estimation has faster estimation error governance. The connection between the GDP regularization and adversarial debiasing is built. The experiments on syntehtic/tabular/graph datasets show the effectiveness and efficiency of the GDP kernel estimation.",
            "main_review": "Strength:\n\n1. The GDP is a generic fairness metric for both continuous and discrete protected attributes.\n2. The histogram and kernel estimations are computationally feasible. The error and complexity analysis of the two estimations show improvement.\n3. The connection of GDP and adversarial debiasing is built.\n4. The comprehensive experiments on multiple datasets show the efficiency and effectiveness of the kernel estimation of GDP.\n\nWeakness:\n\n1. There is a gap between the estimation of GDP and adversarial debiasing. Vanilla GDP relies on an unknown distribution which prevents its application. If the estimation is applied, it is unclear the relationship between $\\hat{GDP}$ and the adversary utility.\n2. It is unclear how to calculate the underlying GDP in the experiments, or it is just an estimation of GDP. Please clarify.\n",
            "summary_of_the_review": "This paper provides a generic fairness metric for both continuous and discrete protected attributes in various tasks. The kernel estimation has good error and complexity properties. The experimental comparison is comprehensive and well designed. \n\nThe connection between GDP and binary demographic parity is built, but I wonder about the relationship between GDP and discrete protected attributes with multiple values, e.g., intersectional fairness. The extension from binary to multiple to continuous would be more smooth.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Please see main review.",
            "main_review": "This paper proposes the generalized demographic parity (GDP). It aims to generalize the existing definition of demographic parity (DP) to incorporate continous sensitive attribute while preserving tractable computation. More specifically, GDP is defined as the weighted total variation distance between local prediction average and global prediction average, with the weights being the probability density function (PDF) of sensitive attribute. Based on the definition, histogram estimation and KDE are applied to estimate the distribution of sensitive attribute. In addition, the authors shows that, under certain assumptions, (1) GDP is equivalent to DP; and (2) GDP is the lower bound of adversarial utility by Madras et al. 2018. Experiments on several real-world datasets across different settings demonstrate the effectiveness of the proposed method against baseline methods. However, I have several concerns about this manuscript as shown below. \n\nConcerns\n- The authors claim that GDP is tractable. I wonder how it is tractable if we do not know the joint distribution of prediction and sensitive attribute (as claimed by the first few sentences in Section 4). \n- The proposed method uses histogram/KDE to estimate the distribution of sensitive attribute. Why cannot we use the same techniques (i.e., histogram/KDE) to estimate the distributions needed for calculating mutual information (e.g., joint distribution of prediction and sensitive attribute,  conditional distribution of prediction given sensitive attribute, or conditional distribution of sensitive attribute given predictions)? \n- The experiments are not convincing. (1) The authors established the relationship between GDP and adversarial debiasing by Madras et al. 2018. Why is there no experiments on comparing the performance between these two methods? (2) The paper barely discusses the work Louppe et al. 2017 but includes it as the only debiasing method for comparison. I wonder the justification behind the choice of baseline methods. (3) Mary et al. 2019 also works on debiasing the continuous sensitive attribute, yet there is no comparison. I believe certain justification is needed as well.\n- Some related works are missing (please see below). And I believe some of them should be included as baseline methods as well.\n\nReferences\n\n[Related to fairness with histogram model]\n\n* Kamishima, T., Akaho, S., Asoh, H., & Sakuma, J. (2012, September). Enhancement of the Neutrality in Recommendation. In Decisions@ RecSys (pp. 8-14).\n* Kamishima, T., Akaho, S., Asoh, H., & Sakuma, J. (2013, September). Efficiency Improvement of Neutrality-Enhanced Recommendation. In Decisions@ RecSys (pp. 1-8).\n\n[Related to fairness with KDE]\n\n* Cho, J., Hwang, G., & Suh, C. (2020). A fair classifier using kernel density estimation. Advances in Neural Information Processing Systems, 33, 15088-15099.\n\n[Related to fairness with mutual information]\n\n* Cho, J., Hwang, G., & Suh, C. (2020, June). A fair classifier using mutual information. In 2020 IEEE International Symposium on Information Theory (ISIT) (pp. 2521-2526). IEEE.\n* Roh, Y., Lee, K., Whang, S., & Suh, C. (2020, November). Fr-train: A mutual information-based approach to fair and robust training. In International Conference on Machine Learning (pp. 8147-8157). PMLR.\n* Lowy, A., Pavan, R., Baharlouei, S., Razaviyayn, M., & Beirami, A. (2021). FERMI: Fair Empirical Risk Minimization via Exponential R\\'enyi Mutual Information. arXiv preprint arXiv:2102.12586.",
            "summary_of_the_review": "Overall, this paper is a bit unclear in some parts and lacks a thorough literature review. The experiments are not convincing enough as well. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper is motivated by the need to account for continuous attributes in fair Machine Learning. In particular, it proposes a Generalized Demographic Parity metric (GDP), i.e., a group fairness metric that can work with both continuous and discrete variables. A key challenge in doing so is preserving tractable computation. \n\nTheoretically, GDP is defined via the weighted total variation distance and measures the distance between the local and global prediction average, where the weight corresponds to the pdf of the continuous sensitive attributes. Since the joint distribution between model prediction and sensitive attributes might not be available in practice, the paper also proposes two estimation methods: histogram estimation and kernel estimation. The former quantizes the continuous sensitive attributes into bins, whereas in the latter method the group indicator of the sample is treated as a kernel function. The kernel method leads to faster estimation error convergence rate in terms of sample size.\nThe paper provides an extensive evaluation with tabular, graph and temporal graph data, synthetic experiments, and classification and regression tasks. It is demonstrated experimentally that the kernel method has better bias mitigation performance.\n\n",
            "main_review": "Comments:\nSection 1: Age is not a particularly successful example of a continuous attribute. A better example might be income differences (although it is not always legally protected, it is definitely of interest to decision-makers).\n\nSection 3: The definition of GDP is precise and relatively natural. However, some justification of this particular choice is necessary. What are potential other definitions? Why is this definition the most natural (and technically sound) extension of the categorical definition? Explaining in simple terms would help provide intuition for the reader. \n\nI think the theoretical foundations are a nice contribution of the paper. I would appreciate a proof sketch or intuition after each theorem. \n\nAlthough I understand that this might be part of follow-up work, I think it is important to include a discussion on how other metrics can be similarly extended to capture continuous attributes. Demographic parity is a natural first step, though.\n\nSection 6.1: why were these parameters chosen? Have robustness checks been performed and what were the results?\n",
            "summary_of_the_review": "\nIn my opinion, the paper touches upon a very important and practical topic in fair ML. Not all attributes are categorical and privacy concerns might require fairness in a neighborhood or region of attributes. Thus, accounting for continuous variables in fairness definitions is crucial in many practical contexts.\n\nThe paper is methodologically sound. It provides a quite extensive theoretical justification and guarantees as well as a complete and convincing empirical evaluation with several datasets. It is well written and has the potential to lead to follow up work (both in terms of other definitions and also inspiring research on continuous attributes in fair ML). \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new extension of demographic parity when the sensitive attribute is continuous. It proposes two techniques to estimate this quantity from data. The authors also consider adding this quantity as a regularization term to control discrimination while designing machine learning models.",
            "main_review": "Overall, the paper is well-written and easy to follow. I like the structure of this paper: it starts with a new definition for measuring fairness; introduces ways to estimate this fairness measure; and presents applications of this measure. However, my main concern is the technical novelty of this paper (see my detailed comments listed below). \n\n1. My main concern is the estimation error bound provided in Theorem 3. It seems that this result relies on a strong assumption (i.e., the prediction function is Lipschitz continuous w.r.t. the sensitive attribute) and it is unclear to me the order presented in this theorem is optimal. \n\n(1) [Strong assumption.] It is very hard for me to come up with a scenario where the Lipschitz condition is satisfied and the Lipschitz constant is small. Note that the Lipschitz constant will affect the optimal bandwidth choice and, potentially, the sample complexity. It would be great if the authors can provide an example to show that this Lipschitz condition is inevitable. \n\n(2) [Optimal order.] The order of estimation error given in Theorem 3 is O(N^{-4/5}). However, it is unclear to me if this is the optimal order. Can the authors derive some converse results to prove that this order cannot be improved anymore?\n\n2. [Intersectionality.] The authors only consider a single sensitive attribute. What if there are multiple sensitive attributes (e.g., age, weight, income, …). How does the number of sensitive attributes affect the estimation error bound in Theorem 3? Does the bound suffer from the curse of dimensionality in this case?\n\n3. The authors wrote in Theorem 3 “Under the optimal bandwidth choice, …”. Can the authors specify what the optimal bandwidth is?\n\n4. Can the authors include an error bar for each figure in the paper? ",
            "summary_of_the_review": "The paper is interesting but the technical novelty may not be sufficient",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}