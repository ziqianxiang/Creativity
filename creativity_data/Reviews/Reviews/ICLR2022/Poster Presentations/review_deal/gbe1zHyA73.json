{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper proposes a method for hybrid model-based/ML learning, where a model is decomposed into an interpretable parametric prior and a neural net residual.  In this case, the prediction error minimization does no identify the parametric component, and an alternating optimization method is proposed to augments prediction error loss with component-specific losses. Empirical and theoretical results are obtained.  Initial questions of several reviewers were addressed."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a method for hybrid model-based/machine-leanring learning, wherein a model is decomposed into an interpretable parametric prior and a residual (typically represented by a neural net). The paper demonstrates that prediction error minimization does not accurately identify the parametric component and proposes an alternating optimization method that augments prediction error loss with component-specific losses. The paper also introduces a variant where auxiliary data can be used to define the loss.\n\nTheoretical guarantees are provided where auxiliary losses are given as upper-bound constraints and exact minimization is performed. Also, uniqueness and convergence guarantees are provided for the linear approximation case using a more practical algorithm.\n\nExperiments show the efficacy of the method in linear and non-linear settings, in both identification and prediction.",
            "main_review": "The paper is generally clear and well-written and the proposed approach is interesting and well supported by experiments.\n\nWith that said, I have a few comments:\nTheory:\n* Proposition 1: It is not clear how the uniqueness of the solution is connected to the accurate identification of h_k. My main concern here is that the paper does not place any condition on the relation between the function spaces H_k and H_u. Contrast this to the linear case where A is invertible and thus cannot realize a constant derivative without D_A.\n\n* It appears that Section 3.3 abandons the \"realizable setting\" assumption, where the true function is assumed to be in the hypothesis space (which is good). I suggest that the authors put more emphasis on this. Otherwise, the transition can be confusing.\n\n* Proposition 2: I am not sure of the implication of the proposition. It seems to only suggest that for each \\hat{A} there exists a unique D_A but does not suggest the uniqueness of A, which is what we would be interested in for interpretability.\n\nExperiments:\nA question that arises is whether alternating optimization is needed in practice or if joint gradient descent will be as good. It would be interesting to see this experiment.\n\nClarity:\n* Section 4.2 needs much larification:\nEquation 17: The del operator needs to be defined. What does TU mean when U is a tuple (u, v). Why do you use parenthesis for S(U) but not TU. Also, that does T_{t-k} mean? Doesn't k stand for \"known\"?\nThe fact that \\theta^* is a function in time (rather than a constant vector) can be confusing and should be spelled-out.\n\n* After eq (16) \"\\theta^* = (\\alpha, \\beta)\", I believe \\beta should be \\gamma?\n\n\n\n",
            "summary_of_the_review": "The paper proposes an interesting method to train hybrid models for both identification and prediction quality. \nConnected two ideas from the literature in a single framework is also a welcome contribution. The proposed method is supported by experiments on different domains. Theoretical aspects are promising but still limited.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Authors propose a general framework for learning and ensuring identifiability with hybrid models. They derive strong theoretical guarantees as well as a proof of convergence in a simple affine setting. They validate their framework in several simulated experiments and the Natl dataset, and draw comparisons with state-of-the-art hybrid model Aphynity as well as Neural ODEs. Experiments confirm that the proposed framework increase interpretability and maintain high prediction performances",
            "main_review": "Authors propose a unifying framework for hybrid models, generalizing recent methods. They derive theoretical guarantees very compelling theoretical guarantees for the framework, as well as a convergence proof for a simple affine model. However, I believe that the paper is quite hard to read, and propose several models without giving intuition for which model to use. The models also seem to need complex hyper-parameter choice, which again is not very intuitive and not discussed extensively in the supplementary material. \n\nHere are two points that I think need to be discussed more extensively in the paper:\n - The paper proposes two models (with and without auxiliary data). When should we use one or the other? Should we just use it whenever auxiliary data is available? Moreover, equation 5 is the equivalent of equation 4 for auxiliary data. However, the model uses alternate optimization to converge. What is the equivalent procedure for auxiliary data? \n - The \"practical optimization\" is not straightforward, and although it is well-detailed, the hyper-parameters chosen for the experiments are pretty complex, which is to me an important drawback of the framework. To give intuition into the model, how do the lambda values relate to the two scalars mu in equation 6? \n \nAlthough the section titled \"THEORETICAL ANALYSIS FOR A LINEAR APPROXIMATION\" is compelling, I think it could be included in the appendix to allow for more space to discuss the previous points. \n\nOther points that would make the paper easier to read : \n - Using \"Ours eq 4\" in the results is not intuitive at all, since the model optimizes a different objective. This makes it a bit confusing. \n - Figures are not rendered properly, many of them appear blurry and could be made bigger (there are lots of white space in the figure that could be removed). This greatly affects the quality of the paper. \n - I would like to see the differences between true and estimated processes in the figures. This would make the results easier to grasp.\n\nThe method is compared to Aphynity and Neural ODE but there are no comparisons to Neural ODE on the simulated datasets (I think it would be beneficial to show the predictive logMSE for Neural ODE in Table 1).\n\n",
            "summary_of_the_review": "Authors propose a unifying framework for hybrid models, generalizing recent methods. They derive theoretical guarantees very compelling theoretical guarantees for the framework, as well as a convergence proof for a simple affine model. However, I believe that the paper is quite hard to read, and propose several models without giving intuition for which model to use. The models also seem to need complex hyper-parameter choice, which again is not very intuitive and not discussed extensively in the supplementary material. \n\nThus, I do not recommend acceptance of this paper, but I am willing to increase my score if authors answer my concerns, as the theoretical guarantees derived here are very strong and surely beneficial for a large class of hybrid models.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the identification and prediction of dynamical systems from observed data by combining statistical approaches with physical priors. The main idea is to decompose the approximation of unknown dynamics into two parts: a parametric physics-based model $h_k$ (used to incorporate prior knowledge, or to identify parameters), and a non-parametric statistical model $h_u$ (used to fit the unknown, or unobserved degrees of freedom). \n\nSince the statistical part has a priori no restrictions, the overall problem is ill-posed in the sense that the statistical part can fit all of the dynamics. To ensure well-posedness, the authors propose to convert the joint learning problem to learning $h_k$ and $h_u$ subject to constraints: $h_k$ is given an allowed error tolerance and $h_u$ is given an allowed deviation from 0. The authors show that this modification implies well-posedness, and subsequently design an alternative minimization algorithms to solve for $h_k$ and $h_u$ iteratively. It is proved that this algorithm converges in the affine setting (via sequential projection to find intersection of convex sets) and it is demonstrated experimentally in low and high dimensional systems that this method works well in practice outside of the convex setting.",
            "main_review": "This paper is very well written and the problem being addressed is very relevant to the machine learning community, especially to its emerging applications in science and engineering problems. The proposed method is sound and quite easy to implement, and the performance is also good as presented. I think it will be of interest to the broad readership on the intersection of machine learning and scientific applications.\n\nBelow, I have some questions that the authors may wish to clarify.\n\n1. The overall approach: The key method introduced to ensure well-posedness is page 4 Eq (7), via constraining each part of the model by $\\mu_u,\\mu_k$. However, to me it is often more convenient in practice to consider the “dual” formulation of optimization with penalization, i.e. instead of (7) one can consider\n   $$\n   \\min_{h_u,h_k} d(f, h_u+h_k) + \\lambda_k \\ell(h_k) + \\lambda_u d(h_u,0)\n   $$\n   This is certainly very easy to implement, and has the same number of hyper-parameters as the proposed case, replacing the $\\mu$‘s by $\\lambda$’s. Can the authors comment on why the current constrained approach is chosen over this approach, and can some quantitative comparisons be made?\n\n2. Algorithm 1: while successive projections to convex sets can find the intersection eventually, the convergence rate may be slow. Partial projections can sometimes speed up the process. Do you observe speedups if equation (8) is not solved to completion in each step?\n\n3. Prop 3: $D$ is proved to converge (and thus $A$) via a contraction argument. Can anything be said about the convergence point? Can it be a local minimum but not global?\n\n4. Table 1: As I understand, the performance of the proposed method will depend on the choice of the hyper-parameters $\\mu_k,\\mu_u$. Can the authors comment on the sensitivity of tuning parameters with respect to the good results shown in this table? How is fair comparison ensured, e.g. how are the baseline method(s) (e.g. Yin et al) tuned to each problem?\n\n5. Section 4.2: This example is interesting as it is more realistic for scientific applications. I believe this part has some clarity issues in the main text\n\n   1. I believe that Eq (32-33) in the appendix should be included in the main text to allow for easier understanding of what is $f_k$ and $f_u$.\n   2. Looking at Alg 3 in the appendix, the alternate minimization is in fact done by gradient descent with soft constraints instead of the alternative projections introduced in Alg 1 in the main paper. Can the authors explain why this modification is made, and whether the prior experiments on toy examples followed Alg 1 or Alg 3?",
            "summary_of_the_review": "This is a good paper tackling a relevant problem. The algorithm introduced is easy to implement and has promising performance. Some theoretical justification on convergence is also presented. I believe that this is a meaningful contribution and should be accepted. There are some clarity issues that the authors can address in a revision.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigate a novel method to recover well-posedness and interpretability for the problem of learning a physical and machine-learning hybrid model, by controlling the ML component and the MB hypothesis. The authors also study the case where auxiliary data are introduced. Experiments show the performance of the proposed method in both synthetic data and real-world ocean dynamics.",
            "main_review": "Strength: The main strength of this paper is the experiments. In the experiment, the results is promising, with better performance in both synthetic and real-world experiments. Ablation studied are also performed to show the model performance with different objective functions.\n\nWeak: I think the weakness of this paper is the technical novelty of the algorithm and theorem. The idea of combining physical models and machine learning models exist for decades. The theoretical derivations in this paper is nothing but applying triangle inequalities here and there. The alternate minimization algorithm is not novel at all, and the convergence of such an algorithm for a linear dynamic system is also well-known. Therefore, the technical contribution of this paper seems trivial.",
            "summary_of_the_review": "The experiment results of this paper is very strong and comprehensive. However, the novelty of this paper is not very good. The motivation, theoretical analysis and algorithms are not very novel.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}