{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper introduce a protein pretraining framework that enhances representations learnt from protein language modeling with knowledge graph embeddings. The new framework, OntoProtein, optimizes jointly a masked Protein objective and a Knowledge Graph Embedding objective producing knowledge-aware protein embeddings. These embeddings are evaluated on downstream tasks including protein-protein interactions and protein GO association prediction. The paper also introduces a new large-scale KG dataset, ProteinKG25.\n\nThe reviewers were in agreement that the paper presents an important research direction and that the work is well framed and motivated. The dataset contribution was also considered important by the reviewers and they are in agreement that the paper is clear and generally easy to understand. Reviewers were concerned that the novelty of the work is in the application of existing techniques to a new domain rather than introducing new domains, but generally the reviewers considered the novelty to be sufficient for publication. There were some other concerns about missing references and some of the presentation, but the authors addressed these concerns in their response and in the updated version that they produced."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces OntoProtein a comprehensive pre-training framework for protein embedding with the knowledge of gene ontology (GO). More specifically, OntoProtein jointly optimizes on both masked Protein Model and Knowledge Graph Embedding model which results in knowledge-aware protein embedding for downstream applications including protein-protein interactions and protein GO association prediction. The authors also create a new benchmark of proteins with aligned annotations to facilitate the OntoProtein training.",
            "main_review": "Reasons to accept: \n* Interdisciplinary study on extending the language model and KG embedding into protein embedding to empower bio appications\n* Insightful strategy and observation on KG negative sampling \n* Extensive experiments on multiple tasks and ablation study (attention analysis) are conducted. New useful resources of enhanced protein datasets are created and available publicly with strong reproducibility. \n\nReasons to reject: \n* The information fusion of text token and protein sequence tokens may face modality challenges in one KG embedding model. \n* Some missing references and potential baselines on the PPI task and other works that jointly trained on protein and gene ontology (see more in detailed comments)\n* Lack of quality assessment of curated dataset ProteinKG25. \n\nDetailed comments: \n\n* One of the major concerns of this paper is that some existing works with joint training on protein with other domains. Examples are [1,2,3] with PPI predictions as one task for evaluation (can be added to PPI baselines). As for PPI baselines, since the sequences have been added as input, the current evaluation only considered GNN-PPI variations, however, there is a large collection of existing work relying on protein sequences (potentially with the help of PSI-BLAST and multiple sequence alignment) and follow-up works of PIPR. It is necessary to show that the purely sequence-based model cannot compete with the proposed OntoProtein with an additional facet of GO knowledge.\n* In OntoProtein, the GO entity embeddings are based on text description while proteins are encoded with their amino acid sequence tokens through a shared model in MPM. The features from different modalities are jointly optimized together through KE loss with TransE. Two features are computed in the same embedding space with a simple translational distance which does not seem to make much sense, especially there are two types of triples (Protein-GO and GO-GO) trained together. This requires more justification on what the embeddings aim to learn. \n* It seems that the performance of OntoProtein does not significantly outperform other baselines in most cases on the TAPE benchmark. What is the benefit of OntoProtein compared with other models (especially ProtBERT)? \n*  It is interesting to see how different aspects can be leveraged into better protein presentations. Since the paper claims that the protein representation learning benefits from gene ontology, and GO can be divided into BP/MF/CO components, it would be interesting to see how individual subsets in GO can result in different improvements. Also, it is said that gene ontology (three subsets) and PPI are not independent of each other (some protein functions are annotated due to their known binding activities with other proteins). It is suggested to see whether such correlations can be identified in the current training framework or the OntoProtein can help solve the incompleteness. \n* There is no quality assessment of the newly curated dataset ProteinKG25. The understanding is that the authors successfully made one collection aligned with external resources of annotations to generate the protein-centric KG. But it is unclear about the quality (accuracy and completeness). It is also suggested to make a comparison of other benchmarks such as HetioNet[4] and DRKG[5] in terms of the detailed information or KG schema.\n* No sensitivity study on the hyperparameter \\alpha in pre-training objective which balances MLM and KE losses.\n\nMinor suggestions and corrections:\n* Figure 1 is somewhat confusing about the nodes and edges, especially the left subfigure and the number annotations. It would be beneficial to layout one snapshot of created Protein\n* The acronym “MLM” (masked language model) was used first in the introduction without full name (Page 2 line 10).\n\nReferences: \n* [1] Onto2Vec: Smaili, F. Z., Gao, X., & Hoehndorf, R. (2018). Onto2vec: joint vector-based representation of biological entities and their ontology-based annotations. Bioinformatics, 34(13), i52-i60.\n* [2] OPA2Vec: Smaili, F. Z., Gao, X., & Hoehndorf, R. (2019). Opa2vec: combining formal and informal content of biomedical ontologies to improve similarity-based prediction. Bioinformatics, 35(12), 2133-2140.\n* [3] Bio-JOIE: Hao, J., Ju, C. J. T., Chen, M., Sun, Y., Zaniolo, C., & Wang, W. (2020, September). Bio-JOIE: Joint representation learning of biological knowledge bases. In Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics (pp. 1-10).\n* [4] HetioNet: Himmelstein, D. S., & Baranzini, S. E. (2015). Heterogeneous network edge prediction: a data integration approach to prioritize disease-associated genes. PLoS computational biology, 11(7), e1004259.\n* [5] DRKG: Ioannidis, Vassilis N., Song, Xiang, Manchanda, Saurav, Li, Mufei, Pan, Xiaoqin, Zheng, Da, Ning, Xia, Zeng, Xiangxiang & Karypis, George. (2020) Drug Repurposing Knowledge Graph for Covid-19. https://github.com/gnn4dr/DRKG/\n",
            "summary_of_the_review": "This paper has merits in multiple directions. It applies the state-of-the-art language modeling in NLP in protein representation learning named OntoProtein. Also, OntoProtein help creates a new augmented protein benchmark with aligned GO annotations (enhanced views for heterogenous BioKG with multiple domains). The knowledge-aware negative sampling strategy has underlying insights on utilizing the hierarchically structured gene ontology which can be generalized onto KG negative sampling with ontology information, not limited in bio domain. The critics of this work are mostly about some missing references and baseline approaches, as well as the limitation on combinations of existing works.  Overall, the reviewer agrees that the merits of this work outweigh the flaws. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable. ",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a method to enrich the representations that are learnt by protein language models with knowledge encapsulated in gene ontologies. To do so, it curates a knowledge graph (ProteinKG25) and applies existing methods in multi-relational data embedding (Bordes et al.) to jointly train knowledge embeddings and protein embeddings, with the objective to enhance the latter and subsequently improve the performance on various downstream tasks (namely TAPE benchmark, protein-protein interaction and protein function prediction).",
            "main_review": "**Strengths**\n1. The research direction is important and well motivated -- a lot of valuable information is captured in gene ontologies and ought to be valuable to improve protein representations and performance on downstream task.\n2. The paper reads very nicely and easy to follow through, except for a few subsections to clarify / re-write (in particular section 2.4, see below) and would benefit for another read through to correct typos.\n3. The method is applied and tested across a diverse set of experiments.\n\n**Weaknesses**\n1. Methodological contributions are limited to leveraging existing multi-relational data embedding methods (Bordes et al.) with no adaptation to the specificities of the GO modality. The negative sampling is very close to random sampling (with constraints based on entity group type).\n2. This translates into marginal benefits across all experimental setups when comparing with the ProtBert baseline (acknowledged by authors in section 3.4). Some of the claims in section 3.2 do not seem backed up by the results. For example, in the PFP experiments, you cannot simultaneously claim that your method outperforms baselines in BPO and that it performs comparably with other methods in other settings (since baselines outperforms OntoProtein in the latter settings by a larger margin that Ontoprotein outperforms them in the BPO setting).\n\n**Clarifying questions**\n1. Section 3.1: “Specifically, we notice that the frequency of leaf GO terms involved in gene annotations and those more specific concepts relative to their parent nodes are less to non-leaf GO terms” -- could you please clarify what you meant here?\n2. Section 2.4. Equation 1: you seem to have a typo in equation 1 since it includes r’ but negative triplets involve the same r (r=r’?) since you only negatively sample the leading/tail entities h’ and t’.\n3. Section 2.4. Equation 2: why is d not an explicit function of t? Shouldn’t it be d(h+r, t) following Bordes et al?\n4. Section 2.4. Equation 3 is confusing. The ‘|’ operator is not standard for sets / not defined anywhere (did you mean union?). Additionally, you state that the negative sampling is achieved by sampling h’ and t’ at random from the same family as h and t resp. -- which I understand means for example that if h is E_{MFO} then also h’ is in E_{MFO} -- but that is not what is expressed mathematically by equation (3). Could you please clarify?\n5. Section 2.4: please clarify which embeddings you use exactly here? Are you using aggregate embeddings H_{GO} and H_{Protein} and none of the token level embeddings?\n6. Section 2.4: Do you do any negative sampling for T_{protein--GO} triplets? Are you limiting to negatively sampling tail (GO) entities?\n7. Section 2.5: How do you form input batches to jointly train this objective? \n8. Figure 4: How is it different from what one would obtain with ProtBert?\n\n**Minor points**\n1. Introduction: \"different from knowledge-enhanced approaches in NLP\" → please cite\n2. Introduction: “text deceptions” (towards the top of page 2) -- did you mean \"test descriptions\"? \n3. Would suggest that you give 1 or 2 concrete examples to illustrate the different objects described in 2.2 and appendix A.1. Table 7 is also a bit confusing (e.g. difference between GO term Vs GO statement?)\n4. The write up of section 2 could be improved -- there was a lot of overlap between section 2.1., 2.2 and 2.3 which was making some information redundant / making things more difficult to read\n5. Section 2: would suggest to include an overall architecture diagram to show how an example input triplet is processed (eg., what goes to which encoder type?).\n6. Section 3: bolding of results is not consistent / misleading. Always bold the best result in each column.\n7. Section 3: would be helpful to have MSA transformer everywhere to put things in perspective (bearing in mind it does have access to MSA data that you do not explicitly leverage).\n",
            "summary_of_the_review": "The research direction of this paper is very important and that the work described here is adequately framed and motivated. A valuable contribution is the ProteinKG25 knowledge graph put together by authors, which could be valuable to other researchers. While there are several points to clear out (see main review), the ideas are overall well presented and the paper reads nicely. \nHowever, the methodological and empirical contributions of this work are somewhat limited: the Knowledge Embedding for the knowledge graph (while applied for the first time to GO data) is largely borrowed from Bordes et al. and not particularly tailored to this very specific data modality; the negative sampling is only marginally different from random sampling (constraining on entity group). To the author’s own account “the gains in our proposed OntoProtein compared to previous pre-trained models using large-scale corpus is still relatively small” since “the knowledge graph ProteinKG25 can only cover a small subset of all proteins, thus, limiting the advancement”. In my view, this is precisely the main challenge to resolve for this type of work attempting to instill established knowledge into large protein language models, which would have made this work a very meaningful contribution.\nAll things considered, I’m leaning weak reject given the minor methodological contributions and weak experimental results.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper argues that informative biology knowledge in KGs can enhance protein representation with external knowledge. To show this, the author proposes a computational framework named OntoProtein that makes use of structure in Gene Ontology into protein pre-training models. This paper conducts various experiments to show the superiority of the proposed method and the benefits of the information of Gene Ontology.",
            "main_review": "Good paper but more efforts are needed.",
            "summary_of_the_review": "\npros:\n1. In computational biology, the prevailing approaches of learning protein representation rarely consider\nincorporating knowledge graphs (KGs), which can provide rich structured knowledge facts. This paper seems to be the first to impose the knowledge graphs of protein.\n\n2. This paper provides a meaningful architecture for simultaneously learning protein knowledge and embedding. \n\n3. To demonstrate the effectiveness of the proposed method, the author constructs a large-scale KG dataset, promoting the research on protein language pre-training.\n\nCons:\n \n1. To make the paper more friendly for the general ML/DL researchers, I think it should be made clearer the definition of the downstream tasks used in this paper.  \n\n2. This paper usually considers one or two baselines in the PPI task, which is are not enough and convincing. It is possible to compare the proposed method with [1] and [2]? If not, please provide the reasons.\n\n3. The presentation of this paper should be further improved. For example, Figures 1 and 4 are hard to read.\n\n[1] Patel, S., Tripathi, R., Kumari, V., & Varadwaj, P. (2017). DeepInteract: deep neural network based protein-protein interaction prediction tool. Current Bioinformatics, 12(6), 551-557.\n\n[2] Liu, X., Luo, Y., Li, P., Song, S., & Peng, J. (2021). Deep geometric representations for modeling effects of mutations on protein-protein binding affinity. PLoS computational biology, 17(8), e1009284.\n\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}