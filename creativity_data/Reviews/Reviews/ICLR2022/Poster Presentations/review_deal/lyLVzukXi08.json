{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a novel model-based Bayesian meta-learning approach that combines a novel conditional dropout posterior a new variational prior for the data-efficient learning and adaptation of deep neural networks. It is applied to tasks such as 1D stochastic regression, image inpainting, and classification.\n\nOverall, this paper received positive reviews: Reviewers thought that the \"the paper makes a nice connection between meta-learning and variational dropout, resulting in an overall elegant approach\" and that the \"reasoning of the novel prior is clear and understandable\" while the \"experiments are comprehensive\".\n\nGiven the agreement of the reviewers and the novel use of dropout for adaptation of a model to different tasks, I recommend accepting this paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a novel Bayesian meta-learning approach to model conditional posterior distribution via an amortized variational inference framework based on task-specific dropout. For tasks with few learning examples, the paper discusses the a novel low-rank product of Bernoulli expert meta-models to efficiently obtain full conditional posterior model over the neural network’s parameters. Few-shot learning experiments from Gaussian Process samples show that the model achieves better performance. The proposed method also outperforms existing models on active learning task with better uncertainty calibration. The method also shows better performance on image completion and few-shot classification tasks.",
            "main_review": "Strength:\n+ The paper is clearly written and easy to follow.\n+ The evaluation of NP-based methods using Reconstructive Log-likelihood (RLL) and predictive log-likelihood (PLL) is an interesting idea.\n+ The reasoning of the novel prior is clear and understandable. \n+ The experiments are comprehensive.\n\nWeakness: \n- Predictive accuracy (Table 3) alone is not sufficient to evaluate the strength of the uncertainty-aware few shot classification model, i.e. many recent few-shot models can easily perform significantly better than the results presented here (e.g. Meta-Learning with Differentiable Convex Optimization).\n- Experiments to demonstrate the applicability of uncertainty estimates (e.g., an active learning setting similar to Bayesian MAML) will further strengthen the results.\n- The comparison with CNP models alone is missing. The evaluation is done with NP + CNP instead.\n- Comparison with state-of-the-arts NP works will help better evaluate the model’s performance, for example, \"Convolutional conditional neural processes, Gordon et al. ICLR 2020\", \"Meta-learning stationary stochastic process prediction with convolutional neural processes,\nFoong et al, NeurIPS 2020\", or \"Attentive neural processes, Kim et al, ICLR 2019\".\n- Although the authors claim to address the issues such as mode-collapsing or over-fitting behavior in the context of meta-learning, the results to support the claim are missing in the paper.\n\n",
            "summary_of_the_review": "an interesting idea, but there is room to further strengthen the paper, particularly for the experiments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "I will put a few bullet points here. For a more detailed summary, see below.\n- Novel meta-learning approach\n- Utilises dropout to adapt a base network to new tasks\n- Amortised variational inference is used to train meta model and base model jointly\n- The meta model outputs task specific dropout rates\n- This results in a conditional dropout posterior akin to variational dropout\n- A *variational prior* over task-dependent dropout rates allows simple calculation of the KL-divergence\n- The approach is evaluated in 4 different problem setting. SOTA or competitive in most.",
            "main_review": "## Summary\nThe authors propose Neural Variational Dropout Processes (NVDPs), a novel approach to meta-learning. First, the task of meta-learning is interpreted as performing Bayesian inference of the model parameters under different environments (multi-task learning). For training, the authors perform approximate Bayesian inference by optimizing a variational lower bound on the multi-task likelihood (eq. (1)). Two neural networks are trained: a base neural network with deterministic parameters that is used across tasks, and a meta neural network that outputs task-specific dropout rates for the weights of the base network given a small support set of data points S (amortized posterior). The authors link their approach to variational dropout (Kingma et al., 2015), but utilize Bernoulli dropout instead of a Gaussian approximation. For regularization the authors use a 'variational prior' that is essentially the aggregated posterior over all data points in a task. This formulation of approximate posterior and variational prior allows the authors to derive a KL-term that is independent of the base network's deterministic parameters (appendix B).\n\nThe authors place their work in the context of model-based approximate posterior approaches (Gordon et al., 2018; Garnelo et al., 2018a;b; Kim et al., 2019; Iakovleva et al., 2020) which all utilize a formulation akin to (eq. 1), with a semantic differences in the outputs of the approximate posterior. The authors claim that their formulation overcomes the scalability problems of VERSA (Gordon et al., 2018), and avoids posterior collapse as is sometimes observed in Neural Processes (NPs) (Garnelo et al., 2018a;b).\n\nThe proposed method is evaluated on several regression and classifications problems: \n\n1. A 1d regression problem where tasks are sampled from a Gaussian Process\n2. Active learning on the above task\n3. Image completion on CelebA, MNIST, and Omniglot (only test)\n4. Few-shot classification on Omniglot and MiniImageNet\n\nIn all problems domains, the results suggest either state-of-the-art or compentitive performance of NVDPs as compared to other approaches.\n\n## Opinion & Recommendation\nI am in support of accepting this paper. The paper makes a nice connection between meta-learning and variational dropout, resulting in an overall elegant approach. The use of dropout for adaptation of a model to different tasks is (as far as I am aware) novel, which would already justify publication. NVDPs are closely related to Neural Processes (NPs). However NPs can suffer from the fact that the outputs of the approximate posterior are ignored by the downstream decoder model. This is not the case for NVDPs as they directly modify the decoder through the dropout process.\n\nThe presented results give a mostly positive impression of the approach. Judging from the image completion problems, however, the limitations of all compared approaches (including NVDPs) becomes apparent. Given a fully sampled support set the resulting image reconstructions are still very blurry. See Kim et al. (2019, fig. 4a) for much more crisp reconstructions. The author's claim \"On the other hand, the samples from NVDPs exhibit sharp reconstruction results while also showing a reasonable amount of variability.\" is in my option an overstatement.\n\n## Questions\n- The authors mention at the end of section 3 that the local reparametrization trick is used for variance reduction. But it looks like it is still necessary loop over different tasks *t*. Is this true? If so, could you discuss the computational limitations of this for large *T* ?\n- Eq. (3) gives a low rank approximation of the dropout rate *P*. It is a product of 3 Bernoulli probabilities. Could you make some comments about the numerical stability of this approximation?",
            "summary_of_the_review": "I recommend acceptance of the presented paper. A novel idea, combined with a good practical justification, and strong results make this paper worth for publication.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors present an interesting model-based meta-learning approach, Neural Variational Dropout Processes (NVDP), by constructing a conditional dropout posterior to predict task-specific dropout rates of model parameters, conditioned on the observations. Together with NVDP, the authors also propose a new type of prior, the task-specific variational prior, approximated by the same posterior model conditioned on the whole task data, which supports robust training. They show the NVDPs improve the model's adaptation, functional variability and generalization to new tasks empirically in the experiments.",
            "main_review": "Some strengths:\n1. The newly proposed NVDPs are more robust to the model-collapsing or overfitting issues, comparing with existing approaches.\n2. The proposed variational prior can regularize the task-specific dropout rates in the optimization of the conditional dropout posterior.\n3. Comparing with previous related approaches, the complexity is greatly reduced, making the algorithm more applicable to larger data set with scalability.\n\nSome weaknesses:\n1. This approach still falls into the SGVB framework when coming to the optimization of the variational parameters, using the re-parameterization trick. This limits its novelty, and restrains the potential of this framework to investigate more complicated distributions other than Bernoulli.\n2. Recently, meta learning has been popular in some application domains, e.g., NLP. The experiments are majorly focused on highly optimized data sets, weakening its wider application on more practical tasks and usage in real scenarios.\n",
            "summary_of_the_review": "I recommend this paper to be accepted, as its novelty on the newly developed variational prior differs from existing works. This is based on the hypothesis that the conditional posterior model to approximate the optimal prior shall depend on the whole data set. This claim is supported by the assumption that the variational model conditioned on the aggregated set with larger context shall be closer to the optimal task-specific prior, than only conditioned on a subset, which is a specific task. In the experiments, the variational prior shows a robust regularization effect.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}