{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The authors propose to combine ideas from SDEs and time series modeling with stochastic optimal control to present a framework for modeling continuous-time stochastic dynamics. The reviewers are in agreement that there are several good ideas presented here and that the interface of the perspectives the authors combine toward their proposed framework is an interesting one to explore. One referee mentions valid concerns in confusing points of the details in the presentation, and the positive reviewers echoed these concerns. In particular, more details and clearer exposition are needed for the decomposition into the subproblems and the problem of many hyper parameters. Nonetheless, my overall impression after a careful read of the paper and discussion is that these concerns are addressable and are to a degree ameliorated by the author response, and that they may be viewed as limitations outweighed by the merits of the novel ideas presented here. I emphasize that all reviewers were surprisingly consistent in their assessment of the shortcomings, and I encourage the authors to take these constructive criticisms seriously in preparing a final version of this paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a framework for learning a stochastic dynamics from observed trajectories. As opposed to conventional strategies based on recurrent neural networks, the procedure learns a model of the stochastic dynamics in real space. Secondly, in contrast to recent approaches based on neural ODEs / SDEs, the training is temporally localized through windowing functions that the authors refer to as temporal privacy. The main technical innovations involve the development of new loss functions that make learning with temporal privacy tractable. ",
            "main_review": "This paper formulates a stochastic differential equation that is controlled by $M$ external agents over compact, non-overlapping time intervals. Due to the lack of overlap between the agents, there is only one agent controlling the dynamics at each sub-interval, a notion that the authors call \"temporal privacy\". The goal, as in works using the neural SDE framework, is to learn stochastic time-series data using this model. Using a stochastic optimal control formulation, the authors use this notion of temporal privacy to decompose the optimization into sub-problems which allows for separate optimization of each agent. \n\nHowever, carrying out the training of this model is nontrivial. To train, the authors first use a loss function that employs random stopping times to efficiently use the data $\\boldsymbol{y}$. The inference then proceeds similarly, propagating the sampled points according to the learned dynamics and subsequently averaging the results. A second loss that describes the evolution of the loss backward in time is used to augment this first loss. I think the connection of the second loss (MBcond) to the nonlinear Feynman-Kac theorem could be better explained in the main text. \n\nThe authors carry out several numerical experiments and achieve impressive results. In all cases the CSDE-TP significantly outperforms alternative strategies.  \n\nMinor comments:\n\nFigure 1 caption: \"It computes the ...\" what is \"it\"?\n\nTypo: \"is shoen in\"\n\n\"main idea is ... to minimize incoherence\" what does this mean? can this notion of incoherence be made more precise?\n\nThe phrase \"rich information\" comes up a few times. Perhaps it is better to simply say \"information\" \n\n\n\n",
            "summary_of_the_review": "The author introduce an original and highly effective optimization scheme for controlled SDEs for modeling stochastic dynamics. While there is not a thorough analysis of the algorithm (aside from a discussion of the theoretical optimality in the appendix) the experiments amply demonstrate that the approach is productive. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a new approach to train stochastic dynamical systems using a set of tools from stochastic optimal control theory. The methodology is built upon _controlled stochastic differential equations_ with multiple control agents that modulate the dynamical evolution. Each agent is deliberately chosen to be \"active\" in a certain time interval, which leads to so-called \"temporally private\" dynamics that allow for dynamic programming based optimization. Since the temporal segmentation of optimization objective requires arbitrary intermediate state $X_s$ with $0<s<T$ as initial values, the Markov forward condition (MFcond), which computes some sort of a mean value for future states, is proposed. To further stabilize the training and compensate for the methodology's lacking in theoretical optimality guarantees, the authors augment the original loss with the Markov backward conditional (MBcond) loss. The method is shown to outperform competing continuous-time methods on standard benchmarks. ",
            "main_review": "Strengths:\n+ The paper concerns a timely topic. The stochastic view of the continuous-time systems makes it even more interesting as ordinary differential equations based approaches dominate the recent literature for the time being.\n+ The empirical findings are strong. The model seems to excel among state-of-the-art continuous-time methods in standard benchmarks.\n+ We also empirically observe that the proposed MFcond significantly boosts the accuracy. Note that this comparison is made against the vanilla optimization objective (6). Since it is known that training continuous-time systems with long sequences suffer from optimization issues; thus, it would be more appropriate to consider an additional baseline in which the training is performed using subsequences (i.e., minibatches along the time dimension).\n\nWeaknesses:\n- Since the model addresses difficult sub-problems along the way, these sub-problems and appearing hyperparameters require a more in-depth investigation/explanation. In turn, this would allow us to appreciate the methodology and different choices the authors have made. This can be done, for instance, by simple empirical evaluations (as in Fig 4.4a) or by contrasting the proposed strategies against any simpler, alternative techniques. Unfortunately, as such, I'm not able to judge how much is gained by the proposed complicated and somewhat demanding procedures. Below are a few simple example questions:\n    - What happens when we discard the MFcond? More generally, what is the role of $\\gamma$ and how did you set it $\\gamma=0.95$?\n    - How do you pick $u$ in (3)? Does the optimization become more difficult/easy with $u$ approaching $t$?\n    - How does the value for $r$ affect the entire routine defined in (4)? Did you randomly pick $r$\n    - How does $\\epsilon$ affect the algorithm? Why is it set as in Alg.1?\n    - What if we simply optimize for shorter sequences instead of (6)?\n    - Setting the initial value to an observation (as in (6),(7)) may deteriorate the performance if the data is noisy. Is not this an issue after all? \n    - What is the justification for replacing $Z_t$ with $Z_t^\\alpha$? Since this directly violates the \"optimal agent\" requirement of (11), this particular choice needs to be carefully explained, possibly paired with a simple numerical illustration showing how it affects (10).\n- Writing and notation should be significantly improved. Below are concrete suggestions:\n    - Since (1) involves several terms, more verbal explanation would help the reader very much. Also, short explanations on \"Markov closed-loop feedback control\" and \"$\\mathcal{F}_t$-adapted process\" are needed.\n    - A cartoon drawing (or anything similar to Figure 1) of Sec 3.1 would be nice.\n    - What are the expectations with respect to in (3)?\n    - A verbal explanation of (7) would be very useful.\n    - The caption of Fig 1 should be improved. It is not clear what \"other trajectories\", \"it\", and \"empty parts\" refer to.\n    - Which distribution is the expectation in (8) with respect to? Does not $\\mathbf{y}(\\cdot)$ refer to data trajectories? \n    - It would be much easier to follow if the paragraph above \"Network inference\" is given much earlier than equations.\n    - The first paragraph in page 6 is very long and difficult to follow.\n    - Sec3.4 could start with a reminder of MBcond as it is not related to the methodology described in Sec 3.3.\n    - What exactly does \"cancels the effect of martingales in the diffusion term\" mean?\n    - Writing (12) in terms of differentials (as in (10)) instead of an integral would help the reader to contrast (10) and (12).\n    - What are the inputs to $\\mathcal{L}_f$? Just $\\alpha$'s as in (14) or $\\mathbf{y}(\\cdot)$ as well as in (8)? Also, why is the $\\alpha^i$ separated from the other $\\alpha$'s in (14)?\n    - Better titles appearing in Fig 4a legend (maybe reflecting the number of agents) would be nice.\n\nMore detailed comments and questions:\n- Why are the Q1 and Q2 important? Q1 needs further explanation and Q2 requires an explanation on the deficiencies of neural controlled differential equations (NCDE).\n- NCDE should be mentioned in A1 and the proposed method should be motivated in comparison with NCDE.\n- What prevents the agents from collapsing to a single mode?\n- Why is only the train MSE plotted in Fig 4a? Also, it would be nice to see the model trained until convergence.",
            "summary_of_the_review": "The paper brings several interesting approaches to address the sub-problems that appear due to the construction of the SDE controlled by multiple agents. As much as I can follow, the equations are accurate and the methodology does not suffer from any theoretical deficiency. On the other hand, this makes it very difficult to evaluate how important different assumptions and building blocks are. As such, the model seems like a magical combination of a number of blocks but it would be much better to isolate each block individually and show how it contributes to the overall performance. Also, I believe the writing should be significantly improved (see my detailed notes in the \"main review\"). Although the method is interesting and timely, I recommend a reject and re-submission after the above-mentioned points are addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to model a time series directly with a neural controlled stochastic differential equation with multiple control agents. It introduces a concept of temporal privacy, which defines the attention of each control agent to be a certain time interval. Then the authors introduce Markov dynamic programming to efficiently minimize a loss function defined in terms of trajectory of the stochastic differential equations. In order to make the proposed temporal privacy work, the authors proposed two loss function: 1) MFcond minimizes the incoherence between the future estimates starting at any intermediate time stamp, 2) MBcond is to ensure the theoretical optimality of the control agents. ",
            "main_review": "The paper seems to be well-structured and the math seems solid. The proposed loss function MFcond is a principled way to incorporate the intermediate observations into the dynamics. The experiments shows significant gains over several state-of-the-art baselines. I have a few comments and concerns:\n - The motivation behind modelling a time series such as stock price as controlled by multiple agent is not clear.\n - It is not clear to me whether multiple agents can be active at the same time. From the definition and the separation of B and B' in equation 4, it seems that the attention interval of two different agents are exclusive, but the example in figure 4 seems to show that agents control overlapped intervals.\n - This paper does not clearly define how the attention of each agent is set for empirical datasets. From the context it seems that each agent controls one interval between two empirical observations, but this need to be explicitly defined.\n - The figure 1-(b) shows a failure case where all simulated trajectories are far from the ground truth, yet the \"averaged\" prediction operator $\\mathcal{T}_{s,t}^{\\alpha}$ is close to the ground truth. Doesn't this mean that l2 distance between the ground truth and the mean predictor is a poor loss function?\n - In figure 1 the observation is plotted in a solid black line. I think scatter plot is more appropriate since the observations are at discrete time.",
            "summary_of_the_review": "This paper proposes some novel ideas that works well in practice. I don't fully understand this paper, but I feel it deserve a broader audience.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose a novel method to model stochastic dynamic of complex time series. They build connections between neural SDE and stochastic optimal control theory. By using the proposed MFcond and MBcond losses, they can train control agents to learn dynamics of time series more accurately than existing methods. ",
            "main_review": "The paper looks pretty solid and novel in the theoretical part. The MBcond loss built on the forward-backward stochastic differential equations and implemented using deep neural nets are new to the area of time series analysis/prediction. The authors also explain/analyze connections between the proposed model and the HJB equation and the non-linear Feynman-Kac theorem. \n\n \n\nMajor issues: \n\n1. Motivation of temporal privacy is not clear. For each agent, it seems that it will only be valid for a specific time interval. The authors may want to explain when a control agent will kick in and when it will be invalid. At the time a control agent kicks in, will it cause jump of the stochastic process and is it still a Markov process in this case? \n\nIn addition, the 'temporal privacy' constrains the agents cannot simultaneously control the dynamics, which requires the equations (1) be a specific controlled stochastic differential equation. Therefore, the multiple control agents can be considered as one single agent.\n\n2. Eq. (4), why the expected terminal loss J given X_t^{\\alpha} is optimized using \\alpha^{-r}? Due to causality,  I think it should only depend on partial control agents which are valid after time u. \n\n3. Page 4, Definition 1. For each observation y, the dependent forward loss is calculated till time t as shown in the integration. So observations sampled at the time far away from t usually will incur larger losses due to drift and diffusion effects accumulated over longer time span. So observations will have different weight or impact on the training process, depends on time spans between observations and test data at time t. I understand that the authors want to evaluate prediction impact by starting CSDE from each observation on the test data. However, I am wondering whether it is the best way to define forward loss. Can we just calculate the forward loss between any two consecutive or adjacent observations y? By this way, we can still make full use of all observations to train control agents. \n\n4. Eq. (10), the authors jump to FBSDEs too quickly and it is hard for readers not familiar with this topic to understand. A diagram showing correspondence between forward/backward stochastic processes X/Z and related functions will help readers a lot. \n\n5. Time series data usually show characteristics like level, trend, cycles and holiday/promotion effects. Also in many real applications, time series data come with covariates which may be very predictive for target variables. State-of-the-art algorithms for time series prediction like DeepAR, NBeats, Prophet and Transformer, to name few, take some of the factors into account in the modeling process. It would be interesting to see discussions on how the proposed method handles these factors. Is it possible that different control agents are trained to be experts specialized on different patterns (e.g. trend and cycles) ? \n\n6. In the experiment, an ablation study on the MFcond loss and MBcond loss is encouraged, e.g., use only MFcond loss or MBcond loss to evaluate the algorithm.  It will help the readers to understand how important each loss is. \n\n7. On page 4, in the paragraph 2) Theoretical Optimality, the authors should point out the regularity conditions for the function $V(\\cdot)$ while using the Verification Theorem or specify the regularity conditions on the function  $l(\\cdot, \\cdot)$ and $$\\Psi(\\cdot), and I think most of the commonly used metric functions in machine learning theory would confirm enough regularity.\n \n8. In Section A.1, I cannot see the necessity to mention the reverse SDE, which has nothing to do with the stochastic control theory the authors referred to in the work. And I think the comparison between the reverse SDE and BSDE is not a related work, I cannot see the motivations or ideas they provide in the work.\n\nMinor issues: \n\n1. Sec. 3.1, define or list reference of \"controled F_t – adapted process\". \n\n2. Sec. 3.1, define variables d and m. \n Following equation (1), the definitions of the coefficient functions b,\\sigma,\\alpha^i have typos,\nshould be written as $b:[0,T]\\times \\mathbb{R}^d\\times\\mathbb{A}\\to \\mathbb{R}^{d}$,  $\\sigma:[0,T]\\times \\mathbb{R}^d\\times\\mathbb{A}\\to \\mathbb{R}^{d\\times d}$ and $\\alpha^i:[0,T]\\times \\mathbb{R}^d\\times\\mathbb{R}^m\\to \\mathbb{R}^n$. And here the admissible control set  $\\mathbb{A}$ is not clarified until I read the appendix.\n\n3. Page 3, first line, what is k? \n\n4. Eq. (3), T is replaced by u in the integration. Please explain the replacement. Also I think J(t, X_t^{\\alpha}) should be j(T, X_t^{\\alpha}) . \n\n5. Eq. (6), there is no y(.) shown in the equation. \n\n6. Eq. (9), what is s in the equation and what is the difference between s and t_s? \n\n7. In the last paragraph on page 2, the 'sub-intervals' of the ordered times $\\mathbb{T}$ is confusion. I guess the authors mean: the closed sub-intervals without intersection interiors, or just $[t_k,t_{k+1}], 1\\le k\\le N$.\n\n8. Another typos in line on page 3: in the expression $w_r(s) = \\mathbf{1}_{t\\le s\\le u}$,  $t, u $ should belong to $\\mathbb{T}$, and $1\\le k \\le N$ should be removed.\n\n9. In Definition 1, the stopping time $\\tau_s$ is not well-defined, and I guess the author means : $\\tau_s := \\inf_{t}\\{t>s: l(t,\\mathcal{T}_{s,t}^\\alpha)>\\epsilon\\}\\bigwedge t$, and the parameter $\\epsilon$ should be mentioned in the definition.\n\n10. On page 7, in Section 4, in Table 1, what does $$\\Sigma^i mean? Does the Vanilla case indicate that the diffusion term has the same form as the drift term and depends on the controls as well?",
            "summary_of_the_review": "The authors propose  a new method for learning dynamics from continuous-time data using neural Markov controlled SDE. It connects stochastic optimal control theory and deep learning which is pretty novel in my opinion. So my recommendation is \"accept\". ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}