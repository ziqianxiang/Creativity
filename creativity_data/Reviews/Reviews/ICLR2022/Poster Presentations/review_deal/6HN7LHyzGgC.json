{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper considers the important problem of performance degradation under distribution shift and proposes a simple yet effective method to alleviate this problem. They do so by considering feature statistic to be non-deterministic and rather a multivariate Gaussian distribution.  The model can be integrated into networks without additional parameters and experiments show that it works better than BN as well as if the assumed distribution was uniform. The latter was added during rebuttal period.\n\nThere were two main concerns regarding distinguishing the work from AdaIN and baseline that were addressed during rebuttal and some parts of the paper were re-written to address repetition."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors proposed to model uncertainty with multivariate Gaussian distribution for better network generalization. Experiments on multiple benchmark datasets show improved result. Several visualizations and analysis also illustrate the effectiveness of the proposed approach.",
            "main_review": "Strength:\n1. The proposed multivariate Gaussian based approach makes sense and technically sound to me. \n2. Extensive experiments, ablation study, and sufficient analysis validated the approach empirically.\n3. Writing is good and easy to follow.\n\nWeakness:\n1. The proposed approach seems to be a small change to AdaIN. \n2. Instead of sampling from a Gaussian distribution, can we just normalize it to be an uniform destruction? Similar idea to Batch Normalization, normalizing the distribution to a standard one seems to be an easier choice.",
            "summary_of_the_review": "Based on the above analysis, I am slightly leaning towards acceptance for now.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the problem of out-of-distribution generalization by modeling uncertainty in feature statistics. It improves the network generalization ability by modeling the uncertainty of domain shifts with synthesized feature statistics during training. Instead of being deterministic values, the feature statistics are hypothesized to follow a multivariate Gaussian distribution. The proposed method is tested on various tasks including image classification, semantic segmentation, and instance retrieval, and shows strong performance compared to state-of-the-art methods.",
            "main_review": "1. Overall, the paper is well written and nicely organized.\n\n2. The proposed method is intuitive and simple. It adds randomness towards the feature statistics to handle out-of-distribution testing. The randomness is also drawn from the observed uncertainty estimations from data, with the hope that the uncertainty estimations from mini-batch could provide variation range to cover the possible unknown domain shifts. The method is simple yet very effective according to the experimental results. This is a big plus for the method.\n\n3. The method is tested on various benchmarks, and shows strong performance compared to recent state-of-the-art methods.\n\n4. Some parts of the paper feel repetitive. For example, Section 3.1, related work and introduction have similar contents, which could be reorganized to improve the presentation and writing.",
            "summary_of_the_review": "In summary, the method is simple yet effective, showing strong performance on various benchmarks.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed a feature augmentation method for domain generalization by generating probabilistic feature statistics, where the distribution of feature statistics enlarges the feature domain with possible domain shifts. The proposed method is evaluated on various vision tasks, including image classification, semantic segmentation, and instance retrieval. It achieves competitive results compared with recent domain generalization methods. ",
            "main_review": "Strengths:\n\n(1) The proposed method can capture the uncertainty of the feature domain, which is very important to address the domain shift issue.\n\n(2) The proposed method can achieve some improvement on cross-dataset evaluation for multiple vision tasks compared with other domain generalization methods.\n\n(3) This paper is well written and well-organized. \n\nWeaknesses:\n\nFor the method:\n\n(1) The proposed method is simple and is similar to batch normalization. Can traditional batch normalization method also improve model generalization ability? Is the proposed method significantly better than traditional batch normalization?\n\n(2) Generating the class-preserving augmented features is important since we do not want the generated features to be far away from the original class domain. The proposed feature augmentation method did not consider the class-preserving issue, i.e., how to guarantee the augmented feature is still located within the feature domain of the same class. \n\n(3) We may expect more discussion on why the domain generalization can be improved by modeling the probabilistic distribution of features. Is there a specific theory or reason? \n\nFor the experiments:\n\n(1) The authors can provide some within-dataset experiments for comparison.\n\n(2) According to Eq.(3) and Eq.(4), the variance of feature statistics is related to batches, it is better to perform an ablation study of the influence of batch size.\n\n(3) For the image classification problem, the performance of DSU drops on Photo style compared with the baseline. Are there some explanations?\n\n",
            "summary_of_the_review": "Overall, this paper is well-written and well-organized for both theories and experiments. The idea is interesting and is easy to implement. The experiment results are empirically sound. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}