{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper studies the behavior cloning based strategies of offline RL algorithms in different type of environments and reports that performance primarily depends on model size and regularization. The results contradict some of the earlier claims, and the authors conjecture that model size and regularization characteristics can explain past results.  \n\nDuring the review period, the reviewers agreed that the paper has certain merits, and on the other hand, they also raised some concerns, regarding some missing technical details, whether the empirical finding could be trusted, the generalization of the findings to more scenarios, and the comparison with some highly related papers. The authors did a good job in their rebuttal, which removed many of the above concerns (although not all) and convinced the reviewers to raise their scores. As a result, we believe it is fine to accept the paper (although somehow like a weak accept)."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper studies the behavior cloning based strategies of offline RL algorithms in different type of environments and reports that performance primarily depends on model size and regularization. The results contradict some of the earlier claims, and the authors conjecture that model size and regularization characteristics can explain past results. The paper also discusses additional insights such as the importance of conditioning and simple validation based evaluation fail to generalize. ",
            "main_review": "It is good to see papers which deep dive into existing methods and study which aspects of the algorithm contribute to improved performance. \n\nMy major concern with the paper is the RvS algorithm is not described completely for me to understand the experiments. It appears that equation (1) in the paper is being used for gradient ascent for policy \\pi parameterized with \\theta. But the accumulation of rewards given by f() is in the denominator, so I do not understand how maximizing this objective leads to a policy that maximizes the rewards. The equation is unlike those given by papers cited (Kumar et al, Srivastava et al). \n\nGiven that this is supervised learning, the basic steps of training the model is also unclear. Essential hyper-parameter values such as learning rate is not provided. Given that there is no good way to determine overfitting as per validation set experiments in the paper, it is not clear how is training stopped in the first place. The number of epochs or steps of gradient update is not provided for any of the methods.\n\nGiven the above observation, it is difficult to believe the results reported in the paper. In addition, the most important insight from the paper is that model capacity and dropout play a key role in final performance. But, given these are contradictory, no further investigation is done to understand why this is the case. Instead, the paper leaves the subject with a conjecture. \n\nThe other aspects of the paper are not entirely surprising, and make sense.",
            "summary_of_the_review": "The algorithm under study is not described completely. Important hyper-parameters are missing. Because of this, the final results are circumspect.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper considers design choices involved in offline RL approaches that consider a reduction to weighted/conditional behavior cloning, a class of approaches referred to as Reinforcement Learning via Supervised Learning (RvS). The paper studies various issues including expressivity of policy architecture, regularization, choice of conditioning variables (e.g. based on goals or rewards). \n\nAt a high level, the takeaways are (a) RvS approaches are successful when using neural networks with appropriate capacity and regularization, (b) with appropriate conditioning variables (goal/reward based conditioning), (c) RvS can obtain policies that exhibit compositional behavior by conditioning on appropriate events.",
            "main_review": "=== post author response: I have increased my score, but, I retain concerns about comparisons with more expressive classes of approaches such as the decision transformer.\n\n1. While the paper presents interesting insights, I am not sure how much these findings can be generalized particularly in offline RL for locomotion tasks, where, typical benchmarks consider all 4 types of data collection strategies (random, medium, medium-replay, medium-expert) -- this paper considers only the latter two variants for their analysis. The other two variants are highly challenging in the offline RL context and require leveraging on elements of pessimism/conservatism to guide policy learning. The paper requires to present studies on these tasks to have a more holistic view of applying RvS methods to offline RL.\n\n2. The comparison of this paper with the Decision Transformer is not well accounted for. The Decision Transformer work of Chen et al. (2021) can be viewed as a general framework used for synthesizing a variety of behaviors through conditioning on the rewards (or other events) during inference time. Such a perspective isn't quite considered by this work, and I am not sure how straightforward it is to programmatically generate a variety of behaviors without recourse to density modeling and sequence modeling. To present a fair comparison to works such as Decision Transformer, the paper needs to present results where a learnt policy can produce behaviors based on appropriate conditioning at inference time (and not just restricted to training time).",
            "summary_of_the_review": "The paper certainly takes a step forward with regards to understanding what components of RvS algorithmic frameworks matter to obtain satisfactory results - but, the paper falls short because (a) the paper doesn't present results with all types of datasets in the D4RL MuJoCo tasks (and this is standard in the offline RL literature), (b) the comparison to decision transformer isn't fully sketched out because the decision transformer framework is more general in that it offers the ability to generate a variety of behaviors at inference time by changing the conditioning variable during inference.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the importance of the design decisions for supervised learning type reinforcement learning algorithms. Through extensive experiments, find that more complex design choices, such as the large sequence models and value-based weighting schemes used in some prior works, are generally not necessary. Our results show that carefully designed RvS methods can attain results that match or exceed the best prior methods across a range of different offline RL benchmarks, including datasets with little or no optimal data.",
            "main_review": "This paper provides empirical studies of offline RL via supervised learning. \n\n## Strengthens\n\nThe biggest findings are two aspects:\n\n* The authors find more complex design choices, such as the large sequence models and value-based weighting schemes used in some prior works, are generally not necessary, which is important in my opinion; especially, obtaining good performance using simple feedforward neural network is interesting;\n* The empirical results show that carefully designed RvS methods can attain results that match or exceed the best prior methods across a range of different offline RL benchmarks, e.g. when goal or reward is carefully chosen.\n\n## Weakness\n\nSome of the expositions are not very clear: \n\n1. The explanation of the objective (1) is not clear, for example, when $f$ uses RCBC, is $|f(\\tau_{t+1}:H)|$ simply $1$? Also, why this objective should be used remain elusive.\n\n2. Other than the goal and reward aspects, what are the other aspects that matter?\n\n\n\n\n\n\n\n\n",
            "summary_of_the_review": "In general, I like the new perspective this paper presented. In particular, the finding \"find that more complex design choices are generally not necessary\" provides a different way of thinking offline RL. Therefore, I prefer to accept at the moment.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper investigates different variants of behavior cloning. The methods investigated aim at achieving better policies for use in offline RL through supervised learning than the average behavior contained in the data. The methods studied are categorized and examined against three to four benchmark problems.",
            "main_review": "Strengths\\\nThe paper provides a broad overview and makes a categorization.\nContributions to the discussion are provided, that might influence future research.\n\nWeaknesses\\\nThe finding that regularization of the learned policy is particularly significant suffers somewhat from the lack of a recipe to perform this regularization offline.\n\nOther comments\\\nI find the sentence \"For example, any experience is optimal for learning to reach the final state in that experience.\" too ambiguous.\n\nThe Introduction talks about a \"reward parameter\", I think at this point it should be made clearer what is meant by this.\n\n\"cloning(Codevilla\" -> \"cloning (Codevilla\"\n\nIn the Related Work section, prior work is classified by whether it uses a value function, uses a dynamics model, or uses uncertainty quantification. It remains unclear that (Yu et al., 2020), (Shen et al., 2021) and (Kidambi et al., 2020) use both the dynamics model and a value function and only (Argenson & Dulac-Arnold, 2020) uses the dynamics model without a value function. At this point, it is helpful to also mention [1] and [2] as other examples of using dynamics models without using the value function.\n\n[1] S. Depeweg et al., Learning and policy search in stochastic dynamical systems with Bayesian neural networks, ICLR 2017\n\n[2] P. Swazinna et al., Overcoming model bias for robust offline deep reinforcement learning, EAAI 2021\n\n\n\"actions s_t\" -> \"actions a_t\"\n\nThe axis labels in figures 1-5 are very small. It would be ideal if it had the same size as the caption font.\n\nThe text \" RvS methods can often attain results that are comparable favorably to the best prior methods,\" should be corrected.\n\nPlease check the bibliography for accidental lower case letters, like „rl“, „q-learning“\n\n",
            "summary_of_the_review": "The paper gives a good overview of different variants of behavior cloning. The contributions to the discussion may be helpful for future research. However, my feeling is that there is little reliable insight provided. Therefore, I think it is also possible that the benefit is small.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}