{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "There are numerous known methods for memory reduction used in CNNs.\nThis paper takes two such---quantization (Q) and random projection (RP)---and applies them to GNNs.  This is a novelty, but I agree with the reviewers: on its own this novelty would not be \"surprising\" enough to report at ICLR.  \n\nThe paper further goes to show empirically that these methods, when applied to a reasonable set of datasets, do indeed produce their predicted memory reductions (unsurprising) with a small ($\\approx 0.5\\%$) drop in accuracy (surprising, in the sense of not being something one could predict without doing the experiment).\n\nAll of the above is in one sense \"just engineering\", with only a small inventive step.  Any real-world deployment of GNNs would, if an army of engineers were available, naturally implement quantization and RP in order to see what kind of improvements they might make.  This would be just two more hyperparameters (R,B) to add to the sweep, and the deployment would vary them until the required accuracy was achieved in the minimum time (OOM is a red herring - one would vary batch size, other compression, or ship values to CPU in order to make progress).\n\nHowever, \"simply adding two more hyperparameters\" is a significant increase in the deployment burden, which is where the paper's third contribution comes in: the theoretical analysis of the effects of the two processes, with straightforward but nonobvious calculations of the effect on gradient variance of the two processes, and, usefully, their interaction.  The value of this theory is twofold: first, it gives us new tools to analyse such processes; and second, it allows us to be much more judicious in the selection of these hyperparameters.\n\nIn all, the reviewers' objection of no great novelty in porting ideas from CNN  to GNN is sustained; but the authors' claim as to the value of the theory is sustained, and no reviewer provides prior art to dispute the novelty of the theory calculations.\n\nThe revised paper has already expanded the key sections in Appendix E, and added welcome experiments which strengthen the paper.  I would encourage a final copy (and certainly the poster presentation) to emphasize some of the insights over the raw experimental numbers.  As the authors hint, those numbers are subject to vagaries of what PyTorch happens to implement, while the underlying analyses are a little longer lasting.\n\nSome other comments: \n\nA lot of discussion time was spent on the question of whether 0.5% is negligible.  This is entirely application dependent, and is part of the hyperparameter/architecture tuning process.\n\n  - the extra time overhead of swapping \"can go up to 80%, which is not feasible in practice\".\n  Not so: if choosing between OOM or 1.8x slowdown, I will of course choose the latter.\n  - \"for a fair comparison, we do not change any hyperparameters\"\n  Again, not relevant: in a real application (which is where this paper contributes), we of course change the learning rate when batch size changes.\n  - \"the accuracy drop of sampling may be greater than EXACT\"\n  Again, whether that drop is too much depends entirely on the actual application.  \n\nAnd please do take a look at typos/grammar/English etc."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper explores the training of GNN with compressed activation maps. It provides an optimized GPU implementation and comprehensively studies the trade-off among the memory saving, time overhead, and accuracy drop. \nExperimental results show that the proposed framework can reduce the memory footprint of activations by up to 32x with only 0.2-0.5% accuracy drop and 10-25% time overhead.",
            "main_review": "Strengths:\n1. The first work that explores the compressed activation in GNN training.\n2. Provide comprehensive empirical evaluation and theoretical analysis of the trade-off among memory consumption, accuracy loss, and time overhead\n3. Impressive results\n\nWeaknesses:\nThere is no major weakness of this paper, but adding more comparisons against other techniques would be great. \n1. Adding experiments to compare EXACT with other memory-saving techniques (e.g., gradient checkpointing, swapping)\n2. Adding experiments to compare EXACT with sampling approaches given a fixed memory budget. Maybe you have to choose slightly different settings of different approaches due to their different memory requirements.\n",
            "summary_of_the_review": "This paper explores a novel direction and gets impressive results. I strongly recommend accepting this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work attempts to train GNN models with reduced memory requirements. Normally when training, we need to store activations for calculating the gradients on the backwards pass, and we typically keep these at full (32-bit) precision. This work argues that we don't need to do this, and instead we can significantly reduce the memory footprint at training time by keeping low precision activations for the backwards pass. They use two approaches for this: quantization, and random projections",
            "main_review": "**Strengths**:\n- I think the work is solving an important problem -- although I would note that we have existing works in this area, which solve the problem pretty well.\n- The results are good, especially when considering quantization only.\n- The authors did a good job choosing datasets -- they go up to OGB-products, which is nice.\n- The choice of models is good -- and I appreciate that the authors included GCNII to evaluate how their method scales with depth. I'd be delighted if they added GAT as well, though :-) -- that's really part of the reason we want their method!\n\n**Weaknesses:**\n- I am not entirely sure what the benefit of this approach is compared to sampling or historical embedding approaches is. Could you explain what is the benefit of this approach over others?\n- It's not really clear what the benefit of random projections are, given that they cause significant accuracy degradation. I am aware that they do improve the compression ratio, but I would argue that in most cases there's little incentive to use it.\n-If we consider just quantization, then there is little novelty to this work I would argue: I am not sure what is novel over the Chakrabati paper at NeurIPS 2019 is. I am aware that they focused on CNNs, but there is little reason to believe that their method would not work for GNNs.\n\nIn fact, I did actually implement the Chakrabati method myself for GNNs -- and unsurprisingly it does work. I never wrote up the results (!) or studied it too carefully. It's useful that this paper has done that though. I am unsurprised it is effective down to 1-bit integer by the way -- the gradients are much \"smoother\" for GNNs than for CNNs, as the effective batch size is very large.\n\n- There is an interesting work that was accepted to NeurIPS this year that you should compare to: \"AC-GC: Lossy Activation Compression with Guaranteed Convergence\"; they look at CNNs and Transformers, but there's no reason why you can't apply their technique to graphs as far as I can tell.\n\nIt is unfortunate that my review seems rather negative. I actually don't have a particularly negative view of the paper: I just have many questions (and unfortunately, they are related to novelty). I don't think that they're unanswerable, and I consider myself to be pretty reasonable at rebuttal if you can present new evidence / answer some questions.\n\nI would make the case that novelty is not the absolute most important thing -- but I am a little skeptical of the contributions to the literature presented by this work. However, the work is likely to be of good use to practitioners -- it's not difficult to implement many parts of this, and the results do seem impressive.",
            "summary_of_the_review": "I am leaning towards a reject for now, but I am happy to be convinced. I am aware of how crushing it can be to authors to read these things -- and I would like to say that I think the work has strong value to the community -- but it is not necessarily a strong piece of novel research. \n\nPost rebuttal:\n\nI am not convinced that my original view has changed regarding novelty. I strongly recommend that the AC rejects the paper since I do not believe the research contribution is sufficient.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes EXACT, a framework for training GNNs with compressed activations with two methods: quantization and random projection. The primary design objective of EXACT is to trim down the memory consumption in GNN training while maintaining acceptable training speed and accuracy. EXACT achieves significant memory savings with 0.2-0.5% accuracy drop and 10-25% slowdown of training throughput across five models and five graph datasets.",
            "main_review": "Limited memory capacity of GPUs impose severe constraints on the model/dataset size of GNNs that can be trained. That said, this paper tackles an important problem. On the positive side, the proposed technique is simple and based on proven ideas for tensor compression such as quantization and random projection. It was a pleasant surprise that those simple techniques work well to reduce the activation size with acceptable accuracy drops and computational overhead. Evaluation demonstrates promising results, which backed by solid theoretical analysis. The paper is well written.\n\nHowever, I still have several questions and concerns as follows:\n\n* *Overall Memory Savings* - It's a bit misleading to report memory savings just for activations (Table 3). The bottomline number should be overall memory savings including other objects such as input data. Also, in Section 5.3, the authors claim EXACT reduces the memory usage in training GraphSAGE on obgn-products to fit in 11GB. However, a sizable portion of memory savings come from AMP, which is orthogonal to EXACT. The authors should separately report memory savings of EXACT from those of AMP (and other techniques, if any).\n\n* *Scalability* - How effective would EXACT be if node-wise or layer-wise sampling methods were used in a mini-batch setting?  Would input data may take a dominant portion of memory usage once the activation map shrinks after aggregation? Also, how scalable would EXACT be for larger (e.g., billion-scale) datasets and/or deeper GNNs?\n\n* *Design Rationale* - EXACT applies random projection first, followed by quantization during the forward pass. What if we switch the order? Is there any convincing reason why RP-followed-by-quantization is preferred over quantization-followed-by-RP?\n\n* *Insight into Activation Compression* - The GNNs in this work are very resilient to activation compression with quite small accuracy loss regardless of their depths. Can you provide some insight into why GNNs are so robust against this somewhat extreme compression of activations?\n\nNits:\n\n* (Section 5) \"it applies the random projection *following by* a 2-bit quantization\" -> followed by \n* (Section 5.2) \"*From Table 3*, the overhead of EXACT (INT2) is roughly 12% âˆ¼ 25%.\" -> \"From Figure 3, ...\"\n",
            "summary_of_the_review": "This paper introduces a simple yet effective technique to reduce the memory usage in GNN training, which demonstrates promising results. However, I still several questions/concerns for evaluation metric, scalability, design rationale, and key insight. I'd ask the authors to address them in their rebuttal.\n",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}