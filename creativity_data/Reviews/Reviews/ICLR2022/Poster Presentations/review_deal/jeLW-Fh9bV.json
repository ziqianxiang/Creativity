{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The main contribution of this paper lies in the novel setting that is being considered: offline data without rewards is combined with meta-training tasks to quickly adapt to new long-horizon tasks at meta-test time. Within this setting, it is shown that the combination of SPiRL and PEARL outperforms the individual algorithms. The technical contribution is limited, as now new methods are introduced. Nevertheless, the setting considered is interesting and the empirical evaluation is solid. For these reasons, I recommend acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work considers a new setting where you can leverage both 1) offline \"play\" data that does not contain reward or task labels, and 2) meta-training tasks in order to quickly learn new tasks at meta-test time. To solve this setting, this work proposes to learn skills from 1) by using the SPiRL algorithm, and then learn a hierarchical policy on top of the learned skills over the meta-training tasks 2) using PEARL. Empirically, the combination of SPiRL and PEARL outperforms both SPiRL and PEARL on their own on a 2d maze navigation task and on a robot kitchen task.",
            "main_review": "The main strength of this work is its strong experimental results in the new setting it proposes.\n- The proposed algorithm improves substantially over reasonably chosen baselines.\n- Additionally, this work includes two relatively informative ablations which illustrate how the proposed approach performs as a function of number of meta-training tasks, and as a function of the alignment between the meta-training tasks and the meta-testing tasks.\n\nOverall, there are no serious flaws with this work.\n\nAt the same time, the main contribution of this work is just a straightforward combination of two existing algorithms, SPiRL and PEARL. Considering and proposing the new setting of combining offline play data with the meta-RL setting is interesting, but feels a little incremental.\n\nAdditional minor comments:\n- The notation in this work is difficult to read in places. For example, $z_T$ is used to denote the task embedding vector for PEARL, while $z_s$ is used to denote the skill variables. These are completely separate concepts, but the notation makes them seem related. Similarly, $\\mathcal{T}$ is used to denote a set of tasks, which looks very similar to the trajectory $\\tau$, even though they are completely different. I would recommend using more visually distinct notation.\n- The PEARL-ft baseline seems somewhat strange to me. PEARL already prescribes the meta-test time behavior -- why should we change that to SAC instead? PEARL also seems to perform surprisingly poorly on its own, although it appears that this might just be due to the horizon length and reward sparsity. Approximately how many timesteps is each of these tasks?\n- This work is missing citations to some of the foundational meta-RL works, such as [1, 2], as well as meta-RL works that do consider sparse reward tasks, such as [3, 4].\n\n[1]: RL2: Fast Reinforcement Learning via Slow Reinforcement Learning. Duan et al., 2016.\n\n[2]: Learning to reinforcement learn. Wang et al., 2016.\n\n[3]: Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices. Liu et al., 2021.\n\n[4]: NoRML: No-Reward Meta Learning. Yang et al., 2020.",
            "summary_of_the_review": "Overall, I think this work is quite reasonable and does not have any serious issues. I find it only borderline on the accept side, due to its incremental nature.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a meta-RL method to learn to solve long-horizon tasks with sparse rewards efficiently. To do so it builds on previous works, mainly SPiRL, to learn a set of skills and skills prior from an offline dataset. Jointly with the skills, a high-level policy generating action in the learned latent space, is trained with meta-RL. The goal of this policy is to learn to compose learned skills to solve new tasks efficiently. Given a small set of target task trajectories, the policy can meta-learn from it and learn quickly to solve the target task. Experimentally the proposed method is shown to be more sample-efficient on two tasks, maze navigation and kitchen manipulation.",
            "main_review": "## Strengths\n\nThe paper is well written and the method is clearly explained. To my understanding it is a clean extension of SPiRL where the main point is to formulate the high level policy learning as a meta-RL problem. \n\n* The meta-RL formulation allows to exploit available target task trajectories at test time efficiently. Indeed as shown in the experiments of Figure 4, when using 20 episodes to warm up the high-level policy, the initial success rate of SiMPL is ~2x higher than other methods after 20 rollouts on the target task environment.\n\n* The meta-training task distribution analysis shown in Figure 6 is also an interesting experiment. Figure 6-(b,c) show the bias learned from the meta-trained dataset and the impact it has on the proposed method. This experiment is insightful and shows the advantage of the method if the train-test tasks are well aligned.\n\n## Weaknesses\nThe method is close to SPiRL, e.g. it reuses the skill extraction from SPiRL and the main novelty comes from the skill-based meta-training part. It is unclear to me what is the real gain coming from the meta-RL method given the current experimental results which is either too restrictive or a bit unfair.\n\n* The method seems to be tested in maze environments more restrictive than SPiRL, e.g. the train-test maze is the same, whereas SPiRL train on a various set of small mazes and test on larger mazes (c.f. Figure 3 left). Is the method trained on a fixed, pre-defined maze or does it work on a large set of mazes ? Could you evaluate the method on the same environments as SPiRL and compare to it ?\n\n* The proposed method use extra annotations at test time which are target trajectories. These trajectories are not available to the strongest baseline SPiRL, so I wonder if the comparisons of Figure 4 are fair. A fairer experiment would be to also provide these extra annotations (target trajectories) to SPiRL. For example, you could add the available target trajectories to the replay buffer of SAC before running SPiRL. This way you could measure SPiRL vs SiMPL in a fair setup where both have access to the same amount of information.\n\n* Overall the proposed method provides a good warm-start but eventually SiMPL and SPiRL methods seem to be converging to the same success rate after a limited number of steps. SPiRL seems already quite sample-efficient and the gains coming from SiMPL are a bit marginal after 200 episodes.",
            "summary_of_the_review": "The paper is well-written and the method is clearly explained. The proposed method builds on SPiRL and proposed a meta-RL learning of the high-level policy. It shows reasonable improvements when the number of samples is limited (maze and kitchen tasks) and also provides insightful experiments (meta-training task distribution analysis).\nHowever the performances are quikcly matched by concurrent baselines after not that many steps and the comparison to SPiRL seems unfair in the current setup. Mainly the set of mazes solved seem more restrictive than SPiRL, is it a restriction of the method? Also the extra annotations used by meta-RL during evaluation (target trajectories), is not made available to the strongest baseline the authors compare to, SPiRL. The experimental section could be strengthen by evaluating SiMPL against SPiRL where the extra-data provided to SiMPL is also provided to SPIRL, and by evaluating on a richer set of environments.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper combines skill-based with meta RL using training tasks.\n\nThe skill-based method (SPiRL, Pertsch et al. 2020) forms a skill embedding that represents $K$-step state-action trajectories.  An action policy provides an action distribution conditioned on the currently-active skill, and a (higher-level) skill policy is trained using RL with the skill embedding as its action space.  Simultaneously a skill prior conditioned on the starting state of a given skill is trained.  Once trained, the skill embedding, the skill-conditioned action policy, and the skill prior are held fixed.\n\nThe off-policy meta RL method (PEARL, Rakelly et al. 2019) forms a task embedding that represents small sets of skills active during a given task. (This is how I understand it; the paper does not explicitly state this. Rakelly et al. do not use skills; their task embedding represents small sets of state-action-state-reward sequences.)  A skill policy provides a skill distribution conditioned on the currently-active task.  Its training is regularized using the above skill prior. Once trained, the task embedding is held fixed.\n\nUp to here, the system is trained in a task-agnostic manner.  To learn a target task, the system further trains the skill policy, conditioned on the target task, again regularized using the skill prior.",
            "main_review": "This work succeeds at combining the best of skill-based RL and meta-learning using task embeddings.  This combination introduces meta-learning at the skill level.  The results make a good case for the efficacy of the approach.\n\nOn the downside, the resulting system is quite a complicated beast.  While the ideas and methods are quite clear at a high level, the details are hard to follow, at least without deep familiarity with the Pertsch and Rakelly work.  Some of the notation is unclear, confusing or even wrong:\n- In skill-based RL (p. 3), both policies are called \"skill policies\".  I'd call only the higher-level policy a skill policy, and the lower-level policy a (skill-conditioned) action policy.\n- $z_s$ is called a \"latent skill representation\", while $z_T$ is called a \"task embedding\".  This (and other wording) makes two very similar concepts look different.  Call both of them \"embeddings\" to highlight the commonalities between skill-based and meta RL (and making their crucial differences stand out even better).\n- The embeddings $z_s$ and $z_T$ are distinguished by their subscripts.  These subscripts are typeset like variables and are easily confused with states and targets, respectively, that use the same respective identifiers.  However, these subscripts are not variables but labels that distinguish the two different uses of $z$.  Typeset them as text (not math italic), make them superscripts to further distinguis them from variable subscripts ($z^{\\textrm{s}}$, $z^{\\textrm{T}}$) or even write them out: $z^{\\textrm{skill}}$, $z^{\\textrm{Task}}$.\n    \n  To further complicate things, in $z_T$ in Eqn. 2, $T$ is not a label but represents an actual training task (sampled from the distribution $p_T$ of training tasks, where $T$ again is a label, and characterized by its reward function $r_T$, where $T$ is this training task).\n\n  One way to clarify this is the $z_{T, \\textrm{target}}$ notation in Sec. 4.3.  Follow this or one of my suggestions throughout.\n\n  This excessive overloading and abuse of notation makes this paper much harder to read than necessariy.\n- In Eqns. 1, 2, 4 and 5 the argument of the summation includes the regularizer; a parenthesis around its two terms is missing.\n- In Eqn. 2, the maximization should also run over the task encoder $q$ (and also over the critic, but the critic is anonymous here, so omitting it should not cause confusion).  Similarly in Eqn. 4(?).\n- Below Eqn. 2, \"skill\" should read \"task\".\n- In Eqn. 3, the plus should be a minus.\n- The skill prior is conditioned on the starting state of a skill.  This is called $s_0$ in Fig. 2 and $s_1$ at the bottom of p. 4.\n\nSome other issues that should be clarified:\n- Do I understand correctly that in the Kitchen task, the only significance of a subtask is that the overall Task's reward function is structured to emit a reward after the completion of each subtask (in order)?\n- In Fig. 4 and 6, SiMPL and PEARL first meta-train for 20 episodes and then train on the target task? Or what is the special significance of the 20th episode?\n\nAnother remark: It is bad news but unsurprising that SiMPL does not perform well if the training tasks are not representative of the target tasks. It would be highly interesting to try your method on Kitchen-like tasks where subtasks depend implicitly on other subtasks, without intermediate rewards.  In such hierarchical/combinatorial settings, there is no such immediate notion of \"representative\", and I can imagine that your method can (meta-)learn to combine skills to solve such hierarchical tasks.  Demonstrating this would be a big win.",
            "summary_of_the_review": "The paper proposes a nontrivial and effective way of combining skills extracted from offline training data with meta-learning from training tasks.  Following the details is hampered by notational ambiguities and inconsistencies.\n\nPost-discussion edit: I upped my score to in response to the authors' improvements, although limited novelty remains a concern, as expressed by my fellow reviewers.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper the authors propose to learn long-horizon policies by following three stages: (i) pretrain a model on offline data to acquire a set of skills, (ii) meta-train a policy to use those skills, and (iii) adapt the meta-trained policy to solve an unseen target task. A series of experiments on maze navigation and manipulation tasks is provided to evaluate the method.",
            "main_review": "Strengths\n----------\n\n- The paper tackles the problem of learning over a long-horizon, which is an important challenge.\n- The experiments show a clear improvement over methods like SPiRL and PEARL. \n- Additional experiments on number of tasks and task misalignment are a valuable source of information.\n- The paper is well written and easy to follow.\n\nWeaknesses\n----------\n\n- Novelty and contribution. The method seems to be a variant of Rakelly et al. (2019) which uses the learnable priors from Pertsch et al. (2020). Overall, the difference with those methods is negligible. The major contribution of the paper is the use of a pre-training stage before meta-training which is a marginal contribution, since pre-training has been already used in meta-learning. I am open to change my mind on this point if the authors can provide concrete evidences of the relevance and novelty of their work.\n\n- Baselines. It is missing a comparison against standard meta-training methods. The authors state that meta-agents cannot learn long-horizon behaviours. However, I did not see any convincing evidence to backup this claim in the environments considered in the paper. A basic experiment would be to pre-train a standard backbone using offline data (e.g. via behavioural cloning), and then use this backbone to train a classical meta-learner such as MAML (Finn et al. 2017), CAVIA (Zintgraf et al. 2019), or more recent variants. The backbone could be finetuned or frozen at meta-training time. Pre-training the backbone of the meta-learner will guarantee the same exposure to the training environment resulting in a fairer comparison with the other methods.\n\n- Domain shift. The authors have explored the performance under misaligned train/test tasks (see Section 5.4, Figures 6b and 6c) but there is not much regarding misalignment between pre-train/meta-train. In Section 3 (Problem Formulation) the authors state that they do not assume that the offline data is part of the set of training tasks nor that it contains demonstrations for solving them. However, it seems to me that the offline data is collected from the same environment, enforcing a strong inductive bias on the pre-trained model. It is not clear if the method can still work under a mild/strong domain shift. In other words, it is not clear the type of constraints one should impose on the offline data in order to guarantee robust performances at test time. It would be beneficial to see an empirical comparison using different offline data to check how they affect the performance. Different sets of offline data could be produced via domain randomization over the base environment. This is an important factor to explore, since it could lead to a performance drop similarly to the one observed in the misaligned tasks experiments.\n\n- Long-horizon. One of the main claim of the authors is that the method is able to cope with long-horizon. It would be interesting to quantify the performance of the method under environments with an increasing horizon. In particular, it would be useful to know when the method starts to be beneficial compared to standard meta-learner. In the maze environment this could be achieved by gradually increasing the distance between the starting point and the reward.\n\nPost rebuttal\n----------------\n\nThe authors have provided a new set of experiments that (partially) answer my concerns and I will raise my score to 6. Overall the paper is still short in terms of novelty and the new experiments have been carried on a simplistic environment. I have provided a more detailed feedback in the comments section.\n\nReferences\n----------\n\nFinn, C., Abbeel, P., & Levine, S. (2017, July). Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning (pp. 1126-1135). PMLR.\n\nPertsch, K., Lee, Y., & Lim, J. J. (2020). Accelerating reinforcement learning with learned skill priors. arXiv preprint arXiv:2010.11944.\n\nRakelly, K., Zhou, A., Finn, C., Levine, S., & Quillen, D. (2019, May). Efficient off-policy meta-reinforcement learning via probabilistic context variables. In International conference on machine learning (pp. 5331-5340). PMLR.\n\nZintgraf, L., Shiarli, K., Kurin, V., Hofmann, K., & Whiteson, S. (2019, May). Fast context adaptation via meta-learning. In International Conference on Machine Learning (pp. 7693-7702). PMLR.",
            "summary_of_the_review": "Given the marginal novelty and the lack of empirical evidences to backup the authors' claim I do not recommend to accept the paper. I am open to change my mind if the authors can provide a convincing rebuttal and more solid experiments.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}