{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper proposes to learn disentangled trends and seasonal representations of time series for forecasting tasks. It shows separating the representation learning and downstream forecasting task to be a more promising paradigm than the standard end-to-end supervised training approach for time-series forecasting. \n\nDuring the post-rebuttal phase, there were interactions from all the reviewers, and reviewer KrXv raised the score. The reviewers think the contrastive learning method is novel and the added experiments have strengthened the paper.  The authors are encouraged to include more standard datasets (M5) in the final version.\n\nBased on the above reasons, I am recommending accepting this paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a contrastive learning framework for time series forecasting CoST. This is done by detangling trend and seasonal features than using the learnt feature representations with a simple regression. Trend feature disentangler consists of a mixture of autoregressive experts followed by average pooling. While the seasonal feature is done by changing the features into the frequency domain via FFT and then applying a linear layer with weights to every different frequency. The overall loss is the weighted time-domain contrastive loss from trend feature disentangler and frequency domain contrastive loss which contains both phase and amplitude loss from seasonal feature disentangler. The proposed method was evaluated on 5 datasets in both a multivariate and univariate setting. CoST was compared against different representation learning and end-to-end learning appraoches. ",
            "main_review": "Strength:\n- The idea of  disentangling trend and seasonal features and using the learnt representations for forecasting is novel.\n- CoST was compared to different representation learning and end-to-end learning appraoches on 5 datasets and showed strong empirical results.\n- The paper preformed ablation studies induding investigating the use of different backbone encoders, the use of different regressors for forecasting and the effect of different componets of CoST on accuracy.\n- Through synthetic datasets the paper showsthat  CoST can actually disentangle seasonal and trend features.\n\nWeakness:\n- Missing simple baselines, like for example linear regression on the raw features rather than the learned representations and linear regression on the features outputted from the backbone encoder.\n- Some issues with paper writting please see clearification questions below.\n- Missing analysis of computational resourses and training time needed by CoST when compared to other methods.\n\nClarification questions:\n\n- In figure3 shouldn't V(l) and V(g) be V(T) and V(S).\n- Is it possible to provide the code to verify the approach and experiments?\n- For the multivariate time series are you forecasting all features or just using them as input and forecasting target features.\n- What does the second half of table 2 show (the detailed breakdown for multivaritate setting).\n- For tables 3-5 is this the mean accross all ETT datasets?",
            "summary_of_the_review": "The overall idea is novel and shows strong empirical results. \nThe paper performs ablation studies justifying different design choices.\nThe paper shows the ability of the proposed method to disentangle trends and seasonal features through synthetic datasets.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper introduce a framework of learning disentangled trend and seasonal representations of time series and its application to forecasting tasks. It employs contrastive losses to distill discriminative trend and seasonal representations. DFT is used in obtaining frequency domain information. Empirical results show the proposed framework is able to outperform end-to-end trained forecasters and other representation learning methods. In addition, the learned representations can be clustered to distinguish multiple trends or seasonalities.",
            "main_review": "## Strengths\n1. The disentanglement of trends and seasonalities is well-motivated. The interpretation of invariance under intervention on E is valid.\n2. The introduced contrastive losses is a good proxy to learn discriminative representations.\n3. The extensive empirical results show the effectiveness of the proposed method and the ability of identifying multiple patterns.\n\n## Weakness\n1. The method of making forecasts from learned representations is not presented, making it incomplete for reproducing the results. In addition, a joint loss with the forecasting error should also be compared with the current contrastive training.\n2.  In case study, besides clustering of multiple patterns, the disentanglement of trends and seasonalities should also be verified. For example, the representation of the same seasonal pattern is supposed to be overlapped even mixed with different trended patterns.\n\n## Comments and Concerns\n1. In ablation study, what is the difference between TFD and MARE? The \"Trend Feature Disentangler\" paragraph in page.4 states TFD is a mixture of AR experts.\n2. In the selected datasets, the seasonalities of samples are highly similar (e.g. daily and weekly) The negative samples in frequency contrastive loss may be indistinguishable from the positive one in the first place. It may hurt the effectiveness of the frequency representations.\n3. Minor: $d$ should be $m$ in the sec 2. \"Problem Formulation\" $\\hat{X}\\in\\mathbb{R}^{k\\times d}$?",
            "summary_of_the_review": "Overall I think this paper is well-written with clear motivation and technical path. It is acceptable but can be further improved in several aspects.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The current training paradigm applied in most time series forecasting approaches jointly learns feature representations and the prediction function. This paper argues that this paradigm may lead to several issues, e.g., overfitting problems, obtaining false relations of the unpredictable noise in the data, and entangling representations.\n\nTo tackle these challenges, this paper proposes a new framework (CoST) to first learn error-free feature representations, and then fine-tune the representations through a simple regressor. In the feature learning stage, a pretext task is constructed by using data augmentation and contrastive learning. The goal is to learn disentangled seasonal and trend representations. Moreover, time domain and frequency domain contrastive losses are incorporated to learn discriminative trends and seasonal representations, respectively.\n",
            "main_review": "Strengths:\nThe idea of this paper seems novel and reasonable. Most parts are easy to follow. The experiments clearly show the improvement against baselines and the effectiveness of each model component.\n\nWeaknesses:\n1. The writing of the paper can be improved, e.g., several places can be more concise.\n2. In the formulas of time domain and frequency domain contrastive losses, it is unclear why only one entry (time step) is selected as the anchor since both the trend and seasonality are concepts containing a series of time steps.\n3. It is still unclear why representation learning methods can outperform end-to-end methods in such a large gap. It would be good if the authors can provide some insights and explanations about this.\n4. When training the regression model on top of the learned backbone, are the parameters of the backbone frozen or not?\n5. What is the specific setting of the TCN method, such as the model architecture, hyper-parameters, etc.\n",
            "summary_of_the_review": "This paper still has space to improve. I will preserve my score before reading the authors' responses.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel way to represent time series learnt with contrastive losses in both the time and frequency domain for the forecasting task. The approach is novel and timely and is clearly described. The authors make a connection to causality which they use also for designing data augmentation schemes.",
            "main_review": "I enjoyed reading the paper. The method for representation learning is original and clearly described and there's not much for me to argue around. \n\nMy main criticism comes from the experiments and the discussion/awareness of related work. I think that these criticisms can be taken care of by editing/toning down the paper and as part of the review process.\n\nIt's rather surprising that an end-to-end forecast approach can be beaten (by a margin) by a two-step approach. This needs much more and much stronger explanation and the one given here (because of the trend-seasonal-decomposition) does not strike me as strong enough -- a transformer-based model with enough parameters should learn this easily for example and there are plenty of other STL-based approaches in forecasting (e.g., implemented in the R Forecast package) that then should fare well. \n\nIn most of the literature on contrastive learning, the performance gets close to the state of the art but I'm not aware of it beating the state of the art by such a margin. Alas, it's unclear to me how the different representation learning approaches are used to produce forecasts. I may have overread this, but an explanation of how we go from representation to forecast would be needed for the main results (there is an ablation study with Table 5, but I'm not sure what was used for the main results in Table 1). In a typical contrastive learning paper, one would have a simple model (e.g., linear) on top of the complex representations and then get a similar performance to the state of the art. Why is a similar empirical effectiveness not happening here and instead, so much more improvement?\n\nIn the experiments, I'm missing natural baselines (e.g., NBEATS or TFT or DeepAR) and datasets that are a bit more standard, like the M5 data sets for example. Of course, reviewers always want more data sets and more methods, but given the claim that end-to-end SOTA approaches are beaten so convincingly, this leaves me wondering whether indeed SOTA methods were used in comparison or whether the data sets used are representative of the forecasting task. \n\nAlso, using the embeddings as co-variates for an end-to-end-learnt forecasting model would be a natural experiment to explain the accuracy gain better (that would basically be Table 5 with another regressor on top)\n\nFurther questions/comments:\n- overfitting doesn't seem to be a major problem in forecasting where model sizes are comparatively small (when compared with NLP for example), so this motivation doesn't convince me (page 1). A more convincing angle may be greater generality or speed. \n\n- page 2: \"Firstly, we highlight that existing work using end-to-end deep forecasting methods directly model\nthe time-lagged relationship along the observed data X\"  There is a considerable body of literature on probabilistic and deep forecasting models (e.g., https://iclr.cc/virtual/2021/spotlight/3409, but also Deep State Space Models at NeurIPS 20219 and much follow up work) which are in direct contradiction to this statement. In particular for multi-variate probabilistic and deep time series models, there is a growing body of literature which addresses the fact that Figure 2 is an over-simplification in the multi-variate context -- multiple X can have direct causal interaction (e.g., products that cannabilize/substitute/cross-sell)\n\n- experiments: are the embeddings trained on a single data set or across data sets? Over which time horizons do we train the embeddings and how do we forecast (rolling window evaluations), etc? Why is MAE and MSE chosen when there are things like sMAPE and MASE which are more generally accepted?\n\n",
            "summary_of_the_review": "Overall, I'd like to recommend the paper for acceptance given the clear merits of its representation learning approach which I applaud the authors for. However, I have some open questions which I would like to clarify as part of the review process so right now, my evaluation is cautious. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}