{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper gives a new method for code generation from natural language queries using pretrained models. The approach follows two steps: (1) given a query, it selects a set of similar training examples using a method called Target Similarity Tuning, and (2) it then uses a method called Constrained Semantic Decoding (built on top a frozen language model) to adapt these examples into syntactically/semantically correct code.\n\nThe reviewers found the paper interesting. There were some concerns about the method's scope and its relationship to prior work but these were mostly addressed during the author response period. Given this, I am delighted to recommend acceptance. Please incorporate the feedback in the reviews (in particular, the review by vLDx) in the final version of the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a framework for more reliable code generation via in-context learning of GPT-3/CodeX. The paper is motivated by the finding that GPT-3/CodeX often generate programs with syntactic and semantic errors. To resolve this problem, the authors propose 1) Target Similarity Tuning (TST) for retrieving 5 relevant examples based on program similarity and 2) Constrained Semantic Decoding (CSD) for constraining the code generation output to a set of valid programs. The authors evaluate their proposed methods on three different code synthesising tasks (Spider, Vega-Lite, and SMCalFlow). Strong complementary gains on these tasks demonstrate the effectiveness and generalizability of the proposed framework on several tasks with different target programs (SQL, Vega-lite, and SMCalFlow). \n",
            "main_review": "1. Although TST works well empirically, I still cannot get the intuition behind it. The similarity score of two questions (u_i, u_j) indicates the relevance of two (p_i, u_i) in many cases. It would be great if the authors can provide more examples as the one shown in Fig 2 (I suspect the example is not that common). On the other hand, p_i and p_j could be equivalent even if their tree edit distance is large. The authors are encouraged to provide some analysis in that case.\n\n2. The results without TST in Table 2 are reported by selecting examples via the vanilla S-BERT without fine-tuned with the TST objective  (v.s. Random selection)? \n\n3. On the paragraph of “Example selection model”, it would be helpful if the authors elaborate more on fine-tuning S-BERT with the TST objective in Spider and SMCalFlow. \n\n4. Results show that the proposed method still underperforms supervised methods, while both TST and supervised methods use the full training set. Given the large size of GPT-3/CodeX, it would be better if the authors further discuss the practical merits of their approach. \n\n5. The proposed CSD method could be applied to the decoding part of supervised methods. It would be interesting to see how much gain CSD will bring to the supervised methods.\n\n6. Maybe I missed the number in the paper, but I wonder how many constraints did you implement in CEs for the three programs? How hard was it to collect them?\n\n7. It is a bit weird to only show integer results in Table 2 (at least keep one decimal place). \n\n8. It’s a bit surprising to see that TST also largely helps generate valid model outputs according to Table 2. Could you please explain a bit more about it?\n\n9. The result of GPT-3 13B + TST performs worse than GPT-3 13B  (14% v.s. 16%), which contradicts all the other results in Table 2. Why? Or a typo?\n\n10. Based on the results in Table 2, Vaga-Lite has relatively simple program grammar. Can you know why GPT-3/CodeX still performs very low on the task? Questions are ambiguous?\n\n11. In Algorithm 1, is the Sample() function adjusted by other advanced sampling variants (e.g., temperature sampling/top-k/top-p)? How do these sampling variants influence the performance? Are the improvements caused by the sampling variants orthogonal to CSD?\n\n\n",
            "summary_of_the_review": "Although most of the high-level motivations and ideas of the paper have been studied in example/prompt selection and constraint-based decoding (e.g., PICAD), the authors are able to refine them and generalize them to more diverse domains and tasks by using GPT-3/CodeX without fine-tuning with a relatively small overhead to the output process. \n\nAlso, results show that the proposed method still underperforms supervised methods, while both TST and supervised methods use the full training set. Given the large size of GPT-3/CodeX, it would be better if the authors further discuss the practical merits of their approach (compared to few-shot learning with median-size language models has more meaningful practical benefits at this time if they are able to achieve competitive performance by fine-tuning <300-shot (a small number) examples.). \n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper aims to improve the reliability of LLM based code generation via two aspects: select better prompting examples based on program similarity and constrain LLM to only generate valid programs incrementally. The authors provide extensive experiments and ablations to demonstrate the improvement of accuracy and validness.",
            "main_review": "To the best of my knowledge, the ideas presented by the paper are novel and interesting. Although the two sources of improvements (better example selection and constraint validation) have already been explored by prior works, the proposed methods can address some of the major limitations of prior methods intuitively: TST further incorporates the similarity of programs instead of only considering language similarity, and CSD is more efficient and effective empirically. Both methods bring significant empirical gains to the reliability of code generation. The methods are also relatively easy to implement/reproduce and drop in existing pipelines. \n\nI have several minor concerns:\n1. Does CSD also bring significant benefits to more widely used languages like python or C++? Intuitively, the language model probably has a better model of these languages due to more training data, such that CSD is not that needed or only marginally more beneficial than generate-then-test. It would be beneficial to provide extra experiments on these languages.\n2. Similarly, more general-purpose languages also have more diverse ways of achieving the same effect. Then “the similarity between target programs” that TST is trying to predict seems less well defined. \n3. The context-sensitive layer of CSD seems highly language-dependent and not scalable to more general cases. Yet I imagine this is perhaps more important to validness than the context-free layer. It would be interesting to see how relatively important these two layers are over the existing datasets as well as more expressive languages. \n",
            "summary_of_the_review": "Overall, the methods proposed by this paper are novel and address major drawbacks of previous methods, leading to significant improvement of code generation performance across three real-world languages. The methods are also relatively straightforward to reproduce and built upon by future works.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper considers the problem of text to code translation, usually done in state-of-the-art models using a natural language input fed into a transformer model together with other similar input/output examples as a few-shot learning and code being directly output by the model.\n\nThe paper proposes two enhancements of this process.\n\n* The first one called Target Similarity Tuning (TST) is a way to pick input/output examples similar to the input text. Previous works use a pretrained models for natural language similarity (Sentence-BERT is compared in the paper) that determine what examples to include in the few-shot prompt and the TST proposes to also fine-tune this model on code examples such that some semantic similarity in encoded - for example using the same structure of the queries [think of SQL queries to synthesize].\n* The other improvement called Constrained Semantic Decoding (CSD) proposes to alter the decoder of the transformer model and to make it avoid generating programs that are impossible to complete to syntactically or semantically correct ones.\n",
            "main_review": "The two contributions of the paper are quite different and have different qualities. What is nice about the paper is that it shows that the two ideas may complement each-other.\n\nWhere the paper falls short is providing understandable limitations of these ideas. I do not agree they are universally good and some of the evaluation results look confusing and difficult to parse and understand. From the evaluation, it is not clear when the two ideas are tested separately and which improvement can be attributed to the specific idea. The output model also does not reach the accuracy level of prior state-of-the-art works, often by a lot. This usually wouldn’t be an issue if the training was really few-shot, but the TST idea removes this as it fine-tunes a transformer model.\n\nTST\n===\nThe main idea of TST is that instead of purely textual similarity input/output examples for the few-shot learning, the fine-tuned model would provide target query similarity metrics to the input. The choice of \"normalized similarity metric between programs\" to be AST edit distance is not clear. Also, there seems to be something different happening than what the explanation in the paper gives. We see cases of programs with low textual similarity, but high AST similarity that the model learns to return. However, this means this model learns how to approximately synthesize programs from the inputs. The paper does not discuss what capacity/complexity this pretrained model would need, but I assume the needs may be similar to the works that do program synthesis without pretrained language models (they outperform this work, this is for similar accuracy).\nminor questions: Why was AST distance chosen here. Why would it do better than pure text distance on the program text?\n\nCSD\n===\nThe idea to check if a sequence could potentially complete during decoding is also not new. It was already done before, for example by Karaivanov et al., Onward 2014 paper Phrase-Based Statistical Translation of Programming Languages. However, CSD here claims to enforce syntax, scope, typing rules, and contextual logic. This is an interesting claim, but this means that quite complex logic in the decoder. First, it is not clear this logic would expand to other languages with more complex syntax. Probably what the authors implement does not do precise enforcement, because this would be too complex, but instead efficiently cuts-off many search branches that would not lead to correct solutions. This is probably a nice contribution, but is not discussed in the text of the paper. It is also not clear which of syntax, scoping, type enforcement or other semantic rules contribute to the increased precision.\n\nThe main baseline to compare to here would be generate-then-check. In such a setting, one can use an unmodified parser, type checker and validator that can probably enforce more properties than the checker during decoding. Given the results shown in the paper, it is not clear why CSD would be of any significant advantage. There is also something unclear about the comparison to generate-then-check - is it compared to the full Synchromesh or only to the CSD part of it? Also, I would expect to see generate-then-check in the main evaluation table.\n",
            "summary_of_the_review": "Overall, the paper gives some interesting ideas, but it does not demonstrate these ideas lead to actual wins and doesn’t discuss its limitations. Combining the two contributions TST and CSD does not lead to state-of-the-art results, while removing some of the advantages of few-shot learning. I believe this work is not generally flawed, but in this state is below the bar of ICLR.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes SYNCHROMESH, a framework for improving the reliability of pre-trained models for code generation. SYNCHROMESH first retrieves few-shot examples from a training set using Target Similarity Tuning. It then feeds the examples to a pre-trained language model and samples programs using Constrained Semantic Decoding, which can constraint the output to a set of valid programs in the target language. The authors evaluate the proposed approach by synthesizing code from natural language descriptions using GPT-3 and Codex in three real-world languages: SQL queries, Vega-Lite visualizations and SMCalFlow programs. The experimental results look promising. \n",
            "main_review": "\nIt is good to see a new code generation approach that considers both syntactic and semantic rules of the output language. Such an approach could prevent implementation errors in the generated programs. The proposed approach is evaluated on three languages (SQL, Vega-Lite, and SMCalFlow). The paper is generally well written.\n \nTo select examples, the authors fetch the 5 closest examples from a training set by cosine similarity. How is the training set constructed? What if there is no similar example in the training set? That is, the returned 5 closest examples all have low similarity to the question. \n\nHow many few-shot examples are selected? Are the few-shot examples the same as the 5 closest examples?\n\nThe proposed approach utilizes pre-trained models for code generation. Actually, a traditional approach (such as the one that is based on code template/pattern analysis) could also work here. Especially, considering the proposed approach also utilizes modules such as similar example retrieval and constraint analysis. Can you simply use a non-DL method to generate code based on the similar examples you retrieved, instead of using a pre-trained model? What are the benefits of using a pre-trained model here?  \n \nThis paper claims to improve the reliability of pre-trained models for code generation. However, it only investigates three domain-specific languages SQL, Vega-Lite, and SMCalFlow, which all lead to relatively simple programs. It is not clear if the proposed approach can be applied to other commonly-used, general-purpose programming languages such as Java, Python, and C/C++, which all have more complex syntactical structures.  \n\nActually, there are already some work that consider constraints in generating more complex Java and Python programs:\nLyu et al., Embedding API dependency graph for neural code generation. https://arxiv.org/abs/2103.15361, 2021.\nFor program synthesis, there are also some related work that are based on input and output examples:\nZaremba et al., Learning simple algorithms from examples. arXiv preprint arXiv:1511.072\nShu et al., Neural Programming by Example, Proc. AAAI 2017, Feb 2017.\n",
            "summary_of_the_review": "Pros:\n\n. A new approach to code generation using pre-trained models.\n\n. Evaluated on three languages (SQL, Vega-Lite, and SMCalFlow)\n\nCons:\n\n. It is unclear if the proposed approach can be applied to other general-purpose programming languages such as Java, Python.\n\n. Some technical details are missing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}