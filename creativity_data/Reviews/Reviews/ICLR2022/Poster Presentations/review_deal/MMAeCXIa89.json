{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper investigates Bayesian optimization where a prior distribution over the optimal is available. The authors conducted a systematic study on a very intuitive prior-augmented acquisition function that multiplication the prior probability with the EI heuristic --- including an asymptotic analysis on the regret, comprehensive (controlled) synthetic experiments, and moderate empirical support on several real-world case studies.\n\nAll reviewers find the paper well-written and appreciate the rigor of the empirical evaluation. The theoretical analysis is also helpful to provide additional justification for the proposed approaches. I would like to add that the paper also included a brief but comprehensive survey on prior work related to leveraging prior in BO, which I find useful for the general audience.\n\nReviewers noted that such a bound could become trivial with a bad prior in practice, and further suggest that one may leverage these theoretical insights as general guidelines to practitioners in designing the prior. I think this is a valuable message to convey and suggest authors take it into account in the revision.\n\nThere were initial confusions pertaining to the experimental details, mainly concerning the effect of the quality of the prior on the performance of the proposed algorithm. The authors provided an effective rebuttal with much concrete empirical support, and after a few rounds of interaction during the discussion phase, the reviewers are convinced about the empirical significance of the proposed work. Overall, this makes a solid work."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a method to incorporate prior information about the optimum in the standard Bayesian optimisation (BO) setting. The prior is specified as a smooth function over the range having high values where the experimenter think the optimum may exist with high probability and low values otherwise. This prior function is incorporated in the BO workflow through multiplication with the EI acquisition function. The authors show that when the prior function is decayed then this BO achieves the usual sublinear regret rate of a standard BO asymptotically. The synthetic experiments are extensive and convincing. The real case studies are limited, but sufficient.",
            "main_review": "I am quite aware of research in BO and I think integrating prior information is an important aspect that has not been looked into much. Existing solutions are either restricted or do not admit a convergence analysis. In that respect, I quite like the idea of the paper. It's quite simple and yet lend itself to an analysis of the convergence. The experiments are sufficient in my opinion to expose the behaviour of the algorithm at different scenarios. \n\nTheoretical analysis is convincing. Although I should say that the upper bound can become uselessly loose if the prior is not designed properly. The constant C_{\\pi, n} can be arbitrarily large if the prior is Gaussian like and narrow. This may make the anytime upper bound not useful. The authors should use this insight to restrict the shape of the prior function. Another limitation of the theoretical analysis is sticking to EI acquisition function. An analysis that admit GP-UCB acquisition function would have completed the work because unfortunately, EI is not proven to converge for noisy function case. \n\nAnother important question is how do the authors propose to scale the prior function \\pi so that can influence the acquisition function. The EI functions are notoriusly peaky in the sense that it can have sharp peaks among the valley of mostly small values. In that case, to make prior count, it needs to be scaled properly. I would like the authors comment on this aspect.",
            "summary_of_the_review": "The paper proposes a simple but elegant method to incorporate prior information about optimum in the BO workflow. It's a long-standing research gap and the proposed solution fills the gap to some degree. The best part is that the simple way of integrating prior makes its analysis straightforward using the current techniques. However, it has got some limitations: 1) lack of extension to GP-UCB acquisition function, 2) lack of discussion around the extreme looseness in the bound that can happen when the prior is chosen improperly, and 3) lack of discussion around the relative scaling between the acquisition function and the prior function and its impact on the influence of the prior function in the process. I would go for weak accept. But I am ready to move up depending on the rebuttal.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a new method to incorporate prior knowledge about optima location in Bayesian optimization. The paper claims that the proposed method is simple to implement, can be used with any acquisition functions and has efficient convergence rate.",
            "main_review": "Overall this paper is well-written and appropriately discusses the related literature. \n\nWhile the problem of incorporating the optimum location makes sense, I am not sure if the experts can provide a proper probability distribution about the optimum. Finding this information in practice seems rare. For example, even the experiments in section 4.4 rely on the priors which are based on already (manually) tuned deep networks. Finding useful priors may thus be hard in practice.\n\nThe method seems simple and as stated in the paper is easy to implement. However, it seems to me that while the paper claims the proposed method to work with any acquisition function, it gives analysis only for EI acquisition function? Is this true?\n\nAlso, there is a claim that the method can recover from any misleading prior, but it may be true only when there is a nonzero support for the true optimum x*. If the prior likelihood for x* is zero, the method may not converge. In case of misleading prior with nonzero support, is the convergence rate still sublinear?\n\nIn Eq 4, what is y^*_{n+1}? It seems that left-hand side of the EI expression in Eq 4 does not depend on x, which is strange.\n\nIn the experiments section, all the methods seem to be starting from different initialization points, if this is true then the comparison between the methods may not be fair. Why does \\pi-BO often start from a higher starting value after the initial design? We do not know if \\pi-BO does well due to the proposed algorithm or due to the better initialization?\n\n\nMinor Comments:\nOn page 1: “documentesd” should be “documented”\nOn page 4: in the third para, “show” should be “shows”\nOn page 5: before Eq 6, “prior-weighed” should be “prior-weighted”",
            "summary_of_the_review": "While the proposed method for this problem makes sense, it may be hard for experts to provide such information in practice. Further, the convergence analysis seems limited to Expected improvement (EI) acquisition function only. Last but very important, different methods seem to be starting from different initialization points, which may be problematic.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "## Summary:\n\nPiBO is a very straightforward paper that seeks to incorporate prior knowledge about the optimum to accelerate Bayesian optimization. \n\nThey achieve this by simply multiplying the acquisition function (Expected Improvement, in the paper) by the prior distribution, and then maximizing that. They then decay the prior in order to deal with mis-specified priors. The authors also provide some theory that this weighted acquisition function performs (for some particular definition of performance) no worse than EI times a constant that depends on the decay rate. It’s a reasonable idea; honestly I’m surprised no paper has tried this before. \n\nSynthetic and real-world experiments indicate that:\n-With a well-specified prior, PiBO demonstrates superior performance over it’s basic EI counterpart. \n-With a poorly-specified prior, PiBO performs poorly worse than it’s EI counterpart but is usually able to recover given enough iterations. \n\n",
            "main_review": "## Review:\n\n### Strengths:\nThe method is simple and sensible, and experiments convincingly demonstrate that it works with a well-specified prior, and more importantly, demonstrate that a poor prior could lead to problems. I know it doesn’t have a lot of moving parts or complicated mathematical formulations that the BO community tends to favor these days, but I am in favor of acceptance for the reasons I mentioned above. It’s a rigorous scientific work that could have a significant impact on the way BO is performed in the HPO community today. \n\n### Weaknesses:\nMy only criticism is that priors used (on continuous params) are Gaussian. The authors may be leaving performance on the table by using such a simple prior. But this does not affect my score that much. \n\n\n## Additional Notes:\n* The decay method you have was used in this workshop paper Cost-aware Bayesian optimization, by Lee et al., 2020 (in a slightly different context, to decrease the impact of a cost model), so you might consider citing that if you want to reinforce that this decaying type modification has precedence in the literature. \n* There is a stronger connection here with metalearning, in the context of HPO, (which seeks to build and then exploit priors) as a whole, and perhaps this could be included as a short paragraph in the related work section w/ some additional citations (e.g., more of Hutter’s work). ",
            "summary_of_the_review": "The method is simple and sensible, and experiments convincingly demonstrate that it works with a well-specified prior, and more importantly, demonstrate that a poor prior could lead to problems. It’s a rigorous scientific work that could have a significant impact on the way BO is performed in the HPO community today. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this article the authors propose to incorporate domain knowledge on where good configurations are located in Bayesian optimization (BO). They do so by weighting the acquisition function by a prior that decays and reverts to uniform as iterations go. The asymptotic rate of convergence is shown to be of the same order as the non-prior version. Empirical tests are provided on various synthetic and  hyperparameter tuning tasks, with diverse baseline methods and ablation studies.",
            "main_review": "The paper is clear and with extensive empirical evaluation. An asymptotic convergence results shows that there is only a constant factor difference with the regular method. The main drawback lies in the apparent simplicity of the approach, in terms of originality. Giving more weights to areas supposed to be better in the acquisition function optimization could already exist in application oriented papers (though I did not found such example with a quick search).\n\nMinor:\n- Consistent colors/symbols for methods across figures would help.\n- Additional details on how to define the prior would make the paper more self contained.\n- Note that the unbounded BO method would apply with the definition of starting bounds (say taking 50% of the volume under the prior).\n- The differences with BOPrO could be better highlighted.\n- I appreciate the theoretical result albeit one could hope to show some improvement from using a good prior.\n\nTypos:\nP1: documentesd",
            "summary_of_the_review": "If not particularly original, the proposition is well motivated and linked to the state of the art, plus it comes with extensive empirical results supporting the approach.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}