{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper studies group equivariant neural posterior estimation which seeks to endow conventional neural posterior estimation method with equivariance of both the data and parameters simultaneously. To test the efficacy of the proposed approach the authors experiment with gravitational wave data and show that the proposed GNPE achieves considerable performance gains.\n\n\nStrengths:\n\n- The approach is independent of neural architectures and does not necessitate knowledge of exact equivariances.\n- The method seems to be much better than regular NPE in cases where there are known equivariances.\n\nWeaknesses:\n\n- the writing of the paper is not clear, which makes the paper difficult to read and evaluate.\n- It is hard to check the correctness of the conclusions and algorithm due to lack of necessary assumptions and derivation steps.\n- the authors are knowledgeable about the subject but present material in a slightly callous way which prevents a precise understanding of their techniques.\n\n\nThis is a borderline paper with two reviewers in favor of acceptance and two with a slight tendency to reject. The two negative reviewers did not engage in a discussion with the authors or did not complete that discussion, failing to confirm their ratings or provide an update of those ratings. They also do not seem to give strong arguments for rejection. Based on this, I recommend the paper for acceptance. However, I encourage the authors to take into account the reviewers' comments, especially the part on clarity and rigor, to improve the paper for its camera-ready version."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose group equivariant neural posterior estimation (GNPE), a posterior estimation method which can self-consistently infer parameters and standardize the pose. For equivariant posterior distributions, the GNPE can achieve better performance than the traditional NPE. Moreover, GNPE can also be applied to cases where the equivariance of the posterior is approximately estimated.\n\nThe major advantage of GNPE over the other flow based models under equivariance constraints is that GNPE is architecture-indepenent, i.e., an arbitrary flow model can be utilized in GNPE and we don't need to design a special network structure for the equivariance.",
            "main_review": "The major disadvantage of the paper is the writing, which makes the paper difficult to read and evaluate. I can understand the key idea of the GNPE, and the problem investigated by this paper is certainly important. But I cannot check the correctness of the conclusions and algorithm due to lack of necessary assumptions and derivation steps.\n\nSome comments:\n\n1) Where does Eq. (4) come from? I think this is a corollary of several assumptions including the equivariance of prior distribution of theta. If so, please provide all the related assumptions and the proof, which are not trivial for readers.\n\n2) It is unclear why we need \\epsilon and \\hat g. Are they introduced for the case of approximate equivariance. The first two paragraph in Section 3.3 seems to discuss the necessity of introducing them, but it is really hard to understand. Is it possible to make the explanation more clear by using the toy example?\n\n3) What is the definition of p(\\theta|x,\\hat g) in Eq. (5)? How is Eq. (5) proved? In addition, please provide proofs of all equations in Section 3.\n\n4) Is there any way for users to choose the distribution of \\epsilon in applications?\n\n5) In experiments, only NPE and GNPE are considered. It is worth comparing the performance of GNPE versus other flow models with equivariant architecture.",
            "summary_of_the_review": "It is an interesting paper, and possibly make important contributions. For now, I give the score \"marginally below the acceptance threshold\" due to the poor writing. But if the authors can well explain all the related conclusions and definitions, I can consider to increase the score.\n\nAfter reading the rebuttal and explanations from authors, I think the revised manuscript is much more understandable, and I change the score to \"marginally above the acceptance threshold\".",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes to incorporate invariances of posterior estimates via a factored posterior estimation approach rather than the standard architectural interventions. The paper proposes to learn an approximate standardizing transformation over data and then learning posteriors over the approximate standardized data.",
            "main_review": "Strengths:\n- The approach is independent of neural architectures and does not necessitate knowledge of exact equivariances.\n- The method seems to be much better than regular NPE in cases where there are known equivariances.\n\nQuestions:\n- I found the writing in Sections 3.4 and 4 quite confusing to follow. I am not sure how changing $g_\\theta$ to the approximate $\\hat{g}$ solves the problem of not having \"pose\" data in images. \n- How is $q_{init}$ trained? Is this a regular NPE trained on the same data used for training invariances? \n- I think Equation 9 needs further discussion in the main body. What is $Q$ and how is it related to lowercase $q$?\n- I think it would be really helpful to show in some toy cases how $q(\\theta | x , \\tau)$ depends on $\\tau$ (or $\\hat{g}$).",
            "summary_of_the_review": "I am awaiting author responses at this point and will update my review based on their explanations.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper the authors present a method for performing Bayesian inference in likehood-free settings where the data and parameters are jointly equivariant or approximately equivariant.  Examples include translational or rotational equivariance, where if the latent parameter are translate or rotated, then the distribution over the data is the same, except that the data are translated or rotated in the same way as the latent parameters.  The method presented here acts by mapping each simulation to a standard \"pose\" and then aims to learn both the original pose and any additional parameters from the data.  There are two main components to this method -- 1) the standard simulation-based inference method of estimating latent parameters from data, but applied to data that have had their pose standardized and 2) a Gibbs sampling framework to use the family of posteriors learned from the first component to jointly estimate the pose and ant other parameters from unposed data.",
            "main_review": "Overall I found the problem and approach to be interesting.  The paper is well-written, but I have a number of comments that I list below.\n\nMajor comments:\n- Despite being central to the main application and some of the setup of the paper, it was not clear to me what \"approximate equivariance\" looks like in practice.  For the damped harmonic oscillator toy example, it's clear that the data are exactly jointly equivariant with $\\tau$: if $\\tau$ is shifted by 1, the data are also shifted by 1.  In the approximately equivariant case, I'm not sure what that would look like.  Is it that the distribution of the other parameters depends on $\\tau$ but only weakly?  In the real data example, it's clear what transformation should be applied to the data to put it in the standardized pose for the exact equivariance, but what do the approximately equivariant data transformations look like?\n- The \"Chained NPE\" (equation (12) in Section 5.3) makes sense as a reasonable baseline, but it is unclear to me why it performs so much worse than the wide $\\kappa$ GNPE, and it would be good to include more discussion of this point.  In particular, it seems like since the GNPE with wide $\\kappa$ only performs one Gibbs sampling iteration, it essentially 1) draws the initial pose estimates from a standard NPE density estimator, 2) draws the a full set of parameters according to equation (8), 3) draws a noisy version of the pose from equation (7), and then 4) redraws the full set of parameters from equation (8).  If I understand the chained NPE approach from its minimal description, it is essentially only performing steps 1) and 2).  If this is the case, then it feels like having a wide $\\kappa$ is just \"blurring\" the estimate of the pose relative to the chained NPE approach, in which case the increased performance going from chained NPE to wide $\\kappa$ NPE is attributable just to having a more diffuse estimate of the pose in the latter.  Does this indicated that chained NPE is overly confident in its estimation of the pose?  Could chained NPE be \"fixed\" by learning a more expressive posterior distribution (e.g., mixture of Gaussians instead of a single Gaussian)?  I ask because chained NPE is conceptually much simpler and doesn't require the additional tuning parameter of $\\kappa$, or Gibbs sampling, making chained NPE -- at least in theory -- much more attractive.\n\nMinor Comments:\n- Beyond the standard issues with choosing neural network architecture, this approach introduces two new algorithmic parameters that need to be tuned: 1) the \"noise tolerance\" distribution in the pose estimation, $\\kappa(\\epsilon)$ and 2) the number of Gibbs Sampling steps that are needed to reach convergence.  2) does not seem so problematic in practice (there are various methods of diagnosing the convergence of Markov chains) although it would be good to describe how this was chosen for this paper (e.g., on p. 8 it is stated that \"We find that fast-mode GNPE converges in one iteration, whereas accurate-model requires 30.\" but it is not stated what was used to determine this).  To me 1) seems more problematic since it is (in general) a distribution, possibly over multiple dimensions.  Is it sufficient to only consider uniform distributions, as in the present paper, or would other distributions result in better performance?  How should one set the parameters of such distributions?\n- In the paragraph following equation (8) on p. 5, burn-in and thinning only result in approximately independent samples.  Similarly, It should be noted that equations (7) and (8) only result in approximate samples from the posterior (due to 1) the asymptotics of Gibbs sampling; and 2) any potential mismatch between the variational approximation $q(\\theta | x, \\hat{g})$ and the true posterior $p(\\theta | x, \\hat{g})$ due to e.g., the variational family not being rich enough, an insufficiently flexible inference network that cannot learn the amortized posterior, not enough samples when training the inference network, or failure in the optimization of the neural network).  \n- I find the c2st metric a bit difficult to interpret.  It would also be good to consider more interpretable metrics, like the MSE of the posterior mean for each method, as well as e.g., the coverage of credible intervals.\n- In contrast, Figure 4 is very nice and easy to interpret.  It would be helpful to also visualize these same marginal posteriors for the \"wide $\\kappa$\" and \"NPE (chained)\" methods to see the effect of $\\kappa$ (see above comment).\n\n\nTypo:\n- on p. 9 \"GNPE achieves a scores\" --> \"GNPE achieves a score\"",
            "summary_of_the_review": "The paper is well-written and presents an interesting method for a methodologically interesting problem with a nice application.  Some of the key aspects of the paper (e.g., what does an approximate equivariance look like in practice? why does the method outperform chained NPE? how should one set the kernel width?) are a bit unclear, but overall the paper is a good contribution.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies group equivariant neural posterior estimation which seeks to endow conventional NPE method with equivariance of both the data and parameters simultaneously. To test the efficacy of the proposed approach the authors experiment with gravitational wave data and show GNPE achieves considerable performance gains. ",
            "main_review": "I found this paper a very enjoyable read, but albeit quite difficult to review for the following reasons. I believe the authors are motivated by extremely interesting problems in physics that leverage non-conventional---atleast in machine learning---datasets like gravitational waves. At best, I can comment from afar that the results look promising and the overall execution is of high quality. \n\nI do have some concerns regarding the technical aspects of the paper though. First, there are a few clarifications that I'd like the authors to address. In all the equivariant literature, including designing equivariant normalizing flows, we are interested in pushing forward a $G$-invariant prior density to a $G$-invariant target. In this paper, the authors instead seek to model a $G$-equivariant density/posterior which is a completely different creature. However, there must be a typo or a misunderstanding on my part as equation 4 is an ***invariant*** distribution and not equivariant as otherwise the group element $g$ must commute and appear in the LHS. This also brings a few points with regards to writing clarity and perhaps mathematical rigor. It is unclear to me what it means for a group element $g$ to act on parameters $\\theta$. For the data $x$ this is not an issue as we take a representation of the group and act on the space, but how does one act on the parameters? Do the authors mean acting on the density $p(\\theta)$? This leads to my central complaint of the paper which is the use of the word pose. Simply put this is vague in the context of equivariance despite the citation to Jaderberg et. al 2015. It seems like the main thrust of defining pose is so that we can find an inverse group element which can then be used to standardize the pose. By the definition of a mathematical group such an element always exist and as a result I did not follow the discussion in section 3.3 that requires a kernel $\\kappa$ over a subset of elements in $G$. Specifically, I do not understand the practical motivation of standardizing the pose to a narrow band. \n\nWith regards to the claims about exact and approximate equivariance achieved by GNPE in comparison to prior work I believe the statement is not the best comparison. To be precise, prior work has focused on guaranteeing equivariance by constructing equivariant maps directly while here equivariance relies on finding a pose standardizing element. It is completely unclear how difficult this could be for many groups of interest and as a result I believe we cannot fully generalize this claim, unless the authors provide an efficient algorithm to do so. \n\nI have no specific comments on the experiments but it does seem that GNPE improves over NPE but the data domain is a bit far removed from my area of expertise.\n\n",
            "summary_of_the_review": "Overall the paper is well written and the experiments seem to demonstrate the power of GNPE. There are a few technical questions that the authors could do well to answer which may change my score and opinion of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}