{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper introduces a convolution where the kernel is parametrised continuously over time (in the context of recurrent networks) to address vanishing gradients issues, by using another neural network to generate the kernels.\nThis is a meaningful idea, addressing an important problem.\nThe paper is well written and clear. The idea is novel (parametrised kernel already exist, but the way it's used here is new).\nThe experimental section is solid, although some reviewers suggests it could be extended with more baselines.\nAll reviewers recommend to accept the paper, therefore I also recommend accept."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper introduces convolutions with continuously parametrized kernels for sequential data. Continuous kernel convolutions, CKConv for short, parametrize the kernel associated to a convolutional layer as a continuous mapping, $\\psi: \\mathbb{R}^+ \\rightarrow \\mathbb{R}^{N_{out} \\times N_{in}}$, from the relative time offset $\\Delta\\tau \\in \\mathbb{R}^+$ to a weights matrix $\\mathbf{W} \\in \\mathbb{R}^{N_{out} \\times N_{in}}$, i.e. $\\psi(\\Delta \\tau) = \\mathbf{W}$. This is in contrast to traditional CNN approaches, which represent the convolution kernel as an explicitly learnable set of weights over an a-priori fixed receptive field.\n\nThe corresponding CKConv operation can be applied\n1) over arbitrarily long receptive fields, including a global one over the history of the input sequence, without the heavy memory burden incurred by representing the convolution kernel explicitly as a set of weights along the horizon;\n2) to sequences that are sampled irregularly or asynchronously, since the admissible set of relative positions are not fixed in advance.\n\nA relation is given, which shows approximately how the CKConv changes under resampling the input sequence (the output gets rescaled by the ratio of the sampling rates), which is suggested by the authors to support well-behavedness under varying input resolutions during training and/or between training and testing. Afterwards, a discussion is given about the parametrization of the kernel. Specifically, various MLP models are considered with different activation functions, such as, ReLU, LeakyReLU and Swish, and it is demonstrated that these MLPs are unable to learn functions with high-frequency oscillations, while a recently introduced Sine activation is able to. \n\nThe experimental evaluation includes 1) classic stress tests for RNNs, 2) discrete sequence tasks (sMNIST, pMNIST, sCIFAR10), 3) time series modelling (CharacterTrajectories, Speech Commands) , where additionally robustness to missing/irregularly-sampled data and resolution changes are also investigated. The proposed model overall performs favourably to common alternatives; and it seems robust with respect to missing data, and moderate resolution changes.",
            "main_review": "# Review\n## Clarity and novelty \nThis is overall a well-written paper, which clearly discusses the limitations of popular sequence models such as CNNs and RNNs, and explains how the proposed CKConv model is able to provide improvements in those aspects. The related work seems to be adequately cited. The main idea, which is to represent the convolution kernel implicitly via parametrizing it by a neural network, is not novel in the literature, but it has not been applied previously for sequential data modelling as far as I know. \n***\n## Strengths\nThe main value of the paper for me is in:\n1) adapting the continuous kernel convolution model to the sequential data context\n   + the idea of using a CKConv with a global memory horizon seems particularly useful, remedying the problem of not being able to learn long-range relationships, typically associated with RNNs and fixed horizon (shallow) CNNs\n   + the ability to generate kernel weights for arbitrary relative offsets naturally also lends itself to learning from irregularly sampled data, which is non-trivial to address with classic approaches\n2) the effort put into the empirical evaluation\n   + observing that convolution kernels might naturally contain high-frequency oscillations, which are difficult to learn via common activations, and identifying MLPs with Sine activations as a suitable candidate for representing the kernel\n   + carrying out the empirical evaluation across a range of tasks, which show strong performance of the proposed model\n***\n## Drawbacks\nMy criticism is regarding some of the claims and conclusions drawn. See these points below.\n\n**Insensitivity to resolution changes**\nIt seems that the relation in equation (5) does not exactly support that the model is robust with respect to larger resolution changes. This is also shown in Table 5, where the performance degrades significantly if there is an order of magnitude difference between training and testing resolutions. This is of course because the discretized convolution in eq.(2) is not a proper discretization of the integral in eq.(1). In particular, if we consider a high-frequency limit (that is, sampling a continuous signal at increasing frequencies), then the summation in (2) does not converge. I wonder why the authors chose to omit the normalization from eq.(2)? They could have normalized by the number samples (or equivalently, by the sampling rate) for equispaced samples, or for irregular data simply multiply the summand by the local step size to obtain Riemann sums that are actually convergent, and hence mapping that is continuous-time convergent and invariant to resolution changes, which it is not right now.\n\n**Asynchronous sampling**\nIt seems to me that regarding asynchronously sampled data, the model formulation is equivalent to combining zero imputation for the missing values with concatenating an observation mask to the input as extra channels. This is a valid approach, but not specific to the model at hand.\n\n**Activations in $\\texttt{MLP}^\\psi$**\nI found the discussion regarding the activation functions in the kernel parametrization quite intriguing. In particular, the experiments shown in Figure 4 coincide with the findings of \"On the Spectral Bias of Neural Networks\", Rahaman et al. 2019, where it is observed that NNs with ReLU activations trained under GD are biased towards learning low-frequency functions. It is also an interesting finding that the Sine activation seems to be able to overcome this limitation in this setting. But then one of the main points the paper tries to communicate seems to be that _Neural networks parameterizing spatial functions should use Sine nonlinearities_, which in my opinion paints too dark of a picture about ReLU and its companions. Especially so, as there have been approaches proposed in the literature to circumvent this limitation without changing the activation itself, see \"Fourier Features Let Networks Learn\nHigh Frequency Functions in Low Dimensional Domains\", Tancik et al. 2020. Without properly testing out if these techniques do help the usual activations, the aforementioned conclusion stands on shaky grounds. Especially so, since ReLUs are well-studied in the literature, while Sine nonlinearities in NNs (i.e. SIRENs) seem to be somewhat of a wildcard. In particular, by the discussion in App. E.3 I am left wondering if SIRENs might be, on the other hand, prone towards learning (unnecessarily) high-frequency functions? \n\nConcretely, I would be curious to know how the conclusions of the experiment in Figure 4, and the ablation study in App. D.1 change if the relative time offset $\\Delta \\tau$ is first passed through a positional encoding or Random Fourier Feature mapping as done in the previously referenced paper. Furthermore, in case any of these preprocessings do help the \"standard\" activations, it would be also good to know if the learned functions still contain the high-frequency oscillations beyond the input time grid as mentioned in App. E.3, or if these end up being smoother?\n\n**Modelling long-range relationships**\nThe authors have noted that dilated convolutions are able to model long-range interactions. I wonder why it wasn't included among the experiments? Clearly, to achieve a larger receptive field, dilated convolutions stack many layers, which means that the inputs are passed through several nonlinear activations before reaching the final representation the prediction layer receives. On the other hand, the CKConvs can increase the receptive field while keeping the model shallow. The benefits of this are not really clear at the moment, and whether the way the sequential information is processed in the dilated model (by passing it through \"extra depth\") has adversarial effects for the quality of the results.\nAdditionally, self-attention models are also able to capture long-range dependencies in their input sequences, so it would be worthwhile to include a discussion about them, and compare against it on more experiments.",
            "summary_of_the_review": "I am in favour of the paper mainly due to the compelling empirical performance, which should be of interest to the community, although some additional baselines and experiments could have been included.  It is clearly written with figures and illustrations to aid the reader. The main idea is not novel, but it has not been adapted to handling sequential data before, and it seemingly has many benefits in this context.  My criticism is regarding some discussions in the paper, and that some properties of the model seem to be overstated.\n\nPOST-REBUTTAL: Given the comprehensive answers given by the authors, and the implemented changes in the paper, I am inclined to raise my rating to an 8 from the previous 6.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This contribution adapts to sequential data the paradigm of continuous\nconvolution kernels in convolutional neural networks (CNNs). While\nusual kernels are an extensive list of weights (one for each\nposition), the introduced CKConv frames these kernels as continuous 1D\nfunctions, parameterized by a small multilayer perceptron. This\nparadigms allows for wider kernels with fewer parameters, making it\npossible to deal with long-range dependencies without\nrecurrence. Several experiments are provided and show that the\nresulting CNN achieves state of the art performances on a variety of\ntasks, and in particular is able to deal with irregularly sampled\ndata.\n",
            "main_review": "The contribution is clear and well written. I reviewed a previous\nversion for another venue and thank the authors for their effort in\nclarifying their manuscript, which I now find very didactic and\npleasant to read.\n\nI found the concept of continuous kernel interesting. The authors make\nit clear that they are adapting it from previous work on 3D data, but\nalso show how its transfer to sequential data is a useful contribution\nin particular for dealing with long-range dependencies without the\nlimitations of RNN or discrete CNN. In particular, they clearly show\nhow a discrete convolution kernel would require much more parameters\nthan a CKConv to model the same range of dependencies.\n\nThey also provide thorough experiments illustrating the superiority of\ntheir proposed continuous kernel CNN (CKCNN) against state of the art\napproaches on all targeted applications: long-range dependency\nmodelling, irregular sampling, varying sampling rate.\n\nI believed that this contribution could both be inspiring for new\nparadigms of networks (e.g. as an alternative or complement to\nattention), and lead to better performances in practice on the\ntargeted applications.\n\nOne point that could be further clarified is the choice of using a single MLP\nfor modelling the Nin x Nout convolution kernels. Among other\npossibilities, a separate network could be used for each of the Nout\nconvolution filters (i.e., one MLP instead of one discrete Nin x\nkernel_size filter). Using a single MLP likely creates dependencies\namong the Nout filters, is there a justification for this choice?\n",
            "summary_of_the_review": "The contribution is original and could have a large impact both because it leads to better performances in several important situations (including long range dependencies) and because it could inspire new designs. The proposed method is thoroughly evaluated.\nThe submission is also clear and well written.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes continuous convolutional kernels parametrized in the form of an MLP. The MLP gets the relative time as input and outputs the column of the convolutional kernel at the given relative time. The authors show that Sine-based (SIREN) non-linearity is best suited for the kernel generating MLP, and an experimental evaluation is performed.",
            "main_review": "**Pros:** \n- Interesting concept of representing arbitrary-sized convolutional kernels.\n- The paper is well written, and the method's limitations are outlined clearly\n- Great evaluation of how a SIREN-based parametrization is best suited for realizing the MLP. \n\n**Cons:**\n- No code is provided during the review (although the authors claim to release it)\n- The experimental evaluation mostly consists of equidistantly sampled fixed-length time series (sMNIST, etc.), i.e., data for which CKConv provides only little benefits over standard CNNs.\n- Experimental evaluation with attention-based architectures on irregularly sampled variable-length sequences datasets, e.g., PhysioNet, MIMIC-III, and the Human Activity dataset, from Shukla et al. 2021 would have demonstrated the advantages and limitations much better.\n\n**Questions**:\n- How was the sequence reduced to a single prediction in the sequence classification tasks (sMNIST, etc.)?\n- The paper Gu et al. 2020 \"HiPPO: Recurrent Memory with Optimal Polynomial Projections\" was published at NeurIPS. Please update the reference.\n\nShukla et al. Multi-Time Attention Networks for Irregularly Sampled Time Series. ICLR 2021",
            "summary_of_the_review": "I am in favor of acceptance as the concept of CKConv provides enough contribution, although the experimental evaluation would have been stronger with standard irregularly time-series datasets (PhysioNet, MIMIC-III)",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper considers convolutional networks for sequence processing and suggests to parameterize convolutional kernels in this setting by small neural networks. These neural networks are similar to implicit networks in the sense that its input represents a time difference to the currently evaluated time point in a sequence. For each time difference, the associated weights of the convolutional kernel are computed.\n\nThis allows to handle arbitrarily large convolutional kernels (albeit the desired size needs to be determined somewhat deterministically apriori), to handle irregularly sampled data and to handle data at different resolutions. Good results on a set of experiments show the practical validity of the idea. The paper does a larger study into the importance of the type of activation functions. \n\n",
            "main_review": "This is a nice paper with one clear idea, which is demonstrated to work well. I like your detailed look at activation functions -- I think you should have a look at \"Gegenbauer Polynomials\". A potential weakness of the paper is it's limited set of experiments: Compared to the 'baseline' paper Bai 2018a, several datasets are missing, and, given the progression of time, several larger-scale datasets e.g. from speech or the financial domain could be added. ",
            "summary_of_the_review": "The described idea is a nice transfer from existing work of implicit neural network representations and continuous kernel formulation. Showing the importance of sine activation functions is very helpful to direct attention to the role of activation functions, depending on the actual modeling task. The empirical validation should be larger, though.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}