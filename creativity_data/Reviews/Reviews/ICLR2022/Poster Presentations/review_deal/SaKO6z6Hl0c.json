{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper received two accept and two marginally accept recommendations. All reviewers find value in the proposed supervised semantic segmentation methodology (making self-supervised representation learning towards dense prediction tasks like segmentation or clustering without explicit manual supervision) and appreciate the experimental gains, but had (mostly practical) criticism that was reasonably well addressed in the rebuttal."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper introduces STEGO (Self-supervised Transformer with Energy-based Graph Optimization), a novel feature correlation refinement method that builds on top of modern self-supervised visual backbones (visual-transformers) that generate dense semantically-correlated features in an effort to improve scene semantic segmentation without any type of labels (unsupervised). Different from previous works, the authors decouple the feature learning from cluster compactification and introduce a novel contrastive loss function (a combination of three correlation factors: KNN, self and random images) in order to further constrain the features to form compact semantic clusters without damaging their consistency throughout the dataset. The authors demonstrate state-of-the-art results on two popular benchmarks CocoStuff and Cityscapes. The results are quite striking in both a quantitative (method improves segmentation by a large margin (+14mIoU and +9mIoU) compared to recently published work) and also a qualitative manner (clearly distinguish the structure of the objects in the scene).",
            "main_review": "Strengths: The authors have properly addressed related work. The paper is well-structured and the method is sound. Design decisions are backed up by ablation studies. The method is feature extractor agnostic, which still makes STEGO relevant in the future as these technologies advance. \n\nWeaknesses: I haven't rigorously checked the mathematical formulation, but at the moment I do not find any reason to reject this paper. The pretrained backbone (DINO). used as a starting point (not fine-tuned) is quite strong and it would have been interesting to see what is the gain of using STEGO on weaker feature extractors. Also, I am a big advocate for highlighting the limitations of the method and also showing some failure cases to clearly state, from the authors' perspective, what other issues should the research community address in the future.  \n\nMinor comment: Found minor typos throughout the manuscript (difficult to pinpoint them due to the current paper format, lines without number) that do not necessarily impact the quality, but proofreading is highly recommended. (e.g. \"an an\", missing a/to, \"O-clamp\" should be \"0-clamp\")",
            "summary_of_the_review": "The contribution is solid - the problem is relevant and quite difficult. There is still so much work to be done in order to close the gap between unsupervised and supervised methods for scene semantic segmentation, but nonetheless, STEGO makes a step in the right direction. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a novel transformer-based unsupervised feature distillation method for semantic segmetation- STEGO (Self-supervised Transformer with Energy-based Graph Optimization). The pipeline has two separate stages, feature learning and cluster compactification (feature distillation). The feature network is DINO (Caron et al., 2021) - pretrained, frozen. \n\nThe main contribution is the cluster compactification(distillation) network. It features a novel 3-term loss function: Knn, self correltation and random image corelation. The authors also propose a number of strategies for performance improvement: clamping the segmentation feature correspondence tensor at 0, five-cropping the dataset (corners+center), spatial centering on the feature correspondences and conditional random fields for better segmentation at the object edges. Training the segmentation head is fast (a couple of hours on a recent GPU with the ViT-small transformer backbone) and the results are state-of-the art on semantic segmentation on CocoStuff and Cityscapes (+14mIou@CocoStuff, +7mIoU@Citiscapes).\n",
            "main_review": "The method builds on top of DINO (Caron et al., 2021). The features extracted by DINO are further refined with a small segmetation head that aims to boost performace by balancing Knn, self-correlation and random image correlation loss. The training batch consisnt of random images and random nearest neighbours (precomputed for the whole dataset). The segmentation head is a simple feed forward network - by avioding the backbone retraining (and implicitly knn recomputation), the method is very efficient to train. Hovewer, there are a nuber of problems related to the backbone - e.g., 40x40 feature resolution, which means small objects are difficult to resolve; the authors address this issue by five-cropping the dataset (corners+center crop), resulting in five times more images to find knns and better details (no rescaling to 224x224- the original transformer input size). The final steps involve clustering the resulting features and further CRF refinement for finer details in object edges. Additionally, the authors propose spatial centering on the feature correspondences, to improve segmentation of small features and clamping the segmetation correspondence at 0, to improve the optimization stability. \n\nOne does note that most performance improvement strategies are related to resolving small details that the original transformer network is incapable of doing. Therefore, I belive a future, better transformer that has fine detail capabilies would render those tricks obsolete. On the other hand, the performance is signinficantly boosted by the transformer. One would assume that future work would boost the proposed method, which has a significant performance boost over the baseline (DINO). Therefore, there are a number of plausible options of future research based on this work.\n\nAfter training STEGO on Coco (ViT-small, no CRF[the code has some issues]), I also also some practical aspects that have not been raised in the paper. Looking at the initial and final confusion matrix, the performance improves on several classes and worsens on others; I believe this is mostly due to the features extracted by DINO, but sometimes the errors are amplified - no investigation on this aspect.\n\nThere is an ablation study for clamping the segmentation feature correspondence tensor at 0, five-cropping the dataset (corners+center), spatial centering on the feature correspondences and CRF. The three term loss gets no ablation study (except for SC). At the very least, I would like to see numbers with knn only, self-correlation only and random correlation only. Can we improve the performance by designing better training pairs? I believe so, and this additional ablation study would have shed light on this issue. Otherwise, we have to manually tune the $\\lambda$s and the $b$ for each dataset (see page 20), and this doesn't look very appealing to me. Can the authors share some insights on this issue? I see the values are wildly different on Coco/Cityscapes. I'm willing to improve my rating, provided this issue is properly addressed. \n\nAlthough well written and illustrated, there are a small number of issues with the content of the paper:\n- 'typical results' images; 28 mIoU is pretty bad and most results from the paper look amazing; I have a folder full of truly random images, if anyone is interested :)\n- page 5, after Eq 4, shoud read 0-clamp, not O-clamp\n- page 15, In The >> In the",
            "summary_of_the_review": "This paper could have been outstanding, but as it stands, it's only acceptable. The best part is the unsupervised performance boost over prevoius methods (+14mIou@CocoStuff, +7mIoU@Citiscapes) and the ability to use the proposed model on top of most feature extractors. That being said, the authors have shown that the larger the transformer, the better the unsupervised segmentation, but have failed to provide a method that 'just works', regardless of the dataset. Instead, we rely on a number of tricks for performance boosting (mostly related to small objects) that could have been incorporated in the original transformer stage (fair enough, except for CRF, which is pretty much a popular solution for squeezing 2%-ish in accuracy) and some variables we need to manually set - the three-pronged loss that is at the core of the paper does not get a proper ablation study, each term is weighted empirically. Apart from the state-of-the-art performance, the training itself is kind of confusing - some classes do not benefit at all from this stage, sometimes the feature noise is happily propagated in the final segmentation, sometimes amplified - the limitations of the feature extractor and loss have not been thoroughly investigated, IMHO. This is not shown in the main paper, but I have tried the provided code on CocoStuff and those are my findings. Nevertheless, being feature extractor agnostic, future work could expand and improve the performance of this method. Furthermore, the authors suggest that label ontologies can be arbitrary - future work could benefit from a hierarchical ontology, for example. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed STEGO, an unsupervised approach for semantic clustering/segmentation using feature refinement on top of self-supervised neural networks. A distilled version of self-supervised features is learned segmentation specifically through a feed-forward network via SGD.\n\nThe main contribution comes from an idea of distilling powerful deep features from strong self-supervised backbones to further improve their semantic discriminativeness. Though the idea is intuitive, several loss functions and regularisations are proposed to avoid trivial solutions and make the idea really work in practice. Extensive qualitative and quantitatve results are done to demonstrate the performance of STEGO and its design choices.",
            "main_review": "Overall I like the idea of this paper and vote for acceptance. The idea of distilling frozen and general self-supervised features for segmentation specific features is neat. Though as expected, there are several regularisations and 'tricks' to really make the idea work in practice, they are well-motivated and valitdated in the ablation experiments.\n\nMy major concern is about more clarifications of details and some additional ablation models (see cons below). Hopefully the authors can address my concern in the rebuttal period. \n\nPros:\n- The paper is well written and easy to follow. The key design choices in the paper are well motivated and described in the paper.\n- The idea of pushing features from self-supervised representation learning towards segmentation task-specific ones is really interesting, simple and effective.\n-Extensive experiments are done to validate the system and promising segmentation quality is achieved.  \n\nCons/Suggestions:\n\n(1) The discussion in Sec 3.3 could be better arranged in my opinion and it is easier for readers to follow the main idea by jumping from Sec 3.2 to Sec. 3.4. I agree the discussion is helpful to connect the idea of this method to undirected graphical model and can be put into later part of paper.\n\n(2) Is the choice of distance very improtant generally? In addition to consine distance, do normalised l2-distance or RBF/Gaussian perform similarly? I think the choice of distance or similarity metric is quite important and worth more discussions.\n\n(3) I would like to see more qualitative results of STEGO without CRF step as CRF usually smooths out the predictions. It is more informative for readers to know the \"raw\" segmentations before CRF to better understand the systems. Do segmentations before CRF usually become noisy or have coarse boundries aligned with images? \n\n(4) More clarifications:\n- Since 5 crops are used before KNN sampling, is there a tendency to select other crops from the same image instead of different images? If it is true, is it intended to do so ? More clarifications would be good. \n\n- Similar to last point,  in Table 2, the effectiveness of 5-crop and SC are not properly adjustified. How about the performance with a full  STEGO model exluding 5-crop or SC only, like the ablation of CRF. I am wondering, instead of adding on top of vanilla baselines, if the contribution of these designs will be less prominent when other modules already exist?\n\n- I was a bit concerned about the huge computational cost in constructing dense correspondence tensors F and S at the beginning. In supplement it is mentioend that 121 samples are taken per step per training image, is this used to alleviate the cost of building these dense matrix? Will STEGO work better given larger training images with more closer details?\n\n\nSome typos: \n\n(1) Sec 2 Unsupervised Semantic Segmentation: between an an-> between an.\n\n(2)  Sec 4.2: We show some examples segmentations -> We show some example segmentations.\n",
            "summary_of_the_review": "I have carefully read the paper and like the idea of this paper in making self-supervised representation leanring towards dense predicton tasks like segmentation/clustering without any explicit mannual supervision.\n\nI am aware of pros and cons in this works and would like to vote for acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the author proposes an unsupervised semantic segmentation approach STEGO. The core of STEGO is a novel contrastive loss function that encourages features to form compact semantic clusters. ",
            "main_review": "Strength:\nOverall, writing of this paper is clear and straightforward. STEGO achieves state of the art performance on both the CocoStuff and Cityscapes segmentation challenges. \n\nWeakness:\n1)\tNovelty: \nThe main contribution is that it trained the additional segmentation head with the proposed contrastive loss over the frozen visual backbone. The novelty of utilizing the contrastive learning to form feature clusters is not so obvious. \n\n2)\tImplementation: \nFirst, the approach seems complex to implement. For example, there are too many hyperparameters (eg, b and lamba) in the contrastive loss definition and it seems that the tunning of the parameters is complex.\nSecond, as is pointed out by the authors, the optimization is sometimes unstable. Adding clamping step seems not completely solves this problem.\n\n3)\tExperiment：\nThe state-of-the-art segmentation performance may be due to many factors such as strong DINO backbone features, CRF processing etc. Hoping more ablation studies are performed on each component of the proposed contrastive loss to verify the effect. For example, more ablation studies based on the ViT-Base architectures of DINO are preferred. The ablation studies on the hyperparameters are also preferred to be added. \n\n\n\n\n\n",
            "summary_of_the_review": "I tend to weakly reject the paper based mainly based on the concerns about implementation and experiments mentioned above. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}