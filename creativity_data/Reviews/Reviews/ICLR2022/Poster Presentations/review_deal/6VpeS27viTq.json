{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper considers a relevant and interesting problem of protecting the intellectual property of data. The goal of the proposed method is to prevent unauthorized usage of the data, and the protection is attained when a model trained on the perturbed dataset will predict poorly and thus cannot be considered as a realistic inference model by the unauthorized attacker. \n\nTechnically, the paper tackles the problem of \"unlearnable examples\": to perturb the images of a labeled dataset to obtain perturbed dataset such that models trained on perturbed dataset have significantly lower performance, the perturbations are small, and one can approximately recover the original labeled dataset with the correct \"secret key\" (learnable parameters).\n\nThe authors propose two invertible transformations to craft adversarial perturbations: linear pixel-wise transformation and convolutional functional transformation based on invertible ResNet. Numerous experiments demonstrate the effectiveness of the proposed transformations in both securing the data (making the data unlearnable when transformation is applied) and unlocking the transformation (making the data learnable when the transformation is inverted).\n\nThe paper is well motivated and exhibits competitive results. Although there are some concerns about the similarity of the work compared with [1], we believe the additional constraint of this work, that one can approximately recover the original labeled dataset with the correct \"secret key\",  justifies a significant contribution. \n\n[1] \"Unlearnable Examples: Making Personal Data Unexploitable\" Huang et al., ICLR '21"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Paper presents an idea of “Learnability lock”, a system that aims to control learnability of individual datapoints or data classes. The system builds on top of recent work on data learnability, where, in essence, noise is applied to individual datapoints in a way to disrupt learning generalisable features. From the description, it appears that it works as a poisoning or a backdoor attack, where the objective is to correlate non-generalisable features. \n\nNow, learnability lock describes a mechanism that enables unlearnable features, whilst at the same time allowing the user to revert unlearnable noise if authorisation is acquired. Authors evaluate Learnability lock with a number of datasets and network architectures to find that it outperforms its competitors in presence of defences.\n\n\n",
            "main_review": "Strengths:\n+ Interesting setting and a challenging threat model\n+ Thorough evaluation \n\nWeaknesses:\n+ Narrative being very far from reality \n+ Unclear contributions over and above Huang et al. \n\n\nFirst, I want to mention that the paper is very well written, thoroughly evaluated and presents a convincing technical story for an ML audience. Yet, for people more familiar with Computer Security, and “do not roll your own crypto” principle, it presents a rather weak argument as to why any protection is provided with such an encryption scheme at all. In fact, the results from adversarial training tell that the system design is broken and the paper contradicts its own story. \n\nSecond, Carlini et al. in ‘Is Private Learning Possible with Instance Encoding?’ talk about lack of rigorous notions for privacy and use of ad-hoc arguments in instance encoding schemes such as InstaHide. It seems that this paper follows the same premise and provides no justifications for its, rather strong, claims. I am very confused about the setting of the paper. \n\n```\nExisting methods include training ML models on encrypted data, where the sensitive information could be hidden through cryptographic approaches in order to prevent malicious manipulation (Hesamifard et al., 2017; Ding et al., 2021). However, those kinds of data processing methods normally do not preserve visual properties from the raw data and thus affect normal use. For example, it does not make sense for one to send a fully encrypted and unrecognizable “selfie” photo to friends just to make sure the photo will not be exploited without authorization.\n```\n\nIndeed, if the goal is confidentiality and control of ones secrets, it does make sense to send data encrypted, a de-facto standard in private end-to-end communication. There is a branch of cryptography focusing on controlled disclosure of information if needed e.g. Partially Revealing Cryptography or even Deniable Cryptography. What is more, it does make sense to send ones ‘selfie’ encrypted over vetted channels and theoretically sound protocols — that way privacy is not going to be violated. In fact, the paper does discuss that data transfer is a potential security risk, yet argues to use a mechanism that leaks data by default:\n\n```\nFrom the data providers’ perspective, this means that any mistake conducted in the data transfer procedure could lead to potential privacy/security risks. This raises great challenges in securely distributing sensitive data from the data providers to the clients.\n```\n\nIt took cryptographic community decades to establish best practices, develop theoretically and practically rigorous attacks and defences. Currently, there are international standards, competitions, and good understanding of privacy guarantees of different encryption mechanisms. ‘’Rolling your own crypto’’ leads to mistakes. Thus, I am unable to understand how the approach proposed in this paper improves upon the state-of-the-art in secure data transfer in Machine Learning.  \n\nIn light of the above, I was unable to understand the threat model in which the paper proposes to provide a sensible mechanism to protect ones privacy, limiting its contributions over and above Huang et al. I also did not understand how the proposed approach innovates over a scheme constructed using standard cryptography toolbox like AES to make learning impossible.  If the authors are able to clarify these concerns, I would be happy to revise my review.\n\n\n\n\n\n",
            "summary_of_the_review": "Paper presents an interesting construction for controlled learnability, but the setting of the paper seems to be a bit misleading. Paper argues that learnability lock provides privacy through their encryption scheme, yet in the same paper authors break their own scheme. Paper also provides no cryptographic underpinning as to why any privacy is provided at all.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "- The paper tackles the problem of 'unlearnable examples': to perturb the images of a labeled dataset $D_c$ to obtain $D_p$ with the desiderata (a) training models on $D_p$ leads to models with significantly lower performance (b) image perturbations are constrained to some $\\epsilon$-ball and (c) with the correct 'secret key' (learnable parameters in this case), one should approximate recover $D_c$.\n- The approach `learnability lock' to make the examples unlearnable involve a bi-level minimization objective over the original network parameters $\\theta$ and parameters $\\psi$ of a 'secret key' network $g_\\psi$ (which could be a linear layer or an invertible ResNet).\n- Experiments demonstrate among other things: (i) training networks on $D_p$ leads to poor performances and on unlocked $D_c$ to high performances (ii) the proposed approach is more robust to augmentation strategies (specifically adversarial training) compared to previous approaches.",
            "main_review": "**Strengths**\n\n1\\. Well-motivated problem and reasonable results\n- Given the prevalence of data being shared and made accessible everyday, it becomes crucial certain authorized uses e.g., preventing malicious entities on training a facial classifier on provided data. This paper takes an interesting step towards addressing this issue. The authors also show certain benefits of the approach e.g., controlling access to who can train models on the given data.\n\n2\\. Writing\n- I highly appreciate the clarity in writing. The paper was clear for the most part and easy to follow. \n\n**Major Concerns**\n\n1\\. Significance of contributions over prior work\n- My first concern is the significance of the contributions over prior work and most notably with [1]. Similar to proposed approach, [1] also propose a bi-level minimization approach to introduce class-specific additive perturbations to prevent training on the perturbed dataset. My sub-concerns: \n- (a) it seems that the improvement of the proposed approach is primarily addressing a specific failure mode -- being more robust to adversarial training on the perturbed dataset. Apart from that, [1] seems to outperform the proposed approach in few other scenarios e.g., when comparing test accuracy (Table 1) [1] appears to outperform (esp. on CIFAR-100 and ImageNet).\n- (b) As for the technical contributions itself, the proposed approach also shares a bi-level minimization problem similar to [1]. But instead of learning fixed additive per-class perturbations, a network $g_\\psi$ is trained to perturb the inputs. Could the authors comment on technical benefits over previous approach? I see that in Table 9 remarks the benefits as \"adv. train\" (which is supported) and \"stealthiness\" (which is unclear; aren't both methods evaluated at a comparable $\\epsilon$?)\n\n[1] \"Unlearnable Examples: Making Personal Data Unexploitable\" Huang et al., ICLR '21\n\n2\\. Optimization formulation\n- I was also a bit unclear (or rather could not intuit) on how the optimization objective (3.1) makes the examples unlearnable.\n- Specifically, wouldn't the optimal $g_\\psi$ be an identity function which would lead to original 'clean' accuracy? In other words, if one were to take a pretrained $\\theta$, I reckon $\\psi$ would correspond to identity function?\n- I'm also wondering how the approach learns good parameters $\\psi$ in spite of an explicit term to maximize the loss of training on the perturbed dataset?\n- I would appreciate if the authors slightly elaborated (to complement an already nice discussion below eq 3.1) on the optimization problem.\n\n**Minor Concerns**\n\n1\\. Table 1\n- I think the results in Table 1 is comparable to previous approaches (e.g., Table 1 of [1])? I would appreciate the authors also added corresponding rows (or alternatively a more descriptive table in appendix) so that direct comparison with prior work when possible. As a reader, I had to flip back and forth to compare the numbers.\n\n2\\. Unauthorized use of perturbed dataset for other tasks\n- Are the authors aware whether training on $D_p$ fails only the envisioned task (e.g., 10-way multiclass classification over predefined CIFAR classes)? It seems problematic if the same images could be used for other tasks (e.g., face recognition) unforeseen by the creator.\n\n**Nitpicks**\n* \"Control over Single Class\": What is the accuracy of the model on locked bird class? What is the number in the heatmap of Fig. 2?",
            "summary_of_the_review": "- Overall, the paper tackles a reasonably well-motivated idea of introducing perturbations to prevent unauthorized training over a labeled dataset. The authors show that the approach achieves this objective and is additionally more robust when compared to previous works.\n- My biggest concern is the significance of improvements over prior work, which appears limited to a highly specific failure mode. I would be glad to increasing my score if the authors could address this.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a new concept called \"learnability lock\" for securing distribution of sensitive data. The idea is to apply an invertible adversarial transformation when releasing the data, which makes the data \"unlearnable\" for machine learning models, but also preserves the visual properties of the data. Therefore, unauthorized practitioners can not use the data for training ML models, however authorized users can use specific key to invert adversarial perturbation and make the data \"learnable\" again. \n\nThe authors propose two invertible transformations to craft adversarial perturbations: linear pixel-wise transformation and convolutional functional transformation based on invertible ResNet. Numerous experiments demonstrate the effectiveness of the proposed transformations in both securing the data (making the data unlearnable when transformation is applied) and unlocking the transformation (making the data learnable when the transformation is inverted). ",
            "main_review": "The idea of using invertible adversarial transformation for securing the data is interesting and novel. The paper is very well written and structured. Numerous experiments are conducted to show effectiveness of the method for breaking the ML models when \"secured\" data is used and for training accurate ML models when the inverse transformation is applied. The authors also show that their method is more robust to adversarial learning than state-of-the-art additive perturbation methods. The method is shown to be robust to strong data augmentation and filtering techniques which are often used as defense methods. Finally, the authors conduct experiment to show that when two different adversarial transformations are computed, their keys can not be used to unlock each other's transformation. \n\nI have two questions for the authors: \n1. What data was used to train the attacker model (f_theta)? Also, I can see that the ResNet-18 model was used for computing the attack, did you try to use other backbones? Does the attack still transfer to other model in this case? It would be great to elaborate on that somewhere in Experiments section. \n2. In section 4.3 it is said \"we train two learnability locks for each transformation separately\". When learning two transformations from the same data, how can one control that the transformations will be different and that the model will not converge to the same solution twice? \n",
            "summary_of_the_review": "Overall, I believe that the idea of the paper is interesting and novel, the results are appealing. The writing is very clear and structured, the method is compared to other attacks and defenses and is shown to outperform previous methods. I vote for accepting the paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces the idea of “learnability lock” for data authorization. This paper along with three prior works mentioned in the paper are all creative works recently proposed for data protection, an important research topic that has been investigated much before.  Compared to the “unlearnable examples” work of Huang et al. 2021, this work proposes to use inevitable linear/convolution transformations to formulate the noise. This was demonstrated to have two benefits that seem to address two important limitations of previous works: 1) making the unlearnable noise more controllable so “learnability” can be flexibly locked and unlocked; 2) the noise seem can survive (to some extent) adversarial training now.",
            "main_review": "Strong points:\n1. An advanced exploration of an important research direction. The idea of “learnability lock” appears fairly interesting, although it is motivated by the “unlearnable examples” idea. The “learnability lock” concept is one step closer to what these works try to achieve: giving data owners some real control of their own data without damaging (much) the original content.\n\n2. The proposed invertible noise generation method looks simple and effective, should be easily applied in real-world scenarios.\n\n3. The proposed method holds two advantages (invertible/controllability and robustness to adv train) over the prior work and these two advantages seem to be quite important for real-world usage.  \n\n4. The paper is well written, pleasant to read. The experiments are comprehensive. \n\nWeak points:\n1. It is not very clear why invertible transformation improves robustness to adversarial training. Although it explains somewhere, conceptually, multiplicative noise is more effective than additive noise, I think some experiments can be conducted to dig a bit deeper on this point. Looking forward to some explanations from the authors.\n\n2. The robustness to adaptive adversarial training part in the appendix is also important. I suggest the authors provide a paragraph of analysis in the main text as well. Also, some experiments in the appendix are not mentioned in the main text.\n\n3. The experiment structure looks quite similar to Huang et al. 2021, I suggest the author acknowledge their work/setting in the experimental setting. \n\n4. The 100% unlearnable rate seems to be a remaining limitation of your and Huang’s work. Any thoughts on this?\n",
            "summary_of_the_review": "Important research on data protection/authorization, simple yet very effective method, some level of technical improvements over prior work but addresses two key limitations, rich experiment, good writing. A quite solid paper overall. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethical concerns are identified for this paper.",
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}