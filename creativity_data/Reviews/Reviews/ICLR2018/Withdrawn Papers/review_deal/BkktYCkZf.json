{
    "Decision": "",
    "Reviews": [
        {
            "title": "A technical contribution to the optimization of neural networks",
            "rating": "3: Clear rejection",
            "review": "In this paper, the authors propose to have a different learning rate depending on the class of the examples when learning a neural network. \n\nThis paper's contribution are quite moderate, as the proposed method seems to be a very natural extension but it is backed up by lots of numerical results. \nHowever, it would have been interesting to show the evolution of the learning rates (for every class) along the epochs and to correlate this evolution with the classes ratio or their separability or to analyse more in depths the properties of the obtained networks.\n\nIt is rather unclear why changing the learning rate affects the performance of the model and it is would have been interesting to discuss.\nMoreover, it would be interesting to show if this class-based learning rates changes the convergence of the model or if the early stopping occurs earlier etc...\n\n",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "An interesting work but lacks of convincing empirical study and sound justifications",
            "rating": "3: Clear rejection",
            "review": "Summary: This paper addresses the problem of training deep fully connected networks (FCNs) by introducing a class-dependent learning rate to each of the network weights. The method works by introducing a parameter associated with each weight to scale its learning rate based on the class label. The introduced parameters are not involved in the forward pass and hence cannot be updated by Backprop. Therefore, the authors derived the update formulate based on the analytical continuation technique. The authors experimented on the UCI data sets with different activation functions to show the efficacy of their proposed method.  The idea of using class label to adjust each weight’s learning rate is interesting and somewhat novel, but unfortunately the efficacy is not well justified in theory. The empirical study is not always convincing, and did not compare with many state-of-the-art baselines. Overall,  the study is interesting and contains some new idea. However, the work is not mature enough for publication. \n\nOriginality:  Training very deep networks have always been important in Deep Learning and the idea of using class label to adjust each weight’s learning rate is somewhat novel. However, the paper falls short in lacking of theoretical justification and convincing empirical results. \n\nClarity:  The paper is clearly presented and easy to follow. However, many related works are missing in the literature, for example, Highway Networks [1],  Deeply-Supervised Nets [2] and Deep Networks with Stochastic Depth [3], etc.\n\nSignificance:  The paper lacks of theoretical justification as well as the experiments are not convincing. Specifically, the paper lacks of justification on why adjusting the learning rate based on the class labels are crucial to improve training FCNs, more specifically, how does it help resolving the exploding and vanishing gradient problems? \n\nThe trick to reset $\\mu$ after half an epoch at the end of Section 3 is too heuristic. There lacks of explanation. \n\nThe experimental results are not convincing. The proposed method doesn’t seem always outperforming the baselines. There is no discussion on the failure cases.  In addition, the authors should compare with more baselines such as [1], [2], [3] and try with deeper networks as many networks used in the experiments are not very deep, with only 8 layers or less. Besides, the idea of using adaptive learning rates are not completely new, and somewhat closely related to second order optimization methods. It will be interesting to compare with some existing second-order optimization algorithms for deep learning. Last but not least, it would be more convincing to show the convergence speed of the proposed method. \n\nother question: In Eqn.4-5 , the terms $O(\\alpha)$ and $O(\\alpha^2)$ are omitted, however, since $\\mu_w^j$ are updated with its own learning rate, would it be better to increase the learning rate $\\alpha_{\\mu}$ and use the term $O(\\alpha^2)$ in the gradient update (6) as it would better approximate the gradient direction?\n\nReferences\n\n[1] Srivastava, Rupesh K., Klaus Greff, and Jürgen Schmidhuber. \"Training very deep networks.\" Advances in neural information processing systems. 2015.\n\n[2] Lee, Chen-Yu, et al. \"Deeply-supervised nets.\" Artificial Intelligence and Statistics. 2015.\n\n[3] Huang, Gao, et al. \"Deep networks with stochastic depth.\" European Conference on Computer Vision. Springer International Publishing, 2016.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Proposing a training trick, not at the expected level",
            "rating": "3: Clear rejection",
            "review": "This paper proposed a training strategy for fully connected neural network, where per class, per weight learning rate is used. Results are reported on about 50 UCI datasets with different topologies.\n\nPros:\n\n1. This paper proposes a simple and intuitive approach for training neural networks. \n\n2. The idea in the approach seems novel (I have not found the similar training strategy) and they tested several UCI datasets with different fully connected network topologies and activation function settings. They found it improved the accuracy.\n\nCons:\n\n1. This paper proposed a rather ad hoc proposal for training neural networks. This is not supported by any strong theory or conceptual idea. Grounding the proposal with some theoretical or conceptual arguments could have made the proposal sounder. For now, it looks like more like a trick than anything else.\n\n2. The analysis of all facets of the proposal is missing. There is no indications on the extra computations required for handling this modification compared to standard training. It is very probably more costful. Likewise analyzing results for varying number of classes (e.g. 100 classes with Cifar-100, 1000 classes with ImageNet), with imbalanced datasets is required.\n\n3. More importantly, the paper tested only on small size datasets (UCI), where the inputs are relatively well-structured. There is not unstructured datasets (e.g., images, text documents, speech) tested, which is where deep learning is making sense. \n\n4. The Experiment section is not well structured, at least for me, I cannot understand it well. In particular, the strategy for choosing the hyperparameters (e.g., \\alpha, \\alpha_mu, local learning rate, \\alpha_mu) need to be developed . Perhaps the authors can make a table listing the hyperparameters or a diagram describing the whole training procedures.\n\n5. For each weight w, we add K learning rates u_w^j. It means that we multiplied by K the number of parameters in our model (K is the number of classes). That’s a lot. Given that we are thus optimizing with (K+1) times the number of weights of the network, I am not sure that comparing with the vanilla network trained in usual way is fair.\n\nSpace was clearly not an issue with the paper, it still have available space to add further explanations\n\nThe paper is proposing a trick to train neural networks that is backed by strong arguments. The assessment of the method is incomplete and not convincing. I thus recommend a clear rejection.\n\nComments on the details of the paper:\n\nThe paper does not write mathematically rigorous. For example, \n- Section Method, equation (4) \n\\partial L/ \\partial w (1 + \\alpha(\\mu_w)^T y (\\partial L / \\partial z \\partial w) = \\partial L/ \\partial w + O(\\alpha)  satisfied if and only if \\partial L / \\partial z \\partial w is bounded. i.e |\\partial L / \\partial z \\partial w|<= B.  However, the author did not suppose this condition. Since L is NON Convex, it could not be automatically considered as bounded.\n - Section method, paragraph under equation (2) L(z(\\alpha),x,y)<=L(w,x,y) is NOT necessary. It should be supposed L is at least locally convex. \n-   Equation (5)  should be - O(\\alpha^2) \n\nThe choosing of \\alpha_\\mu is generally large (10^4-10^5). Does it have some support?\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}