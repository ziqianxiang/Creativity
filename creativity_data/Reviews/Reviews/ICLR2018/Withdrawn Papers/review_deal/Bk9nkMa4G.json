{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting idea, not corresponding title/focus",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The main goal of this paper is to learn a ConvNet classifier which performs better for classes in the tail of the class occurrence distribution, ie for classes with relatively few annotated examples. In order to do so, they constrain the final softmax layer, using weights and biases based on the class means, in a nearest-class-mean style layer. In practice the class means are \"learned\", yet regularised towards the batch class means.\n\nMy main concern with the paper is in the theoretical underpinning of the work. From the title a Bayesian approach is suggested, while in practice a rather standard softmax classifier is learned, albeit with a different regulariser (last layer is regularised towards batch class means). Also the Gaussian Mixture Model, is not a true mixture model, in the sense that normally GMMs are used for describing a distribution of unlabelled data, in this case, each class is described with a \"Gaussian\", and thus the class probabilities are the reseponsibilities proportional to the class Gaussian. To take this one further, it is assumed that there is equal class probabilities and each class has a the same Identity matrix as covariance matrix. Taking away a large part of the Gaussian distribution. The relation (Eq 6) with Softmax is insightful, yet already discussed in eg Mensink et al 2013 (already cited for the Nearest Class Mean classifier). \n\nA second concern is the experimental exploration. First of all, it is unclear if the method works much better for the tail than the standard softmax. That is not apparent from the results. For example, Fig 4 shows -except for CIFAR10- not a clear relation between class index and proposed relative improvement, it is also unclear if there is just a difficult class (eg at index 150), or that the experiment has been repeated several times. Moreover, when the performance becomes more stable for the classes in the tail, I'd have expected that the standard deviation of the mean class accuracy would decrease, from the results there is no difference between Softmax and the proposed method: 44 +/-1 for Softmax (miniImageNet) to 41 +/-1 for the proposed NCM approach. In the final experiment is the regularised version  compared to an unregularised one, which shows that the first performs better. However, I'm a little unsure about these conclusions, what is the unregularized version exactly doing, how is it different from a standard softmax?\n\nRemaining (minor) remarks:\n- It is unclear how iCaRL has been used - it has been proposed as an iterative classification method.\n- Eq 2: how would this perform on a learned Softmax representation? Preferably including the (co)variance and class priors?\n- Figure 4: Gain -> Relative performance\n- The batch size must have a great influence on the functioning of the regularisation (especially when there are many classes, in that case just a single example counts for the class mean). This is not explored in the paper.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea. Not sure about novelty and depth of experiments.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper aims to address a common issue in many classification applications: that the number of training data from different classes is much unbalanced. The paper proposes a Bayesian framework to address it with a Gaussian mixture model.\n\nOverall the math looks reasonable. I am not sure about the novelty of the paper, as it is a relatively standard definition of Bayesian math. Essentially, instead of computing a softmax prediction which is the discrimination probability of each class given the input, one uses a logistic regression type interpretation (equation 1). This has been used in multi-class classification before. For example, many early SVM papers deal with multi-class classification by training 1-vs-all classifiers on each class and then choose the one having the highest score (possibly with a class-prior adjustment).\n\nNote that this actually changes the underlying assumption a bit: softmax basically assumes the classes are mutually exclusive, while this interpretation implicitly assumes that the classes are not related to each other - an image could belong to multiple classes. This probably does not match the assumption of many of the datasets being tested upon (CIFAR, MNIST) but I don't consider that a fundamental issue.\n\nI am quite a bit concerned about the experimentation protocol as well. The datasets are relatively smaller scale, and datasets such as MNIST and CIFAR are known to overfit. As a result, although there are approaches taken to generate unbalanced datasets out of them (e.g. MNIST). Regardless, the results seem to suggest that the proposed method is similar to softmax performance - which is expected as they are similar - but I am not sure if it accurately evaluated / analyzed the possible application and performance gain of the proposed method.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "No Title",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper presents a method based on a Bayesian classifier that improves classification of rare classes in datasets with long tail class distributions. The method is based on balance the class-priors to generalize well for rare classes. By using a Gaussian Mixture Model (GMM), authors are able to obtain a factorization of class-likelihoods and class-priors leading to a closed-form maximum likelihood estimation that can be integrated to differente classification models, such as current deep learning classifiers. Authors, also propose an evaluation approach that addresses the bias towards the head and intra-class-variation of classes in the tail.\n\nThey face class imbalance problems, particular long tail distributions, by fixing: i) The covariance matrices of all the classes to be the identity, and ii) The priors over each class to be uniform. So all classes, popular and rare, have equal weight for Bayesian classification. To me, this is not a fundamental way to solve the long-tail problem, in the sense that by fixing isotropic likelihoods and flat priors, authors are also ignoring information that can be relevant in some classification problems, where a good prior can be useful to disambiguate confusing situations. On other hand, using a unimodal function to model each class is an over-simplification that ignores intra-class complexity. \n\nThe datasets used by the authors are balanced, so they artificially transform them into long-tailed, it will be good to test directly on real long-tailed datasets. The experimentation is only performed using small to medium datasets (< 80K instances), it will be good to show if the benefits of the proposed approach can also be present in the case of large datasets. In this sense, I agree with the authors that the evaluation protocol for long-tailed datasets can't be just based on average accuracy, however, the protocol proposed requires to train the model several times, therefore, it does not scale properly to large datasets that are the common rule in the deep learning world. Results respect to similar state-of-the-art techniques shows a reasonable improvement (depends of the dataset, approx. 1-3%).",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}