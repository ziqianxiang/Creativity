{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This work proposes DIVE (distributional inclusion vector embeddings), an unsupervised method for hypernymy discovery that preserves the inclusion property of old fashioned sparse BOW representations (i.e., that hypernyms tend to have higher counts in more varies contexts than hyponyms). The learned representations are evaluated on a large set of datasets and are shown to work well. The work is very thorough, and almost reads as a systematic study in places. Unfortunately, this makes it hard to follow sometimes, and although the work is generally interesting, I am left to wonder about its novelty and general impact on the field.\n\nThe main proposition essentially comes down to a slight tweak in word2vec's SGNS method: instead of shifting the PMI value of word co-occurrences by 1/k, we shift by #w/(k*|D|/|V|) and add a non-negativity constraint, i.e., we explicitly force Eq. 2 to be true in perfect reconstructions. This makes sense and I am surprised nobody has done it before. It feels like a minor tweak, however, and any major improvement seems to be largely dependent on the scoring function, rather than on this slightly different objective. Consequently, I feel like the paper is in two minds about what it wants to be: a systematic study of hypernymy detection methods, or an introduction of a novel algorithm for learning distributional inclusion embeddings.\n\nI like the main ideas of this work and I think it's great that the study is so thorough, but I don't think it should be accepted in its current form. Main concerns:\n\n- Presentation: the experiments show that (1) DIVE outperforms GE, HyperScore and H-Feature baselines; (2) which scoring function works best for DIVE; (3) DIVE outperforms SBOW in many cases, but not always, though better on average; and (4) we can use DIVE for WSD (only shown qualitatively). The paper spans 13 pages, excluding appendices, which is rather long. I feel that (2), (3) and (4) are part of a systematic study/review paper, while (1) could be interesting in and of itself, if it included a full comparison against alternative methods. The way results are presented in the tables is confusing, and it's unclear why only one baseline is included in each case.\n\n- Comparison: it appears that there are methods missing from the results tables. On HyperLex, for example, results as high as 0.512 (Poincare embeddings), 0.540 (HyperVec) and 0.686 (LEAR) have been reported, while the highest result in the paper is 34.5. That's a big difference. On Weeds' BLESS, it's 68.6 versus e.g. 0.75 for Kiela et al. (unsupervised, using images) and 0.850 for HyperVec. If these results were omitted due to space, I think experiment (2) and (4) can safely be moved to the appendix. Especially for something that hinges on being a review paper, such as this work, it is important to be complete.\n\nIn short, I think the presentation doesn't help; I am left to wonder what the main contribution is of the work; and I think the comparison to previous work is incomplete.\n\nQuestions:\n- Why use WaCkypedia? It's old and, by now, small. It would be interesting to try all scoring functions and types of model (PPMI, PPMI with shift, PPMI with inclusion shift) trained on the exact same corpus and same negatives, and showing that it works best there.\n- General question: Is Vendrov's test set only yes/no, e.g. if I set the threshold really low, I get 100% accuracy, or does it contain negatives? Same for BLESS. If so how valuable is this evaluation?\n- To what extent are the baselines tuned in the semi-supervised case? If the numbers are from papers, that should be mentioned. It would be better to use the same corpus, and the same amount of attention to tuning the results, for both cases.\n\nMinor/Typos:\n- Medicl\n- \"From the recent review (Santus et al) ... suggested by the review study (Vulic et al)\" this way of citing reviews isn't very pretty",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Clever model, problematic experiments",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper presents a new method for detecting hypernymy by extending the distributional inclusion hypothesis to low-rank embeddings.\n\nThe first half of the paper is written superbly, providing a sober account of the state-of-the-art in hypernymy detection via distributional semantics, and a clear, well-motivated explanation of the main algorithm (DIVE). The reformulation of PMI is interesting; the authors essentially replace P(w) with the uniform distribution (this should be stated explicitly in the paper, BTW). They then augment SGNS to reflect this change by dynamically selecting the number of negatively-sampled contexts according to the target word (w). This is a clever trick.\n\nI was also very happy to see that the authors evaluated on virtually every lexical inference dataset available. However, the remainder of the paper describing the experiments and their results was extremely difficult to parse. First of all, the line between the authors' contribution and prior art is blurry, because they seem to be introducing new metrics for measuring hypernymy as part of the experiments. There are also too many moving parts: datasets, evaluation metrics, detection metrics, embedding methods, hyperparameters, etc. These need to be organized, controlled for, and clearly explained.\n\nI have two major concerns regarding the experiments, beyond intelligibility. First, the authors used a specific corpus (with specific preprocessing) to train their vectors, but compared their results to those reported in other papers, which are not based on the same data. This invalidates these comparisons. Instead, the other methods should be replicated and rerun on exactly the same corpus. I would also recommend using a much larger corpus, since 50M tokens is considered quite small when training word embeddings.\n\nMy second concern is that summation (dS) is doing all the heavy lifting. In Table 2, we can see that the difference is only 0.1 between dS and W * dS (where W is also not trained using DIVE). Since dS is basically a proxy for difference in word frequency, could it be that the proposed method is just computing which word is more general? This looks awfully familiar to Levy et al's prototypical hypernym result.\n\nMiscellaneous Comments:\n- There's a very recent paper on using vector norms for detecting hypernymy that might be worth contrasting with: https://arxiv.org/pdf/1710.06371.pdf \n- Micro-averaging these datasets is problematic, because some of the datasets based on WordNet are much larger than the hand-annotated ones, and will likely drown them out. Because these datasets are so different, I think it is critical to look at the details and not only at the averages.\n- PMI filtering needs to be controlled/ablated in the experiments.\n- While explaining equation 6, the authors say that the gradients for x and y are similar; this is not true, because k is a function of x (or y), and if one appears more often in general (not necessarily with c), then the gradients will be different as well.\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting approach, but paper needs to improve in presentation and experiments",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper presents DIVE, a unsupervised method to learn word embeddings while respecting the distributional inclusion hypothesis (DIH). DIH suggests that the context of a hypernym tends to contain the context of its hyponyms. DIVE may be thought of as a unsupervised and direction-reversed version of previously proposed order embeddings. DIVE captures the DIH  heuristic requirement by posing the embedding learning problem as a NMF problem. Modulo construction of the input matrix, the method is quite similar to other word embedding methods like GloVe.\n\nThis is overall an ok paper, but I have a few concerns. Firstly, even though the authors have explained in Sec 2.1 how the inclusion constraints are incorporated in the matrix factorization itself, it is not very clear. The authors need to do a better job in improving clarity in this part.\n\nSecondly, and more importantly, I am not sure why SBOW is used as a baseline, even other unsupervised embeddings such Word2Vec or GloVe are readily available and likely to perform better than SBOW. This is a serious limitation which makes it really hard to evaluate true significance of the results.\n\nThirdly, the fact that all baselines are not applied on all or most of the datasets is unsatisfactory. I think the paper will improve significantly if an uniform evaluation is presented.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}