{
    "Decision": "",
    "Reviews": [
        {
            "title": "The experiments cover the presented scope of the work well but the scope needs to be motivated and may need to be expanded, and the goals of the work should be justified.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "OVERALL\nThe paper validates the idea that deep convolutional neural networks could learn to cluster input data better than other clustering methods. This is explained by noting the ability of a deep network to interpret the context of every input point due to a large field of view. However, this explanation is not explored. Also, the clustering method used is supervised and is compared against unsupervised methods. The utility of supervised clustering, as distinct from segmentation, is not explained. Furthermore, the possibility of using weak labels is put forth but not explained and no ideas on how this may be done are proposed. The experimental setup is fairly solid. However the thesis of this work should be better motivated and explained. The range of experiments may also need to be expanded to support this work's thesis.\n\nEXPERIMENTS\nThe use of supervised convolutional neural networks, as opposed to unsupervised networks, should be motivated. It is already clear that autoencoders (and of course convolutional autoencoders) learn semantic clusters at the bottleneck. Also, deep belief networks learn a similar partitioning of the data in the latent domain. These methods are unsupervised and are trained on data -- they should be compared against the proposed method, for clustering. It should also be made clear why clustering with a supervised objective in the fully convolutional network (FCN) is preferable to using unsupervised autoencoders or deep belief networks. It would also be helpful to test soft k-means, alongside the already tested k-means algorithm (this should perform similarly to an autoencoder with a single hidden layer).\n\nThe improved performance of the FCN over the tested unsupervised clustering methods is put down to the compositional structure of the FCN and the high field of view attained from depth. This should be tested by varying the depth of the network while keeping the same number of parameters. Especially since better performance could also be attributed to this method being the only one which is trained on the dat -- and in a supervised manner, at that.\n\nThe experiment that evaluates sensitivity to noise could optionally be made more interesting by including a class that models unintersting points (such as noise). The network ould then be trained with a low (but non-zero) amount of noise and then tested on varying amounts of noise, while its ability to classify the noise as noise would be evaluated. Also, while an accuracy-like measure (rand index) is explored while varying noise, there is no evaluation of a precision-like measure -- this would be useful.\n\nTEXT AND STYLE\nThe text is fairly well written and could use some polishing to remove typos and a some awkward phrasing. However the introduction needs to be improved to better motivate the work. The conclusion should also be improved as it does not make the significance of the work or the results clear.\n\nIn tables 1 and 2, the metric used should be noted in their descriptions. In the text, the statistical significance of the results should also be noted.\n\nIt would be helpful to include an equation in the paper for computing the rand index since it is explained but only with words. The terminology should also be consistent, referring always to rand index rather than to the rand index, accuracy, or pairwise accuracy, interchangeably.\n\nThe in-text citation format should be fixed as currently, author names are placed in sentences with no linking words or separating punctuation.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Clustering vs classification: learning with labels",
            "rating": "3: Clear rejection",
            "review": "The aim of this work is to combine deep learning for feature representation with the task of human-like unsupervised grouping. The intuition of brining feature learning into the clustering process is absolutely correct. However, the authors seem to be unaware of the subfield of deep learning, \"Deep metric learning\", where this exact task is addressed and optimized for.\n\nFurthermore, using 2d point sets is not acceptable in 2017. It was acceptable in NIPS 2003, but not anymore. These experimental results are lacking. ",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}