{
    "Decision": "",
    "Reviews": [
        {
            "title": "ICLR 2018 official review (Reviewer 3)",
            "rating": "3: Clear rejection",
            "review": "This paper gives an empirical estimation of the intrinsic dimensionality of the convolutional neural network VGG19 due to Simonyan and Zisserman.  To estimate the ID, the authors apply singular value decomposition to the matrix of activation vectors at each of the layers of the network, in which the intrinsic dimension is determined (in a more or less standard way) by the rank at which two consecutive singular values has a ratio exceeding some threshold. For the convolutional layers of VGG19, they observe that the sum of IDs for each feature map is roughly equal to the ID of the matrix formed by concatenating the vectors over all feature maps. They also observe that the ID drops with each successive layer.\n\nThe authors' findings are intuitively obvious, and certainly not surprising. Although a thorough and careful empirical investigation of this phenomenon would be a welcome addition to the research literature, this paper does not yet reach this standard. First and foremost, a result for a single neural network does not constitute enough evidence to justify the authors' conclusions. Second, the latter half of the paper is concerned with details of the experimental results, without offering any insights as to the implications for deep learning. Third, the paper is not well presented and organized: the introduction is scant; the notational formulism is not at all clear, rigorous, or consistent; the paper overall lacks polish, with many grammatical errors. \n\nOverall, I feel that while this line of research is worthwhile, at this stage the work is not yet ready for publication.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Unclear motivation, insufficient theory and small evaluation",
            "rating": "3: Clear rejection",
            "review": "This paper analyzed the dimensionality of feature maps and fully connected layers of pre-trained CNN on images within a same category. The local dimensionality of this paper is the SVD dimensionality on the augmented images of the same class images which classified by the neural network as high probability. However, the motivation of the analysis of this paper is unclear. I could not understand how such analysis contributes advance of representation learning. \n\nFurther, the analysis of this paper is not convincing.  \n-      I cannot believe the sum of SVD dimensionality of each feature maps becomes equals to the dimensionality to concatenated feature maps. As shown in in Fig.8, the estimated dimensions and original dimensions are very different. Although by looking some features maps, this rule might be hold as shown in Fig.5. However, the analysis is done on small examples without any theoretical analysis. \n\n-\tThe authors experimented one trained CNN and tested on images on only three categories (Persian Cat, Container Ship, and Volcao). It is not clear if the same rules holds to other CNNs, and images of other categories. \n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper use SVD to determine the intrinsic structure of deep neural network. But the  novelty of this work is not enough.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper try to analyze the intrinsic structure of VGG19 and give a new insight of deep neural networks. The authors propose to use SVD tools to estimate the dimension of the deep manifolds, and conduct experiments on three categories of ImageNet. The papers are written well and easy to follow. The analysis of manifold structure of DNN is important direction, but I am afraid novelty and insight of this work is not enough for acceptance.\npros:\n 1. The paper is well written and easy to follow.\n 2. Manifold analysis of the intrinsic structure of DNN is a important direction for further study.\ncons:\n 1. SVD is a standard tool for subspace and manifold analysis for decades of years. I do not think using it in DNN is a big contribution.\n2. The authors should explain why choosing VGG19 for analysis. Do other deep neural networks, such as Resnet, Googlenet, can  have the same phenomenon?\n3. Why the authors choose Persian Cat, Container Ship, and Volcano in the experiments? Do other categories have the similar results?\n4. The authors can indicate the application scenario of this work. For example, this work may guide to design better CNN structure for higher accuracy and lower computation cost. It may help the readers better understand the values of this work.\n    ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}