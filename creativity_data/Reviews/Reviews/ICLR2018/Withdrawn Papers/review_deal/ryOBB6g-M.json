{
    "Decision": "",
    "Reviews": [
        {
            "title": "Not the first spatial latent variable VAE",
            "rating": "3: Clear rejection",
            "review": "The paper proposes to use spatial latent representation (similar to convolutional network hidden layers) for Variational Autoencoders  (Kingma and Welling, 2014; Rezende et al., 2014) and a Kronecker factorization of the posterior variance and mean vectors over spatial coordinates to reduce computation. Their proposed method achieve better qualitative results than non-spatial latent representation and, for the spatial latent representation, the factorization allows better performance than fully parametrized.\nThe claim that previous work did not use spatial latent representation is demonstrably false, as it was used for example in Kingma et al. (2016), where a convolutional Inverse Autoregressive Flow is used. This paper seems to ignore that fact and do not compare with Inverse Autoregressive Flow.\nThe experimental procedure requires some improvements:\n- the baselines are just  a Variational Autoencoder model with diagonal gaussian approximate posterior with different parametrization;\n- the evaluation is qualitatively only visual on the samples and quantitatively only using training computation time (on CelebA) and Parzen window estimator of the log-likelihood (for MNIST), which is an unnecessarily bad estimator given that the variational lower bound (or Evidence Lower BOund, ELBO) and importance sampling estimator (Rezende et al., 2014; Burda et al., 2017) can be used. \nNo quantitative results is shown for CIFAR-10. \nWhile this work might convince that the proposed factorization decrease the computational cost, which factorization method often do, over a basic baseline, it does not demonstrate its advantage over more up-to-date published methods.\n\n\nDiederik P. Kingma, Tim Salimans, Rafal Józefowicz, Xi Chen, Ilya Sutskever, Max Welling: Improving Variational Autoencoders with Inverse Autoregressive Flow. NIPS 2016\nDanilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra: Stochastic Back-propagation and Variational Inference in Deep Latent Gaussian Models. ICML 2014\nYuri Burda, Roger B. Grosse, Ruslan Salakhutdinov: Importance Weighted Autoencoders. ICLR 2017",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A potentially interesting idea with poor experimental validation. ",
            "rating": "5: Marginally below acceptance threshold",
            "review": "\n- Summary\n\nThe paper proposes a VAE implementation that allows the latent space to carry useful spatial information. Specifically, it proposes using a convolutional encoder that outputs a spatial feature map for the VAE encoder where the feature map is used to infer the means and variances of the latent representation. The sampled latent code is then passed to a deconvolutional decoder for reconstructing the input image. In order to properly sampling from the spatial feature maps in a way that respects the interaction between the neurons at different locations, the paper proposes modeling the latent space using a matrix variate normal distribution. Furthermore, it proposed a low-rank modeling for the pixel mean. In the experiment section, improved log-likelihood scores on the MNIST dataset are shown across different variants of the proposed method as well as the VAE baseline. Visualization comparison with different variants of the proposed method as well as the VAE baseline was reported using the CIFA10 and CelebA dataset.\n\n- Missing comparison to closely related works\n\nAs also reviewed in the paper, PixelVAEs are closely related. However, the paper fails to include a comparison with the PixelVAE work. In addition, several papers improved upon the VAE work including VAE with IAF (NIPS'16). To further establish the benefit of the proposed method, proper comparison with the prior works should be included.\n\nAlso, VAE methods are often benchmarked using the CIFAR10 datasets using the log-likelihood score. The paper should also report performances using the same setting. This way the reader can better understand the performance of the proposed method.\n\n- Poor justification\n\nIt is not clear from the paper why using the matrix variate normal distribution or the low rank approximation is a better approach, other than reducing the number of parameters in the model. If the goal is reduce number of parameters, isn't it can be achieved by reducing the feature dimension?",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review of Spatial Variational Auto-Encoding via Matrix-Variate Normal Distributions",
            "rating": "4: Ok but not good enough - rejection",
            "review": "\nThe paper proposes to encode spatial information directly into the correlation matrix between the latent variables of a VAE using the Matrix Variate Normal distribution(MVN). I like the idea of modelling the correlation directly between the latent variables in a VAE trained on data with spatial information. However overall I don’t think the experimental section is adequate to convince that the proposed models work better than a fair baseline VAE. Especially I’m missing some quantitative results for comparing the models as I find the qualitative sample comparisons hard to judge and generalize from. Below is some more detailed comments/questions\n\nQ1: The experimental section only presents a single quantitative experiments for model comparisons using Parzen window estimates of the log-likelihood. As the models are optimized with variational inference it would be common to compare the models using the ELBO. Why is the Parzen estimate chosen instead of the ELBO (i except the KL of MVN is even analytically tractable)? This is especially in relation to the well known deficiencies of the parzen window metric (see eg. Theis et al 2015, that is also referenced in the paper)?\n\nQ2: I’m a bit confused about the difference between the a normal VAE and the ‘naive’ spatial VAE. Since the models contain a fully connected layer before and after the latent variables z the reshape operation C -> d x d x N seems to have no effect? \n\nQ3:  Related to the above. Is it correct that the only difference between these models are the number of latent variables (e.g. for the CelebA experiments 81 and 3*3*64 for the VAE and naive spatial VAE). ? If this is correct i feel that the normal VAE is put at a disadvantage due to the fewer numbers of latent variables.\n\nQ4: I think the experimental section would benefit from more baseline models. Especially i would have like to see a) a fully convolutional VAE model where the spatial information is indirectly encoded through convolutional filter and b) a VAE with correlation between the latent variables, like normalizing flows, Inverse autoregressive flows or Auxiliary variables. Can the authors comment on the relative merits of the baseline models and why they were chosen?\n\nAlbeit I like the idea of directly encoding spatial dependencies in the covariance matrix i don't think the experimental section currently warrents an publication at ICLR.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}