{
    "Decision": "",
    "Reviews": [
        {
            "title": "The novelty of this paper is not enough",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper introduces the coreset sampling idea for active learning of neural networks. The idea of coreset comes from geometric algorithms, and heuristically produce representative samples. The paper focuses on the long-tail setting, where we start with a set of random examples, and apply active learning on top of it. Experiments show that the proposed algorithm is favorable compared to uncertainty sampling and random sampling.\n\nClarity: The paper is mostly well-written. Some comments:\n1. I think the connection between Section 2 and Section 3 is a bit weak - Algorithm 1 collects examples such that each class share the same portion. However, if the labels are hidden (as in Algorithm 2), then this property is not perserved.\n\n2. In section 4.4, the problem setup is favorable to the FF algorithm, hence I think it is inappropriate to present. The reason is, FF automatically ignores repeating unlabeled examples, while other algorithms seem not. \n\nQuality: I think the comparison done in this paper is not extensive enough. Although the focus of this paper is on the long tail setting, why the setting is interesting is questionable. Is it because active learning algorithm, if start from scratch, will fall into local optima? It would be great if the paper can provide comparisons between this model and the original active learning model.\n\nOriginality: This is the part I am most concerned about. The paper (Baram et al. 2004) has already proposed the furthest-first traversal heuristic for active learning. I think this makes Section 2 less valuable. \n\nThe example in Section 5 is useful for illustration purposes, although I think the idea has already appeared in (Baram et al. 2004).\n\nSignificance: Although I think using representative sampling is a good idea, I think the contribution of this paper is limited for publication.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting idea but experiments are weak.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper presents an active learning method for deep models. The method queries consecutive points using farthest-first traversals in the space of neural activation over a representation layer.\n\nEmploying core-set for active selection is interesting and reasonable. The idea is clearly presented.\n\nThe method assumes that a deep model has already been initially trained with reasonable accuracy. However, such a reasonable model usually requires a large set of training examples, which is less practical in the case of active learning.\n\nAuthors performed experiments on MNIST, CIFAR10 and CIFAR100. The results are quite mixed. I can hardly observe significant superiority of the proposed method from the figures. Given that the baseline (uncertainty sampling) is not a strong one, I have doubt on the effectiveness of the proposed method.\n\nAuthors are suggested to perform more experiments on various datasets and compare to more state-of-the-art active learning methods.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Deep active learning",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper proposes a novel active learning method for deep neural networks. A seed subset is randomly selected from the whole set at the beginning, and then additional batches of training data are consecutively selected by using the farthest-first algorithm. The distances between data points are measured using the outputs in the top representation layer of neural networks. Experiments on MNIST, CIFAR-10 and CIFAR-100 show improvements of the proposed method over softmax response and random method.\n\nThe research of efficient active learning algorithms for deep learning models is important and valuable. However, this paper is in a draft version. The algorithm is simple but the experiments are not convincing. The competing algorithms are too limited. The experiments does not run long enough to see the performance of all competing algorithm till they obtain similar performance.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}