{
    "Decision": "",
    "Reviews": [
        {
            "title": "Why model-free in car setting? Obvious popular baselines missing.",
            "rating": "3: Clear rejection",
            "review": "\nSummary:\n\nAuthors propose a method which uses a Q-learning-based high-level policy which is combined with a contextual mask derived from safety-contraints and low-level controllers which disable certain actions from being selectable at certain states.\n\nThe high-level policy is learnt via fairly standard Q-learning (epsilon-greedy exploration policy and a NN function approximator.)\n\nExperiments in a simple car simulator on a task which requires the car to take a certain exit while navigating through traffic are presented with two baselines: 1. a greedy policy which navigates to the right-most lane asap and then follows traffic till the exit is reached. 2. human subjects driving cars in the simulator.\n\n\nComments:\n\n- Why use a model-free technique like Q-learning especially when one knows the model of the car in autonomous driving setting and can simply run model-predictive control (MPC) (convolve forward the model to get candidate trajectories of certain reasonable horizon, evaluate and pick the best trajectory, execute selected trajectory for a few time-steps and then rinse-and-repeat. This is a very well-accepted method actually used in real-world autonomous cars. See the Urmson et al. 2008 paper in the bibliography.) At the very least this technique should be a baseline. This method is not learning-based, doesn't need training data in a simulator, generalizes to **any** exit and lane configuration and variants of this basic technique continue to be used on real-world autonomous cars.\n\n- What kind of safety constraints cannot be expressed by masking actions? It seems that most safety constraints can be expressed via masking. But certain kinds of safety constraints like 'do not drive in the blindspot of other vehicles' sometimes require the ego car to speed up for a bit beyond the speed limit to pass the blindspot area and then slow down. This is an example of a constraint which cannot be expressed by masking actions and in fact requires breaking the top speed limit for a bit in order to be safer in the longer term. \n\n- This work also assumes that other cars in the vicinity can be simply observed without any perception uncertainty or even through occlusions. Figure 1c is pretty unrealistic to obtain for a real vehicle, especially for the four cars near the top where the topmost vehicles would be occluded at least partially from the vantage point of the ego-car. ",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Limited Q-learning for a car simulator",
            "rating": "3: Clear rejection",
            "review": "This paper considers the problem of autonomous lane changing for self-driving cars in multi-lane multi-agent slot car setting. The authors propose a new learning strategy called Q-masking which couples well a defined low level controller with a high level tactical decision making policy.\n\nThe authors rightly say that one of the skills an autonomous car must have is the ability to change lanes, however this task is not one of the most difficult for autonomous vehicles to achieve and this ability has already been implemented in real vehicles.  Real vehicles also decouple wayfinding with local vehicle control, similar to the strategy employed here.  To make a stronger case for this research being relevant to the real autonomous driving problem, the authors would need to compare their algorithm to a real algorithm and prove that it is more “data efficient.”  This is a difficult comparison since the sensing strategies employed by real vehicles – LIDAR, computer vision, recorded, labeled real maps are vastly different from the slot car model proposed by the authors.  In term of impact, this is a theoretical paper looking at optimizing a sandbox problem where the results may be one day applicable to the real autonomous driving case.\nIn this paper the authors investigate “the use and place” of deep reinforcement learning in solving the autonomous lane change problem they propose a framework that uses Q-learning to learn “high level tactical decisions” and introduce “Q-masking” a way of limiting the problem that the agent has to learn to force it to learn in a subspace of the Q-values.\nThe authors claim that “By relying on a controller for low-level decisions we are also able to completely eliminate collisions during training or testing, which makes it a possibility to perform training directly on real systems.”  I am not sure what is meant by this since in this paper the authors never test their algorithm on real systems and in real systems it is not possible to completely eliminate collisions.  If it were, this would be a much sought breakthrough. Additionally for their experiment authors use the SUMO top view driving simulator.  This choice makes their algorithm not currently relevant to most autonomous vehicles that use ego-centric sensing.  \nThis paper presents a learning algorithm that can “outperform a greedy baseline in terms of efficiency” and “humans driving the simulator in terms of safety and success” within their top view driving game.  The game can be programmed to have an “n” lane highway, where n could reasonable go up to five to represent larger highways.  The authors limit the problem by specifying that all simulated cars must operate between a preset minimum and maximum and follow a target (random) speed within these limits.  Cars follow a fixed model of behavior, do not collide with each other and cannot switch lanes.  It is unclear if the simulator extends beyond a single straight section of highway, as shown in Figure 1.  The agent is tasked with driving the ego-car down the n-lane highway and stopping at “the exit” in the right hand lane D km from the start position. \nThe authors use deep Q learning from Mnih et al 2015 to learn their optimal policy.  They use a sparse reward function of +10 for reaching the goal and -10x(lane difference from desired lane) as a penalty for failure.  This simple reward function is possible because the authors do not require the ego car to obey speed limits or avoid collisions.    \n\nThe authors limit what the car is able to do – for example it is not allowed to take actions that would get it off the highway.  This makes the high level learning strategy more efficient because it does not have to explore these possibilities (Q-masking).  The authors claim that this limitation of the simulation is made valid by the ability of the low level controller to incorporate prior knowledge and perfectly limit these actions.  In the real world, however, it is unlikely that any low level controller would be able to do this perfectly.\nIn terms of evaluation, the authors do not compare their result against any other method.  Instead, using only one set of test parameters, the authors compare their algorithm to a “greedy baseline” policy that is specified a “always try to change lanes to the right until the lane is correct” then it tries to go as fast as possible while obeying the speed limit and not colliding with any car in front.   It seems that baseline is additionally constrained vs the ego car due to the speed limit and the collision avoidance criteria and is not a fair comparison.  So given a fixed policy and these constraints it is not surprising that it underperforms the Q-masked Q-learning algorithm.  \nWith respect to the comparison vs. human operators of the car simulation, the human operators were not experts.  They were only given “a few trials” to learn how to operate the controls before the test.  It was reported that the human participants “did not feel comfortable” with the low level controller on, possibly indicating that the user experience of controlling the car was less than ideal.  With the low level controller off, collisions became possible.  It is possibly not a fair claim to say that human drivers were “less safe” but rather that it was difficult to play the game or control the car with the safety module on.  This could be seen as a game design issue.  It was not clear from this presentation how the human participants were rewarded for their performance.  In more typical HCI experiments the gender distribution and ages ranges of participants are specified as well as how participants were recruited and how the game was motivated, including compensation (reward) are specified.  \nOverall, this paper presents an overly simplified game simulation with a weak experimental result.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Preliminary Work with Limited Contributions and Weak Evaluation",
            "rating": "3: Clear rejection",
            "review": "The paper describes a deep Q-learning approach to the problem of lane changing, whereby the action space is abstracted to high-level maneuvers that are then associated with low-level controllers. The paper proposes a \"Q-masking\" strategy that reduces the action space according to constraints or prior knowledge. The method is trained and evaluated in a multi-lane simulator and compared against a baseline approach and human drivers.\n\nPlanning lane-change maneuvers is an interesting, important problem for self-driving vehicles. What makes this problem particularly challenging is the need to predict/respond to the actions of other drivers. However, these issues are ignored here,  and it is is unclear why existing optimization/planning approaches are poorly suited to this problem, which is a fundamental assumption being made here. Indeed, there is a long history of motion planning research that specifically addresses the problem of planning in the face of dynamic obstacles, as well as work that plans using predictive models of vehicle behavior (e.g., see the work by Jon How's group). However, the related work discussion is significantly lacking. The paper does an insufficient job describing why deep RL is the right way to formulate this problem. There are vague references to the policy being difficult to define, but that motivates the importance of learning in general, not deep RL. Why is it reasonable given: (i) the challenge in defining appropriate rewards (i.e., it's not clear to me what would constitute the right reward for this problem); (ii) the large amount of data required to learn the policy; and (iii) the significant risks associated with training with a physical vehicle; One can see the merits in employing a hierarchical action space, whereby decision making operates over high-level actions, each associated with low-level controllers, but that the adopted formulation is not fundamental to this abstraction. Indeed, this largely regulates the hard problems (i.e., controlling the low-level actions of the vehicle while avoiding collisions) to a separate controller. Further, Q-masking largely amounts to simply removing actions that are infeasible (e.g., changing lanes to the left when in the left-most lane), but is seems to be no more than a heuristic, the advantages of which are not evaluated.\n\nThe method is evaluated in simulation with comparisons to a simple baseline that tries to get over to the right lane as well as human performance. In the runs that reach the goal, the proposed method is about 20% faster than the simple baseline, though it does not reach the goal every time. Given the claim that not reaching the goal is considered a failure, it isn't clear which performance is preferred. Meanwhile, the evaluation could be improved with the use of a better baseline (e.g., using an existing planning framework such as a predictive RRT that plans to the goal).\n\nAdditional comments/questions:\n\n* The description of the Q-learning implementation is unclear. How is the terminal time known a priori? Why are two buffers necessary?\n\n* The paper claims that the method permits training without any collisions, even for real training runs (strong claim), however it isn't clear how this is guaranteed beyond the assumption that you have a low-level controller that can ensure collisions are avoided. This is secondary to the proposed framework.\n\n* The paper overstates the contributions of Q-masking, emphasizing improvements to data efficiency among others. The authors should validate these claims with an ablation study that compares performance with and without masking. This would help to address the contribution of Q-masking vs. simply abstracting the action space.\n\n* The network takes as input a 2.5m (this is large) occupancy grid representation of the local environment. How sensitive is the network to errors in this model? Does the occupancy grid account for sensing limitations (e.g., occlusions)?\n\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}