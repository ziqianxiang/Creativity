{
    "Decision": "",
    "Reviews": [
        {
            "title": "limited novelty",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Summary:\nThe paper presents analytical results comparing various initialization and training methods of transferring knowledge from VGG network to a smaller student network by replacing blocks of layers with single layers. \n\nReview:\nThe paper is well written and explains the experiments well, however the results are somehow obvious. The method also lacks novelty and looks like a generalized version of fitnets.\nThe insights into initialization schemes are valuable, however I don't think this is enough for conference track.\n\nComments:\nIntro mentions that shortcut connections overcome overfitting whereas they are important for preserving signal flow and enabling training of deeper architectures. \nPage4: fine-tine->fine-tune\nDetails of the experimental setup are missing e.g. optimizer.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper presented 5 methods for doing 'triaging' or block layer compression for deep networks. However it is not super clear the central thesis of this paper.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper presented 5 methods for doing 'triaging' or block layer compression for deep networks. However it is not super clear the central thesis of this paper.\n\nThis paper proposes 5 ways to perform local block layer compression. Methods include using random weights or mean of the original weights. Global fine-tuning and student-teacher type distilling algorithms were also employed for for some of the algorithms. \n\nThe results show that globally fine-tuned compression works better than non globally fine tuned algorithms and sometimes the compressed network does better than the original uncompressed networks. \n\nWhile demonstrating that compressed networks performs better than the parent is very exciting, it is unclear if those results are statistically significant. The results on Cifar10 is not near to the current state-of-the art. Does the author believe compression of certain layers would always improve performance for all datasets including potentially ImageNet?\n\nIn addition, how do we assess the criticality of the layers? Is it simply by the empirical plots of Figure 2?\n\nOverall this is an ok paper, but it is a bit unclear of the central thesis or take-away of the paper. \n\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple, clean paper proposing network compression. Lack of comparison to literature, and novelty/significance is in doubt.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper proposes a method to compress a block of layers in a NN (particularly CNNs) evaluates several different sub-approaches. It is easy to follow the paper. Given experiments are nicely setup with intuitive explanations. My main concern about the paper is novelty/importance and lack of experimental comparison to other techniques attacking similar complexity problems.\n\nAuthors provide a good intro for the problem and existing common approaches addressing network complexity. They locate themselves well in the literature. They provide a simple compact explanation of what their methods are in section 2.3. I find the coding/notation of the sub-methods a little encrypted though. Simple English or better encoding could be used for better readability. Experimental results are explained pretty well. \n\nI think the given results/messages are valuable to the community. However the generalizability to different models/network architectures is in question as the paper is mostly based on VGG16. Also the methods/significance seem to be more like an experimental paper and not significant enough for ICLR. I am borderline about this. \n\nComparison between the proposed methods is nicely done but comparison to other literature pruning/compressing models is not experimentally shown. Ideally we expect a reader designing a new architecture to have an idea for which way to go for model compression after reading this paper—results are good to decide between CLRW –STN, but not informative on how they compare to literature. I think this is a significant missing point. ",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}