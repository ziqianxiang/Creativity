{
    "Decision": "",
    "Reviews": [
        {
            "title": "Implicit weight normalization",
            "rating": "3: Clear rejection",
            "review": "The paper consider a method for \"weight normalization\" of layers of a neural network.  The weight matrix is maintained normalized, which helps accuracy.  However, the simplest way to normalize on a fully connected layer is quadratic (adding squares of weights and taking square root).\n\n The paper proposes \"FastNorm\", which is a way to implicitly maintain the normalized weight matrix using much less computation.  Essentially, a normalization vector is maintained an updated separately.\n\n  Pros:   Natural method to do weight normalization efficeintly\n\n  Cons:   A very natural and simple solution that is fairly obvious.\n\n          Limited experiments \n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper proposes a computationally fast method to train neural networks with normalized weights.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper proposes a computationally fast method to train neural networks with normalized weights. Experiments demonstrate that their method is promising compared to the competitor “NormProp” which explicitly normalizes the weights of neural networks.\n\nPros: \n(1) The paper is easy to follow.\n\n(2) Authors use figures that are easy to understand to explain their core idea, i.e., maintaining a vector which estimates the row norm of weight matrix and implicitly normalizing weights.\n\nCons:\n(1) If we count the matrix multiplication operation in fc layer along with normalization (in common cases normalization should follow a weighted layer), the whole computation complexity becomes O(mn) rather than O(n+m), so I doubt how fast it could be in the common case.\n\n(2) Authors did a MNIST experiment with a 2-fc layer neural network for comparing their FastNorm to NormProp. It is a bit strange that they do not show the difference of speed, but show that FastNorm can outperform NormProp in terms of classification accuracy with a higher learning rate. Since the efficiency is one of the main contributions, I suggest authors add this comparison.\n\n(3) The proposed FastNorm improves the stability by observing the standard deviation of validation accuracies in training phase. The authors attribute this to the reduction of accumulated rounding error in training process, which is somewhat against the community’s consensus, i.e., float precision is not that important so we can use float32 or even float16 to train/do inference for neural networks. I’m curious if this phenomenon still holds if authors use float64 in the experiments.\n\nSome typos:\nFirst line in page 3: “brining” should be “bringing”\n\nOverall, I think the current version of the paper is not ready for ICLR conference. Authors need more experiments to show their approach’s effectiveness. For example, batching and convolution as mentioned by authors would be more significant. \n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This authors proposed to use an implicit weight normalization approach to replace the explicit weight normalization used in training of neural networks. The contribution is limited.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This authors proposed to use an implicit weight normalization approach to replace the explicit weight normalization used in training of neural networks. The authors claimed to obtain efficiency improvement and better numerical stability.\n\nThis is a short paper that contains five pages. The idea of the proposed implicit weight normalization is to apply the normalization to scaling the input rather than the rows of the matrices. In terms of the overall time complexity, the improvement seems quite limited considering that the normalization is not the bottleneck operations in the training. In addition, it is not very clear how the proposed approach benefits the mini-batch training of the network. In terms of numerical stability, though experimental results were reported, there is no theoretical analysis. The experiments are quite limited.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}