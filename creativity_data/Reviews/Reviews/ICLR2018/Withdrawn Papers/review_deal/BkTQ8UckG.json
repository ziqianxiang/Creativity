{
    "Decision": "",
    "Reviews": [
        {
            "title": "Using hard negatives in triplet loss improves embedding. Not enough novelty for ICLR.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper describes learning joint embedding of sentences and images. The main point is in using a triplet loss that is applied to hardest-negatives, instead of averaging over all triplets. This led to improvement over SOTA in a task of caption ranking on MS-COCO, nd good performance on flickr30K. \n\nThe main issue with this paper is novelty. Using hard negatives is routinely  used in many embedding tasks, and has been discussed in many publications. For instance recently Wu et all in ICCV2017, bu also many other papers. \nWhen used in practice wit real-world datasets, taking the max (hardest negative) tends to be very sensitive to label noise, since the hardest negative is sometime just a positive sample with incorrect label. In these cases focusing on the hardest negative reduces performance. \n\nWhile it is good to know that using hard negatives improves recall measures on coco, it is not clear that this paper provides enough novel insight to be interesting enough for the ICLR audience. It may be a better fit in a conference that stresses empirical performance, like in machine vision conferences. \n ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}