{
    "Decision": "",
    "Reviews": [
        {
            "title": "The paper is about incorporating information from different modalities, i.e. text, images, and numerical values, into link prediction approaches like DistMult. To this end, the DistMult model is extended with models for the different modalities. Additionally, 2 datasets are extended with multi-modal information for learning and evaluating the multi-modal extensions.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper is well-written but is lacking detailed information in some areas (see list of questions). The approach of incorporating all the different facts around an entity is worthwhile but pretty straight-forward. The evaluation part of this paper is hard to assess due to the unavailability of the 2 datasets and appropriate baselines. Therefore, I am currently leaning towards rejecting this paper.\n\n? p.3: What parts are pre-trained? Is e_o fixed for the non-structured knowledge? Or is it joint learning and you learn all LSTMs and CNNs yourself? (Besides the reuse of VGG, I could not find this information explicitly stated within the paper.) \n? p.4: The word embeddings for the CNN are pre-trained word2vec/Glove/xyz embeddings? How do you deal with words (or even the whole string) for which you have no word embedding? \n? p.6: Do you have one model for all the relations or does every relation has its own LSTM, CNN, feed-forward network? I.e. 1 or 3 feed-forward networks for age, zip code, and release dates?\n? p.6: How does “Ratings Only” work as DistMult gets no information of the specific entities? Is it just choosing the most common class?\n? p.7: What does “find the mid-point of the bin” mean and should it not be 1018 instead of 1000 bins?\n\n+ Insights on how different modalities affect the prediction results.\n+ The approach is capable of theoretically handling all linked information to an entity as additional information to the link structure.\n- As the evaluation data is not available, it is really hard to assess the quality of the models. No simple baseline like the Unstructured [1] + simple concatenation of an image vector is provided.\n- Training of CNNs, LSTMs and so on is not clear. (See question regarding whether the models are pre-trained or whether the models are also directly learned from the data.)\n\nFurther comments:\n* In Figure 1, the feed-forward network looks like an encoder-decoder network and it does not show the projection from r to R^d which is mentioned in the text.\n* The found hyperparameter of the grid-search would also be interesting to know.\n\n[1] Bordes, A.; Glorot, X.; Weston, J.; and Bengio, Y. 2012. A semantic matching energy function for learning with multi- relational data. Machine Learning 1–27. \n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper introduces a new approach to learn embeddings of relational data using multimodal information such as images and text. For this purpose, the method learns joint embeddings of symbolic data, images and text to predict the links in a knowledge graph. The multimodal embeddings are evaluated on newly created datasets, which extend the MovieLens-100k and YAGO-10 with multimodal information.\n\nThe paper is written well, good to understand, and technically sound. I especially liked the general idea of using multiple modalities to improve embeddings of relational data. This direction is not only interesting because of the improvements it brings for link prediction tasks, but also because it is a promising direction towards constructing commonsense knowledge knowledge graphs via grounded embeddings. The technical novelty of the paper is somewhat limited, as the proposed method consists of a mostly straightforward combination of existing methods.\n\nWith regard to related work: Recently, [1] proposed similar multimodal embeddings and showed that they improve embedding quality for semantic similarity tasks and entity-type prediction tasks. This reference should be included in the related work. The authors mention also in the last sentence of Section 3 that previous approaches cannot handle missing data or uncertainty. This claim needs to be discussed clearer as it is not clear to me why this would be the case.\n\nWith regard to the evaluation: Overall, I found the evaluation to be good, especially with regard to the different ablations. However, it would be nice to see results for more sophisticated models than DistMult (which, due to its symmetry, shouldn't be used on directed graphs anyway) as the improvements that can be gained might be less for these models. It would also be interesting to see how predictions using only the non-symbolic modalities would do (e.g. in Table 3). Furthermore, Section 5.3 would clearly benefit from a better analysis and discussion, as it isn't very informative in its current form and the analysis is quite hand-wavy (e.g. \"two of the predicted titles for Die Hard have something to do with dying and being buried\").\n\nFurther comments:\n- The proposed method to incorporate numerical data seems quite ad hoc. What are the motivations for this particular approach?\n- Are the image features fixed or learned? In the later case: how much do the results change with pretrained CNNs (e.g., on ImageNet)\n- p.3: We use an appropriate encoder is repeated twice.\n- Since the datasets are newly introduced, it would be good to provide a more detailed analysis of their characteristics.\n\n\n[1] Thoma et al: \"Towards Holistic Concept Representations: Embedding Relational Knowledge, Visual Attributes, and Distributional Word Semantics\", 2017.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper proposes to perform link prediction in Knowledge Bases by supplementing the original entities with multimodal information such as text description or images. A model, based on DistMult, able to encode all sort of information when scoring triples is presented with experiments on 2 new datasets based on Yago and MovieLens.\n\nThis paper reads well and the results appear sound. Unfortunately, the contribution seems rather small to be accepted for ICLR. This is a straight application and combination of existing pieces with not much originality and without being backed up by very strong experimental results.\n\n* Having only results on new datasets makes it hard to compare the objective quality of the DistMult baselines and hence of the improvements due to the multimodal info. Isn't there any existing benchmark where this could have an impact?\n* The much better performance of ConvE is worrying there. It is suggested that the proposed approach could be incorporated in ConvE to lead to similar improvements than on DistMult. The paper would be much stronger with those. \n* Are we sure that the textual description do not explicitly contain the information of the triple to be predicted? This would explain the massive gains in Yago.\n* For Table 8, the similarities are not striking. What were the nearest neighboring posters in the original VGG space? They should not be that bad too.\n* The work on multimodal embeddings like \"Multimodal Distributional Semantics\" by Bruni et al. or \"Multi-and Cross-Modal Semantics Beyond Vision: Grounding in Auditory Perception.\" by Kiela et al. could be discussed/cited.\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}