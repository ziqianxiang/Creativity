{
    "Decision": "",
    "Reviews": [
        {
            "title": "The manuscript needs to be clarified more precisely.",
            "rating": "3: Clear rejection",
            "review": "This paper proposes a neural variational sparse topic model to model topics of short text via variational auto encoding and sparse coding. \n\nOverall, many parts of the paper need to be further clarified or justified. I tried to list some major and minor concerns/comments below:\n\n* 'Collapsed document code \\theta': In section 3.2, I'm not sure what 'collapse the document code' means, but from the context, I assume that it means removing prior of the word codes. This is still a valid choice, however, there are many pieces of evidence that removing prior of word codes may lead to a serious overfitting problems such as the case of pLSI. Maybe the structure of the variational distribution relaxes the overfitting problem, but even it's the case, this choice is not justified well in the text. Additionally, if we remove the prior over word codes, then the inference can be tractable in some way I guess, which means we may not need the variational approximation method.\n\n* The word codes need to be non-negative to obtain distributions in Table 2, however, it is not clear how this non-negativity can be obtained during the inference procedure. In the STC, the authors made a strong assumption of non-negativity on document and word codes in order to keep the interpretability of the model. As a consequence, they had to rely on some property of convex function during training, while I cannot find these detail in the current manuscript. In a similar sense, which projection method has been used to project topic dictionary after relu layer? Did you normalise the vector using the sum of elements?\n\n* Reparameterisation: From Equation (5) I found that the natural log can take a negative value if the epsilon is greater than 0.5, over which the log is undefined. In addition, why do you need sign function since the epsilon is always positive by definition?\n\n* Non-stochastic beta?: the word codes are defined as stochastic variables. Why not for the topic dictionary beta? Which variable needs to be stochastic or not?\n\n* The second term in RHS of Equation 3: 's' should be removed from the term if you meant the marginal probability of observations here.\n\n* Experiment details: How the parameters of models are chosen? Are the default parameters used throughout the experiments? \n\n* Please use a single character to denote a matrix. Word embedding matrix WE seems a multiplication of two matrices.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "good illustration of deep-neural network machinery but poor connection to related/prior work",
            "rating": "5: Marginally below acceptance threshold",
            "review": "\"without complicated mathematical inference\" (on end of page 10)\nHeaven forbid we should expect our graduate students to know a bit of statistics!  Sorry,  I am being sarcastic.  Yes, I agree its good to automate as much of the \"tedium\" as possible.  Perhaps word this differently.\n\nTable 5 should probably be in appendix.  With comparison to other methods, this isn't\ntoo informative.\n\nPlease properly capitalise some words in the references. The good Reverend bayes I'm sure is turning in his grave.  I note \"bayes\", \"lda\" and \"ibp\" still occur in the revision!\n\nSection 3.4 is a nice trick.  \n\nFirst version experimental work\n==========================\nFor short text, you missed a whole body of work that uses word background knowledge, like word correlations or embeddings, to regularise topics or as an informed prior on topics.  A lot of progress on this recently, and it produces dramatic improvements using GloVe embeddings, for instance, for the prior or regulariser.  Given the lack of comparison with these, it makes your experimental work lacking.   There are also many more deep NN methods for topic models, e.g., DocNADE.\n\nRevised experimental work\n==================\nBig improvement.  Its now believably good at classification.  Still, I'd argue better existing\nalgorithms could be used, i.e.,  much better ones still exist (in my view)\nbut at least you have made an effort this time.  \n\nMathematical formulation of 3.1 is very sloppy\n==============================================\nA)       $\\theta$ looks to be scalar in line 1, but you earlier\n        said it was a vector; also should $\\theta$ be indexed by $d$?\nB)   \t from 2(b) I presume $s_n$ is a vector but appears\n\t  to be a scalar in 2(a)\nC)       what is a supergaussian, and what are the 2 parameters for it?\n         my only thought is that it is a scalar distribution e^{-|x|^c}\n\t for 1<c<2 and x>0, but how does $\\theta$ fit in?\nC)       if $s_n$ is scalar, the whole distribution is too trivial to be\n         a topic model\nD)      $\\beta$ is a topic dictionary, I presume a matrix, what then\n        is $\\beta_n$ ... I'm guessing a $K$ dim vector\nE)      is $n$ the $n$-th word in the document or the word with\n       dictionary index $n$ ... very different!\nSo I'll guess $\\beta$ is $N$ (vocab dim) times $K$, and the supergaussian\nis K dimensional, but only on the positive quadrant, so all entries of the\nvector $s_n$ are positive. Not sure about $\\theta$!\nThat is a lot of work for me to do just to read 3.1.\nAnd I'm still unsure.  Had to clear everything by reading\nthe STC paper.   Note STC was a UAI paper.   You have it listed as CoRR.\nThey define super-Gaussian, which is non-standard, so you need to\ndefine it in your paper.\n\nDetails of 3.2 also confusing\n=============================\nI gather when you say $s_n \\sim U(....)$ you really mean\n$s_{d,k} \\sim U(....)$, since $s_n$ has to be $K$ dimnesional.\nGenerally, a lot more care needed with definition\nof distributions.\n\nLine 4 you present how to collapse the doc code.\nI'm lost by this.  Doesn't that make $\\theta_d$ a scalar?\nShouldn't it be a vector?  Or did you mean $\\theta_k$?\nWhat is $s_{d,nk}$ mean?  What is $nk$?  Presume you meant\n$s_{d,k}$.  Regardless, I don't see how the super-Gaussian\nand the Poisson (2a) and 2b)) can be collapsed\nto give a simple summation estimate for $\\theta$.\nBasically, I don't believe Bai et als, formula\nis strictly correct, though its probably a\ngood summary statistic.\n\nHowever, since the $s_n$ are vectors for each word, right,\nyou have plenty of latent variable dimensions floating around,\nall the $s_n$, so you have a rich latent space.\nIts not easy to intepret, but seems OK.\n\n\"To yield sparse word codes, we choose the Laplace distribution as the variational distribution of word codes, \" ... sorry, I am confused.\nEither word codes have a uniform distribution or they have a Laplace\ndistribution, they cannot have both.  The variational encoder is just\nan inference algorithm ... it should change what the prior is.\n\nMathematical formulation of 3.1 is very sloppy\n==============================================\nA)       $\\theta$ looks to be scalar in line 1, but you earlier\n        said it was a vector; also should $\\theta$ be indexed by $d$?\nB)   \t from 2(b) I presume $s_n$ is a vector but appears\n\t  to be a scalar in 2(a)\nC)       what is a supergaussian, and what are the 2 parameters for it?\n         my only thought is that it is a scalar distribution e^{-|x|^c}\n\t for 1<c<2 and x>0, but how does $\\theta$ fit in?\nC)       if $s_n$ is scalar, the whole distribution is too trivial to be\n         a topic model\nD)      $\\beta$ is a topic dictionary, I presume a matrix, what then\n        is $\\beta_n$ ... I'm guessing a $K$ dim vector\nE)      is $n$ the $n$-th word in the document or the word with\n       dictionary index $n$ ... very different!\nSo I'll guess $\\beta$ is $N$ (vocab dim) times $K$, and the supergaussian\nis K dimensional, but only on the positive quadrant, so all entries of the\nvector $s_n$ are positive. Not sure about $\\theta$!\nThat is a lot of work for me to do just to read 3.1.\nAnd I'm still unsure.  Had to clear everything by reading\nthe STC paper.   Note STC was a UAI paper.   You have it listed as CoRR.\nThey define super-Gaussian, which is non-standard, so you need to\ndefine it in your paper.\n\nDetails of 3.2 also confusing\n=============================\nI gather when you say $s_n \\sim U(....)$ you really mean\n$s_{d,k} \\sim U(....)$, since $s_n$ has to be $K$ dimnesional.\nGenerally, a lot more care needed with definition\nof distributions.\n\nLine 4 you present how to collapse the doc code.\nI'm lost by this.  Doesn't that make $\\theta_d$ a scalar?\nShouldn't it be a vector?  Or did you mean $\\theta_k$?\nWhat is $s_{d,nk}$ mean?  What is $nk$?  Presume you meant\n$s_{d,k}$.  Regardless, I don't see how the super-Gaussian\nand the Poisson (2a) and 2b)) can be collapsed\nto give a simple summation estimate for $\\theta$.\nBasically, I don't believe Bai et als, formula\nis strictly correct, though its probably a\ngood summary statistic.\n\nHowever, since the $s_n$ are vectors for each word, right,\nyou have plenty of latent variable dimensions floating around,\nall the $s_n$, so you have a rich latent space.\nIts not easy to intepret, but seems OK.\n\n\"To yield sparse word codes, we choose the Laplace distribution as the variational distribution of word codes, \" ... sorry, I am confused.\nEither word codes have a uniform distribution or they have a Laplace\ndistribution, they cannot have both.  The variational encoder is just\nan inference algorithm ... it should change what the prior is.\n\nSummary\n==============\nThis is good looking research.  \nPros:\n* its a good illustration of deep-neural network machinery at work, and well put together\n* the experimental results show it works very well\n* good experimental work (different data sets, different algorithms)\nCons:\n* sloppy mathematical presentation\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice topic, but inadequate investigation",
            "rating": "3: Clear rejection",
            "review": "The paper presents a novel model called Neural Variational Sparse Topic Model (NVSTM), that captures word-level sparse representation by employing a Laplacian prior, and enriches semantics by using external word embeddings. The experimental results have shown that NVSTM yields good results in terms of document classification and sparse word representation against 3 related models. \n\nThere are several issues that the authors should address in the future.\n-\tWord co-occurrence information is very important to shape topics. The paper also mentioned about that.  However, in model perspective, it is surprised that NVSTM does not have any variables (or hyper-parameters) to model each document. It means that NVSTM seems not to capture the word co-occurrence information. \n-\tThe paper only uses the variational autoencoder (VAE) to do inference for NVSTM. The authors should detail the approximate posterior (q), variational parameters $\\Theta$, model parameters $\\Phi$, and objective function $L$ in NVSTM. It is confusing when the encoderâ€™s distribution q(s_n) and the prior p(s_n) of the latent variable $s_n$ are both Laplace(0,b_n)  with the same parameter. There might be some typos here. \n-\tThe assumptions of NVSTM are not different between short and normal text. Why does the paper target at short-text? \n-\tThe paper should survey more about short-text and word embedding as prior knowledge in topic models. The authors should discuss related work, and take some baselines which use word embedding as prior into comparison.\n-\tThe authors should do experiment on more datasets to have higher confidence. Many short and normal text datasets are freely available. \n-\tIn the experiment settings, what is the size of the hidden layer? Is it the number of topics $K$? \n-\tThe settings for the baselines were not described in the paper. To make a fair comparison, the settings for the baselines should be chosen thoughtfully.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}