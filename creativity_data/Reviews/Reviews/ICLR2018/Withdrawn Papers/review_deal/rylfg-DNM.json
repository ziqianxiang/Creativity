{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting but not novel.",
            "rating": "3: Clear rejection",
            "review": "This work proposes adding options to an agent's action space to accelerate exploration.  The options seem to be the product set of the original action space.  Experiments are run on cartpole and 4 Atari games and show promising results.\n\nAlthough the idea is appealing, it is unfortunately not novel.  Additionally, this paper fails to situate itself in the wider RL literature.  It would have been good for the authors to have looked a bit deeper into existing work as they would have been able to better situate themselves relative to existing approaches to option-based learning.  Although I don't have older citations off the top of my head, a quick google search turns up an identical yet more advanced approach from earlier this year: \"Multi-Level Discovery of Deep Options\".  I appreciate the effort put into this paper and hope the authors can extend their work to better integrate the options literature and move forward constructively relative to existing work in the field.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A bit of a let down...",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper examines the effects of RL in an augmented action space which includes sequences of actions (e.g. meta actions) as well as the primitive actions defined by the MDP. The authors extend a GPU-based A3C implementation to include meta actions and show that their algorithm can achieve better sample complexity / and higher performance in most cases.\nThe paper is well written, but fails to mention the relationship between meta-actions and the Options framework (Sutton et al 99). In particular, it seems that meta-actions can just be viewed as a set of predefined options given to the agent. Much prior work has studied how to combine options with Deep RL. To name a few: Multi-Level Discovery of Deep Options (Fox et al 17), Classifying Options for Deep RL (Arulkumaran 16), and Deep Exploration via Bootstrapped DQN (Osband et al). The former even learns the options rather than pre-defining them. This connection needs to be made explicit.\n\nI have concerns regarding the results in this paper:\n•\t Why on Qbert is the switching agent able to do so much better than both IU and DU? I suspect the curves may not be averaged over enough trials and results may be noisy, as it seems this shouldn’t be possible. Results curves should show the standard deviation or variance of the 3 runs.\n•\tI am concerned this approach will not scale to games that have more actions than the 4 games explored. The concern is that A4C exponentially increases the size of the action space as a function of k. Cartpole as well as the explored Atari games all have relatively small action spaces, so I think it is critical to show that it scales to games with larger spaces as well. My concern is that in larger actions spaces, the gains A4C gets from meta-actions will be outweighed by the difficulty of having to learn with so many different actions.\n•\tWhat is the value for k used in Atari experiments?\n\nOverall, I was very excited about this paper after reading the introduction. I really like the idea of allowing the network to decide on sequences of actions, and I think many games do have opportunity to identify and re-use combos of primitive actions (e.g. to stay between lanes in Beam Rider). However, I don’t think the architecture, algorithm, and results live up to this motivation. Simply augmenting the action space with all possible sequences of actions begs for a better solution.\n\nPros:\n•\tThe authors show that in certain domains, exploration can be aided by pre-defined meta actions.\n•\tThe authors introduce an algorithm to squeeze more gradient updates out of a meta-action (DU-A4C). This is related to the insight that meta actions can be thought of as sequences of primitive actions.\n\nCons:\n•\tRelationship to Options is not identified.\n•\tResults are only given for games with small action spaces. Unclear how the method scales to larger action spaces.\n•\tMethod for augmenting action space is not particularly interesting.\n•\tHeuristic switching is somewhat undesirable. It would be nice to understand why DU stops working well and how to improve it.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Missing comparison with prior work very limited results.",
            "rating": "2: Strong rejection",
            "review": "The paper propose to expand the action set by adding longer sequence of actions to the action set that the policy can choose from.\n\nThe update rules is very similar to n-step return that has widely studied in the literature but no mention or reference and no theoretical comparison as how this model differs. I understand that the choice of action are larger and softmax is being taken over larger action set and that’s main difference of this method. \n\nThe results are extremely preliminary on only few basic Atari games and there is no way any conclusion could be drawn from this limited results.\n\nParagraph 2 in Introduction is misinterpreting why Deep RL methods started working\n\nThe statement that the method gets more update per each state is a wrong as it using the future reward for the update, hence means future states have been visited.\n\nCover of related prior work is poor.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}