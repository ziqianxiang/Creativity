{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "As identified by most reviewers, this paper does a very thorough empirical evaluation of a relatively straightforward combination of known techniques for distributed RL. The work also builds on \"Distributed prioritized experience replay\", which could be noted more prominently in the introduction.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "the proposed method is simple",
            "rating": "5: Marginally below acceptance threshold",
            "review": "\nComment: The paper proposes a simple extension to DDPG that uses a distributional Bellman operator for critic updates, and introduces two simple modifications which are the use of N-step returns and parallelizing evaluations. The method is evaluated on a wide variety of many control and robotic talks. \n\nIn general, the paper is well written and organised. However I have some following major concerns regarding the quality of the paper:\n\n- The proposal, D4PG, is quite straightforward which is simply use the idea of distributional value function by Bellemare et al. (previously used in DQN). Two modifications are also simple and well-known techniques. It would be nicer if the description in Section 3 is less straightforward by giving more justifications and analysis why and how distributional updates are necessary in the context of policy search methods like DDPG. \n\n- A positive side of the paper is a large set of evaluations on many different control and robotic tasks. For many tasks, D4PG performs better than the variant that does not use distributional updates (D3PG), however by not much. There are some tasks showing no-difference. On the other hand, the choice of N=5 in comparisons is hard to understand and lacks further experimental justifications. Different setting and new performance metrics (e.g. data efficiency, number of episodes in total) might also reveal more properties of the proposed methods.\n\n\n\n* Other minor comments:\n\n- Algorithm 1 consists of two parts but there are connection between them. It might be confused for ones who are not familiar with the actor-critic framework.\n\n- It would be nicer if all expectation operators in Section 3 comes with corresponding distributions. \n\n- page 2, second paragraph: typos in \"hence my require less samples to learn\"\n\n- it might be better if the reference on arXiv should be changed to relevant publication conferences with archival proceeding: work by Marc G. Bellemare at ICML 2017",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Thorough investigation of distributional policy gradients for continuous problems",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "review": "A DeepRL algorithm is presented that represents distributions over Q values, as applied to DDPG,\nand in conjunction with distributed evaluation across multiple actors, prioritized experience replay, and \nN-step look-aheads. The algorithm is called Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG.\nSOTA results are generated for a number of challenging continuous domain learning problems,\nas compared to benchmarks that include DDPG and PPO, in terms of wall-clock time, and also (most often) in terms\nof sample efficiency.\n\npros/cons\n+ the paper provides a thorough investigation of the distributional approach, as applied to difficult continuous\n  action problems, and in conjunction with a set of other improvements (with ablation tests)\n- the story is a bit mixed in terms of the benefits, as compared to the non-distributional approach, D3PG\n- it is not clear which of the baselines are covered in detail in the cited paper:\n  \"Anonymous. Distributed prioritized experience replay. In submission, 2017.\", \n   i.e., should readers assume that D3PG already exists and is attributable to this other submission?\n\nOverall, I believe that the community will find this to be interesting work.\n\nIs a video of the results available?\n\nIt seems that the distributional model often does not make much of a difference, \nas compared to D3PG non-prioritized.  However, sometimes it does make a big difference, i.e., 3D parkour; acrobot.\nDo the examples where it yields the largest payoff share a particular characteristic?\n\nThe benefit of the distributional models is quite different between the 1-step and 5-step versions. Any ideas why?\n\nOccasionally, D4PG with N=1 fails very badly, e.g., fish, manipulator (bring ball), swimmer.\nWhy would that be? Shouldn't it do at least as well as D3PG in general?\n\nHow many atoms are used for the categorical representation?\nAs many as [Bellemare et al.], i.e., 51 ?\nHow much \"resolution\" is necessary here in order to gain most of the benefits of the distributional representation?\n\nAs far as I understand, V_min and V_max are not the global values, but are specific to the current distribution.\nHence the need for the projection. Is that correct?\n\nWould increasing the exploration noise result in a larger benefit for the distributional approach?\n\nFigure 2: DDPG performs suprisingly poorly in most examples. Any comments on this,\nor is DDPG best avoided in normal circumstances for continuous problems? :-)\n\nIs the humanoid stand so easy because of large (or unlimited) torque limits?\n\nThe wall-clock times are for a cluster with K=32 cores for Figure 1?\n\n\"we utilize a network architecture as specified in Figure 1 which processes the terrain info in order to reduce its dimensionality\"\nFigure 1 provides no information about the reduced dimensionality of the terrain representation, unless I am somehow failing to see this.\n\n\"the full critic architecture is completed by attaching a critic head as defined in Section A\"\nI could find no further documenation in the paper with regard to the \"head\" or a separate critic for the \"head\".\nIt is not clear to me why multiple critics are needed.\n\nDo you have an intuition as to why prioritized replay might be reducing performance in many cases?\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "good evaluation, but lacking in originality",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper investigates a number of additions to DDPG algorithm and their effect on performance. The additions investigated are distributional Bellman updates, N-step returns, and prioritized experience replay.\n\nThe paper does a good job of analyzing these effects on a wide range of continuous control tasks, from the standard benchmark suite, to hand manipulation, to complex terrain locomotion and I believe these results are valuable to the community.\n\nHowever, I have a concern about the soundness of using N-step returns in DDPG setting. When a sequence of length N is sampled from the replay buffer and used to calculate N-step return, this sequence is generated according a particular policy. As a result, experience is non-stationary - for the same state-action pair, early iterations of the algorithm will produce structurally different (not just due to stochasticity) N-step returns because the policy to generate those N steps has changed between algorithm iterations. So it seems to me the authors are using off-policy updates where strictly on-policy updates should be used. I would like some clarification from the authors on this point, and if it is indeed the case to bring attention to this point in the final manuscript.\n\nIt would also be useful to evaluate the effect of N for values other than 1 and 5, especially given the significance this addition has on performance. I can believe N-step returns are useful, possibly due to effectively enlarging simulation timestep, but it would be good to know at which point it becomes detrimental.\n\nI also believe \"Distributional Policy Gradients\" is an overly broad title for this submission as this work still relies on off-policy updates and does not tackle the problem of marrying distributional updates with on-policy methods. \"Distributional DDPG\" or \"Distributional Actor-Critic\" or variant perhaps could be more fair title choices?\n\nAside from these concerns, lack of originality of contributions makes it difficult to highly recommend the paper. Nonetheless, I do believe the experimental evaluation if well-conducted and would be of interest to the ICLR community. ",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}