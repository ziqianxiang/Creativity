{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper implements Group convolutions on inputs defined over hexagonal lattices instead of square lattices, using the roto-translation group. The internal symmetries of the hexagonal grid allow for a larger discrete rotation group than when using square pixels, leading to improved performance on CIFAR and aerial datasets.\n\nThe paper is well-written and the reviewers were positive about its results. That said, the AC wonders what is the main contribution of this work relative to existing related works (such as Group Equivarant CNNS, Cohen & Welling'16, or steerable CNNs, Cohen & Welling'17). While it is true that extending GCNNs to hexagonal lattices is a non-trivial implementation task, the contribution lacks significance in the mathematical/learning fronts, which are perhaps the ones ICLR audience will care more about. Besides, the numerical results, while improved versus their square lattice counterparts, are not a major improvement over the state-of-the-art.\n\nIn summary, the AC believes this is a borderline paper. The unanimous favorable reviews tilt the decision towards acceptance. ",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Interesting general approach, not really convinced by the practical use",
            "rating": "7: Good paper, accept",
            "review": "\nThe authors took my comments nicely into account in their revision, and their answers are convincing. I increase my rating from 5 to 7. The authors could also integrate their discussion about their results on CIFAR in the paper, I think it would help readers understand better the advantage of the contribution.\n\n----\n\nThis paper is based on the theory of group equivariant CNNs (G-CNNs), proposed by Cohen and Welling ICML'16.\n\nRegular convolutions are translation-equivariant, meaning that if an image is translated, its convolution by any filter is also translated. They are however not rotation-invariant for example.  G-CNN introduces G-convolutions, which are equivariant to a given transformation group G.\n\nThis paper proposes an efficient implementation of G-convolutions for 6-fold rotations (rotations of multiple of 60 degrees), using a hexagonal lattice. The approach is evaluated on CIFAR-10 and AID, a dataset of aerial scene classification. The approach outperforms G-convolutions implemented on a squared lattice, which allows only 4-fold rotations on AID by a short margin. On CIFAR-10, the difference does not seem significative (according to Tables 1 and 2).\nI guess this can be explained by the fact that rotation equivariance makes sense for aerial images, where the scene is mostly fronto-parallel, but less for CIFAR (especially in the upper layers), which exhibits 3D objects.\n\nI like the general approach of explicitly putting desired equivariance in the convolutional networks. Using a hexagonal lattice is elegant, even if it is not new in computer vision (as written in the paper). However, as the transformation group is limited to rotations, this is interesting in practice mostly for fronto-parallel scenes, as the experiences seem to show. It is not clear how the method can be extended to other groups than 2D rotations.\n\nMoreover, I feel like the paper sometimes tries to mask the fact that the proposed method is limited to rotations. It is admittedly clearly stated in the abstract and introduction, but much less in the rest of the paper.\n\nThe second paragraph of Section 5.1 is difficult to keep in a paper. It says that \"From a qualitative inspection of these hexagonal interpolations we conclude that no information is lost during the sampling procedure.\"  \"No information is lost\" is a strong statement from a qualitative inspection, especially of a hexagonal image.  This statement should probably be removed. One way to evaluate the information lost could be to iterate interpolation between hexagonal and squared lattices to see if the image starts degrading at some point.\n\n\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper extends group equivariant convolutional networks to images with hexagonal pixelation.  While performance gains w.r.t. to the original squared lattices are not very large, the work can be inspiring for further research. ",
            "rating": "7: Good paper, accept",
            "review": "The paper proposes G-HexaConv, a framework extending planar and group convolutions for hexagonal lattices. Original Group-CNNs (G-CNNs) implemented on squared lattices were shown to be invariant to translations and rotations by multiples of 90 degrees. With the hexagonal lattices defined in this paper, this invariance can be extended to rotations by multiples of 60 degrees. This shows small improvements in the CIFAR-10 performances, but larger margins in an Aerial Image Dataset. \n\nDefining hexagonal pixel configurations in convolutional networks requires both resampling input images (under squared lattices) and reformulate image indexing. All these steps are very well explained in the paper, combining mathematical rigor and clarifications. \n\nAll this makes me believe the paper is worth being accepted at ICLR conference. \n\nSome issues that would require further discussion/clarification: \n- G-HexaConv critical points are memory and computation complexity. Authors claim to have an efficient implementation but the paper lacks a proper quantitative evaluation.  Memory complexity and computational time comparison between classic CNNs and G-HexaConv should be provided.\n- I encourage the authors to open the source  code for reproducibility and comparison with future transformational equivariant representations \n-Also, in Fig.1, I would recommend to clarify that image ‘f’ corresponds to a 2D view of a hexagonal image pixelation.  My first impression was a rectangular pixelation seen from a perspective view.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A good submission that shows how to practically implement G-convolutional layers for DNNs on hexagonal lattices and the benefits of doing so.",
            "rating": "7: Good paper, accept",
            "review": "The paper presents an approach to efficiently implement planar and group convolutions over hexagonal lattices to leverage better accuracy of these operations due to reduced anisotropy. They show that convolutional neural networks thus built lead to better performance - reduced inductive bias - for the same parameter budget.\n\nG-CNNs were introduced by Cohen and Welling in ICML, 2016. They proposed DNN layers that implemented equivariance to symmetry groups. They showed that group equivariant networks can lead to more effective weight sharing and hence more efficient networks as evinced by better performance on CIFAR10 & CIFAR10+ for the same parameter budget. This paper shows G-equivariance implemented on hexagonal lattices can lead to even more efficient networks. \n\nThe benefits of using hexagonal lattices over rectangular lattices is well known in the signal processing as well as in computer vision. For example, see   \n\nGolay M. Hexagonal parallel pattern transformation. IEEE Transactions on Computers 1969. 18(8): p. 733-740.\n\nStaunton R. The design of hexagonal sampling structures for image digitization and their use with local operators. Image and Vision Computing 1989. 7(3): p. 162-166. \n\nL. Middleton and J. Sivaswamy, Hexagonal Image Processing, Springer Verlag, London, 2005\n\nThe originality of the paper lies in the practical and efficient implementation of G-Conv layers. Group-equivariant DNNs could lead to more robust, efficient and (arguably) better performing neural networks.\n\nPros\n\n- A good paper that systematically pushes the state of the art towards the design of invariant, efficient and better performing  DNNs with G-equivariant representations.\n\n- It leverages upon the existing theory in a variety of areas - signal & image processing and machine learning, to design better DNNs.\n\n - Experimental evaluation suffices for a proof of concept validation of the presented ideas.   \n\n \nCons\n\n- The authors should relate the paper better to existing works in the signal processing and vision literature.\n\n- The results are on simple benchmarks like CIFAR-10. It is likely but not immediately apparent if the benefits scale to more complex problems.\n\n- Clarity could be improved in a few places\n\n: Since * is used for a standard convolution operator, it might be useful to use *_g as a G-convolution operator.\n\n: Strictly speaking, for translation equivariance, the shift should be cyclic etc.\n\n: Spelling mistakes - authors should run a spellchecker.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}