{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The submission proposes a loss surrogate for top-k classification, as in the official imagenet evaluation.  The approach is well motivated, and the paper is very well organized with thorough technical proofs in the appendix, and a well presented main text.  The main results are: 1) a theoretically motivated surrogate, 2) that gives up to a couple percent improvement over cross-entropy loss in the presence of label noise or smaller datasets.\n\nIt is a bit disappointing that performance is limited in the ideal case and that it does not more gracefully degrade to epsilon better than cross entropy loss.  Rather, it seems to give performance epsilon worse than cross-entropy loss in an ideal case with clean labels and lots of data.  Nevertheless, it is a step in the right direction for optimizing the error measure to be used during evaluation.  The reviewers uniformly recommended acceptance.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Promising extension of SVM's top-k loss to deep models",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper is clear and well written. The proposed approach seems to be of interest and to produce interesting results. As datasets in various domain get more and more precise, the problem of class confusing with very similar classes both present or absent of the training dataset is an important problem, and this paper is a promising contribution to handle those issues better.\n\nThe paper proposes to use a top-k loss such as what has been explored with SVMs in the past, but with deep models. As the loss is not smooth and has sparse gradients, the paper suggests to use a smoothed version where maximums are replaced by log-sum-exps.\n\nI have two main concerns with the presentation.\n\nA/ In addition to the main contribution, the paper devotes a significant amount of space to explaining how to compute the smoothed loss. This can be done by evaluating elementary symmetric polynomials at well-chosen values.\n\nThe paper argues that classical methods for such evaluations (e.g., using the usual recurrence relation or more advanced methods that compensate for numerical errors) are not enough when using single precision floating point arithmetic. The paper also advances that GPU parallelization must be used to be able to efficiently train the network.\n\nThose claims are not substantiated, however, and the method proposed by the paper seems to add substantial complexity without really proving that it is useful.\n\nThe paper proposes a divide-and-conquer approach, where a small amount of parallelization can be achieved within the computation of a single elementary symmetric polynomial value. I am not sure why this is of interest - can't the loss evaluation already be parallelized trivially over examples in a training/testing minibatch? I believe the paper could justify this approach better by providing a bit more insights as to why it is required. For instance:\n\n- What accuracies and train/test times do you get using standard methods for the evaluation of elementary symmetric polynomials?\n- How do those compare with CE and L_{5, 1} with the proposed method?\n- Are numerical instabilities making this completely unfeasible? This would be especially interesting to understand if this explodes in practice, or if evaluations are just a slightly inaccurate without much accuracy loss.\n\n\nB/ No mention is made of the object detection problem, although multiple of the motivating examples in Figure 1 consider cases that would fall naturally into the object detection framework. Although top-k classification considers in principle an easier problem (no localization), a discussion, as well as a comparison of top-k classification vs., e.g., discarding localization information out of object detection methods, could be interesting.\n\nAdditional comments:\n\n- Figure 2b: this visualization is confusing. This is presented in the same figure and paragraph as the CIFAR results, but instead uses a single synthetic data point in dimension 5, and k=1. This is not convincing. An actual experiment using full dataset or minibatch gradients on CIFAR and the same k value would be more interesting.\n\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The paper is well written and the contribution is sound",
            "rating": "7: Good paper, accept",
            "review": "This paper made some efforts in smoothing the top-k losses proposed in Lapin et al. (2015).  A family of smooth surrogate loss es was proposed, with the help of which the top-k error may be minimized directly. The properties of the smooth surrogate losses were studied and the computational algorithms for SVM with these losses function were also proposed. \n\nPros:\n1, The paper is well presented and is easy to follow.\n2, The contribution made in this paper is sound, and the mathematical analysis seems to be correct. \n3, The experimental results look convincing. \n\nCons:\nSome statements in this paper are not clear to me. For example, the authors mentioned sparse or non-sparse loss functions. This statement, in my view, could be misleading without further explanation (the non-sparse loss was mentioned in the abstract).\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper. Should be accepted",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "This paper introduces a smooth surrogate loss function for the top-k SVM, for the purpose of plugging the SVM to the deep neural networks. The idea is to replace the order statistics, which is not smooth and has a lot of zero partial derivatives, to the exponential of averages, which is smooth and is a good approximation of the order statistics by a good selection of the \"temperature parameter\". The paper is well organized and clearly written. The idea deserves a publication.\n\nOn the other hand, there might be better and more direct solutions to reduce the combinatorial complexity. When the temperature parameter is small enough, both of the original top-k SVM surrogate loss (6) and the smooth loss (9) can be computed precisely by sorting the vector s first, and take a good care of the boundary around s_{[k]}.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}