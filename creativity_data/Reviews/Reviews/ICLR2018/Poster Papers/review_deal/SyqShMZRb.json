{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper presents a more complex version of the grammar-VAE, which can be used to generate structured discrete objects for which a grammar is known, by adding a second 'attribute grammar', inspired by Knuth.\n\nOverall, the idea is a bit incremental, but the space is wide open and I think that structured encoder/decoders is an important direction.  The experiments seem to have been done carefully (with some help from the reviewers) and the results are convincing.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Interesting idea but poor presentation",
            "rating": "3: Clear rejection",
            "review": "The paper presents an approach for improving variational autoencoders for structured data that provide an output that is both syntactically valid and semantically reasonable.  The idea presented seems to have merit , however, I found the presentation lacking. Many sentences are poorly written making the paper hard to read, especially when not familiar with the presented methods. The experimental section could be organized better. I didn't like that two types of experiment are now presented in parallel. Finally, the paper stops abruptly without any final discussion and/or conclusion. ",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Let me first note that I am not very familiar with the literature on program generation, \nmolecule design or compiler theory, which this paper draws heavily from, so my review is an educated guess. \n\nThis paper proposes to include additional constraints into a VAE which generates discrete sequences, \nnamely constraints enforcing both semantic and syntactic validity. \nThis is an extension to the Grammar VAE of Kusner et. al, which includes syntactic constraints but not semantic ones.\nThese semantic constraints are formalized in the form of an attribute grammar, which is provided in addition to the context-free grammar.\nThe authors evaluate their methods on two tasks, program generation and molecule generation. \n\nTheir method makes use of additional prior knowledge of semantics, which seems task-specific and limits the generality of their model. \nThey report that their method outperforms the Character VAE (CVAE) and Grammar VAE (GVAE) of Kusner et. al.  \nHowever, it isn't clear whether the comparison is appropriate: the authors report in the appendix that they use the kekulised version of the Zinc dataset of Kusner et. al, whereas Kusner et. al do not make any mention of this. \nThe baselines they compare against for CVAE and GVAE in Table 1 are taken directly from Kusner et. al though. \nCan the authors clarify whether the different methods they compare in Table 1 are all run on the same dataset format?\n\nTypos:\n- Page 5: \"while in sampling procedure\" -> \"while in the sampling procedure\"\n- Page 6: \"a deep convolution neural networks\" -> \"a deep convolutional neural network\"\n- Page 6: \"KL-divergence that proposed in\" -> \"KL-divergence that was proposed in\" \n- Page 6: \"since in training time\" -> \"since at training time\"\n- Page 6: \"can effectively computed\" -> \"can effectively be computed\"\n- Page 7: \"reset for training\" -> \"rest for training\" ",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "Strong paper presents state-of-the-art results",
            "rating": "7: Good paper, accept",
            "review": "NOTE: \n\nWould the authors kindly respond to the comment below regarding Kekulisation of the Zinc dataset? Fair comparison of the data is a serious concern. I have listed this review as a good for publication due to the novelty of ideas presented, but the accusation of misrepresentation below is a serious one and I would like to know the author's response.\n\n*Overview*\n\nThis paper presents a method of generating both syntactically and semantically valid data from a variational autoencoder model using ideas inspired by compiler semantic checking. Instead of verifying the semantic correctness offline of a particular discrete structure, the authors propose “stochastic lazy attributes”, which amounts to loading semantic constraints into a CFG and using a tailored latent-space decoder algorithm that guarantees both syntactic semantic valid. Using Bayesian Optimization, search over this space can yield decodings with targeted properties.\n\nMany of the ideas presented are novel. The results presented are state-of-the art. As noted in the paper, the generation of syntactically and semantically valid data is still an open problem. This paper presents an interesting and valuable solution, and as such constitutes a large advance in this nascent area of machine learning.\n\n*Remarks on methodology*\n\nBy initializing a decoding by “guessing” a value, the decoder will focus on high-probability starting regions of the space of possible structures. It is not clear to me immediately how this will affect the output distribution. Since this process on average begins at high-probability region and makes further decoding decisions from that starting point, the output distribution may be biased since it is the output of cuts through high-probability regions of the possible outputs space. Does this sacrifice exploration for exploitation in some quantifiable way? Some exploration of this issue or commentary would be valuable. \n\n*Nitpicks*\n\nI found the notion of stochastic predetermination somewhat opaque, and section 3 in general introduces much terminology, like lazy linking, that was new to me coming from a machine learning background. In my opinion, this section could benefit from a little more expansion and conceptual definition.\n\nThe first 3 sections of the paper are very clearly written, but the remainder has many typos and grammatical errors (often word omission). The draft could use a few more passes before publication.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}