{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "Meta score: 7\n\nThis paper presents a novel architecture for neural network based TTS using a memory buffer architecture.  The authors have made good efforts to evaluate this system against other state-of-the-art neural TTS systems, although this is hampered by the need for re-implementation and the evident lack of optimal hyperparameters for e.g. tacotron.  TTS is hard to evaluate against existing approaches, since it requires subjective user evaluation.  But overall, despite its limtations, this is a good and interesting paper which I would like to see accepted\n\nPros:\n - novel architecture\n - good experimentation on multiple databases\n - good response to reviewer comments\n - good results\n\nCons:\n - some problems with the experimental comparison (baselines compared against)\n - writing could be clearer, and sometimes it feels like the authors are slightly overclaiming\n\nI take  the point that this might be more suitable for a speech conference, but it seems to me that paper offers enough to the ICLR community for it to be worth accepting.\n\n",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "a good paper. accept.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "This is an interesting paper investigating a novel neural TTS strategy that can generate speech signals by sampling voices in the wild.  The main idea here is to use a working memory with a shifting buffer.  I also listened to the samples posted on github and the quality of the generated voices seems to be OK considering that the voices are actually sampled in the wild. Compared to other state-of-the-art systems like wavenet, deep voice and tacotron, the proposed approach here is claimed to be simpler and relatively easy to deploy in practice.   Globally this is a good piece of work with solid performance. However, I have some (minor) concerns.\n\n1.  Although the authors claim that there is no RNNs involved in the architectural design of the system,  it seems to me that  the working memory with a shifting buffer which takes the previous output as one of its inputs is a network with recurrence. \n\n2. Since the working memory is the key in the architectural design of VoiceLoop, it would be helpful to show its behavior under various configurations and their impact to the performance. For instance,  how will  the length of the running buffer affect the final quality of the voice? \n\n3. A new speaker's voice is generated by only providing the speaker's embedding vector to the system.  This will require a large number of speakers in the training data in the first place to get the system learn the spread of speaker embeddings in the latent (embedding) space.  What will happen if a new speaker's acoustic characteristics are obvious far away from the training speakers?  For instance, a girl voice vs. adult male training speakers.  In this case, the embedding of the girl's voice will show up in the sparse region of the embedding space of training speakers.  How does it affect the performance of the system?  It would be interesting to know. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A good paper, but it could be better for writing and baseline comparisons",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper studies the problem of text-to-speech synthesis (TTS) \"in the wild\" and proposes to use the shifting buffer memory. \n\nSpecifically, an input text is transformed to phoneme encoding and then context vector is created with attention mechanism. With context, speaker ID, previous output, and buffer, the new buffer representation is created with a shallow fully connected neural network and inserted into the buffer memory. Then the output is created by buffer and speaker ID with another fully connected neural network. A novel speaker can be adapted just by fitting it with SGD while fixing all other components.\n\nIn experiments, authors try single-speaker TTS and multi-speaker TTS along with speaker identification (ID), and show that the proposed approach outperforms baselines, namely, Tacotron and Char2wav. Finally, they use the challenging Youtube data to train the model and show promising results.\n\nI like the idea in the paper but it has some limitations as described below:\n\nPros:\n1. It uses relatively simple and less number of parameters by using shallow fully-connected neural networks. \n2. Using shifting buffer memory looks interesting and novel.\n3. The proposed approach outperforms baselines in several tasks, and the ability to fit to a novel speaker is nice. But there are some issues as well (see Cons.)\n\nCons:\n1. Writing is okay but could be improved. Some notations were not clearly described in the text even though it was in the table. \n2. Baselines. The paper says Deep Voice 2 (Arik et al., 2017a) is only prior work for multi-speaker TTS. However, it was not compared to. Also for multi-speaker TTS, in (Arik et al., 2017a), Tacotron (Wang et al., 2017) was used as a baseline but in this paper only Char2wav was employed as a baseline. Also for Youtube dataset, it would be great if some baselines were compared with like  (Arik et al., 2017a).\n\n\nDetailed comment:\n1. To demonstrate the efficiency of the proposed model, it would be great to have the numbers of parameters for the proposed model and baseline models.\n2. I was not so clear about how to fit a new speaker and adding more detail would be good.\n3. Why do you think your model is better than VCTK test split, and even VCTK85 is better than VCTK101?",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper - not sure if ICLR is the right venue",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper present the application of the memory buffer concept to speech synthesis, and additionally learns a \"speaker vector\" that makes the system adaptive and work reasonably well on \"in-the-wild\" speech data. This is a relevant problem, and a novel solution, but synthesis is a wicked problem to evaluate, so I am not sure if ICLR is the best venue for this paper. I see two competing goals:\n\n- If the focus is on showing that the presented approach outperforms other approaches under given conditions, a different task would be better (for example recognition, or some sort of trajectory reconstruction)\n- If the focus is on showing that the system outperforms other synthesis systems, then a speech oriented venue might be best (and it is unfortunate that optimized hyper-parameters for the other systems are not available for a fair comparsion)\n- If fair comparisons with the other appraoches cannot be made, my sense is that the multi-speaker (post-training fitting) option is really the most interesting and novel contribution here, which could be discussed in mroe detail\n\nStill, the approach is creative and interesting and deserves to be presented. I have a few questions/ suggestions:\n\nIntroduction\n\n- The link to Baddeley's \"phonological loop\" concept seems weak at best. There is nothing phonological about the features that this model stores and retrieves, and no evidence that the model behaves in a way consistent with \"phonologcial\" (or articulatory) assumptions or models - maybe best to avoid distracting the reader with this concept and strengthen the speaker adaptation aspect?\n- The memory model is not an RNN, but it is a recurrently called structure (as the name \"phonological loop\" also implies) - so I would also not highlight this point much\n- Why would the four properties of the proposed method (mid of p. 2, end of introduction: memory buffer, shared memory, shallow fully connected networks, and simple reader mechanism) lead to better robustness and improve performance on noisy and limited training data? Maybe the proposed approach works better for any speech synthesis task? Why specifically for \"in-the-wild\" data? The results in Table 2 show that the proposed system outperforms other systems on Blizzard 2013, but not Blizzard 2011 - does this support the previous argument?\n- Why not also evaluate MCD scores? This should be a quick and automatic way to diagnose what the system is doing? Or is this not meaningful with the noisy training data?\n\nPrevious work\n\n- Please introduce abbreviations the first time they are used (\"CBHG\" for example)\n- There is other work on using \"in-the-wild\" speech as well: Pallavi Baljekar and Alan W Black. Utterance Selection Techniques for TTS Systems using Found Speech, SSW 2016, Sunnyvale, USA Sept 2016\n\nThe architecture\n- Please explain the \"GMM\" (Gaussian Mixture Model?) attention mechanism in a bit more detail, how does back-propagation work in this case?\n- Why was this approach chosen? Does it promise to be robust or good for low data situations specifically?\n- The fonts in Figure 2 are very small, please make them bigger, and the Figure may not print well in b/w. Why does the mean of the absolute weights go up for high buffer positions? Is there some \"leaking\" from even longer contexts?\n- I don't understand \"However, human speech is not deterministic and one cannot expect [...] truth\". You are saying that the model cannot be excepted to reproduce the input exactly? Or does this apply only to the temporal distribution of the sequence (but not the spectral characteristics)? The previous sentence implies that it does. And how does teacher-forcing help in this case?\n- what type of speed is \"x5\"? Five times slower or faster than real-time?\n\nExperiments\n- Table 2: maybe mention how these results were computed, i.e. which systems use optimized hyper parameters, and which don't? How do these results support the interpretation of hte results in the introruction re in-the-wild data and found data?\n- I am not sure how to read Figure 4. Maybe it would be easier to plot the different phone sequences against each other and show how the timings are off, i.e. plot the time of the center of panel one vs the time of the center of panel 2 for the corresponding phone, and show how this is different from a straight line. Or maybe plot phones as rectangles that get deformed from square shape as durations get learned?\n- Figure 5: maybe provide spectrograms and add pitch contours to better show the effect of the dfifferent intonations? \n- Figure 4 uses a lot of space, could be reduced, if needed\n\nDiscussion\n- I think the first claim is a bit to broad - nowhere is it shown that the method is inherently more robust to clapping and laughs, and variable prosody. The authors will know the relevant data-sets better than I do, maybe they can simply extend the discussion to show that this is what happens. \n- Efficiency: I think Wavenet has also gotten much faster and runs in less than real-time now - can you expand that discussion a bit, or maybe give estimates in times of FLOPS required, rather than anecdotal evidence for systems that may or may not be comparable?\n\nConclusion\n- Now the advantage of the proposed model is with the number of parameters, rather than the computation required. Can you clarify? Are your models smaller than competing models?\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}