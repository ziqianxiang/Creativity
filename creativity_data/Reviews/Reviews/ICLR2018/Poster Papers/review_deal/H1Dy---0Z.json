{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "meta score: 8\n\nThe paper present a distributed architecture using prioritized experience replay for deep reinforcement learning.  It is well-written and the experimentation is extremely strong.  The main issue is the originality - technically, it extends previous work in a limited way;  the main contribution is practical, and this is validated by the experiments.  The experimental support is such that the paper has meaningful conclusions and will surely be of interest to people working in the field.  Thus I would say it is comfortably over the acceptance threshold.\n\nPros:\n - good motivation and literature review\n - strong experimentation\n - well-written and clearly presented\n - details in the appendix are very helpful\nCons:\n - possibly limited originality in terms of modelling advances\n",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "A clear proof that parallelizing DQN computations works",
            "rating": "7: Good paper, accept",
            "review": "A parallel aproach to DQN training is proposed, based on the idea of having multiple actors collecting data in parallel, while a single learner trains the model from experiences sampled from a central replay memory. Experiments on Atari game playing and two MuJoCo continuous control tasks show significant improvements in terms of training time and final performance compared to previous baselines.\n\nThe core idea is pretty straightforward but the paper does a very good job at demonstrating that it works very well, when implemented efficiently over a large cluster (which is not trivial). I also appreciate the various experiments to analyze the impact of several settings (instead of just reporting a new SOTA). Overall I believe this is definitely a solid contribution that will benefit both practitioners and researchers... as long as they got the computational resources to do so!\n\nThere are essentially two more things I would have really liked to see in this paper (maybe for future work?):\n- Using all Rainbow components\n- Using multiple learners (with actors cycling between them for instance)\nSharing your custom Tensorflow implementation of prioritized experience replay would also be a great bonus!\n\nMinor points:\n- Figure 1 does not seem to be referenced in the text \n- « In principle, Q-learning variants are off-policy methods » => not with multi-step unless you do some kind of correction! I think it is important to mention it even if it works well in practice (just saying « furthermore we are using a multi-step return » is too vague)\n- When comparing the Gt targets for DQN vs DPG it strikes me that DPG uses the delayed weights phi- to select the action, while DQN uses current weights theta. I am curious to know if there is a good motivation for this and what impact this can have on the training dynamics.\n- In caption of Fig. 5 25K should be 250K\n- In appendix A why duplicate memory data instead of just using a smaller memory size?\n- In appendix D it looks like experiences removed from memory are chosen by sampling instead of just removing the older ones as in DQN. Why use a different scheme?\n- Why store rewards and gamma’s at each time step in memory instead of just the total discounted reward?\n- It would have been better to re-use the same colors as in Fig. 2 for plots in the appendix\n- Would Fig. 10 be more interesting with the full plot and a log scale on the x axis?",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A somewhat trivial extension of Prioritized Experience Replay by adding parallelization in actor algorithm",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper proposes a distributed architecture for deep reinforcement learning at scale, specifically, focusing on adding parallelization in actor algorithm in Prioritized Experience Replay framework. It has a very nice introduction and literature review of Prioritized experience replay and also suggested to parallelize the actor algorithm by simply adding more actors to execute in parallel, so that the experience replay can obtain more data for the learner to sample and learn. Not surprisingly, as this framework is able to learn from way more data (e.g. in Atari), it outperforms the baselines, and Figure 4 clearly shows the more actors we have the better performance we will have. \n\nWhile the strength of this paper is clearly the good writing as well as rigorous experimentation, the main concern I have with this paper is novelty. It is in my opinion a somewhat trivial extension of the previous work of Prioritized experience replay in literature; hence the challenge of the work is not quite clear. Hence, I feel adding some practical learnings of setting up such infrastructure might add more flavor to this paper, for example. ",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A new way of parallelizing distributed deep RL",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "review": "This paper examines a distributed Deep RL system in which experiences, rather than gradients, are shared between the parallel workers and the centralized learner. The experiences are accumulated into a central replay memory and prioritized replay is used to update the policy based on the diverse experience accumulated by all the of the workers. Using this system, the authors are able to harness much more compute to learn very high quality policies in little time. The results very convincingly show that Ape-X far outperforms competing algorithms such as recently published Rainbow. \n\nIt’s hard to take issue with a paper that has such overwhelmingly convincing experimental results. However, there are a couple additional experiments that would be quite nice:\n•\tIn order to understand the best way for training a distributed RL agent, it would be nice to see a side-by-side comparison of systems for distributed gradient sharing (e.g. Gorila) versus experience sharing (e.g. Ape-X). \n•\tIt would be interesting to get a sense of how Ape-X performs as a function of the number of frames it has seen, rather than just wall-clock time. For example, in Table 1, is Ape-X at 200M frames doing better than Rainbow at 200M frames?\n\nPros:\n•\tWell written and clear.\n•\tVery impressive results.\n•\tIt’s remarkable that Ape-X preforms as well as it does given the simplicity of the algorithm.\n\nCons:\n•\tHard to replicate experiments without the deep computational pockets of DeepMind.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}