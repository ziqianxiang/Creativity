{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper is tackling an important open problem.\n\nAnonReviewer3 identified some technical issues that led them to rate the manuscript 5 (i.e., just below the acceptance threshold). Many of these issues are resolved by the reviewer in their review, and the author response makes it clear that these fixes are indeed correct.  However, other issues that the reviewer raises are not provided with solutions.  The authors address these points, but in one case at least (regarding w_infinity), I find the new text somewhat hand-waivy. Regardless, I'm inclined to accept the paper because the issues seem to be straightforward. Ultimately, the authors are responsible for the correctness of the results.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Very interesting characterisation of limiting behaviour of the log-loss minimisaton",
            "rating": "7: Good paper, accept",
            "review": "Paper focuses on characterising behaviour of the log loss minimisation on the linearly separable data. As we know, optimisation like this does not converge in a strict mathematical sense, as the norm of the model will grow to infinity. However, one can still hope for a convergence of normalised solution (or equivalently - convergence in term of separator angle, rather than parametrisation). This paper shows that indeed, log-loss (and some other similar losses), minimised with gradient descent, leads to convergence (in the above sense) to the max-margin solution. On one hand it is an interesting property of model we train in practice, and on the other - provides nice link between two separate learning theories.\n\nPros:\n- easy to follow line of argument\n- very interesting result of mapping \"solution\" of unregularised logistic regression (under gradient descent optimisation) onto hard max margin one\n\nCons:\n- it is not clear in the abstract, and beginning of the paper what \"convergence\" means, as in the strict sense logistic regression optimisation never converges on separable data. It would be beneficial for the clarity if authors define what they mean by convergence (normalised weight vector, angle, whichever path seems most natural) as early in the paper as possible.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting paper, but issues with correctness and presentation",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper offers a formal proof that gradient descent on the logistic\nloss converges very slowly to the hard SVM solution in the case where\nthe data are linearly separable. This result should be viewed in the\ncontext of recent attempts at trying to understand the generalization\nability of neural networks, which have turned to trying to understand\nthe implicit regularization bias that comes from the choice of\noptimizer. Since we do not even understand the regularization bias of\noptimizers for the simpler case of linear models, I consider the paper's\ntopic very interesting and timely.\n\nThe overall discussion of the paper is well written, but on a more\ndetailed level the paper gives an unpolished impression, and has many\ntechnical issues. Although I suspect that most (or even all) of these\nissues can be resolved, they interfere with checking the correctness of\nthe results. Unfortunately, in its current state I therefore do not\nconsider the paper ready for publication.\n\n\nTechnical Issues:\n\nThe statement of Lemma 5 has a trivial part and for the other part the\nproof is incorrect: Let x_u = ||nabla L(w(u))||^2.\n  - Then the statement sum_{u=0}^t x_u < infinity is trivial, because\n    it follows directly from ||nabla L(w(u))||^2 < infinity for all u. I\n    would expect the intended statement to be sum_{u=0}^infinity x_u <\n    infinity, which actually follows from the proof of the lemma.\n  - The proof of the claim that t*x_t -> 0 is incorrect: sum_{u=0}^t x_u\n    < infinity does not in itself imply that t*x_t -> 0, as claimed. For\n    instance, we might have x_t = 1/i^2 when t=2^i for i = 1,2,... and\n    x_t = 0 for all other t.\n\nDefinition of tilde{w} in Theorem 4:\n  - Why would tilde{w} be unique? In particular, if the support vectors\n    do not span the space, because all data lie in the same\n    lower-dimensional hyperplane, then this is not the case.\n  - The KKT conditions do not rule out the case that \\hat{w}^top x_n =\n    1, but alpha_n = 0 (i.e. a support vector that touches the margin,\n    but does not exert force against it). Such n are then included in\n    cal{S}, but lead to problems in (2.7), because they would require\n    tilde{w}^top x_n = infinity, which is not possible.\n\nIn the proof of Lemma 6, case 2. at the bottom of p.14:\n  - After the first inequality, C_0^2 t^{-1.5 epsilon_+} should be \n    C_0^2 t^{-epsilon_+}\n  - After the second inequality the part between brackets is missing an\n    additional term C_0^2 t^{-\\epsilon_+}.\n  - In addition, the label (1) should be on the previous inequality and\n    it should be mentioned that e^{-x} <= 1-x+x^2 is applied for x >= 0\n    (otherwise it might be false).\nIn the proof of Lemma 6, case 2 in the middle of p.15:\n  - In the line of inequality (1) there is a t^{-epsilon_-} missing. In\n    the next line there is a factor t^{-epsilon_-} too much.\n  - In addition, the inequality e^x >= 1 + x holds for all x, so no need\n    to mention that x > 0.\n\nIn Lemma 1:\n  - claim (3) should be lim_{t \\to \\infty} w(t)^\\top x_n = infinity\n  - In the proof: w(t)^top x_n > 0 only holds for large enough t.\n\nRemarks:\n\np.4 The claim that \"we can expect the population (or test)\nmisclassification error of w(t) to improve\" because \"the margin of w(t)\nkeeps improving\" is worded a little too strongly, because it presumes\nthat the maximum margin solution will always have the best\ngeneralization error.\n\nIn the proof sketch (p.3):\n  - Why does the fact that the limit is dominated by gradients that are\n    a linear combination of support vectors imply that w_infinity will\n    also be a non-negative linear combination of support vectors?\n  - \"converges to some limit\". Mention that you call this limit\n    w_infinity\n\n\nMinor Issues:\n\nIn (2.4): add \"for all n\".\n\np.10, footnote: Shouldn't \"P_1 = X_s X_s^+\" be something like \"P_1 =\n(X_s^top X_s)^+\"?\n\nA.9: ell should be ell'\n\nThe paper needs a round of copy editing. For instance:\n  - top of p.4: \"where tilde{w} A is the unique\"\n  - p.10: \"the solution tilde{w} to TO eq. A.2\"\n  - p.10: \"might BOT be unique\"\n  - p.10: \"penrose-moorse pseudo inverse\" -> \"Moore-Penrose\n    pseudoinverse\"\n  \nIn the bibliography, Kingma and Ba is cited twice, with different years.\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper analyzes the implicit regularization introduced by gradient descent for optimizing the smooth monotone exponential tailed loss function with separable data. The proposed result is very interesting since it illustrates that using gradient descent to minimize such loss function can lead to the L_2 maximum margin separator. ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "(a) Significance\nThe main contribution of this paper is to characterize the implicit bias introduced by gradient descent on separable data. The authors show the exact form of this bias (L_2 maximum margin separator), which is independent of the initialization and step size. The corresponding slow convergence rate explains the phenomenon that the predictor can continue to improve even when the training loss is already small. The result of this paper can inspire the study of the implicit bias introduced by gradient descent variants or other optimization methods, such as coordinate descent. In addition, the proposed analytic framework seems promising since it may be extended to analyze other models, like neural networks.\n\n(b) Originality\nThis is the first work to give the detailed characterizations of the implicit bias of gradient descent on separable data. The proposed assumptions are reasonable, but it seems to limit to the loss function with exponential tail. I’m curious whether the result in this paper can be applied to other loss functions, such as hinge loss.\n\n(c) Clarity & Quality \nThe presentation of this paper is OK. However, there are some places can be improved in this paper. For example, in Lemma 1, results (3) and (4) can be combined together. It is better for the authors to use another section to illustrate experimental settings instead of writing them in the caption of Figure 3.1. \n\nMinor comments: \n1. In Lemma 1 (4), w^T(t)->w(t)^T\n2. In the proof of Lemma 1, it’s better to use vector 0 for the gradient L(w)\n3. In Theorem 4, the authors should specify eta\n4. In appendix A, page 11, beta is double used\n5. In appendix D, equation (D.5) has an extra period\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}