{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "All three reviewers recommend acceptance. The authors did a good job at the rebuttal which swayed the first reviewer to increase the final rating. This is a clear accept.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Interesting paper",
            "rating": "7: Good paper, accept",
            "review": "This was a pretty interesting read. Thanks. A few comments: \n\nOne sentence that got me confused is this: “Unlike other work (Battaglia et al., 2016) this function is not commutative and we opt for a clear separation between the focus object k and the context object i as in previous work (Chang et al., 2016)”. What exactly is not commutative? The formulation seems completely align with the work of Battaglia et al, with the difference that one additionally has an attention on which edges should be considered (attention on effects). What is the difference to Battaglia et al. that this should highlight? \n\nI don’t think is very explicit what k is in the experiments with bouncing balls. Is it 5 in all of them ? When running with 6-8 balls, how are balls grouped together to form just 5 objects? \n\nIs there any chance of releasing the code/ data used in this experiments? ",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Very interesting work and the proposed approach is well explained. The experimental section could be improved.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "Summary:\nThe manuscript extends the Neural Expectation Maximization framework by integrating an interaction function that allows asymmetric pairwise effects between objects. The network is demonstrated to learn compositional object representations which group together pixels, optimizing a predictive coding objective. The effectiveness of the approach is demonstrated on bouncing balls sequences and gameplay videos from Space Invaders. The proposed R-NEM model generalizes\n\nReview:\nVery interesting work and the proposed approach is well explained. The experimental section could be improved.\nI have a few questions/comments:\n1) Some limitations could have been discussed, e.g. how would the model perform on sequences involving more complicated deformations of objects than in the Space Invaders experiment? As you always take the first frame of the 4-frame stacks in the data set, do the objects deform at all?\n2) It would have been interesting to vary K, e.g. study the behaviour for K in {1,5,10,25,50}. In Space Invaders the model would probably really group together separate objects. What happens if you train with K=8 on sequences of 4 balls and then run on 8-ball sequences instead of providing (approximately) the right number of components both at training and test time (in the extrapolation experiment).\n3) One work that should be mentioned in the related work section is Michalski et al. (2014), which also uses noise and predictive coding to model sequences of bouncing balls and NORBvideos. Their model uses a factorization that also discovers relations between components of the frames, but in contrast to R-NEM the components overlap.\n4) A quantitative evaluation of the bouncing balls with curtain and Space Invaders experiments would be useful for comparison.\n5) I think the hyperparameters of the RNN and LSTM are missing from the manuscript. Did you perform any hyperparameter optimization on these models?\n6) Stronger baselines would improve the experimental section, maybe Seo et al (2016). Alternatively, you could train the model on Moving MNIST (Srivastava et al., 2015) and compare with other published results.\n\nI would consider increasing the score, if at least some of the above points are sufficiently addressed.\n\nReferences:\nMichalski, Vincent, Roland Memisevic, and Kishore Konda. \"Modeling deep temporal dependencies with recurrent grammar cells\"\".\" In Advances in neural information processing systems, pp. 1925-1933. 2014.\nSeo, Youngjoo, Michaël Defferrard, Pierre Vandergheynst, and Xavier Bresson. \"Structured sequence modeling with graph convolutional recurrent networks.\" arXiv preprint arXiv:1612.07659 (2016).\nSrivastava, Nitish, Elman Mansimov, and Ruslan Salakhudinov. \"Unsupervised learning of video representations using lstms.\" In International Conference on Machine Learning, pp. 843-852. 2015.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Clear contribution to neural intuitive physics models",
            "rating": "7: Good paper, accept",
            "review": "Summary\n---\nThis work applies a representaion learning technique that segments entities to learn simple 2d intuitive physics without per-entity supervision. It adds a relational mechanism to Neural Expectation Maximization and shows that this mechanism provides a better simulation of bouncing balls in a synthetic environment.\n\nNeural Expectation Maximization (NEM) decomposes an image into K latent variables (vectors of reals) theta_k. A decoder network reconstructs K images from each of these latent variables and these K images are combined into a single reconstruction using pixel-wise mixture components that place more weight on pixels that match the ground truth. An encoder network f_enc() then updates the latent variables to better explain the reconstructions they produced.\nThe neural nets are learned so that the latent variables reconstruct the image well when used by the mixture model and match a prior otherwise. Previously NEM has been shown to learn variables which represent individual objects (simple shapes) in a compositional manner, using one variable per object.\n\nOther recent neural models can learn to simulate simple 2d physics environments (balls bouncing around in a 2d plane). That work supervises the representation for each entity (ball) explicitly using states (e.g. position and velocity of balls) which are known from the physics simulator used to generate the training data. The key feature of these models is the use of a pairwise embedding of an object and its neighbors (message passing) to predict the object's next state in the simulation.\n\nThis paper paper combines the two methods to create Relational Neural Expectation Maximization (R-NEM), allowing direct interaction at inference time between the latent variables that encode a scene. The encoder network from NEM can be seen as a recurrent network which takes one latent variable theta_k at time t and some input x to produce the next latent variable theta_k at time t+1. R-NEM adds a relational module which computes an embedding used as a third input to the recurrent encoder. Like previous relational models, this one uses a pairwise embedding of the object being updated (object k) and its neighbors. Unlike previous neural physics models, R-NEM uses a soft attention mechanism to determine which objects are neighbors and which are not. Also unlike previous neural models, this method does not require per-object supervision.\n\nExperiments show that R-NEM learns compositional representations that support intuitive physics more effectively than ablative baselines. These experiments\nshow:\n1) R-NEM reconstructs images more accurately than baselines (RNN/LSTM) and NEM (without object interaction).\n2) R-NEM is trained with 4 objects per image. It does a bit worse at reconstructing images with 6-8 objects per image, but still performs better than baselines.\n3) A version of R-NEM without neighborhood attention in the relation module matches the performance of R-NEM using 4 objects and performs worse than R-NEM at 6-8 objects.\n4) R-NEM learns representations which factorize into one latent variable per object as measured by the Adjusted Rand Index, which compares NEM's pixel clustering to a ground truth clustering with one cluster per object.\n5) Qualitative and quantitative results show that R-NEM can simulate 2d ball physics for many time steps more effectively than an RNN and while only suffering gradual divergence from the ground truth simulation.\n\nQualitative results show that the attentional mechanism attends to objects which are close to the context object together, acting like the heuristic neighborhood mechanism from previous work.\n\nFollow up experiments extend the basic setup significantly. One experiment shows that R-NEM demonstrates object permanence by correctly tracking a collision when one of the objects is completely occluded. Another experiment applies the method to the Space Invaders Atari game, showing that it treats columns of aliens as entities. This representation aligns with the game's goal.\n\n\nStrengths\n---\n\nThe paper presents a clear, convincing, and well illustrated story.\n\nWeaknesses\n---\n\n* RNN-EM BCE results are missing from the simulation plot (right of figure 4).\n\nMinor comments/concerns:\n\n* 2nd paragraph in section 4: Are parameters shared between these 3 MLPs (enc,emb,eff)? I guess not, but this is ambiguous.\n\n* When R-NEM is tested against 6-8 balls is K set to the number of balls plus 1? How does performance vary with the number of objects?\n\n* Previous methods report performance across simulations of a variety of physical phenomena (e.g., see \"Visual Interaction Networks\"). It seems that supervision isn't needed for bouncing ball physics, but I wonder if this is the case for other kinds of phenomena (e.g., springs in the VIN paper). Can this method eliminate the need for per-entity supervision in this domain?\n\n* A follow up to the previous comment: Could a supervised baseline that uses per-entity state supervision and neural message passsing (like the NPE from Chang et. al.) be included?\n\n* It's a bit hard to qualitatively judge the quality of the simulations without videos to look at. Could videos of simulations be uploaded (e.g., via anonymous google drive folder as in \"Visual Interaction Networks\")?\n\n* This uses a neural message passing mechanism like those of Chang et. al. and Battaglia et. al. It would be nice to see a citation to neural message passing outside of the physics simulation domain (e.g. to \"Neural Message Passing for Quantum Chemistry\" by Gilmer et. al. in ICML17).\n\n* Some work uses neighborhood attention coefficients for neural message passing. It would be nice to see a citation included.\n    * See \"Neighborhood Attention\" in \"One-Shot Imitation Learning\" by Duan et. al. in NIPS17\n    * Also see \"Programmable Agents\" by Denil et. al.\n\n\nFinal Evaluation\n---\n\nThis paper clearly advances the body of work on neural intuitive physics by incorporating NEM entity representation to allow for less supervision. Alternatively, it adds a message passing mechanism to the NEM entity representation technique. These are moderately novel contributions and there are only minor weaknesses, so this is a clear accept.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}