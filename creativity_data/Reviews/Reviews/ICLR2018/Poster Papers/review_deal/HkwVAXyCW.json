{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper explores what might be characterized as an adaptive form of ZoneOut.\nWith the improvements and clarifications added to the paper during the rebuttal the paper could be accepted.\n",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Review",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The authors proposed a novel RNN model where both the input and the state update of the recurrent cells are skipped adaptively for some time steps. The proposed models are learned by imposing a soft constraint on the computational budget to encourage skipping redundant input time steps. The experiments in the paper demonstrated skip RNNs outperformed regular LSTMs and GRUs o thee addition, pixel MNIST and video action recognition tasks.\n\n\n\nStrength:\n- The experimental results on the simple skip RNNs have shown a good improvement over the previous results.\n\nWeakness:\n- Although the paper shows that skip RNN worked well, I found the appropriate baseline is lacking here. Comparable baselines, I believe, are regular LSTM/GRU whose inputs are randomly dropped out during training.\n\n- Most of the experiments in the main paper are on toy tasks with small LSTMs. I thought the main selling point of the method is the computational gain. Would it make more sense to show that on large RNNs with thousands of hidden units? After going over the additional experiments in the appendix, and I find the three results shown in the main paper seem cherry-picked, and it will be good to include more NLP tasks.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting idea. requires more experiment to be convincing.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper proposes an idea to do faster RNN inference via skip RNN state updates. \nI like the idea of the paper, in particular the design which enables calculating the number of steps to skip in advance. But the experiments are not convincing enough. First the tasks it was tested on are very simple -- 2 synthetic tasks plus 1 small-scaled task. I'd like to see the idea works on larger scale problems -- as that is where the computation/speed matters. Also besides the number of updates reported in table, I think the wall-clock time for inference should also be reported, to demonstrate what the paper is trying to claim.\n\nMinor -- \nCite Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation by Yoshua Bengio, Nicholas Leonard and Aaron Courville for straight-through estimator.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting model, but lacking comparison to prior work",
            "rating": "6: Marginally above acceptance threshold",
            "review": "UPDATE: Following the author's response I've increased my score from 5 to 6. The revised paper includes many of the additional references that I suggested, and the author response clarified my confusion over the Charades experiments; their results are indeed close to state-of-the-art on Charades activity localization (slightly outperformed by [6]), which I had mistakenly confused with activity classification (from [5]).\n\nThe paper proposes the Skip RNN model which allows a recurrent network to selectively skip updating its hidden state for some inputs, leading to reduced computation at test-time. At each timestep the model emits an update probability; if this probability is over a threshold then the next input and state update will be skipped. The use of a straight-through estimator allows the model to be trained with standard backpropagation. The number of state updates that the model learns to use can be controlled with an auxiliary loss function. Experiments are performed on a variety of tasks, demonstrating that the Skip-RNN compares as well or better than baselines even when skipping nearly half its state updates.\n\nPros:\n- Task of reducing computation by skipping inputs is interesting\n- Model is novel and interesting\n- Experiments on multiple tasks and datasets confirm the efficacy of the method\n- Skipping behavior can be controlled via an auxiliary loss term\n- Paper is clearly written\n\nCons:\n- Missing comparison to prior work on sequential MNIST\n- Low performance on Charades dataset, no comparison to prior work\n- No comparison to prior work on IMDB Sentiment Analysis or UCF-101 activity classification\n\nThe task of reducing computation by skipping RNN inputs is interesting, and the proposed method is novel, interesting, and clearly explained. Experimental results across a variety of tasks are convincing; in all tasks the Skip-RNNs achieve their goal of performing as well or better than equivalent non-skipping variants. The use of an auxiliary loss to control the number of state updates is interesting; since it sometimes improves performance it appears to have some regularizing effect on the model in addition to controlling the trade-off between speed and accuracy.\n\nHowever, where possible experiments should compare directly with prior published results on these tasks; none of the experiments from the main paper or supplementary material report any numbers from any other published work.\n\nOn permuted MNIST, Table 2 could include results from [1-4]. Of particular interest is [3], which reports 98.9% accuracy with a 100-unit LSTM initialized with orthogonal and identity weight matrices; this is significantly higher than all reported results for the sequential MNIST task.\n\nFor Charades, all reported results appear significantly lower than the baseline methods reported in [5] and [6] with no explanation. All methods work on “fc7 features from the RGB stream of a two-stream CNN provided by the organizers of the [Charades] challenge”, and the best-performing method (Skip GRU) achieves 9.02 mAP. This is significantly lower than the two-stream results from [5] (11.9 mAP and 14.3 mAP) and also lower than pretrained AlexNet features averaged over 30 frames and classified with a linear SVM, which [5] reports as achieving 11.3 mAP. I don’t expect to see state-of-the-art performance on Charades; the point of the experiment is to demonstrate that Skip-RNNs perform as well or better than their non-skipping counterparts, which it does. However I am surprised at the low absolute performance of all reported results, and would appreciate if the authors could help to clarify whether this is due to differences in experimental setup or something else.\n\nIn a similar vein, from the supplementary material, sentiment analysis on IMDB and action classification on UCF-101 are well-studied problems, but the authors do not compare with any previously published results on these tasks.\n\nThough experiments may not show show state-of-the-art performance, I think that they still serve to demonstrate the utility of the Skip-RNN architecture when compared side-by-side with a similarly tuned non-skipping baseline. However I feel that the authors should include some discussion of other published results.\n\nOn the whole I believe that the task and method are interesting, and experiments convincingly demonstrate the utility of Skip-RNNs compared to the author’s own baselines. I will happily upgrade my rating of the paper if the authors can address my concerns over prior work in the experiments.\n\n\nReferences\n\n[1] Le et al, “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units”, arXiv 2015\n[2] Arjovsky et al, “Unitary Evolution Recurrent Neural Networks”, ICML 2016\n[3] Cooijmans et al, “Recurrent Batch Normalization”, ICLR 2017\n[4] Zhang et al, “Architectural Complexity Measures of Recurrent Neural Networks”, NIPS 2016\n[5] Sigurdsson et al, “Hollywood in homes: Crowdsourcing data collection for activity understanding”, ECCV 2016\n[6] Sigurdsson et al, “Asynchronous temporal fields for action recognition”, CVPR 2017",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}