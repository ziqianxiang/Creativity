{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper proposes a way of detecting statistical interactions in a dataset based on the weights learned by a DNN. The idea is interesting and quite useful as is showcased in the experiments. The reviewers feel that the paper is also quite well written and easy to follow.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Feature interaction identification by multiplying |neural network weight matrices|",
            "rating": "7: Good paper, accept",
            "review": "Based on a hierarchical hereditary assumption, this paper identifies pairwise and high-order feature interactions by re-interpreting neural network weights, assuming higher-order interactions exist only if all its induced lower-order interactions exist. Using a multiplication of the absolute values of all neural network weight matrices on top of the first hidden layer, this paper defines the aggregated strength z_r of each hidden unit r contributing to the final target output y. Multiplying z_r by some statistics of weights connecting a subset of input features to r and summing over r results in final interaction strength of each feature interaction subsets, with feature interaction order equal to the size of each feature subset. \n\nMain issues:\n\n1. Aggregating neural network weights to identify feature interactions is very interesting. However, completely ignoring \nactivation functions makes the method quite crude. \n\n2. High-order interacting features must share some common hidden unit somewhere in a hidden layer within a deep neural network. Restricting to the first hidden layer in Algorithm 1 inevitably misses some important feature interactions.\n\n3. The neural network weights heavily depends on the l1-regularized neural network training, but a group lasso penalty makes much more sense. See Group Sparse Regularization for Deep Neural Networks (https://arxiv.org/pdf/1607.00485.pdf).\n\n4. The experiments are only conducted on some synthetic datasets with very small feature dimensionality p. Large-scale experiments are needed.\n\n5. There are some important references missing. For example, RuleFit is a good baseline method for identifying feature interactions based on random forest and l1-logistic regression (Friedman and Popescu, 2005, Predictive learning via rule ensembles); Relaxing strict hierarchical hereditary constraints, high-order l1-logistic regression based on tree-structured feature expansion identifies pairwise and high-order multiplicative feature interactions (Min et al. 2014, Interpretable Sparse High-Order Boltzmann Machines); Without any hereditary constraint, feature interaction matrix factorization with l1 regularization identifies pairwise feature interactions on datasets with high-dimensional features (Purushotham et al. 2014, Factorized Sparse Learning Models with Interpretable High Order Feature Interactions). \n\n6. At least, RuleFit (Random Forest regression for getting rules + l1-regularized regression) should be used as a baseline in the experiments.\n\nMinor issues:\n\nRanking of feature interactions in Algorithm 1 should be explained in more details.\n\nOn page 3: b^{(l)} \\in R^{p_l}, l should be from 1, .., L. You have b^y.\n\n\nIn summary, the idea of using neural networks for screening pairwise and high-order feature interactions is novel, significant, and interesting.  However, I strongly encourage the authors to perform additional experiments with careful experiment design to address some common concerns in the reviews/comments for the acceptance of this paper.\n \n========\nThe additional experimental results are convincing, so I updated my rating score.\n  ",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting method, more work to do on the experimental side",
            "rating": "7: Good paper, accept",
            "review": "This paper presents a method to identify high-order interactions from the weights of feedforward neural networks.  \n\nThe main benefits of the method are:\n1)\tCan detect high order interactions and there’s no need to specify the order (unlike, for example, in lasso-based methods).\n2)\tCan detect interactions appearing inside of non-linear function (e.g. sin(x1 * x2))\n\nThe method is interesting, in particular if benefit #2 holds experimentally. Unfortunately, there are too many gaps in the experimental evaluation of this paper to warrant this claim right now.\n\nMajor:\n\n1)\tArguably, point 1 is not a particularly interesting setting. The order of the interactions tested is mainly driven by the sample size of the dataset considered, so in some sense the inability to restrict the order of the interaction found can actually be a problem in real settings. \nBecause of this, it would be very helpful to separate the evaluation of benefit 1 and 2 at least in the simulation setting. For example, simulate a synthetic function with no interactions appearing in non-linearities (e.g. x1+x2x3x4+x4x6) and evaluate the different methods at different sample sizes (e.g. 100 samples to 1e5 samples). The proposed method might show high type-1 error under this setting. Do the same for the synthetic functions already in the paper. By the way, what is the sample size of the current set of synthetic experiments?\n2)\tThe authors claim that the proposed method identifies interactions “without searching an exponential solution space of possible interactions”. This is misleading, because the search of the exponential space of interactions happens during training by moving around in the latent space identified by the intermediate layers. It could perhaps be rephrased as “efficiently”.\n3)\tIt’s not clear from the text whether ANOVA and HierLasso are only looking for second order interactions. If so, why not include a lasso with n-order interactions as a baseline?\n4)\tWhy aren’t the baselines evaluated on the real datasets and heatmaps similar to figure 5 are produced?\n5)\tIs it possible to include the ROC curves corresponding to table 2?\n\n\nMinor:\n\n1)\tHave the authors thought about statistical testing in this framework? The proposed method only gives a ranking of possible interactions, but does not give p-values or similar (e.g. FDRs).\n2)\t12 pages of text. Text is often repetitive and can be shortened without loss of understanding or reproducibility.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting use case of neural networks for a traditional statistical problem!",
            "rating": "7: Good paper, accept",
            "review": "This paper develops a novel method to use a neural network to infer statistical interactions between input variables without assuming any explicit interaction form or order. First the paper describes that an 'interaction strength' would be captured through a simple multiplication of the aggregated weight and the weights of the first hidden layers. Then, two simple networks for the main and interaction effects are modeled separately, and learned jointly with posing L1-regularization only on the interaction part to cancel out the main effect as much as possible. The automatic cutoff determination is also proposed by using a GAM fitting based on these two networks. A nice series of experimental validations demonstrate the various types of interactions can be detected, while it also fairly clarifies the limitations.\n\nIn addition to the related work mentioned in the manuscript, interaction detection is also originated from so-called AID, literally intended for 'automatic interaction detector' (Morgan & Sonquist, 1963), which is also the origin of CHAID and CART, thus the tree-based methods like Additive Groves would be the one of main methods for this. But given the flexibility of function representations, the use of neural networks would be worth rethinking, and this work would give one clear example.\n\nI liked the overall ideas which is clean and simple, but also found several points still confusing and unclear.\n\n1) One of the keys behind this method is the architecture described in 4.1. But this part sounds quite heuristic, and it is unclear to me how this can affect to the facts such as Theorem 4 and Algorithm 1. Absorbing the main effect is not critical to these facts? In a standard sense of statistics, interaction would be something like residuals after removing the main (additive) effect. (like a standard test by a likelihood ratio test for models with vs without interactions)\n\n2) the description about the neural network for the main effect is a bit unclear. For example, what does exactly mean the 'networks with univariate inputs for each input variable'? Is my guessing that it is a 1-10-10-10-1 network (in the experiments) correct...? Also, do g_i and g_i' in the GAM model (sec 4.3) correspond to the two networks for the main and interaction effects respectively?\n\n3) mu is finally fixed at min function, and I'm not sure why this is abstracted throughout the manuscript. Is it for considering the requirements for any possible criteria?\n\nPros:\n- detecting (any order / any form of) statistical interactions by neural networks is provided.\n- nice experimental setup and evaluations with comparisons to relevant baselines by ANOVA, HierLasso, and Additive Groves.\n\nCons:\n- some parts of explanations to support the idea has unclear relationship to what was actually done, in particular, for how to cancel out the main effect.\n- the neural network architecture with L1 regularization is a bit heuristic, and I'm not surely confident that this architecture can capture only the interaction effect by cancelling out the main effect.\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}