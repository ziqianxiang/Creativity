{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "All reviewers agreed that, despite the lack of novelty, the proposed method is sound and correctly linked to existing work. As the topic of automatically learning the stepsize is of great practical interest, I am glad to have this paper presented as a poster at ICLR.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Somewhat weak novelty, but well written, complete, and potentially impactful.",
            "rating": "7: Good paper, accept",
            "review": "The authors consider a method (which they trace back to 1998, but may have a longer history) of learning the learning rate of a first-order algorithm at the same time as the underlying model is being optimized, using a stochastic multiplicative update. The basic observation (for SGD) is that if \\theta_{t+1} = \\theta_t - \\alpha \\nabla f(\\theta_t), then \\partial/\\partial\\alpha f(\\theta_{t+1}) = -<\\nabla f(\\theta_t), \\nabla f(\\theta_{t+1})>, i.e. that the negative inner product of two successive stochastic gradients is equal in expectation to the derivative of the tth update w.r.t. the learning rate \\alpha.\n\nI have seen this before for SGD (the authors do not claim that the basic idea is novel), but I believe that the application to other algorithms (the authors explicitly consider Nesterov momentum and ADAM) are novel, as is the use of the multiplicative and normalized update of equation 8 (particularly the normalization).\n\nThe experiments are well-presented, and appear to convincingly show a benefit. Figure 3, which explores the robustness of the algorithms to the choice of \\alpha_0 and \\beta, is particularly nicely-done, and addresses the most natural criticism of this approach (that it replaces one hyperparameter with two).\n\nThe authors highlight theoretical convergence guarantees as an important future work item, and the lack of them here (aside from Theorem 5.1, which just shows asymptotic convergence if the learning rates become sufficiently small) is a weakness, but not, I think, a critical one. This appears to be a promising approach, and bringing it back to the attention of the machine learning community is valuable.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "interesting idea, but weak experiments",
            "rating": "7: Good paper, accept",
            "review": "\nThis paper revisits an interesting and important trick to automatically adapt the stepsize. They consider the stepsize as a parameter to be optimized and apply stochastic gradient update for the stepsize. Such simple trick alleviates the effort in tuning stepsize, and can be incorporated with popular stochastic first-order optimization algorithms, including SGD, SGD with Nestrov momentum, and Adam. Surprisingly, it works well in practice.\n\nAlthough the theoretical analysis is weak that theorem 1 does not reveal the main reason for the benefits of such trick, considering their performance, I vote for acceptance. But before that, there are several issues need to be addressed. \n\n1, the derivation of the update of \\alpha relies on the expectation formulation. I would like to see the investigation of the effect of the size of minibatch to reveal the variance of the gradient in the algorithm combined with such trick. \n\n2, The derivation of the multiplicative rule of HD relies on a reference I cannot find. Please include this part for self-containing. \n\n3, As the authors claimed, the Maclaurin et.al. 2015 is the most related work, however, they are not compared in the experiments. Moreover, the empirical comparisons are only conducted on MNIST. To be more convincing, it will be good to include such competitor and comparing on practical applications on CIFAR10/100 and ImageNet. \n\nMinors: \n\nIn the experiments results figures, after adding the new trick, the SGD algorithms become more stable, i.e., the variance diminishes. Could you please explain why such phenomenon happens?",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "good, but not perfect",
            "rating": "6: Marginally above acceptance threshold",
            "review": "SUMMARY:\n\nThe authors reinvent a 20 years old technique for adapting a global or component-wise learning rate for gradient descent. The technique can be derived as a gradient step for the learning rate hyperparameter, or it can be understood as a simple and efficient adaptation technique.\n\n\nGENERAL IMPRESSION:\n\nOne central problem of the paper is missing novelty. The authors are well aware of this. They still manage to provide added value.\nDespite its limited novelty, this is a very interesting and potentially impactful paper. I like in particular the detailed discussion of related work, which includes some frequently overlooked precursors of modern methods.\n\n\nCRITICISM:\n\nThe experimental evaluation is rather solid, but not perfect. It considers three different problems: logistic regression (a convex problem), and dense as well as convolutional networks. That's a solid spectrum. However, it is not clear why the method is tested only on a single data set: MNIST. Since it is entirely general, I would rather expect a test on a dozen different data sets. That would also tell us more about a possible sensitivity w.r.t. the hyperparameters \\alpha_0 and \\beta.\n\nThe extensions in section 5 don't seem to be very useful. In particular, I cannot get rid of the impression that section 5.1 exists for the sole purpose of introducing a convergence theorem. Analyzing the actual adaptive algorithm would be very interesting. In contrast, the present result is trivial and of no interest at all, since it requires knowing a good parameter setting, which defeats a large part of the value of the method.\n\n\nMINOR POINTS:\n\npage 4, bottom: use \\citep for Duchi et al. (2011).\n\nNone of the figures is legible on a grayscale printout of the paper. Please do not use color as the only cue to identify a curve.\n\nIn figure 2, top row, please display the learning rate on a log scale.\n\npage 8, line 7 in section 4.3: \"the the\" (unintended repetition)\n\nEnd of section 4: an increase from 0.001 to 0.001002 is hardly worth reporting - or am I missing something?\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}