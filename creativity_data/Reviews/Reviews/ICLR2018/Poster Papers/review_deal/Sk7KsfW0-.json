{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "PROS:\n1. good results; the authors made it work\n2. paper is largely well written\n\nCONS:\n1. some found the writing to be unclear and sloppy in places\n2. the algorithm is complicated -- a chain of sub-algorithms\n\nA few small points:\n\n-I initially found Algorithm 1 to be confusing because it wasn't clear whether it was intended to be invoked for each task (making the training depend on all the datasets).  I finally convinced myself that this was not the intention and that the inner loop of the algorithm is what is actually executed incrementally.\n\n",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Method for lifelong learning with neural networks",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "In this paper, the authors propose a method (Dynamically Expandable Network) that addresses issues of training efficiency, how to dynamically grow the network, and how to prevent catastrophic forgetting.\n\nThe paper is well written with a clear problem statement and description of the method for preventing each of the described issues. Interesting points include the use of an L1 regularization term to enforce sparsity in the weights, as well as the method for identifying which neurons have “drifted” too far and should be split. The use of timestamps is a clever addition as well.\n\nOne question would be how sparse training is done, and how this saves computation, especially with the breadth-first search described on page 5. A critique would be that the base networks (a two layer FF net and LeNet) are not very compelling.\n\nExperiments indicate that the method works well, with a clear improvement over progressive networks. Thus, though there isn’t one particular facet of the paper that leaps out, overall the method and results seem solid and worthy of publication.",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting approach to continual learning",
            "rating": "7: Good paper, accept",
            "review": "The paper was clearly written and pleasant to read. I liked the use of sparsity- and group-sparsity-promoting regularizers to select connections and decide how to expand the network.\n\nA strength of the paper is that the proposed algorithm is interesting and intuitive, even if relatively complex, as it requires chaining a sequence of sub-algorithms. It was good to see the impact of each sub-algorithm studied separately (to some degree) in the experimental section. The results are overall strong.\n\nIt’s hard for me to judge the novelty of the approach though, as I’m not an expert on this topic.\n\nJust a few points below:\n- The experiments focus on a relevant continual learning problem, where each new task corresponds to learning a new class. In this setup, the method consistently outperforms EWC (e.g., Fig. 3), as well as the progressive network baseline.\nDid the authors also check the performance on the permuted MNIST benchmark, as studied by Kirkpatrick et al. and Zenke et al.? It would be important to see how the method fares in this setting, where the tasks are the same, but the inputs have to be remapped, and network expansion is less of an issue.\n\n- Fig. 4 would be clearer if the authors showed also the performance and how much the selected connection subsets would change if instead of using the last layer lasso + BFS, the full L1-penalized problem was solved, while keeping the rest of the pipeline intact.\n\n- Still regarding the proposed selective retraining, the special role played by the last hidden layer seems slightly arbitrary. It may well be that it has the highest task-specificity, though this is not trivial to me. This special role might become problematic when dealing with deeper networks.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Investigation of DENs and variants applied to lifelong learning",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The topic is of great interest to the community, and the ideas explored by the authors are reasonable, but I found the conclusion less-than-clear. Mainly, I was not sure how to interpret the experimental findings, and did not have a clear picture of the various models being investigated (e.g. \"base DNN regularized with l2\"), or even of the criteria being examined. What is \"learning capacity\"? (If it's number of model parameters, the authors should just say, \"number of parameters\"). The relative performance of the different models examined, plotted in the top row of Figure 3, is quite different, and though the authors do devote a paragraph to interpreting the results, I found it slightly hard to follow, and was not sure what the bottom line was.\n\nWhat does the \"batch model\" refer to?\n\nre. \" 11.9%p − 51.8%p\"; remove \"p\"?\n\nReference for CIFAR-100? Explain abbreviation for both CIFAR-100 and AWA-Class?\n\nre. \"... but when the number of tasks is large, STL works better since it has larger learning capacity than MTL\": isn't the number of parameters matched? If so, why is the \"learning capacity\" different? What do the authors mean exactly by \"learning capacity\"?\n\nre. Figure 3, e.g. \"Average per-task performance of the models over number of task t\": this is a general point, but usually the expression \"<f(x)> vs. <x>\" is used rather than \"<f(x)> over <x>\" when describing a plot.\n\n\"DNN: dase (sic) DNN\": how is this trained?\n\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}