{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "this submission proposes a learning algorithm for resnets based on their interpreration of them as a discrete approximation to a continuous-time dynamical system.  all the reviewers have found the submission to be clearly written, well motivated and have proposed an interesting and effective learning algorithm for resnets.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Review of \"Multi-level Residual Networks from Dynamical Systems View\"",
            "rating": "7: Good paper, accept",
            "review": "\n\nThis paper proposes a new method to train residual networks in which one starts by training shallow ResNets, doubling the depth and warm starting from the previous smaller model in a certain way, and iterating.  The authors relate this idea to a recent dynamical systems view of ResNets in which residual blocks are viewed as taking steps in an Euler discretization of a certain differential equation.  This interpretation plays a role in the proposed training method by informing how the “step sizes” in the Euler discretization should change when doubling the depth of the network.  The punchline of the paper is that the authors are able to achieve similar performance as “full ResNet training” but with significantly reduced training time.\n\nOverall, the proposed method is novel — even though this idea of going from shallow to deep is natural for residual networks, tying the idea to the dynamical systems perspective is elegant.  Moreover the paper is clearly written.  Experimental results are decent — there are clear speedups to be had based on the authors' experiments.  However it is unclear if these gains in training speed are significant enough for people to flock to using this (more complicated) method of training.\n\nI only have a few small questions/comments:\n* A more naive way to do multi-level training would be to again iteratively double the depth, but perhaps not halve the step size.  This might be a good baseline to compare against to demonstrate the value of the dynamical systems viewpoint.\n* One thing I’m unclear on is how convergence was assessed… my understanding is that the training proceeds for a fixed number of epochs (?) - but shouldn’t this also depend on the depth in some way? \n* Would the speedups be more dramatic for a larger dataset like Imagenet?\n* Finally, not being very familiar with multigrid methods from the numerical methods literature — I would have liked to hear about whether there are deeper connections to these methods.\n\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Pleasant to read, well executed paper.",
            "rating": "7: Good paper, accept",
            "review": "I enjoyed reading the paper. This is a very well written paper, the authors propose a method for speeding up the training time of Residual Networks based on the dynamical system view interpretation of ResNets. In general I have a positive opinion about the paper, however, I’d like to ask for some clarifications.\n\nI’m not fully convinced by the interpretation of Eq. 5: “… d is inversely proportional to the norm of the residual modules G(Yj)”. Since F(Yj) is not a constant, I think that d is inversely proportional to ||G(Yj)||/||F(Yj)||, however, in the interpretation the dependence on ||F(Yj)|| is ignored. Could the authors comment on that?\n\nSection 4. 1 “ Each cycle itself can be regarded as a training process, thus we need to reset the learning rate value at the beginning of each training cycle and anneal the learning rate during that cycle.” Is there any empirical evidence for this? What would happen if the learning rate is not reset at the beginning of each cycle? \n\nQuestions with respect to dynamical systems point of view: Eq. 4 assumes small value of h. However, for ResNet there is no guarantee that the h would be small (e. g. in Appendix C the values between 0.25 and 1 are used). Would the authors be willing to comment on the importance of the value of h? In figure 1, pooling (strided convolutions) are not depicted between network stages. I have one question w.r.t. feature maps dimensionality changes inside a CNN: how does pooling (or strided convolution) fit into dynamical systems view?\n\nTable 3 and 4. I assume that the training time unit is a minute, I couldn’t find this information in the paper. Is the batch size the same for all models (100 for CIFAR and 32 for STL-10)? I understand that the models with different #Blocks have different capacity, for clarity, would it be possible to add # of parameters to each model? For multilevel method, would it be possible to show intermediate results in Table 3 and 4, e. g. at the end of cycle 1 and 2? I see these results in Figure 6, however, the plots are condensed and it is difficult to see the exact number at the end of each cycle. \n\nThe citation (E, 2017) seems to be wrong, could the authors check it?\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "review",
            "rating": "7: Good paper, accept",
            "review": "This paper interprets deep residual network as a dynamic system, and proposes a novel training algorithm to train it in a constructive way. On three image classification datasets, the proposed algorithm speeds up the training process without sacrificing accuracy. The paper is interesting and easy to follow. \n\nI have several comments:\n1.\tIt would be interesting to see a comparison with Stochastic Depth, which is also able to speed up the training process, and gives better generalization performance. Moreover, is it possible to combine the proposed method with Stochastic Depth to obtain further improved efficiency?\n2.\tThe mollifying networks [1] is related to the proposed method as it also starts with shorter networks, and ends with deeper models. It would be interesting to see a comparison or discussion. \n[1] C Gulcehre, Mollifying Networks, 2016\n3.\tCould you show the curves (on Figure 6 or another plot) for training a short ResNet (same depth as your starting model) and a deep ResNet (same depth as your final model) without using your approach?",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}