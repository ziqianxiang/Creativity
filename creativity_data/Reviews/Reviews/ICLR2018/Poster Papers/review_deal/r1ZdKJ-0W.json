{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper proposes a method to embed graph nodes into a gaussian distribution rather than the standard latent vector embeddings. The reviewers concur that the method is interesting and the paper is well-written especially after the opportunity to update.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "The paper brings some new ideas for learning unsupervised graph node embeddings. The experiments are fine, but not so conclusive.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper proposes to learn Gaussian embeddings for directed attributed graph nodes. Each node is associated to a Gaussian representation (mean and diagonal covariance matrix). The mean and diagonal representations for a node are learned as functions of the node attributes. The algorithm is unsupervised and optimizes a ranking loss: nodes at distance 1 in the graph are closer than nodes at distance 2, etc. Distance between nodes representation is measured via KL divergence. The ranking loss is a square exponential loss proposed in energy based models. In order to limit the complexity, the authors propose the use of a sampling scheme and show the convergence in expectation of this strategy towards the initial loss. Experiments are performed on two tasks: link prediction and node classification. Baselines are unsupervised projection methods and a (supervised) logistic regression. An analysis of the algorithm behavior is then proposed.\nThe paper reads well. Using a ranking loss based on the node distance together with Gaussian embeddings is probably new, even if the novelty is not that big.  The comparisons with unsupervised methods shows that the algorithm learns relevant representations.\nDo you have a motivation for using this specific loss Eq. (1), or is it a simple heuristic choice? Did you try other ranking losses?\nFor the link prediction experiments, it is not indicated how you rank candidate links for the different methods and how you proceed with the logistic. Did you compare with a more complex supervised model than the logistic? Fort the classification tasks, it would be interesting to compare to supervised/ semi-supervised embedding methods. The performance of unsupervised embeddings for graph node classification is usually much lower than supervised/ semi-supervised methods.  Having a measure of the performance gap on the different tasks would be informative. Concerning the analysis of uncertainity, discovering that uncertainty is higher for nodes with neighbors of distinct classes is interesting. In your setting this might simply be caused by the difference in the node attributes. I was not so convinced by the conclusions on the dimensionality of the hidden representation space. An immediate conclusion of this experiment would be that only a small dimensional latent space is needed. Did you experiment with this?\nDetailed comments:\nThe title of the paper is “Deep …”. There is nothing Deep in the proposed model since the NN are simple one layer MLPs. This is not a criticism, but the title should be changed.\nThere is a typo in KL definition (d should be replaced by the dimension of the embeddings). Probably another typo: the energy should be + D_KL and not –D_KL. The paragraph below eq (1) should be modified accordingly.\nAll the figures are too small to see anything and should be enlarged.\nOverall the paper brings some new ideas. The experiments are fine, but not so conclusive.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This work proposed a method to embed a network with node attributes into multi-dimensional Gaussian distributions that captures uncertainties and is capable of generalizing to unseen nodes without further training. The model adopts a neural network to transform node attributes into parameters for embedding distributions. ",
            "rating": "7: Good paper, accept",
            "review": "This paper is well-written and easy follow. I didn't find serious concern and therefore suggest an acceptance.\n\nPros\nMethodology\n1. inductive ability: can generalize to unseen nodes without any further training\n2. personalized ranking: the model uses natural ranking that embeddings of closer nodes (considers node pairs of any distance) should be closer in the embedding space, which is more general than prevailing first and second order proximity\n3. sampling strategy: the proposed node-anchored sampling method gives unbiased estimates of loss function and successfully reduces the time complexity\n\nExperiment\n1. Evaluation tasks including link prediction and node classification are conducted across multiple datasets with additional parameter sensitivity and missing-link robustness experiments\n2. Compared with various baselines with diverse model designs such as GCN and node2vec as well as compared with naive baseline (using original node attributes as model inputs)\n3. Demonstrated the model captures uncertainties and the learned uncertainties can be used to infer latent dimensions\nRelated Works\nThe survey of related work is sufficiently wide and complete.\n\nCons\nAuthors should include which kind of model is used to do the link prediction task given embedding vectors from different models as inputs.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking",
            "rating": "7: Good paper, accept",
            "review": "This paper proposes Graph2Gauss (G2G), a node embedding method that embeds nodes in attributed graphs (can work w/o attributes as well) into Gaussian distributions rather than conventionally latent vectors. By doing so, G2G can reflect the uncertainty of a node's embedding. The authors then use these Gaussian distributions and neighborhood ranking constraints to obtain the final node embeddings. Experiments on link prediction and node classification showed improved performance over several strong embedding methods. Overall, the paper is well-written and the contributions are remarkable. The reason I am giving a less possible rating is that some statements are questionable and can severely affect the conclusions claimed in this paper, which therefore requires the authors' detailed response. I am certainly willing to change my rating if the authors clarify my questions.\n\nMajor concern 1: Is the latent vector dimension L really the same for G2G and other compared methods? \nIn the first paragraph of Section 4, it is stated that \"in all experiments if the competing techniques use an embedding of\ndimensionality L, G2G’s embedding is actually only half of this dimensionality so that the overall number of ’parameters’ per node (mean vector + variance terms) matches L.\"  This setting can be wrong since the degree of freedom of a L-dim Gaussian distribution should be L+L(L-1)/2, where the first term corresponds to the mean and the second term corresponds to the covariance. If I understand it correctly, when any compared embedding method used an L-dim vector, the authors used the dimension of L/2. But this setting is wrong if one wants the overall number of ’parameters’ per node (mean vector + variance terms) matches L, as stated by the authors. Fixing L, the equivalent dimension L_G2G for G2G should be set such that L_G2G +L_G2G (L_G2G -1)/2=L, not 2*L_G2G=L.  Since this setting is universal to the follow-up analysis and may severely degrade the performance of GSG due to less embedding dimensions, I hope the authors can clarify this point.\n\nMajor concern 2: The claim on inductive learning\nInductive learning is one of the major contributions claimed in this paper. The authors claim G2G can learn an embedding of an unseen node solely based on their attributes. However, is it not clear why this can be done. In the learning stage of Sec. 3.3, the attributes do not seem to play a role in the energy function. Also, since no algorithm descriptions are available, it's not clear how using only an unseen node's attributes can yield a good embedding under G2G work (so does Sec. 4.5). \nMoreover, how does it compare to directly using raw user attributes for these tasks?\n\nMinor concern/suggestions: The \"similarity\" measure in section 3.1 using KL divergence should be better rephased by \"dissimilarity\" measure. Otherwise, one has a similarity measure $Delta$ and wants it to increase as the hop distance k decreases (closer nodes are more similar). But the ranking constraints are somewhat counter-intuitive because you want $Delta$ to be small if nodes are closer. There is nothing wrong with the ranking condition, but rather an inconsistency between the use of \"similarity\" measure for KL divergence. \n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}