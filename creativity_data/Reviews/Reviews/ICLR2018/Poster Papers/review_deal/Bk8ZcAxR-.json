{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper on automatic option discovery connects recent research on successor representations with eigenoptions. This is a solidly presented, conceptual paper with results in tabular and atari environments. ",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "review",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "review": "- This paper shows an equivalence between proto value functions and successor representations. It then derives the idea of eigen options from the successor representation as a mechanism for option discovery. The paper shows that even under a random policy, the eigen options can lead to purposeful options\n\n- I think this is an important conceptual paper. Automatic option discovery from raw sensors is perhaps one of the biggest open problems in RL research. This paper offers a new conceptual setup to look at the problem and consolidates different views (successor repr, proto values, eigen decomposition) in a principled manner. \n\n- I would be keen to see eigen options being used inside of the agent. Have authors performed any experiments ? \n\n- How robust are the eigen options for the Atari experiments? Basically how hand picked were the options? \n\n- Is it possible to compute eigenoptions online? This seems crucial for scaling up this approach",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting preliminary extension of eigenoptions to stochastic domains with learned non-linear features",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper extends the idea of eigenoptions, recently proposed by Machado et al. to domains with stochastic transitions and where state features are learned. An eigenoption is defined as an optimal policy for a reward function defined by an eigenvector of the matrix of successor representation (SR), which is an occupancy measure induced here by a uniform policy. In high-dimensional state space, the authors propose to approximate that matrix with a convolutional neural network (CNN). The approach is evaluated in a tabular domain (i.e., rooms) and Atari games.\n\nOverall the paper is well-written and quite clear. The proposed ideas for the extension seem natural (i.e., use of SR and CNN). The theorem stated in the paper seems to provide an interesting link between SR and the Laplacian. However, a few points are not clear to me:\n- Is the result new or not? If I understand correctly, Stachenfeld et al. discussed this result, but didn't prove it. Is that correct? So the provided proof is new?\n- Besides, how are D and W exactly defined? \n- Finally, as the matrix is not symmetric, do real eigenvalues always exist?\n\nThe execution of the proposed ideas in the experiments was a bit disappointing to me. The approximated eigenoption was simply computed as a one-step greedy policy. Besides, the eigenoptions seem to help for exploration (as a uniform policy was used) as indicated by plot 3(d), but could they help for other tasks (e.g., learn to play Atari games faster or better)? I think that would be a more useful measure for the learned eigenoptions.\n\nDuring learning SR and the features, what would be the impact if the gradient for SR estimation were also propagated?\n\nIn Figure 4, the trajectories generated by the different eigenoptions are barely visible.\n\nSome typos:\n- Section 2.1:\nin the definition of G_t, the expectation is taken over p as well\nI_w and T_w should be a subset of S\n\n- in (2), the hat is missing over \\Psi\nin the definition of v_\\pi(s), r only depends on s'? This seems inconsistent with the previous definition of \\psi\n\n- p. 6:\nin the definition of L_{SR}(s, s'), why \\psi takes \\phi(s) as argument?\n\n- in conclusion:\nthat that",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good paper connecting previous works from the literature to propose an algorithm for automatic option discovery. ",
            "rating": "7: Good paper, accept",
            "review": "Eigenoption Discovery Through the Deep Successor Representation\n\nThe paper is a follow up on previous work by Machado et al. (2017) showing how proto-value functions (PVFs) can be used to define options called “eigenoptions”. In essence, Machado et al. (2017) showed that, in the tabular case, if you interpret the difference between PVFs as pseudo-rewards you end up with useful options. They also showed how to extend this idea to the linear case: one replaces the Laplacian normally used to build PVFs with a matrix formed by sampling differences phi(s') - phi(s), where phi are features. The authors of the current submission extend the approach above in two ways: they show how to deal with stochastic dynamics and how to replace a linear model with a nonlinear one. Interestingly, the way they do so is through the successor representation (SR). Stachenfeld et al. (2014) have showed that PVFs can be obtained as a linear transformation of the eigenvectors of the matrix formed by stacking all SRs of an MDP. Thus, if we have the SR matrix we can replace the Laplacian mentioned above. This provides benefits already in the tabular case, since SRs naturally extend to domains with stochastic dynamics. On top of that, one can apply a trick similar to the one used in the linear case --that is,  construct the matrix representing the diffusion model by simply stacking samples of the SRs. Thus, if we can learn the SRs, we can extend the proposed approach to the nonlinear case. The authors propose to do so by having a deep neural network similar to Kulkarni et al. (2016)'s Deep Successor Representation. The main difference is that, instead of using an auto-encoder, they learn features phi(s) such that the next state s' can be recovered from it (they argue that this way psi(s) will retain information about aspects of the environment the agent has control over).\n\nThis is a well-written paper with interesting (and potentially useful) insights. I only have a few comments regarding some aspects of the paper that could perhaps be improved, such as the way eigenoptions are evaluated.\n\nOne question left open by the paper is the strategy used to collect data in order to compute the diffusion model (and thus the options). In order to populate the matrix that will eventually give rise to the PVFs the agent must collect transitions. The way the authors propose to do it is to have the agent follow a random policy. So, in order to have options that lead to more direct, \"purposeful\" behaviour, the agent must first wander around in a random, purposeless, way, and hope that this will lead to a reasonable exploration of the state space. \n\nThis problem is not specific to the proposed approach, though: in fact, any method to build options will have to resolve the same issue. One related point that is perhaps more specific to this particular work is the strategy used to evaluate the options built: the diffusion time, or the expected number of steps between any two states of an MDP when following a random walk. First, although this metric makes intuitive sense, it is unclear to me how much it reflects control performance, which is what we ultimately care about. Perhaps more important, measuring performance using the same policy used to build the options (the random policy) seems somewhat unsatisfactory to me. To see why, suppose that the options were constructed based on data collected by a non-random policy that only visits a subspace of the state space. In this case it seems likely that the decrease in the diffusion time would not be as apparent as in the experiments of the paper. Conversely, if the diffusion time were measured under another policy, it also seems likely that options built with a random policy would not perform so well (assuming that the state space is reasonably large to make an exhaustive exploration infeasible). More generally, we want options built under a given policy to reduce the diffusion time of other policies (preferably ones that lead to good control performance).\n\nAnother point associated with the evaluation of the proposed approach is the method used to qualitatively assess options in the Atari experiments described in Section 4.2. In the last paragraph of page 7 the authors mention that eigenoptions are more effective in reducing the diffusion time than “random options” built based on randomly selected sub-goals. However, looking at Figure 4, the terminal states of the eigenoptions look a bit like randomly-selected  sub-goals. This is especially true when we note that only a subset of the options are shown: given enough random options, it should be possible to select a subset of them that are reasonably spread across the state space as well. \n\nInterestingly, one aspect of the proposed approach that seems to indeed be an improvement over random options is made visible by a strategy used by the authors to circumvent computational constraints. As explained in the second paragraph of page 8, instead of learning policies to maximize the pseudo-rewards associated with eigenoptions the authors used a myopic policy that only looks one step ahead (which is the same as having a policy learned with a discount factor of zero). The fact that these myopic policies are able to navigate to specific locations and stay there suggests that the proposed approach gives rise to dense pseudo-rewards that are very informative. As a comparison, when we define a random sub-goal the resulting reward is a very sparse signal that would almost certainly not give rise to useful myopic policies. Therefore, one could argue that the proposed approach not only generate useful options, it also gives rise to dense pseudo-rewards that make it easier to build the policies associated with them.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}