{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "the proposed approach nicely incorporates various ideas from recent work into a single meta-learning (or domain adaptation or incremental learning or ...) framework. although better empirical comparison to existing (however recent they are) approaches would have made it stronger, the reviewers all found this submission to be worth publication, with which i agree.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Very interesting use of episodic memory, could be even stronger",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "This article introduces a new method to improve neural network performances on tasks ranging from continual learning (non-stationary target distribution, appearance of new classes, adaptation to new tasks, etc) to better handling of class imbalance, via a hybrid architecture between nearest neighbours and neural net.\nAfter an introduction summarizing their goal, the authors introduce their Model-based parameter adaptation: this hybrid architecture enriches classical deep architectures with a non-parametric “episodic” memory, which is filled at training time with (possibly learned) encodings of training examples and then polled at inference time to refine the neural network parameters with a few steps of gradient in a direction determined by the closest neighbours in memory to the input being processed.  The authors justify this inference-time SGD update with three different interpretations: one linked in Maximum A Posteriori optimization, another to Elastic Weight Regularisation (the current state of the art in continual learning), and one generalising attention mechanisms (although to be honest that later was more elusive to this reviewer). The mandatory literature review on the abundant recent uses of memory in neural networks is then followed by experiments on continual learning tasks involving permuted MNIST tasks, ImageNET incremental inclusion of classes, ImageNet unbalanced, and two language modeling tasks. \n\nThis is an overall very interesting idea, which has the merit of being rather simple in its execution and can be combined with many other methods: it is fully compatible with any optimiser (e.g. ADAM) and can be tacked on top of EWC (which the authors do). The justification is clear, the examples reasonably thorough. It is a very solid paper, which this reviewer believes to be of real interest to the ICLR community.\n\n\nThe following important clarifications from the authors could make it even better:\n*  Algorithm 1 in its current form seems to imply an infinite memory, which the experiments make clear is not the case. Therefore: how does the algorithm decide what entries to discard when the memory fills up?\n* In most non-trivial settings, the parameter $gamma$ of the encoding is learned, and therefore older entries in the memory lose any ability to be compared to more recent encodings. How do the authors handle this obsolescence of the memory, other than the trivial scheme of relying on KNN to only match recent entries?\n* Because gamma needs to be “recent”, this means “theta” is also recent: could the authors give a good intuition on how the two sets of parameters can evolve at different enough timescales to really make the episodic memory relevant? Is it anything else than relying on the fact that the lower levels of a neural net converge before the upper levels?\n* Table 1:  could the authors explain why the pre-trained Parametric (and then Mixture) models have the best  AUC in the low-data regime, whereas MbPA was designed very much to be superior in such regimes?\n* Paragraph below equation (5), page 3: why not including the regularisation term, whereas the authors just went to great pain to explain it? Rationale? Not including it is also akin to using an improper non-information prior on theta^x independent of theta, which is quite a strong choice to be made “by default”.\n* The extra complexity of choosing the learning rate alpha_M and the number of  MpAB steps is worrying this reviewer somewhat. In practice, in Section 4.1the authors explain using grid search to tune the parameters. Is this reviewer correct in understanding that this search is done across all tasks, as opposed to only the first task? And if so, doesn’t this grid search introduce an information leak by bringing information from the whole pre-determined set of task, therefore undermining the very “continuous learning” aim? How do the algorithm performs if the grid search is done only on the first task?\n* Figure 3:  the text could clarify that the accuracy is measured across all tasks seen so far. It would be interesting to add a figure (in the Appendix) showing the evolution of the accuracy *per task*, not just the aggregated accuracy. \n* In the related works linking neural networks to encoded episodic memory, the authors might want to include the stream of research on HMAX of Anselmi et al 2014 (https://arxiv.org/pdf/1311.4158.pdf) , Leibo et al 2015 (https://arxiv.org/abs/1512.08457), and Blundell et al 2016 (https://arxiv.org/pdf/1606.04460.pdf ).\n\nMinor typos:\n* Figure 4: the title of the key says “New/Old” but then the lines read, in order, “Old” then “New” -- it would be nicer to have them in the same order.\n* Section 5: missing period between \"ephemeral gradient modifications\" and \"Further\".\n* Section 4.2, parenthesis should be \"perform well across all 1000 classes\", not \"all 100 classes\".\n \nWith the above clarifications, this article could become a very remarked contribution.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "In general, I found this paper interesting but it needs improvement in writing and more clarity in the contribution. There are quite similarities (e.g. memory architecture and model components) with previous works which authors need to be more clear about their contributions. ",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper proposes a non-parametric episodic memory that can be used for the rapid acquisition of new knowledge while preserving the old ones. More specially, it locally adapts the parameters of a network using the episodic memory structure. \n\nStrength:\n+   The paper works on a relevant and interesting problem.\n+   The experiment sections are very thorough and I like the fact that the authors selected different tasks to compare their models with. \n+ The paper is well-written except for sections 2 and 3. \nWeakness and Questions:\n-    Even though the paper addresses the interesting and challenging problem of slow adaption when distribution shifts, their episodic memory is quite similar (if not same as) to the Pritzel et al., 2017. \n- In addition, as the author mentioned in the text, their model is also similar to the Kirkpatrick et al., 2017,  Finn et al., 2017, Krause et al., 2017. That would be great if the author can list \"explicitly\" the contribution of the paper with comparing with those. Right now, the text mentioned some of the similarity but it spreads across different sections and parts. \n- The proposed model does adaption during the test time, but other papers such as Li & Hoiem, 2016 handles the shift across domain in the train time. Can authors say sth about the motivation behind adaptation during test time vs. training time? \n- There are some inconsistencies in the text about the parameters and formulations:\n      -- what is second subscript in {v_i}_i? (page 2, 3rd paragraph)\n      -- in Equation 4, what is the difference between x_c and x?\n      -- What happened to $x$ in Eq 5?\n      -- The \"−\" in Eq. 7 doesn't make sense. \n- Section 2.2, after equation 7, the text is not that clear.\n- Paper is well beyond the 8-page limit and should be fitted to be 8 pages.\n- In order to make the experiments reproducible, the paper needs to contain full details (in the appendix) about the setup and hyperparameters of the experiments.   \n\nOthers:\nDo the authors plan to release the codes?\n\n\n------------------------------------\n------------------------------------\nUpdate after rebuttal:\nThanks for the revised version and answering my concerns.  \nIn the revised version, the writing has been improved and the contribution of the paper is more obvious. \nGiven the authors' responses and the changes, I have increased my review score.\n\nA couple of comments and questions:\n1. Can you explain how/why $x_c$ is replaced by $h_k$ in eq_7? \n2. In the same equation (7), how $\\log p(v_k| h_k,\\theta_x, x)$ will be calculated? I have some intuition but not sure.  Can you please explain?\n3. in equation(8), what happened to $x$ in log p(..)?\n4. How figure 2 is plotted? based on a real experiment? if yes, what was the setting? if not, how?\n5. It'd be very useful to the community if the authors decide to release their codes. \n\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The authors propose a memory-based mechanism to adapt model parameters with local context lookup. ",
            "rating": "6: Marginally above acceptance threshold",
            "review": "Overall, the idea of this paper is simple but interesting. Via weighted mean NLL over retrieved neighbors, one can update parameters of output network for a given query input. The MAP interpretation provides a flexible Bayesian explanation about this MbPA.\n\nThe paper is written well, and the proposed method is evaluated on a number of relevant applications (e.g., continuing learning, incremental learning, unbalanced data, and domain shifts.)\n\nHere are some comments:\n1 MbPA is built upon memory. How large should it be? Is it efficient to retrieve neighbors for a given query?\n2 For each test, how many steps of MbPA do we need in general? Furthermore, it is a bit unfair for me to retrain deep model, based on test inputs. It seems that, you are implicitly using test data to fit model.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}