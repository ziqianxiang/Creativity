{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper extends last year's paper on PATE to large-scale, real-world datasets.  The model works by training multiple \"teacher\" models -- one per dataset, where a dataset might be for example, one user's data -- and then distilling those models into a student model. The teachers are all trained on disjoint data. Differential privacy is guaranteed by aggregating the teacher responses with added noise.  The paper shows improved teacher consensus by adding more concentrated noise and allowing the teacher to simply not respond to a student query.  The new results beat the old results convincingly on a variety of measures.\n\nQuality and Clarity: The reviewers and I thought the paper was well written.\n\nOriginality: In some sense this work is incremental, extending and improving the existing PATE framework.  However, the extensions and new analysis are non-trivial and the results are good.\n\nPROS:\n1. Well written though difficult in places for somebody like myself who is not involved in this area.\n2. Much improved scalability to real datasets\n3. Good theoretical analysis supporting the extensions.\n4. Comparison to related work (with a new comparison to UCI medical datasets used in the original paper and better results)\n\nCONS:\n1. Perhaps a little dense for the non-expert\n\n\n",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "This work investigates scalable applications of PATE ",
            "rating": "6: Marginally above acceptance threshold",
            "review": "Summary:\nIn this work, PATE, an approach for learning with privacy,  is modified to scale its application to real-world data sets. This is done by leveraging the synergy between privacy and utility, to make better use of the privacy budget spent when transferring knowledge from teachers to the student. Two aggregation mechanisms are introduced for this reason.  It is demonstrated that sampling from a Gaussian distribution (instead from a Laplacian distribution) facilitates the aggregation of teacher votes in tasks with large number of output classes. \n\non the positive side:\n\nHaving scalable models is important, especially models that can be applied to data with privacy concerns. The extension of an approach for learning with privacy to make it scalable is of merit. The paper is well written, and the idea of the model is clear. \n\n\non the negative side:\n\nIn the introduction, the authors introduce the problem by the importance of privacy issues in medical and health care data. This is for sure an important topic. However, in the following paper, the model is applied no neither medical nor healthcare data. The authors mention that the original model PATE was applied to medical record and census data with the UCI diabetes and adult data set. I personally would prefer to see the proposed model applied to this kind of data sets as well. \n\nminor comments: \n\nFigure 2, legend needs to be outside the Figure, in the current Figure a lot is covered by the legend",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novel techniques to improve private learning with PATE",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper proposes novel techniques for private learning with PATE framework. Two key ideas in the paper include the use of Gaussian noise for the aggregation mechanism in PATE instead of Laplace noise and selective answering strategy by teacher ensemble. In the experiments, the efficacy of the proposed techniques has been demonstrated. I am not familiar with privacy learning but it is interesting to see that more concentrated distribution (Gaussian) and clever aggregators provide better utility-privacy tradeoff. \n\n1. As for noise distribution, I am wondering if the variance of the distribution also plays a role to keep good utility-privacy trade-off. It would be great to discuss and show experimental results for utility-privacy tradeoff with different variances of Laplace and Gaussian noise.\n\n2. It would be great to have an intuitive explanation about differential privacy and selective aggregation mechanisms with examples. \n\n3. It would be great if there is an explanation about the privacy cost for selective aggregation. Intuitively, if teacher ensemble does not answer, it seems that it would reveal the fact that teachers do not agree, and thus spend some privacy cost.\n\n\n\n\n\n\n\n\n\n",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "Clarification needed for data-dependent privacy guarantee.",
            "rating": "7: Good paper, accept",
            "review": "This paper considers the problem of private learning and uses the PATE framework to achieve differential privacy. The dataset is partitioned and multiple learning algorithms produce so-called teacher classifiers. The labels produced by the teachers are aggregated in a differentially private manner and the aggregated labels are then used to train a student classifier, which forms the final output. The novelty of this work is a refined aggregation process, which is improved in three ways:\na) Gaussian instead of Laplace noise is used to achieve differential privacy.\nb) Queries to the aggregator are \"filtered\" so that the limited privacy budget is only expended on queries where the teachers are confident and the student is uncertain or wrong.\nc) A data-dependent privacy analysis is used to attain sharper bounds on the privacy loss with each query.\n\nI think this is a nice modular framework form private learning, with significant refinements relative to previous work that make the algorithm more practical. On this basis, I think the paper should be accepted. However, I think some clarification is needed with regard to item c above:\n\nTheorem 2 gives a data-dependent privacy guarantee. That is, if there is one label backed by a clear majority of teachers, then the privacy loss (as measured by Renyi divergence) is low. This data-dependent privacy guarantee is likely to be much tighter than the data-independent guarantee.\nHowever, since the privacy guarantee now depends on the data, it is itself sensitive information. How is this issue resolved? If the final privacy guarantee is data-dependent, then this is very different to the way differential privacy is usually applied. This would resemble the \"privacy odometer\" setting of Rogers-Roth-Ullman-Vadhan [ https://arxiv.org/abs/1605.08294 ]. \nAnother way to resolve this would be to have an output-dependent privacy guarantee. That is, the privacy guarantee would depend only on public information, rather than the private data. The widely-used \"sparse vector\" technique [ http://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf#page=59 ] does this.\nIn any case, this is an important issue that needs to be clarified, as it is not clear to me how this is resolved.\n\nThe algorithm in this work is similar to the so-called median mechanism [ https://www.cis.upenn.edu/~aaroth/Papers/onlineprivacy.pdf ] and private multiplicative weights [ http://mrtz.org/papers/HR10mult.pdf ]. These works also involve a \"student\" being trained using sensitive data with queries being answered in a differentially private manner. And, in particular, these works also filter out uninformative queries using the sparse vector technique. It would be helpful to add a comparison.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}