{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. The method compares well to baselines and has good accuracy in 2d and 3d envs. All three reviewers are in favor of acceptance due to the novelty and competitive performance of the approach.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Provides Reasonable Evaluation but would Benefit from Clearer Motivation",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper describes a neural network-based approach to active localization based upon RGB images. The framework employs Bayesian filtering to maintain an estimate of the agent's pose using a convolutional network model for the measurement (perception) function. A convolutional network models the policy that governs the action of the agent. The architecture is trained in an end-to-end manner via reinforcement learning. The architecture is evaluated in 2D and 3D simulated environments of varying complexity and compared favorably to traditional (structured) approaches to passive and active localization.\n\nAs the paper correctly points out, there is large body of work on map-based localization, but relatively little attention has been paid to decision theoretic formulations to localization, whereby the agent's actions are chosen in order to improve localization accuracy. More recent work instead focuses on the higher level objective of navigation, whereby any effort act in an effort to improve localization are secondary to the navigation objective. The idea of incorporating learned representations with a structured Bayesian filtering approach is interesting, but it's utility could be better motivated. What are the practical benefits to learning the measurement and policy model beyond (i) the temptation to apply neural networks to this problem and (ii) the ability to learn these in an end-to-end fashion? That's not to say that there aren't benefits, but rather that they aren't clearly demonstrated here. Further, the paper seems to assume (as noted below) that there is no measurement uncertainty and, with the exception of the 3D evaluations, no process noise.\n\nThe evaluation demonstrates that the proposed method yields estimates that are more accurate according to the proposed metric than the baseline methods, with a significant reduction in computational cost. However, the environments considered are rather small by today's standards and the baseline methods almost 20 years old. Further, the evaluation makes a number of simplifying assumptions, the largest being that the measurements are not subject to noise (the only noise that is present is in the motion for the 3D experiments). This assumption is clearly not valid in practice. Further, it is not clear from the evaluation whether the resulting distribution that is maintained is consistent (e.g., are the estimates over-/under-confident?). This has important implications if the system were to actually be used on a physical system. Further, while the computational requirements at test time are significantly lower than the baselines, the time required for training is likely very large. While this is less of an issue in simulation, it is important for physical deployments. Ideally, the paper would demonstrate performance when transferring a policy trained in simulation to a physical environment (e.g., using diversification, which has proven effective at simulation-to-real transfer).\n\nComments/Questions:\n\n* The nature of the observation space is not clear.\n\n* Recent related work has focused on learning neural policies for navigation, and any localization-specific actions are secondary to the objective of reaching the goal. It would be interesting to discuss how one would balance the advantages of choosing actions that improve localization with those in the context of a higher-level task (or at least including a cost on actions as with the baseline method of Fox et al.).\n\n* The evaluation that assigns different textures to each wall is unrealistic.\n\n* It is not clear why the space over which the belief is maintained flips as the robot turns and shifts as it moves.\n\n* The 3D evaluation states that a 360 deg view is available. What happens when the agent can only see in one (forward) direction?\n\n* AML includes a cost term in the objective. Did the author(s) experiment with setting this cost to zero?\n\n* The 3D environments rely upon a particular belief size (70 x 70) being suitable for all environments. What would happen if the test environment was larger than those encountered in training?\n\n* The comment that the PoseNet and VidLoc methods \"lack a strainghtforward method to utilize past map data to do localization in a new environment\" is unclear.\n\n* The environments that are considered are quite small compared to the domains currently considered for\n\n* Minor: It might be better to move Section 3 into Section 4 after introducing notation (to avoid redundancy).\n* The paper should be proofread for grammatical errors (e.g., \"bayesian\" --> \"Bayesian\", \"gaussian\" --> \"Gaussian\")\n\n\nUPDATES FOLLOWING AUTHORS' RESPONSE\n\n(Apologies if this is a duplicate. I added a comment in light of the authors' response, but don't see it and so I am updating my review for completeness).\n\nI appreciate the authors's response to the initial reviews and thank them for addressing several of my comments.\n\nRE: Consistency\nMy concerns regarding consistency remain. For principled ways of evaluating the consistency of an estimator, see Bar-Shalom \"Estimation with Applications to Tracking and Navigation\".\n\nRE: Measurement/Process Noise\nThe fact that the method assumes perfect measurements and, with the exception of the 3D experiments, no process noise is concerning as neither assumptions are valid for physical systems. Indeed, it is this noise in particular that makes localization (and its variants) challenging.\n\nRE: Motivation\nThe response didn't address my comments about the lack motivation for the proposed method. Is it largely the temptation of applying an end-to-end neural method to a new problem? The paper should be updated to make the advantages over traditional approaches to active localization.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "End-to-end training of two parts of an active localization system",
            "rating": "7: Good paper, accept",
            "review": "This is an interesting paper that builds a parameterized network to select actions for a robot in a simulated environment, with the objective of quickly reaching an internal belief state that is predictive of the true state.  This is an interesting idea and it works much better than I would have expected.  \n\nIn more careful examination it is clear that the authors have done a good job of designing a network that is partly pre-specified and partly free, in a way that makes the learning effective.  In particular\n- the transition model is known and fixed (in the way it is used in the belief update process)\n- the belief state representation is known and fixed (in the way it is used to decide whether the agent should be rewarded)\n- the reward function is known and fixed (as above)\n- the mechanics of belief update\nBut we learn\n- the observation model\n- the control policy\n\nI'm not sure that global localization is still an open problem with known models.  Or, at least, it's not one of our worst.\n\nEarly work by Cassandra, Kurien, et al used POMDP models and solvers for active localization with known transition and observation models.   It was computationally slow but effective.\n\nSimilarly, although the online speed of your learned method is much better than for active Markov localization, the offline training cost is dramatically higher;  it's important to remember to be clear on this point.\n\nIt is not obvious to me that it is sensible to take the cosine similarity between the feature representation of the observation and the feature representation of the state to get the entry in the likelihood map.   It would be good to make it clear this is the right measure.\n\nHow is exploration done during the RL phase?  These domains are still not huge.\n\nPlease explain in more detail what the memory images are doing.\n\nIn general, the experiments seem to be well designed and well carried out, with several interesting extensions.\n\nI have one more major concern:  it is not the job of a localizer to arrive at a belief state with high probability mass on the true state---it is the job of a localizer to have an accurate approximation of the true posterior under the prior and observations.   There are situations (in which, for example, the robot has gotten an unusual string of observations) in which it is correct for the robot to have more probability mass on a \"wrong\" state.  Or, it seems that this model may earn rewards for learning to make its beliefs overconfident.  It would be very interesting to see if you could find an objective that would actually cause the model to learn to compute the appropriate posterior.\n\nIn the end, I have trouble making a recommendation:\nCon:  I'm not convinced that an end-to-end approach to this problem is the best one\nPro: It's actually a nice idea that seems to have worked out well\nCon: I remain concerned that the objective is not the right one\n\nMy rating would really be something like 6.5 if that were possible.\n\n\n\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Convincing paper about an active learning neural version of Bayesian filter localisation",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "I have evaluated this paper for NIPS 2017 and gave it an \"accept\" rating at the time, but the paper was ultimately not accepted. This resubmission has been massively improved and definitely deserves to be published at ICLR.\n\nThis paper formulates the problem localisation on a known map using a belief network as an RL problem. The goal of the agent is to minimise the number of steps to localise itself (the agent needs to move around to accumulate evidence about its position), which corresponds to reducing the entropy of the joint distribution over a discretized grid over theta (4 orientations), x and y. The model is evaluated on a grid world, on textured 3D mazes with simplified motion (Doom environment) and on a photorealistic environment using the Unreal engine. Optimisation is done through A3C RL. Transfer from the crude simulated Doom environment to the photorealistic Unreal environment is achieved.\n\nThe belief network consists of an observation model, a motion prediction model that allows for translations along x or y and 90deg rotation, and an observation correction model that either perceives the depth in front of the agent (a bold and ambiguous choice) and matches it to the 2D map, or perceives the image in front of the agent. The map is part of the observation.\n\nThe algorithm outperforms Bayes filters for localisation in 2D and 3D and the idea of applying RL to minimise the entropy of position estimation is brilliant. Minor note: I am surprised that the cognitive map reference (Gupta et al, 2017) was dropped, as it seemed relevant.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}