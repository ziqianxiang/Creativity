{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The reviewers consider the paper to promising, but raise issues with the increase in the complexity of the MDP caused by the authors' parameterization of the action space, and comparisons with earlier work (Pazis and Lagoudakis).   While the authors cite this work, and say that they that they needed to make changes to PL to make it work in their setting (in addition to adding the deep networks), they do not explicitly show comparisons in the paper to any other discretization schemes.   "
    },
    "Reviews": [
        {
            "title": "review",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper describes a new RL technique for high dimensional action spaces.  It discretizes each dimension of the action space, but to avoid an exponential blowup, it selects the action for each dimension in sequence.  This is an interesting approach.  The paper reformulates the MDP with a high dimensional action space into an equivalent MDP with more time steps (one per dimension) that each selects the action in one dimension.  This makes sense.\n\nWhile I do like very much the model, I am perplex about the training technique.  The lower MDP is precisely the new proposed model with unidimensional actions and therefore it should be sufficient.  However, the paper also describes an upper MDP that seems to be superfluous.  The two MDPs are mathematically equivalent, but their Q-values are obtained differently (TD-0 for the upper MDP and Q-learning for the lower MDP) and yet the paper tries to minimize the Euclidean distance between them.  This is really puzzling since the different training algorithms suggest that the Q-values should be different while minimizing the Euclidean distance between them tries to make them equal.  The paper suggests that divergence occurs without the upper MDP.  This is really suspicious. The approach feels like a band-aid solution to cover a problem that the authors could not identify.  While the empirical results are good, I don't think the paper should be published until the authors figure out a principled way of training.\n\nThe proposed approach reformulates the MDP with high dimensional actions into an equivalent one with uni dimensional actions.  There is a catch.  This approach effectively hides the exponential action space into the state space which becomes exponential.  Since u contains all the actions of the previous dimensions, we are effectively increasing the state space by an exponential factor.  The paper should discuss this and explain what are the consequences in practice.  In the end, the MDP does not become simpler.\n\nOverall, this is an interesting paper with a good idea, but the training technique is not mature enough for publication.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The paper presents a correct, reasonably effective, but not groundbreaking approach to Q value approximation in MDPs with high-dimensional action spaces",
            "rating": "7: Good paper, accept",
            "review": "The paper presents Sequential Deep Q-Networks (SDQNs), which select actions from discretized high-dimensional action spaces.  This is done by introducing another, undiscounted MDP in which each action dimension is chosen sequentially by an agent.  By training a Q network to best choose these action dimensions, and loosely enforcing equality between the original and new MDPs at points where they are equivalent, the new MDP can be successfully navigated, resulting in good action selection for the original MDP.  This is experimentally compared against DDPG in several domains.  There are no theoretical results.\n\nThis work is correct and clearly written.  Experiments do demonstrate improved effectiveness in the chosen domains, and the authors do a nice job of illustrating the range of performance by their approach (which has low variance in some domains, but high variance in others).  Because of the clarity of the paper, the effectiveness of the approach, and the high quality experiments, I encourage acceptance.\n\nIt doesn't strike me as world-changing, however.  The MDP-within-an-MDP approach is quite similar to the Pazis and Lagoudakis MDP decomposition for the same problem (work which is appropriately cited, but maybe too briefly compared against).  In other words, it strikes me as merely being P&L plus networks, dampening my enthusiasm.\n\nMy one question for the authors is how much the order of action dimension selection matters.  This seems probably quite important practically, but is undiscussed.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A useful method to handle continuous action spaces; however comes with additional cost of training as many networks as the number of actions",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Originality\n--------------\nWhen the action space is N-dimensional, computing argmax could be problematic. The paper proposes to address the problem by creating N MDPs with 1-D actions. \n\nClarity\n---------\n1) Explicitly writing down DDPG will be helpful\n2) The number of actions in each of the domains will also be useful\n\nQuality\n----------\n1) The paper reports experimental results on order of actions as well as binning, and the results confirm with what one would expect from intuition. \n2) It will be important to talk about the case when the action dimension N is very large, what happens in that case? Does the proposed method would work in such a scenario? A discussion is needed.\n3) Given that the ordering of actions does not matter, what is the real take away of looking at them as 'sequence' (which has not temporal structure because action order could be arbitrary)?\n\n\nSignificance\n----------------\nWhile the proposed method seems a reasonable approach to handle the argmax problem, it still requires training multiple networks for Q^i (i=1,..N) for Q^L, which is a limitation. Further, since the actions could be arbitrary, it is unclear where 'sequence' approach helps. These limit the understand and hence significance.\n",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        }
    ]
}