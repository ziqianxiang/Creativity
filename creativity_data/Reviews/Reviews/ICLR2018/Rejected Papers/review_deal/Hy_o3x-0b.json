{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper proposes a VAE variant by embedding spatial information with multiple layers of latent variables. Although the paper reports state-of-the-art results on multiple datasets, some results may be due to a bug. This has been discussed, and the author acknowledges the bug. We hope the problem can be fixed, and the paper reconsidered at another venue.\n"
    },
    "Reviews": [
        {
            "title": "Comments on the motivation, originality and experiments",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper combines several recent advances on generative modelling including a ladder variational posterior and a PixelCNN decoder together with the proposed convolutional stochastic layers to boost the NLL results of the current VAEs. The numbers in the tables are good but I have several comments on the motivation, originality and experiments.\n\nMost parts of the paper provide a detailed review of the literature. However, the resulting model is quite like a combination of the existing advances and the main contribution of the paper, i.e. the convolution stochastic layer, is not well discussed. Why should we introduce the convolution stochastic layers? Could the layers encode the spatial information better than a deterministic convolutional layer with the same architecture? What's the exact challenge of training VAEs addressed by the convolution stochastic layer? Please strengthen the motivation and originality of the paper.\n\nThough the results are good, I still wonder what is the exact contribution of the convolutional stochastic layers to the NLL results?  Can the authors provide some results without the ladder variational posterior and the PixelCNN decoder on both the gray-scaled and the natural images?\n\nAccording to the experimental setting in the Section 3 (Page 5 Paragraph 2), \"In case of gray-scaled images the stochastic latent layers are dense with sizes 64, 32, 16, 8, 4 (equivalent to Sønderby et al. (2016)) and for the natural images they are spatial (cf. Table 1). There was no significant difference when using feature maps (as compared to dense layers) for modelling gray-scaled images.\" there is no stochastic convolutional layer.  Then is there anything new in FAME on the gray images? Furthermore, how could FAME advance the previous state-of-the-art? It seems because of other factors instead of the stochastic convolutional layer. \n\nThe results on the natural images are not complete. Please present the generation results on the ImageNet dataset and the reconstruction results on both the CIFAR10 and ImageNet datasets. The quality of the samples on the CIFAR10 dataset seems not competitive to the baseline papers listed in the table. Though the visual quality does not necessarily agree with the NLL results but such large gap is still strange. Besides, why FAME can obtain both good NLL and generation results on the MNIST and OMNIGLOT datasets when there is no stochastic convolutional layer? Meanwhile, why FAME cannot obtain good generation results on the CIFAR10 dataset? Is it because there is a lot randomness in the stochastic convolutional layer? It is better to provide further analysis and it is not safe to say that the stochastic convolutional layer helps learn better latent representations based on only the NNL results.\n\nMinor things:\n\nPlease rewrite the sentence \"When performing reconstructions during training ... while also using the stochastic latent variables z = z 1 , ..., z L.\" in the caption of Figure 1.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review: Impressive experimental results but low novelty",
            "rating": "6: Marginally above acceptance threshold",
            "review": "Update:  In light of Yoon Kim's retraction of replication, I've downgraded my score until the authors provide further validation (i.e. CIFAR and ImageNet samples).\n\nSummary\n\nThis paper proposes VAE modifications that allow for the use multiple layers of latent variables.  The modifications are: (1) a shared en/decoder parametrization as used in the Ladder VAE [1], (2) the latent variable parameters are functions of a CNN, and (3) use of a PixelCNN decoder [2] that is fed both the last layer of stochastic variables and the input image, as done in [3].  Negative log likelihood (NLL) results on CIFAR 10, binarized MNIST (dynamic and static), OMNIGLOT, and ImageNet (32x32) are reported.  Samples are shown for CIFAR 10, MNIST, and OMNIGLOT.        \n\n\nEvaluation\n\nPros:  The paper’s primary contribution is experimental: SOTA results are achieved for nearly every benchmark image dataset (the exception being statically binarized MNIST, which is only .28 nats off).  This experimental feat is quite impressive, and moreover, in the comments on OpenReview, Yoon Kim claims to have replicated the CIFAR result.  I commend the authors for making their code available already via DropBox.  Lastly, I like how the authors isolated the effect of the concatenation via the ‘FAME No Concatenation’ results.                 \n\nCons:  The paper provides little novelty in terms of model or algorithmic design, as using a CNN to parametrize the latent variables is the only model detail unique to this paper.  In terms of experiments, the CIFAR samples look a bit blurry for the reported NLL (as others have mentioned in the OpenReview comments).  I find the authors’ claim that FAME is performing superior global modeling interesting.  Is there a way to support this experimentally?  Also, I would have liked to see results w/o the CNN parametrization; how important was this choice?  \n\n\nConclusion\n\nWhile the paper's conceptual novelty is low, the engineering and experimental work required (to combine the three ideas discussed in the summary and evaluate the model on every benchmark image dataset) is commendable.  I recommend the paper’s acceptance for this reason.\n\n\n[1]  C. Sonderby et al., “Ladder Variational Autoencoders.”  NIPS 2016.\n[2]  A. van den Oord et al., “Conditional Image Generation with PixelCNN Decoders.” ArXiv 2016.\n[3]  I. Gulrajani et al., “PixelVAE: A Latent Variable Model for Natural Images.”  ICLR 2017.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "limited novelty. possibly incorrect?",
            "rating": "3: Clear rejection",
            "review": "The description of the proposed method is very unclear. From the paper it is very difficult to make out exactly what architecture is proposed. I understand that the prior on the z_i in each layer is a pixel-cnn, but what is the posterior? Equations 8 and 9 would suggest it is of the same form (pixel-cnn) but this would be much too slow to sample during training. I'm guessing it is just a factorized Gaussian, with a separate factorized Gaussian pseudo-prior? That is, in figure 1 all solid lines are factorized Gaussians and all dashed lines are pixel-cnns?\n\n* The word \"layers\" is sometimes used to refer to latent variables z, and sometimes to parameterized neural network layers in the encoder and decoder. E.g. \"The top stochastic layer z_L in FAME is a fully-connected dense layer\". No, z_L is a vector of latent variables. Are you saying the encoder produces it using a fully-connected layer?\n* Section 2.2 starts talking about \"deterministic layers h\". Are these part of the encoder or decoder? What is meant by \"number of layers connecting the stochastic latent variables\"?\n* Section 2.3: What is meant by \"reconstruction data\"?\n\nIf my understanding of the method is correct, the novelty is limited. Autoregressive priors were used previously in e.g. the Lossy VAE by Chen et al. and IAF-VAE by Kingma et al. The reported likelihood results are very impressive though, and would be reason for acceptance if correct. However, the quality of the sampled images shown for CIFAR-10 doesn't match the reported likelihood. There are multiple possible reasons for this, but after skimming the code I believe it might be due to a faulty implementation of the variational lower bound. Instead of calculating all quantities in the log domain, the code takes explicit logs and exponents and stabilizes them by adding small quantities \"eps\": this is not guaranteed to give the right result. Please fix this and re-run your experiments. (I.e. in _loss.py don't use x/(exp(y)+eps) but instead use x*exp(-y). Don't use log(var+eps) with var=softplus(x), but instead use var=softplus(x)+eps or parameterize the variance directly in the log domain).",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}