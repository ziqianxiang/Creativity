{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper proposes iterative training strategies for learning teacher and student models. They show how iterative training can lead to interpretable strategies over joint training on multiple datasets. All the reviewers felt the idea was interesting, although, one of the reviewers had concerns about the experimentation.\n\nHowever, there is a BIG problem with this submission. The author names appear in the manuscript thus disregarding anonymity."
    },
    "Reviews": [
        {
            "title": "This is a compelling paper on an interesting topic: the interpretability of learning strategies.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "This is a well written paper on a compelling topic: how to train \"an automated teacher\" to use intuitive strategies  that would also apply to humans. \n\nThe introduction is fairly strong, but this reviewer wishes that the authors would have come up with an intuitive example that illustrates why the strategy \"1) train S on random exs; 2) train T to pick exs for S\" makes sense. Such an example would dramatically improve the paper's readability.\n\nThe paper appears to be original, and the related work section is quite extensive.\n\nA second significant improvement would be to add an in-depth  running example in section 3, so that the authors could illustrate why the BR strategy makes sense (Algorithm 2).",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A great first-step towards interpretable teaching for deep-learning methods",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "The authors define a novel method for creating a pair of models, a student and a teacher model, that are co-trained in a manner such that the teacher provides useful examples to the student to communicate a concept that is interpretable to people. They do this by adapting a technique from computational cognitive science called rational pedagogy. Rather than jointly optimize the student and teacher (as done previously), they have form a coupled relation between the student and teacher where each is providing a best response to the other. The authors demonstrate that their method provides interpretable samples for teaching in commonly used psychological domains and conduct human experiments to argue it can be used to teach people in a better manner than random teaching. \n\nUnderstanding how to make complex models interpretable is an extremely important problem in ML for a number of reasons (e.g., AI ethics, explainable AI). The approach proposed by the authors is an excellent first step in this direction, and they provide a convincing argument for why a previous approach (joint optimization) did not work. It is an interesting approach that builds on computational cognitive science research and the authors provide strong evidence their method creates interpretable examples. They second part of their article, where they test the examples created by their models using behavioral experiments was less convincing. This is because they used the wrong statistical tests for analyzing the studies and it is unclear whether their results would stand with proper tests (I hope they will! â€“ it seems clear that random samples will be harder to learn from eventually, but I also hoped there was a stronger baseline.).\n\nFor analysis, the authors use t-tests directly on KL-divergence and accuracy scores; however, this is inappropriate (see Jaeger, 2008; Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models. Journal of Memory and Language, 59(4), 434-446.). This is especially applicable to the accuracy score results and the authors should reanalyze their data following the paper referenced above. With respect to KL-divergence, a G-test can be used (see https://en.wikipedia.org/wiki/G-test#Relation_to_Kullback.E2.80.93Leibler_divergence). I suspect the results will still be meaningful, but the appropriate analysis is essential to be able to interpret the human results.\n\nAlso, a related article: One article testing rational pedagogy in more ML contexts and using it to train ML models that is\nHo, M. K., Littman, M., MacGlashan, J., Cushman, F., & Austerweil, J. L. (NIPS 2016). Showing versus Doing. Teaching by Demonstration.\n\nFor future work, it would be nice to show that the technique works for finding interpretable examples in more complex deep learning networks, which motivated the current push for explainable AI in the first place.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea, but not convincing enough",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper looks at a specific aspect of the learning-to-teach problem, where the learner is assumed to have a teacher that selects training examples for the student according to a strategy. The teacher's strategy should also be  learned from data.  In this case the authors look at finding interpretable teaching strategies.  The authors define the \"good\" strategies as similar to intuitive strategies (based on human intuition about the structure of the domain) or strategies that are effective for teaching humans.  \nThe suggested method follow an iterative process in which the student and teacher are interchangeably used. At each iteration the teacher generates  examples based on the students current concept. \n\nI found it very difficult to follow the claims in the paper. Why is it assumed that human intuition is necessarily good?  The experiments do not answer these questions, but are designed to show that the suggested approach follows human intuition. There are not enough details to get a good grasp of the suggested method and the different choices for it,  and similarly the experiments are not described in a very convincing way. Specifically - the domains picked seem very contrived,  there actual results are not reported, the size of the data seems minimal so it's not clear what is actually learned.\nHow would you analyze the teaching strategy in realistic cases, where there is no simple intuitive strategy? This would be more convincing.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}