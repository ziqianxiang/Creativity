{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The idea studied here is interesting, if incremental. The empirical results are not particularly stellar, but it's clear that the authors have done their best to provide reproducible and defensible results. A few sticking points: a) The use of the term 'UCB', as mentioned in an anonymous comment, is somewhat misleading. \"Approximate Confidence Interval\" might be less controversial; b) there are a number of recent research results on exploration that are worth paying attention to (Plappert et al, O'Donoghue et al.) and worth comparing to, and c) the theoretical results are not always justified or useful (e.g. Equation 9: the bound is trivial, posterior >= 0 or 1). "
    },
    "Reviews": [
        {
            "title": "A good paper that shows UCB-ensemble outperforms Thompson Sampling ensemble (like bootstrapped DQN) in experiments.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper paper uses an ensemble of networks to represent the uncertainty in deep reinforcement learning.\nThe algorithm then chooses optimistically over the distribution induced by the ensemble.\nThis leads to improved learning / exploration, notably better than the similar approach bootstrapped DQN.\n\nThere are several things to like about this paper:\n- It is a clear paper, with a simple message and experiments that back up the claims.\n- The proposed algorithm is simple and could be practical in a lot of settings and even non-DQN variants.\n- It is interesting that Bootstrapped DQN gets such poor performance, this suggests that it is very important in the original paper https://arxiv.org/abs/1602.04621 that \"ensemble voting\" is applied to the test evaluation... (why do you think this is by the way, do you think it has something to do with the data being *more* off-policy / diverse under a TS vs UCB scheme?)\n\nOn the other hand:\n- The novelty/scope of this work is somewhat limited... this is more likely (valuable) incremental work than a game-changer.\n- Something feels wrong/hacky/incomplete about just doing \"ensemble\" for uncertainty without bootstrapping/randomization... if we had access to more powerful optimization techniques then this certainly wouldn't be sensible - I think that you should mention that you are heavily reliant on \"random initialization + SGD/Adam + specific network architecture\" to maintain this idea of uncertainty. For example, this wouldn't work for linear value functions!\n- I think the original bootstrapped DQN used \"ensemble voting\" at test time, so maybe you should change the labels or the way this is introduced/discussed. It's definitely very interesting that *essentially* the learning benefit is coming from ensembling (rather than \"raw\" bootstrapped DQN) and UCB still looks like it does better.\n- I'm not convinced that page 4 and the \"Bayesian\" derivation really add too much value to this paper... alternatively, maybe you could introduce the actual algorithm first (train K models in parallel) and then say \"this is similar to particle filter\" and add the mathematical derivation after, rather than as if it was some complex formula derived. If you want to reference some justification/theory for ensemble-based uncertainty approximation you might consider https://arxiv.org/pdf/1705.07347.pdf instead.\n- I think this paper might miss the point of the \"bigger\" problem of efficient exploration in RL... or even how to get \"deep\" exploration with deep RL. Yes this algorithm sees improvements across Atari, but it's not clear why/if this is a step change versus simply increasing the amount of replay or tuning the learning rate.  (Actually I do believe this algorithm can demonstrate deep exploration... but it looks like we're not seeing the big improvements on the \"sub-human\" games you might hope.)\n\nOverall I do think this is a pretty good short paper/evaluation of UCB-ensembles on Atari.\nThe scope/insight of the paper isn't groundbreaking, but I think it delivers a clear short message on the Atari benchmark.\nPerhaps this will encourage people to dig deeper into some of these issues... I vote accept.\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper shows improvement over baselines. But does not seem to offer significant insight or dramatic improvement.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper introduces a number of different techniques for improving exploration in deep Q learning. The main technique is to use UCB (upper confidence bound) to speedup exploration. The authors also introduces \"Ensemble voting\" facilitate exploitation.\n\nThis paper shows improvement over baselines. But does not seem to offer significant insight or dramatic improvement. The techniques introduced are a small permutation of previous results. The baselines are not particularly strong either.\n\nThe paper appeared to have be rushed. The presentation is not always clear.\n\nI also have the following questions I hope the authors could help me with:\n\n1. I failed to understand how Eqn (5). Could you please clarify.\n\n2. What is the significance of the math introduced in section 3? All that was proposed was: (1) Majority voting, (2) UCB exploration.\n\n3. Why comparing to A3C+ which is not necessarily better than A3C in final performance?\n\n4. Why not comparing to Bootstrapped DQN since the proposed method is based on it?\n\n5. Why is the proposed method better than Bootstrapped DQN, since UCB does not necessarily outperform Thompson sampling in the case of bandits?\n\n6. If there is a section on INFOGAIN exploration, why not mention it in the main text?",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "New exploration method for DeepRL with some good results.",
            "rating": "7: Good paper, accept",
            "review": "The authors propose a new exploration algorithm for Deep RL. They maintain an ensemble of Q-values (based on different initialisations) to model uncertainty over Q. The ensemble is then used to derive a confidence interval at each step, which is used to select actions UCB-style.\n\nThere is some attempt at a Bayesian interpretation for the Bellman update. But to me it feels a bit like shoehorning the probabilistic interpretation into an already existing update - I’m not sure this is justified and necessary here. Moreover, the UCB strategy is generally not considered a Bayesian strategy, so I wasn’t convinced by the link to Bayesian RL in this paper.\n\nI liked the actual proposed method otherwise, and the experimental results on Atari seem good (but see also latest SOTA Atari results, for example the Rainbow paper). Some questions about the results:\n-How does it perform compared to epsilon-greedy added on top of Alg1, or is there evidence that this produces any meaningful exploration versus noise? \n-How does the distribution of Q values look like during different phases of learning?\n-Was epsilon-greedy used in addition to UCB exploration? Question for both Alg 1 and Alg 2.\n-What’s different between Alg 1 and bootstrapped DQN (other than the action selection)?\n\nMinor things:\n-Missing propto in Eq 7?\n-Maybe mention that the leftarrows are not hard updates. Maybe you already do somewhere…\n-it looks more a Bellman residual update as written in (11).\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}