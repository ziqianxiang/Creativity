{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper identifies an interesting problem in sigmoid deep nets, addressed diffferently by batchnorm, and proposes a different simple fix. It shows empirically that constraining neuron's weights to sum to zero improves training of a 100 layers sigmoid MLP.\nThe work is currenlty limited in its theoretical contribution, and regarding the showcased practical interest of the method compared to batchnorm (it's not appplicable to RELUs and shows positive effect on optimization but not generalization).\n "
    },
    "Reviews": [
        {
            "title": "This paper raises the concept of \"angle bias\" and introduces the so-called LCW method to reduce angle bias. The paper implies underlying connections between angle bias and the gradient vanishing problem and suggests that LCW is a cure for both issues. Although vanishing gradients in deep networks is an interesting topic for the community, the manuscript does not bring any novel theoretical understanding and there is also not enough empirical evidence to backup the claims made in the paper. ",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Pros:\nThe paper is easy to read. Logic flows naturally within the paper.\n\nCons:\n\n1. Experimental results are neither enough nor convincing. \n\nOnly one set of data is used throughout the paper: the Cifar10 dataset, and the architecture used is only a 100 layered MLP. Even though LCW performs better than others in this circumstance, it does not prove its effectiveness in general or its elimination of the gradient vanishing problem. For the 100 layer MLP, it's very hard to train a simple MLP and the training/testing accuracy is very low for all the methods. More experiments with different number of layers and different architecture like ResNet should be tried to show better results. \n\nIn Figure (7), LCW seems to avoid gradient vanishing but introduces gradient exploding problem.\n\nThe proposed concept is only analyzed in MLP with Sigmoid activation function. In the experimental parts, the authors claim they use both ReLU and Sigmoid function, but no comparisons are reflected in the figures. \n\n2. The whole standpoint of the paper is quite vague and not very convincing.\nIn section 2, the authors introduce angle bias and suggest its effect in MLPs that with random weights, showing that different samples may result in similar output in the second and deeper layers. However, the connection between angle bias and the issue of gradient vanishing lacks a clear analytical connection. The whole analysis of the connection is built solely on this one sentence \"At the same time, the output does not change if we adjust the weight vectors in Layer 1\", which is nowhere verified. \n\nFurther, the phenomenon is only tested on random initialization. When the network is trained for several iterations and becomes more settled, it is not clear how \"angle affect\" affects gradient vanishing problem.\n\n\nMinors:\n1. Theorem 1,2,3 are direct conclusions from the definitions and are mis-stated as Theorems.\n\n2. 'patters' -> 'patterns'\n\n3. In section 2.3, reasons 1 and 2 state the similar thing that output of MLP has relatively small change with different input data when angle bias occurs. Only reason 1 mentions the gradient vanishing problem, even though the title of this section is \"Relation to Vanishing Gradient Problem\". \n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "more disadvantages vs few advatages as of now.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The authors introduce the concept of angle bias (angle between a weight vector w and input vector x)  by which the resultant pre-activation (wx) is biased if ||x|| is non-zero or ||w|| is non-zero (theorm 2 from the article). The angle bias results in almost constant activation independent of input sample resulting in no weight updates for error reduction.   Authors chose to add an additional optimization constraint LCW (|w|=0) to achieve zero-mean pre-activation while, as mentioned in the article, other methods like batch normalization BN tend to push for |x|=0 and unit std to do the same. \n\nClearly, because of lack of scaling factor incase of LCW, like that in BN, it doesnot perform well when used with ReLU. When using with sigmoid the activation being bouded (0,1) seems to compensate for the lack of scaling in input. While BN explicitly makes the activation zero-mean LCW seems to achieve it through constraint on the weight features. Though it is shown to be computationally less expensive LCW seems to work in only specific cases unlike BN.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Studies an interesting phenomenon but with preliminary results",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper studies the impact of angle bias on learning deep neural networks, where angle bias is defined to be the expected value of the inner product of a random vectors (e.g., an activation vector) and a given vector (e.g., a weight vector).  The angle bias is non-zero as long as the random vector is non-zero in expectation and the given vector is non-zero.  This suggests that the some of the units in a deep neural network have large values (either positive or negative) regardless of the input, which in turn suggests vanishing gradient.  The proposed solution to angle bias is to place a linear constraint such that the sum of the weight becomes zero.  Although this does not rule out angle bias in general, it does so for the very special case where the expected value of the random vector is a vector consisting of a common value.  Nevertheless, numerical experiments suggest that the proposed approach can effectively reduce angle bias and improves the accuracy for training data in the CIFAR-10 task.  Test accuracy is not improved, however.\n\nOverall, this paper introduces an interesting phenomenon that is worth studying to gain insights into how to train deep neural networks, but the results are rather preliminary both on theory and experiments.\n\nOn the theoretical side, the linearly constrained weights are only shown to work for a very special case.  There can be many other approaches to mitigate the impact of angle bias.  For example, how about scaling each variable in a way that the mean becomes zero, instead of scaling it into [-1,+1] as is done in the experiments?  When the mean of input is zero, there is no angle bias in the first layer.  Also, what about if we include the bias term so that b + w a is the preactivation value?\n\nOn the experimental side, it has been shown that linearly constrained weights can mitigate the impact of angle bias on vanishing gradient and can reduce the training error, but the test error is unfortunately increased for the particular task with the particular dataset in the experiments.  It would be desirable to identify specific tasks and datasets for which the proposed approach outperforms baselines.  It is intuitively expected that the proposed approach has some merit in some domains, but it is unclear exactly when and where it is.\n\nMinor comments:\n\nIn Section 2.2, is Layer 1 the input layer or the next?",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}