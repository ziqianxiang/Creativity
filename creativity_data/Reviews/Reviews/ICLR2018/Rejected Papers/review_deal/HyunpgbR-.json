{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The reviewers feel there are two issues that make this paper fall short of acceptance: first, the\nlack of a clear emphasis and focus (evidenced by the significant revisions) and second, a lack of\ncomparison to similar, existing methods for multi-agent reinforcement learning."
    },
    "Reviews": [
        {
            "title": "Interesting method, missing related work and baselines, very limited experiments. Unclear if this is really a 'multi-agent' paper",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper suggests an interesting algorithmic innovation, consisting of hierarchical latent variables for coordinated exploration in multi-agent settings. \n\nMain concern: This work heavily relies on the multi-agent aspect for novelty : \n\"Houthooft et al. (2016) learned exploration policies via information gain using variational methods. However, these only consider 1 agent\".  However, in the current form of the paper this is a questionable claim. As the problems investigated combine fully observable states, purely cooperative payouts and global latent variables, they reduce to single agent problems with a large action space. Effectively the 'different agents' are nothing but a parameterized action space of a central controller. \nUsing hierarchical latent variables for large action spaces is like a good idea, but placing the work into multi-agent seems like a red herring. \n\nGiven that this is a centralized controller, it would be really helpful to compare quantitatively to other approaches for structured exploration, eg [3] and [4].\n\nDetailed comments:\n-\"we restrict to fully cooperative MDPs that are fully observable, deterministic and episodic.\" Since the rewards are also positive, a very relevant baseline (from a MARL point of view) is distributed Q-learning [1].\n-Figure 3: Showing total cumulative terminal rewards is difficult to interpret. I would be interested in seeing standard 'training curves' which show the average return per episode after a given amount of training episodes. Currently it is difficult to judge whether training has converged on not.\n-Related work is missing a lot of relevant research. Apart from references below, please see [2] for a relevant, if dated, overview.\n-\"Table 1: Total terminal reward (averaged over 5 best runs)\" - how does the mean and median compare across methods for all runs rather than just the top 5? \n\n\nReferences:\n[1] Lauer, M., Riedmiller, M.: An algorithm for distributed reinforcement learning in cooperative\nmulti-agent systems. In: Proceedings 17th International Conference on Machine Learning\n(ICML-00), pp. 535–542. Stanford University, US (2000)\n[2] L. Bus¸oniu, R. Babuska, and B. De Schutter: Multi-Agent Reinforcement Learning: An Overview\n[3] Nicolas Usunier, Gabriel Synnaeve, Zeming Lin, Soumith Chintala: Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks\n[4] Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, Marcin Andrychowicz: Parameter Space Noise for Exploration\n\n\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Paper presents interesting and potentially novel method.",
            "rating": "7: Good paper, accept",
            "review": "The paper proposes a method to coordinate agent behaviour  by using policies that have a shared latent structure. The authors derive a variational policy optimisation method to optimise the coordinated policies. The approach is investigated empirically on 2 predator prey type games.\n\nThe method presented in the paper seems quite novel. The authors present a derivation of their variational, hierarchical  update. Not all steps in this derivation are equally well explained, especially the introduction of the variational posterior could be more detailed. The appendix also offers very little extra information compared to the main text, most paragraphs concerning the derivations are identical. The comparison to existing approaches using variational inference is quite brief. It would be nice to have a more detailed explanation of the novel steps in this approach.\n\n It also seems that by assuming a shared model, shared global state and a fully cooperative problem, the authors remove many of the complexities of a multi-agent system. This also brings the derivations closer to the single agent case.\n\nA related potential criticism is the feasibility of using this approach in a multi-agent system. The authors are essentially creating a (partially) centralised learner. The cooperative rewards and shared structure assumptions structures mentioned above seem limiting in a multi-agent system. Even giving each agent local state observations is known to potentially create coordination problems. The predator prey games where agents with agents physically distributed over the environment are probably not the best motivational examples.\n\nOther remarks: \n\nEmpirical result show a clear advantage for this method over the baselines. The evaluation domains are relatively simple, but it was nice to see that the authors also make an attempt to investigate the qualitative behaviour of their method.\n\nThe overview of related work was relatively brief and focused mostly on recent deep MARL approaches. There is a very large body on coordination in multi-agent RL. It would be nice to situate the research somewhat better within this field (or at least refer to an overview such as Busoniu et al, 2010).\n\nIt seems like a completely factorised approach (i.e. independent agents) would make a nice baseline for the experiments, in addition to the shared architecture approaches.\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Seems interesting, but with significant weaknesses",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper proposes an approach to improve exploration in multiagent reinforcement learning by allowing the policies of the individual agents to be conditioned on an external coordination signal \\lambda. In order to find such parametrized policies, the approach combines deep RL with a variational inference approach (ELBO optimization). The paper presents an empirical evaluation, which seems encouraging, but that is also somewhat difficult to interpret given the lack of comparison to other state-of-the-art methods.\n\nOverall, the paper seems interesting, but (in addition to the not completely convincing empirical evaluation), it has two main weaknesses: lack of clarity and grounding in related literature.\n\n=Issues with clarity=\n\n-\"This problem has two equivalent solutions\". \nThis is not so clear; depending on the movement of the preys it might well be that the optimal solution will switch to the other prey in certain cases?\n\n-It is not clear what is really meant with the term \"structured exploration\". It just seems to mean 'improved'?\n\n-It is not clear that the improvements are due to exploration; my feeling is that is is due to improved statistical strength on a more abstract state feature (which is learned), not unlike:\nGeramifard, Alborz, et al. \"Online discovery of feature dependencies.\" Proceedings of the 28th International Conference on Machine Learning (ICML-11). 2011.\nHowever, there is no clear indication that there is an improved exploration policy.\n\n-The problem setting is not quite clear:\nThe paper first introduces \"multi-agent RL\", which seems to correspond to a \"stochastic game\" (also \"Markov game\"), but then moves on to restrict to the \"fully cooperative setting\" (which would make it a \"Multiagent MDP\", Boutilier '96).\n\nIt subsequently says it deals only with deterministic problems (which would reduce the problem further to a learning version of a multiagent classical planning problem), but in the experiments do consider stochastically moving preys.\n\n-The paper says the problem is fully observable, but fails to make explicit if this is *individually* fully observable, or jointly. I am assuming the former, but is it not clear how the agents observe this full state in the experimental evaluation.\n\nThis is actually a crucial confusion, as it completely changes the interpretation of what the approach does: in the individually observable case, the approach is adding a redundant source of information which is more abstract and thus seems to facilitate faster learning. In the latter case, where agents would have individual observations, it is actually providing the agents with more information.\n\nAs such, I would really encourage the authors to better define the task they are considering. E.g., by building on the taxonomies of problems that researchers have developed in the community focusing on decentralized POMDPs, such as:\nGoldman, Claudia V., and Shlomo Zilberstein. \"Decentralized control of cooperative systems: Categorization and complexity analysis.\" (2004).\n\n-\"Compared to the single-agent RL setting, multi-agent RL poses unique difficulties. A central issue\nis the exploration-exploitation trade-off\"\nThat now in particular happens to be a central issue in single agent RL too.\n\n-\"Finding the true posteriors P (λ t |s t ) ∝ P (s t |λ t )P (λ t ) is intractable in general\"\nThe paper did not explain how this inference task is required to solve the RL problem.\n\n-In general, I found the technical description impossible to follow, even after carefully looking at the appending. For instance, (also) there the term P (λ |s ) is suddenly introduced without explaining what the term exactly is? Why is the term P(a|λ) not popping up here? That also needs to be optimized, right? I suppose \\phi is the parameter vector of the variational approximation, but it is never really stated. The various shorthand notations introduced for clarity do not help at all, but only make the formulas very cryptic.\n\n-The main text is not readable since definitions, e.g., L(Q_r,\\tehta,\\phi), that are in the appendix are now missing.\n\n-It is not clear to me how the second term of (10) is now estimated?\n\n-\"Shared (shared actor-critic): agents share a deterministic hidden layer,\"\nWhat kind of layer is this exactly? How does it relate to \\lambda ?\n\n-\"The key difference is that this model does not sample from the shared hidden layer\"\nWhy would sampling help? Given that we are dealing with a fully observable multiagent MDP, there is no inherent need to randomize at all? (there should be a optimal deterministic joint policy?)\n\n-\"There is shared information between the agents\"\nWhat information is referred to exactly? \nAlso: It is not quite clear if for these domains cloned would be better than completely independent learners (without shared weights)?\n\n-I can't seem to find anywhere what is the actual shape (or type? I am assuming a vector of reals) of the used \\lambda.\n\n-in figure 5, rhs, what is being shown exactly? What do the colors mean? Why does there seem to be a \\lambda *per* agent now?\n\n\n\n=Related work=\n\nI think the paper could/should be hugely improved in this respect. \n\nThe idea of casting MARL as inference has also been considered by:\n\nLearning for Decentralized Control of Multiagent Systems in Large, Partially-Observable Stochastic Environments.\nM Liu, C Amato, EP Anesta, JD Griffith, JP How - AAAI, 2016\n\nStick-breaking policy learning in Dec-POMDPs\nM Liu, C Amato, X Liao, L Carin, JP How\nInternational Joint Conference on Artificial Intelligence (IJCAI) 2015\n\nWu, F.; Zilberstein, S.; and Jennings, N. R. 2013. Monte-carlo\nexpectation maximization for decentralized POMDPs. In Proc.\nof the 23rd Int’l Joint Conf. on Artificial Intelligence (IJCAI-\n13).\n\nI do not think that these explicitly make use of a mechanism to coordinate the policies, since they address to true Dec-POMDP setting where each agent only gets its own observations, but in the Dec-POMDP literature, there also is the notion of 'correlation device', which is an additional controller (say corresponding to a dummy agent), which of which the states can be observed by other agents and used to condition their actions on:\n\nBernstein DS, Hansen EA, Zilberstein S. Bounded policy iteration for decentralized POMDPs. InProceedings of the nineteenth international joint conference on artificial intelligence (IJCAI) 2005 Jun 6 (pp. 52-57).\n\n(and clearly this could be directly included in the aforementioned learning approaches). \n\n\nThis notion of a correlation device also highlights to potential relation to methods to learn/compute correlated equilibria. E.g.,:\n\nGreenwald A, Hall K, Serrano R. Correlated Q-learning. In ICML 2003 Aug 21 (Vol. 3, pp. 242-249).\n\n\nA different connection between MARL and inference can be found in:\n\nZhang, Xinhua and Aberdeen, Douglas and Vishwanathan, S. V. N., \"Conditional Random Fields for Multi-agent Reinforcement Learning\", in (New York, NY, USA: ACM, 2007), pp. 1143--1150.\n\n\nThe idea of doing something hierarchical of course makes sense, but also here there are a number of related papers:\n\n-putting \"hierarchical multiagent\" in google scholar finds works by Ghavamzadeh et al., Saira & Mahadevan, etc.\n\n-Victor Lesser has pursued coordination for better exploration with a number of students.\n\nI suppose that Guestrin et al.'s classical paper:\nGuestrin, Carlos, Michail Lagoudakis, and Ronald Parr. \"Coordinated reinforcement learning.\" ICML. Vol. 2. 2002.\nwould deserve a citation, and the MARL field is moving ahead fast, an explanation of the differences with COMA:\nCounterfactual Multi-Agent Policy Gradients\nJ Foerster, G Farquhar, T Afouras, N Nardelli, S Whiteson\nAAAI 2018\nis probably also warranted.\n\n\n\n\n\n\n\n\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}