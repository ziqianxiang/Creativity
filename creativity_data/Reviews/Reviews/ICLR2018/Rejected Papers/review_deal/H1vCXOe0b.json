{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper proposes a new method for interpreting the hidden units of neural networks by employing an Indian Buffet Process. The reviewers felt that the approach was interesting, but at times hard to follow and more analysis was needed. In particular, it was difficult to glean any advantage of this method over others. The authors did not provide a response to the reviews."
    },
    "Reviews": [
        {
            "title": "Hard to follow some parts; more analysis needed.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Pros\n- The paper proposes a novel formulation of the problem of finding hidden units\n  that are crucial in making a neural network come up with a certain output.\n- The method seems to be work well in terms of isolating a few hidden units that\n  need to be kept while preserving classification accuracy.\n\nCons\n- Sections 3.1 and 3.2 are hard to understand. There seem to be inconsistencies\n  in the notation. For example,\n(1) It would help to clarify whether y^b_n is the prediction score or its\ntransformation into [0, 1]. The usage is inconsistent.\n(2) It is not clear how \"y^b_n can be expressed as \\sum_{k=1}^K z_{nk}f_k(x_n)\"\nin general. This is only true for the penultimate layer, and when y^b_n denotes\nthe input to the output non-linearity. However, this analysis seems to be\napplied for any hidden layer and y^b_n is the output of the non-linearity unit\n(\"The new prediction scores are transformed into a scalar ranging from 0 to 1,\ndenoted as y^b_n.\")\n(3) Section 3.1 denotes the DNN classifier as F(.), but section 3.2 denotes the\nsame classifier as f(.).\n(4) Why is r_n called the \"center\" ? I could not understand in what sense is\nthis the center, and of what ? It seems that the max value has been subtracted\nfrom all the logits into a softmax (which is a fairly standard operation).\n\n- The analysis seems to be about finding neurons that contribute evidence for\n  a particular class. This does not address the issue of understanding why the\nnetwork makes a certain prediction for a particular input. Therefore this\napproach will be of limited use.\n\n- The paper should include more analysis of how this method helps interpret the\n  actions of the neural net, once the core units have been identified.\nCurrently, the focus seems to be on demonstrating that the classifier\nperformance is maintained as a significant fraction of hidden units are masked.\nHowever, there is not enough analysis on showing whether and how the identified\nhidden units help \"interpret\" the model.\n\nQuality\nThe idea explored in the paper is interesting and the experiments are described\nin enough detail. However, the writing still needs to be polished.\n\nClarity\nThe problem formulation and objective function (Section 3.1) was hard to follow.\n\nOriginality\nThis approach to finding important hidden units is novel.\n\nSignificance\nThe paper addresses an important problem of trying to have more interpretable\nneural networks. However, it only identifies hidden units that are important for\na class, not what are important for any particular input.  Moreover, the main\nthesis of the paper is to describe a method that helps interpret neural network\nclassifiers. However, the experiments only focus on identifying important hidden\nunits and fall short of actually providing an interpretation using these hidden\nunits.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper proposes to identify the core units of a deep neural network in a one-vs-remaining manner. It is unclear what is the use of identifying these core units.",
            "rating": "3: Clear rejection",
            "review": "The paper intends to interpret a well-trained multi-class classification deep neural network by discovering the core units of one or multiple hidden layers for prediction making. However, these discovered core units are specific to a particular class, which are retained to maintain the deep neural network’s ability to separate that particular class from the other ones. Thus, these non-core units for a particular class could be core units for separating another class from the remaining ones. Consequently, the aggregation of all class-specific core units could include all hidden units of a layer. Therefore, it is hard for me to understand what’s the motivation to identify the core units in a one-vs-remaining manner. At this moment, these identified class-specific core units are useful for neither reducing the size of the network, nor accelerating computation. ",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "review",
            "rating": "3: Clear rejection",
            "review": "The paper develops a technique to understand what nodes in a neural network are important\nfor prediction. The approach they develop consists of using an Indian Buffet Process \nto model a binary activation matrix with number of rows equal to the number of examples. \nThe binary variables are estimated by taking a relaxed version of the \nasymptotic MAP objective for this problem. One question from the use of the \nIndian Buffet Process: how do the asymptotics of the feature allocation determine \nthe number of hidden units selected? \n\nOverall, the results didn't warrant the complexity of the method. The results are neat, but \nI couldn't tell why this approach was better than others.\n\nLastly, can you intuitively explain the additivity assumption in the distribution for p(y')",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}