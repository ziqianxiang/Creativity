{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper considers graph neural representations that use Cayley polynomials of the graph Laplacian as generators. These polynomials offer better frequency localization than Chebyshev polynomials. The authors illustrate the advantages of Cayleynets on several benchmarks, producing modest improvements.\n\nReviewers were mixed in the assessment of this work, highlighting on the one hand the good quality of the presentation and the theoretical background, but on the other hand skeptical about the experimental section significance. In particular, some concerns were centered about the analysis of complexity of Cayley versus the existing alternatives.\n\nOverall, the AC believes this paper is perhaps more suited to an audience more savvy in signal processing than ICLR, which may fail to appreciate the contributions. "
    },
    "Reviews": [
        {
            "title": "We find this work is interesting, timely, and of good quality to be presented in ICLR",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "This paper is on construction graph CNN using spectral techniques. The originality of this work is the use of Cayley polynomials to compute spectral filters on graphs, related to the work of Defferrard et al. (2016) and Monto et al. (2017) where Chebyshev filters were used. Theoretical and experimental results show the relevance of the Cayley polynomials as filters for graph CNN.\n\nThe paper is well written, and connections to related works are highlighted. We recommend the authors to talk about some future work.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting new filter for graph CNNs, although experiments are not fully convincing",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper proposes a new filter for spectral analysis on graphs for graph CNNs. The filter is a rational function based on the Cayley transform. Unlike other popular variants, it is not strictly supported on a small graph neighborhood, but the paper proves an exponential-decay property on the norm of a filtered vertex indicator function.\n\nThe paper argues that Cayley filters allow better spectral localization than Chebyshev filters. While Chebyshev filters can be applied efficiently using a recursive method, evaluation of a  Cayley filter of order r requires solving r linear system in dimension corresponding to the number of vertices, which is expensive. The paper proposes to stop after a small number of iterations of Jacobi's method to alleviate this problem.\n\nThe paper is clear and well written.\n\nThe proposed method seems of interest, although I find the experimental section only partly convincing. \n\nThere seems to be a tradeoff here. The paper demonstrates that CayleyNet achieves similar efficiency as ChebNet in multiple experiments while using smaller filter orders. Although using smaller filter orders (and better-localized filters) is an interesting property, it is not necessarily a key objective, especially as this seems to come at the cost of a significantly increased computational complexity. \n\nThe paper could help us understand this tradeoff better. For instance:\n- Middle and right panels of Figure 4 could use a more precise Y scale. How much slower is CayleyNet here with respect to the ChebNet?\n\n- Figure 4 mentions time corresponds to \"test times on batches of 100 samples\". Is this an average value over multiple 100-sample batches? What is the standard deviation? How do the training times compare?\n\n- MNIST accuracies are very similar (and near perfect) -- how did the training and testing time compare? Same for the MovieLens experiment. The improvement in performance is rather small, what is the corresponding computational cost?\n\n- CORA results are a bit confusing to me. The filter orders used here are very small, and the best amongst the values considered seems to be r=1. Is there a reason only such small values have been considered? Is this a fair evaluation of ChebNet which may possibly perform better with larger filter orders?\n\n- The paper could provide some insights as to why ChebNet is unable to work with unnormalized Laplacians while CayleyNet is (and why the ChebNet performance seems to get worse and worse as r increases?).\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Propose new filters based on Cayley transform -- interesting filter, but unconvincing theory / experiments",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Summary: This paper proposes a new graph-convolution architecture, based on Cayley transform of the matrix. Succinctly, if L denotes the Laplacian of a graph, this filter corresponds to an operator that is a low degree polynomial of C(L) = (hL - i)/(hL+i), where h is a scalar and i denotes sqrt(-1). The authors contend that such filters are interesting because they can 'zoom' into a part of the spectrum, depending on the choice of h, and that C(L) is always a rotation matrix with eigenvalues with magnitude 1. The authors propose to compute them using Jacobi iteration (using the diagonal as a preconditioner), and present experimental results.\n\nOpinion: Though the Cayley filters seem to have interesting properties,  I find the authors theoretical and experimental justification insufficient to conclude that they offer sufficient advantage over existing methods. I list my major criticisms below:\n1. The comparison to Chebyshev filters  (small degree polynomials in the Chebyshev basis) at several places is unconvincing. The results on CORA (Fig 5a) compare filters with the same order, though Cayley filters have twice the number of variables for the same order as Chebyshev filters. Similarly for Fig 1, order 3 Cayley should be compared to Order 6 Chebyshev (roughly).\n\n2. Since Chebyshev polynomials blow up exponentially when applied to values larger than 1, applying Chebyshev filters to unnormalized Laplacians (Fig 5b) is an unfair comparison.\n\n3. The authors basically apply Jacobi iteration (gradient descent using a diagonal preconditioner) to estimate the Cayley filters, and contend that a constant number of iterations of Jacobi are sufficient. This ignores the fact that their convergence rate scales quadratically in h and the max-degree of the graph. Moreover, this means that the Filter is effectively a low degree polynomial in (D^(-1)A)^K, where A is the adjacency matrix of the graph, and K is the number of Jacobi iterations. It's unclear how (or why) a choice of K might be good, or why does it make sense to throw away all powers of D^(-1)Af, even though we're computing all of them.\nAlso, note that this means a K-fold increase in the runtime for each evaluation of the network, compared to the Chebyshev filter.\n\nAmong the other experimental results, the synthetic results do clearly convey a significant advantage at least over Chebyshev filters with the same number of parameters. The CORA results (table 2) do convey a small but clear advantage. The MNIST result seems a tie, and the comparison for MovieLens doesn't make it obvious that the number of parameters is the same. \n\nOverall, this leads me to conclude that the paper presents insufficient justification to conclude that Cayley filters offer a significant advantage over existing work.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}