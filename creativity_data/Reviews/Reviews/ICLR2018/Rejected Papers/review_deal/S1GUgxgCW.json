{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper combines existing models to detect topics and generate responses, and the resulting model is shown to be slightly preferred by human evaluators over baselines. This is quite incremental and the results are not impressive enough to stand on their own merit."
    },
    "Reviews": [
        {
            "title": "interesting combination of seq2seq and neural topic models, but weak evaluation",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper proposes a conversational model with topical information, by combining seq2seq model with neural topic models. The experiments and human evaluation show the model outperform some the baseline model seq2seq and the other latent variable model variant of seq2seq.\n\nThe paper is interesting, but it also has certain limitations:\n\n1) To my understanding, it is a straightforward combination of seq2seq and one of the neural topic models without any justification.\n2) The evaluation doesn't show how the topic information could influence word generation. No of the metrics in table 2 could be used to justify the effect of topical information.\n3) There is no analysis about the model behavior, therefore there is no way we could get a sense about how the model actually works. One possible analysis is to investigate the values $l_t$ and the corresponding words, which to some extent will tell us how the topical information be used in generation. In addition, it could be even better if there are some analysis about topics extracted by this model.\n\nThis paper also doesn't pay much attention to the existing work on topic-driven conversational modeling. For example \"Topic Aware Neural Response Generation\" from Xing et al., 2017.\n\nSome additional issues:\n\n1) In the second line under equation 4, y_{t-1} -> y_{t}\n2) In the first paragraph of section 3, two \"MLP\"'s are confusing\n3) In the first paragraph of page 6, words with \"highest inverse document frequency\" are used as stop words?",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "topic modeling + seq2seq",
            "rating": "6: Marginally above acceptance threshold",
            "review": "I enjoyed this paper a lot. The paper addresses the issue of enduring topicality in conversation models. The model proposed here is basically a mash-up between a neural topic model and a seq2seq-based dialog system. The exposition is relatively clear and a reader with sufficient background in ML should have no following the model. My only concern about the paper is that is very incremental in nature -- the authors combine two separate models into a relatively straight-forward way. The results do are good and validate the approach, but the paper has little to offer beyond that.  ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The evaluation and details of experiments are not sufficient.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper proposed the combination of topic model and seq2seq conversational model.\nThe idea of this combination is not surprising but the attendee of ICLR might be interested in the empirical results if the model clearly outperforms the existing method in the experimental results.\nHowever, I'm not sure that the empirical evaluation shows the really impressive results.\nIn particular, the difference between LV-S2S and LTCM seem to be trivial.\nThere are many configurations in the LSTM-based model.\nCan you say that there is no configuration of LV-S2S that outperforms your model?\nMoreover, the details of human evaluation are not clear, e.g., the number of users and the meaning of each rating.\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}