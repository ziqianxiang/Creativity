{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The authors make an argument for constructing an MDP from the formal structures of temporal logic and associated finite state automata and then applying RL to learn a policy for the MDP. This does not provide a solution for low-level skill composition, because there are discontinuities between states, but does provide a means for high level skill composition.\n\nThe reviewers agreed that the paper suffered from sloppy writing and unclear methods. They had concerns about correctness, and were not impressed by the novelty (combining TL and RL has been done previously). These concerns tip this paper to rejection."
    },
    "Reviews": [
        {
            "title": "Makes a connection between LTL task representation and RL subtasks, providing some capability for composing skills.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper argues for structured task representations (in TLTL) and shows how these representations can be used to reuse learned subtasks to decrease learning time.\n\nOverall, the paper is sloppily put together, so it's a little difficult to assess the completeness of the ideas. The problem being solved is not literally the problem of decreasing the amount of data needed to learn tasks, but a reformulation of the problem that makes it unnecessary to relearn subtasks. That's a good idea, but problem reformulation is always hard to justify without returning to a higher level of abstraction to justify that there's a deeper problem that remains unchanged. The paper doesn't do a great job of making that connection.\n\nThe idea of using task decomposition to create intrinsic rewards seems really interesting, but does not appear to be explored in any depth. Are there theorems to be had? Is there a connection to subtasks rewards in earlier HRL papers?\n\nThe lack of completeness (definitions of tasks and robustness) also makes the paper less impactful than it could be.\n\nDetailed comments:\n\n\"learn hierarchical policies\" -> \"learns hierarchical policies\"?\n\n\"n games Mnih et al. (2015)Silver et al. (2016),\": The citations are a mess. Please proof read.\n\n\"and is hardly reusable\" -> \"and are hardly reusable\".\n\n\"Skill composition is the idea of constructing new skills with existing skills (\" -> \"Skill composition is the idea of constructing \nnew skills out of existing skills (\".\n\n\"to synthesis\" -> \"to synthesize\".\n\n\"set of skills are\" -> \"set of skills is\".\n\n\"automatons\" -> \"automata\".\n\n\"with low-level controllers can\" -> \"with low-level controllers that can\".\n\n\"the options policy π o is followed until β(s) > threshold\": I don't think that's how options were originally defined... beta is generally defined as a termination probability.\n\n\"The translation from TLTL formula FSA to\" -> \"The translation from TLTL formula to FSA\"?\n\n\"four automaton states Qφ = {q0, qf , trap}\": Is it three or four?\n\n\"learn a policy that satisfy\" -> \"learn a policy that satisfies\".\n\n\"HRL, We introduce the FSA augmented MDP\" -> \"HRL, we introduce the FSA augmented MDP.\".\n\n\" multiple options policy separately\" -> \" multiple options policies separately\"?\n\n\"Given flat policies πφ1 and πφ2 that satisfies \" -> \"Given flat policies πφ1 and πφ2 that satisfy \".\n\n\"s illustrated in Figure 3 .\" -> \"s illustrated in Figure 2 .\"?\n\n\", we cam simply\" -> \", we can simply\".\n\n\"Figure 4 <newline> .\" -> \"Figure 4.\".\n\n\", disagreement emerge\" -> \", disagreements emerge\"?\n\nThe paper needs to include SOME definition of robustness, even if it just informal. As it stands, it's not even clear if larger \nvalues are better or worse. (It would seem that *more* robustness is better than less, but the text says that lower values are \nchosen.)\n\n\"with 2 hidden layers each of 64 relu\": Missing word? Or maybe a comma?\n\n\"to aligns with\" -> \"to align with\".\n\n\" a set of quadratic distance function\" -> \" a set of quadratic distance functions\".\n\n\"satisfies task the specification)\" -> \"satisfies the task specification)\".\n\nFigure 4: Tasks 6 and 7 should be defined in the text someplace.\n\n\"current frame work i\" -> \"current framework i\".\n\n\" and choose to follow\" -> \" and chooses to follow\".\n\n\" this makes\" -> \" making\".\n\n\"each subpolicies\" -> \"each subpolicy\".\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good objective but I don't think the primary result is correct",
            "rating": "3: Clear rejection",
            "review": "I very much appreciate the objectives of this paper:  learning compositional structures is critical for scaling and transfer.  \n\nThe first part of the paper offers a strategy for constructing a product MDP out of an original MDP and the automaton associated with an LTL formula, and reminds us that we can learn within that restricted MDP.  Some previous work is cited, but I would point the authors to much older work of Parr and Russell on HAMs (hierarchies of abstract machines) and later work by Andre and Russell, which did something very similar (though, indeed, not in hybrid domains).  The idea of extracting policies corresponding to individual automaton states and making them into options seems novel, but it would be important to argue that those options are likely to be useful again under some task distribution. \n\nThe second part offers an exciting result:  If we learn policy pi_1 to satisfy objective phi_1 and policy pi_2 to satisfy objective phi_2, then it will be possible to switch between pi_1 and pi_2 in a way that satisfies phi_1 ^ phi_2.   This just doesn't make sense to me.  What if phi_1 is o ((A v B) Until C) and phi_2 is o ((not A v B) Until C).   Let's assume that o(B Until C) is satisfiable, so the conjunction is satisfiable.  However, we may find policy pi_1 that makes A true and B false (in general, there is no single optimal policy) and find pi_2 that makes A false and B false, and it will not be possible to satisfy the phi_1 and phi_2 by switching between the policies.    But, perhaps I am misunderstanding something.\n\nSome other smaller points:\n- \"zero-shot skill composition\" sounds a lot like what used to be called \"planning\" or \"reasoning\"\n- The function rho is originally defined on whole trajectories but in eq 7 it is only on a single s':  I'm not sure exactly what that means.\n- Section 4:  How is \"as soon as possible\" encoded in this objective?\n- How does the fixed horizon interact with conjoining goals?\n- There are many small errors in syntax;  it would be best to have this paper carefully proofread.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper presents a method to connect truncated linear temporal logic formulas to reinforcement learning policies, but several relevant details are not sufficiently clear.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper proposes to join temporal logic with hierarchical reinforcement learning to simplify skill composition.  The combination of temporal logic formulas with reinforcement learning was developed previously in the literature, and the main contribution of this paper is for fast skill composition.  The system uses logic formulas in truncated linear temporal logic (TLTL), which lacks an Always operator and where the LTL formula (A until B) also means that B must eventually hold true. The temporal truncation also requires the use of a specialized MDP formulation with an explicit and fixed time horizon T.  The exact relationship between the logical formulas and the stochastic trajectories of the MDP is not described in detail here, but relies on a robustness metric, rho.  The main contributions of the paper are to provide a method that converts a TLTL formula that specifies a task into a reward function for a new augmented MDP (that can be used by a conventional RL algorithm to yield a policy), and a method for quickly combining two such formulas (and their policies) into a new policy.  The proposed method is evaluated on a small Markov chain and a simulated Baxter robot.\n\nThe main problem with this paper is that the connections between the TLTL formulas and the conventional RL objectives are not made sufficiently clear.  The robustness term rho is essential, but it is not defined.  I was also confused by the notation $D_\\phi^q$, which was described but not defined.  The method for quickly combining known skills (the zero-shot skill composition in the title) is switching between the two policies based on rho.  The fact that there may be many policies which satisfy a particular reward function (or TLTL formula) is ignored.  This means that skill composition that is proposed in this paper might be quite far from the best policy that could be learned directly from a single conjunctive TLTL formula. It is unclear how this approach manages tradeoffs between objectives that are specified as a conjunction of TLTL goals. is it better to have a small probability of fulfilling all goals, or to prefer a high probability of fulfilling half the goals?  In short the learning objectives of the proposed composition algorithm are unclear after translation from TLTL formulas to rewards.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}