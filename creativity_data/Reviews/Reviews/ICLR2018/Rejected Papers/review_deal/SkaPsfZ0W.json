{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper proposes a multiscale variant of Graph Convolutional Networks (GCN) , obtained by combining separate GCN modules using powers of normalized adjacency as generators. The model is tested on several node classification semi-supervised tasks obtaining excellent numerical performance.\n\nReviewers acknowledged the good empirical performance of the model, but all raised the issue of limited novelty, relative to the growing body of literature on graph neural networks. In particular, they missed an analysis that compares random walks powers to other multiscale approaches and justifies its performance in the context of semi-supervised learning. Overall, the AC believes this is a good paper, but it can be significantly stronger with an extra iteration that addresses these limitations. "
    },
    "Reviews": [
        {
            "title": "Interesting approach, but lacks justification",
            "rating": "5: Marginally below acceptance threshold",
            "review": "In this work a new network of GCNs is proposed. Different GCNs utilize different powers of the transition matrix to capture varying neighborhoods in a graph. As an aggregation mechanism of the GCN modules two approaches are considered: a fully connected layer on top of stacked features and attention mechanism that uses a scalar weight per GCN. The later allows for better interpretability of the effects of varying degree of neighborhoods in a graph.\n\nProposed approach, as authors noted themselves, is quite similar to DCNN (Atwood and Towsley, 2016) and becomes equivalent if the combined GCNs have one layer each. While comparison to vanilla GCN is quite extensive, there is no comparison to DCNN at all. I would be curious to see at least portion of the experiments of the DCNN paper with the proposed approach, where the importance of number of GCN layers is addressed. DCNN did well on Cora and Pubmed when more training samples were used. It also was tested on graph classification datasets, but the results were not as good for some of the datasets. I think that comparison to DCNN is important to justify the importance of using multilayer GCN modules.\n\nSome questions and concerns:\n- I could not quite figure out how many layers did each GCN have in the experiments and how impactful is this parameter \n- Why is it necessary to replicate GCNs for each of the transition matrix powers? In section 4.3 it is mentioned that replication factors r = 1 and r = 4 were used, but it is not clear from Table 2 what are the results for respective r.\n- Early stopping implementation seems a bit too intense. \"We invoke many runs over all datasets\" - how many? Mean and standard deviation are reported for top 3 performers, which is not enough to get a sense of standard deviation and mean. Kipf and Welling (2017) report results over 100 runs without selecting top performers if I understood correctly their setup. Could you please report mean and standard deviation of all the runs? Given relatively small performance improvement (comparatively to GCN), more than 3 (selected) runs are needed for comparison.\n- I liked the attention idea and its interpretation in Fig. 2. Could you please add the error bars for the attention weights. It is interesting to see them shifting towards higher powers of the transition matrix, but also it is important to know if this phenomena is statistically significant.\n- Following up on the previous item - did you try not including self connections when computing transition matrix powers? This way the effect of different degrees of neighborhoods in a graph could be understood better. When self-connections are present, each subsequent transition matrix power contains neighborhoods of lower degrees and interpretation becomes not as apparent.\n\nMinor comments:\n- Understanding of this paper quite heavily relies on the reader knowing Kipf and Welling (2017) paper. Particularly, the comment about approximations derived by Kipf and Welling (2017) in Section 3.3 and how directed graph was converted to undirected (Section 4.1) require a bit more details.\n- I am not quite sure why Section 2.3 is needed. Connection to graph embeddings is not given much attention in the paper later on (except t-SNE picture).\n- Typo in Fig. 1 caption - right and left are mixed up.\n- Typo in footnote on page 3.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "possibly interesting ideas",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper proposes a novel graph convolutional network in which a variety of random walk steps are involved with multiple GCNs.\n\nThe basic idea, introducing long rage dependecy, would be interesting. Robustness for the feature remove is also interesting.\n\nThe validation set would be important for the proposed method, but for creating larger validation set, labeled training set would become small. How the good balance of training-and-validation can be determined?\n\nDiscussing choice of the degree would be informative. In introducing many degrees (GCNs) for small labeled nodes semi-supervised setting seems to cause over-fitting.",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting idea to boost GCN.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper presents a Network of Graph Convolutional Networks (NGCNs) that uses\nrandom walk statistics to extract information from near and distant neighbors\nin the graph.\n\nThe authors show that a 2-layer Graph Convolutional Network, with linear\nactivation and W0 as identity matrix, reduces to a one-step random walk.\nThey build on this notion to  introduce the idea to make the GCN directly operate\non random walk statistics to better model information across distant nodes.\n\nGiven that it is not clear how many steps of random walk to use a-priori it is\nproposed to make a mixture of models whose outputs are combined by a\nsoftmax classifier, or by an attention based mixing (learning the mixing coefficients).\n\nI find that the comparison can be considered slightly unfair as NGCN has k-times\nthe number of GCN models in it. Did the authors compare with a deeper GCN, or\nsimply with a mixture of plain GCN using one-step random walk?\nThe datasets used for comparison are extremely simple, and I am glad that the\nauthors point out that this is a significant issue for benchmark driven research.\nHowever, doing calibration on a subset of the validation nodes via gradient\ndescent is not very clean as by doing it one implicitly uses those nodes for training.\nThe improvement of the calibrated model on 5 nodes per class (Table 3) seems\nto hint that this peeking into the validation is indeed happening.\n\nThe authors mention that feeding explicitly the information on distant nodes\nmakes learning easier and that otherwise such information it would be hard to\nextract from stacking several GCN layers. While this is true for the small datasets\nusually considered it is not clear at all whether this still holds when we will\nhave large scale graph benchmarks.\n\nExperiments are well conducted but lack a comparison with GraphSAGE and MoNet,\nwhich are the reference models for the selected benchmarks. A comparison would have made the contribution stronger in my opinion. Improvements in performance are minor\nexcept for decimated inputs setting reported in Table 3. In this last case though\nno statistics over multiple runs are shown.\n\nOverall I like the interpretation, even if a bit forced, of GCN as using one-step\nrandom walk statistics. The paper is clearly written.\nThe main issue I have with the approach is that it does not bring a very novel\nway to perform deep learning on graphs, but rather improves marginally upon\na well established one.\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}