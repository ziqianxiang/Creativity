{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The reviewers were largely agreed that the paper presented an interesting idea and has potential but needs a better empirical evaluation.  It seems that the authors largely agree and are working to improve it.\n\nPROS:\n1. Improving the speed of program synthesis is a useful problem\n2. Good treatment of related work, e.g. CEGIS\n\nCONS:\n1. The approach likely does not scale\n2. The architecture is underspecified making it hard to reproduce\n3. Only 1 domain for evaluation"
    },
    "Reviews": [
        {
            "title": "Interesting formulation, but execution lets the paper down",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper presents a method for choosing a subset of examples on which to run a constraint solver\nin order to solve program synthesis problems. This problem is basically active learning for\nprogramming by example, but the considerations are slightly different than in standard active\nlearning. The assumption here is that labels (aka outputs) are easily available for all possible\ninputs, but we don't want to give a constraint solver all the input-output examples, because it will\nslow down the solver's execution.\n\nThe main baseline technique CEGIS (counterexample-guided inductive synthesis) addresses this problem\nby starting with a small set of examples, solving a constraint problem to get a hypothesis program,\nthen looking for \"counterexamples\" where the hypothesis program is incorrect.\n\nThis paper instead proposes to learn a surrogate function for choosing which examples to select. The\npaper isn't presented in exactly these terms, but the idea is to consider a uniform distribution\nover programs and a zero-one likelihood for input-output examples (so observations of I/O examples\njust eliminate inconsistent programs). We can then compute a posterior distribution over programs\nand form a predictive distribution over the output for all the remaining possible inputs. The paper\nsuggests always adding the I/O example that is least likely under this predictive distribution\n(i.e., the one that is most \"surprising\").\n\nForming the predictive distribution explicitly is intractable, so the paper suggests training a\nneural net to map from a subset of inputs to the predictive distribution over outputs.  Results show\nthat the approach is a bit faster than CEGIS in a synthetic drawing domain.\n\nThe paper starts off strong. There is a start at an interesting idea here, and I appreciate the\nthorough treatment of the background, including CEGIS and submodularity as a motivation for doing\ngreedy active learning, although I'd also appreciate a discussion of relationships between this approach \nand what is done in the active learning literature.Once getting into the details of the proposed approach, \nthe quality takes a downturn, unfortunately. \n\nMain issues:\n- It's not generally scalable to build a neural network whose size scales with the number\nof possible inputs. I can't see how this approach would be tractable in more standard program\nsynthesis domains where inputs might be lists of arrays or strings, for example.  It seems that this\napproach only works due to the peculiarities of the formulation of the only task that is considered,\nin which the program maps a pixel location in 32x32 images to a binary value.\n\n- It's odd to write \"we do not suggest a specific neural network architecture for the\nmiddle layers, one should seelect whichever architecture that is appropriate for the domain at\nhand.\" Not only is it impossible to reproduce a paper without any architectural details, but the\nresult is then that Fig 3 essentially says inputs -> \"magic\" -> outputs. Given that I don't even\nthink the representation of inputs and outputs is practical in general, I don't see what the \ncontribution is here.\n\n- This paper is poor in the reproducibility category. The architecture is never described,\nit is light on details of the training objective, it's not entirely clear what the DSL used in the\nexperiments is (is Figure 1 the DSL used in experiments), and it's not totally clear how the random\nimages were generated (I assume values for the holes in Figure 1 were sampled from some\ndistribution, and then the program was executed to generate the data?).\n\n- Experiments are only presented in one domain, and it has some peculiarities relative to \nmore standard program synthesis tasks (e.g., it's tractable to enumerate all possible inputs).  It'd\nbe stronger if the approach could also be demonstrated in another domain.\n\n- Technical point: it's not clear to me that the training procedure as described is consistent\nwith the desired objective in sec 3.3. Question for the authors: in the limit of infinite training\ndata and model capacity, will the neural network training lead to a model that will reproduce the\nprobabilities in 3.3?\n\nTypos:\n- The paper needs a cleanup pass for grammar, typos, and remnants like \"Figure blah shows our \nneural network architecture\" on page 5.\n\nOverall: There's the start of an interesting idea here, but I don't think the quality is high enough\nto warrant publication at this time.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good idea, but some misgivings about accepting in current state.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "General-purpose program synthesizers are powerful but often slow, so work that investigates means to speed them up is very much welcome—this paper included. The idea proposed (learning a selection strategy for choosing a subset of synthesis examples) is good. For the most paper, the paper is clearly-written, with each design decision justified and rigorously specified. The experiments show that the proposed algorithm allows a synthesizer to do a better job of reliably finding a solution in a short amount of time (though the effect is somewhat small).\n\nI do have some serious questions/concerns about this method:\n\nPart of the motivation for this paper is the goal of scaling to very large sets of examples. The proposed neural net setup is an autoencoder whose input/output size is proportional to the size of the program input domain. How large can this be expected to scale (a few thousand)? \n\nThe paper did not specify how often the neural net must be trained. Must it be trained for each new synthesis problem? If so, the training time becomes extremely important (and should be included in the “NN Phase” time measurements in Figure 4). If this takes longer than synthesis, it defeats the purpose of using this method in the first place.\nAlternatively, can the network be trained once for a domain, and then used for every synthesis problem in that domain (i.e. in your experiments, training one net for all possible binary-image-drawing problems)? If so, the training time amortizes to some extent—can you quantify this?\nThese are all points that require discussion which is currently missing from the paper.\n\nI also think that this method really ought to be evaluated on some other domain(s) in addition to binary image drawing. The paper is not an application paper about inferring drawing programs from images; rather, it proposes a general-purpose method for program synthesis example selection. As such, it ought to be evaluated on other types of problems to demonstrate this generality. Nothing about the proposed method (e.g. the neural net setup) is specific to images, so this seems quite readily doable.\n\nOverall: I like the idea this paper proposes, but I have some misgivings about accepting it in its current state.\n\n\n\n\nWhat follows are comments on specific parts of the paper:\n\n\nIn a couple of places early in the paper, you mention that the neural net computes “the probability” of examples. The probability of what? This was totally unclear until fairly deep into Section 3.\n - Page 2: “the neural network computes the probability for other examples not in the subset”\n - Page 3: “the probability of all the examples conditioned on…”\n\nOn a related note, I don’t like the term “Selection Probability” for the quantity it describes. This quantity is ‘the probability of an input being assigned the correct output.’ That happens to be (as you’ve proven) a good measure by which to select examples for the synthesizer. The first property (correctness) is a more essential property of this quantity, rather than the second (appropriateness as an example selection measure).\n\nPage 5: “Figure blah shows our neural network architecture” - missing reference to Figure 3.\n\nPage 5: “note that we do not suggest a specific neural network architecture for the middle layers, one should select whichever architecture that is appropriate for the domain at hand” - such as? What are some architectures that might be appropriate for different domains? What architecture did you use in your experiments?\n\nThe description of the neural net in Section 3.3 (bottom of page 5) is hard to follow on first read-through. It would be better to lead with some high-level intuition about what the network is supposed to do before diving into the details of how it’s set up. The first sentence on page 6 gives this intuition; this should come much earlier.\n\nPage 5: “a feed-forward auto-encoder with N input neurons…” Previously, N was defined as the size of the input domain. Does this mean that the network can only be trained when a complete set of input-output examples is available (i.e. outputs for all possible inputs in the domain)? Or is it fine to have an incomplete example set?\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting work, but underwhelming empirical evaluation.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper proposes a method for identifying representative examples for program\nsynthesis to increase the scalability of existing constraint programming\nsolutions. The authors present their approach and evaluate it empirically.\n\nThe proposed approach is interesting, but I feel that the experimental section\ndoes not serve to show its merits for several reasons. First, it does not\ndemonstrate increased scalability. Only 1024 examples are considered, which is\nby no means large. Even then, the authors approach selects the highest number of\nexamples (figure 4). CEGIS both selects fewer examples and has a shorter median\ntime for complete synthesis. Intuitively, the authors' method should scale\nbetter, but they fail to show this -- a missed opportunity to make the paper\nmuch more compelling. This is especially true as a more challenging benchmark\ncould be created very easily by simply scaling up the image.\n\nSecond, there is no analysis of the representativeness of the found sets of\nconstraints. Given that the results are very close to other approaches, it\nremains unclear whether they are simply due to random variations, or whether the\nproposed approach actually achieves a non-random improvement.\n\nIn addition to my concerns about the experimental evaluation, I have concerns\nabout the general approach. It is unclear to me that machine learning is the\nbest approach for modeling and solving this problem. In particular, the\nselection probability of any particular example could be estimated through a\nheuristic, for example by simply counting the number of neighbouring examples\nthat have a different color, weighted by whether they are in the set of examples\nalready, to assess its \"borderness\", with high values being more important to\nachieve a good program. The border pixels are probably sufficient to learn the\nprogram perfectly, and in fact this may be exactly what the neural net is\nlearning. The above heuristic is obviously specific to the domain, but similar\nheuristics could be easily constructed for other domains. I feel that this is\nsomething the authors should at least compare to in the empirical evaluation.\n\nAnother concern is that the authors' approach assumes that all parameters have\nthe same effect. Even for the example the authors give in section 2, it is\nunclear that this would be true.\n\nThe text says that rand+cegis selects 70% of examples of the proposed approach,\nbut figure 4 seems to suggest that the numbers are very close -- is this initial\nexamples only?\n\nOverall the paper appears rushed -- the acknowledgements section is left over\nfrom the template and there is a reference to figure \"blah\". There are typos and\ngrammatical mistakes throughout the paper. The reference to \"Model counting\" is\nincomplete.\n\nIn summary, I feel that the paper cannot be accepted in its current form.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}