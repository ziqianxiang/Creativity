{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The contribution of this paper basically consists of using MLPs in the attention mechanism of end-2-end memory networks. Though it leads to some improvements on bAbI (which may not be so surprising - MLP attention has been shown preferable in certain scenarious), it does not seem to be a sufficient contribution. The motivation is also confusing - the work is not really that related to relation networks, which were specifically designed to deal with situations where *relations* between objects matter. The proposed architecture does not model relations.\n\n+  improvement on bAbI over the baselines\n-  limited novelty (MLP attention is fairly standard)\n-  the presentation of the idea is confusing (if the claim is about relations -> other datasets need to be considered)\n\nThere is a consensus between reviewers. "
    },
    "Reviews": [
        {
            "title": "Finding ReMO review",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper introduces Related Memory Network (RMN), an improvement over Relationship Networks (RN). RMN avoids growing the relationship time complexity as suffered by RN (Santoro et. Al 2017). RMN reduces the complexity to linear time for the bAbi dataset. RN constructs pair-wise interactions between objects in RN to solve complex tasks such as transitive reasoning. RMN instead uses a multi-hop attention over objects followed by an MLP to learn relationships in linear time.\n\nComments for the author:\n\nThe paper addresses an important problem since understanding object interactions are crucial for reasoning. However, how widespread is this problem across other models or are you simply addressing a point problem for RN? For example, Entnet is able to reason as the input is fed in and the decoding costs are low. Likewise, other graph-based networks (which although may require strong supervision) are able to decode quite cheaply. \n\nThe relationship network considers all pair-wise interactions that are replaced by a two-hop attention mechanism (and an MLP). It would not be fair to claim superiority over RN since you only evaluate on bABi while RN also demonstrated results on other tasks. For more complex tasks (even over just text), it is necessary to show that you outperform RN w/o considering all objects in a pairwise fashion. More specifically, RN uses an MLP over pair-wise interactions, does that allow it to model more complex interactions than just selecting two hops to generate attention weights. Showing results with multiple hops (1,2,..) would be useful here.\n\nMore details are needed about Figure 3. Is this on bAbi as well? How did you generate these stories with so many sentences? Another clarification is the bAbi performance over Entnet which claims to solve all tasks. Your results show 4 failed tasks, is this your reproduction of Entnet?\n\nFinally, what are the savings from reducing this time complexity? Some wall clock time results or FLOPs of train/test time should be provided since you use multiple hops.\n\nOverall, this paper feels like a small improvement over RN. Without experiments over other datasets and wall clock time results, it is hard to appreciate the significance of this improvement. One direction to strengthen this paper is to examine if RMN can do better than pair-wise interactions (and other baselines) for more complex reasoning tasks.\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not sure what is novel",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper proposes to address the quadratic memory/time requirement of Relation Network (RN) by sequentially attending (via multiple layers) on objects and gating the object vectors with the attention weights of each layer. The proposed model obtains state of the art in bAbI story-based QA and bAbI dialog task.\n\nPros:\n- The model achieves the state of the art in bAbI QA and dialog. I think this is a significant achievement given the simplicity of the model.\n- The paper is clearly written.\n\nCons:\n- I am not sure what is novel in the proposed model. While the authors use notations used in Relation Network (e.g. 'g'), I don't see any relevance to Relation Network. Rather, this exactly resembles End-to-end memory network (MemN2N) and GMemN2N. Please tell me if I am missing something, but I am not sure of the contribution of the paper. Of course, I notice that there are small architectural differences, but if these are responsible for the improvements, I believe the authors should have conducted ablation study or qualitative analysis that show that the small tweaks are meaningful.\n \nQuestion:\n- What is the exact contribution of the paper with respect to MemN2N and GMemN2N?",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper proposes an alternative to the relation network architecture whose computational complexity is linear in the number of objects present in the input. The model achieves good results on bAbI compared to memory networks and the relation network model. From what I understood, it works by computing a weighted average of sentence representations in the input story where the attention weights are the output of an MLP whose input is just a sentence and question (not two sentences and a question). This average is then fed to a softmax layer for answer prediction. I found it difficult to understand how the model is related to relation networks, since it no longer scores every combination of objects (or, in the case of bAbI, sentences), which is the fundamental idea behind relation networks. Why is the approach not evaluated on CLEVR, in which the interaction between two objects is perhaps more critical (and was the main result of the original relation networks paper)? The fact that the model works well on bAbI despite its simplicity is interesting, but it feels like the paper is framed to suggest that object-object interactions are not necessary to explicitly model, which I can't agree with based solely on bAbI experiments. I'd encourage the authors to do a more detailed experimental study with more tasks, but I can't recommend this paper's acceptance in its current form.\n\nother questions / comments:\n- \"we use MLP to produce the attention weight without any extrinsic computation between the input sentence and the question.\" isn't this statement false because the attention computation takes as input the concatenation of the question and sentence representation?\n- writing could be cleaned up for spelling / grammar (e.g., \"last 70 stories\" instead of \"last 70 sentences\"), currently the paper is very hard to read and it took me a while to understand the model",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}