{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper proposes to use multiple discriminators to stabilize the GAN training process. Additionally, the discriminators only see randomly projected real and generated samples.\n\nSome valid concerns raised by the reviewers which makes the paper weak:\n - Multiple discriminators have been tried before and the authors do not clearly show experimentally / theoretically if the random projection is adding any value.\n- Authors compare only with DCGAN and the results are mostly subjective. How much improvement the proposed approach provides when compared to other GAN models that are developed with stability as the main goal is hence not clear."
    },
    "Reviews": [
        {
            "title": "Review of Stabilizing GAN Training with Multiple Random Projections",
            "rating": "5: Marginally below acceptance threshold",
            "review": "\nThe paper proposes to stabilize GAN training by using an ensemble of discriminators, each workin on a random projection of the input data, to provide the training signal for the generator model.\n\nQ1: “In relation to “Theorem 3.1. … will produce samples from a distribution whose marginals along each of the projections W_k match those of the true distribution”.. I presume an infinite number of generator distributions could give rise to the correct marginals however not necessarily be converged to the data distribution. In Theorem A.2 the authors upperbound this residual as a function of the smoothness and support of the distributions as well as the projections presented to the discriminators. Can the authors comment on how tight this bound is e.g. as a function the number of used discriminators or the choosen projection methods ? \n\nQ2: Related to the above. Did the authors do or considered any frequency analysis of the ensemble of random projection? I guess you could easily do a numeric simulation of the expected frequency spectrum of the combined set discriminators?\n\n\nQ3: My primary concern with the work is the above mentioned computational complexity of running K discriminators in parallel. This is especially in relation to the experimental results showing significant high-frequency artefacts when running with K=12 classifiers (K=12 celebA results and “Random Imagenet-Canine Images: Proposed Method” in suplementary results). I think this is as expected as the authors are effectively fitting each classifier to the distributions of smoothed  (with 8x8 random kernel)  subsampled version of the input image. I would expect that each discriminator sees none or only a very limited amount  the high frequency component in the images.  Do the authors have any comments on how the sampling of the projection kernels affects the image results especially if the number of needed classifiers can be reduced somehow? I would expect that a combination of smoothing and high frequency filters would be needed to remove the high frequency artefacts?\n\nQ4: Whats the explanation of the oscilating patterns in figure 2?\n\nQ5: In the conclusion the authors mention that their framework is currently limited by the computational of running K discriminators and proposes:\n\n“In our current framework, the number of discriminators is limited by computational cost. In future work, we plan to investigate training with a much larger set of discriminators, employing only a small subset of them at each iteration, or every set of iterations”\n\nIn the extreme case of only using a single randomly discriminator the approach is quite similar to the quite widely used input dropout to the discriminator?\n\nOverall I like the simplicity of the proposed idea. However i’m not completely convinced that the “marginal” convergence proof holds for the relative low number of discriminators possible to use in practice. At least i would like the authors to touch on this key aspect of the method both theoretically and with experiments/simulations. Also several other methods have recently been proposed to improve stability of GANs, however no experimental comparisons is made with these methods (WGAN, EGAN, LSGAN etc.)\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Stabilizing GAN Training with Multiple Random Projections",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "The paper proposes a new approach to GAN training whereby they train one generator against an ensemble of discriminative that each receive a randomly projected version of the data. The authors show that this approach provides stable gradients to train the generator. \n\nThis is a nice idea, and both the theoretical analysis presented and the experiments on image data sets are interesting. Although the idea to train an ensemble of learning machines is not new, see e,.g. [1,2] -- and it would be useful to add some background on this, and the regularisation effect that emerges from it --  it does become new in the new context considered here, as the paper shows that such ensemble can also fulfil the role of stabilising GAN training. \nThe results are quite convincing that the proposed method is useful in practice,\n\nIt would be interesting to know if weighting the discriminators, or discarding the unlucky random projections as it was done in [1] would have potential in this context?\n\n[1] Timothy I. Cannings, Richard J. Samworth. Random-projection ensemble classification. Journal of the Royal Statistical Society B, 79(4), 2017, Pages 959-1035.     \n[2] Robert J. Durrant, Ata Kabán. Random projections as regularizers: learning a linear discriminant from fewer observations than dimensions. Machine Learning 99(2), 2015, Pages 257-286.\n\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Due to poor experimental validation and inconclusive results, the reviewer does not recommend acceptance of the paper.",
            "rating": "3: Clear rejection",
            "review": "\n- Paper summary\n\nThe paper proposes a GAN training method for improving the training stability. The key idea is to let a GAN generator competes with multiple GAN discriminators where each discriminator takes a random low-dimensional projection of an input image for differentiate whether the input image is a real or generated one. Visual generation results from the proposed method with comparison to those generated by the DCGAN were used as the main experimental validation for the merit of the proposed method. Due to poor experimental validation and inconclusive results, the reviewer does not recommend the acceptance of the paper.\n\n- Inconclusive results\n\nThe paper fails to compare the proposed method with the GMAN framework [a], which was the first work proposing utilizing multiple discriminators for more stable GAN training. Without comparing to the GMAN work, we do not know whether the benefit is from using multiple discriminators proposed in the GMAN work or from using the random low dimensional projections proposed in this paper. If it is former, then the proposed method has no merits at all.\n\nIn addition, the generator loss curve shown in Figure 2 is not making much sense. The generator loss curve will be meaningful if each discriminator update is optimal. However, this is not the case in the proposed method. There is little to conclude from Figure 2.\n\n[a] Durugkar et al. \"Generative multi-adversarial networks.\" ICLR 2017\n\n- Poor experimental validation\n\nThe paper fails to utilize more established performance metrics such as the inception loss or human evaluation score to evaluate its benefit. It does not compare to other approaches for stabilizing GAN training such as WGAN or LSGAN. The main results shown in the paper are generating 64x64 human face images, which is not impressive.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}