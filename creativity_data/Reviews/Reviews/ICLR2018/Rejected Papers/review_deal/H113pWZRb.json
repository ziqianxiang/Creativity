{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The authors provide an extension to GCNs of Kipf and Welling in order to incorporate information about higher order neighborhoods. The extension is well motivated (and  though I agree that it is not trivial modification of the K&W approach to the second order,  thanks to the authors for the clarification).  The improvements are relatively moderate.\n\nPros:\n-- The approach is well motivated\n-- The paper is clearly written\nCons:\n-- The originality and impact (as well as motivation) are questioned by the reviewers\n"
    },
    "Reviews": [
        {
            "title": "Major revision is recommended",
            "rating": "4: Ok but not good enough - rejection",
            "review": "In this paper a new neural network architecture for semi-supervised graph classification is proposed. The new construction builds upon graph polynomial filters and utilizes them on each successive layer of the neural network with ReLU activation functions.\n\nIn my opinion writing of this paper requires major revision. The first 8 pages mostly constitute a literature review and experimental section provides no insights about the performance of the TAGCN besides the slight improvement of the Cora, Pubmed and Citeseer benchmarks.\n\nThe one layer analysis in sections 2.1, 2.2 and 2.3 is simply an explanation of graph polynomial filters, which were previously proposed and analyzed in cited work of Sandryhaila and Moura (2013). Together with the summary of other methods and introduction, it composes the first 8 pages of the paper. I think that the graph polynomial filters can be summarized in much more succinct way and details deferred to the appendix for interested reader. I also recommend stating which ideas came from the Sandryhaila and Moura (2013) work in a more pronounced manner.\n\nNext, I disagree with the statement that \"it is not clear how to keep the vertex local property when filtering in the spectrum domain\". Graph Laplacian preserves the information about connectivity of the vertices and filtering in the vertex domain can be done via polynomial filters in the Fourier domain. See Eq. 18 and 19 in [1].\n\nFinally, I should say that TAGCN idea is interesting. I think it can be viewed as an extension of the GCN (Kipf and Welling, 2017), where instead of an adjacency matrix with self connections (i.e. first degree polynomial), a higher degree graph polynomial filter is used on every layer (please correct me if this comparison is not accurate). With more experiments and interpretation of the model, including some sort of multilayer analysis, this can be a good acceptance candidate.\n\n\n[1] David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst.\nThe emerging field of signal processing on graphs: Extending high-dimensional data analysis to\nnetworks and other irregular domains. IEEE Signal Processing Magazine, 30(3):83â€“98, 2013.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting improvement idea, clarity could be improved",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper introduces Topology Adaptive GCN (TAGCN) to generalize convolutional\nnetworks to graph-structured data.\nI find the paper interesting but not very clearly written in some sections,\nfor instance I would better explain what is the main contribution and devote\nsome more text to the motivation. Why is the proposed approach better than the\npreviously published ones, and when is that there is an advantage in using it?\n\nThe main contribution seems to be the use of the \"graph shift\" operator from\nSandryhaila and Moura (2013), which closely resembles the one from\nShuman et al. (2013). It is actually not very well explained what is the main\ndifference.\n\nEquation (2) shows that the learnable filters g are operating on the k-th power\nof the normalized adjacency matrix A, so when K=1 this equals classical GCN\nfrom T. Kipf et al.\nBy using K > 1 the method is able to leverage information at a farther distance\nfrom the reference node.\n\nSection 2.2 requires some polishing as I found hard to follow the main story\nthe authors wanted to tell. The definition of the weight of a path seems\ndisconnected from the main text, ins't A^k kind of a a diffusion operator or\nrandom walk?\nThis makes me wonder what would be the performance of GCN when the k-th power\nof the adjacency is used.\n\nI liked Section 3, however while it is true that all methods differ in the way they\ndo the filtering, they also differ in the way the input graph is represented\n(use of the adjacency or not).\n\nExperiments are performed on the usual reference benchmarks for the task and show\nsensible improvements with respect to the state-of-the-art. TAGCN with K=2 has\ntwice the number of parameters of GCN, which makes the comparison not entirely\nfair. Did the author experiment with a comparable architecture?\nAlso, how about using A^2 in GCN or making two GCN and concatenate them in\nfeature space to make the representational power comparable?\n\nIt is also known that these benchmarks, while being widely used, are small and\nresult in high variance results. The authors should report statistics over\nmultiple runs.\nGiven the systematic parameter search, with reference to the actual validation\n(or test?) set I am afraid there could be some overfitting. It is quite easy\nto probe the test set to get best performance on these benchmarks.\n\nAs a minor remark, please make figures readable also in BW.\n\nOverall I found the paper interesting but also not very clear at pointing out\nthe major contribution and the motivation behind it. At risk of being too reductionist:\nit looks as learning a set of filters on different coordinate systems given\nby the various powers of A. GCN looks at the nearest neighbors and the paper\nshows that using also the 2-ring improves performance.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Reasonable technical contribution, solid evaluation, well written",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The authors propose a new CNN approach to graph classification that generalizes previous work. Instead of considering the direct neighborhood of a vertex in the convolution step, a filter based on outgoing walks of increasing length is proposed. This incorporates information from more distant vertices in one propagation step.\n\nThe proposed idea is not exceptional original, but the paper has several strong points:\n\n* The relation to previous work is made explicit and it is show that several previous approaches are generalized by the proposed one.\n* The paper is clearly written and well illustrated by figures and examples. The paper is easy to follow although it is on an adequate technical level.\n* The relation between the vertex and spectrum domain is well elaborated and nice (although neither important for understanding nor implementing the approach).\n* The experimental evaluation appears to be sound. A moderate improvement compared to other approaches is observed for all data sets.\n\nIn summary, I think the paper can be accepted for ICLR.\n----------- EDIT -----------\nAfter reading the publications mentioned by the other reviewers as well as the following related contributions\n\n* Network of Graph Convolutional Networks Trained on Random Walks (under review for ICLR 2018)\n* Graph Convolution: A High-Order and Adaptive Approach, Zhenpeng Zhou, Xiaocheng Li (arXiv:1706.09916)\n\nI agree that the relation to previous work is not adequately outlined. Therefore I have modified my rating accordingly.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}