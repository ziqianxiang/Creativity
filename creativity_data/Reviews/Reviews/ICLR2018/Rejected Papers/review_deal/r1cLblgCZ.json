{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper applies a form of recurrent autoencoder for a specific type of industrial sensor signal analysis.  The application is very narrow and the data set is proprietary.  The approach is not clearly described, but seems very straightforward and is not placed in context of prior work.  It is therefore not clear how to evaluate the contribution of the method.  The authors have revised the paper to include more details and prior work, but it still needs a lot more work on all of the above dimensions before it can make a significant contribution to the ICLR community."
    },
    "Reviews": [
        {
            "title": "Important background information/related work missing",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper proposes a strategy that is inspired by the recurrent auto-encoder model, such that clustering of multidimensional time series data can be performed based on the context vectors generated by the encoding process. Unfortunately, the paper in its current form is a bit thin on content.\n \nMain issues:\nNo related works (such as those using RNN for time series analysis or clustering of time series data streams etc.) were described by the paper, no baselines were used in the comparison evaluations, and no settings/details were provided in the experiment section. As a result, it is quite difficult to judge the merits and novelty of the paper.\n \nOther issues:\nsome contribution claims highlighted in the Discussion Section, i.e., Section 4, are arguable and should be further extended. For example, the authors claim that the proposed LSTM-based autoencoder networks can be natively scaled up to data with very high dimensionality. I would like the authors to explain it in more details or empirically demonstrate that, since a LSTM-based model could be computationally expensive. As another example, the authors claim that reducing the dimensionality of the output sequence is one of the main contributions of the paper. In this sense, further elaborations from that perspective would be very beneficial since some networks already employ such a mechanism. \n\nIn short, the paper in its current form does not provide sufficient details for the reviewer to judge its merits and contributions.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "substandard quality",
            "rating": "2: Strong rejection",
            "review": "This writeup describes an application of recurrent autoencoder to analysis of multidimensional time series. The quality of writing, experimentation and scholarship is clearly below than what is expected from a scientific article. The method is explained in a very unclear way, there is no mention of any related work. I would encourage the authors to take a look at other ICLR submissions and see how rigorously written they are, how they position the reported research among comparable works. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Trivial results and obscure data set.",
            "rating": "2: Strong rejection",
            "review": "The paper describes a sequence to sequence auto-encoder model which is used to learn sequence representations. The authors show that for their application, better performance is obtained when the network is only trained to reconstruct a subset of the data measurements. The paper also presents some visualizations the similarity structure of the learned representations and proposes a window-based method for processing the data.\n\nAccording to the paper, the experiments are done using a data set which is obtained from measurements of an industrial production process. Figure 2 indicates that reconstructing fewer dimensions of this dataset leads to lower MSE scores. I don’t see how this is showing anything besides the obvious fact that reconstructing fewer dimensions is an easier task than reconstructing all of them.  The only conclusions I can draw from the visual analysis is that the context vectors are more similar to each other when they are obtained from time steps in the data stream which are close to each other. Since the paper doesn’t describe much about the privately owned data at all, there is no possibility to replicate the work. The paper doesn’t frame the work in prior research at all and the six papers it cites are only referred to in the context of describing the architecture.\n\nI found it very hard to distil what the main contribution of this work was according to the paper. There were also not many details about the precise architecture used. It is implied that GRU networks and were used but the text doesn’t actually state this explicitly. By saying so little about the data that was used, it was also not clear what the temporal correlations of the context vectors are supposed to tell us. \n\nThe paper describes how existing methods are applied to a specific data set. The benefit of only reconstructing a subset of the input dimensions seems very data specific to me and I find it hard to consider this a novel idea by itself. Presenting sequential data in a windowed format is a standard procedure and not a new idea either. All in all I don't think that the paper presents any new ideas or interesting results.\n\nPros:\n* The visualizations look nice.\n\nCons:\n* It is not clear what the main contribution is.\n* Very little information about the data. \n* No clear experiments from which conclusions can be drawn.\n* No new ideas.\n* Not well rooted in prior work.\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}