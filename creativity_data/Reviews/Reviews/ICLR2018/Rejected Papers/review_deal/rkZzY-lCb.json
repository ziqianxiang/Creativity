{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper presents an approach for learning continuous-valued vector representations combining multiple input feature sets of different types, in both unsupervised and supervised settings.  The revised paper is a merger of the original submission and another ICLR submission.  This meta-review takes into account all of the comments on both submissions and revisions.\n\nThe merged paper is an improvement over the two separate ones.  However, the contribution over previous work is still a bit unclear.  It still does not sufficiently compare to/discuss in the context of other recent work on combining multiple feature groups.\n\nThe experiments are also quite limited.  The idea is introduced as extremely general, but the experiments focus on a small number of specific tasks, some of them non-standard."
    },
    "Reviews": [
        {
            "title": "Interesting paper with convincing results, but the approach has limited novelty. Proposed method is based on an approach that is concurrently under review at ICLR18",
            "rating": "7: Good paper, accept",
            "review": "Summary:\nThis paper proposes an approach to learn embeddings for structured datasets i.e. datasets which have heterogeneous set of features, as opposed to just words or just pixels. The paper proposes an approach called Feat2vec that relies on Structured Deep-In Factorization machines-- a paper that is concurrently under review at ICLR2018, which I haven't read in depth. The paper compares against a Word2vec baseline that pools all the heterogeneous content learns just one set of embeddings. Results are shown on IMDB movies and a proprietary education platform datasets. In both the tasks, Feat2vec leads to significant reduction in error compared to Word2vec.\n\nComments:\n\nThe paper is well written and addresses an important problem of learning word embeddings when there is inherent structure in the feature space. It is a very practically relevant problem. The novelty of the proposed approach seems limited in light of the related paper that is concurrently under review at ICLR2018, on which this paper heavily relies. Perhaps the authors should consider combining the two papers into one complete paper? The structured deep-in factorization machines allow higher-level interactions in embedding learning which allows the authors to learn embeddings for heterogeneous set of features. The sampling approaches proposed seem pretty straightforward adaptations of existing methods and not novel enough.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Not clear what I can learn from this",
            "rating": "2: Strong rejection",
            "review": "SUMMARY.\n\nThe paper presents an extension of word2vec for structured features.\nThe authors introduced a new compatibility function between features and, as in the skipgram approach, they propose a variation of negative sampling to deal with structured features.\nThe learned representation of features is tested on a recommendation-like task.\n\n\n----------\n\nOVERALL JUDGMENT\nThe paper is not clear and thus I am not sure what I can learn from it.\nFrom what is written on the paper I have trouble to understand the definition of the model the authors propose and also an actual NLP task where the representation induced by the model can be useful.\nFor this reason, I would suggest the authors make clear with a more formal notation, and the use of examples, what the model is supposed to achieve.\n\n----------\n\nDETAILED COMMENTS\nWhen the authors refer to word2vec is not clear if they are referring to skipgram or cbow algorithm, please make it clear.\nBottom of page one: \"a positive example is 'semantic'\", please, use another expression to describe observable examples, 'semantic' does not make sense in this context.\nLevi and Goldberg (2014)  do not say anything about factorization machines, could the authors clarify this point?\nEquation (4), what do i and j stand for? what does \\beta represent? is it the embedding vector? How is this formula related to skipgram or cbow?\nThe introduction of structured deep-in factorization machine should be more clear with examples that give the intuition on the rationale of the model.\nThe experimental section is rather poor, first, the authors only compare themselves with word2ve (cbow), it is not clear what the reader should learn from the results the authors got.\nFinally, the most striking flaw of this paper is the lack of references to previous works on word embeddings and feature representation, I would suggest the author check and compare themselves with previous work on this topic.",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Neat representation learning scheme for structured features",
            "rating": "7: Good paper, accept",
            "review": "This paper provides a clean way of learning embeddings for structured features that can be discrete -- indicating presence / absence of a certain quality. Further, these features can be structured i.e. a set of them are of the same 'type'. Unlike, word2vec there is no hard constraint that similar objects must have similar representations and so, the learnt embeddings reflect the likelihood of the observed features. Therefore, this can be used as a multi-label classifier by using two feature types -- the input and the set of categories. This proposed scheme is evaluated on two datasets -- movies and education in a retrieval setting. \n\nI would like to see an evaluation of these features in a classification setting to further demonstrate the utility of these embeddings as compared to directly embedding the discrete features and then performing a K-way classification. For example, I am aware of -- http://manikvarma.org/downloads/XC/XMLRepository.html contains some interesting datasets which have a large number of discrete features and classes. ",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}