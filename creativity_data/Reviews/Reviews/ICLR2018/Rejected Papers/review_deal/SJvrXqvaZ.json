{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "Reviewers are unanimous in scoring this paper below threshold for acceptance.  The authors did not submit any rebuttals of the reviews.\n\nPros:\nPaper is generally clear.\nHardware results are valuable.\n\nCons:\nLimited simulation results.\nProposed method is not really novel.\nInsufficient empirical validation of the approach."
    },
    "Reviews": [
        {
            "title": "Interesting approach and hardware validation, but methods and comparisons are lacking",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Positive:\n- Interesting approach\n- Hardware validation (the RL field needs more of this!)\n\nNegative:\n- Figure 2: what is the reward here? The one from Section 5.1?\n- No comparisons to other methods: Single pendulum swing-up is a very easy task that has been solved with various methods (mostly in a cart-pole setup). Please compare to existing methods such as PILCO, basic Q-learning, classical methods... \n- I'm not sure what's going on with the grammar in Section 5.3 (\"like crazy\", \"super hot\"...). This section also seems irrelevant (move to an appendix/supplementary or remove).\n- You should plot a typical control curve for the motors (requested torques). This might explain your heat problem (I'm guessing the motor is effectively controlled by a bang-bang controller).\n- Why did you pick this task? It's fine to only validate on a single task in hardware, but why not include additional simulation results (e.g. double pendulum)?",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Ok but not good enough",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Clarity \nThe paper is clear in general. \n\nOriginality\nThe novelty of the method is limited. The proposed method is a simple extension of L. Pinto et al. by replacing TRPO with A3C. No evidence is provided to show the proposed method is competitive with the original TRPO version. \n\nSignificance\n- The empirical results on the hardware are valuable. \n- The simulated results are very limited. The neural networks used in the simulation have only one hidden layer. The method is tested on the Pendulum domain. \n\nPros:\n- Real hardware results are provided. \n\nCons:\n- Limited simulation results. \n- Lacking technical novelty. \n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The proposed technique is of modest contribution and the experimental results do not provide sufficient validation of the approach.  ",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The authors propose an extension of adversarial reinforcement learning to A3C. The proposed technique is of modest contribution and the experimental results do not provide sufficient validation of the approach.  \n\nThe authors propose extending A3C to produce more robust policies by training a zero-sum game with two agents: a protagonist and an antagonist. The protagonist is attempting to achieve the given task while the antagonist's goal is for the task to fail. \n\nThe contribution of this work, AR-A3C, is extending adversarial reinforcement learning, namely robust RL (RRL) and robust adversarial RL (RARL), to A3C. In the context of this prior work the novelty is extending the family of adversarial RL methods. However, the proposed method is still within the same family methods as demonstrated by RARL.\n\nThe authors state that AR-A3C requires half as many rollouts as compared to RARL. However, no empirical comparison between the two methods is performed. The paper only performs analysis against the A3C and no other adversarial baseline and on only one environment: cartpole.  While they show transfer to the real world cartpole with this technique, there is not sufficient analysis to satisfactorily demonstrate the benefits of the proposed technique. \n\nThe paper reads well. There are a few notational issues in the paper that should be addressed. The authors mislabel the value function V as the  action value, or Q function. The action value function is action dependent where the value function is not.  As a much more minor issue, the authors introduce y as the discount factor, which deviates from the standard notation of \\gamma without any obvious reason to do so.\n\nDouble blind was likely compromised with the youtube video, which was linked to a real name account instead of an anonymous account.\n\nOverall, the proposed technique is of modest contribution and the experimental results do not provide sufficient validation of the approach.    ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}