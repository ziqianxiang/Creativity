{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "Three reviewers recommended rejection, and there was no rebuttal."
    },
    "Reviews": [
        {
            "title": "Lack of novelty",
            "rating": "3: Clear rejection",
            "review": "This paper proposes an out-of-the-box embedding for image classification task. Instead of taking one single layer output from pre-trained network as the feature vector for new dataset, the method first extracts the activations from all the layers, then runs spatial average pooling on all convolutional layers, then normalizes the feature and uses two predefined thresholds to discretize the features to {-1, 0, 1}. Final prediction is learned through a SVM model using those embeddings. Experimental results on nine different datasets show that this embedding outperforms baseline of using one single layer. I think in general this paper lacks novelty and it shouldn't be surprising that activations from all layers should be more representative than one single layer representation. Moreover, in Table 4, it shows that discretization actually hurts the performance. It is also very heuristic to choose the two thresholds.  \n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Poor presentation and lack of novelty",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Paper claims to propose a deep transfer learning method. There are several reasons not to consider this paper for ICLR at this point.\n\nPaper is badly written and the problem it tries to solve is not clearly stated.\nProposed feature embedding is incremental (lack of novelty and technical contribution)\nObtained results are encouraging but not good enough.\nLack of experimental validation.\nI think paper can be improved significantly and is not ready for publication at this point.\n\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "has novelty issue, results are not impressive",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper addresses the scenario when using a pretrained deep network as learnt feature representation for another (small) task where retraining is not an option or not desired. In this situation it proposes to use all layers of the network to extract feature from, instead of only one layer. \nThen it proposes to standardize different dimensions of the features based on their response on the original task. Finally, it discretize each dimension into {-1, 0, 1} to compress the final concatenated feature representation. \nDoing this, it shows improvements over using a single layer for 9 target image classification datasets including object, scene, texture, material, and animals.\n\nThe reviewer does not find the paper suitable for publication at ICLR due to the following reasons:\n- The paper is incremental with limited novelty.\n- the results are not encouraging\n- the pipeline of standardization, discretization is relatively costly, the final feature vector still large. \n- combining different layers, as the only contribution of the paper, has been done in the literature before,  for instance:\n“The Treasure beneath Convolutional Layers: Cross-convolutional-layer Pooling\nfor Image Classification” CVPR 2016\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}