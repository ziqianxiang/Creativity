{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper investigates the performance of various second-order optimization methods for training neural networks. Comparing different optimizers is worthwhile, but as this is an empirical paper which doesn't present novel techniques, the bar is very high for the experimental methodology. Unfortunately, I don't think this paper clears the bar: as pointed out by the reviewers, the comparisons miss several important methods, and the experiments miss out on important aspects of the comparison (e.g. wall clock time, generalization). I don't think there is enough of a contribution here to merit publication at ICLR, though it could become a strong submission if the reviewers' points were adequately addressed.\n"
    },
    "Reviews": [
        {
            "title": "Important topic, not enough evidence",
            "rating": "3: Clear rejection",
            "review": "This paper presents a comparative study on second-order optimization methods for CNNs. Overall, the topic is interesting and would be useful for the community.\n\nHowever, I think there are important issues about the paper:\n\n1) The paper is not very well-written. The language is sometimes very informal, there are many grammatical mistakes and typos. The paper should be carefully proofread.\n\n2) For such a comparative study, the number of algorithms and the number of datasets are quite little. The authors do not mention several important methods such as (not exhaustive)\n\nSchraudolph, N. N., Yu, J., and GÃ¼nter, S. A stochastic quasi-Newton method for online convex optimization.\nGurbuzbalaban et al, A globally convergent incremental Newton method (and other papers of the same authors)\nA Linearly-Convergent Stochastic L-BFGS Algorithm, Moritz et al\n\n3) The experiment details are not provided. It is not clear what parameters are used and how. \n\n4) There are some vague statements such as \"this scheme does not work\" or \"fixed learning rates are not applicable\". For instance, for the latter I cannot see a reason and the paper does not provide any convincing results.\n\nEven though the paper attempts to address an important point in deep learning, I do not believe that the presented results form evidence for such rather bold statements.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper attempts a well-needed experimentation of second order methods for training large CNNs. However, the results are mostly negative, it is not bold, and it is incomplete.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "A good experimentation of second order methods for training large DNNs in comparison with the popular SGD method has been lacking in the literature. This paper tries to fill that gap. Though there are some good experiments, I feel it could have been much better and more complete.\n\nSeveral candidates for second order methods are considered. However, their discussion and the final choice of the three methods is too rapid. It would have been useful to include an appendix with more details about them.\n\nThe results are mostly negative. The second order methods are much slower (in time) than SGD. The quasi-Newton methods are way too sensitive to hyperparameters. SHG is better in that sense, but it is far too slow. Distributed training is mentioned as an alternative, but that is just a casual statement - communication bottleck can still be a huge issue with large DNN models.\n\nI wish the paper had been bolder in terms of making improvements to one or more of the second order methods in order to make them better. For example, is it possible to come up with ways of choosing hyperparameters associated with the quasi-Newton implementations so as to make them robust with respect to batch size? Second order methods are almost dismissed off for RelU - could things be better with the use of a smooth version of RelU? Also, what about non-differentiability brought in my max pooling?\n\nOne disappointing thing about the paper is the lack of any analysis of the generalization performance associated with the methods, especially with the authors being aware of the works of Keskar et al and Kawaguchi et al. Clearly, the training method is having an effect on generlaization performance, with noise associated with stochastic methods being a great player for leading solutions to flat regions where generalization is better. One obvious question I have is: could it be that, methods such as SHG which have much less noise in them, have poor generalization properties? If so, how do we correct that?\n\nOverall, I like the attempt of exploring second order methods, but it could have come out a lot better.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "See below.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper conducts an empirical study on 2nd-order algorithms for deep learning, in particular on CNNs to answer the question whether 2nd-order methods are useful for deep learning.  More modestly and realistically, the authors compared stochastic Newton method (SHG) and stochastic Quasi- Newton method (SR1, SQN) with stochastic gradient method (SGD).  The activation function ReLu is known to be singular at 0, which may lead to poor curvature information, but the authors gave a good numerical comparison between the performances of 2nd-order methods with ReLu and the smooth function, Tanh.  The paper presented a reasonably good overview of existing 2nd-order methods, with clear numerical examples and reasonably well written.\n\nThe paper presents several interesting empirical findings, which will no doubt lead to follow up work. However, there are also a few critical issues that may undermine their claims, and that need to be addressed before we can really answer the original question of whether 2nd-order methods are useful for deep learning. \n\n1. There is no complexity comparison, e.g. what is the complexity for a single step of different method.\n\n2. Relatedly, the paper reports the performance over epochs, but it is not clear what \"per epoch\" means for 2nd-order methods.  In particular, it seems to me that they did not count the inner CG iterations, and it is known that this is crucial in running time and important for quality.  If so, then the comparison between 1st-order and 2nd-order methods are not fair or incomplete.\n\n3. The results on 2nd-order methods behave similarly to 1st-order methods, which makes me wonder how many CG iterations they used for 2nd-order method in their experiment, and also the details of the data.  In particular, are they looking at parameter/hyperparameter settings for which 2nd-order methods aren't really necessary.\n\n4. In deep learning setting, the training objective is non-convex, which means the Hessian can be non-PSD.  It is not clear how the stochastic inexact-Newton method mentioned in Section 2.1 could work.  Details on implementations of 2nd-order methods are important here.\n\n5. For 2nd-order methods, the author used line search to tune the step size.  It is not clear in the line search, the author used the whole training objective or batch loss.  Assuming using the batch loss, I suspect the training curve will be very noisy (depending on how large the batch size is).  But the paper only show the average training curves, which might be misleading.\n\nHere are other points.\n\n1. There is no figure showing training/ test accuracy.  Aside from being interested in test error, it is also of interest to see how 2nd order methods are similar/different than 1st order methods on training versus test.\n\n2. Since it is a comparison paper, it only compares three 2nd-order methods with SGD.  The choices made were reasonable, but 2nd-order methods are not as trivial to implement as SGD, and it isn't clear whether they have really \"spanned the space\" of second order methods\n\n3. In the paper, the settings of LeNet, AlexNet are different with those in the original paper.  The authors did not give a reason.\n\n4. The quality of figures is not good.\n\n5. The setting of optimization is not clear, e.g. the learning rate of SGD, the parameter of backtrack line search.  It's hard to reproduce results when these are not described.\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}