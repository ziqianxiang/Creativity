{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper was reviewed by 3 expert reviews and received largely negative reviews, with concerns about the toy-ish nature of the 2D environments and limited novelty.\n\nSince ICLR18 received multiple papers on similar topics, we took additional measures to ensure that papers were similar papers were judged under the same criteria. Specifically, we asked reviewers of (a) this paper and (b) of a concurrent submission that also studies language grounding in 2D environments to provide opinions on (b) and (a) respectively. Unfortunately, while they may be on similar topic and both working on 2D environments, we received unanimous feedback that (b) was much higher quality (\"comparison with multiple baselines, better literature review, no bold claims about visual attention, etc). We realize this may be disappointing but we encourage the authors to incorporate reviewer feedback to make their manuscript stronger. "
    },
    "Reviews": [
        {
            "title": "While the paper aims to address an important and interesting problem, the novelty is limited and the experimental results are not convincing.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "**Paper Summary**\nThe paper studies the problem of navigating to a target object in a 2D grid environment by following given natural language description as well as receiving visual information as raw pixels. The proposed architecture consists of a convoutional neural network encoding visual input,  gated recurrent unit encoding natural language descriptions, an attention mechanism fusing multimodal input, and a policy learning network. To verify the effectiveness of the proposed framework, a new environment is proposed. The environment is 2-D grid based and it consists of an agent, a list of objects with different attributes, and a list of obstacles. Agents perceive the environment throught raw pixels with a limited visible region, and they can perform actions to move in the environment to reach target objects.\n\nThe problem has been studied for a while and therefore it is not novel. The proposed framework is incremental. The proposed environment is trivial and therefore it is unclear if the proposed framework is able to scale up to a more complicated environment. The experiemental results do not support several claims stated in the paper. Overall, I would vote for rejection.\n\n\n    - This paper solves the problem of navigating to the target object specified by language instruction in a 2D grid environment. It requires understanding of language, language grounding for visual features, and navigating to the target object while avoiding non-target objects. An attention mechanism is used to map a language instruction into a set of 1x1 convolutional filters which are intended to distinguish visual features described in the instruction from others. The experimental results show that the proposed method performs better than other methods.\n\n\n    - This paper presents an end-to-end trainable model to navigate an agent through visual sources and natural language instructions. The model utilizes a proposed attention mechanism to draw correlation between the objects mentioned in the instructions with deep visual representations, without requiring any prior knowledge about these inputs. The experimental results demonstrate the effectiveness of the learnt textual representation and the zero-shot generalization capabilities to unseen scenarios. \n\n\n**Paper Strengths**\n- The paper proposes an interesting task which is a navigation task with language instructions. This is important yet relatively unxplored.\n- The implementation details are included, including optimizers, learning rates with weight decayed, numbers of training epochs, the discount factor, etc. \n- The attention mechanism used in the paper is reasonable and the learned language embedding clearly shows meaningful relationships between instructions.\n- The learnt textual representation follows vector arithmetic, which enables the agent to perceive unseen instructions as a new combination of the attributes and perform zero-shot generalization.\n\n\n\n**Paper Weaknesses**\n- The problem of following natural language descriptions together with visual representations of environments is not completely novel. For example, Both the problem and the proposed method are similar to those already introduced in the Gated Attention method (Chaplot et al., 2017). Although the proposed method performs better than the prior work, the approach is incremental. \n\n- The proposed environment is simple. The vocabulary size is 40 and the longest instruciton only consists of 9 words. Whether the proposed framework is able to deal with more complicated environments is not clear. The experimental results shown in Figure 5 is not convincing that the proposed method only took less than 20k iterations to perform almost perfectly. The proposed environment is small and simple compared to the related work. It would be better to test the proposed method in a similar scale with the existing 3D navigation environments (Chaplot et al., 2017 and Hermann et al., 2017).\n\n- The novelty of the proposed framework is unclear. This work is not the first  one which proposes the multimodal fusion network incorporating a CNN achitecture dealing with visual information and a GRU architecture encoding language instructions. Also, the proposed attention mechanism is an obvious choice.\n\n- The shown visualized attention maps are not enough to support the contribution of proposing the attention mechanism. It is difficult to tell whether the model learns to attend to correct objects. Also, the effectiveness of incorporating the attention mechanism is unclear.\n\n- The paper claims that the proposed framework is flexible and is able to handle a rich set of natural language descriptions. However, the experiemental results are not enough to support the claim.\n\n- The presentaiton of the experiment is not space efficient at all.\n\n- The reference of the related papers which fuse multimodal data (vision and language) are missing.\n\n- Comapred to 8 pages was the suggested page limit, 13 pages is a bit too long.\n\n- Stating captions of figures above figures is not recommended.\n\n- It would be better to show where each 1x1 filter for multimodal fusion attends on the input image.  Ideally, one filter should attend on the target object and others should attend on non-target objects. However, I wonder how RNN can generate filters to detect non-target objects given an instruction. Although Figure 6 and Figure 7 try to show insights about the proposed attention model, they don’t tell which kernel is in charge of which visual feature. Blurred attention maps in Figure 6 and 7 make it hard to interpret the behavior of the model. \n\n- The graphs shown in the Figure 5 are hard to interpret because of their large variance. It would be better to smoothing curves, so that comparing methods clearly.\n\n- For zero-shot generalization evaluation, there is no detail about the training steps and comparisons to other methods.\n\n- A highly related paper (Hermann et al., 2017) is missing in the references.\n\n- Since the instructions are simple, the model does not require attention mechanism on the textual sources. If the framework can take more complex language, might be worthwhile to try visual-text co-attention mechanism. Such demonstration will be more convincing.\n\n- The attention maps of different attribute is not as clear as the paper stated. Why do we need several “non-target” objects highlight if one can learn to consolidate all of them?\n\n- The interpretation of n in the paper is vague, the authors should also show qualitatively why n=5 is better than that of n=1,10. If the attention maps learnt are really focusing on different attributes, given more and more objects, shouldn’t n=10 have more information for the policy learning?\n\n- The unseen scenario generalization should also include texture change on the grid environment and/or new attribute combinations on non-target objects to be more convincing.\n\n- The contribution in the visual part is marginal.\n\n\n** Preliminary Evaluation**\n- The modality fusion technique which leads to the attention maps is an effective and seem to work well approach, however, the author should present more thorough ablated analysis. The overall architecture is elegant, but the capability of it to be extended to more complex environment is in doubt. The vector arithmetic of the learnt textual embedding is the key component to enable zero-shot generalization, while the effectiveness of this method is not convincing if more complex instructions such that it contains object-object relations or interactions are perceived by the agent.  \n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Toy experiments; Not much novelty; Missing references",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Paper summary: The paper tackles the problem of navigation given an instruction. The paper proposes an approach to combine textual and visual information via an attention mechanism. The experiments have been performed on a 2D grid, where the agent has partial observation.\n\nPaper Strengths:\n- The proposed approach outperforms the baselines.\n- Generalization to unseen combination of objects and attributes is interesting.\n\nPaper Weaknesses:\nThis paper has the following issues so I vote for rejection: (1) The experiments have been performed on a toy environment, which is similar to the environments used in the 80's. There is no guarantee that the conclusions are valid for slightly more complex environments or real world. I highly recommend using environments such as AI2-THOR or SUNCG. (2) There is no quantitative result for the zero-shot experiments, which is one of the main claims of the paper. (3) The ideas of using instructions for navigation or using attention for combining visual and textual information have been around for a while. So there is not much novelty in the proposed method either. (4) References to attention papers that combine visual and textual modalities are missing.\n\nMore detailed comments:\n\n- Ego-centric is not a correct word for describing the input. Typically, the perspective changes in ego-centric views, which does not happen in this environment.\n\n- I do not agree that the attention maps focus on the right objects. Figures 6 and 7 show that the attention maps focus on all objects. The weights should be shown using a heatmap to see if the model is attending more to the right object.\n\n- I cannot find any table for the zero-shot experiments. In the rebuttal, please point me to the results in case I am missing them.\n\n\nPost Rebuttal:\nI will keep the initial rating. The environment is too simplistic to draw any conclusion from. The authors mention other environments are unstable, but that is not a good excuse. There are various environments that are used by many users. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting Problem, but Limited Novelty and Flawed Evaluation",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Interesting Problem, but Limited Novelty and Flawed Evaluation\n\n\nThe paper considers the problem of following natural language instructions given an first-person view of an a priori unknown environment. The paper proposes a neural architecture that employs an RNN to encode the language input and a CNN to encode the visual input. The two modalities are fused and fed to an RNN policy network. The method is evaluated on a new dataset consisting of short, simple instructions conveyed in simple environments.\n\nThe problem of following free-form navigation instructions is interesting and has achieved a fair bit of attention, previously with \"traditional\" structured approaches (rule-based and learned) and more recently with neural models. Unlike most existing work, this paper reasons over the raw visual input (vs., a pre-processed representation such as a bag-of-words model). HoA notable exception is the work of Chaplot et al. 2017, which addresses the same problem with a nearly identical architecture (see below). Overall, this paper constitutes a reasonable first-pass at this problem, but there is significant room for improvement to address issues related to the stated contributions and flawed evaluations.\n\nThe paper makes several claims regarding the novelty and expressiveness of the model and the contributions of the paper that are either invalid or not justified by the experimental results. As noted, a neural approach to instruction following is not new (see Mei et al. 2016) nor is a multimodal fusion architecture that incorporates raw images (see Chaplot et al.). The paper needs to make the contributions and novelty relative to existing methods clear (e.g., those stated in the intro are nearly identical to those of Mei et al. and Chaplot et al.). This includes discussion of the attention mechanism, for which the contributions and novelty are justified only by simple visualizations that are not very insightful. Related, the paper omits a large body of work in language understanding from the NLP and robotics domains, e.g., the work of Yoav Artzi, Thomas Howard, and Stefanie Tellex, among others (see below). While the approaches are different, it is important to describe this work in the context of these methods.\n\n\nThere are important shortcomings with the evaluation. First, one of the two scenarios involves testing on instructions from the training set. The test set should only include held-out environments and instructions, which the paper incorrectly refers to as the \"zero-shot\" scenario. This test set is very small, with only 19 instructions. Related, there is no mention of a validation set, and the discussion seems to suggest that hyperparameters were tuned on the test set. Further, the method is compared to incomplete implementations of existing baselines that admittedly don't attempt to replicate the baseline architectures. Consequently, it isn't clear what if anything can be concluded from the evaluation. There is a\n\n\n\nComments/Questions\n\n* The action space does not include an explicit stop action. Instead, a run is considered to be finished either when the agent reaches the destination or a timeout is exceeded. This is clearly not valid in practice. The model should determine when to stop, as with existing approaches.\n\n* The paper makes strong claims regarding the sophistication of the dataset that are unfounded. Despite the claims, the environment is rather small and the instructions almost trivially simple. For example, compare to the SAIL corpus that includes multi-sentence instructions with an average of 5 sentences/instruction (vs. 2); 37 words/instruction (vs. a manual cap of 9); and a total of 660 words (vs. 40); and three \"large\" virtual worlds (vs. 10x10 grids with 3-6 objects).\n\n* While the paper makes several claims regarding novelty, the contributions over existing approaches are unclear. For example, Chaplot et al. 2017 propose a similar architecture that also fuses a CNN-based representation of raw visual input with an RNN encoding of language, the result of which is fed to a RNN policy network. What is novel with the proposed approach and what are the advantages? The paper makes an incomplete attempt to evaluate the proposed model against Chaplot et al., but without implementing their complete architecture, little can be inferred from the comparison.\n\n* The paper claims that the fusion method realizes a *minimalistic* representation, but this statement is only justified by an experiment that involves the inclusion of the visual representation, but it isn't clear what we can conclude from this comparison (e.g., was there enough data to train this new representation?).\n\n* It isn't clear that much can be concluded from the attention visualizations in Figs. 6 and 7, particularly regarding its contribution. Regarding Fig 6. the network attends to the target object (large apple), but not the smaller apple, which would be necessary to reason over their relative size. Further, the attention figure in Fig. 7(b) seems to foveate on both bags. In both cases, the distractor objects are very close to the true target, and one would expect the behavior to be similar irrespective of which one was being attended to.\n\n* The conclusion states that the method is \"highly flexible\" and able to handle a \"rich set of natural language instructions\". Neither of these claims are justified by the discussion (please elaborate on what makes the method \"highly flexible\", presumably the end-to-end nature of the architecture) or the experimental results.\n\n* The significance of randomly moving non-target objects that the agent encounters is unclear. What happens when the objects are not moved, as in real scenarios?\n\n* A stated contribution is that the \"textual representations are semantically meaningful\" but the importance is not justified.\n\n* Figure captions should appear below the figure, not at top.\n\n* Figures and tables should appear as close to their first reference as possible (e.g., Table 1 is 6 pages away from its reference at the beginning of Section 7).\n\n\n* Many citations should be enclosed in parentheses.\n\n\n\nReferences:\n\n* Artzi and Zettlemoyer, Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions, TACL 2013\n\n* Howard, Tellex, and Roy, A Natural Language Planner Interface for Mobile Manipulators, ICRA 2014\n\n* Chung, Propp, Walter, and Howard, On the performance of hierarchical distributed correspondence graphs for efficient symbol grounding of robot instructions, IROS 2015\n\n* Paul, Arkin, Roy, and Howard, Efficient Grounding of Abstract Spatial Concepts for Natural Language Interaction with Robot Manipulators, RSS 2016\n\n* Tellex, Kollar, Dickerson, Walter, Banerjee, Teller and Roy, Understanding natural language commands for robotic navigation and mobile manipulation, AAAI 2011",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}