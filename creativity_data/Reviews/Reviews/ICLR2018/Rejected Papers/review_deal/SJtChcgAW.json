{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "Dear authors,\n\nWhile the reviewers appreciated the idea, the significant loss of accuracy was a concern. Even though you made significant changes to the submission, it is unfortunately unrealistic to ask the reviewers to do another review of a heavily modified version in such a short amount of time.\n\nThus, I cannot accept this paper for publication but I encourage you to address the reviewers' concerns and resubmit at a later conference."
    },
    "Reviews": [
        {
            "title": "This paper provides an interesting but incomplete analysis of a NetTrim-inspired pruning algorithm.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper casts the pruning optimization problem of NetTrim as a difference of convex problems, and uses DCA to obtain the smaller weight matrix; this algorithm is also analyzed theoretically to provide a bound on the generalization error of the pruned network.\n\nHowever, there are many questions that aren't answered in the paper that make it difficult to evaluate: in particular, some experimental results leave open more questions for performance analysis. \n\nQuality: of good quality, but incomplete.\nClarity: clear with some typos\nOriginality: a new approach to the NetTrim algorithm, which is somewhat original, and a new generalization bound for the algorithm.\nSignificance: somewhat significant.\n\nPROS\n- A very efficient algorithm for pruning, which can run orders of magnitude faster than the approaches that were compared to on certain architectures.\n- An interesting generalization bound for the pruned network which is in line experimentally with decreasing robustness to pruning on layers close to the input.\n\nCONS\n- Non-trivial loss of accuracy on the pruned network, which cannot be estimated for larger-scale pruning as the experiments only prune one layer.\n- No in-depth analysis of the generalization bound.\n\nMain questions:\n- You mention you use a variant of DCA: could you detail what differences Alg. 2 has with classical DCA?\n- Where do you use the 0-1 loss in Thm. 3.2?\n- I think your result in Theorem 3.2 would be significantly stronger if you could provide an analysis of the bound you obtain: in which cases can we expect certain terms to be larger or smaller, etc.\n- Your experiments in section 4.2 show a non-trivial degradation of the accuracy with FeTa. Although the time savings seem worth the tradeoff to prune *one* layer, have you run the same experiments when pruning multiple layers? Could you comment on how the accuracy evolves with multiple pruned layers?\n- It would be nice to see the curves for NetTrim and/or LOBS in Fig. 2.\n- Have you tried retraining the network after pruning? Did you observe the same behavior as mentioned in (Dong et al., 2017) and (Wolfe et al., 2017)? \n- It would be interseting to plot the theoretical (pessimistic) GE bound as well as the experimental accuracy degradation. \n\nNitpicks:\n-Ubiquitous (first paragraph)\n-difference of convex problemS\n- The references should be placed before the appendix.\n- The amount of white space should be reduced (e.g. around Eq. (1)).",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The manuscript mainly presents a cheap pruning algorithm for dense layers of DNNs. It reformulates the non-convex optimization problem in (Aghasi et al., 2016) as a difference of convex (DC) problem, which can be solved quite efficiently using the DCA algorithm (Tao and An, 1997). The contribution is valuable since the complexity is significantly reduced, but there are many syntax errors and the accuracy of the model is not satisfactory.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The manuscript mainly presents a cheap pruning algorithm for dense layers of DNNs. The proposed algorithm is an improvement of Net-Trim (Aghasi et al., 2016), which is to enforce the weights to be sparse.\n\nThe main contribution of this manuscript is that the non-convex optimization problem in (Aghasi et al., 2016) is reformulated as a difference of convex (DC) problem, which can be solved quite efficiently using the DCA algorithm (Tao and An, 1997). The complexity of the proposed algorithm is much lower than Net-Trim and its fast version LOBS (Dong et al., 2017). The authors also analyze the generalization error bound of DNN after pruning based on the work of (Sokolic et al., 2017).\n\nAlthough this is an incremental work built upon (Aghasi et al., 2016) and an existing algorithm (Tao and An, 1997) is adopted for optimization, the contribution is valuable since the complexity is significantly reduced by utilizing the proposed difference of convex reformulation. Although the main idea is clearly presented, there are many syntax errors and I suggest the authors carefully checking the manuscript.\n\nPros:\n1.\tThe motivation is clear and the presented reformulation is reasonable.\n\n2.\tThe generalization error analysis and the conclusion of “layers closer to the input are exponentially less robust to pruning” is interesting.\n\nCons:\n1.\tThere are many syntax errors, e.g., “Closer to our approach recently in Aghasi et al. (2016) the authors”, “an cheap pruning algorithm”, etc. Besides, there is no discussion for the results in Table 1.\n\n2.\tAlthough the complexity of the proposed method is much lower than the compared approaches (Net-Trim and LOBS), there seems to be a large sacrifice on accuracy. For example, the accuracy drops from 95.2% to 91% compared with Net-Trim in the LeNet-5 model and from 80.5% to 74.6% compared with LOBS in the CifarNet model. The proposed method is only better than hard-thresholding.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting use of DC functions",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The problem of pruning DNNs is an active area of study.\nThis paper addresses this problem by posing the Net-trim objective function as  a Difference of convex(DC) function. This allows for an immediate application of DC function minimization using existing techniques. An analysis of Generalization error \nis also given. \n\nThe main novelty seems to be the interesting connection to DC function minimization. The benefits seem to be a faster algorithm for pruning. \n\nAbout the generalization error the term C_2 needs to be more well defined otherwise the coefficient of  A would be -ve which may lead to complications.\n\nExperimental investigations are reasonable and the results are convincing.\n\nA list of Pros:\n1. Interesting connection to DC function\n2. Attempt to analyze generalization error \n3. Faster speed of convergence empirically\n\nA list of Cons:\n1. The contribution in posing the objective as a DC function looks limited as it is very straightforward. Also the algorithm is \ndirect application\n2. The time complexity analysis is imprecise. Since the proposed algorithm is iterative time complexity would depend on the number of iterations.\n\n\n\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}