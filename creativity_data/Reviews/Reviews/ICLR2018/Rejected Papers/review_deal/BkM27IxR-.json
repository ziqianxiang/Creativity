{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The presented work is a good attempt to expand the work of Li and Malik to the high-dimensional, stochastic setting. Given the reviewer comments, I think the paper would benefit from highlighting the comparatively novel aspects, and in particular doing so earlier in the paper.\n\nIt is very important, given the nature of this work, to articulate how the hyperparameters of the learned optimizers, and of the hand-engineered optimizers are chosen. It is also important to ensure that the amount of time spent on each is roughly equal in order to facilitate an apples-to-apples comparison.\n\nThe chosen architectures are still quite small compared to today's standards. It would be informative to see how the learned optimizers compare on realistic architectures, at least to see the performance gap.\n\nPlease clarify the objective being optimized, and it would be useful to report test error.\n\nThe approach is interesting, but does not yet meet the threshold required for acceptance."
    },
    "Reviews": [
        {
            "title": "Learning to Optimize Neural Nets ",
            "rating": "6: Marginally above acceptance threshold",
            "review": "Summary of the paper\n---------------------------\nThe paper derives a scheme for learning optimization algorithm for high-dimensional stochastic problems as the one involved in shallow neural nets training. The main motivation is to learn to optimize with the goal to design a meta-learner able to generalize across optimization problems (related to machine learning applications as learning a neural network) sharing the same properties. For this sake, the paper casts the problem into reinforcement learning framework and relies on guided policy search (GPS) to explore the space of states and actions. The states are represented by the iterates, the gradients, the objective function values, derived statistics and features, the actions are the update directions of parameters to be learned. To make the formulated problem tractable, some simplifications are introduced (the policies are restricted to gaussian distributions family, block diagonal structure is imposed on the involved parameters). The mean of the stationary non-linear policy of GPS is modeled as a recurrent network with parameters to be learned. A hatch of how to learn the overall process is presented. Finally experimental evaluations on synthetic or real datasets are conducted to show the effectiveness of the approach.\n\nComments\n-------------\n- The overall idea of the paper, learning how to optimize, is very seducing and the experimental evaluations (comparison to normal optimizers and other meta-learners) tend to conclude the proposed method is able to learn the behavior of an optimizer and to generalize to unseen problems.\n- Materials of the paper sometimes appear tedious to follow, mainly in sub-sections 3.4 and 3.5. It would be desirable to sum up the overall procedure in an algorithm. Page 5, the term $\\omega$ intervening in the definition of the policy $\\pi$ is not defined.\n- The definitions of the statistics and features (state and observation features) look highly elaborated. Can authors provide more intuition on these precise definitions? How do they impact for instance changing the time range in the definition of $\\Phi$) in the performance of the meta-learner?\n- Figures 3 and 4 illustrate some oscillations of the proposed approach. Which guarantees do we have that the algorithm will not diverge as L2LBGDBGD does? How long should be the training to ensure a good and stable convergence of the method?\n- An interesting experience to be conducted and shown is to train the meta-learner on another dataset (CIFAR for example) and to evaluate its generalization ability on the other sets to emphasize the effectiveness of the method. ",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper proposed a reinforcement learning (RL) based method to learn an optimal optimization algorithm for training shallow neural networks. This work is an extended version of [Li &Malik 2016] aiming to address the high-dimensional problem.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper proposed a reinforcement learning (RL) based method to learn an optimal optimization algorithm for training shallow neural networks. This work is an extended version of [1], aiming to address the high-dimensional problem.\n\n\n\nStrengths:\n\nThe proposed method has achieved a better convergence rate in different tasks than all other hand-engineered algorithms.\nThe proposed method has better robustess in different tasks and different batch size setting.\nThe invariant of coordinate permutation and the use of block-diagonal structure improve the efficiency of LQG.\n\n\nWeaknesses:\n\n1. Since the batch size is small in each experiment, it is hard to compare convergence rate within one epoch. More iterations should be taken and the log-scale style figure is suggested. \n\n2. In Figure 1b, L2LBGDBGD converges to a lower objective value, while the other figures are difficult to compare, the convergence value should be reported in all experiments.\n\n3. “The average recent iterate“ described in section 3.6 uses recent 3 iterations to compute the average, the reason to choose “3”, and the effectiveness of different choices should be discussed, as well as the “24” used in state features.\n\n4. Since the block-diagonal structure imposed on A_t, B_t, and F_t, how to choose a proper block size? Or how to figure out a coordinate group?\n\n5. The caption in Figure 1,3, “with 48 input and hidden units” should clarify clearly.\nThe curves of different methods are suggested to use different lines (e.g., dashed lines) to denote different algorithms rather than colors only.\n\n6. typo: sec 1 parg 5, “current iterate” -> “current iteration”.\n\n\nConclusion:\n\nSince RL based framework has been proposed in [1] by Li & Malik, this paper tends to solve the high-dimensional problem. With the new observation of invariant in coordinates permutation in neural networks, this paper imposes the block-diagonal structure in the model to reduce the complexity of LQG algorithm. Sufficient experiment results show that the proposed method has better convergence rate than [1]. But comparing to [1], this paper has limited contribution.\n\n[1]: Ke Li and Jitendra Malik. Learning to optimize. CoRR, abs/1606.01885, 2016.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "See below for details",
            "rating": "5: Marginally below acceptance threshold",
            "review": "[Main comments]\n\n* I would advice the authors to explain in more details in the intro\nwhat's new compared to Li & Malik (2016) and Andrychowicz et al. (2016).\nIt took me until section 3.5 to figure it out.\n\n* If I understand correctly, the only new part compared to Li & Malik (2016) is\nsection 3.5, where block-diagonal structure is imposed on the learned matrices.\nIs that correct?\n\n* In the experiments, why not comparing with Li & Malik (2016)? (i.e., without\n  block-diagonal structure)\n\n* Please clarify whether the objective value shown in the plots is wrt the training\n  set or the test set. Reporting the training objective value makes little\nsense to me, unless the time taken to train on MNIST is taken into account in\nthe comparison. \n\n* Please clarify what are the hyper-parameters of your meta-training algorithm\n  and how you chose them.\n\nI will adjust my score based on the answer to these questions.\n\n[Other comments]\n\n* \"Given this state of affairs, perhaps it is time for us to start practicing\n  what we preach and learn how to learn\"\n\nThis is in my opinion too casual for a scientific publication...\n\n* \"aim to learn what parameter values of the base-level learner are useful\n  across a family of related tasks\"\n\nIf this is essentially multi-task learning, why not calling it so?  \"Learning\nwhat to learn\" does not mean anything.  I understand that the authors wanted to\nhave \"what\", \"which\" and \"how\" sections but this is not clear at all.\n\nWhat is a \"base-level learner\"? I think it would be useful to define it more\nprecisely early on.\n\n* I don't see the difference between what is described in Section 2.2\n  (\"learning which model to learn\") and usual machine learning (searching for\nthe best hypothesis in a hypothesis class).\n\n* Typo: p captures the how -> p captures how\n\n* The L-BFGS results reported in all Figures looked suspicious to me.  How do you\n  explain that it converges to a an objective value that is so much worse?\nMoreover, the fact that there are huge oscillations makes me think that the\nauthors are measuring the function value during the line search rather than\nthat at the end of each iteration.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}