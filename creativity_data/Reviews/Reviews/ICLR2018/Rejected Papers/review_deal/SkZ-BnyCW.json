{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The reviewers agreed that while this is a well-written paper, it is low on novelty and does not make a substantial enough contribution. They also pointed out that although the reported MNIST results are highly competitive, possibly due to the use of a powerful ResNet decoder, the CIFAR10/ImageNet results are underwhelming."
    },
    "Reviews": [
        {
            "title": "The paper proposes to augment a variational auto encoder (VAE) with an binary restricted Boltzmann machine (RBM) in the role of the prior of the generative model. Clarity could be improved in some aspects and the advantages of the proposed model compared to existing ones did not become totally clear to me.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Summary of the paper:\nThe paper proposes to augment a variational auto encoder (VAE) with an binary restricted Boltzmann machine (RBM) in the role of the prior of the generative model. To yield a good initialisation of the parameters of the RBM and the inference network a special pertaining procedure is introduced. The model produces competitive Likelihood results on MNIST and was further tested on CIFAR 10. \n\nClarity and quality: \n\n1. From the description of the pertaining procedure and the appendix B I got the impression that the inference network maps into [0,1] and not into {0,1}.  Does it mean, you are not really considering binary latent variables (making the RBM model the values in [0,1] by its probability p(z|h))? \n\n2. on page 2:\nRWS....\"derive a tighter lower bound\": Where does the \"tighter\" refer to? \n\n3. \"multivariate Bernoulli modeled by an RBM\": Note,  while in a multivariate Bernoulli the binary variables would be independent from each others, this is usually not the case for the visible variables of RBMs (only in the conditional distribution given the state of the hidden variables).\n\n4. The notation could be improved, e.g.:\n-x_data and x_sample are not explained\n- M is not defined in equation 5. \n\n5. \"this training method has been previously used to produce the best results on MNIST\" Note, that parallel tempering often leads to better results when training RBMs (see http://proceedings.mlr.press/v9/desjardins10a/desjardins10a.pdf) . Furthermore, centred RBMs are also get better results than vanilla RBMs (see: http://jmlr.org/papers/v17/14-237.html).\n\nOriginality and significance:\nAs already mentioned in a comment on open-review the current version of the paper misses to mention one very related work: \"discrete variational auto encoders\". Also \"bidirectional Helmholtz machines\" could be mentioned as generative model with discrete latent variables.  The results for both should also be reported in Table 1 (discrete VAEs: 81,01, BiHMs: 84,3). \n\nFrom the motivation the advantages of the model did not become very clear to me. Main advantage seems to be the good likelihood result on MNIST (but likelihood does not improve compared to IWAE on CIFAR 10 for example). However, using an RBM as prior has the disadvantage that sampling from the generative model requires running a Markov chain now while having a solely directed generative model allows for fast sampling. \n\nExperiments show good likelihood results on MNIST. Best results are obtained when using a ResNet decoder. I wondered how much a standard VAE is improved by using such a powerful decoder. Reporting this, would allow to understand, how much is gained from using a RBM for learning the prior. \n\nMinor comments:\npage 1:\n\"debut of variational auto encoder (VAE) and reparametrization trick\" -> debut of variational auto encoders (VAE) and the reparametrization trick\",\npage 2:\n\"with respect to the parameter of  p(x,z)\" -> \"with respect to the parameters of  p(x,z)\"\n\"parameters in p\" -> \"parameters of p\" \n\"is multivariate Bernoulli\" ->  \"is a multivariate Bernoulli\"\n\"we compute them\" -> \"we compute it\" \npage 3:\n\"help find a good\" ->  \"help to find a good\"\npage 7:\n\"possible apply\" -> \"possible to apply\"",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "OK, but fairly incremental and results underwhelming",
            "rating": "5: Marginally below acceptance threshold",
            "review": "While I acknowledge that training generative models with binary latent variables is hard, I'm not sure this paper really makes valuable progress in this direction. The only results that seem promising are those on binarized MNIST, for the non-convolutional architecture, and this setting isn't particularly exciting. All other experiments seem to suggest that the proposed model/algorithm is behind the state of the art. Moreover, the proposed approach is fairly incremental, compared to existing work on RWS, VIMCO, etc.\n\nSo while this work seem to have been seriously and thoughtfully executed, I think it falls short of the ICLR acceptance bar.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting work",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Interesting work, but I’m not convinced by the arguments nor by the experiments. Similar models have been trained before; it’s not clear that the proposed pretraining procedure is a practical step forwards. And quite some decisions seem ad-hoc and not principled. \n\nNevertheless, interesting work for everyone interested in RBMs as priors for “binary VAEs”. \n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}