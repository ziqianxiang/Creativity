{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "the paper validates the benefit of multi-task learning on MNIST datasets, which is not sufficient for ICLR publication"
    },
    "Reviews": [
        {
            "title": "The paper applies multi-task learning to MNIST type image datasets and gets reasonably interesting and expected results. However, there is not too much novelty in methodology or formalism in the work.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper applies multi-task learning to MNIST (M), FashionNIST (F), and NotMNIST (N) datasets. That is, the authors first train a neural network (with a specific architecture; in this case, it is an all-convolutional network) on a combination of the datasets (M+F; F+N; N+M; M+F+N) and then use the learned weights (in all but the output layer) to initialize the weights for task-specific training on each of the datasets. The authors observe that for each of the combinations, the above approach does better than training on a dataset individually. Further, in all but one case, initializing weights based on training on M+F+N gives the best performance. The improvements are not striking but are noticeable. ",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The manuscript applies multi-task learning to the three MNIST-like datasets by simply pre-training the parameters of the networks using samples of datasets. The presented idea is quite simple and there is no technique contribution.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The manuscript mainly utilizing the data from all three MNIST-like datasets to pre-train the parameters of joint classification networks, and the pre-trained parameters are utilized to initialize the disjoint classification networks (of the three datasets).\n\nThe presented idea is quite simple and the authors only re-affirm that multi-task learning can lead to performance improvement by simultaneously leverage the information of multiple tasks. There is no technique contribution.\n\nPros:\n1.\tThe main idea is clearly presented.\n2.\tIt is interesting to visualize the results obtained with/without multi-task learning in Figure 6.\n\nCons:\n1.\tThe contribution is quite limited since the authors only apply multi-task learning to the three MNIST-like datasets and there is no technique contribution.\n2.\tThere is no difference between the architecture of the single-task learning network and multi-task learning network.\n3.\tMany unclear points, e.g., there is no description for “zero-padding” and why it can enhance target label. What is the “two-stage learning rate decay scheme” and why it is implemented? It is also unclear what can we observed from Figure 4.\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "a paper with limited novelty",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper presents a multi-task neural network for classification on MNIST-like datasets.\n\nThe main concern is that the technical innovation is limited. It is well known that multi-task learning can lead to performance improvement on similar tasks/datasets. This does not need to be verified in MNIST-like datasets. The proposed multi-task model is to fine tune a pretrained model, which is already a standard approach for multi-task and transfer learning. So the novelty of this paper is very limited.\n\nThe experiments do not bring too much insights.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}