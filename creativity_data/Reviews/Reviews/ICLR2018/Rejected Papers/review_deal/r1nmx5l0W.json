{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "Pros and cons of the paper can be summarized as follows:\n\nPros:\n* The underlying idea may be interesting\n* Results are reasonably strong on the test set used\n\nCons:\n* Testing on the single dataset indicates that the model may be of limited applicability\n* As noted by reviewer 2, core parts of the paper are extremely difficult to understand, and the author response did little to assuage these concerns\n* There is little mathematical notation, which compounds the problems of clarity\n\nAfter reading the method section of the paper, I agree with reviewer 2: there are serious clarity issues here. As a result, I do cannot recommend that this paper be accepted to ICLR in its current form. I would suggest the authors define their method precisely in mathematical notation in a future submission."
    },
    "Reviews": [
        {
            "title": "Interesting idea, but requires more datasets to show.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper proposed a method that tries to generate both accurate and diverse samples from RNNs. \nI like the basic intuition of this paper, i.e., using mistakes for creativity and refining on top of it. I also think the evaluation is done properly. I think my biggest concern is that the method was only tested on a single dataset hence it is not convincing enough. Also on this particular dataset, the method does not seem to strongly dominate the other methods. Hence it's not clear how much better this method is compared to previously proposed ones.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Very interesting approach for training a sequential generator adversarially",
            "rating": "7: Good paper, accept",
            "review": "Overall the paper is good: good motivation, insight, the model makes sense, and the experiments / results are convincing. I would like to see some evidence though that the strong generator is doing exactly what is advertised: that it’s learning to clean up the mistakes from variation. Can we have some sort of empirical analysis that what you say is true? \n\nThe writing grammar quality fluctuates. Please clean up.\n\nDetailed notes\nP1:\nWhy did you pass on calling it Self-improving collaborative adversarial learning (SICAL)?\nI’m very surprised you don’t mention VAE RNN here (Chung et al 2015) along with other models that leverage an approximate posterior model of some sort.\n\nP2:\nWhat about scheduled sampling?\nIs the quality really better? How do you quantify that? To me the ones at the bottom of 2(c) are of both lower quality *and* diversity.\n“Figure 2(d) displays human-drawn sketches of fire trucks which demonstrate that producing sequences–in this case sketches–with both quality and variety is definitely achievable in real-world applications”: I’m not sure I follow this argument. Because people can do it, ML should be able to?\n\nP3:\n“Recently, studies start to apply GANs to generate the sequential output”: fix this\nGrammar takes a brief nose-dive around here, making it a little harder to read.\nCaption: “bean search”\nChe et al also uses something close to Reinforcement learning for discrete sequences.\n“nose-injected”: now you’re just being silly\nMaybe cite Bahdanau et al 2016 “An actor-critic algorithm for sequence prediction”\n“does not require any variety reward/measure to train” What about the discriminator score (MaliGAN / SeqGAN)? Could this be a simultaneous variety + quality reward signal? If the generator is either of poor-quality or has low variety, the discriminator could easily distinguish its samples from the real ones, no?\n\nP6:\nDid you pass only the softmax values to the discriminator?\n\nP7:\nI like the score scheme introduced here. Do you see any connection to inception score?\nSo compared to normal GAN, does SIC-GAN have more parameters (due to the additional input)? If so, did you account for this in your experiments?",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A novel architecture for generating greater variety on QuickDraw dataset, but seems confused about what it's actually doing.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper baffles me. It appears to be a stochastic RNN with skip connections (so it's conditioned on the last two states rather than last one) trained by an adversarial objective (which is no small feat to make work for sequential tasks) with results shown on the firetruck category of the QuickDraw dataset. Yet the authors claim significantly more importance for the work than I think it merits.\n\nFirst, there is nothing variational about their variational RNN. They seem to use the term to be equivalent to \"stochastic\", \"probabilistic\" or \"noisy\" rather than having anything to do with optimizing a variational bound. To strike the right balance between pretension and accuracy, I would suggest substituting the word \"stochastic\"  everywhere \"variational\" is used.\n\nSecond, there is nothing self-improving or collaborative about their self-improving collaborative GAN. Once the architecture is chosen to share the weights between the weak and strong generator, the only difference between the two is that the weak generator has greater noise at the output. In this sense the architecture should really be seen as a single model with different noise levels at alternating steps. In this sense, I am not entirely clear on what the difference is between the SIC-GAN and their noisy GAN baseline - presumably the only difference is that the noisy GAN is conditioned on a single timestep instead of two at a time? The claim that these models are somehow \"self-improving\" baffles me as well - all machine learning models are self-improving, that is the point of learning. The authors make a comparison to AlphaGo Zero's use of self-play, but here the weak and strong generators are on the same side of the game, and because there are no game rules provided beyond \"reproduce the training set\", there is no possibility of discovery beyond what is human-provided, contrary to the authors' claim.\n\nThird, the total absence of mathematical notation made it hard in places to follow exactly what the models were doing. While there are plenty of papers explaining the GAN framework to a novice, at least some clear description of the baseline architectures would be appreciated (for instance, a clearer explanation of how the SIC-GAN differs from the noisy GAN). Also the description of the soft $\\ell_1$ loss (which the authors call the \"1-loss\" for some reason) would benefit from a clearer mathematical exposition.\n\nFourth, the experiments seem too focused on the firetruck category of the QuickDraw dataset. As it was the only example shown, it's difficult to evaluate their claim that this is a general method for improving variety without sacrificing quality. Their chosen metrics for variety and detail are somewhat subjective, as they depend on the fact that some categories in the QuickDraw dataset resemble firetrucks in the fine detail while others resemble firetrucks in outline. This is not a generalizable metric. Human evaluation of the relative quality and variety would likely suffice.\n\nLastly, the entire section on the strong-weak collaborative GAN seems to add nothing. They describe an entire training regiment for the model, yet never provide any actual experimental results using that model, so the entire section seems only to motivate the SIC-GAN which, again, seems like a fairly ordinary architectural extension to GANs with RNN generators.\n\nThe results presented on QuickDraw do seem nice, and to the best of my knowledge it is the first (or at least best) applications of GANs to QuickDraw - if they refocused the paper on GAN architectures for sketching and provided more generalizable metrics of quality and variety it could be made into a good paper.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}