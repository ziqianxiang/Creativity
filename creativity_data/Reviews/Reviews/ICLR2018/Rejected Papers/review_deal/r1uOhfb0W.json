{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper is interesting since it goes to showing the role of model averaging. The clarifications made improve the paper, but the impact of the paper is still not realised: the common confusion on the retraining can be re-examined, clarifications in the methodology and evaluation, and deeper contextulaisation of the wider literature."
    },
    "Reviews": [
        {
            "title": "A useful approach for making model averaging more feasible",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The authors note that several recent papers have shown that bayesian model averaging is an effective and universal way to improve hold-out performance, but unfortunately are limited by increased computational costs.   Towards that end, the authors of this manuscript propose several modifications to this procedure to make it computationally feasible and indeed improve performance.\n\nPros:\nThe authors demonstrate an effective procedure for FNN and LSTMs that makes model averaging improve performance.\nEmpirical evidence is convincing on the utility of the approach.\n\nCons:\nNot clear how this approach would be used with convolutional structures\nMuch of the benefit appears to come from the sparse prior, pruning, and retraining (Figure 3).  The model averaging seems to have a smaller contribution.  Due to that, it seems that the nature of the contribution needs to be clarified compared to the large literature on sparsifying neural networks, and the introductory comments of the paper should be rewritten to reflect that reality.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A more principled way for training ensemble neural networks, nice idea and experiments",
            "rating": "6: Marginally above acceptance threshold",
            "review": "In this paper, the authors present a new framework for training ensemble of neural networks. The approach is based on the recent scalable MCMC methods, namely the stochastic gradient Langevin dynamics.\n\nThe paper is overall well-written and ideas are clear. The main contributions of the paper, namely using SG-MCMC methods within deep learning, and then increasing the computational efficiency by group sparsity+pruning are valuable and can have a significant impact in the domain. Besides, the proposed approach is more elegant the competing ones, while still not being theoretically justified completely. \n\nI have the following minor comments:\n\n1) The authors mention that retraining significantly improves the performance, even without pruning. What is the explanation for this? If there is no pruning, I would expect that all the samples would converge to the same minimum after retraining. Therefore, the reason why retraining improves the performance in all cases is not clear to me.\n\n2) The notation |\\theta_g| is confusing, the authors should use a different symbol.\n\n3) After section 4, the language becomes quite informal sometimes, the authors should check the sentences once again.\n\n4) The results with SGD (1 model) + GSP + PR should be added in order to have a better understanding of the improvements provided by the ensemble networks. \n\n5) Why does the performance get worse \"obviously\" when the pruning is 95% and why is it not obvious when the pruning is 90%?\n\n6) There are several typos\n\npg7: drew -> drawn\npg7: detail -> detailed\npg7: changing -> challenging\npg9: is strongly depend on -> depends on\npg9: two curve -> two curves",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Justification for the proposed algorithm is weak + weak experiments.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The authors propose a procedure to generate an ensemble of sparse structured models. To do this, the authors propose to (1) sample models using SG-MCMC with group sparse prior, (2) prune hidden units with small weights, (3) and retrain weights by optimizing each pruned model. The ensemble is applied to MNIST classification and language modelling on PTB dataset. \n\nI have two major concerns on the paper. First, the proposed procedure is quite empirically designed. So, it is difficult to understand why it works well in some problems. Particularly. the justification on the retraining phase is weak. It seems more like to use SG-MCMC to *initialize* models which will then be *optimized* to find MAP with the sparse-model constraints. The second problem is about the baselines in the MNIST experiments. The FNN-300-100 model without dropout, batch-norm, etc. seems unreasonably weak baseline. So, the results on Table 1 on this small network is not much informative practically. Lastly, I also found a significant effort is also desired to improve the writing. \n\nThe following reference also needs to be discussed in the context of using SG-MCMC in RNN.\n- \"Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling\", Zhe Gan*, Chunyuan Li*, Changyou Chen, Yunchen Pu, Qinliang Su, Lawrence Carin",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}