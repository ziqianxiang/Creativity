{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper presents a technique for feature map compression at inference time. As noted by reviewers, the main concern is that the method is applied to one NN architecture (SqueezeNet), which severely limits its impact and applicability to better performing state-of-the-art models."
    },
    "Reviews": [
        {
            "title": "high compression rate for marginal accuracy loss, but approach requires specialized architecture",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Strengths:\n- Unlike most previous approaches that suffer from significant accuracy drops for good feature map compression, the proposed method achieves reductions in feature map sizes of 1 order of magnitude at effectively no loss in accuracy.\n- Technical approach relates closely to some of the prior approaches (e.g., Iandola et al. 2016) but can be viewed as learning the quantization rather than relying on a predefined one.\n- Good results on both large-scale classification and object detection.\n- Technical approach is clearly presented.\n\nWeaknesses:\n- The primary downside is that the approach requires a specialized architecture to work well (all experiments are done with SqueezeNets). Thus, the approach is less general than prior work, which can be applied to arbitrary architectures.\n- From the experiments it is not fully clear what is the performance loss due to having to use the SqueezeNet architecture rather than state-of-the-art models. For example, for the image categorization experiment, the comparative baselines are for AlexNet and NIN, which are outdated and do not represent the state-of-the-art in this field. The object detection experiments are based on a variant of Faster R-CNN where the VGG16 feature extractor is replaced with a SqueezeNet model. However, the drop in accuracy caused by this modification is not discussed in the paper and, in any case, there are now much better models for object detection than Faster R-CNN.\n- In my view the strengths of the approach would be more convincingly conveyed visually with a plot reporting accuracy versus memory usage, rather than by the many numerical tables in the paper.\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "initial review",
            "rating": "7: Good paper, accept",
            "review": "The method of this paper minimizes the memory usage of the activation maps of a CNN. It starts from a representation where activations are compressed with a uniform scalar quantizer and fused to reduce intermediate memory usage. This looses some accuracy, so the contribution of the paper is to add a pair of convolution layers in the binary domain (GF(2)) that are trained to restore the lost precision. \n\nOverall, this paper seems to be a nice addition to the body of works on network compression. \n\n+ : interesting approach and effective results. \n\n+ : well related to the state of the art and good comparison with other works. \n\n- : somewhat incremental. Most of the claimed 100x compression is due to previous work.\n\n- : impact on runtime is not reported. Since there is a caffe implementation it would be interesting to have an additional column with the comparative execution speeds, even if only on CPU. I would expect the FP32 timings to be hard to beat, despite the claims that it uses only binary operations.\n\n- : the paper is sometimes difficult to understand (see below)\n\ndetailed comments: \n\nEquations (3)-(4) are difficult to understand. If I understand correctly, b just decomposes a \\hat{x} in {0..2^B-1} into its B bits \\tilda{x} \\in {0,1}^B, which can be then considered as an additional dimension in the activation map where \\hat{x} comes from. \n\nIt is not stated clearly whether P^l and R^l have binary weights. My understanding is that P^l has but R^l not.\n\n4.1 --> a discussion of the large mini-batch size (1024) could be useful. My understanding is that large mini-batches are required to use averaged gradients and get smooth updates. \n\nend of 4.1 --> unclear what \"equivalent bits\" means\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "DNN Feature Map Compression using Learned Representation",
            "rating": "5: Marginally below acceptance threshold",
            "review": "In order to compress DNN intermediate feature maps the authors covert fixed-point activations into vectors over the smallest finite field, the Galois field of two elements (GF(2)) and use nonlinear dimentionality reduction layers.\n\nThe paper reads well and the methods and experiments are generally described in sufficient detail.\n\nMy main concern with this paper and approach is the performance achieved. According to Table 1 and Table 2 there is a small accuracy benefit from using the proposed approach over the \"quantized\" SqueezeNet baseline. If I am weighing in the need to alter the network for the proposed approach in comparison with the \"quantized\" setting then, from practical point of view, I would prefer the later \"quantized\" approach.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}