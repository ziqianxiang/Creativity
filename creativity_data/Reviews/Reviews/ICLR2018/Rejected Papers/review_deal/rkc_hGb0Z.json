{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The reviewers are unanimous that the paper is not sufficiently clear and could be improved with better empirical results."
    },
    "Reviews": [
        {
            "title": "Review",
            "rating": "5: Marginally below acceptance threshold",
            "review": "There are two anonymity violations in the paper. The first is in the sentence \"The 7-DoF robot result presented shortly previously appeared in our abstract that introduced robust GPS [21]\". The second is in the first linked video, which links to a non-anonymized youtube video. The second linked video, a dropbox link, does not have the correct permissions set, and thus cannot be viewed. Also, the citation style does not seem to follow the ICLR style guidelines.\n\nDisregarding the anonymity and style violations, I will review the paper.  I do not have background in H_inf control theory, but I will review the paper to the best of my ability.\n\nThis paper proposes a guided policy search method for training deep neural network policies that are robust to worst-case additive disturbances. To my knowledge, the approach presented in the paper is novel, though some relevant references are missing. The experimental results demonstrate the method on two simulated experimental domains, demonstrating robustness to adversarial perturbations. The paper is generally well-written, but has some bugs and typos. The paper is substantially longer than the strongly suggested page limit. There are parts of the paper that should be moved to an appendix to accommodate the page limit.\n\nRegarding the experiments:\nMy main concerns are with regard to the completeness of the experiments. First, the experimental results report performance in terms of cost/reward, which is extremely difficult to interpret. It would be helpful to also provide success rate for all experiments, where the authors can define success as, e.g. getting the peg in the hole or being within a certain threshold of the goal.\nSecond, the paper should provide a comparison of policy robustness between the proposed approach and (1) a policy trained with standard GPS, (2) a policy trained with GPS and random perturbations, and ideally, (3) prior approaches to robustness, e.g. Pinto et al., Madelkar et al. [1], or Rajeswaran et al. [2].\n\nRegarding related work and clarity:\nThere are a few papers that consider the problem of building deep neural network policies that are robust [1,2] that should be discussed and ideally compared to.\nRecent deep reinforcement learning work has studied the problem of robustness to adversarial perturbations in the observation space, e.g. [3,4,5,6]. As such, it would be helpful to clarify in the introduction that this paper is considering additive perturbations in the action space.\nThe paper switches between using rewards and costs. It would be helpful to pick one term and stick with it for the entire paper, rather than switching. Further, it seems like there are errors due to the switching. e.g. on page 3, \\ell is defined as the expected reward and in equation 3, it seems like the protaganist policy is trying to minimize \\ell, contradicting the earlier definition.\nLastly, section 5.1 is currently rather difficult to follow. It would help to see more top-down direction in the derivation and more details in section 5.1, 5.2, and 5.3 to be moved to an appendix.\n\nRegarding correctness:\nThere seem to be some issues in the math and/or notation:\nThe most major issue is in Algorithm 2, which is probably the most important part of the paper to be correct, given that it provides a complete picture of the algorithm. I believe that steps 3 and 4 are incorrect and/or incomplete. Step 4 should be referring to the local policy p rather than the global policy pi (assuming the notation in sections 5.1 and 5.3). Also, the local policy p(v|x) appears nowhere in the algorithm, and probably should appear in step 4. In step 5, how is this regression different from step 7?\nIn equation 1 and elsewhere in section 4, there is a mix of notation, using u and z inter-changably. (e.g. in equation 1 and the following equation, I believe the u should be switched to z or the z should be switched to u).\n\nMinor feedback:\n> \"requires fewer samples to generalize to the real world\"\nNone of these experiments are in the real world, so the term \"real world\" should not be used.\n> \"algoruithm\" -> \"algorithm\"\n> \"when unexpected such as when there exists\" - bad grammar / typo\n> \"ertwhile sensy-sensitivity parameter\" - typo\n- reference 1 is missing authors\n\nIn summary, I think the paper would be significantly improved with further experimental comparisons, further discussion of related work, and clarifications/corrections on the notation, equations, and algorithms.  In its current form, I don't think the paper is fit for publication. My rating is between a 4 and a 5.\n\n[1] http://vision.stanford.edu/pdf/mandlekar2017iros.pdf\n[2] https://arxiv.org/abs/1610.01283\n[2] https://arxiv.org/abs/1701.04143\n[3] https://arxiv.org/abs/1702.02284\n[4] https://www.ijcai.org/proceedings/2017/0525.pdf\n[5] https://arxiv.org/abs/1705.06452",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting idea and good discussion, but unclear method and results",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The authors propose to incorporate elements of robust control into guided policy search, in order to devise a method that is resilient to perturbations and (presumably) model mismatch.\n\nThe idea behind the method and the discussion in the introduction and related work is interesting and worthwhile, and I think that combining elements from robust control and reinforcement learning is a very promising direction to explore. However, in its present state, the paper is very hard to evaluate, perhaps because the submission was a bit rushed. It may be that the authors can clarify some of these issues in the response period.\n\nFirst, the authors repeatedly state that perturbations are applied to the policy parameters. This seems very strange to me, as typically robust control considers perturbations to the state or control. And reading the actual method, I can't actually figure out how perturbations are applied to the parameters -- as near as I can tell, the perturbations are indeed applied to the controls. So which is it?\n\nThere is quite a lot of math in the derivation, and it's unclear which parts relate to the standard guided policy search algorithm, and which parts are new. After reading the technical sections several times, my best guess is that the method corresponds to using an adversarial trajectory optimization setup to generate supervision for training a policy. So only the trajectory optimization phase is actually different. Is that true? Or are there other modifications? Some sort of summary of the overall method would have been appreciated, or else a clearer separation of new and old components.\n\nThe evaluation also leaves a lot to be desired. What kind of perturbations are actually being considered? Are they all adversarial perturbations? Do the authors actually test model mismatch or other more natural conditions where robustness would be beneficial? In the end, I was unable to really interpret what the experiments are trying to get across, which makes it hard for me to tell if the method actually works or improves on anything.\n\nIn its present state, the paper is very hard to parse, and the evaluation appears too rushed for me to be able to deduce how well the method works. Hopefully the authors can clarify some of these issues in the response period.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Promising approach, but oversold and lacking good experimental evidence.",
            "rating": "3: Clear rejection",
            "review": "The paper presents a method for evaluating the sensitivity and robustness of deep RL policies, and proposes a dynamic game approach for learning robust policies.\n\nThe paper oversells the approach in many ways. The authors claim that \"experiments confirm that state-of-the-art reinforcement learning algorithms fail in the presence of additive disturbances, making them brittle when used in situations that call for robustness\". However, their methods and experiments are only applied to Guided Policy Search (GPS), which seems like a specialized RL algorithm. Conclusions drawn from empirically running GPS on a problem cannot be generalized to all \"state-of-the-art RL algorithms\".\n\nIn Fig 3, the authors conclude that \"our algorithm uses lesser number for the GMMs and requires fewer samples to generalize to the real-world\". I'm not sure how this can be concluded from Fig 3 [LEFT]. The two line graphs for different values of gamma almost overlay each other, and the cost seems to go up and down, even with number of samples on a log scale. If this shows the variance in the procedure, then the authors should run enough repeats of the experient to smooth out the variance and show the true signal (with error bars if possible). All related conclusions with regards to the dynamic game achieving higher sample efficiency for GMM dynamics fitting need to be backed up with better experimental data (or perhaps clearer presentation, if such data already exists).\n\nFigures 2 and 3 talk about optimal adversarial costs. The precise mathematical definition of this term should be clarified somewhere, since there are several cost functions described in the paper, and it's unclear which terms are actually being plotted here.\n\nThe structure of the global policies used in the experiments should be mentioned somewhere.\n\nNote about anonymity: Citation [21] breaks anonymity, since it's referred to in the text as \"our abstract\". The link to the YouTube video breaks author anonymity. Further, the link to a shared dropbox folder breaks reviewer anonymity, hence I have not watched those videos.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}