{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The concerns raised by AnonReviewer3 point out that, despite the effort of the authors to bridge the SM / ML divide, there is still some work to be done. The gulf between thermodynamic limits and finite effects is oft-cited in the author response. This seems to be a catch all. This gap needs to be addressed early. The authors might even suggest some open (empirical) questions looking for these phase transitions in finite systems in cases where they think engineering has placed us \"not too close\".  "
    },
    "Reviews": [
        {
            "title": "Fascinating but unconvincing.",
            "rating": "3: Clear rejection",
            "review": "The authors suggest that ideas from statistical mechanics will help to understand the \"peculiar and counterintuitive generalization properties of deep neural networks.\" The paper's key claim (from the abstract) is that their approach \"provides a strong qualitative description of recently-observed empirical results regarding the inability of deep neural networks not to overfit training data, discontinuous learning and sharp transitions in the generalization properties of learning algorithms, etc.\" This claim is restated on p. 2, third full paragraph.\n\nI am sympathetic to the idea that ideas from statistical mechanics are relevant to modern learning theory. However, I do not find this paper at all convincing. I find the paper incoherent: I am unable to understand the argument for the central claims. On the one hand, the paper seems to be written as a \"response\" to Zhang et al.'s \"Understanding Deep Learning Requires Rethinking Generalization\", (henceforth Z): the introduction mentions Z multiple times, and the title of this work refers to Z. On the other hand, none of the issues raised by Z are (as far as I can tell) addressed in any substantial way by this paper. In somewhat more detail, this work discusses two major observations:\n\n1. Neural nets can easily overtrain, even to random data.\n2. Popular ways to regularize may or may not help.\n\nZ certainly observes 1 and arguably observes 2. (I'd argue against, see below, but it's at least arguable.) I do not see how this paper addresses either observation. Instead, what the statistical mechanics (SM) approach seems to do is explain (or predict) the existence of phase transitions, where we suddenly go from a regime of poor generalization to good generalization or vice versa. However, neither Z nor, as far as I can tell, any other reference given here, suggests that these phase transitions are frequently observed in modern deep learning. The most relevant bit from Z is Figure 1c, which suggests that as the noise level is increased (corresponding to alpha decreasing in this paper), the generalization error increases smoothly. This seems to be in direct contradiction to the predictions made by the theories presented here.\n\nIf the authors wish to hold to the claim that their work \"can provide a qualitative explanation of recently-observed empirical properties that are not easily-understandable from within PAC/VC theory of generalization, as it is commonly-used in ML\" (p. 2), it is absolutely critical that they be more specific about which specific observations from which papers they think they are explaining. As written, I simply do not see which actual observations they think they explain.\n\nIn observation 2, the authors suggest that many popular ways to implement regularization \"do not substantially improve the situation\". A careful reading of Z (and this was corroborated by discussion with the authors) is that Z observed that regularization with parameters commonly used in practice (or, put differently, regularization parameters that led to the highest holdout accuracy in other papers) still led to substantial overtraining on noisy data. I think it is almost certainly true (see below for more discussion) that much larger values of regularization can prevent overfitting, at the cost of underfitting. It's also worth noting that Z agrees with basically all practitioners that various regularization techniques can make an important difference to practitioners who want to minimize test error; what they don't do (at least at moderate values) is *qualitatively* destroy a network's ability to overfit to noise. It is unclear to me how this paper explains observation 2 (see below for extensive discussion).\n\nI don't actually understand the first full paragraph on p. 2 well. It is true that we can always avoid overtraining by tuning regularization parameters to get better generalization *error* (difference beween train and test) on the test data set (but possibly worse generalization accuracy); the rest of the paper seems to take the opposite side on this. A Gaussian kernel SVM with a small enough bandwidth and small enough regularization parameter can also overfit to noise. The argument needs to be sharpened here.\n\nI find the discussion of noise at the bottom of p. 2 confusing. The authors describe tau \"having to do with noise in the learning process\", but then suggest that \"adding noise decreases the effective load.\" This is the first time noise is really talked about, and it seems like maybe noise in the data is about alpha, but noise in the \"learning process\" is about tau? This should be clarified.\n\nOn p. 3, the authors refer to \"the two parameters used by Z and many others.\" I am honestly not sure what's being referred to here. I just reread Z and I don't get it.  What two parameters are used by Z?\n\np. 3, figure. The authors should be clear about what recent (ideally widely-discussed) experimental results look anything like this figure. I found nothing in Z et al. In Appendix A.4, there is a mention of Figure 3 of Chromanska et al. 2014; that figure also seems to be totally consistent with smooth transitions and does not (to me) present any obvious evidence of a sharp phase transition. (In any case, the main paper should not rely heavily on the appendix for its main empirical evidence.)\n\np. 3, figure 1a. What is essential in this figure? A single phase transition? That the error be very low on the r.h.s. of the phase transition (probably not that, judging from the related models in the\nAppendix).\n\np. 3, figure 1b/c. What does SG stand for? As far as I can tell it's never discussed.\n\np. 4. \"Thus, an important more general insight from our approach is that --- depending strongly on details of the model, the specific details of the learning algorithm, the detailed properties of the data and their noise etc. --- going beyond worst-case bounds can lead to a rich and complex array of manners in which generalization can depend on the control parameters of the ML process.\" This is well-known to all practitioners. This paper does not seem to offer any specific testable explanations or predictions of any sort. I certainly agree that the study of SM models is \"interesting\", but what would\nmake this valuable would be a more direct analogy, a direct explanation of some empirical phenomenon.\n\nSection 2 in general. The authors discuss a couple different types of observations: (1) \"strong discontinuities in generalization performance as a function of control parameters\" aka phase transitions, and (2) generalization performance can depend sensitively on details of the model, details of algorithms, implicit regularization properties, detailed properties of data and noise, etc.\" (1) shows up in the SM literature from the 90's discussed in Appendix A. I don't think it shows up in modern practice, and I don't think it shows up in Z. (2) is absolutely relevant to modern practitioners, but I don't see what this paper has to say about it beyond \"SM literature from the 90's exhibits similar phenomena.\" The model introduced in Section 3 abstracts all such concerns away.\n\nSection 3. I am not super comfortable with the idea of \"Claims\", especially since the 3 Claims seem to be different sorts of things. I would normally think of a \"Claim\" as something that could be true or false, possibly with some argument for its truth.\n\nClaim 1 introduces a model (VSDL), but I wouldn't call this a claim, since nothing is actually \"claimed.\" The subpoints of Claim 1 are arguably claims, but they're not introduced as such. I address these\nin turn:\n\n\"Adding noise decreases an effective load alpha.\" The paper states \"N is the effective capacity of the model trained on these data\", but \"effective capacity\" is never defined. Certainly, if we *define* alpha = m_eff / N and *define* m_eff = m - m_rand, the (sub)claim follows, but why are those definitions good?  I *think* what's going on here is hidden in the sentence \"empirical results indicate that for realistic DNNs it is close to 1. Thus, the model capacity N of realistic DNNs scales with m and not m_eff.\", where \"it\" refers to the Rademacher complexity. Well, OK, but if we agree with that, then aren't we just *assuming* the main result of Z rather than explaining it? We're basically just stating that the models can memorize the data?\n\nI don't really understand the point the last part of the paragraph is trying to make (everything after what I quoted above).\n\n\"Early stopping increases an effective temperature tau.\" I find this plausible but don't understand the argument at all. To this reader, it's just \"stuff from SM I don't understand.\" I think the typical ML reader of this paper won't necessarily be familiar with any of \"the weights evolve according to a relaxation Langevin equation\", \"from the fluctuation-dissipation theorem\", or the reference to annealing rate schedules. Consider either explaining this more or just appealing to SM and relegating this to an appendix.\n\nAfter the claim, the paper mentions that the VSDL model ignores other \"knobs\". This is fine for a model, but I think it's totally disingenuous to then suggest that this model explains anything about other popular ways to regularize (Observation 2 in the intro, see also my comment on Section 2). In the intro, the claim is \"Other regularizations sometimes help and sometimes don't and we don't understand why\" (the claim is about overfitting but it's also true for improving performance in general), which is basically true. But introducing a model which completely abstracts these things away cannot possibly explain anything about the behavior.\n\nClaim 2 is that we should consider a thermodynamic limit where model complexity grows with data (the paper says grows with the number of parameters, I assume this is a typo). I would probably call this one an \"Assumption\", with some arguments for the justification. I think this is one of the most interesting and important ideas in the paper, and I don't fully understand it, even after reading the appendix. I have questions. How should / could this apply to practitioners, who cannot in general hope to obtain arbitrary amounts of data? Are we assuming that any (or all) modern DNN experiments are in the asymptotic regime? Are we assuming the experiments in Z are in this regime? Is there any relevance to the fact that in an ML problem (unlike in say a spin glass, at least as far as I know) the \"complexity\" of the *task* is *not* increasing with the data size, so eventually one will have seen \"enough\" data to \"saturate\" the task?  I'd love to know more.\n\nClaim 3 is more of an \"Informal Theorem\" that under the model of Claim 1 and the assumption of Claim 2, the phase diagrams of Figure 1 hold. The \"proof\" is a reference to SM papers. This should be clarified.\n\nYet again, I point out that I do not know any modern large-scale NN experiments that correspond to any of the pictures in Figure 1.\n\nThere's a mention of \"tau = 0 or t > 0.\" What is the significance of tau = 0? How should an ML reader think about this?\n\nSection 3.2 suggests that Claim 3 (the existence of the 1 and 2d phase diagrams) \"explain\" Observations 1 and 2 from the Appendix. I simply do not see this. \n\nFor Observation 1, that NNs can easily overtrain, the \"argument\" seems to boil down to \"the system is in a phase where it cannot help but overtrain.\" This is hardly an explanation at all. How do we know what phase these experiments were in? How do we know these experiments were in the thermodynamic limit?\n\nFor Observation 2, the authors point out that in VSDL, \"the only way to prevent overfitting is to decrease the number of iterations.\" This seems true but vacuous: the authors introduced a model where regularization doesn't correspond to any knobs, so of course to the extent that that model explains reality, the knobs don't stop overfitting. But this feels like begging the question. If we accept the VSDL model, we'd also accept that various regularizations can't improve generalization, which goes directly against basically all practice. I guess I technically have to concede that \"Given the three\nclaims\", Observation 2 follows, but Claim 1 by itself seems to be already assuming the conclusion.\n\nMinor writing issues:\n\nThe authors mention at least four times that reproducing others' results is not easy (p. 1 observation 1, p. 4 first paragraph, p. 4 footnote 6, last sentence of the main text). While I think this statement is true, it is quite well-known, and I suggest that the authors may simply alienate readers by harping on it here.\n\np. 1. \"may always overtrain\" is unclear. I don't know what it means. Is the claim that SOTA DNNs wll always overtrain when presented with enough data? I don't think so from the rest of the paper, but I'm not sure.\n\nI'm a little unclear what the authors mean by \"generalization error\" (or \"generalization accuracy\", which seems to only be used on p. 2). Z use \"generalization error = training error - test error\". Check the appendix for consistency here too.\n\nReplace \"phenomenon\" with \"phenomena\", at least twice where appropriate.\n\np. 3, first paragraph. I think the reference to the Hopfield model should be relegated to a footnote. The text \"two or more such parameter holds more generally\" is confusing; is it two, or is it two or more? What will I understand differently if I use more than two parameters? The next paragraph, starting with \"Given these two identifications, which are novel to this work,\" seems odd, since we've\njust seen 7+ references and a claim that they have similar parameterizations, so it's unclear what's novel.\n\nAppendix A.5. \"For non-linear dynamical systems... NNs from the 80s/90s or our VSDL model or realistic DNNs today .. there is simply no reason to expect this to be true.\" where \"this\" refers to \"one can always choose a value of lambda to prevent overfitting, potentially at the expense of underfitting.\" I don't understand, and I also think this disagrees with the first full paragraph on p. 2. Is there some thermodynamic limit argument required here? The very next bullet states that x = argmin_x f(x) + lambda g(x) can prevent overfitting with large lambda. What's different? I'm overall not clear what's being implied here. Consider a modern DNN for classification. A network with all zero weights will have some empirical loss L(0). If I minimize, for the weights of a network w, L(w) + lambda ||w||^2, I have that L(w) + lambda ||w||^2 <= L(0) (assuming I can solve the optimization), and assuming L is non-negative, lambda ||w||^2 <= L(0), or ||w||^2 <= L(0) / lambda. So for very large lambda, I can drive ||w||^2 arbitrarily close to zero. How is this importantly different from the linear case?  What am I missing?\n\np. 3. \"inability not to overfit.\" Avoid the double negative.\n\nIntro, last paragraph. Weird section order description, with ref to Section A coming before section 4.\n\nFootnote 2. \"but it can be quite limiting.\" More detail needed. Limiting how?\n\nFootnotes 3 and 4. The text says there are \"technical\" and \"non-technical\" reasons, but 3 and 4 both seem technical to me.\n\nAppendix A.2. \"on a randomly chosen subset of X.\" Is it really subset? Are we picking subsets uniformly at random?\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting set of ideas and direction, but lack of quantitative analysis supporting the results.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "\nThis papers provides an interesting set of ideas related to theoretical understanding generalization properties of multilayer neural networks. It puts forward a qualitative analogy between some recently observed behaviours in deep learning and results stemming from previous quantitative statistical physics analysis of single and two-layer neural networks. The paper serves as a nice highlight into the not-so recent progress made in statistical physics for understanding of various models of neural networks. I agree with the authors that this line of work, that is not very well known in the current machine learning community, includes a number of ideas that should be able to shed light on some of the currently open theoretical questions. As such the paper would be a nice contribution to ICLR.\n\nOn the negative side, the paper is only qualitative. The Very Simple Deep Learning model that it introduces is not even a model in the physics or statistics sense, since it cannot be fit on data, it does not specify any macroscopic details. I only saw something like that to be called a *model* in experimental biology papers ... The models that are reviewed in the appendix, i.e. the continuous and Ising perceptron and the committee machine are more relevant. However, the present paper only reviews existing results about them. And even in that there are flaws, because it is not always clear from what previous works are the results taken nor is it clear how exactly they were obtained (e.g.  Fig. 2 (a) is for Ising or continuous weights? How was it computed? Why in Fig. 3(a) the training and generalization error is the same while in Fig. 3(c) they are different? What exact formulas were evaluated to obtain these figures?). \n\nConcerning the lack of mathematical rigour in the statistical physics literature on which the authors comment, they might want to relate to a very recent work https://arxiv.org/pdf/1708.03395.pdf work that sets all the past statistical physics results on optimal generalization in single layer neural networks on fully rigorous basis by proving that the corresponding formulas stemming from the replica method are indeed correct. \n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting remarks, nice review, but maybe lack new results?",
            "rating": "7: Good paper, accept",
            "review": "I find myself having a very hard time making a review of this paper,  because I mostly agree with the intro and discussion, and certainly agree that the \"typical\" versus \"worse case\" analysis is certainly an important point.  The authors are making a strong case for the use of these models to understand overfitting and generalization in deep leaning.\n\nThe problem is however that, except from advocating the use of these \"spin glass\" models studied back in the days by Seung, Sompolinksy, Opper and others, there are little new results presented in the paper. The arguments using the Very Simple Deep Learning (VSDL) are essentially a review of old known results --which I agree should maybe be revisited-- and the motivation to their application to deep learning stems from the reasoning  that, since this is the behavior observed in all these model, well then deep learning should behave just the same as well. This might very well be, but this is precisly the point: is it ? \n\nAfter reading the paper,  I agree with many points and enjoyed reading the discussion. I found interesting ideas discussed and many papers reviewed, and ended up discovering interesting papers on arxiv as a concequence.\n\nThis is all nice, interesting, and well written, but at the end of the day, the paper is not doing too much beyond being a nice review of all ideas. While this has indeed some values, and might trigger a renewal of interested for these approaches, I will let the comity decide if this is the material they want in ICLR.\n\nA minor comment: The generalization result of [9,11] obtained with heuristic tools (the replica method of statistical mechanics) and plotted in Fig.1 (a) has been proven recently with rigorous mathematical methods in arxiv:1708.03395 \n\nAnother remark:  if deep learning is indeed well described by these models, then again so are many other simpler problems, such as compressed sensing, matrix and tensor factorization, error corrections, etc etc... with similar phase diagram as in fig. 1.  For instance gaussian mixtures are discussed in http://iopscience.iop.org/article/10.1088/0305-4470/27/6/016/and  SVM (which the authors argue should behave quite differently) methods have been treated by statistical mechanics tools in https://arxiv.org/pdf/cond-mat/9811421.pdf with similar phase diagrams. I am a bit confused what would be so special about deep learning then?\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}