{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The reviewers agree that the idea of incorporating humans in the training of generative adversarial networks is interesting and worthwhile exploring. However, they felt that the paper fell short in providing strong support for their approach. The AC agrees. The authors are encouraged to strengthen their work and resubmit to a future venue."
    },
    "Reviews": [
        {
            "title": "interesting direction; but results are preliminary",
            "rating": "4: Ok but not good enough - rejection",
            "review": "+ Quality:\nThe paper discusses an interesting direction of incorporating humans in the training of a generative adversarial networks in the hope of improving generated samples. I personally find this exciting/refreshing and will be useful in the future of machine learning.\n\nHowever, the paper shows only preliminary results in which the generator trained to maximize the PIR score (computed based on VGG features to simulate human aesthetics evaluation) indeed is able to do so. However, the paper lacks discussion / evidence of how hard it is to optimize for this VGG-based PIR score. In addition, if this was challenging to optimize, it'd be useful to include lessons for how the authors manage to train their model successfully.\nIn my opinion, this result is not too surprising given the existing power of deep learning to fit large datasets and generalize well to test sets. \n\nAlso, it is not clear whether the GAN samples indeed are improved qualitatively (with the incorporation of the PIR objective score maximization objective) vs. when there is no PIR objective. The paper also did not report sample quantitative measures e.g. Inception scores / MS-SSIM.\n\nI'd be interested in how their proposed VGG-based PIR actually correlates with human evaluation.\n\n+ Clarity: \n- Yosinski et al. 2014 citation should be Nguyen et al. 2015 instead (wrong author order / year).\n- In the abstract, the authors should emphasize that the PIR model used in this paper is based on VGG features.\n\n+ Originality: \n- The main direction of incorporating human feedback in the loop is original.\n\n+ Significance: \n- I think the paper contribution is lighter vs. ICLR standard. Results are preliminary.\n\nOverall, I find this direction exciting and hope the authors would keep pushing in this direction! However, the current manuscript is not ready for publication.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Practically relevant problem but paper is premature.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper proposes a technique to improve the output of GANs by maximising a separate score that aims to mimic human interactions. \n\nSummary:\nThe goal of the technique to involve human interaction in generative processes is interesting. The proposed addition of a new loss function for this purpose is an obvious choice, not particularly involved. It is unclear to me whether the paper has value in its current form, that is without experimental results for the task it achieves. It feels to premature for publication. \n\n\nMore comments:\nThe main problem with this paper is that the proposed systems is designed for a human interaction setting but no such experiment is done or presented. The title is misleading, this may be the direction where the authors of the submission want to go, but the title  “.. with human interactions” is clearly misleading. “Model of human interactions” may be more appropriate. \n\nThe technical idea of this paper is to introduce a separate score in the GAN training process. This modifies the generator objective.  Besides “fooling” the discriminator, the generator objective is to maximise user interaction with the generated batch of images. This is an interesting objective but since no interactive experiments presented in this paper, the rest of the experiments hinges on the definition of “PIR” (positive interaction rate)using a model of human interaction. Instead of real interactions, the submission proposes to maximise the activations of hidden units in a separate neural network. By choosing the hierarchy level and type of filter the results of the GAN differ. \n\nI could not appreciate the results in Figure 2 since I was missing the definition of PIR, how it is drawn in the training setup. Further I found it not surprising that the PIR changes when a highly parameterised model is trained for this task. The PIR value comes from a separate network not directly accessible during training time, nonetheless I would have been surprised to not see an increase. Please comment in the rebuttal and I would appreciate if the details of the synthetic PIR values on the training set could be explained.\n\n- Technically it was a bit unclear to me how the objective is defined. There is a PIR per level and filter (as defined in C4) but in the setup the L_{PIR} was mentioned to be a scalar function, how are the values then summarized? There is a PIR per level and feature defined in C4. \n- What does the PIR with the model in Section 3 stand for? Shouldn’t be something like “uniqueness”, that is how unique is an image in a batch of images be a better indicator? Besides, the intent of what possibly interesting PIR examples will be was unclear. \nE.g., the statement at the end of 2.1 is unclear at that point in the document. How is the PIR drawn exactly? What does it represent? Is there a PIR per image? It becomes clear later, but I suggest to revisit this description in a new version.\n- Also I suggest to move more details from Section C4 into the main text in Section 3. The high level description in Section 3. \n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for \"Improving image generative models with human interactions\"",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Summary:\nThis paper proposes an approach to generate images which are more aesthetically pleasing, considering the feedback of users via user interaction. However, instead of user interaction, it models it by a simulated measure of the quality of user interaction and then feeds it to a Gan architecture. \n\nPros:\n+ The paper is well-written and has just a few typos: 2.1: “an Gan”.\n+ The idea is very interesting. \n\nCons:\n\n- Page 2- section 2- The reasoning that a deep-RL could not be more successful is not supported by any references and it is not convincing.\n\n- Page 3- para 3 - mathematically the statement does not sound since the 2 expressions are exactly equivalent. The slight improvement may be achieved only by chance and be due to computational inefficiency, or changing a seed.  \n\n- Page 3- 2.2. Using a crowd-sourcing technique, developing a similarly small dataset (1000 images with 100 annotations) would normally cost less than 1k$.\n\n- Page 3- 2.2.It is highly motivating to use users feedback in the loop but it is poorly explained how actually the user's' feedback is involved if it is involved at all. \n\n- Page 4- sec 3 \".. it should be seen as a success\"; the claim is not supported well.\n\n- Page 4- sec 3.2- last paragraph.\nThis claim lacks scientific support, otherwise please cite proper references. The claim seems like a subjective understanding of conscious perception and unconscious perception of affective stimuli is totally disregarded.\nThe experimental setup is not convincing.\n\n- Page 4. 3.3) \"Note that.. outdoor images\" this is implicitly adding the designers' bias to the results. The statement lacks scientific support.\n\n- Page 4. 3.3) the importance of texture and shape is disregarded. “In the Eye of the Beholder: Employing Statistical Analysis and Eye Tracking for Analyzing Abstract Paintings, Yanulevskaya et al”\nThe architecture may lead in overfitting to users' feedback (being over-fit on the data with PIR measures)\n\n- Page 6-Sec 4.2) \" It had more difficulty optimizing for the three-color result\" why? please discuss it.\n\n- The expectation which is set in the abstract and the introduction of the paper is higher than the experiments shown in the Experimental setup.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}