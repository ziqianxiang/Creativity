{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper presents a neural compositional model for visual question answering.  The overall idea may be exciting but the committee agrees with the evaluation of Reviewer 1:  the experimental section is a bit thin and it only evaluates against an artificial dataset for visual QA that does not really need a knowledge base.  It would have been better to evaluate on more traditional question answering settings where the answer can be retrieved from a knowledge base (WebQuestions, Free917, etc.), and then compare with state of the art on those."
    },
    "Reviews": [
        {
            "title": "Review",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper presents a model for visual question answering that can learn both\nparameters and structure predictors for a modular neural network, without\nsupervised structures or assistance from a syntactic parser. Previous approaches\nfor question answering with module networks can (at best) make a hard choice\namong a small number of structures. By contrast, this approach computes a\nbottom-up approximation to the softmax over all possible tree-shaped network\nlayouts using a CKY-style dynamic program. On a slightly modified set of\nstructured scene representations from the CLEVR dataset, this approach\noutperforms two LSTM baselines with incomplete information, as well as an\nimplementation of Relation Networks.\n\nI think the core technical idea here is really exciting! But the experimental\nvalidation of the approach is a bit thin, and I'm not ready to accept the paper\nin its current form.\n\nRELATED WORK / POSITIONING\n\nThe title & first couple of paragraphs in the intro suggest that the\ndenotational interpretation of the representations computed by the modules is\none of the main contributions of this work. It's worht pointing out that the\nconnection between formal semantics and these kinds of locally-normalized\n\"attentions\" to entities was already made in the cited NMN papers. Meanwhile,\nrecent work by Johnson et al. and Perez et al. has found that explicitly\nattentional / denotational models are not necessarily helpful for the CLEVR\ndataset.\n\nIf the current paper really wants to make denotational semantics part of the\ncore claim, I think it would help to talk about the representational\nimplications in more detail---what kinds of things can and can't you model\nonce you've committed to set-like bottlenecks between modules? Are there things\nwe expect this approach to do better than something more free-form (a la\nJohnson)? Can you provide experimental evidence of this?\n\nAt the same time, one of these things that's really nice about the\nstructure-selection part of this model is that it doesn't care what kind of\nmessages the modules send to each other! It might be just as effective to focus\non the dynamic programming aspect and not worry so much about the semantics of\nindividual modules.\n\nMODELING\n\nFigure 2 is great. It would be nice to have a little bit of discussion about the\nmotivation for these particular modeling implementations---some are basically\nthe same as in Hu et al. (2017), but obviously the type system here is richer\nand it might be helpful to highlight some of the extra things it can do.\n\nThe phrase type semantic potential seems underpowered relative to the rest of\nthe model---is it really making decisions on the basis of 6 sparse features for\nevery (span, type) pair, with no score for the identity of the rule (t_1, t_2 ->\nt)? What happens if you use biRNN representations of each anchored token, rather\nthan the bare token alone? (This is standard in syntactic parsing these days.)\nIf you tried richer things and found that they didn't help, you should show\nablation experiments.\n\nEXPERIMENTS\n\nAs mentioned above, I think this is the only really disappointing piece of this\npaper. As far as I know, nobody else has actually worked with the structured KBs\nin CLEVR---the whole point of the dataset (and VQA, and the various other recent\nquestion answering datasets) is to get away from requiring structured knowledge\nbases. The present experiments involve both fake language data and fake,\nstructured world representations, so it's not clear how much we should trust the\nproposed approach to generalize to real tasks.\n\nWe know that more traditional semantic parsing approaches with real logical\nforms are capable of getting excellent accuracy on structured QA tasks with a\nlot more complexity and less data than this one. I think fairness really\nrequires a comparison to an approach for semantic parsing with denotations. \n\nBut more importantly, why not just run on images? Results on VQA, CLEVR, and\nNLVR (even if they're not all state of the art!) would make this paper much more\nconvincing.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not convinced by only synthetic data evaluation",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper describes an end to end differentiable model to answer questions based on a knowledge base. They learn the composition modules which combine representations for parts of the question to generate a representation of the whole question.   \n\nMy major complaint is the evaluation on a synthetically generated data set. Given the method of generating the data, it was not a surprise that the method which leverages hierarchical structure can do better than other methods which do not leverage that. I will be convinced if evaluation can be done on a real data set.  \n\nMinor complaints: \n\nThe paper does not compare to NMN, or a standard semantic parser. I understand that all other methods will use a predefined set of predicates, but its still worthwhile to see how much we loose when trying to learn predicates from scratch.\n\nThe paper mentions that they enumerate all parses. That is true only if the groundings are not considered part of the parse. They actually enumerate all parses based on types, and then find the right groundings for the best parse. This two step inference is an approximation, which should be mentioned somewhere.\n\nResponse to rebuttal: \n\nI agree that current data sets have minimal compositionality,  and that \"if existing models cannot handle the synthetic data, they will not handle real data\". However, its not clear that your method will be better than the alternatives when you move to real data. Also, some work on CLEVR had some questions collected from humans, maybe you can try to evaluate on that. I am going to keep my rating the same.  ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Beautiful and elegant model but evaluation is not satisfying.",
            "rating": "7: Good paper, accept",
            "review": "This paper proposes for training a question answering model from answers only and a KB by learning latent trees that capture the syntax and learn the semantic of words, including referential terms like \"red\" and also compositional operators like \"not\".\n\nI think this model is elegant, beautiful and timely. The authors do a good job of explaining it clearly. I like the modules of composition that seem to make a very intuitive sense for the \"algebra\" that is required and the parsing algorithm is clean. \n\nHowever, I think that the evaluation is lacking, and in some sense the model exposes the weakness of the dataset that it uses for evaluation.\n\nI have 2.5 major issues with the paper and a few minor comments: \n\nParsing:\n\n* The authors don't really say what is the base case for \\Psi that scores tokens (unless I missed it and if indeed it is missing it really needs to be added) and only provide the recursive case. From that I understand that the only features that they use are whether a certain word makes sense in a certain position of the rule application in the context of the question. While these features are based on Durrett et al.'s neural syntactic parser it seems like a pretty weak signal to learn from. This makes me wonder, how does the parser learn whether one parse is better than the other? Only based on this signal? It makes me suspicious that the distribution of language is not very ambiguous and that as long as you can construct a tree in some context you can do it in almost any other context. This is probably due to the fact that the CLEVR dataset was generated mostly using templates and is not really natural utterances produced by people. Of course many people have published on CLEVR although of its language limitations, but I was a bit surprised that only these features are enough to solve the problem completely, and this makes me curious as to how hard is it to reverse-engineer the way that the language was generated with a context-free mechanism that is similar to how the data was produced.\n\n* Related to that is that the decision for a score of a certain type t for a span (i,j) is the sum for all possible rule applications, rather than a max, which again means that there is no competition between different parse trees that result with the same type of a single span. Can the authors say something about what the parser learns? Does it learn to extract from the noise clear parse trees? What is the distribution of rules in those sums? is there some rule that is more preferred than others usually? It seems like there is loss of information in the sum and it is unclear what is the effect of that in the paper.\n\nEvaluation:\n\n* Related to that is indeed the fact that they use CLEVR only. There  is now the Cornell NLVR dataset that is more challenging from a language perspective and it would be great to have an evaluation there as well. Also the authors only compare to 3 baselines where 2 don't even see the entire KB, so the only \"real\" baseline is relation net. The authors indeed state that it is state-of-the-art on clevr. \n\n* It is worth noting that relation net is reported to get 95.5 accuracy while the authors have 89.4. They use a subset so this might be the reason, but I am not sure how they compared to relation net exactly. Did they re-tune parameters once you have the new dataset? This could make a difference in the final accuracy and cause an unfair advantage.\n\n* I would really appreciate more analysis on the trees that one gets. Are sub-trees interpretable? Can one trace the process of composition? This could have been really nice if one could do that. The authors have a figure of a purported tree, but where does this tree come from? From the mode? Form the authors?\n\nScalability:\n* How much of a problem would it be to scale this? Will this work in larger domains? It seems they compute an attention score over every entity and also over a matrix that is squared in the number of entities. So it seems if the number of entities is large that could be very problematic. Once one moves to larger KBs it might become hard to maintain full differentiability which is one of the main selling points of the paper. \n\nMinor comments:\n* I think the phrase \"attention\" is a bit confusing - I thought of a distribution over entities at first. \n* The feature function is not super clearly written I think - perhaps clarify in text a bit more what it does.\n* I did not get how the denotation that is based on a specific rule applycation t_1 + t_2 --> t works. Is it by looking at the grounding that is the result of that rule application?\n* Authors say that the neural enquirer and neural symbolic machines produce flat programs - that is not really true, the programs are just a linearized form of a tree, so there is nothing very flat about it in my opinion.\n\nOverall, I really enjoyed reading the paper, but I was left wondering whether the fact that it works so well mostly attests to the way the data was generated and am still wondering how easy it would be to make this work in for more natural language or when the KB is large.\n\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}