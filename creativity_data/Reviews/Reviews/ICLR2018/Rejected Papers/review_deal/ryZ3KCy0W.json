{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection."
    },
    "Reviews": [
        {
            "title": "Novelty is not enough",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper presents a generic approach to graph link weight prediction problems based on node enbeddings. After introducing several existing methods, the paper proposes a \"generic\" link weight prediction approach that uses the node embedding produced by any node embedding techniques. Six datasets are used for evaluation. \n\nOverall, the difference to the existing method [1] is minor. I don't think there is much novelty in the \"generic\" approach. More essential abstraction and comprehensive analysis is needed for a strong ICLR paper. \n\n[1] Yuchen Hou and Lawrence B Holder. Deep learning approach to link weight prediction. In Neural\nNetworks (IJCNN), 2017 International Joint Conference on, pp. 1855â€“1862. IEEE, 2017.\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper aims at tackling the weight prediction problem for potential links in graphs, and proposes a deep-learning approach (called model S) to solve this particular problem. The proposed model is formulated as a multi-layered neural network, which consider two node embeddings of the vertices on both side of edges.",
            "rating": "3: Clear rejection",
            "review": "Although this paper aims at an interesting and important task, the reviewer does not feel it is ready to be published.\nBelow are some detailed comments:\n\nPros\n- Numerous public datasets are used for the experiments\n- Good introductions for some of the existing methods.\nCons\n- The novelty is limited. The basic idea of the proposed method is to simply concatenate the embeddings of two nodes (via activation separately) from both side of edges, which is straightforward and produces only marginal improvement over existing methods (the comparison of Figure 1 and Figure 3 would suggest this fact). The optimization algorithm is not novel either.\n- Lack of detailed description and analysis for the proposed model S. In Section 5.2, only brief descriptions are given for the proposed approach.\n- The selected baseline methods are too weak as competitors, some important relevant methods are also missing in the comparisons. For the graph embedding learning task, one of the state-of-the-art approach is conducting Graph Convolutional Networks (GCNs), and GCNs seem to be able to tackle this problem as well. Moreover, the target task of this paper is mathematically identical to the rating prediction problem (if we treat the weight matrix of the graph as the rating matrix, and consider the nodes as users, for example), which can be loved by a classic collaborative filtering solution such as matrix factorization. The authors probably need to survey and compared against the proposed approach.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "link weight prediction with pre-trained node embeddings",
            "rating": "3: Clear rejection",
            "review": "The authors propose to use pretrained node embeddings in a deep learning model for link weight prediction in graphs. \nThe embedding of the source node and the destination node are concatenated and fed into a fully connected neural network which produces the link weight as its output.\nExisting work by Hou and Holder 2017 trains the same architecture, but the node embeddings are learned together with the weights of the neural network. In my professional opinion, the idea of using pretrained node embeddings and training only the neural network is not enough of a contribution.\n\nSince the proposed method does not build on the SBM or pWSBM the detailed equations on page 2 are not necessary. Also, Figure 1, 2, and 3 are not necessary. Fully connected neural networks are widely used and can be explained briefly without drawing the architecture. \n\nPros:\n+ interesting problem\n+ future work. evaluation of embeddings is indeed a hard problem worth solving.\n\nCons:\n- not novel",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}