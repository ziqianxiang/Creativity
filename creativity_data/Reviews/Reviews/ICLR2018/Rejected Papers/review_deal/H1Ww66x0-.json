{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The output kernel idea for lifelong learning is interesting, but insufficiently developed in the current draft."
    },
    "Reviews": [
        {
            "title": "This paper addresses the problem of lifelong multitask learning and proposes an efficient updating rule to learning the inter-task relationship and a new budget maintenance scheme to overcome the out of memory issue.  Experiments are conducted to demonstrate the effectiveness of the proposed method in the limited budget situation. ",
            "rating": "3: Clear rejection",
            "review": "CONTRIBUTION\nThe main contribution of the paper is not clearly stated.  To the reviewer, It seems “life-long learning” is the same as “online learning”.  However, the whole paper does not define what “life-long learning” is.\nThe limited budget scheme is well established in the literature. \n1. J. Hu, H. Yang, I. King, M. R. Lyu, and A. M.-C. So. Kernelized online imbalanced learning with fixed budgets. In AAAI, Austin Texas, USA, Jan. 25-30 2015.  \n2. Y. Engel, S. Mannor, and R. Meir. The kernel recursive least-squares algorithm. IEEE Transactions on Signal Processing, 52(8):2275–2285, 2004.\nIt is not clear what the new proposal in the paper.\n\nWRITING QUALITY\nThe paper is not well written in a good shape. Many meanings of the equations are not stated clearly, e.g., $phi$ in eq. (7). Furthermore, the equation in algorithm 2 is not well formatted. \n\nDETAILED COMMENTS\n1. The mapping function $phi$ appears in Eq. (1) without definition.\n2. The last equation in pp. 3 defines the decision function f by an inner product. In the equation, the notation x_t and i_t is not clearly defined.  More seriously, a comma is missed in the definition of the inner product.\n3. Some equations are labeled but never referenced, e.g., Eq. (4).\n4. The physical meaning of Eq.(7) is unclear.  However, this equation is the key proposal of the paper.   For example, what is the output of the Eq. (7)? What is the main objective of Eq. (7)?  Moreover, what support vectors should be removed by optimizing Eq. (7)?  One main issue is that the notation $phi$ is not clearly defined.   The computation of f-y_r\\phi(s_r) makes it hard to understand.  Especially,  the dimension of $phi$ in Eq.(7) is unknown. \n\nABOUT EXPERIMENTS\n1.\tIt is unclear how to tune the hyperparameters.\n2.\tIn Table 1, the results only report the standard deviation of AUC. No standard deviations of nSV and Time are reported.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A lifelong mutlitask learning scheme with online kernel learning, with low technical novelty ",
            "rating": "2: Strong rejection",
            "review": "Summary: The paper proposed a two-dimensional approach to lifelong learning, in the context of multi-task learning. It receives instances in an online setting, where both the prediction model and the relationship between the tasks are learnt using a online kernel based approach. It also proposed to use budgeting techniques to overcome computational costs. In general, the paper is poorly written, with many notation mistakes and inconsistencies. The idea does not seem to be novel, technical novelty is low, and the execution in experiments does not seem to be reliable. \n\nQuality: No obvious mistakes in the proposed method, but has very low novelty (as most methods follows existing studies in especially for online kernel learning). Many mistakes in the presentation and experiments.  \n\nOriginality: The ideas do not seem to be novel, and are mostly (trivially) using existing work as different components of the proposed technique. \n\nClarity: The paper makes many mistakes, and is difficult to read. [N] is elsewhere denoted as \\mathbb{N}. The main equation of Algorithm 2 merges into Algorithm 3. Many claims are made without justification (e.g. 2.2. “Cavallanti 2012 is not suitable for lifelong learning”… why?; “simple removal scheme … highest confidence” – what is the meaning of highest confidence?), etc. The removal strategy is not at all well explained – the objective function details and solving it are not discussed. \n\nSignificance: There is no theoretical guarantee on the performance, despite the author’s claiming this as a goal in the introduction itself (“goal of lifelong learner … computation”). The experiments are not reliable. Perceptron obtains a better performance than PA algorithms – which is very odd. Moreover, many of the multi-task baselines obtain a worse performance than a simple perceptron (which does not account for multi-task relationships). \n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The paper in an online budgeted version of an existing lifelong learning algorithm. The methodological contribution is minor and the experiments are not well designed.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper proposes a budgeted online kernel algorithm for multi-task learning. The main contribution of the paper is an online update of the output kernel, which measures similarity between pairs of tasks. The paper also proposes a removal strategy that bounds the number of support vectors in the kernel machine. The proposed algorithm is tested on 3 data sets and compared with several baselines.\n  Positives:\n- the output kernel update is well justified\n- experimental results are encouraging\n  Negatives:\n- the methodological contribution of the paper is minimal\n- the proposed approach to maintain the budget is simplistic\n- no theoretical analysis of the proposed algorithm is provided\n- there are issues with the experiments: the choice of data sets is questionable (all data sets are very small so there is not need for online learning or budgeting; newsgroups is a multi-class problem, so we would want to see comparisons with some good multi-class algorithms; spam data set might be too small), it is not clear what were hyperparameters in different algorithms and how they were selected, the budgeted baselines used in the experiments  are not state of the art (forgetron and random removal are known to perform poorly in practice, projectron usually works much better), it is not clear how a practitioner would decide whether to use update (2) or(3)",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}