{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The chief contribution of this paper is to show that a single set of policy parameters can be optimized in an alternating fashion while the design parameters of the body are also optimized with policy gradients and sampled. The fact that this simple approach seems to work is interesting and worthy of note. However, the paper is otherwise quite limited - other methods are not considered or compared, incomplete experimental results are given, and important limitations of the method are not addressed. As it is an interesting but preliminary work, the workshop track would be appropriate.",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "Paper of broad interest for control tasks",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "review": "This is a well written paper, very nice work.\nIt makes progress on the problem of co-optimization of the physical parameters of a design\nand its control system.  While it is not the first to explore this kind of direction,\nthe method is efficient for what it does; it shows that at least for some systems, \nthe physical parameters can be optimized without optimizing the controller for each \nindividual configuration. Instead, they require that the same controller works over an evolving\ndistribution of the agents.  This is a simple-but-solid insight that makes it possible\nto make real progress on a difficult problem.\n\nPros:  simple idea with impact;  the problem being tackled is a difficult one\nCons:  not many;  real systems have constraints between physical dimensions and the forces/torques they can exert\n       Some additional related work to consider citing.  The resulting solutions are not necessarily natural configurations, \n   given the use of torques instead of musculotendon-modeling.  But the current system is a great start.\n\nThe introduction could also promote that over an evolutionary time-frame, the body and\ncontrol system (reflexes, muscle capabilities, etc.) presumably co-evolved.\n\nThe following papers all optimize over both the motion control and the physical configuration of the agents.\nThey all use derivative free optimization, and thus do not require detailed supervision or precise models\nof the dynamics.\n\n- Geijtenbeek, T., van de Panne, M., & van der Stappen, A. F. (2013). Flexible muscle-based locomotion\n  for bipedal creatures. ACM Transactions on Graphics (TOG), 32(6), 206.\n  (muscle routing parameters, including insertion and attachment points) are optimized along with the control).\n\n- Sims, K. (1994, July). Evolving virtual creatures. In Proceedings of the 21st annual conference on\n  Computer graphics and interactive techniques (pp. 15-22). ACM.\n  (a combination of morphology, and control are co-optimized)\n\n- Agrawal, S., Shen, S., & van de Panne, M. (2014). Diverse Motions and Character Shapes for Simulated\n  Skills. IEEE transactions on visualization and computer graphics, 20(10), 1345-1355.\n  (diversity in control and diversity in body morphology are explored for fixed tasks)\n\nre: heavier feet requiring stronger ankles\nThis commment is worth revisiting.  Stronger ankles are more generally correlated with \na heavier body rather than heavy feet, given that a key role of the ankle is to be able\nto provide a \"push\" to the body at the end of a stride, and perhaps less for \"lifting the foot\".\n\nI am surprised that the optimization does not converge to more degenerate solutions\ngiven that the capability to generate forces and torques is independent of the actual\nlink masses, whereas in nature, larger muscles (and therefore larger masses) would correlate\nwith the ability to generate larger forces and torques.  The work of Sims takes these kinds of \nconstraints loosely into account (see end of sec 3.3).\n\nIt would be interesting to compare to a baseline where the control systems are allowed to adapt to the individual design parameters.\n\nI suspect that the reward function that penalizes torques in a uniform fashion across all joints would\nfavor body configurations that more evenly distribute the motion effort across all joints, in an effort\nto avoid large torques. \n\nAre the four mixture components over the robot parameters updated independently of each other\nwhen the parameter-exploring policy gradients updates are applied?  It would be interesting\nto know a bit more about how the mean and variances of these modes behave over time during\nthe optimization, i.e., do multiple modes end up converging to the same mean? What does the\nevolution of the variances look like for the various modes?\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Well-written paper that could use additional results to show the method's merits and generality",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper presents a model-free strategy for jointly optimizing robot design and a neural network-based controller. While it is well-written and covers quite a lot of related work, I have a few comments with regards to the algorithm and experiments.\n\n- The algorithm boils down to an alternating policy gradient optimization of design and policy parameters, with policy parameters shared between all designs. This requires the policy to have to generalize across the current design distribution. How well the policy generalizes is then in turn fed back into the design parameter distribution, favoring those designs it could improve on the quickest. However, these designs are not guaranteed to be optimal in the long run, with further specialization. The results for the Walker2d might be hinting at this. A comparison between a completely shared policy vs. a specialized policy per design, possibly aided by a meta-learning technique to speed up the specialization, would greatly benefit the paper and motivate the use of a shared policy more quantitatively. If the condition of a common state/action space (morphology) is relaxed, then the assumption of smoothness in design space is definitely not guaranteed.\n- Related to that, it would be interesting to see a visualization of the design space distribution. Is the GMM actually multimodal within a single run (which implies the policy is able to generalize across significantly different designs)? \n- There are a separate number of optimization steps for the design and policy parameters within each iteration of the training loop, however the numbers used for the experiments are not listed.  It would be interesting to see what the influence of the ratio of these steps is, as well as to know how many design iterations were taken in order to get to those in Fig. 4. This is especially relevant if this technique is to be used with real physical systems. One could argue that, although not directly used for optimization or planning, the physics simulator acts a cheap dynamics model to test new designs.\n- I wonder how robust and/or general the optimized designs are with respect to the task settings. Do small changes in the task or reward structure (i.e. friction or reward coefficients) result in wildly different designs? In practice, good robot designs are also robust and flexible and it would be great to get an idea how locally optimal the found designs are.\n\nIn summary, while the paper presents a simple but possibly effective and very general co-optimization procedure, the experiments and discussion don't definitively illustrate this.\n\nMinor remarks:\n- Sec. 1, final paragraph: \"To do the best of our knowledge\"\n- Sec. 2, 3rd paragraph: \"contract constraints\"\n- Sec. 4.1: a convolutional neural network consisting only of fully-connected layers can hardly be called convolutional\n- Fig. 3: Â±20% difference to the baseline for the Walker2d is borderline of what I would call comparable, but it seems like some of these runs have not converged yet so that difference might still decrease.\n- Fig. 3: Please add x & y labels.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Nice to see this topic pop up again, but paper is lacking comparisons and insights.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "I'm glad to see the concept of jointly learning to control and evolve pop up again!\n\nUnfortunately, this paper has a number of weak points that - I believe - make it unfit for publication in its current state.\nMain weak points:\n- No comparisons to other methods (e.g. switch between policy optimization for the controller and CMA-ES for the mechanical parameters). The basic result of the paper is that allowing PPO to optimize more parameters, achieves better results...\n- One can argue that this is not true joint optimization Mechanical and control parameters are still treated differently. This begs the question: How should one define mechanical \"variables\" in order for them to behave similarly to other optimization variables (assuming that mechanical and control parameters influence the performance in a similar way)?\n\nAdditional relevant papers (slightly different approach):\nhttp://www.pnas.org/content/108/4/1234.full#sec-1\nhttp://ai2-s2-pdfs.s3.amazonaws.com/ad27/0104325010f54d1765fdced3af925ecbfeda.pdf\n\nMinor issues:\nFigure 1: please add labels/captions\nFigure 2: please label the axes\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}