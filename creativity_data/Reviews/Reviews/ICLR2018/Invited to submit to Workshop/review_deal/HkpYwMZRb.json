{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper sets out to analyze the problem of exploding gradients in deep nets which is of fundamental importance. Reviewers largely acknowledge the novelty of the main ideas in the paper towards this goal, however it is also strongly felt that the writing/presentation of the paper needs significant improvement to make it into a coherent and clean story before it can be published. There are also some concerns on networks used in the experiments not being close to practice.\n\nI recommend invitation to the workshop track as it has novel ideas and will likely generate interesting discussion. ",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "This paper needs some major reworking to emphasize the overall narrative introducing the Gradient Scale Coefficient and its potential impact. ",
            "rating": "3: Clear rejection",
            "review": "Summary of paper - The paper introduces the Gradient Scale Coefficient and uses it to demonstrate issues with the current understanding of where and why exploding gradients occur. \n\nReview - The paper attempts to contribute to the discussion about the exploding gradient problem by both introducing a metric for discussing this issue and by showing that current understanding of the exploding gradient problem may be incorrect. It is admirable that the authors are seeking to add to the understanding about theory of neural nets instead of contributing a new architecture with better error rates but without understanding why said error rates are lower. While the authors list 7 contributions, the current version of the text is a challenge to read and makes it challenging to distill an overarching theme or narrative to these contributions. \n\nThe authors do mention experiments on page 8, but confess that some of the results are somewhat underwhelming. Unfortunately, all tables with the experimental results are left to the appendix. As this is a mostly theoretical paper, pushing experimental results to the appendix does make sense, but the repeated references to these tables suggest that these experimental results are crucial for the authors’ overall points.\n\nWhile the authors do attempt to accomplish a lot in these nearly 16 pages of text, the authors' main points and overall narrative gets lost due to the writing that is a bit jumbled at times and that relies heavily on the supplement. There are several places where it is not immediately clear why a certain block of text is included (i.e. the proof outlines on pages 8 and 10). At other points the authors default to an chronological narrative that can be useful at times (i.e. page 9), but here seems to distract from their overall narrative. \n\nThis paper has a lot of content, but not all of it appears to be relevant to the authors’ central points. Furthermore, the paper is nearly double the recommended page length and has a nearly 30 page supplement. My biggest recommendations for this paper are for the authors to 1) articulate one theme and then 2) look at each part (whether that be section, paragraph, or sentence) and ask what does that part contribute to that theme. \n\n\n\nPros - \n* This paper attempts to add the understanding of neural nets instead of only contributing better error rates on benchmark datasets. \n* At several points, the authors seek to make the work accessible by offering lay explanations for their more technical points. \n* The practical suggestions on page 16 are a true highlight and could provide an outline for possible revisions. \n\n\nCons - \n* The main narrative is lost in the text, leaving a reader unsure of the authors main points and contributions as they read. For example, the authors’ first contribution is hidden among the text presentation of section 2. \n* The paper relies heavily on the supplement to make their central points. \n* It is nearly double the recommended page length with a nearly 30 page supplement\n\n\nMinor issues - \n* Use one style for introducing and defining terms either use italics or single quotes. The latter is not recommended since the authors use double quotes in the abstract to express that the exploding gradient problem is not solved. \n* The citation style of Authors (YEAR) at times leads to awkward sentence parsing. \n* Given that many figures have several subfigures, the authors should consider using a package that will denote subfigures with letters. \n* The block quotes in the introduction may be quite important for points later in the paper, but summarizing the points of these quotes may be a better use of space. The authors more successfully did this in paragraph 2 of the introduction. \n* All long descriptions of the appendix should be carefully revisited and possibly removed due to page length considerations. \n* In the text, figure 4 (which is in the supplement) is referenced before figure 3 (which is in the text).\n\n=-=-=-= Response to the authors\n\nDuring the initial reviewing period, I was unable to distill the significance of the authors’ contributions from the current literature in large part due to the nature of the writing style. After reading the authors responses and consulting the differences between the versions of the paper, my review remains the same. It should be noted that all three reviewers pointed out the length of the paper as a weakness of the paper, and that in the most recent draft, the authors made the main text of the paper longer. \n\nConsulting the differences between the paper revisions, I was initially intrigued with the volume of differences that shown in the summary bar. Upon closer inspection, I read a much stronger introduction and appreciated the summaries at the ends of sections 4.4 and 6. However, I did notice that the majority of these changes were superficial re-orderings of the original text. Given the limited substantive changes to the main text, I did not deeply re-read the text of the paper beyond the introduction.",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Official Review for Gradients explode, deep nets are shallow, resnet explained",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Paper Summary:\nThis is a very long paper (55 pages), and I did not read it in its entirety. The first part (up to page 11), focuses on better understanding the exploding gradients problem, and challenges the fact that current techniques to address gradient explosion work as claimed. To do so, they first motivate a new measure of gradient size, the Gradient Scale Coefficient which averages the singular values of the Jacobian and takes a ratio of different layers. The motivation for this measure is that it is invariant to simple rescaling of layers that preserves the function. (I would have liked to have seen what was meant by preserved the function here -- did you mean preserve the same class outputs e.g.?) \n\nThey focus on linear MLPs in the paper for computational simplicity. With this setup, and assuming the Jacobian decomposes, they prove that the GSC increases exponentially (Proposition 5). They empirically test this out for networks 50 layers deep and 100 layers wide, where they find that some architectures have exploding gradients after random initialization, and others do not, but those that do not have other drawbacks. \n\nThey then overview the notion of effective depth for a residual network: a linear residual network can be written as a product of terms of the form (I + r_i).  Expanding out, each term is a product of some of the r_i and some of the identities I. If all r_i have a norm < 1, then the terms that dominate will be those that consist of fewer r_i, resulting in a lower effective depth. This is described in Veit et al, 2016. While this analysis was originally used for residual networks, they relate this to any network by letting I turn into an arbitrary initial function. Their main theoretical result from this is that deeper networks take exponentially longer to train (under certain conditions), which they test out with (linear?) networks of depth 50 and width 100.\n\nThey also propose that the reason gradients explode is because networks try to preserve their domain going forward, which requires Jacobians to have determinant 1 and leads to a higher Q-norm.\n\nMain Comments:\nThis could potentially be a very nice paper, but I feel the current presentation is not ready for acceptance. In particular, the paper would benefit greatly from being made much shorter, and having more of the important details or proof outlines for the various propositions in the main text. Right now, it is quite confusing to follow, and I fail to see the motivation for some of the analysis. For example, the Gradient Scale Coefficient appears to be motivated because (bottom page 3), with other norm measurements, we could take any architecture and rescale the parameters, and inversely scale the gradients to make it \"easy to train\". But typically easy to train does not involve a specific preprocessing of gradients. Other propositions e.g. Theorem 1, proposition 6, could do with clearer intuition leading to them. I think the assumptions made in the results should also be clearer. (It's fine to have results, but currently I can't tell under what conditions the results apply and under what conditions they don't. E.g. are there any extensions of this that apply to non-linear networks?)\n\nI also have issues with their experimental setup: why choose to experiment on networks of depth 50 and width 100? This doesn't really look anything like networks that are trained in practice. Calling these \"popular architectures\" is misleading. \n\nIn summary, I think this paper needs more work on the presentation to make clear what they are proving and under what conditions, and with experiments that are closer to those used in practice to support their claims.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Claiming much of common intuition around tricks for avoiding gradient issues are incorrect. ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "The paper makes some bold claims. In particular about commonly accepted intuition for avoiding exploding/vanishing gradients and why all the recent bag of tricks (BN, Adam) do not actually address the problems they set out to alleviate. \n\nThis is either a very important paper or the analysis is incorrect but it's not my area of expertise. Actually understanding it at depth and validating the proofs and validity of the experiments will require some digestion. It's possible some of the issues arise from the particular architectures they choose to investigate and demonstrate on (eg I have mostly seen ResNets in the context of CNNs but they analyze on FC topologies, the form of the loss, etc) but that's a guess and there are some further analysis in the supp material for these networks which I haven't looked at in detail. \n\nRegardless - an important note to the authors is that it's a particularly long and verbose paper, coming in at 16 pages of the main paper(!) with nearly 50 (!) pages of supplementary material where the heart and meat of the proofs and experiments reside. As such it's not even clear if this is proper for a conference. The authors have already provided several pages worth of additional comments on the website on further related work. I view this as an issue in and of itself. Being succinct and applying rigour in editing is part of doing science and reporting findings, and a wise guideline to follow. While the authors may claim it's necessary to use that much space to make their point I will argue that this length is uncalibrated to standards. I've seen many papers that need to go through much more complicated derivations and theory and remain within a 8-10 page limit by being precise and strictly to the point. Perhaps Godel could be a good inspiration here, with a 21 page PhD thesis that fundamentally changed mathematics.\n\nIn addition to being quite bold in claims, it is also somewhat confrontational in style. I understand the authors are trying to make a very serious claim about much of the common wisdom, but again, having reviewed papers for many years, this is highly unusual and it is questionable whether it is necessary. \n\nSo, while I cannot vouch for the correctness, I think it can and should go through a serious revision to make it succinct and that will likely considerably help in making it accessible to a wider readership and aligned to the expectations from a conference paper in the field. ",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        }
    ]
}