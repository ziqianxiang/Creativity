{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "All the reviewers agree that the paper is studying an important problem and makes a good first step towards understanding learning in GANs. But the reviewers are concerned that the setup is too simplistic and not relevant in practical settings. I recommend the authors to carefully go through reviews and to present it at the workshop track. This will hopefully foster further discussions and lead to results in more practically relevant settings.",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "too simple a model?",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The authors proposes to study the impact of GANS in two different settings:\n1. at each iteration, train the discriminator to convergence and do a (or a few) gradient steps for updating the generator\n2. just do a few gradient steps for the discriminator and the generator\nThis is done in a very toy example: a one dimensional equally weighted mixture of two Gaussian distributions.\n\nClarity: the text is reasonably well written, but with some redundancy (e.g. see section 2.1) , and quite a few grammatical and mathematical typos here and there. (e.g. Lemma 4.2., $f$ should be $g$, p7 Rect(0) is actually the empty set, etc..)\n\nGaining insights into the mechanics of training GANs is indeed important. The authors main finding is that, in this very particular setting, it seems that training the discriminator to convergence leads to convergence. Indeed, in real settings, people have tried such strategies for WGAN for examples. For standard GANs, if one adds a little bit of noise to the labels for example, people have also reported good result for such a strategy (although, without label smoothing, this will indeed leads to problems).\n\nAlthough I have not checked all the mathematical fine details, the approach/proof looks sound (although it is not at all clear too me why the choice of gradient step-sizes does not play a more important roles the the stated results). My biggest complain is that the situation analyzed is so simple (although the convergence proof is far from trivial) that I am not at all convinced that this sheds much light on more realistic examples. Since this is the main meat of the paper (i.e. no methodological innovations), I feel that this is too little an innovation for deserving publication in ICLR2018.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting theoretical approach for GAN, but not enough",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Although GAN recently has attracted so many attentions, the theory of GAN is very poor. This paper tried to make a new insight of GAN from theories and I think their approach is a good first step to build theories for GAN. \n\nHowever, I believe this paper is not enough to be accepted. The main reason is that the main theorem (Theorem 4.1) is too restrictive.\n\n1.\tThere is no theoretical result for failed conditions. \n2.\tTo obtain the theorem, they assume the optimal discriminator. However, most of failed scenarios come from the discriminator dynamics as in Figure 2. \n3.\tThe authors could make more interesting results using the current ingredients. For instance, I would like to check the conditions on eta and T to guarantee d_TV(G_mu*, G_hat{mu})<= delta_1 when |mu*_1 – mu*_2| >= delta_2 and |hat{mu}_1 – hat{mu}_2| >= delta_3. In Theorem 4.1, the authors use the same delta for delta_1, delta_2, delta_3. So, it is not clear which initial condition or target performance makes the eta and T.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good (but limited) step towards understanding dynamics of adversarial training in GANs",
            "rating": "7: Good paper, accept",
            "review": "Summary:\n\nThis paper studies the dynamics of adversarial training of GANs for Gaussian mixture model. The generator is a mixture of two Gaussians in one dimension. Discriminator is union of two intervals. Synthetic data is generated from a mixture of two Gaussians in one dimension. On this data, adversarial training is considered under three different settings depending on the discriminator updates: 1) optimal discriminator updates, 2) standard single step gradient updates, 3) Unrolled gradient updates with 5 unrolling steps.\n\nThe paper notices through simulations that in a grid search over the initial parameters of generator optimal discriminator training always succeeds in recovering the true generator parameters, whereas the other two methods fail and exhibit mode collapse. The paper also provides theoretical results showing global convergence for the optimal discriminator updates method.\n\n\n\nComments:\n1) This is an interesting paper studying the dynamics of GANs on a simpler model (but rich enough to display mode collapse). The results establish the standard issues noticed in training  GANs. However no intuition is given as to why the mode collapse happens or why the single discriminator updates fail (see for ex. https://arxiv.org/abs/1705.10461)?\n\n2) The proposed method of doing optimal discriminator updates cannot be extended when the discriminator is a neural network. Does doing more unrolling steps simulate this behavior? What happens in your experiments as you increase the number of unrolling steps?\n\n3) Can you write the exact dynamics used for Theorem 4.1 ? Is the noise added in each step? \n\n4) What is the size of the initial discriminator intervals used for experiments in figure 2?\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}