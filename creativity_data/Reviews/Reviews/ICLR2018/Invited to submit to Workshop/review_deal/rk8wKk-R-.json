{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "meta score: 5\n\nThis paper gives a thorough experimental comparison of convolutional vs recurrent networks for a variety of sequence modelling tasks.  The experimentation is thorough, but the main point of the paper,  that convolutional networks are unjustly ignored for sequence modelling, is overstated as there are several areas where convolutional networks are well explored.\nPros:\n clear and well-written\n thorough set of experiments\nCons\n original contribution is not strong\n it is not as radical to consider convolutional networks for sequence modeling as the authors seem to suggest\n",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "Nice results but not much novelty and I don't think that the views are as contrarian as the paper claims.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The authors claim that convolutional networks should be considered as possible replacements of recurrent neural networks as the default choice for solving sequential modelling problems. The paper describes an architecture similar to wavenet with residual connections. Empirical results are presented on a large number of tasks where the convolutional network often outperforms modern recurrent baselines or reaches similar performance.\n\nThe biggest strength of the paper is the large number of tasks on which the models are evaluated. The experiments seem sound and the information in both the paper and the appendix seem to allow for replication. That said, I don’t think that all the tasks are very relevant for comparing convolutional and recurrent architectures. While the time windows that RNNs can deal with are infinite in principle, it is common knowledge that the effective length of the dependencies RNNs can model is quite limited in practise. Many of the artificial task like the adding problem and sequential MNIST have been designed to highlight this weakness of RNNs. I don’t find it very surprising that these tasks are easy to solve with a feedforward architecture with a large enough context window. The more impressive results are in my opinion those on the language modelling tasks where one would indeed expect RNNs to be more suitable for capturing dependencies that require stack-like memory functionality. \n\nWhile the related work is quite comprehensive, it downplays the popularity of convolutional architectures throughout history a bit. Especially in speech recognition, RNNs have only recently started to gain popularity while deep feedforward networks applied to overlapping time windows (i.e., 1D convolutions) have been the state-of-the-art for years. Of course the recent successes of dilated convolutions are likely to change the landscape in this application domain yet again.\n\nThe paper is well-structured and written. If anything, it is perhaps a little bit wordy at times but I prefer that over obscurity due to brevity.\n\nThe ideas in the paper are not novel and neither do the authors claim that they are. Unfortunately, I also think that the impact of the work is also somewhat limited due to the enormous success of the wavenet architecture. I do think that the results on the real-world tasks are valuable and worthy of publication. However, I feel that the authors exaggerate the extent to which researchers in this field still consider RNNs superior models for sequences. \n\n+ Many experiments and tasks.\n+ Well-written and clear.\n+ Good results\n- Somewhat exaggerated claims about the extent to which RNNs are still being considered more suitable sequence models\n than dilated convolutions. Especially in light of the success of Wavenet.\n- Not much novelty/originality.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The authors benchmark a general-purpose convolutional architecture on several sequence modeling tasks across a variety of domains. The results will be of broad use to the community, although some of the claims in the paper could do with more justification.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "In this paper, the authors argue for the use of convolutional architectures as a general purpose tool for sequence modeling. They start by proposing a generic temporal convolution sequence model which leverages recent advances in the field, discuss the respective advantages of convolutional and recurrent networks, and benchmark their architecture on a number of different tasks.\n\nThe paper is clearly written and easy to follow, does a good job of presenting both the advantages and disadvantages of the proposed method, and convincingly makes the point that convolutional architectures should at least be considered for any sequence modeling task; they are indeed still often overlooked, in spite of some strong performances in language modeling and translation in recent works.\n\nThe only part which is slightly less convincing is the section about effective memory size. While it is true that learning longer term dependencies can be difficult in standard RNN architectures, it is interesting to notice that the SoTA results presented in appendix B.3 for language modeling on larger data sets are architectures which focus on remedying this difficulty (cache model and hierarchical LSTM). It would also be interesting to see how TCN works on word prediction tasks which are devised explicitly to test for longer memory, such as Lambada (1) or Children Books Test (2).\n\nAs a minor point, adding a measure of complexity in terms of number of operations could be a useful hardware-independent indication of the computational cost of the architecture.\n\nPros:\n- Clearly written, well executed paper\n- Makes a strong point for the use of convolutional architecture for sequences\n- Provides useful benchmarks for the community\n\nCons:\n- The claims on effective memory size need more context and justification\n\n1: The LAMBADA dataset: Word prediction requiring a broad discourse context, Paperno et al. 2016\n2: The Goldilocks principle: reading children's books with explicit memory representation, Hill et al. 2016",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Convolutional networks are good for solving sequence tasks",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper argues that convolutional networks should be the default\napproach for sequence modeling.\n\nThe paper is nicely done and rather easy to understand. Nevertheless, I find\nit difficult to assess its significance. In order to support the original hypothesis,\nI think that a much larger and more diverse set of experiments should have\nbeen considered. As pointed out by another reviewer please add  https://arxiv.org/abs/1703.04691\nto your references.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}