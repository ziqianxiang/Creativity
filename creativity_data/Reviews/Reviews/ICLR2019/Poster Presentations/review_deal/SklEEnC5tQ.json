{
    "Decision": {
        "metareview": "This paper proposes distributional concavity regularization for GANs which encourages producing generator distributions with higher entropy.\n\nThe reviewers found the contribution interesting for the ICLR community. R3 initially found the paper lacked clarity, but the authors took the feedback in consideration and made significant improvements in their revision. The reviewers all agreed that the updated paper should be accepted.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "A solid contribution to regularize GANs"
    },
    "Reviews": [
        {
            "title": "Sound method and good results",
            "review": "Summary:\nThis paper proposes distributional concavity regularization for GANs which encourages producing generator distributions with higher entropy. The paper motivates the proposed method as follows:\n-       Using the concept of functional gradient, the paper interprets the update in the generator parameters as an update in the generator distribution\n-       Given this functional gradient perspective, the paper proposes updating the generator distribution toward a target distribution which has *higher entropy and satisfies monoticity*\n-       Then, the paper proves that this condition can be satisfied by ensuring that generator’s objective (L) is concave\n-       Since it’s difficult to ensure concavity when parametrizing generators as deep neural networks, the paper proposes adding a simple penalty term that encourages the concavity of generator objective\nExperiments confirm the validity the proposed approach. Interestingly, the paper shows that performance of multiple GAN variants can be improved with their proposed method on several image datasets\n \nStrengths:\n-   \tThe proposed method is very interesting and is based on sound theory\n-   \tConnection to optimal transport theory is also interesting\n-   \tIn practice, the method is very simple to implement and seems to produce good results\n \nWeaknesses:\n-       Readability of the paper can be generally improved. I had to go over the paper many times to get the idea.\n-       Figures should be provided with more detailed captions, which explain main result and providing context (e.g. explaining baselines).\n \nQuestions/Comments:\n-       Equation (7) has typos (uses theta_old instead of theta in some places)\n-       Section 4.1 (effect of monoticity) is a bit confusing. My understanding is that parameter update rule of equation (3) and (6) are equivalent, but you seem to use (6) there. Can you clarify what you do there and in general this experiment a bit more?\n-       Comparing with entropy maximization method of EGAN (Dai et al, 2017) is a good idea, but I’m wondering if you can compare it on low dimensional settings (e.g. as in Fig 2). It is also not clear why increasing entropy with EGAN-VI is worse than baselines in Table 1.\n\n \nOverall recommendation:\nThe paper is based on sound theory and provides very interesting perspective. The method seems to work in practice on a variety of experimental setting. Therefore, I recommend accepting it.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A good paper",
            "review": "The authors make use of the theory of functional gradient, based on optimal transport, to develop a method that can promote the entropy of the generator distribution without directly estimating the entropy itself. Theoretical results are provided as well as necessary experiments to support their technique's outperformance in some data sets. I found that this is an interesting paper, both original ideal and numerical results.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "potentially useful heuristic for GANs with vague maths",
            "review": "GANs (generative adversarial network) represent a recently introduced min-max generative modelling scheme with several successful applications. Unfortunately, GANs often show unstable behaviour during the training phase. The authors of the submission propose a functional-gradient type entropy-promoting approach to tackle this problem, as estimating entropy is computationally difficult.\n\nWhile the idea of the submission might be useful in some applications, the work is rather vaguely written, it is in draft phase:\n1. Abbreviations, notations are not defined: GAN, WGAN-GP, DNN, FID (the complete name only shows up in Section 4), softplus, sigmoid, D_{\\theta_{old}}, ...\n2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.\n3. Estimating entropies is a standard practice in statistics and machine learning, with an arsenal of estimators; the motivation of the submission is questionable.\n4. Differentiation w.r.t. functions (or more generally elements in normed spaces) is a well-defined concept in mathematics, including the notions of Gateaux, Frechet and Hadamard differentiability. It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.\n\nWhile the idea of the work might be useful in practice, the current submission requires significant revision and work before publication.\n\n---\n\nAfter paper revisions: \n\nThank you for the updates. The submission definitely improved. I have changed my score to '6: Marginally above acceptance threshold'; the suggested regularization can be a useful heuristic for the GAN community. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice experimental paper (with theory backing) ",
            "review": "In this paper, the authors claim that they are able to update the generator better to avoid generator mode collapse and also increase the stability of GANs training by indirectly increasing the entropy of the generator until it matches the entropy of the original data distribution using functional gradient methods.\n\nThe paper is interesting and well written. However, there is a lot of work coming out in the field of GANs currently, so I am not able to comment on the novelty of this regularization approach, and I am interested to know how this method performs when compared to other techniques to avoid mode collapse such as feature matching and mini-batch discrimination, etc.  \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        }
    ]
}