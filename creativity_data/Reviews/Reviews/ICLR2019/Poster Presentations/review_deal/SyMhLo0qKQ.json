{
    "Decision": {
        "metareview": "All the reviewers and AC agrees that the main strength of the paper that it studies a rather important question of the validity of using linear interpolation in evaluating GANs. The paper gives concrete examples and theoretical and empirical analysis that shows linear interpolation is not a great idea. The potential weakness is that the paper doesn't provide a very convincing new evaluation to replace the linear interpolation. However, given that it's largely unclear what are the right evaluations for GANs, the AC thinks the \"negative result\" about linear interpolation already deserves an ICLR paper. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "This paper discusses an interesting topic without any clear conclusions.",
            "review": "The paper discusses linear interpolations in the latent space, which is one of the common ways used nowadays to evaluate a  quality of implicit generative models. More precisely, what researchers often do in this field is to (a) take a trained model (which often comes with a \"decoder\" or a \"generator\", that is a function mapping a noise sampled from a prior distribution Pz defined over the latent space Z to the data space), (b) sample two independent points Z1 and Z2 from Pz, and (c) report images obtained by decoding linear interpolations between Z1 and Z2 in the latent space. Researchers often tend to judge the quality of the model based on these interpolations, concluding that the model performs poorly if the interpolations don't look realistic and vice versa. The authors of the paper argue that this procedure has drawbacks, because in typical modern use cases (Gaussian / uniform prior Pz) the aforementioned interpolations are not distributed according to Pz anymore, and thus are likely to be out of the domain where decoder was actually trained. \n\nI would say the main contributions of the paper are:\n(1) The sole fact that the paper highlights the problems of linear interpolation based evaluation is already important.\n(2) Observation 2.2, stating that if (a) Pz has a finite mean and (b) aforementioned linear interpolations are still distributed according to Pz, then Pz is a Dirac distribution (a point mass).\n(3) The authors notice that Cauchy distribution satisfies point (a) from (2), but as a result does not have a mean. The authors present some set of experiments, where DCGAN generator is trained on the CelebA dataset with the Cauchy prior. The interpolations supposedly look nice but the sampling gets problematic, because a heavy tailed Cauchy often results in the Z samples with excessively large norm, where generator performs poorly.\n(4) The authors propose several non-linear ways to interpolate, which keep the prior distribution unchanged (Sections 3.4 and 3.5). In other words, instead of using a linear interpolation and Pz compatible with it (which is necessarily is heavy tailed as shown in Observation 2.2), the authors propose to use non linear interpolations which work with nicer priors Pz, in particular the ones with finite mean.\n\nI think this topic is very interesting and important, given there is still an unfortunate lack of well-behaved and widely accepted evaluation metrics in the field of unsupervised generative models. \n\nUnfortunately, I felt the exposition of the paper was rather confusing and, more importantly, I did not find a clear goal of the paper or any concrete conclusions. One possible conclusion could be that the generative modelling community should stop reporting the linear interpolations. However, I feel the paper is lacking a convincing evidence (from what I could find the authors base all the conclusions on one set of similar experiments performed with one generator architecture on one data set) in order to be viewed as a significant contribution to the generative modeling field. On the other hand, the paper has not enough insights to constitute a significant theoretical contribution (I would expect Observation 2.2 to be already known in the probability field). \n\nOverall, I have to conclude that the paper is not ready to be published but I am willing to give it a chance. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting ideas and observations;  very early work",
            "review": "== Paper overview ==\nGiven a latent variable model (deep generative model), the paper ask how we should interpolate in the latent space. The key idea is to derive a natural interpolant from the prior distribution p(z), where z is the latent variable. The idea is that the interpolation function you apply to a variable z should not change the distribution of z of the start and end points of the interpolation curve are identically distribution. Example: consider two unit-length points drawn from a standard Gaussian, then linear interpolation of these points will result in points of smaller norm and hence different distribution. Differerent priors and corresponding interpolants are demonstrated and discussed. Empirical results are more of an illustrative nature.\n\n== Pros/cons ==\n+ The paper contribute a new and relevant point to an ongoing discussion on the geometry of the latent space.\n+ The key point is well-articulated and relevant mathematical details are derived in detail along the way.\n\n- I have some concerns about the idea itself (see below); yet, while I disagree with some of the presented view points, I don't think that diminishes the contribution.\n- The empirical evaluation hardly qualifies as such. A few image interpolations are shown, but it is unclear what conclusions can really be drawn from this. In the end, it remains unclear to me which approach to interpolation is better.\n\n== Concerns / debate ==\nI have some concerns about the key idea of the paper (in essence, I find it overly simplistic), but I nonetheless find that the paper brings an interesting new idea to the table.\n\n1) In section 1.1, the authors state \"one would expect the latent space to be organized in a way that reflects the internal structure of the training dataset\". My simple counter-question is: why? I know that this is common intuition, but I don't see anything in the cost functions of e.g. VAEs or GANs to make the statement true. Generative models, as far as I can see, only assume that the latent variables are somehow compressed versions of the data points; no assumptions on structure seems to be made.\n\n2) Later in the same section, the authors state \"In absence of any additional knowledge about the latent space, it feels natural to use the Euclidean metric\". Same question: why? Again, I know that this is a common assumption, but, again, there is nothing in the models that seem to actually justify such an assumption. I agree that it would be nice to have a Euclidean latent space, but doesn't make it so.\n\n3) In practice, we often see \"holes\" in the \"cloud\" of latent variables, that is regions of the latent space where only little data resides. I would argue that a good interpolant should not cross over a hole in the data manifold; none of the presented interpolants can satisfy this as they only depend no the start and end points, but not on the actual distribution of the latent points. So if the data does not fit the prior or are not iid, then the proposed interpolants will most likely perform poorly. A recent arXiv paper discuss one way to deal with such holes: https://arxiv.org/abs/1806.04994",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting analysis of linear interpolations with respect to the prior in the latent space, however the paper needs some improvements especially in the motivation and its implications.",
            "review": "The authors study the problem of when the linear interpolant between two random variables follows the same distribution. This is related to the prior distribution of an implicit generative model. In the paper, the authors show that the Cauchy distribution has such a property, however due to the heavy-tails is not particularly useful. In addition, they propose a non-linear interpolation that naturally has this property.\n\nTechnically the paper in my opinion is solid. Also, the paper is ok-written, but I think it needs improvements (see comments).\n\nComments:\n\n#1) In my opinion the motivation is not very clear and should be improved. In the paper is mentioned that the goal of shortest path interpolation is to get smooth transformations. So, in principle, I am really skeptical when the linear interpolant is utilized as the shortest path. Even then, what is the actual benefit of having the property that the linear interpolants follow the same distribution as the prior? How this is related to smoother transformations? What I understand is that, if we interpolate between several random samples, we will get less samples near the origin, and additionally, these samples will follow the prior? But how this induces smoothness in the overall transformation? I think this should be explained properly in the text i.e. why is it interesting to solve the proposed problem.\n\n#2) From Observation 2.2. we should realize that the distribution matching property holds if the distribution has infinite mean? I think that this is implicitly mentioned in Section 2.2. paragraph 1, but I believe that it should be explicitly stated.\n\n#3) Fig.1 does not show something interesting, and if it does it is not explained. In Fig. 2 I think that interpolations between the same images should be provided such that to have a direct comparison. Also, in Fig. 3 the norm of Z can be shown in order to be clear that the Cauchy distribution has the desired property. \n\n#4) Section 2.2. paragraph 6, first sentence. Here it is stated that the distribution \"must be trivial or heavy-tailed\". This refers only to the Cauchy distribution? Since earlier the condition was the infinite mean. How these are related? Needs clarification in the text.\n\n#4) In Figure 4, I believe that the norms of the interpolants should be presented as well, such that to show if the desired property is true. Also in Figure 5, what we should see? What are the improvements when using the proposed non-linear interpolation?\n\n\nMinor comments:\n\n#1) Section 1.2. paragraph 2. For each trained model the latent space usually has different structure e.g. different untrained regions. So I believe that interpolations is not the proper way to compare different models.\n\n#2) Section 1.3 paragraph 1, in my opinion the term \"pathological\" should be explained precisely here. So it makes clear to the reader what he should expect.\n\n#3) Section 2.2. paragraph 2. The coordinate-wise implies that some Z_i are near zero and some others significantly larger? \n\nIn generally, I like the presented analysis. However, I do not fully understand the motivation. I think that choosing the shortest path guarantees smooth transformations. I do not see why the distribution matching property provides smoother transformations. To my understanding, this is simply a way to generate less samples near the origin, but this does not directly means smoother transformations of the generated images. I believe that the motivation and the actual implications of the discussed property have to be explained better.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}