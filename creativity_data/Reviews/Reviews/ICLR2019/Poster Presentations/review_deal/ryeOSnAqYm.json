{
    "Decision": {
        "metareview": "This paper analyzes existing approaches to program induction from I/O pairs, and demonstrates that naively generating  I/O pairs results in a non-uniform sampling of salient variables, leading to poor performance. The paper convincingly shows, via strong evaluation, that uniform sampling of these variables can much result in much better models, both for explicit DSL and implicit, neural models. The reviewers feel the observation is an important one, and the paper does a good job providing sufficiently convincing evidence for it.\n\nThe reviewers and AC note the following potential weaknesses: (1) the paper does not propose a new model, but instead a different data generation strategy, somewhat limiting the novelty, (2) Salient variables that need to be uniformly sampled are still user specified, (3) there were a number of notation and clarity issues that make it difficult to understand the details of the approach, and finally, (4) there are concerns with the use of rejection sampling.\n\nThe authors provided major revisions that address the clarity issues, including an addition of new proofs, cleaner notation, and removal of unnecessary text. The authors also included additional results, such as KL divergence evaluation to show how uniform the distribution is. The authors also described the need for rejection sampling, especially for Karel dataset, and clarified why the Calculator domain, even though is not \"program synthesis\", still faces similar challenges. The reviewers agreed that not having a new model is not a chief concern, and that using rejection sampling is a reasonable first step, with more efficient techniques left for others for future work.\n\nOverall, the reviewers agreed that the paper should be accepted. As reviewer 1 said it best, this paper \"is a timely contribution and I think it is important for future program synthesis papers to take the results and message here to heart\".",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Important observation, backed by solid work"
    },
    "Reviews": [
        {
            "title": "Nice evaluations, empirically sound methodology, but no new model",
            "review": "This is a nice paper. It makes novel contributions by investigating (a) the problem of skewed dataset distributions in neural program synthesis, specifically program induction from given I/O pairs, and (b) the extent to which making them uniform would improve model performance. \n\nThe paper argues that there are inevitable and artificial sparsities as well as skews in existing datasets (e.g. pruning illegal I/O pairs, naive random sampling tends not to generate complex nested control-flow statements), and the principled way to minimize these sparsities and skews is to make distributions over salient random variables uniform. The authors evaluate their hypothesis empirically on two flavors of neural program synthesis methods: program inductions on explicit DSL represented by Karel, and implicit differentiable neural program synthesizers (such as stack, RAM, GPU as cited in section 2) represented by a Calculator example. In evaluations, they construct few challenging “narrower” datasets and show baseline models perform significantly worse than models trained on datasets with uniform distributions (by 39-66 pp). Along this line, they also show uniform models consistently perform much better than baseline ones on other out-of-distribution test sets. To show how bad a model would perform if it were trained on a skewed training set, they train models on narrower datasets and evaluate them on different narrower sets.\n\nThe strength of this paper are:\n(1) It has complete and empirically sound evaluations: both showing how much better uniform models would be and how much worse non-uniform models would be.\n\n(2) Although we might doubt the salient random variables are handcrafted and rejection sampling wouldn’t make the dataset completely uniform, they include evaluations on out-of-distribution datasets (e.g. CS106A dataset in section 5.2) to show that uniform models still perform better and thus their sampling scheme does cover some non-obvious sparsities and skews.\n\n(3) Despite the doubt on efficiencies of rejection sampling, they include both a proof and empirical results (section 8.3 and 8.4) to show they need sample O(1/ε) times before finishing.\n\nWeaknesses:\n(1) No new model. This work has solely using the existing model from Bunel et al. (2018) in the Karel domain and didn’t propose a new model that illustrates possibly a way to utilize/demonstrate the uniformity of dataset.\n\n(2) The calculator example is relatively too trivial to represent the whole genre of implicit differentiable neural program synthesizer (e.g. stack, GPU, RAM). \n\n(3) No statistical tests (such as chi-square test) to support the claim about uniformity (even on chosen salient variables) \n\nQuestions:\n(1) What if the distribution of real-world programs are skewed and neural synthesizers are supposed to take advantage of their skewness?\n\n(2) Why would you claim the calculator example is not a program synthesis task while intending to use it to represent another genre of program synthesis methods?\n\nSuggestions:\n(1) To show that current salient random variables do not make the dataset theoretically uniform but are still approximate enough, why not construct some distinct held-out salient variables (such as memory/grid/marker query times, executing time) from existing ones, construct narrower test sets accordingly, and hopefully show uniform models still perform significantly better than baseline?\n\n(2) In section 8.2, why not write the proportionality statement in two lines so that people wouldn’t be confused to think Pr[X=x] = 1 while intending to show Pr[X=x] ∝ 1(an arbitrary constant) so that Pr[X] is uniform?\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Nice presentation of a serious issue, with some flaws",
            "review": "This paper provides a good presentation of a serious problem in evaluating (as well as training!) performance of machine learning models for program synthesis / program induction: considering specifically the problem of learning a program which corresponds to given input/output pairs, since large datasets of \"real-world\" programs typically do not exist it necessary to construct a synthetic dataset for training and testing; this requires both (a) generating programs, and (b) generating input/output examples for these programs. Enumerating either all possible programs or examples is typically impossible, and so a sampling scheme is used to simulate \"reasonable\" programs and examples. This may hinder generalization to other data not often produced by the sampling scheme.\n\nTo address this, the paper then argues that programs should be synthesized from a distribution which as as uniform as possible over a set of user-specified statistics (the \"salient variables\") as well as over the input space. Intuitively, this makes sense: maximizing the entropy of the synthetic data should provide good coverage over the entire input space. However, there are a few ways in which the particular approach is unsatisfying:\n\n(1) It requires manual curation of salient random variables. This sort of punts the decision of \"what should my sampling procedure be\" to \"what is my choice of salient variables to make uniform\". I agree that this is still an improvement.\n\n(2) The procedure described for generating synthetic examples is essentially a rejection sampling algorithm, and it will fail to generate examples in a reasonable timeframe if the original proposal distribution is highly non-uniform, or if the salient random variables include values which fall in the tail of the proposal distribution.\n\nAlso, relatedly, I don't follow the description of correctness in section 8.2 at all. What is meant by the \"= 1\" at the end of the line right before \"… And thus…\"? Clearly P_r[X=x] cannot both equal 1, and equal k. Is the \"=1\" meant to only mean the summand itself? If so, please fix the notation. Also, I assume that k is meant to be the cardinality of the set {s: X(s) = x}, but this is not defined anywhere. Notational issues aside, unless the mapping X(s) from sample to salient variable is one-to-one, then I'm not clear how the P_q[X = X(s)] would relate to q(s). This should be made more clear. Also, I believe there need to be conditions on q(s), e.g. such that min_x P_q[X = x] must always be greater than zero.\n\n\nThese issues aside, the empirical demonstrations on the Karel the Robot examples are nicely presented and make the point well. My primary question here would be around section 5, the \"real-world benchmarks\", where it is observed that the baseline model performs less well than re-training on a uniform / homogenized dataset. While it is nice that it performed better, I don't understand why even the better number (19.4%) is so low; the performance of the uniform model in table 1 tends to be much higher (in the 60% to 70% range). This would suggest that the uniform model perhaps is significantly *underweighting* important parts of the space. What is causing this? e.g. what do the salient variables look like for real-world examples?\n\n\nFinally, I am not sure I understand how the calculator example fits into this paper. Unless I misunderstand, it is not a program synthesis task, but rather a regression task. Clearly it does still depend on generation of synthetic data, but that is more a different task (as described in section 2). I feel its inclusion somewhat dilutes the paper. Rather, it would be nice to see more discussion or investigation into the failure modes of these trained models; for example, looking deeper at the handling of control flow and recursion, or at whether particular values of salient variables tended to be correlated with success or failure under different train / test regimes.\n\n\n\n===== after updates =====\n\nThanks for the edits — I believe the overall paper is more clearly presented, now.\n\nI still think it is a stretch to consider the calculator domain is a program induction problem: it is a regression problem, from an input string to an output integer, or alternately a classification problem, since it computes the result mod 10. The only way I could understand this as a program induction problem is rather obliquely, if the meaning is that any system which is able to compute the result of the calculator evaluation has implicitly replicated internally, in some capacity, the sequence of instructions which are evaluated. I don't think this is really very clear though; for example, given two calculator programs, one a subprogram of another (e.g., \"4*(3+2)\" and \"3+2\"), do the resulting \"induced\" computations share the same compositional structure? The examples of program induction in section 2 are largely architectures which are explicitly designed to have properties which mimic conventional programming languages (e.g. extra data structures as memories, compositionality, …). In contrast, the calculator example in this paper simply uses an LSTM. \n\nThat said, I think it's still a great example! Learning a fast differentiable model which accurately mimics existing non-differentiable model has tons of applications, and has exactly the same challenges regarding synthetic data. \n\n\n\nI have to say I find the new section 8.3 a bit intuitively challenging; e.g. it's not clear really how long a waiting time of 48 log(2|X|/\\delta) / (p|X|^2 z^2) really is. But, to that end, I appreciate the empirical discussion in 8.4–8.6.\n\nI've updated my review to increase my score — I lean towards accepting this paper, as it is a timely contribution and I think it is important for future program synthesis papers to take the results here to heart. I've reduced my confidence slightly, as I have not fully reviewed the new proof in 8.3.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Lacking arguments why the proposed method generalizes well to other problem settings",
            "review": "The paper presents a methodology for improved program synthesis by generating datasets for program induction and synthetic tasks from uniform distributions. This method is evaluate on two problem settings. \n\nThe methodology is presented in section 3. Even though the outline does not seem to be complicated, the presentation in section 3 leaves me puzzled. The the second paragraph two sets of silent variables are introduced X_1,...,X_n and Z_1,...,Z_m but never used again the rest of the paper. In the third and forth paragraph details about the Karel domain are presented without the Karel domain having been introduced. It seems you are using rejection sampling to sample from a uniform distribution. Why can you not sample from a uniform distribution directly? What do you mean with the notation X(s)? What are you proving in Appendix? Would maybe be clearer if you presented it as a theorem/lemma.\n\nThe remaining part of the paper evaluates this methodology on two specific problem settings, the Karel domain and Calculator domain. The generalization performance is increased when trained on datasets generated by the method presented in the paper. However, I cannot find and strong arguments in the paper why this property should generalize to other problem settings. To me the analysis and experimental results seems to be tailored to the two problems settings used in the paper.\n\n==== After revision ====\n\nThe authors have done a great job addressing the concerns I had about the clarity. Consequently, I have raised my score, whereas my fairly low confidence still remains.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}