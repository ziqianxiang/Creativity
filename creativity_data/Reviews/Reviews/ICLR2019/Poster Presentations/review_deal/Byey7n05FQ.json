{
    "Decision": {
        "metareview": "The paper makes novel explorations into how MPC and approximate-DP / value-function approaches, with value-fn ensembles to model value-fn uncertainty, can be effectively combined.  The novelty lies in exploring their combination. The experiments are solid. The paper is clearly written.\n\nOpen issues include overall novelty, and delineating the setting in which this method is appropriate.\n\nThe reviewers and AC are in agreement on what is in the paper. The open question is whether\nthe combination of the ideas is interesting. \n\nAfter further reviewing the paper and results. the AC believes that the overall combination of ideas and related evaluations that make a useful and promising contribution. As evidenced in some of the reviewer discussion, there is often a\nconsiderable schism in the community regarding what is considered fair to introduce in terms of\nprior knowledge, and blurred definitions regarding planning and control. The AC discounted some of the\nconcerns of R2 that related more to discrete action settings and theoretical considerations; these \noften fail to translate to difficult problems in continuous action settings.  The\nAC believes that R3 nicely articulates the issues of the paper that can be (and should be) addressed in the writing, i.e., to\ndescribe and motivate the settings that the proposed framework targets, as articulated in the reviews and ensuing discussion. \n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Useful combination of MPC, approximate-DP, and value-function ensembles"
    },
    "Reviews": [
        {
            "title": "Lucid paper with nice ideas, but problem setting not completely clear",
            "review": "This paper was a joy to read.   The description and motivation of the POLO framework was clear, smart, and sensible.  The fundamental idea is to explore the interplay between value-function estimation and model-predictive control and demonstrate how they benefit one another.  None of these ideas is fundamentally new, but the descriptions and their combination is very nice.\n\nAs I finished the paper, though, I was left with a lingering lack of understanding of the exact problem setting that is being addressed. The name is cute but didn't help clarify.    As I understand it:\n- we have a correct dynamics model (I'm assuming that's what \"nominal dynamics model\" means) and a good trajectory optimization algorithm\n- the agent has limited online cognitive capacity\n- there is no opportunity for offline computation\nIf offline computation time were available, then we could run this algorithm (or your favorite other RL algorithm) in the agent's head before taking any actions in the actual world.   That does not seem to be the setting here, although it does seem to me that you might be able to show that POLO is a good algorithm for finding a value function, offline, with no actual interaction with the world.\n\nSo, fundamentally, this paper is about action under computational time constraints.   One strategy would be for the robot to use 7 of its cores to run your favorite approximate DP / RL algorithm in parallel with 1 core that's used for action selection.  Why is that worse than your algorithm 1?\n\nSetting this question aside, I had some other comments:\n- It is better *not* to use \"trajectory optimization\" and \"model-predictive control\" interchangeably.  I can use traj opt in other circumstances (e.g. with open loop trajectory following) and could use other planners for MPC.\n- Some version of lemma 2 probably (almost certainly) already exists somewhere in the literature;  I'm sorry, though, that I can't point you to a concrete reference.\n- The argument about MPC letting us approximate H Bellman backups is plausible, but seems somewhat subtle;  it would be good to elaborate it in some more detail.\n- The set of assertions and experiments is very nice.\n- Why are no variances shown in figure 3?   Why does performance seem to degrade after a certain horizon.\n\nThis paper doesn't seem really to be about learning representations.  I don't know if that's important to the ICLR decision-making.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "limited insight and novelty",
            "review": "In this paper, the authors propose POLO, a reinforcement learning algorithm which has access to the model of the environment and performs RL to mitigate the planning cost. For the planning, POLO uses the known model of the environment up to a fixed horizon H and then use an approximated value function in the leaf nodes. This way, instead of planning for an infinite horizon, the planning is factored to a shorter horizon, resulting in lower computation cost.\n\nThe novelty and motivation behind this approach is limited. Similar or even more general approach for discrete action space is introduced in \"Sample-Efficient Deep RL with Generative Adversarial Tree Search\" where they also learn the model of the environment and additionally consider the error due to the model estimation. There is also a clear motivation in the mentioned paper while I could not find a convincing one for the current paper. \nPutting the novel limitation aside,  both of these paper, the current paper, and the paper I mentioned, suffer from very lose estimation bounds. Both of these works bound somewhat similar (not the same) things via L_inf error of value function which in practice does not necessarily result in useful or insightful upper bounds (distribution dependent bound is desired). Moreover, with the assumption of knowing the environment model, the implication of the current work is significantly limited.\n\nThe authors do a good job of writing the paper and the paper is clear which is appreciatable.\n\nIn equation 6 the authors use log-sum-exp and claim it corresponds to UCB, but they do not provide any evidence to support their claim. \n\nIn addition, the Bayesian linear regression in the tabular setting is firstly proposed in Generalization and Exploration via Randomized Value Functions and beyond tabular setting (the setting in the current paper) was proposed in Efficient Exploration through Bayesian Deep Q-Networks. \n\nThe claims in this paper are not strong enough and the empirical study does not strongly support or provide sufficient insight. For example experiments in section 3.2 does not provide much insight beyond common knowledge.\n\nWhile bridging the gap between model based and model free approaches in RL are significantly important research directions in RL, I do not find the current draft significant enough to shed sufficient light into this topic.\n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Nice results but lean technical contribution",
            "review": "This paper proposes to combine fitted value iteration with model predictive control (MPC) to speed up the learning process. The value iteration is the \"Learn offline\" subsystem while MPC is the \"Plan online\" subsystem. In addition, this paper also proposes an exploration technique that increases exploration if the multiple value function estimators disagree. The evaluation is complete and shows nice results.\n\nHowever, I did not rank this paper high for two reasons. First, it is not clear to me how the model is acquired in MPC. Does the method learn the model? Does the method linearize the dynamics and assume a linear model? I am not sure. I suspect that the method just uses the simulator as the model. If it is the case, the method is not so useful because for complexity systems, such as humanoids, we do not know the model. And the comparisons with model-free learning algorithms are not fair because the paper assumes that the model is given. If this is not the case, I suggest that a more detailed description of MPC should be presented in Section 2.3.\n\nSecond, the technical contributions are lean. The three main components, 1) fitted value iteration, 2) MPC and 3) exploration based on multiple value function estimates, are not novel. The combination of them seems straight forward. For example, the H-step Bellman update (Section 2.3) is a blend between Monte-Carlo method and Q learning. It seems to be similar to the TD(\\lambda) method. Thus, it is not surprising that it can accelerate convergence of value function.\n\nFor the above reasons, I would not recommend accepting this paper at this time.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}