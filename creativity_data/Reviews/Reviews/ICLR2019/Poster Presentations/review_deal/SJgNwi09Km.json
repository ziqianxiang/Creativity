{
    "Decision": {
        "metareview": "A well-written paper that proposes an original approach for leaning a structured prior for VAEs, as a latent tree model whose structure and parameters are simultaneously learned. It describes a well-principled approach to learning a multifaceted clustering, and is shown empirically to be competitive with other unsupervised clustering models. \nReviewers noted that the approach reached a worse log-likelihood than regular VAE (which it should be able to find as a special case), hinting towards potential optimization difficulties (local minimum?). This would benefit form a more in-depth analysis. \nBut reviewers appreciated the gain in interpretability and insights from the model, and unanimously agreed that the paper was an interesting novel contribution worth publishing.\n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "Interesting approach to completely learn a structured prior, but reaching worse likelihood "
    },
    "Reviews": [
        {
            "title": "Well written, carefully thought through, and very interesting paper with impressive empirical results",
            "review": "This paper introduces a new VAE model, the latent tree VAE (LTVAE), which aims to learn models with multifaceted clustering, that is separate clusterings are enforced on different subsets of the latent features.  This is achieved using a tree-structured prior on a set of discrete \"super latent variables\" (Y_1,...,Y_L) that identify which cluster the datapoint falls into for each separate facet (i.e. there is a separate clustering associated with each Y_n).  The subset of the standard latent variables z then form a Gaussian mixture model (GMM) for each Y_n.   Both the structure of this setup (i.e. the associated graphical model) and the parameters (i.e. means and variances of the clusters) are learned during training.  This introduces a number of computational challenges not usually seen in for VAE training, for which, seemingly well thought through, novel schemes are introduced, most notably a message passing scheme for calculating gradients of the log marginal p(z).\n\nOverall I think this is a very good paper.  The exposition of the work is, for the most part, very good - the paper was a pleasure to read.  I think that the key idea is novel and adds something unique and useful to the literature, I thus think it is work which will be of substantial interest to the ICLR community.  The quality of the paper is also very good: algorithmic details seem to have been well thought through and the experimental evaluation is above average, both in terms of apparent performance and in the breadth of experiments considered.  I would very much like to see this work accepted to ICLR and I think that the extra use of space over 8 pages in the submission is justified.  However, I do have some questions and concerns that I would like to see addressed in the rebuttal period and I may lower my score if they are not.\n\nThe key issues I would like to see addressed further discussion on are:\na) There is no discussion about what is done for the encoder in the paper.  This is surely a very important consideration here as if the encoder is not expressive enough, this will impact the learned models.  For example, the dependency structures of the latent space induce particular dependencies in the posterior that must be carefully handled to avoid harming the learning (see e.g. https://arxiv.org/abs/1712.00287).\nb) I would like to see some numerical results for the similarity between the different clusterings that are learned.  A lot of the novelty of the work rests on being able to pick up different clusterings with the different facets.  However, the results suggest that the clusterings may actually have very significant overlap and so this should be quantified.\nc) The approach is presuming substantially slower than a setup where the structure is pre-fixed.  I think it is fine even if there is a big slow down, but I would like to see timing information so that the reader can assess how much higher the time cost is.\nd) As far as I can tell (sorry if I have made a mistake), the presented results are from single runs.  I would like to see information about the variability across different runs so that the fragility of the approach can be assessed.\ne) I would like to see more justification for having a dependency structure between the Y's, ideally both in motivating this choice and in experimental evaluation to check it (more generally ablation studies for different components of the algorithm would improve the paper).  Might it be possible to use this in a way the encourages the different clusterings to be distinct from one another?\n\nOther comments:\n1) Though the writing is generally very good, there are a few exceptions:\n- The second paragraph in the intro becomes a list of related work from the point where DEC is introduced.  This should be moved to the related work to improve the flow (just cite those papers at the end of the first sentence in the third paragraph) and it would be good for it to be less of a list of separate things and more something that puts the current work in the context of other approaches.\n- The paragraph after Eq 3 needs some rewriting\n- The explanations around and including equations 5 and 6 were quite poor: \\pi is referred to but not used, it is not made clear that that g is the gradient of log p(z) instead of p(z), use brackets for the log in Eq 6 to avoid ambiguity\n2) The reference formatting is wrong (i.e. cite is used everywhere instead of citep)\n3) I thought the motivation for the approach in the intro was very good\n4) As the seemingly most related work, it would be good to elaborate more on the Goyal et al paper and the differences of your approach to theirs.  Is there a reason this is not used as a baseline in the experiments?\n5) I could not understand the step from the gradient to the gradient of the log in Eq 6.  Is this because p(y_b|z) = f(y_b) Norm(..)?\n6) The text in figures 2 and 3 is too small and difficult to make out.\n7) I think it is misleading to talk about p(z) as being a marginal likelihood and would use the term marginal prior, or just marginal, instead.\n8) I thought Figure 4b provided a nice demonstration.\n9) Is there a reason that log likelihood / ELBO scores are only provided on MNIST and only for the LTVAE / VAE?  I might be wrong, but I thought at least some of the other baselines provide this and those results presumably already exist as a side effect from calculating the clustering scores?  Relatedly, I'm aware that a previous version of this work included estimates of the normalized mutual information -- is there any reason these are no longer included?\n10) Did the larger dimensional latent spaced used for the qualitative results improve or worsen the performance of previous metrics?\n\nMinor points / typos\n- mehod -> method\n- of generation network -> of the generation network\n- brackets in eq 7\n- MoG not defined in section 4.5",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach, but experiments could have a more in-depth analysis",
            "review": "The authors propose to augment the Variational AutoEncoder [1] with a latent prior modeled by a Gaussian Latent Tree Model [2], allowing to introduce a hierarchical structure of clusters in the learned representation. The LT-VAE not only learns the location of each cluster to best represent the data, but also their number and the hierarchical structure of the underlying tree. This is achieved by a three-step learning algorithm. Step 1 is a traditional training of the encoder and decoder neural networks to improve their fitting of the data. Step 2 is an EM-like optimization to better fit the parameters of latent prior to the learned posterior. And step 3 adapts the structure of the latent prior to improve its BIC score [3], which balances a good fit of the latent posterior with the number of parameter (and thus complexity) of the latent prior.\n\nExperiments on synthetic data confirms the ability of the model to discover latent multifaceted clustering, and tests on 4 datasets shows it to be competitive with other unsupervised clustering models. Qualitative interpretation of samples from the learned model shows that the model learns a clustering that is clearly relevant to the data, while maybe not obvious to interpret.\n\nThe paper is well written and easy to follow (I however found a few typos and small mistakes that I'll list at the end of this review). The idea of using a structure on the latent prior of a VAE to learn a clustering of the data is not new, but the authors propose here an interesting approach to it, with a clearly described algorithm.\n\nHowever, I would have liked to see a more in-depth analysis of the behavior of the model on the various datasets, and my reading of this paper raised several questions that found no answer:\n\n1. What gains does the hierarchical structure on the Y variables provide? The paper does not analyze whether the models they trained actually learned conditional dependencies on the Y_i variables. How would this compare to the same model, with the only difference that the Y_i are fixed to be independent of each other (but still learning the number of Y_i and how the z_j are distributed between them) ?\n\n2. This is linked to the previous one. On the tests of the dataset, how do the different facets interact with each other? How are the samples from the different clusters of facet 2 when facet 1 is fixed to a particular cluster? Assuming the learned dependency is that Y_1 is the parent of Y_2, does the interpretation of each value of Y_2 change depending on the value of Y_1?\n\n3. The VAE with diagonal gaussian latent has a natural tendency to achieve sparcity in its latent space [4], making it robust to having too many latent neurons. Does this property hold with LT-VAE? If so, are the \"unused\" neurons organized in a particular way among the different learned facets?\n\nI'd be reluctant to accept this paper without answers to points 1 and 2, which in my opinion are needed to justify the \"tree\" part of the \"latent tree model\" choice for the latent space. I'd also be very interested in an answer to point 3, which would give good insights regarding the design choices for applying this model to new problems (how important is the choice of the size of the latent space?), but I'm not considering it blocking acceptance.\n\n[1] https://arxiv.org/abs/1312.6114\n[2] http://jmlr.org/papers/volume5/zhang04a/zhang04a.pdf\n[3] https://projecteuclid.org/euclid.aos/1176344136\n[4] https://arxiv.org/abs/1706.05148\n\n--------------------------------\n\nNotes and typos:\n\n- In the introduction, \"Deep clustering network network (DCN)\", the word \"network\" is repeated \n- After equation 5, \"... where \\pi( . ) denotes the parent node ...\", the \"pi\" symbol does not appear in the equation at all, neither in the following equation, so I guess this part of the sentence should be removed\n- In section 3.3, you write that you define 5 operators, but follow by listing 7 (NI, ND, SI, SD, NR, PO and UP)\n- In section 4.1, I believe W lives in R^(10x4) not R^(10x2)\n- In section 4.5 the acronym \"MoG\" (\"Mixture of Gaussian\" I guess) is used without being introduced previously",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review",
            "review": "Revision post-discussion: The paper's notation and model has been clarified, and my concerns about the paper have been addressed. Proposing a latent tree structure on the latent space of generative models is a strong contribution, the model performs well and seems to find meaningful and interpretable structure in the latent space.\n\n\nThe paper proposes a latent tree superstructure for the latent space of VAE’s. The idea itself is novel and interesting, and could have major impact in learning structured manifolds.\n\nThe overall presentation of the method is direct but slightly confusing. It seems that the zb grouping corresponds to different dimensions of the full z_i-vector of a single data point x_i. This should be made more explicit. \n\nThe method itself has three levels of groupings: the zb’s, the conditioned variables Yb, and the connections between the Y’s. The method is also called a  Bayesian Network, but the paper seems to avoid defining it as a BN. I wonder if the method could be presented in a simpler form, if all the structure is necessary, and if the method could be defined directly as a BN. For instance, why do the Y’s have to have a hierarchical tree structure, wouldn’t a “flat” grouping into zb's be sufficient? \n\nIn eq 2 the p(z) is defined as a mixture of Y-conditioned Gaussians, while in eq 4 its defined in the conventional encoder form N(z ; mu_x, sigma_x). These forms don’t seem to be compatible with each other. The term H seems to be entropy, but its not explained. It can’t be computed if we use the eq 2 definition of p(z). The interplay between these two structures is unclear. Furthermore, in fig 1 the tree is showed as a network (no arrows), while in fig 2 its a tree. I can’t find the definition for the dependencies P(Y | Y’), are these simply conditional density tables, or are they implicit? I also can’t see how are the \\Sigma_{yb} defined. Are they of full rank? What is their dimension?\n\nThe inference sections are well motivated and efficient techniques are used. \n\nThe synthetic experiment has 4 dimensional “z”, but the “W” matrix is 10x2, these do not match. What is the connection between Y_1 and Y_2 (in fig4 there is a dependency between)? Why is the dependency undirected if the model is a tree? The fig4b does not show ground truth to assess how well the model fits. The experiment should also include comparisons to the mentioned earlier works, and show how they perform. Why is there an arrow from the green scatter to the z3/z4? The main problem of the synthetic example is that it does not demonstrate why the tree structure learning is useful. The experiment should highlight a case where there is a natural latent tree structure corresponding to some realistic phenomena in real datasets.\n\nThe section 4.3. shows that the proposed method does find better representations of the MNIST than VAE, but does not mention that there are numerous extended VAE methods (and others) that would perform better than the LTVAE here. Those should be at least acknowledged, and preferably compared to.\n\nThe main results of the paper are very good with great performance in clustering, and the facets and clusters look great. The system has clearly learnt meaningful latent structures.\n\nThere are no learning curves or running time analyses. One would expect the proposed method to be slow with multiple levels of inference (tree structure, tree parameters, AE networks), and this should be discussed. How large datasets can it handle?\n\nOverall the paper proposes a BN-style structure on VAE latent space with great performance, but somewhat incomplete experimental section, and some presentation issues.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}