{
    "Decision": {
        "metareview": "To borrow the succinct summary from R1, \"the paper suggests a method for generating representations that are linked to goals  in reinforcement learning. More precisely, it wishes to learn a representation so that two states are similar if the \npolicies leading to them are similar.\" The reviewers and AC agree that this is a novel and worthy idea.\n\nConcerns about the paper are primarily about the following.\n(i) the method already requires good solutions as input, i.e., in the form of goal-conditioned policies, (GCPs)\nand the paper claims that these are easy to learn in any case.\nAs R3 notes, this then begs the question as to why the actionable representations are needed.\n(ii) reviewers had questions regarding the evaluations, i.e., fairness of baselines, additional comparisons, and \nadditional detail. \n\nAfter much discussion, there is now a fair degree of consensus.  While R1 (the low score) still has a remaining issue with evaluation, particularly hyperparameter evaluation, they are also ok with acceptance. The AC is of the opinion that hyperparameter tuning is of course an important issue, but does not see it as the key issue for this particular paper. \nThe AC is of the opinion that the key issue is issue (i), raised by R3. In the discussion, the authors reconcile the inherent contradiction in (i) based on the need of additional downstream tasks that can then benefit from the actionable representation, and as demonstrated in a number of the evaluation examples (at least in the revised version). The AC believes in this logic, but believes that this should be stated more clearly in the final paper. And it should be explained\nthe extent to which training for auxiliary tasks implicitly solve this problem in any case.\n\nThe AC also suggests nominating R3 for a best-reviewer award.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "good idea;  general consensus"
    },
    "Reviews": [
        {
            "title": "A good idea, but suffers from lack of clarity",
            "review": "The paper suggests a method for generating representations that are linked to goals in reinforcement learning. More precisely, it wishes to learn a representation so that two states are similar if the policies leading to them are similar.\n\nThe paper leaves quite a few details unclear. For example, why is this particular metric used to link the feature representation to policy similarity? How is the data collected to obtain the goal-directed policies in the first place? How are the different methods evaluated vis-a-vis data collection?  The current discussion makes me think that the evaluation methodology may be biased. Many unbiased experiment designs are possible. Here are a few:\n\nA. Pre-training with the same data\n\n1. Generate data D from the environment (using an arbitrary policy).\n2. Use D to estimate a model/goal-directed policies and consequenttly features F. \n3. Use the same data D to estimate features F' using some other method.\n4. Use the same online-RL algorithm on the environment and only changing features F, F'.\n\nB. Online training\n\n1. At step t, take action $a_t$, observe $s_{t+1}$, $r_{t+1}$\n2. Update model $m$ (or simply store the data points)\n3. Use the model to get an estimate of the features \n\nIt is probably time consuming to do B at each step t, but I can imagine the authors being able to do it all with stochastic value iteration. \n\nAll in all, I am uncertain that the evaluation is fair.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Paper lacks many important details.",
            "review": "The paper presents a method to learn representations where proximity in euclidean distance represents states that are achieved by similar policies. The idea is novel (to the best of my knowledge), interesting and the experiments seem promising. The two main flaws in the paper are the lack of details and missing important experimental comparisons.\n\nMajor remarks:\n\n- The author state they add experimental details and videos via a link to a website. I think doing so is very problematic, as the website can be changed after the deadline but there was no real information on the website so it wasn’t a problem this time.\n\n- While the idea seems very interesting, it is only presented in very high-level. I am very skeptical someone will be able to reproduce these results based only on the given details. For example - in eq.1 what is the distribution over s? How is the distance approximated? How is the goal-conditional policy trained? How many clusters and what clustering algorithm?\n\n- Main missing details is about how the goal reaching policy is trained. The authors admit that having one is “a significant assumption” and state that they will discuss why it is reasonable assumption but I didn’t find any such discussion  (only a sentence in 6.4).  \n\n- While the algorithm compare to a variety of representation learning alternatives, it seems like the more natural comparison are model-based Rl algorithms, e.g. “Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning”. This is because the representation tries to implicitly learn the dynamics so it should be compared to models who explicitly learn the dynamics. \n\n- As the goal-conditional policy is quite similar to the original task of navigation, it is important to know for how long it was trained and taken into account.\n\n- I found Fig.6 very interesting and useful, very nice visual help.\n\n- In fig.8 your algorithm seems to flatline while the state keeps rising. It is not clear if the end results is the same, meaning you just learn faster, or does the state reach a better final policy. Should run and show on a longer horizon.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Quite interesting idea, but unsufficiently mature piece of research",
            "review": "In this paper, the authors propose a new approach to representation learning in the context of reinforcement learning.\nThe main idea is that two states should be distinguished *functionally* in terms of the actions that are needed to reach them,\nin contrast with generative methods which try to capture all aspects of the state dynamics, even those which are not relevant for the task at hand.\nThe method of the authors assumes that a goal-conditioned policy is already learned, and they use a Kullback-Leibler-based distance\nbetween policies conditioned by these two states as the loss that the representation learning algorithm should minimize.\nThe experimental study is based on 6 simulated environments and outlines various properties of the framework.\n\nOverall, the idea is interesting, but the paper suffers from many weaknesses both in the framework description and in the experimental study that make me consider that it is not ready for publication at a good conference like ICLR.\n\nThe first weakness of the approach is that it assumes that a learned goal-conditioned policy is already available, and that the representation extracted from it can only be useful for learning \"downstream tasks\" in a second step. But learning the goal-conditioned policy from the raw input representation in the first place might be the most difficult task. In that respect, wouldn't it be possible to *simultaneously* learn a goal-conditioned policy and the representation it is based on? This is partly suggested when the authors mention that the representation could be learned from only a partial goal-conditioned policy, but this idea definitely needs to be investigated further.\n\nA second point is about unsufficiently clear thoughts about the way to intuitively advocate for the approach. The authors first claim that two states are functionally different if they are reached from different actions. Thinking further about what \"functionally\" means, I would rather have said that two states are functionally different if different goals can be reached from them. But when looking at the framework, this is close to what the authors do in practice: they use a distance between two *goal*-conditioned policies, not *state*-conditioned policies. To me, the authors have established their framework thinking of the case where the state space and the goal space are identical (as they can condition the goal-conditioned policy by any state=goal). But thinking further to the case where goals and states are different (or at least goals are only a subset of states), probably they would end-up with a different intuitive presentation of their framework. Shouldn't finally D_{act} be a distance between goals rather than between states?\n\nSection 4 lists the properties that can be expected from the framework. To me, the last paragraph of Section 4 should be a subsection 4.4 with a title such as \"state abstraction (or clustering?) from actionable representation\". And the corresponding properties should come with their own questions and subsection in the experimental study (more about this below).\n\nAbout the related work, a few remarks:\n- The authors do not refer to papers about using auxiliary tasks. Though the purpose of these works is often to supply for additional reward signals in the sparse reward context, then are often concerned with learning efficient representations such as predictive ones.\n- The authors refer to Pathak et al. (2017), but not to the more recent Burda et al. (2018) (Large-scale study of curiosity-driven learning) which insists on the idea of inverse dynamical features which is exactly the approach the authors may want to contrast theirs with. To me, they must read it.\n- The authors should also read Laversanne-Finot et al. (2018, CoRL) who learn goal space representations and show an ability to extract independently controllable features from that.\n\nA positive side of the experimental study is that the 6 simulated environments are well-chosen, as they illustrate various aspects of what it means to learn an adequate representation. Also, the results described in Fig. 5 are interesting. A side note is that the authors address in this Figure a problem pointed in Penedones et al (2018) about \"The Leakage Propagation problem\" and that their solution seems more convincing than in the original paper, maybe they should have a look.\nBut there are also several weaknesses:\n- for all experiments, the way to obtain a goal-conditioned policy in the first place is not described. This definitely hampers reproducibility of the work. A study of the effect of various optimization effort on these goal-conditioned policies might also be of interest.\n- most importantly, in Section 6.4, 6.5 and 6.6, much too few details are given. Particularly in 6.6, the task is hardly described with a few words. The message a reader can get from this section is not much more than \"we are doing something that works, believe us!\". So the authors should choose between two options:\n* either giving less experimental results, but describing them accurately enough so that other people can try to reproduce them, and analyzing them so that people can extract something more interesting than \"with their tuning (which is not described), the framework of the authors outperforms other systems whose tuning is not described either\".\n* or add a huge appendix with all the missing details.\nI'm clearly in favor of the first option.\n\nSome more detailed points or questions about the experimental section:\n- not so important, Section 6.2 could be grouped with Section 6.1, or the various competing methods could be described directly in the sections where they are used.\n- in Fig. 5, in the four room environment, ARC gets 4 separated clusters. How can the system know that transitions between these clusters are possible?\n- in Section 6.3, about the pushing experiment, I would like to argue against the fact that the block position is the important factor and the end-effector position is secundary. Indeed, the end-effector must be correctly positioned so that the block can move. Does ARC capture this important constraint?\n- Globally, although it is interesting, Fig.6 only conveys a quite indirect message about the quality of the learned representation.\n- Still in Fig. 6, what is described as \"blue\" appears as violet in the figures and pink in the caption, this does not help when reading for the first time.\n- In Section 6.4, Fig.7 a, ARC happens to do better than the oracle. The authors should describe the oracle in more details and discuss why it does not provide a \"perfect\" representation.\n- Still in Section 6.4, the authors insist that ARC outperforms VIME, but from Fig.7, VIME is not among the best performing methods. Why insist on this one? And a deeper discussion of the performance of each method would be much more valuable than just showing these curves.\n- Section 6.5 is so short that I do not find it useful at all.\n- Section 6.6 should be split into the HRL question and the clustering question, as mentioned above. But this only makes sense if the experiments are properly described, as is it is not useful.\n\nFinally, the discussion is rather empty, and would be much more interesting if the experiments had been analyzed in more details.\n\ntypos:\n\np1: that can knows => know\np7: euclidean => Euclidean\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}