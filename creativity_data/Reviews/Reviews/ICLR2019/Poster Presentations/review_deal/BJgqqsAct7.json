{
    "Decision": {
        "metareview": "The paper combines PAC-Bayes bound with network compression to derive a generalization bound for large-scale neural nets such as ImageNet. The approach is novel and interesting and  the paper is well-written. The authors provided detailed replies and improvements in response to reviewers questions, and all reviewers agree this is a very nice contribution.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Nice paper with novel results"
    },
    "Reviews": [
        {
            "title": "A very inspiring implementation but too many important details are missing.",
            "review": "This paper tries to push forward in important directions the seemingly increasingly powerful approach of using PAC-Bayesian formalism to explain low risks of training neural nets on real-life data. They take an interesting approach to evaluate these bounds by setting up a prior distribution as a mixture of Gaussians centered on possible heuristic compressions of the net  and this prior's variances are obtained by doing a layerwise grid search. This seems to give good risk bounds on certain known compressible nets using image data sets. \n\nLet me list out a bunch of issues that seem to be somewhat confusing in this paper (some of these were in the comment thread I had with the authors but I am repeating nonetheless for completeness) \n\n0.\nFirstly this form of the PAC-Bayes formula used here (Theorem 2.1) is of a more complicated form than what has been previously used in say these papers, https://arxiv.org/abs/1707.09564 Given this I strongly feel that there is a need for an explanation connecting this formalism to the usual one - particularly something that proves how this is stronger than the one in the paper I referred to earlier. \n\n1. \nIn the statement of Theorem 2.1 there is a \\lambda parameter over which the infimum is being taken. If I understand right in the experiments one is substituting the upperbound on KL from Theorem 4.3 into this RHS of Theorem 2.1 and evaluating this. Now there is also a \\lambda parameter in Theorem 4.3. Is this the same \\lambda as in Theorem 2.1 and when a grid-search is being done over \\lambda is the \"whole\" thing (theorem 2.1 upperbound with theorem 4.3 substituted) being minimized by choosing a good \\lambda? \n\nIf the two \\lambda s are different then is the choice of the 2 \\lambda s being optimized separately? \n\n(...the authors had earlier clarified that this is so and I strongly feel this is a very important clarification should be updated into the paper..)\n\n2. \nHow is the \\sigma of Theorem 4.3 chosen in the experiments? Am I right in thinking that this \\sigma is the posterior variance about which it is being said towards the end of page 6 that \"We add Gaussian noise with standard deviation equal to 5% of the difference between the largest and smallest weight in the filter.\" ? \n\nSo am I to understand that this is an arbitrary choice? Or is this choice dictated by some need to ensure that the posterior variance sigma is chosen so that under this distribution the sampled nets approximately compute the same function on the training data? (If yes, then what in the theory is motivating this?). \n\nTo the best of my understanding the results are highly dependent on this choice of sigma but there is virtually no explanation for this choice which was not even found by grid search. (As of now this is merely reflective of the fact that trained nets often have some noise resilience but its not a priori clear as to why that should be important to the PAC-Bayes formalism here.)   \n\n3.\nThe code based compression seems a bit mysterious to me given that I do not have enough familiarity with the algorithm that is being referred to. Hence it seems a bit weird as to why there is a sum over codebooks in the proof of Theorems 4.3. Naively I would have thought that there is a fixed codebook for a given compression scheme but here it feels that the compression scheme is a randomized algorithm which also generates a new codebook in every run of it. This seems unusual and seems to need more explanation and at the very least a detailed pseudocode explaining exactly how this compression is working.  \n\nThis point ties in with a somewhat larger issue I describe next...\n\n4.\nIn the previous reply to my comment the authors had shared their anonymized code and l had a look through the code. Its pretty evident from the code there are an enormous number of tweaks and hyperparameter tunings to make this work. There is very little insight otherwise as to why \"Dynamic Network Surgery' should work and its great that the authors have found an implementation that works on their image data. \n\nBut then the question arises that there should have been a cleanly abstracted out pseudocode explaining how the compression was done and how the dynamic network surgery was done. To my mind this implementation is the main contribution of the paper and giving the pseudocode for it in the paper seems not only important for essential completeness of the current paper but that could also then act as a springboard for many future attempts at trying to come up with theory for these mysterious procedures. \n\n\n\n\n\n\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "a nice bound for the ImageNet  ",
            "review": "The paper presents an application of PAC-Bayesian bounds to the problem of \nImangeNet classification (a deep neural network model). The authors provide \ninteresting empirical bounds for the risk of the ImageNet classifier. More specifically, \nthe authors introduce some clever choices for the prior distribution (on the \nhypothesis space) that allow one to incoperate a compression scheme and obtain \na (non-vacuous) bound for the predictor. \nOverall, This is an original work with clear presentation.\n\nMajor comments:\n1). In Theorem 2.1, why do you need \\lambda > 1 ?\nTo my knowledge, \\lambda only needs to be positive.\nWhy do you have to introduce the parameter \\alpha here? \nand consequently the additional log term?\n2) It is unclear for me, why are your bounds non-vacuous?\nProbably, a more clear explanation of Theorem 4.3 is to be required.\nAlso, some comparisions with the bounds in [Neyshabur et al 2018] and [Barlett et al 2017]\nwould make the paper more significant and interesting.\n\nMinor comments:\n1) in Theorem 2.1, after the formula (3), the \\Phi^{-1} should be  \\Phi^{-1}_{\\gamma}.\n2) in the sentence, page 4,: \"To strengthen a na√Øve Occam bound, we use the idea that that deep networks are insensitive to mild... \"   an extra \"that\" should be removed.\n3) in Section 5, the first paragraph, in sentence:  \"The lone exception is Dziugaite & Roy (2017), which succeeds by ....\"\nshould be \"The one exception....\"\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good paper provided the authors have answers to some technical questions.",
            "review": "This paper gives the first nonvacuous generalization bounds for\nmeaningful Imagenet models.  These bounds are given in terms of the\nbit length of compressions of learned models together with a method\nfor taking into account symmetries of the uncompressed parameters.\n\nThese bounds are nonvacuous only when the compressed models are small\n--- on the order of 500 Kilobytes.  State of the art compressed models\nof this size achieve Imagenet accuracies slightly better than Alexnet,\n16% error for top 5, and this paper reports a nonvacuous\ngeneralization guarantees of 89% error for top 5.  While there is\nstill a large gap between the actual generalization and the guarantee,\nthis would still be a significant accomplishment.\n\nI have one major concern.  The generalization bound involves adding an\nempirical loss and a regularization term computed from a KL\ndivergence.  I am convinced that the authors have correctly handles\nthe KL divergence term.  But the paper does not contain sufficient\ndetail to determine if the authors correctly handle the empirical loss\nterm.  It is NOT correct to use the training loss of the\n(deterministic) compressed model.  The generalization bound requires\nthat the training loss be measured under the parameter noise of the\nposterior distribution.  The paper needs to be clear that this has\nbeen done. The comments in Appendix B on noise robustness are\ndisturbing in this regard.\n\nIf the training loss  has been calculated correctly in the bound,\nthe results are significant.\n\nAssuming correctness, I would comment that the Catoni bound, while sqeaking\nout all available tightness, is very opaque.  I might be good to\nconsider the more transparent bounds, claimed to be essentially the\nsame, given in McAllester's tutorial.  If the more transparent bounds\nachieve equivalent numerical results, they would make the nature of\nthe bounds clearer.\n\nAnother comment involves a largely ignored detail in (Dzuigaite and\nRoy 17). Their bounds become vacuous if they center their Gaussian\nprior at zero.  Instead they center the prior on the initial value of\nthe parameters.  This yields a dramatic improvement in the bound.  In\nthe context of the present paper, this suggests a modification of the\nprior distribution on the compressed model.  We represent the model by\nfirst selecting the r code values.  I think a distribution could be\ndefined on the code book that would improve its log probability, but I\nwill ignore that.  Given the r code values we can define a\ndistribution over the possible compressed representations of a weight\nw_i in terms of a prior on w_i defined in terms of its initial value.\nThis gives a probability distribution over the compressed\nrepresentation.  Using log probability of the compressed\nrepresentation should then be a significant improvement on the first\nterm in (8).  This shift in the prior on compressed models has no\neffect on the second term of (8) so things should only get better.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}