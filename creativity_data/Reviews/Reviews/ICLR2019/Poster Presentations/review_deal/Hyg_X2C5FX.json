{
    "Decision": {
        "metareview": "The paper proposes an interesting framework for visualizing and understanding GANs, that will be of clear help for understanding existing models and might provide insights for developing new ones. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Intersting framework for the analysis of GANs"
    },
    "Reviews": [
        {
            "title": "New methods for interpreting GANs, with nice practical contribution for improving GANs outputs.",
            "review": "The paper proposes a method for visualizing and understanding GANs representation. This seems an important topic as several such methods were performed for networks trained in supervised learning, which relate\nto the predicted outcome, but there is lack of methods for interpreting GANs which are learned in an unsupervised manner and it is generally unclear what is the representation learned by GANs. \nThe method is finding correlations between the appearance of objects and the activation of units in each layer of the learned network. \nIn addition, the paper presents a 'causal' measure, where a causal effect of a unit is measured by removing and adding this unit from/to the network and computing the average effect on object appearance.\nThe authors demonstrate how the methods are applied by improving the appearance of images, by modifying units which were detected as important for specific objects. \nThe authors also provide an interactive interface where users can manually examine and modify their trained GANs in order to add/remove objects and to remove artifacts. \n\nThe method proposed by the authors seem to be appropriate for convolutional neural networks, where 'units' in each layer may correspond to objects and can be searched for in particular locations of image. \nIt is not clear to me if and how one can apply the author's methods to other architecture, and to other application domains (besides images), or whether the method is limited to vision applications. \nThe authors do not explain specifically how do they choose the 'units' for which they seek interpretation when reporting their results. It is written that each layer is divided into two sets: \nu  and u-bar, where we seek interpretation of u. But how large does u tend to be? how would one choose it? is it one filter out of all filters in a certain layer? when optimizing for sets of units together\n(using the alpha probabilities and the optimization in eq. 6) what is d? is it performed for all units in a single layer? more details would be useful here. \n\nThe paper is overall clearly written, with lots of visual examples demonstrating the methods presented in it. \nThe paper presents a new methodological idea, which allows for nice practical contribution. There is no theoretical contribution or any deep analysis. \nThere is no reference in the paper to the supp. info. figures and therefore it is not clear if and how the supp. info. adds valuable information to the reader. \nThe authors use scores like SWD and FIT for performance, but give no explanations for what do these scores measure. \n\n\nMinor: \n\nAbstract: immprovements -> improvements \n\nPage 6, middle: 'train on four LSUN' -> 'trained on four LSUN'\n\nPage 7, bottom: Fig. 14a and 14b should be Fig. 8a and 8b\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting idea to visualize and explain the representation of GANs and to provide a new potential way to further improve the quality of the generated images by GANs",
            "review": "## Summary\nThis work proposes a novel analytic framework exploited on a semantic segmentation model to visualize GANs at unit (feature map) level. The authors show that some GAN representations can be interpreted, correlate with the parsing result from the semantic segmentation model but as variables that have a causal effect on the synthesis of semantic objects in the output. This framework could allow to detect and remove the artifacts to improve the quality of the generated images.\n\nThe paper is well-written and organized. The dissection and intervention for finding relationships between representation units and objects are simple, straightforward and meaningful. The visualizations are convincing and insightful. I recommend to accept the paper.\n\n## Detail comments\nAbout diagnosing and improving GANs, please give more details of the human annotation for the artifacts.\n\nI think there is a typo in the first and second paragraphs in section 4.3, Figure 14 -> Figure 8.  \n\nThe whole framework is based on a semantic segmentation model. The model is highly possibly imperfect and could have very different performances on different objects. Have you ever considerate to handle these imperfect models?\n\nIs there a way to apply the framework to the training process of GANs?\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper reveals the essence of GAN through experiments.",
            "review": "This paper provides a visualization framework to understand the generative neural network in GAN models. To achieve this, they first find a group of interpretable units and then quantify the causal effect of interpretable units. Finally, the contextual relationship between these units and their surrounding is examined by inserting the discovered object concepts into new images. Extensive experiments are presented and a video is provided.\n\nOverall, I think this paper is very valuable and well-written. The experiments clearly show the questions proposed in the introduction are answered. Two concerns are as follows.\n\nCons:\n1) The visualization seems to be very heuristic. What I want to know is the theoretical interpretation of the visualization. For example, the Class Activation Maps (CAM) can be directly calculated by the output values of softmax function. How about the visual class for the generative neural networks?\n2) I am also very curious, how is the rate of finding the correct sets of units for a particular visual class?\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}