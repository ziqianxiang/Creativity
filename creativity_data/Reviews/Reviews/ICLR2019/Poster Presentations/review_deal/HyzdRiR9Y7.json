{
    "Decision": {
        "metareview": "This paper presents Universal Transformers that generalizes Transformers with recurrent connections. The goal of Universal Transformers is to combine the strength of feed-forward convolutional architectures (parallelizability and global receptive fields) with the strength of recurrent neural networks (sequential inductive bias). In addition, the paper investigates a dynamic halting scheme (by adapting Adaptive Computation Time (ACT) of Graves 2016) to allow each individual subsequence to stop recurrent computation dynamically.\n\nPros: \nThe paper presents a new generalized architecture that brings a reasonable novelty over the previous Transformers when combined with the dynamic halting scheme. Empirical results are reasonably comprehensive and the codebase is publicly available.\n\nCons:\nUnlike RNNs, the network recurs T times over the entire sequence of length M, thus it is not a literal combination of Transformers with RNNs, but only inspired by RNNs. Thus the proposed architecture does not precisely replicate the sequential inductive bias of RNNs. Furthermore, depending on how one views it, the network architecture is not entirely novel in that it is reminiscent of the previous memory network extensions with multi-hop reasoning (--- a point raised by R1 and R2). While several datasets are covered in the empirical study, the selected datasets may be biased toward simpler/easier tasks (--- R1). \n\nVerdict:\nWhile key ideas might not be entirely novel (R1/R2), the novelty comes from the fact that these ideas have not been combined and experimented in this exact form of Universal Transformers (with optional dynamic halting/ACT), and that the empirical results are reasonably broad and strong, while not entirely impressive (R1). Sufficient novelty and substance overall, and no issues that are dealbreakers. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Universal Transformers (with optional dynamic halting/ACT)"
    },
    "Reviews": [
        {
            "title": "Recursively applying multihead self-attention block in Transformer, small change leads to effective improvements on multiple tasks.",
            "review": "This paper extends Transformer by recursively applying a multi-head self-attention block, rather than stack multiple blocks in the vanilla Transformer. An extra transition function is applied between the recursive blocks. This combines the idea from RNN and attention-based models. But the RNN structure here is not applied to the input sequence, but to the sequence of blocks inside the Transformer encoder/decoder. In addition, it also uses a dynamic adaptive computation time (ACT) halting mechanism on each position, as suggested by the previous ACT paper. In fact, it can be seen as a memory network with a dynamic number of hops at the symbol level. \n\nThe paper is well-written and easy to follow. The experimental results demonstrate that the proposed model can achieve state-of-the-art prediction quality in several algorithmic and NLP tasks.\n\nPros\n1. The proposed UT is compatible with both algorithmic and NLP tasks by combining the Transformer with weight sharing of recurrence and dynamic halting. In contrast, previous algorithmic and NLP takes can only be solved by more specific neural architectures (e.g., NTM for algorithmic tasks and the Transformer for NLP tasks).\n2. The empirical results verify the effectiveness of the UT on several benchmarks. \n3. The careful experimental analyses not only show the insight of dynamic halting in QA task but demonstrate the ACT is very useful for algorithmic tasks. \n4. The publicly-released codes could make great contributions to the NLP community. \n\nCons\n1. It proposes an incremental change to the original Transformer by introducing recursive connection between multihead self-attention blocks with ACT. The idea behind UT is similar to memory networks and multi-hop reasoning. \n2. The recursive structure is not applied to the input sequence, so UT does not have the advantage of RNN/LSTM on capturing sequential information and high-order features. \n3. Although evaluated on multiple datasets and tasks, they only cover simple QA task and EN-DE translation task. Comparing to other papers applying modifications to Transformer, it is better to include at least one heavy task on large/challenging dataset/task.  \n4. On machine translation task, why does the model without dynamic halting achieve the SOTA performance? This is in contrast to the claim of the advantage of using dynamic halting.\n5. The ablation studies focus only on the dynamic halting, but what if weight sharing is removed from the UT? ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Solid and empirically promising model which merges Transformer and recurrent models but without strong intuitive or theoretical support to back up its claims.",
            "review": "This paper describes a transformer with recurrent structure to take advantage of self-attention mechanism. The number of recurrences can be dynamically determined through ACT-like halting depending on the difficulty of the input. A series of experiments on language modeling tasks have been demonstrated to show promising performances.\n\nThe overall concerns about this paper is that while the performances are quite promising, the theoretical claims and comparisons in the discussion section are of question. The authors attempt to provide connections to other networks (i.e., Natural GPU, RNN) since UT is an amalgamation of both transformers and RNN, they sound a little “hand-wavy” (i.e., comments about UT effectively interpolating between the feed-forward, fixed-depth Transformer and a gated recurrent architecture). In short, while empirically completely acceptable, intuitively or theoretically it is hard to grasp why UT is superior other than the dynamic/sharing layers across t (not time). I believe that improving this aspect could make this paper even better. Based on the comments below and the responses with the authors, I am willing to improve my score.\n\nPros:\n1.\tThe best of both worlds from parallelizable transformer and recurrent structure for repeated self-attention mechanism. Essentially, the “depth” of the transformer can vary if we “unroll” the recurrent stacks.\n\n2.\tExtensive experiments showing the performance of UT.\n\n3.\tAnalysis of the effect of the recurrent aspect of UT and how it can vary depending on the task difficulty.\n\nComments/cons:\n1.\tI am having trouble understanding the “universal” aspect of the transformer. Is this because the variability of the depth of UT (since “given sufficient memory” was mentioned)? If so, such characteristic of “computational universality” does not seem much unique to UT compared to infinite memory for a transformer or a simple RNN across stack (i.e., input is the while sequence and recurrent step is through the stack analogous to UT stack). Please comment on this.\n\n2.\tIt is nice to see many experiments, but without preexisting knowledge about the datasets and their tasks, I can only make relative judgements based on the provided comparisons against other methods. It would be nice to see slightly more detailed descriptions of each task (particularly LAMBADA LM), not necessarily in the main paper (due to space) but in the appendix if possible for improved self-containedness. \n\n3.\tIn the discussion, the crucial difference between UT and RNN is that RNN is stated to be that RNN cannot access memory in the recurrent steps while UT can. This seems to be the case for not just UT but any Transformer-type model by construction.\n\n4.\tThe authors stated that the “recurrent step” for RNN is through time (as the authors stated) while the “recurrent step” in UT is not through time. While this claim is completely correct itself, the RNN’s inability to access memory in its “recurrent steps” was compared with how UT could still access memory throughout its “recurrent steps”. In this sense, we may argue that the UT cannot access memory across its own t (stacking across t). I am not sure if it is fair to make such implications by putting both “recurrent steps” to be of same nature and pointing out one’s weakness. Perhaps the authors could comment on this.\n\nMinor:\n1.\tTable 2.: Best Stack-RNN for 1 attractor is the highest but not bold-faced.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Good paper, contribution moderate, experiments promising",
            "review": "My summary: A new model, the UT, is based on the Transformer model, with added recurrence and dynamic halting of the recurrence. The UT should unite the computational universality properties of Neural Turing Machines and Neural GPU with good performance on disparate language and algorithmic tasks.\n\n(I have read your author feedback and have modified my rating according to my understanding.)\n\nReview:\nThe paper is well written and proofread, concrete and clear. The model is quite clearly explained, especially with the additional space of the supplementary material, appendices A and B (note fig 4 is less good quality than fig 2 for some reason) -- I’m fine with the use of the Supp Mat for this purpose.\n \nThe experiments have been conducted well, and demonstrate a wide range of tasks, which seems to suggest that the UT has pretty general purpose. The range of algorithmic tasks is limited, e.g. compared to the NTM paper.\nI miss any experimental details at all on training.\nI miss a comparison to Neural GPU and Stack RNN in 3.1, 3.2.\n\nI miss a proof that the UT is computationally equivalent to a Turing machine. It does not have externally addressable, shared memory like a tape, and I’m not sure how to transpose read/write heads either.\n\nThe argument that the UT offers a good balance between inductive bias and expressivity is weak, though it may be the best one can hope for of a statistical model in a way. I note that in 3.1, the Transformer overfits, while it seems to underfit in 3.3 (lower LM and RC accuracy, higher LM perplexity), while the UT fare well, which suggests that the UT hits the balance better than the Transformer, at least.\n\nFrom the point of view of network structure, it seems natural to lift further constraints on the model: \nwhy should width of intermediate layers be exactly equal to sequence length?\nwhy should all hidden state vectors be size $d$, the size of the embeddings chosen at the first layer, which might be chosen out of purely practical reasons like the availability of pre-trained word embeddings?\n\nWhat is the contribution of this work? It starts from the Transformer, the ACT idea for dynamic halting in recurrent nets, the need for models fit for algorithmic tasks. \nThe UT’s building blocks are near-identical to the Transformers (and the paper is upfront and does a good job of explaining these similarities, fortunately)\n- cf eq1-5: residuals, multi-headed self attention, and layer norm around all this. \n- shared weights among all such units\n- encoder-decoder architecture\n- autoregressive decoder with teacher forcing\n- decoder units like the encoder’s but with extra layer of attention to final output of encoder\n- coordinate embeddings\nThe authors may correct me, but I believe that the UT with FC layers is exactly identical to the Transformer described in Vaswani 2017 for T=6. \nSo this paper introduces the idea of varying T, interprets it as a form of recurrence, and adds dynamic halting with ACT to that. Interestingly, the recurrence is not over sequence positions here.\nThis contribution is not major, on the other hand the experimental validation suggests the model is promising.\n\nTypos and writing suggestions\nabove eq 8: masked such that -> masked so that\neq 8: dimensions of O and H^T are incompatible: d*V, m*d; to evacuate the notation issue for transposition, cf footnote 1, here and elsewhere, you could use either ${^t A}$ or $A^\\top$ or $A^\\intercal$. You could also write $t=T$ instead of just $T$.\nsec3.3 line -1: designed such that -> designed so that\nTowards the beginning of the paper, it may be useful to stabilise terminology for $t$: depth (as opposed to width for $m$), time steps, recurrence dimension, revisions, refinements\n\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}