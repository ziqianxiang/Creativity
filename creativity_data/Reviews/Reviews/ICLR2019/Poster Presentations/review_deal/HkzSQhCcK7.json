{
    "Decision": {
        "metareview": "The paper presents a generative model of sequences based on the VAE framework, where the generative model is given by CNN with causal and dilated connections. \n\nNovelty of the method is limited; it mainly consists of bringing together the idea of causal and dilated convolutions and the VAE framework. However, knowing how well this performs is valuable the community.\n\nThe proposed method appears to have significant benefits, as shown in experiments. The result on MNIST is, however, so strong that it seems incorrect; more digging into this result, or sourcecode, would have been better.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Meta-Review"
    },
    "Reviews": [
        {
            "title": "Ok paper with a reasonable -- though somewhat obvious -- approach to generative modeling of sequence data",
            "review": "This paper presents a generative sequence model based on the dilated CNN\npopularized in models such as WaveNet. Inference is done via a hierarchical\nvariational approach based on the Variational Autoencoder (VAE). While VAE\napproach has previously been applied to sequence modeling (I believe the\nearliest being the VRNN of Chung et al (2015)), the innovation where is the\nintegration of a causal, dilated CNN in place of the more typical recurrent\nneural network. \n\nThe potential advantages of the use of the CNN in place of\nRNN is (1) faster training (through exploitation of parallel computing across\ntime-steps), and (2) potentially (arguably) better model performance. This\nsecond point is argued from the empirical results shown in the\nliterature. The disadvantage of the CNN approach presented here is that\nthese models still need to generate one sample at a time and since they are\ntypically much deeper than the RNNs, sample generation can be quite a bit\nslower.\n\nNovelty / Impact: This paper takes an existing model architecture (the\ncausal, dilated CNN) and applies it in the context of a variational\napproach to sequence modeling. It's not clear to me that there are any\nsignificant challenges that the authors overcame in reaching the proposed\nmethod. That said, it certainly useful for the community to know how the\nmodel performs.\n\nWriting: Overall the writing is fairly good though I felt that the model\ndescription could be made more clear by some streamlining -- with a single\npass through the generative model, inference model and learning. \n\nExperiments: The experiments demonstrate some evidence of the superiority\nof this model structure over existing causal, RNN-based models. One point\nthat can be drawn from the results is that a dense architecture that uses multiple levels of the\nlatent variable hierarchy directly to compute the data likelihood is\nquite effective. This observation doesn't really bear on the central message\nof the paper regarding the use of causal, dilated CNNs. \n\nThe evidence lower-bound of the STCN-dense model on MNIST is so good (low)\nthat it is rather suspicious. There are many ways to get a deceptively good\nresult in this task, and I wonder if all due care what taken. In\nparticular, was the binarization of the MNIST training samples fixed in\nadvance (as is standard) or were they re-binarized throughout training? \n\nDetailed comments:\n- The authors state \"In contrast to related architectures (e.g. (Gulrajani et\nal, 2016; Sonderby et al. 2016)), the latent variables at the upper layers\ncapture information at long-range time scales\" I believe that this is\nincorrect in that the model proposed in at least Gulrajani et al also \n\n- It also seems that there is an error in Figure 1 (left). I don't think\nthere should be an arrow between z^{2}_{t,q} and z^{1}_{t,p}. The presence\nof this link implies that the prior at time t would depend -- through\nhigher layers -- on the observation at t. This would no longer be a prior\nat that point. By extension you would also have a chain of dependencies\nfrom future observations to past observations. It seems like this issue is\nisolated to this figure as the equations and the model descriptions are\nconsistent with an interpretation of the model without this arrow (and\nincluding an arrow between z^{2}_{t,p} and z^{1}_{t,p}.\n\n- The term \"kla\" appears in table 1, but it seems that it is otherwise not\ndefined. I think this is the same term and meaning that appears in Goyal et\nal. (2017), but it should obviously be defined here.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting new architecture, but some clarity issues",
            "review": "This paper introduces a new stochastic neural network architecture for sequence modeling. The model as depicted in figure 2 has a ladder-like sequence of deterministic convolutions bottom-up and stochastic Gaussian units top-down.\n\nI'm afraid I have a handful of questions about aspects of the architecture that I found confusing. I have a difficult time relating my understanding of the architecture described in figure 2 with the architecture shown in figure 1 and the description of the wavenet building blocks. My understanding of wavenet matches what is shown in the left of figure 1: the convolution layers d_t^l depend on the convolutional layers lower-down in the model, thus with each unit d^l having dependence which reaches further and further back in time as l increases. I don't understand how to reconcile this with the computation graph in figure 2, which proposes a model which is Markov! In figure 2, each d_{t-1}^l depends only on on the other d_{t-1} units and the value of x_{t-1}, which then (in the left diagram of figure 2) generate the following x_t, via the z_t^l. Where did the dilated convolutions go…? I thought at first this was just a simplification for the figure, but then in equation (4), there is d_t^l = Conv^{(l)}(d_t^{l-1}). Shouldn't this also depend on d_{t-1}^{l-1}…? or, where does the temporal information otherwise enter at all? The only indication I could find is in equation (13), which has a hidden unit defined as d_t^1 = Conv^{(1)}(x_{1:t}).\n\nAdding to my confusion, perhaps, is the way that the \"inference network\" and \"prior\" are described as separate models, but sharing parameters. It seems that, aside from the initial timesteps, there doesn't need to be any particular prior or inference network at all: there is simply a transition model from x_{t-1} to x_{t}, which would correspond to the Markov operator shown in the left and middle sections of figure 2. Why would you ever need the right third of figure 2? This is a model that estimates z_t given x_t. But, aside from at time 0, we already have a value x_{t-1}, and a model which we can use to estimate z_t  given x_{t-1}…!\n\nWhat are the top-to-bottom functions f^{(l)} and f^{(o)}? Are these MLPs?\n\nI also was confused in the experiments by the >= and <= on the reported numbers. For example, in table 2, the text describes the values displayed as log-likelihoods, in which case the ELBO represents a lower bound. However, in that case, why is the bolded value the *lowest* log-likelihood? That would be the worst model, not the best — does table 2 actually show negative log-likelihoods, then? In which case, though, the numbers from the ELBO should be upper bounds, and the >= should be <=. Looking at figure 4, it seems like visually the STCN and VRNN have very good reconstructions, but the STCN-dense has visual artifacts; this would correspond with the numbers in table 2 being log-likelihoods (not negative), in which case I am confused only by the choice of which model to bold.\n\n\n\nUPDATE:\n\nThanks for the clarifications and edits. FWIW I still find the depiction of the architecture in Figure 2 to be incredibly misleading, as well as the decision to omit dependencies from the distributions p and q at the top of page 5, as well as the use in table 3 of \"ELBO\" to refer to a *negative* log likelihood.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Clearly written, but lacking comparisons",
            "review": "The focus on novelty (mentioned in both the abstract, and conclusion as a direct claim) in the presentation hurts the paper overall. Without stronger comparison to other closely related work, and lack of citation to several closely related models, the claim of novelty isn't defined well enough to be useful. Describing what parts of this model are novel compared to e.g. Stochastic WaveNet or the conditional dilated convolutional decoder of \"Improved VAE for Text ...\" (linked below, among many others) would help strengthen the novelty claim, if the claim of novelty is needed or useful at all. Stochastic WaveNet in particular seems very closely related to this work, as does PixelVAE. In addition, use of autoregressive models conditioned on (non-variational, in some sense) latents have been shown in both VQ-VAE and ADA among others, so a discussion would help clarify the novelty claim.\n\nEmpirical results are strong, though (related to the novelty issue) there should be greater comparison both quantitatively and qualitatively to further work. In particular, many of the papers linked below show better empirical results on the same datasets. Though the results are not always directly comparable, a discussion of *why* would be useful - similar to how Z-forcing was included.\n\nIn the qualitative analysis, it would be good to see a more zoomed out view of the text (as in VRNN), since one of the implicit claims of the improvement from dense STCN is improved global coherence by direct connection to the \"global latents\". As it stands now the text samples are a bit too local to really tell. In addition, the VRNN samples look quite a bit different than what the authors present in their work - what implementation was used for the VRNN samples (they don't appear to be clips from the original paper)? \n\nOn the MNIST setting, there are many missing numbers in the table from related references (some included below), and the >= 60.25 number seems so surprising as to be (possibly) incorrect - more in-depth analysis of this particular result is needed. Overall the MNIST result needs more description and relation to other work, for both sequential and non-sequential models.\n\nThe writing is well-done overall, and the presented method and diagrams are clear. My primary concern is in relation to related work, clarification of the novelty claim, and more comparison to existing methods in the results tables. \n\nVariational Bi-LSTM https://arxiv.org/abs/1711.05717\n\nStochastic WaveNet https://arxiv.org/abs/1806.06116\n\nPixelVAE https://arxiv.org/abs/1611.05013\n\nFiltering Variational Objectives https://github.com/tensorflow/models/tree/master/research/fivo\n\nImproved Variational Autoencoders for Text Modeling using Dilated Convolutions https://arxiv.org/abs/1702.08139\n\nTemporal Sigmoid Belief Networks for Sequential Modeling http://papers.nips.cc/paper/5655-deep-temporal-sigmoid-belief-networks-for-sequence-modeling\n\nNeural Discrete Representation Learning (VQ-VAE) https://arxiv.org/abs/1711.00937\n\nThe challenge of realistic music generation: modelling raw audio at scale (ADA) https://arxiv.org/abs/1806.10474\n\nLearning hierarchical features from Generative Models https://arxiv.org/abs/1702.08396\n\nAvoiding Latent Variable Collapse with Generative Skip Models https://arxiv.org/abs/1807.04863\n\nEDIT: Updated score after second revisions and author responses",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}