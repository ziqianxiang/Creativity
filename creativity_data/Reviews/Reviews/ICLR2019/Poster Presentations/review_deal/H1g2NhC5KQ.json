{
    "Decision": {
        "metareview": "The paper shows how techniques introduced in the context of unsupervised machine translation can be used to build a style transfer methods.\n\nPros:\n\n-  The approach is simple and questions assumptions made by previous style transfer methods (specifically, they show that we do not need to specifically enforce disentanglement).  \n\n-  The evaluation is thorough and shows benefits of the proposed method\n\n-  Multi-attribute style transfer is introduced and benchmarks are created\n\n-  Given the success of unsupervised NMT, it makes a lot of sense to see if it can be applied to the style transfer problem\n\nCons:\n\n- Technical novelty is limited \n\n- Some findings may be somewhat trivial (e.g., we already know that offline classifiers are stronger than the adversarials, e.g., see Elazar and Goldberg, EMNLP 2018).\n\n\n\n\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "A simple and effective approach to style transfer based on recent developments in unsupervised NMT"
    },
    "Reviews": [
        {
            "title": "This paper presents a model for text rewriting for multiple attributes. ",
            "review": "This paper presents a model for text rewriting for multiple attributes, for example gender and sentiment, or age and sentiment. The contributions and strengths of the paper are as follows. \n\n* Problem Definition\nAn important contribution is the new problem definition of multiple attributes for style transfer. While previous research has looked at single attributes for rewriting, \"sentiment\" for example, one could imagine controlling more than one attribute at a time. \n\n* Dataset Augmentation\nTo do the multiple attribute style transfer, they needed a dataset with multiple attributes. They augmented the Yelp review dataset from previous related paper to add gender and restaurant category. They also worked with microblog dataset labeled with gender, age group, and annoyed/relaxed. In addition to these attributes, they modified to dataset to include longer reviews and allow a larger vocabulary size. In all, this fuller dataset is more realistic than the previously release dataset.\n\n* Model\nThe model is basically a denoising autoencoder, a well-known, relatively simple model. However, instead of using an adversarial loss term as done in previous style transfer research, they use a back-translation term in the loss. A justification for this modeling choice is explained in detail, arguing that disentanglement (which is a target of adversarial loss) does not really happen and is not really needed. The results show that the new loss term results in improvements.\n\n* Human Evaluation\nIn addition to automatic evaluation for fluency (perplexity), content preservation (BLEU score), and attribute control (classification), they ask humans to judge the output for the three criteria. This seems standard for this type of task, but it is still a good contribution.\n\nOverall, this paper presents a simple approach to multi-attribute text rewriting. The positive contributions include a new task definition of controlling multiple attributes, an augmented dataset that is more appropriate for the new task, and a simple but effective model which produces improved results.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good work but better presentation needed",
            "review": "This work proposes a new model that controls several factors of variation in textual data where the condition on disentanglement is replaced with a simpler mechanism based on back-translation. It allows control over multiple attributes, and a more fine-grained control on the trade-off between content preservation and change of style with a pooling operator in the latent space.\n\nOne of the major arguments is it is unnecessary to have attribute-disentangled latent representations in order to have good style-transferring rewriting. In Table 2, the authors showed that \"a classifier that is separately trained on the resulting encoder representations has an easy time recovering the sentiment\" when the discriminator during training has been fooled. Is there any difference between the two discriminators/classifiers? If the post-fit classifier on top of the encoder representation can easily predict the correct sentiment, there should be enough signal from the discriminator to adapt the encoder in order to learn a more disentangled representation. On the other hand, this does not answer the question if a \"true\" disentangled representation would give better performance. The inferior performance from the adversarially learned models could be because of the \"entangled\" representations.\n\nAs the author pointed out, the technical contributions are the pooling operator and the support for multiple attributes since the loss function is the same as that in (Lample et. al 2018). These deserve more elaborated explanation and quantitative comparisons. After all, the title of this work is \"multiple-attribute text rewriting\". For example, the performance comparison between the proposed how averaged attribute  embeddings and simple concatenation, and the effect of the introduced trade-off using temporal max-pooling.\n\nHow important is the denoising autoencoder loss in the loss function (1)? From the training details in the supplementary material, it seems like the autoencoder loss is used as \"initialization\" to some degree. As pointed out by the authors, the main task is to get fluent, attribute-targeted, and content-preserving rewriting. As long as the \"back-translation\" gives expected result, it seems not necessary to have \"meaningful\" or hard \"content-preserving\" latent representations when the generator is powerful enough.\n\nI think the last and most critical question is what the expected style-transferred rewriting look like. What level or kind of \"content-preserving\" do we look for? In Table 4, it shows that the BLEU between the input and the referenced human rewriting is only 30.6 which suggest many contents have been modified besides the positive/negative attribute. This can also be seen from the transferred examples. In Table 8, one of the Male example: \"good food. my wife and i always enjoy coming here for dinner. i recommend india garden.\" and the Female transferred rewriting goes as \"good food. my husband and i always stop by here for lunch. i recommend the veggie burrito\". It's understandable that men and women prefer different types of food even though it is imagination without providing context. But the transfer from \"dinner\" to \"lunch\" is kind of questionable. Is it necessary to change the content which is irrelevant to the attributes?\n\n\nOther issues:\n- Towards the end of Section 3, it says that \"without back-propagating through the back-translation generation process\". Can you elaborate on this and the reason behind this choice?\n- What does it mean by \"unknown words\" in \"... with 60k BPE codes, eliminating the presence of unknown words\" from Section 4?\n- There is no comparison with (Zhang et. al. 2018), which is the \"most relevant work\".\n- In Table 4, what is the difference among the three \"Ours\" model?\n- In Table 4, the perplexity of \"Input Copy\" is very high compared with generated sentences.\n- In Table 7, what does the \"attention\" refer to?\n- In the supplementary material, there are lambda_BT and lambda_AE. But there is only one lambda in the loss function (1).\n- Please unify the citation style.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Impressive experiments, but hard to determine how much is methodologically new here",
            "review": "The paper proposes \"style transfer\" approaches for text rewriting that allow for controllable attributes. For example, given one piece of text (and the conditional attributes associated with the user who generated it, such as their age and gender), these attributes can be changed so as to generate equivalent text in a different style.\n\nThis is an interesting application, and somewhat different from \"style transfer\" approaches that I've seen elsewhere. That being said I'm not particularly expert in the use of such techniques for text data.\n\nThe architectural details provided in the paper are quite thin. Other than the starting point, which as I understand adapts machine translation techniques based on denoising autoencoders, the modifications used to apply the technique to the specific datasets used here were hard to follow: basically just a few sentences described at a high level. Maybe to somebody more familiar with these techniques will understand these modifications fully, but to me it was hard to follow whether something methodologically significant had been added to the model, or whether the technique was just a few straightforward modifications to an existing method to adapt it to the task. I'll defer to others for comments on this aspect.\n\nOther than that the example results shown are quite compelling (both qualitatively and quantitatively), and the experiments are fairly detailed.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}