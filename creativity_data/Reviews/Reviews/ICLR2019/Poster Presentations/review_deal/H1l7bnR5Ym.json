{
    "Decision": {
        "metareview": "The paper proposes a new method that builds on the Bayesian modelling framework for GANs and is supported by a theoretical analysis and an empirical evaluation that shows very promising results. All reviewers agree, that the method is interesting and the results are convincing, but that the model does not really fit in the standard Bayesian setting due to a data dependency of the priors. I would therefore encourage the authors to reflect this by adapting the title and making the differences more clear in the camera ready version.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Interesting new model with good performance "
    },
    "Reviews": [
        {
            "title": "experimental work now conclusive",
            "review": "PRIOR COMMENT:   This paper should be rejected based on the experimental work.\nExperiments need to be reported for larger datasets.  Note the MGAN\npaper reports results on STL-10 and ImageNet as well.\n\nNOTE:  this was addressed by the 27/11 revision, which included good\n   results for these other data sets, thus I now withdraw the comment\n\nNote, your results on CIFAR-10 are quite different to those in the\nMGAN paper.  Your inceptions scores are worse and FIDs are better!!  I\nexpect you have different configurations to their paper, but it would\nbe good for this to be explained.  NOTE:   explained in response!\n\nNOTE:  this was addressed by the 27/11 revision\n\nI thought the related work section was fabulous, and as an extension\nto BGAN, the paper is a very nice idea.  So I benefited a lot from reading\nthe paper.\n\nI have some comments on Bayesian treatment.  In Bayesian theory, the\ntrue distribution $p_{data}$ cannot appear in any evaluated formulas,\nas you have it there in Eqn (1) which is subsequently used in your\nlikelihood Eqn (2).  Likelihoods are models and cannot involve \"truth\".\n\nLemma 1:  Very nice observation!!  I was trying to work that out,\nonce I got to Eqn (3), and you thought of it. \n\nAlso, you do need to explain 3.2 better.  The BGAN paper, actually, is\na bit confusing from a strict Bayesian perspective, though for\ndifferent reasons.  The problem you are looking at is not a\ntime-series problem, so it is a bit confusing to be defining it as\nsuch.  You talk about an iterative Bayesian model with priors and\nlikelihoods.  Well, maybe that can be *defined* as a probabilistic\nmodel, but it is not in any sense a Bayesian model for the estimation\nof $p_{model}$.\n\nNOTE:  anonreviewer2 expands more on this\n\nWhat you do with Equation (3) is define a distribution on\n$q_g(\\theta_g)$ and $q_d(\\theta_d)$ (which, confusingly, involves the\n\"true\" data distribution ... impossible for a Bayesian formulation).\nYou are doing a natural extension of the BGAN papers formulation in\ntheir Eqs (1) and (2).  This, as is alluded to in Lemma 1.  Your\nformulation is in terms of two conditional distributions, so\nconditions should be given that their is an underlying joint\ndistribution that agrees with these.  Lemma 1 gives a negative result.\nYou have defined it as a time series problem, and apparantly one wants\nthis to converge, as in Gibbs sampling style.  Like BGAN, you have\njust arbitrarily defined a \"likelihood\".\n\nTo me, this isn't a Bayesian model of the unsupervised learning task,\nits a probabilistic style optimisation for it, in the sense that you are defining a probability\ndistribution (over $q_g(\\theta_g)$ and $q_d(\\theta_d)$) and sampling\nfrom it, but its not really a \"likelihood\" in the formal sense.  A\nlikelhood defines how data is generated.  Your \"likelihood\" is over\nmodel parameters, and you seem to have ignored the data likelihood,\nwhich you define in sct 3.1 as $p_{model}()$.\n\nAnyway, I'm happy to go with this sort of formulation, but I think you\nneed to call it what it is, and it is not Bayesian in the standard sense.  The theoretical\ntreatment needs a lot of cleaning up.  What you have defined is a\nprobabilistic time-series on $q_g(\\theta_g)$ and $q_d(\\theta_d)$.\nFair enough, thats OK.  But you need to show that it actually works in\nthe estimation of $p_{model}$.  Because one never has $p_{data}$, all\nyour Theorem 1 does is show that asymptotically, your method works.\nUnfortunately, I can say the same for many crude algorithms, and most\nof the existing published work.  Thus, we're left with requiring a\nsubstantial empirical validation to demonstrate the method is useful.\n\nNow my apologies to you: I could make somewhat related statements\nabout the theory of the BGAN paper, and they got to publish theirs at\nICLR!  But they did do more experimentation.\n\nOh, and some smaller but noticable grammar/word usage issues.\n\nNOTE:  thanks for your good explanation of the Bayesian aspects of the model ...\nyes I agree, you have a good Bayesian model of the GAN computation , but it\nis still not a Bayesian model of the unsupervised inference task.  This is a somewhat\nminor point, and should not in anyway influence worth of the paper ... but clarification\nin paper would be nice.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Stripping the priors from Bayesian GANs",
            "review": "Summary\n=========\nThe paper extends Bayesian GANs by altering the generator and discriminator parameter likelihood distributions and their respective priors. \nThe authors further propose an SGHMC algorithm to collect samples of the resulting posterior distributions on each parameter set and evaluate their approach on both a synthetic and the CIFAR-10 data set. \nThey claim superiority of their method, reporting a higher distance to mode centers of generated data points and better generator space coverage for the synthetic data set and better inception scores for the real world data for their method.\n\nReview\n=========\nAs an overall comment, I found the language poor, at times misleading.\nThe authors should have their manuscript proof-read for grammar and vocabulary.\nExamples: \n- amazing superiority (page 1, 3rd paragraph)\n- Accutally... (page 1, end of 3rd paragraph)\n- the total mixture of generated data distribution (page 3, mid of 3.1)\n- Similarity we define (page 3, end of 3.1)\n- etc.\nOver the whole manuscript, determiners are missing.\n\nThe authors start out with a general introduction to GANs and Bayesian GANs in particular, \narguing that it is an open research question whether the generator converges to the true data generating distribution in Bayesian GANs.\nI do not agree here. The Bayesian GAN defines a posterior distribution for the generator that\nis proportional to the likelihood that the discriminator assigns to generated samples.\nThe better the generator, the higher the likelihood that the discriminator assign to these samples.\nIn the case of a perfect generator, here the discriminator is equally unable to distinguish real and generated samples and consequently degenerates to a constant function.\nUsing the same symmetry argument as the authors, one can show that this is the case for Bayesian GANs.\n\nWhile defining the likelihood functions, the iterator variable t is used without introduction.\n\nFurther, I a confused by their argument of incompatibility.\nFirst, they derive a Gibbs style update scheme based on single samples for generator and discriminator parameters using\nposteriors in which the noise has been explicitly marginalized out by utilizing a Monte Carlo estimate.\nSecond, the used posteriors are conditional distributions with non-identical conditioning sets.\nI doubt that the argument still holds under this setting.\n\nWith respect to the remaining difference between the proposed approach and Bayesian GAN,\nI'd like the authors elaborate where exactly the difference between expectation of objective value\nand objective value of expectation is.\nSince the original GAN objectives used for crafting the likelihoods are deterministic functions,\nrandomness is introduced by the distributions over the generator and discriminator parameters.\nI would have guessed that expectations propagate into the objective functions.\n\nIt is, however, interesting to analyze the proposed inference algorithm, especially the introduced posterior distributions.\nFor the discriminator, this correspond simply to the likelihood function.\nFor the generator, the likelihood is combined with some prior for which no closed form solution exists.\nIn fact, this prior changes between iterations of the inference algorithm.\nThe resulting gradient of the posterior decomposes into the gradient of the current objective and the sum over all previous gradients.\nWhile this is not a prior in the Bayesian sense (i.e. in the sense of an actual prior belief), it would be interesting to have a closer look at the effect this has on the sampling method.\nMy educated guess is, that this conceptually adds up to the momentum term in SGHMC and thus slows down the exploration of the parameter space and results in better coverage.\n\nThe experiments are inspired by the ones done in the original Bayesian GAN publication.\nI liked the developed method to measure coverage of the generator space although I find the\nterm of hit error misleading.\nGiven that the probabilistic methods all achieve a hit rate of 1, a lower hit error actually points to worse coverage.\nI was surprised to see that hit error and coverage are only not consistently negatively correlated.\nAdding statistics over several runs of the models (e.g. 10) would strengthen the claim of superior performance.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A Bayesian GAN with where data distribution is an equilibrium",
            "review": "Mode collapse in the context of GANs occurs when the generator only learns one of the \nmultiple modes of the target distribution. Mode collapsed can be tackled, for instance, using Wasserstein distance instead of Jensen-Shannon divergence. However, this sacrifices accuracy of the generated samples.\n\nThis paper is positioned in the context of Bayesian GANs (Saatsci & Wilson 2017) which, by placing a posterior distribution over the generative and discriminative parameters, can potentially learn all the modes. In particular, the paper proposes a Bayesian GAN that, unlike previous Bayesian GANs, has theoretical guarantees of convergence to the real distribution.\n\nThe authors put likelihoods over the generator and discriminator with logarithms proportional to the traditional GAN objective functions. Then they choose a prior in the generative parameters which is the output of the last iteration. The prior over the discriminative parameters is a uniform improper prior (constant from minus to plus infinity). Under this specifications, they demonstrate that the true data distribution is an equilibrium under this scheme. \n\nFor the inference, they adapt the Stochastic Gradient HMC used by Saatsci & Wilson. To approximate the gradient of the discriminator, they take samples of the generator parameters. To approximate the gradient of the generator they take samples of the discriminator parameters but they also need to compute a gradient of the previous generator distribution. However, because this generator distribution is not available in close form they propose two simple approximations.\n\nOverall, I enjoyed reading this paper. It is well written and easy to follow. The motivation is clear, and the contribution is significant. The experiments are convincing enough, comparing their method with Saatsci's Bayesian GAN and with the state of the art of GAN that deals with mode collapse. It seems an interesting improvement over the original Bayesian GAN with theoretical guarantees and an easy implementation.\n\nSome typos:\n\n- The authors argue that compare to point mass...\n+ The authors argue that, compared to point mass...\n\n- Theorem 1 states that any the ideal generator\n+ Theorem 1 states that any ideal generator\n\n- Assume the GAN objective and the discriminator space are symmetry\n+ Assume the GAN objective and the discriminator space have symmetry\n\n- Eqn. 8 will degenerated as a Gibbs sampling\n+ Eqn. 8 will degenerate as a Gibbs sampling",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}