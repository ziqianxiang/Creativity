{
    "Decision": {
        "metareview": "The paper develops an original extension/generalization of standard batchnorm (and group norm) by employing a mixture-of-experts to separate incoming data into several modes and separately normalizing each mode. The paper is well written and technically correct, and the method yields consistent accuracy improvements over basic batchnorm on standard image classification tasks and models.\nReviewers and AC noted the following potential weaknesses: a) while large on artificially mixed data, improvements are relatively small on single standard datasets (<1% on CIFAR10 and CIFAR100)  b) the paper could better motivate why multi-modality is important e.g. by showing histograms of node activations c) the important interplay between number of modes and batch size should be more thoroughly discussed\nd) the closely related approach of Kalayeh & Shah 2018 should be presented and contrasted with in more details in the paper. Also comparing to it in experiments would enrich the work.\n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "Original generalization of batchnorm that yields small accuracy improvement."
    },
    "Reviews": [
        {
            "title": "Normalization method that assumes multi-modal distributions",
            "review": "The authors proposed a normalization method that learns multi-modal distribution in the feature space. The number of modes $K$ is set as a hyper-parameter. Each sample $x_{n}$ is distributed (softly assigned) to modes by using a gating network. Each mode keeps its own running statistics. \n\n1) In section 3.2, it is mentioned that the MN didn't need and use any regularizer to encourage sparsity in the gating network. Is MN motivated to assign each sample to multiple modes evenly or to a distinct single mode? It would be better to provide how the gating network outputs sparse assignment along with the qualitative analysis.\n\n2) The footnote 3 showed that individual affine parameters doesn't improve the overall performance. How can this be interpreted? If the MN is assuming multi-modal distribution, it seems more reasonable to have individual affine parameters.\n\n3) The overall results show that increasing the number of modes $K$ doesn't help that much. The multi-task experiments used 4 different datasets to encourage diversity, but K=2 showed the best results. Did you try to use K=1 where the gating network has a sigmoid activation?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Solid paper proposing a generalisation of Batch Normalisation",
            "review": "The paper proposes a generalisation of Batch Normalisation (BN) under the assumption that the statistics of the unit activations over the batches and over the spatial dimensions (in case of convolutional networks) is not unimodal. The main idea is to represent the unit activation statistics as a mixture of modes and to re-parametrise by using mode specific means and variances. The \"posterior\" mixture weights for a specific unit are estimated by gating functions with additional affine parameters (followed by softmax). A second, similar variant applies to Group Normalisation, where the statistics is taken over channel groups and spatial dimensions (but not over batches). \n\nTo demonstrate the approach experimentally, the authors first consider an \"artificial\" task by joining data from MNIST, Fashion MNIST, CIFAR10 and SVHN and training a classifier (LeNet) for the resulting 40 classes. The achieved error rate improvement is 26.9% -> 23.1%, when comparing with standard BN. In a second experiment the authors apply their method to \"single\" classification tasks like CIFAR10, CIFAR100 and ILSVRC12 and use large networks as e.g. VGG13 and ResNet20. The achieved improvements when comparing with standard BN are one average 1% or smaller.\n\nThe paper is well written and technically correct.\n\nFurther comments and questions to the authors:\n\n- The relevance of the assumption and the resulting normalisation approach would need further justification. The proposed experiments seem to indicate that the node statistics in the single task case are \"less multi-modal\" as compared to the multi-task. Otherwise we would expect the comparable improvements by mode normalisation in both cases? On the other hand, it should be easy to verify the assumption of multi-modality experimentally, by collecting node statistics in the learned network (or at some specific epoch during learning ). It should be also possible to give some quantitative measure for it.\n\n- Please explain the parametrisation of the gating units more precisely (paragraph after formula (3)). Is the affine mapping X -> R^k a general one? Assuming that X has dimension CxHxW, this would require a considerable amount of additional parameters and  thus increase the VC dimension of the network (even if its primary architecture is not changed). Would this require more training data then? I miss a discussion of this aspect.\n\n- When comparing different numbers of modes (sec. 4.1, table 1), the size of the batch size was kept constant(?). The authors explain the reduction of effectiveness of higher mode numbers as a consequence of finite estimation (decreasing number of samples per mode). Would it not be reasonable to increase the batch size proportionally, such that the amount of samples per mode is kept constant?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Might have already been published and pushes BN towards small mini-batches",
            "review": "Summary:\nBatch Normalization (BN) suffers from 2 flaws: 1) It performs poorly when the batch size is small and 2) computing only one mean and one variance per feature might be a poor approximation for multi-modal features. To alleviate 2), this paper introduces Mode Normalization (MN) a new normalization technique based on BN. It uses a gating mechanism, similar to an attention mechanism, to project the examples in the mini-batch onto K different modes and then perform normalization on each of these modes.\n\nClarity:\nThe paper is clearly written, and the proposed normalization is well explained.\n\nNovelty: \nThe proposed normalization is somewhat novel. I also found a similar paper on arXiv (submitted for review to IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018): M. M. Kalayeh, M. Shah, Training Faster by Separating Modes of Variation in Batch-normalized Models, arXiv 2018. I didn’t took the time to read this paper in details, but the mixture normalization they propose seems quite close to MN. Could the authors comment on this?\n\nPros and Cons:\n+ Clearly written and motivated\n+ Try to address BN’s weakness, which is an important direction in deep learning\n- I found similar papier in the literature\n- The proposed method aims to make BN perform better, but pushes it toward small batch settings, which is where BN performs poorly.\n- Misses comparisons with other techniques (see detailed comments).\n\nDetailed Comments:\n1. Multi-modality:\nIt is not clear if the features are multimodal when performing classification tasks. Some histograms of a few features in the network would have help motivate the proposed normalization. However, it seems indeed to be an issue when training GANs: to make BN work when placed in the discriminator, the real and fake examples must be normalized separately, otherwise the network doesn't train properly. Moreover, when dealing with multimodal datasets (such as the one you created by aggregating different datasets), one can use the FiLM framework (V. Dumoulin et al., Feature-wise transformations, Distill 2018), and compute different means and variances for each datasets. How would the proposed method perform against such method?\n2. Larger scale:\nIt would be nice to see how MN performs on bigger networks (such as the ResNet50, or a DenseNet), and maybe a more interesting fully-connected benchmark, such as the deep autoencoder.\n3. Small batch regime:\nIt seems that the proposed method essentially pushes BN towards a regime of smaller mini-batch size, where it is known to performs poorly. For instance, the gain in performances on the ImageNet experiments drops quite a lot already, since the training is divided on several GPUs (and thus the effective mini-batch is already reduced quite a lot). This effect gets worse as the size of the network increases, since the effective mini-batch size gets smaller. This problem also appears when working on big segmentation tasks or videos: the mini-batch size is typically very small for those problems. So I fear that MN will scale poorly on bigger setups. I also think that this is the reason why you need to use extremely small K.\n4. Validation set:\nWhat validation sets are you using in your experiments? In section 4.1, the different dataset and their train / test splits are presented, but what about validation?\n\nConclusion:\nGiven the similarity with another paper already in the literature, I reject the paper. Also, it seems to me that the technique actually pushed BN towards a small batch regime, where it is known to perform poorly. Finally, it misses comparison with other techniques.\n\nRevision:\nAfter the rebuttal, I increased my rating to a 6. I feel this paper could still be improved by better motivating why multi-modality is important for single tasks (for example, by plotting histograms of activations from the network). I also think that the paper by Kalayeh & Shah should be presented in more details in the related work, and also be compared to in the experimental setup (for example on a small network), especially because the authors say they have experience with GMMs.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}