{
    "Decision": {
        "metareview": "This paper builds on the recent DCFNet (Decomposed Convolutional Filters) architecture to incorporate rotation equivariance while preserving stability. The core idea is to decompose the trainable filters into a steerable representation and learn over a subset of the coefficients of that representation. \nReviewers all agreed that this is a solid contribution that advances research into group equivariant CNNs, bringing efficiency gains and stability guarantees, albeit these appear to be incremental with respect to the techniques developed in the DCFNet work. In summary, the AC believes this to be a valuable contribution and therefore recommends acceptance. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Provably stable rotation equivariant networks"
    },
    "Reviews": [
        {
            "title": "Principled method with good results",
            "review": "Summary:\nThis paper combines the benefits of using joint steerable filters (using the SO(2) group) for designing rotation-equivariant CNNs with those of decomposing the filters (using Fourier-Bessel bases) for reducing the computational complexity. In addition, this leads to a compressed model and filter regularization. The authors give theoretical guarantees on the rotation equivariance and representation stability with respect to in and out of plane rotation. Empirical results show that the model attains better accuracy compared to CNNs and non-rotation-equivariant deep networks while using fewer parameters and also performs similarly to a rotation-equivariant model with much bigger capacity.\n\nPros:\n- Theoretical guarantees, elegant approach\n- Good empirical results compared to other models\n- Desirable properties: rotation-equivariance, lower computational complexity, fewer parameters, robustness and guaranteed stability to deformations\n\nCons:\n- Somewhat incremental technical novelty: combination of two previously published methods (Qiu et al. 2018 & Weiler et al. 2017)\n\nComments:\n1. I believe the related work section can be improved by explaining more clearly the connection between your work and the cited ones and emphasizing the advantages and limitations of RotDCF compared to other methods In particular, a reader should be able to precisely understand what is the novelty of this work is and what were the technical challenges in combining previously published ideas (such as DCF and SFCNN) \n2. How do you determine the truncation in practice? How robust is the method to this choice? What are the trade-offs between using a value that is too low or too high? It would be interesting to show how performance and complexity vary with this parameter\n3. It would also be helpful to have a discussion on choosing the parameters K_{alpha} and N_{theta} and how this affects the performance, computational complexity and number of parameters. This would provide more intuition on the limits of this method and the types of data it can be used for\n4. In section 2.3, it would be helpful to specify an estimated range for the parameter reduction from the non-bases rotation-equivariant CNN to RotDCF (similar to the ½ factor from RotDCF to regular CNN) \n5. Eq. (4) seems to be missing the definition of R_{m,q}\n6. The notation for the supplementary material was confusing at times. I would suggest using the more standard notation for the appendix which can also be a more specific reference (e.g. A.1, A.2, etc.)\n\n\n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Good paper: ",
            "review": "Group-equivariant deep networks are used as a solution for rotation-equivariance in CNNs. However, they are computationally expensive as the number of filters increases by a factor proportional to the number of groups. Inspired by ideas of filter decomposition used in CNN model compression, the authors of this work instead propose to use steerable filters across space and rotation, as basis filters for achieving rotation-equivariance, which leads to computational efficiency. \n\nThe authors show improved accuracy and model compression with their proposed approach versus regular CNNs for several different tasks (MNIST, CIFAR, autoencoders and face recognition) for rotated and upright images.\n\nFurthermore the authors theoretically prove and demonstrate empirically (via multiple experiments) the group equivariance property and the representational stability under input variations of their proposed architecture. \n\nThe work is novel and it solves an open research problem.\n\nHowever, the one major criticism of the work is that in the experimental section, especially for the rotated MNIST and rotated face recognition tasks, the authors should compare the accuracy of their method with the latest state-of-the-art group-equivariant deep networks instead of just regular CNNs. This will help to truly understand whether their method is superior or comparable to the more computationally expensive group-equivariant networks that are specifically designed to handle rotations in terms of accuracy as well or not. The regular CCNs, which are not designed to handle rotations, are obviously bound to be inferior to their approach.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting work with promising results with issues in experimental section",
            "review": "This work extends on [1] by constructing CNN filters using Fourier-Bessel (FB) bases for rotation equivariant networks. Additionally to [1] it extends the process with using SO(2) bases which allow to learn combination of rotated FB bases and ultimately achieve good performance with less parameters than standard CNN networks thanks to filter truncation.\n\nIn general, this work is well written and shows interesting results. However it lacks context with regards to other existing works. For example [2] also uses steerable filters for achieving rotation equivariance, however with different steerable bases (rotation harmonics instead of FB). It would be useful to clarify why FB bases are more appropriate for truncation, eventually providing empirical evidence (even though rotation harmonics would probably need more parameters). Authors mention [2], however disregard it due to computational complexity, which would be the same if the rotation harmonics bases were truncated as well.\n\nSimilarly, this work is not strong in evaluating against existing methods. It provides evaluation of the vanilla group equivariant networks in a similar configuration, but due to design choices in the training and test set, it is not possible to compare it against other algorithms and other steerable bases such as those from [2]. This degrades the results slightly as it does not allow to verify the baseline results from other works.\n\nAdditionally, it would be useful to provide an ablation study which would show how important the bases in SO(2) are important for the model accuracy. This would allow to compare the results against the [1] as the FB filters are steerable as well (Equation 4).\n\nIt is hard to reach a final rating for this submission. On one hand, it can be seen as an incremental improvement of [1] for a new domain of tasks, without a thorough comparison against existing methods. On the other hand, the paper is well written and the results look promising - evaluation verifies that the algorithm performs well in multiple tasks with a fraction of parameters.\n\nConsidering that authors plan to release the source code and that this conference aims for publishing novel ideas (and the goal of this work is to achieve rotation equivariance with less parameters, which hasn't been tackled before), I am inclined towards acceptance of this paper, even though the experiments can be significantly improved.\n\nUnfortunately, I was not able to verify correctness of the provided proofs.\n\nAdditional minor issues:\n* The paper does not specify what FB bases exactly are being used (such as in [table 1;1]), mainly it does not seem to specify the SO(2) bases.\n* It would be useful to visualise K and K_\\alpha in Figure 1.\n* Citations, if not part of the sentence, should be in parentheses to improve readability (\\citep for natbib).\n* On page 8, end of first paragraph - wrong reference (see S.M.)\n* L, in section 2.3 is not defined.\n\n[1] Qiu, Qiang, et al. \"DCFNet: Deep Neural Network with Decomposed Convolutional Filters.\", ICML 2018\n[2] Weiler, Maurice, et al. “Learning Steerable Filters for Rotation Equivariant CNNs.” CVPR 2018\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}