{
    "Decision": {
        "metareview": "1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.\n\n- improvements to a transformer model originally designed for machine translation\n- application of this model to a different task: music generation\n- compelling generated samples and user study.\n\n2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.\n\n- lack of clarity at times (much improved in the revised version)\n\n3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, itâ€™s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.\n\nThe main contention was novelty. Some reviewers felt that adapting an existing transformer model to music generation and achieving SOTA results and minute-long music sequences was not sufficient novelty. The final decision aligns with the reviewers who felt that the novelty was sufficient.\n\n4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.\n\nA consensus was not reached. The final decision is aligned with the positive reviews for the reason mentioned above.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "successful adaptation of transformer networks to generating long coherent music sequences"
    },
    "Reviews": [
        {
            "title": "Implementation trick to reduce memory footprint of transformer / experiments on music generation",
            "review": "This paper presents an implementation trick to reduce the memory footprint of relative attention within a transformer network. Specifically, the paper points out redudant computation and storage in the traditional implementation and re-orders matrix operations and indexing schemes to optimize. As an appllication, the paper applies the new implementation to music modeling and generation. By reducing the memory footprint, the paper is able to train the transformer with relative attention on longer musical sequences and larger corpora. The experimental results are compelling -- the transformer with relative attention outperforms baselines in terms of perplexity on development data (though test performance is not reported) and by manual evaluation in a user study.\n\nOverall, I'm uncomfortable accepting this paper in its current form because I'm not sure it constitutes a large enough unit of novel work. The novelty here, as far as I can tell, is essentially an implementation trick rather than an algorithm or model. Transformer networks have been applied to music in past work -- the only difference here is that because of the superior implementation the model can be trained from larger musical sequences. All that said, I do think the proposed implementation is useful and that the experimental results are compelling. Clearly, when trained from sufficient data, transformer networks have something to offer that is different from past techniques.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Cool idea, memory usage could be analysed deeper",
            "review": "The authors address the problem raised by applying a fully attentional network (FAN) to model music. \nThey argue clearly for the need of relational positional embedding in that problem (instead of absolute positional as in vanilla FAN), and highlight the quadratic memory footprint of the current solution (Shaw et al. 2018).\n\nThe main contribution of the paper is a solution to this, consisting in a smart idea (sect 3.4.1 and 3.4.2) which allows them to compute relative embeddings without quadratic overhead.\nThe model performs indeed better than Shaw et al.'s on the single data-set they compared both. On the other one, the argument is that Shaw et al. 2018 cannot be applied because the sequences are too long.\n\nI have two concerns with the paper:\n\t1/ it is very hard to read at times. In particular, the main contribution took me several passes the understand. I list below a few recommendations for improvement \n\t2/ the main argument is that the model requires less memory and is faster. However, the only empirical evidence in that direction is given in the introduction (Sect 1.1., second paragraph).\n\t\tThe following points remain unclear to me:\n\t\t\ta) why can't the Relative Transformer be applied to Piano-e composition. What is the maximal length that is possible?\n\t\t\tb) how much faster / less memory is the relative music transformers? The only data-point is in Sect 1.1., which seems indeed impressive (but then one wonders why this is not exploited further). A deeper analysis of the comparative memory footprint would greatly strengthen the paper in my opinion.\n\t\t\t\nWhy \"music\" relative transformers? Nothing in the model restrict it to that use case. The use of FAN over audio has been explored with limited success, one of the reasons being that - similarly to this use-case here - audio sequences tend to be longer than text.\n\t\t\nminor comments:\n\t- abstract, ln9: there seems to be a verb missing\n\t- p1,ln-2: \"dramatic\" improvements seems to be exaggerated\n\t- p2,ln11: \"too long\". too long for what?\n\t- p4,ln15: (Table 1). is one sentence by itself. Also, a clear explanation of that table is missing\n\t- p5,item 2: an explanation in formula would be helpful for those not familiar with reshaping\n\t- Fig3: it seems very anecdotical. Similar green bloxes might be placed on the left plot\n\t- sect4.1.1,ln3. that sentence does not parse\n\t- Table 2: what is cpsi?\n\t- $l$ is nicer formatted as $\\ell$\n\t- care should be taken to render the Figures more readable (notably the quality of Fig 4, and labels of Fig 7)\n\t- footnotes in Figures are not displayed (Table 2 and 4)\n\t- the description of the human evaluation leaves some open questions. I could not come up with 180 ratings (shouldn't it be 180 * 3 ratings?). Also, at least the values of Relative Transformer vs other 3 models should be shown (or all 6 comparisons). Here you call \"relative transformer\" your model, previously you used that term to refer to (Shaw et al. 2018).\n\t\twhen reporting statistical significance, there are some omissions which should be clarified.\n\t- (Shaw et al. 2018) has been published at NAACL. For such an important citation, you should update the reference from the arxiv version.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An application of transformer to music generation",
            "review": "In this paper the authors propose an algorithm to reduce the memory\nrequirements for calculating relative position vectors in a\nself-attention (transformer) network, based on the work of [Vaswani et\nal., 2017; Shaw et al. 2018]. The authors applied their model to a music\ngeneration task, and evaluated it on two datasets (J.S. Bach Chorales\nand Piano-e-Competition). Their model obtained improvements over the\nstate-of-the-art in the Piano-e-Competition set in terms of\nlog-likelihoods. Additionally, they performed human evaluation on the\nPiano-e-Competition set showing preference of the participants for their\nmethod over the state-of-the-art.\n\nThe application of the transformer network seems suitable for the task,\nand the authors fairly justify their motivations and choices. They show\nimprovements over the-state-of-the-art for one data-set and explained\ntheir results. They also show an interesting application of\nsequence-to-sequence models for generating complete pieces of music\nbased on a given melody.\n\nMy main concern is the novelty of the paper. The authors use the model\nproposed by [Shaw et al. 2018] with an additional modification to manage\nvery long sequences proposed by [Liu et al., 2018; Parmar et al., 2018],\n(chunking the input sequences in non-overlaping blocks and calculating\nattention only on the current and the previous blocks). Their main\ncontribution is to reduce the memory requirement for matrix operations\nfor calculating the relative position vectors of the self-attention\nfunction, which was sub-optimal in [Shaw et al. 2018]. The memory\nreduction is from O(L^2D+L^2) to O(LD+L^2). I would qualify this as an\noptimization in the implementation of the existing method rather than a\nnew approach.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Improved efficiency of transformer on long sequences, but a bit difficult to follow",
            "review": "This paper describes a method for improving the (sequence-length) scalability of the Transformer architecture, with applications to modeling long-range interactions in musical sequences. The proposed improvement is applied to both global and local relative attention formulations of self-attention, and consists of a clever re-use (and re-shaping) of intermediate calculations. The result shaves a factor of L (sequence length) from the (relative) memory consumption, facilitating efficient training of long sequences. The method is evaluated on MIDI(-like) data of Bach chorales and piano performances, and compares favorably to prior work in terms of perplexity and a human listener evaluation.\n\nThe results in this paper seem promising, though difficult to interpret.  The quantitative evaluation consists of perplexity\nscores (Tables 2 and 3), and the qualitative listening study is analyzed by pairwise comparisons between methods. While the proposed method achieves the highest win-rate in the listening study, other results in the study (LSTM vs Transformer) run contrary to the ranking given by the perplexity scores in Table 3. This immediately raises the question of how perceptually relevant the (small) differences in perplexity might be, which in turn clouds the overall interpretation of the results. Of course, perplexity is not the whole story here: the focus of the paper seems to be on efficiency, not necessarily accuracy, but one might expect improved efficiency to afford higher model capacity and improve on accuracy.\n\n\nThe core contributions of this work are described in sections 3.4 and 3.5, and while I get the general flavor of the idea, I find the exposition here both terse and difficult to follow. Figures 1 and 2 should illustrate the core concept, but they lack axis labels (and generally sufficient detail to decode properly), and seem to use the opposite color schemes from each-other to convey the same ideas.  Concrete image maps using real data (internal feature activations) may have been easier to read here, along with an equation that describes how the array indices map after skewing.\n\nThe description in 3.4 of the improved memory enhancement is also somewhat difficult to follow.  The claim is a reduction from O(DL^2) to O(DL), but table 1 lists this as O(DL^2) to O(DL + L^2).  In general, I would expect L to dominate D, which still leaves the memory usage in quadratic space, so it's not clear how or why this constitutes an improvement. The improvement due to moving from global to local attention is clear, but this does not appear to be a contribution of this work.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}