{
    "Decision": {
        "metareview": "The paper provides a novel attack method and contributes to evaluating the robustness of neural networks with recently proposed defenses. The evaluation is convincing overall and the authors have answered most questions from the reviewers. We recommend acceptance. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Good paper, accept."
    },
    "Reviews": [
        {
            "title": "Interesting and novel idea, needs more experimental validation",
            "review": "The authors study the problem of generating strong adversarial attacks on binarized neural networks (networks whose weights are binary valued and have a sign function nonlinearity).  Since these networks are not continuous (due to the sign function nonlinearity), it is possible that standard gradient-based attack algorithms are not effective at producing adversarial examples. While this problem can be encoded as a mixed integer linear program, off-the-shelf MILP solvers are not scalable to larger/deeper networks. Thus, the authors propose a new target propagation style algorithm that attempts to infer desired activations at each layer (from the perspective of maximizing the adversary's objective) starting at the final layer and moving towards the input. The propagation at each layer requires solving another MILP (albeit a much smaller one). Further, in order to prevent the target propagation from discovering assignments at upper layers that are unachievable given the constraints at lower layers, the authors propose two heuristics (making small moves and penalizing deviations from the previous target values) to obtain an effective attack algorithm. The authors validate their approach experimentally on MNIST/Fashion MNIST image classifiers.\n\nQuality: The paper is reasonably well written and the key ideas are communicated well. However, the experimental section needs to be improved significantly.\n\nClarity: The paper is easy to understand and organized well.\n\nOriginality: The application of target propagation in the context of adversarial examples is certainly novel and so are the specific enhancements proposed in the context of adversarial example generation. The \n\nSignificance: The study of adversarial examples for binarized networks is novel and important and effective attack generation algorithms are a significant first step towards training robust models of this type - this could enable deployment of robust and compact binarized classifiers in on-device settings (where model size is important).\n\nCons\nMy main concerns with this paper are regarding the experimental evaluation - I do not feel these are sufficient to justify the strength of the attack method proposed. Here are my broad concerns:\n1. Even though the datasets used are small (MNIST/Fashion MNIST), the experimental validation of adversarial attacks is only performed on 100 test examples. This is not sufficiently representative (given experimental evidence with adversarial attacks on non-binarized models) and this needs to be addressed for the results to be considered conclusive.\n\n2. The attack method is only compared to FSGM, which is known to be a rather poor attack even on non-binarized networks. The authors should compare to stronger gradient based attacks (like PGD) and gradient free attacks which have been used to break adversarial defenses that are nondifferentiable in prior work - https://arxiv.org/abs/1802.00420 and https://arxiv.org/abs/1802.05666). Further, the MILP approach used can be strengthened by doing better bound propagation (like in https://arxiv.org/pdf/1711.00455.pdf)\n\n3. The attack radii used are very small compared to what has been used in non-binarized networks, where networks have been trained to even be verifiably robust to adversarial pertrubations of much larger radii (see for example https://arxiv.org/pdf/1805.12514.pdf). Given the existence of this work, it is important to evaluate the algorithms proposed on larger radii (since it is possible to construct non-binarized networks that are indeed robust to perburbations of eps=.1-.3 on MNIST).\n\n4. Motivation for binarization: I assume that motivation for binarized models arising from faster training/inference times and smaller model sizes. However, to justify this, the authors need to compare their BNNs to comparable non-binarized neural networks (for example,ones that are similar  in terms of number of bits used to represent the model) on training time, inference time and adversarial robustness. Otherwise, it seems hard to see why binarized networks are valuable from a robustness.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "an interesting paper ",
            "review": "This paper proposed a new attack algorithm based on MILP on binary neural networks. In addition to the full MILP formulation, the authors proposed an integer target propagation algorithm (IProp) to find adversarial examples by solving a smaller (instead of the full) MILP.  \n\nThe topic is important but the clarity should be improved. It is less clear when describing the Iprop algorithm.  \n\nQuestions:\n1. Can IProp work for other architectures? It looks like the propagation steps work on only fully connected layers (or conv layers) with activation functions. Does it work for pooling layers?\n2. The results in Figure 2 look weird and might be wrong:\nsince MIP is the exact solution (green bar), how is it possible that the prediction flip rate of IProp larger than MIP? See top row figures where some red bars are larger than green bars. \n3. Also, is the FGSM method comparing in Figure 2 operating on the approximate BNN as described in the related work? How does the performance of PGD (Madry etal) compared to IProp?  \n4. How are the big M parameters in equation 4 and 5 computed? Is the formulation eq (1) to (8) the same as that in Tjeng 2018? Since BNN is a special case of general neural networks. Please elaborate. \n5. In Sec 2 related work, why \"there's no objective function\" for verification method? ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Reviewer comments",
            "review": "This paper presents an algorithm to find adversarial attacks to binary neural networks.  Binary neural networks uses sign functions as nonlinearities, making the network essentially discrete.  Previous attempts at finding adversarial attacks for binary neural networks either rely on relaxation which cannot find very good adversarial examples, or calling a mixed integer linear programming (MILP) solver which doesnâ€™t scale.  This paper proposes to decompose the problem and iteratively find desired representations layer by layer from the top to the input.  This so called Integer Propagation (IProp) algorithm is more efficient than solving the full MILP as it solves much smaller MILP problems, one for each layer, thus each step can be solved relatively quickly.  The authors then proposed a few more improvements to the IProp algorithm, including ways to do local adjustments to the solutions, and warming starting from an existing solution.  Experiments on binary neural nets trained for MNIST and Fashion MNIST show the superiority of the proposed method over MILP and relaxation based algorithms.\n\nOverall I found the paper to be very clear and the proposed method is sound.  I think combining ideas from discrete / combinatorial optimization with deep learning is an important research direction and can shed light on training and verifying models with discrete components, like the hard nonlinearities in the binary neural nets studied in this paper.\n\nIn terms of the particular proposed approach, it is hard for me to imagine the blind IProp that does not take the input into account until the last layer is ever going to work.  The small step size modifications make a lot more sense.  Regarding the selection of the set S, in the paper the authors simply sampled elements to be in S uniformly, but it seems possible to make use of the information from the forward pass, and choose the hidden units that are the closed to reaching the desired activations.  Would that be any better?\n\nA few minor comments:\n- when reporting warm start results, it would be good to also show the performance of the FGSM solution used for warm starting, in addition to the other two results shown in Figure 6 to have a more complete comparison\n- the hidden units h_{l,j} were formulated to be in {0, 1} in equation (7), but everywhere else in the paper they are assumed to be in {-1, +1}, which is not consistent and slightly confusing.\n\nOverall I think this is a solid paper and support accepting it for publication.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}