{
    "Decision": {
        "metareview": "The paper proposes an improved method for uncertainty estimation in deep neural networks.\n\nReviewer 2 and AC note that the paper is a bit isolated in terms of comparing the literature.\n\nHowever, as all of reviewers and AC found, the paper is well written and the proposed idea is clearly new/interesting.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Interesting idea"
    },
    "Reviews": [
        {
            "title": "Bias-reduced uncertainty estimation for deep neural classifiers",
            "review": "This paper presents an improved method for uncertainty estimation in deep neural networks, based on  their observations that the confidence scores based on highly confident points and low confidence points would be quite different. \n\nThe paper is in general well presented. The proposed method is well motivated (as in section 5). The results of the AES algorithm support well the proposed idea, which nevertheless looks simple. \n\nSection 3 needs further improvement in clarity. \n\nFigure 1 needs to be better presented. \n\nFigure 2(a) - please make the curves color-blind friendly. \n\nSGD (stochastic gradient descent?) needs to be defined, and you can't assume everybody knows what it is. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper but contribution feels isolated from related work",
            "review": "-- Paper Summary --\n\nThe proposed methodology draws on the connection between boosting in ensemble learning and SGD for training DNNs, whereby misclassified instances are implicitly targeted in later training iterations once easier examples have been classified correctly. The authors observe that this incurs a trade-off in which easily-classified examples become susceptible to overfitting at later stages in the training procedure when the network parameters adapt to fit more complex examples. Two early stopping algorithms are proposed in order to mitigate this issue. The first approach, PES, is more robust, but too computationally expensive to be applied in practice; on the other hand, AES approximates the former procedure by directly assuming that easier training examples will be learnt earlier on in the training procedure. The proposed technique is shown to calibrate the confidence scores obtained from state-of-the-art approaches for training deep nets, resulting in substantial performance improvements with respect to the proposed E-AURC metric. \n\n-- General Commentary --\n\n- The paper isolates itself from other post-calibration methods by stating that ‘our focus here is only on the core task of ranking uncertainties’. In doing so, there is no comparison to other calibration methods, which makes it difficult to properly assess the impact of this work in comparison to other papers addressing the poor calibration of uncertainty typically associated with deep nets. The authors immediately dismiss PES as being too computationally expensive, so I’d be interested in at least seeing AES be compared to more lightweight calibration methods.\n\n - This paper champions the use of an alternative metric (E-AURC) for assessing model quality, which is the sole quantity of interest in the experimental evaluation. While the E-AURC metric is indeed well-motivated in Section 3, I could see there being some scepticism as to why more traditional metrics such as log likelihood aren’t used here. This would also facilitate comparison to other post-calibration methods. In this regard, the authors should consider supplementing their experiments with more widely-used metrics not limited to uncertainty ranking.\n\n- I would be interested in seeing the analysis shown in Figure 2 extended to each of the baseline models discussed in the paper. Such examples would give a clearer perspective of which methods are particularly susceptible to the overfitting problem targeted by the methodology proposed in this work.\n\n- Some of the notation in the problem statement is a bit confusing, with i being simultaneously  used as the training iteration number as well as an index for Y. This needs to be updated. \n\n- There’s a lot of whitespace in Figure 1 which could be avoided by giving additional examples of how the metric works.\n\n- ‘Early Stopping without a Validation Set (Mahsereci et al, 2017)’ warrants a citation here.\n\n- The paper is otherwise generally well-written and a pleasure to read. Some spotted typos:\n\nP1: for highly confident instance(s)\nP3: which borrows element(s)\nP3: ‘unit-less’ : this is unhyphenated in another part of the text\nP5: Final reference to Figure 2(b) should refer to Figure 2(c) instead   \nP7: which (is) initialized\n\n-- Recommendation --\n\nI admit to feeling fairly ambivalent about this paper - on one hand, the paper is well-written and its contributions are effectively communicated. While myopic, the experiments also convincingly showcase the performance improvements obtained by applying AES over the baseline methods. On the downside, this paper limits itself to comparing the proposed approaches to baseline methods where no other calibration is carried out. Lack of direct comparison against other post-calibration methods results in the paper adding little to the overall literature on DNNs other than asserting that calibration through early stopping is better than not doing anything else.\n\nPros/Cons: \n\n+ Properly-motivated contributions and well-written paper.\n+ The two early stopping algorithms are explained well, even if the appealing connection to boosting gets lost somewhere along the way.\n+ Results show that AES improves the results of several DNN training approaches.\n\n- Use of E-AURC as the sole metric for assessing quality in the Experiments section exposes this paper to instant criticism.\n- The notion of preserving model snapshots can be problematic when training requires thousands of epochs.\n- No comparison to other post-calibration techniques. \n\n\n** Post-rebuttal\n\nScore increased to a 7 following rebuttal and paper revision.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Non-bayesian uncertainty estimation for deep nets.",
            "review": "In this papers, the authors introduce a new technique to output uncertainty estimates from any family of neural nets. The key insight in this paper is that when considering existing SGD methods the following behavior occurs: if we think of \"easy\" and \"hard\" to classify datapoints, a NN trained with SGD will output good uncertainty estimates early on in training, but once the network focusses on tuning the parameters for the hard cases, the uncertainty estimates for the easy datapoints deteriorates. The algorithms proposed by the authors takes an existing uncertainty method (or confidence score function) and uses intermediate snapshots of SGD training to improve the final uncertainty estimates. Note that the focus in this work is on ranking uncertainties (and the authors suggest to leave calibrating uncertainties to existing methods).\n\nThe paper generally is well written (e.g. section 5) although I found section 3 to be a bit hard to follow. I'm not very familiar with the area itself but I was surprised to see in Section 7 that the results are not compared to full Bayesian methods (possibly on a dataset that lends itself well to that).\n\nNotes:\n- Section 3, \"A selective classifier ...\" -> I think this section could use some additional untuition to make the explanation more understandable.\n- Section 3, \"defined to be the selective risk as a function of coverage.\" -> do you mean as a sequence of functions g?\n- ",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}