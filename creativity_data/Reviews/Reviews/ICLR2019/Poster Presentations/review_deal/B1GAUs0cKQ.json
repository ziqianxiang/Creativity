{
    "Decision": {
        "metareview": "The authors describe a very counterintuitive type of layer: one with mean zero Gaussian weights. They show that various Bayesian deep learning algorithms tend to converge to layers of this variety. This work represents a step forward in our understanding of bayesian deep learning methods and potentially may shine light on how to improve those methods. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Interesting and counter-intuitive result"
    },
    "Reviews": [
        {
            "title": "An interesting paper, but a few questions needed to be answered",
            "review": "This paper investigates the effects of mean of variational posterior and proposes variance layer, which only uses variance to store information.\n\nOverally, this paper analyzes an important but not well explored topic of variational dropout methods—the mean propagation at test time, and discusses the effect of weight variance in building a variational posterior for Bayesian neural networks. This findings are interesting and I appreciate the analysis. \n\nHowever, I think the claim for benefits of variance layer is not well supported. Variance layer requires test-time averaging in test time to achieve competitive accuracy, while the additive case in Eq. (14) using mean propagation achieves similar performance (e.g., the results in Table 1). The results in Sec 6 lack comparison to other Bayesian methods (e.g., the additive case in Eq. (14)). \n\nBesides, there exists several problems which needs to be addressed.\n\nSec 5.\nSec 5 is a little hard to follow. Which prior is chosen to produce the results in Table 1? KL(q||p)=0 for the zero-mean case corresponds to the fact that the variational posterior equals the prior, which implies the ARD prior if I did not misunderstand. In this case, the ground truth posterior p(w|D) for different methods is different and corresponding ELBO for them are incomparable.\n\nSec 6. \nThe setting in Table 2 is also unclear. As ``Variance’’ stands for variational dropout, what does ``Dropout’’ means? The original Bernoulli dropout? Besides, I’m wondering why directly variance layer (i.e., zero-mean case in Eq. (14)) is not implemented in this case.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "variance network uses a variational distribution with zero mean, but it achieves nice performances. ",
            "review": "This paper studies variance neural networks, which approximate the posterior of Bayesian neural networks with zero-mean Gaussian distributions. The inference results are surprisingly well though there is no information in the mean of the posterior. It further shows that the several variational dropout methods are closed related to the proposed method. The experiment indicates that the ELBO can actually better optimized with this restricted form of variational distribution. \n\nThe paper is clearly written and easy to follow. The technique in the paper is solid.\n\nHowever, the authors might need to clarify a few questions below. \n\n\nQ1:  if every transformation is antisymmetric non-linear, then it seems that the expected distribution of $t$ in (2) is zero. Is this true or not? In another word, class information has to be read out from the encoding of instances in Fig 1. It seems antisymmetric operators cannot do so, as it will only get symmetric distributions from symmetric distributions. \n\nQ2: it is not straightforward to see why KL term needs to go zero. In my understanding, the posterior aims to fit two objectives: maximizing data likelihood and minimizing KL term. When the signal from the data is strong (e.g. large amount of data), the first objective becomes more important. Then q does not really try to make KL zero, and alpha has no reason to go infinity. Can you explain more? \n\nQ3: Is the claimed benefit from the optimization procedure or the special structure of the variance layer? Is it possible to test the hypothesis by 1) initializing a q distribution with learnable mean by the solution of variance neural network and then 2) optimizing q? Then the optimization procedure should continue to increase ELBO. Then compare the learned q against the variance neural network. If the learned q is better than the variance network -- it means the network structure is better for optimization, but the structure itself might not be so special. If the learned q is worse than the variance network, then the structure is interesting. \n\n\nA few detailed comments:\n\n1. logU used without definition. \n2. if the paper has a few sentence explaining \"Gaussian dropout approximate posterior\", section 4 will be smoother to read. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Stochastic neural networks with zero mean posterior on weights",
            "review": "This paper introduced a new stochastic layer termed variance layer for Bayesian deep learning, where the posterior on weight is a zero-mean symmetric distribution (e.g., Gaussian, Bernoulli, Uniform). The paper showed that under 3 different prior distributions, the Gaussian Dropout layer can converge to variance layer. Experiments verified that it can achieve similar accuracies as conventional binary dropout in image classification and reinforcement learning tasks, is more robust to adversarial attacks, and can be used to sparsify deep models.\n\nPros:\n(1)\tProposed a new type of stochastic layer (variance layer)\n(2)\tCompetitive performance on a variety of tasks: image classification, robustness to adversarial attacks, reinforcement learning, model compression\n(3)\tTheoretically grounded algorithm\n\nCons:\n(1)\tMy main concern is verification. Most of the comparisons are between variance layer (zero-mean) and conventional binary dropout, while the main argument of the paper is that it’s better to set Gaussian posterior’s mean to zero. So in all the experiments the paper should compare zero-mean variance layer against variational dropout (neuron-wise Eq. 14) and sparse variational dropout (additive Eq. 14), where the mean isn’t zero.\n(2)\tThe paper applies variance layers to some specific layers. Are there any guidelines to select which layers should be variance layers?\n\nSome minor issues:\n(1)\tPage 4, equations of Gaussian/Bernoulli/Uniform variance layer, they should be w_ij=…, instead of q(w_ij)= …\n(2)\tWhat’s the prior distribution used in the experiment of Table 1?\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}