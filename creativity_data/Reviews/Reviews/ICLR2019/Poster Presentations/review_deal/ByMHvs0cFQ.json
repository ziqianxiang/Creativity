{
    "Decision": {
        "metareview": "The authors derive and experiment with quaternion-based recurrent neural networks, and demonstrate their effectiveness on speech recognition tasks (TIMIT and WSJ), where the authors demonstrate that the proposed models can achieve the same accuracy with fewer parameters than conventional models. The reviewers were unanimous in recommending that the paper be accepted.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Interesting work applying quaternion representations to neural networks"
    },
    "Reviews": [
        {
            "title": "Explores a good and important direction, with encouraging results",
            "review": "The paper takes a good step toward developing more structured representations by exploring the use of quaternions in recurrent neural networks.  The idea is motivated by the observation that in many cases there are local relationships among elements of a vector that should be explicitly represented.  This is also the idea behind capsules - to have each \"unit\" output a vector of parameters to be operated upon rather than a single number.   Here the authors show that by incorporating quaternions into the representations used by RNNs or LSTMs, one achieves better performance at speech recognition tasks using fewer parameters.\n\nThe quaternionic representation of the spectrogram chosen here seems a bit arbitrary.  Why are these the attributes to be packaged together?  its not obvious.  Shouldn't this be learned?\n\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Quaternion Recurrent Neural Networks",
            "review": "Quality: sufficient though there are issues. Work done in automatic speech recognition on numerous variants of recurrent models, such as interleaved TDNN and LSTM (Peddinti 2017), is completely ignored [addressed in the revision]. The description of derivatives needs to mention the linear relationship between input features and derivatives (see trajectory HMMs by Zen and Tokuda) [addressed in the revision]. TIMIT is a very simple task [addressed by adding WSJ experiments]. Derivations in the appendices could be connected better [addressed in the revision]. \n \nClarity: sufficient. It would be good to see some discussion of 1) split activations and other possible options [short comment added in the revision] if any 2) expressions of derivatives and their connection to standard RNN derivatives [short comment added in the revision], 3) computational complexity [addressed in the revision]. \n\nOriginality: sufficient. This paper describes the extension of quaternion feed-forward neural networks to recurrent neural networks and a parameter initialisation method in the quaternial domain.\n\nSignificance: sufficient. \n\nPros: Audience interested in quaternial neural networks would benefit from this publication. Experimental results even if limited suggest that quaternial representation may offer a significant reduction in the number of model parameters at no loss in performance.  \n\nCons: The choice of derivatives to yield quaternions as there are other more interesting views to contemplate both in speech and other fields. A simple task makes it hard to judge how the quaternion extension would scale.  \n\nOther:\n\nThe format of references, the use of a number in parentheses, is unusual and distractive. [fixed in the revision] \nPlease at least name all the terms used in the main paper body even if they are defined later in the appendix (e.g. h_{t}^{*} in equation 10). [fixed in the revision]\nDo both W_{hh} and b_{h} contain the same \\delta_{hh}^{t} term in their update equation 11? [fixed in the revision]\nPage 7 by mistake mentions 18.2% which cannot be found in the Table 1. [fixed in the revision]\nPage 12 \"is equals to\" [remains in the revision]\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Simple but nice development of the framework based on mostly well-known algebra but lacks experimental validation",
            "review": "After the discussion with authors, I am happy to recommend acceptance.\n————————————————————\n\n1.\tIn “Consequently, for each input vector of size N, output vector of size M, dimensions are split into four parts: the first one equals to r, the second is xi, the third one equals to yj, and the last one to zk to compose a quaternion Q = r1 + xi + yj + zk”, are you splitting dimension M or M\\times N? And if you split M \\times N (I believe that’s what you are doing), in which order you are splitting (row major right?) Please explain.\n2.\tI did not understand why authors didn’t go in the negative direction of the gradient in Eq. (10-11)?\n3.\tIn section 3.4, authors mentioned “Moreover, an hyper-complex parameter cannot be simply initialized randomly and component-wise, due to the interactions between components.” which I strongly agree. But in Eq. (7) and (9) why the update rules and activation function are applied component wise?\n4.\tI really like the elegance in the parameter initialization. Couple of minor things here: (1) It’s better to mention in Eq. (16) why E(|W|) is 0 because of symmetry. (2) Reference should be 6.1 instead of 5.1.\n5.\tAnother reasonable baseline will be using a complex network like (https://openreview.net/forum?id=H1T2hmZAb) and use the first two terms in Eq. (19) for representation. This will also possibly justify the usefulness of using higher order partials. \n6.\tThe authors mentioned multiple times about the achieved state-of-the-art results without giving any citation. As a reader not well versed in the acoustic domain, it will be nice to see some references to cross-validate the claim made.\n\n\n\nGeneral Comments:\n1.\tI understand the necessity of defining RNN/ LSTM model in the space of quaternions. But unit quaternions can be identified with other spaces where convolution is defined recently, e.g., with S^3 (https://arxiv.org/abs/1809.06211). I can see that this paper is contemporary, but at least can authors comment on the applicability of this general method in their case? Given that in NIPS’18 the following paper talked about RNN model on non-Euclidean spaces (https://arxiv.org/pdf/1805.11204.pdf), one can extend these ideas to develop an RNN model in the space of quaternions. Authors should look into it rigorously as future directions? But at least please comments on the applicability.\n2.\tThe experimental results section is somewhat weak, the overall claim of using fewer parameters and achieving comparable results is only validated on TIMIT data. More experimentation is necessary. \n3.\tIn terms of technical novelty, though quaternion algebra is well-known, I like the parameter initialization algorithm. I can see the merit of this in ML/ vision community.  \n\nPros: \n1. Nice well grounded methodological development on well-known algebra. (simple but elegant, so that's good).\n2. Nicely written and all the maths check out (that's good).\n3. Experimental result on TIMIT dataset shows usefulness in terms of using fewer parameters (but still can achieve SOA results).\n\nCons:\n1. See my comments above. I expect the authors to rebut/ address the aforementioned comments. Overall though simple but nice (and necessary) development of RNN/ LSTM framework in the space of quaternions. \n2. Lacks extensive experimental validation.\n\nMy reason for my rating is mainly because of (1) lack of experimental validation. (2) being aware of the recent development of general RNN model on non-Euclidean spaces, I want some comments in this direction (see detailed comment and reference above).\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}