{
    "Decision": {
        "metareview": "The paper is well written and easy to follow. The experiments are adequate to justify the usefulness of an identity for improving existing multi-Monte-Carlo-sample based gradient estimators for deep generative models. The originality and significance are acceptable, as discussed below.\n\nThe proposed doubly reparameterized gradient estimators are built on an important identity shown in Equation (5). This identity appears straightforward to derive by applying both score-function gradient and reparameterization gradient to the same objective function, which is expressed as an expectation. The AC suspects that this identity might have already appeared in previous publications / implementations, though not being claimed as an important contribution / being explicitly discussed. While that identity may not be claimed as the original contribution of the paper if that suspicion is true, the paper makes another useful contribution in applying that identity to the right problem: improving three distinct training algorithms for deep generative models. The doubly reparameterized versions of IWAE and reweighted wake-sleep (RWS) further show how IWAE and RWS are related to each other and how they can be combined for potentially further improved performance. \n\nThe AC believes that the paper makes enough contributions by well presenting the identity in (5) and applying it to the right problems. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "An useful identity that helps improve existing training algorithms for deep generative models"
    },
    "Reviews": [
        {
            "title": "Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives",
            "review": "This paper applies a reparameterization trick to estimate the gradients objectives encountered in variational autoencoder based frameworks with continuous latent variables.  Especially the authors use this double reparameterization trick on Importance Weighted Auto-Encoder (IWAE) and Reweighted Wake-Sleep (RWS)  methods. Compared to IWAE, the developed method's SNR does not go to zero with increasing the number of particles.\n\nOverall, I think the idea is nice and the results are encouraging. I checked all the derivations, and they seem to be correct. Thus I recommend this paper to be accepted in its current form.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good paper",
            "review": "The paper observes the gradient of multiple objective such as IWAE, RWS, JVI are in the form of some “reward” multiplied with score function which can be calculated with one more reparameterization step to reduce the variance. The whole paper is written in a clean way and the method is effective.\n\nI have following comments/questions:\n\n1. The conclusion in Eq(5) is correct but the derivation in Sec. 8.1. may be arguable. Writing \\phi and \\tilde{\\phi} at the first place sets the partial derivative of \\tilde{\\phi}  to \\phi as 0. But the choice of \\tilde{\\phi} in the end is chosen as \\phi. If plugging  \\phi to \\tilde{\\phi}, the derivation will change. The better way may be calculating both the reparameterization and reinforce gradient without redefining a \\tilde{\\phi}.\n\n2. How does the variance of gradient calculated where the gradient is a vector? And how does the SNR defined in the experiments?\n\n3. How does the variance reduction from DReG changes with different value of K?\n\n4. Is there any more detailed analysis or intuition why the right hand side of Eq(5) has lower variance than the left hand side?",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Proposed method is interesting but additional experiments are needed",
            "review": "Overall:\nThis paper works on improving the gradient estimator of the ELBO. Author experimentally found that the estimator of the existing work(STL) is biased and proposed to reduce the bias by using the technique like  REINFORCE.\nThe problem author focused on is unique and the solution is simple, experiments show that proposed method seems promising.\n\nClarity:\nThe paper is clearly written in the sense that the motivation of research is clear, the derivation of the proposed method is easy to understand.\n\nSignificance:\nI think this kind of research makes the variational inference more useful, so this work is significant. But I cannot tell the proposed method is really useful, so I gave this score.\nThe reason I doubt the reason is that as I written in the below, the original STL can handle the mixture of Gaussians as the latent variable but the proposed method cannot. So I do not know which is better and whether I should use this method or use the original STL with flexible posterior distribution to tighten the evidence lower bound. I think additional experiments are needed. I know that motivation is a bit different for STL and proposed method but some comparisons are needed.\n\nQuestion and minor comments:\nIn the original paper of STL, the author pointed out that by freezing the gradient of variational parameters to drop the score function term, we can utilize the flexible variational families like the mixture of Gaussians.\nIn this work, since we do not freeze the variational parameters, we cannot utilize the mixture of Gaussians as in the STL. IWAE improves the lower bound by increasing the samples, but we can also improve the bound by specifying the flexible posteriors like the mixture of Gaussians in STL.\nFaced on this, I wonder which strategy is better to tighten the lower bound, should we use the STL with the mixture of Gaussians or use the proposed method?  \nTo clarify the usefulness of this method, I think the additional experimental comparisons are needed.\n\nAbout the motivation of the paper, I think it might be better to move the Fig.1 about the Bias to the introduction and clearly state that the author found that the STL is biased \"experimentally\".\n\nFollowings are minor comments.\nIn experiment 6.1, I'm not sure why the author present the result of K ELBO estimator in the plot of Bias and Variance.\nI think author want to point that when K=1, STL is unbiased with respect to the 1 ELBO, but when k>1, it is biased with respect to IWAE estimator.\nHowever, the objective of K ELBO and IWAE are different, it may be misleading. So this should be noted in the paper.\n\nIn Figure 3, the left figure, what each color means? Is the color assignment is the same with the middle figure?\n(Same for Figure 4)",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}