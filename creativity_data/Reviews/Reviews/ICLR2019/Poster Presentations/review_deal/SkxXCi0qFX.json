{
    "Decision": {
        "metareview": "The paper studies the credit assignment problem in meta-RL, proposes a new algorithm that computes the right gradient, and demonstrates its superior empirical performance over others.  The paper is well written, and all reviewers agree the work is a solid contribution to an important problem.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Interesting work, novel contribution"
    },
    "Reviews": [
        {
            "title": "an interesting trial to correct the current algorithm, but weak support to the claim",
            "review": "\nIn this paper, the authors investigate the gradient calculation in the original MAML (Finn et al. 2017) and E-MAML (Al-Shedivat et al. 2018). By comparing the differences in the gradients of these two algorithms, the authors demonstrate the advantages of the original MAML in taking the casual dependence into account. To obtain the correct estimation of the gradient through auto-differentiation, the authors exploit the DiCE formulation. Considering the variance in the DiCE objective formulation, the authors finally propose an objective which leads to low-variance but biased gradient. The authors verify the proposed methods in meta-RL tasks and achieves comparable performances to MAML and E-MAML. \n\n\nAlthough the ultimate algorithm proposed by this paper is not far away from MAML and E-MAML, they did a quite good job in clarify the differences in the existing variants of MAML from the gradient computation perspective and reveal the potential error due to the auto-differentiation. The proposed new objective and the surrogate is well-motivated from such observation and the trade-off between variance and bias. \n\n\nMy major concern is how big the effect is if we use (3) comparing to (4) in calculate the gradient. As the authors showed, the only difference between (3) and (4) is the weights in front of the term \\nabla_\\theta\\log\\pi_\\theta: the E-MAML is a fixed weight and the MAML is using a adaptive through the inner product. Whether the final difference in Figure 4 between MAML and E-MAML is all caused by such difference in gradient estimation is not clearly. In fact, based on the other large-scale high-dimension empirical experiments in Figure 2, it seems the difference in gradient estimator (3) and (4) does not induced too much difference in final performances between MAML and E-MAML. Based on such observation, I was wondering the consistent better performance of the proposed algorithm might not because the corrected gradient computation from the proposed objective. It might because the clip operation or other components in the algorithm. To make a more convincing argument, it will be better if the authors can evaluate different gradient within the same updates.\n\nI am willing to raise my score if the author can address the question. \n\nminor:\n\nThe gradients calculation in Eq (2) and (3) are not consistent with the Algorithm and the appendix.\n\nThe notation is not consistent with common usage: \\nabla^2 is actually used for denoting the Laplace operator, i.e., \\nabla^2 = \\nabla \\cdot \\nable, which is a scalar. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "In this paper, the author proposed an efficient surrogate loss for estimating  Hessian in the setting of Meta-reinforcement learning (Finn.et al, 2017), which significantly reduce the variance while introducing small bias. The author verified their proposed method with other meta-learning algorithms on the Mujoco benchmarks. The author also compared with unbiased higher order gradient estimation method-DiCE in terms of gradient variance and average return. \n\nThe work is essentially important due to the need for second-order gradient estimation for meta-learning (Finn et al., 2017) and other related work such as multi-agent RL. The results look promising and the method is easy to implement. I have two detail questions about the experiment:\n\n1) As the author states, the new proposed method introduces bias while reducing variance significantly. It is necessary to examine the MSE, Bias, Variance of the gradient estimatorsquantitatively  for the proposed and related baseline methods (including MAML, E-MAML-TRPO, LVC-VPG, etc). If the bias is not a big issue empirically, the proposed method is good to use in practice.\n\n2)  The author should add DiCE in the benchmark in section 7.1, which will verify its advantage over DiCE thoroughly.\n\nOverall this is a good paper and I vote for acceptance.\n\n\nFinn, Chelsea, et al. \"Model-agnostic meta-learning for fast adaptation of deep networks.\" ICML 2017.\n\nFoerster, Jakob, et al. \"DiCE: The Infinitely Differentiable Monte-Carlo Estimator.\" ICML 2018.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Strong paper, Strong accept",
            "review": "The paper first examines the objective function optimized in MAML and E-MAML and interprets the terms as different credit assignment criteria. MAML takes into account the dependences between pre-update trajectory and pre-update policy, post-update trajectory and post-update policy by forcing the gradient of the two policies to be aligned, which results in better learning properties. \nThought better, the paper points out MAML has incorrect estimation for the hessian in the objective. To address that, the paper propose a low variance curvature estimator (LVC). However, naively solving the new objective with LVC with TRPO is computationally prohibitive. The paper addresses this problem by proposing an objective function that combines PPO and a slightly modified version of LVC.\n\nQuality: strong, clarity:strong, originality:strong, significance: strong,\n\nPros:\n- The paper provides strong theoretical results. Though mathematically intense, the paper is written quite well and is easy to follow.\n- The proposed method is able to improve in sample complexity, speed and convergence over past methods.\n- The paper provides strong empirical results over MAML, E-MAML. They also show the effective of the LVC objective by comparing LVC over E-MAML using vanilla gradient update.\n- Figure 4 is particularly interesting. The results show different exploration patterns used by different method and is quite aligned with the theory.  \nCons:\n- It would be nice to add more comparison and analysis on the variance. Since LVC is claimed to reduce variance of the gradient, it would be nice to show more empirical evidences that supports this. (By looking at Figure 2, although not directly related, LVC-VPG seems to have pretty noisy behaviour)\n\n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}