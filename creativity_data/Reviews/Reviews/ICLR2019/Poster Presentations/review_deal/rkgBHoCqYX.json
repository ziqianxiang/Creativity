{
    "Decision": {
        "metareview": "The manuscript studies a random matrix approach to recover sparse principal components. This work extends prior work using soft thresholding of the sample covariance matrix to enable sparse PCA. In this light, the main contribution of the paper is a study of generalizing soft thresholding to a broader class of functions and showing that this improves performance. The contributions of this paper are primarily theoretical.\n\nThe reviewers and AC note issues with the discussion that can be further improved to better illustrate contributions, and place this work in context. In particular, multiple reviewers assumed that \"kernel\" referred to the covariance matrix. The authors provide a satisfactory rebuttal addressing these issues.\n\nWhile not unanimous, overall the reviewers and AC have a positive opinion of this paper and recommend acceptance.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "Meta review"
    },
    "Reviews": [
        {
            "title": "Motivation and contribution to be narrowed and clarified",
            "review": "The paper aims at generalizing a result by Deshpande and Montanari.\n\nDeshpande and Montanari prove a result for the covariance thresholding algorithm which consists of (1) removing noise from an empirical covariance matrix of a (single) spiked sparse PCA model using soft thresholding, then (2) computing the leading eigenvector of the denoised matrix and finally (3) picking the leading coordinates of the leading eigenvector.\n\nThe current paper focuses on step (1) in the above mentioned process. They claim to generalize the use of soft thesholding to more general functions applied element-wise to the empirical covariance matrix.\n\nThere is a questionable term in the paper's title: the word \"kernel\" is mistakenly used. In fact a symmetric matrix is the Gram matrix of a kernel if it is positive semi-definite. The empirical covariance is PSD, but when you apply a function to it elementwise it has no guarantee of conserving the PSD property.\n\nRegarding the motivation of the paper (1) the paper claims to study the rank K case with arbitrary K>=1. Deshpande and Montanari studied K=1 and the analysis is already an interesting result. Generalizing it to higher ranks poses many questions, including the sparse vector supports' overlaps. \nIt is not clear to me why authors insist on trying to generalize the result to a function f that has broader properties. The main motivation is to recover the support of the (leading) sparse eigenvector / PC. It should not be to try denoising the empirical covariance with a complicated function f. If the focus is on generalizing the soft-thresholding part of the approach, then the real question can be formulated as what is the optimal f given that we have this or that property in the data? This often leads to Bayesian analysis of the problem. \n\nThe numerical experiments do not show any substantial improvement obtained using the prescribed method over using the baseline method (covariance thresholding). I suspect that authors can emphasis the benefit of their method by picking f to hold certain properties that reflect the noise process and beat covariance thresholding in those regimes.\nFigure 1 right hand side. I do not see why authors refer to phase transition. I don't see a phase transition happening there.\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good paper, with some issues in notations",
            "review": "This paper discusses the sparse PCA problem from the random matrix perspective. First, it establishes a theorem that connect a f(sample covariance) to f(actual covariance) in Theorem 1, and shows that when f if three-times continuous differentiable,  f(sample covariance) can be written in terms of actual covariance and $1/n XX^T-I$. Then, based on this theorem, it shows that if f'(0)=f''(0)=0, then f(sample covariance) is well approximated by f(actual covariance). \n\nBased on this result, a procedure for sparse PCA is proposed: first, soft threshold the sample covariance (the thresholding function is described in (9)); second, calculate the top eigenvectors of the thresholded sampled covariance. In simulations, it has similar performance as some popular sparse PCA algorithms.\n\nWhile I think the result of the paper is certainly interesting and worth publication, many notations in the papers are not clear and I can not verify the proofs completely as a result. For example:\n1. Z\\in C E(c,.) in Definition 1---what is the set E(c,.) and is it the same as N(c,.) as implied in definition 1? But it E(c,.) is the same as N(c,.), why they are treated differently in Proposition 2?\n\n\n2.  the subscript {.,i} in the first paragraph Section 4.1 (I guess it means the i-th column).\n\n3. how are f and f^{(k)} defined for matrices in Theorem 1 and what is the supscript {\\odot k}--elementwisely k-th power?\n\n4. The notation O_\\eta in (8).\n\nSome other thought: is the assumption before Theorem 1 reasonable? Can the author(s) add some comments and show that it holds for a reasonable Sigma=I+P: the assumption and Theorem 1 would hold for very small P, but that is not an interesting case. \n\nThe function in (9) is essentially a soft-threshoding procedure. Can the method in this work be used to prove other thresholding procedures such as hard thresholding?",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A kernel matrix-based algorithm for PCA",
            "review": "This paper proposes an algorithm to approximate kernel matrix based on the Taylor expansion of the element-wise functions. The authors provide a spectral norm based error bound for their method and the corresponding results for the special case, \\epsilon-sparse matrix.\n\nI have some comment as follow.\n\n1. Can you provide some comparison with Nystrom methods? It is very popular for kernel approximation and looks more efficient than the proposed algorithm. \n\n2. The analysis relies on the Gaussian assumption on the input matrix. Can we extend it to more general case?\n\n3. In section 5, the paper said “as our method consists in computing the sparse eigenvectors of a p \\times p matrix which can be done by power method, the complexity of estimating the principal component is about O(ps) where s is the sparsity level”. The time complexity of the proposed algorithm is not clearly.\na) Is there any bound for the sparsity level s? Why the eigenvectors of p \\times p matrix is sparse?\nb) The convergence of power method is heavily affected by the eigen-gap of the matrix. Is there any theoretical or empirical result for the convergence behavior of power method on approximate matrix and original matrix?\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}