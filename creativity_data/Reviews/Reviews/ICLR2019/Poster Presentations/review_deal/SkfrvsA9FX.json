{
    "Decision": {
        "metareview": "This work is novel, reasonably clearly written with a thorough literature survey. The proposed approach also empirically seems promising. The paper could be improved with a bit more discussion about the sensitivity, particularly as a two-timescale approach can be more difficult to tune.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Novel approach to constrained policy optimization with room for improvement in understanding the strategy"
    },
    "Reviews": [
        {
            "title": "Interesting paper, with some details not so clear to me",
            "review": "This paper proposed a general framework for policy optimization of constrained MDP. Compared with the traditional methods, such as Lagrangian multiplier methods and trust region approach, this method shows better empirical results and theoretical merits. \n\nMajor concerns: \nMy major concern is about the time-scales. RCPO algorithm, by essence, is multi-scale, which usually has a stringent requirement on the stepsizes and is difficult to tune in practice to obtain the optimal performance. The reviewer would like to see how robust the algorithm is if the multi-time-scale condition is violated, aka, is the algorithm's performance robust to the stepsizes? \n\nMy second concern is the algorithm claims to be the first one to handle mean-value constraint without reward shaping. I did not get the reason why (Dalal et al. 2018) and (Achiam et al., 2017) cannot handle this case. Can the authors explain the reason more clearly? \n\nSome minor points: \nThe experiments are somewhat weak. The author is suggested to compare with more baseline methods. Mujoco domain is not a very difficult domain in general, and the authors are suggested to compare the performance on some other benchmark domains. \n\nThis paper needs to consider the cases where the constraints are the squares of returns, such as the variance. In that case, computing the solution often involves double sampling problem. Double sampling problem is usually solved by adding an extra variable at an extra time scale (if gradient or semi-gradient methods are applied), such as in many risk-sensitive papers.​",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Well written, addresses relevant problem in reinforcement learning, proposes interesting solution ",
            "review": "This work tackles the difficult problem of solving Constrained Markov Decision Processes. It proposes the RCPO algorithm as a way to solving CMDP. The benefits of RCPO is that it can handle general constraints, it is reward agnostic and doesn't require prior knowledge. The key is that RCPO trains the actor and critic using an alternative penalty guiding signal.\n\nPros:\n- the motivations for the work are clearly explained and highly relevant\n- comprehensive overview of related work \n- clear and consistent structure and notations throughout\n- convergence proof is provided under certain assumptions\n\n\nCons:\n- no intuition is given as to why RCPO isn't able to outperform reward shaping in the Walker2d-v2 domain\n\nminor remarks:\n- in Table 2, it would be good if the torque threshold value appeared somewhere \n- in Figure 3, the variable of the x-axis should appear either in the plots or in the legend\n- in appendix B, r_s should be r_step and r_T should be r_goal to stay consistent with notation in 5.1.1",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Important topic but limited experimental validation",
            "review": "The paper introduces RCPO, a model-free deep RL algorithm for learning optimal policies that satisfy some per-state constraint on expectation. The derivation of the algorithm is quite straightforward, starts from the definition of constrained optimization problem, and proceed by forming and optimizing the Lagrangian. Additionally, a value function for the constraint is learned. The algorithm is only compared to a baseline optimizing the Lagrangian directly using Monte-Carlo sampling.\n\nThe paper has two major problems. First, while the derivation of the method makes intuitively sense, it is supported by vaguely stated theorems, which mixes rigorous guarantees with practical approximations. For example, Equation 4 assumes strong duality. How would the result change if weak duality was used instead? The main result in Theorem 1 makes the assumption that dual variable is constant with respect the policy, which might be true in practice, but it is not obvious how the approximation affects the theory. Further, instead of simply referring to prior convergence results, I would strongly suggest including the exact prior theorems and assumptions in the appendix.\n\nThe second problem is the empirical validation, which is incomplete and misleading. Constrained policy optimization is not a new topic (e.g. work by Achiam et al.), so it is important to compare to the prior works. It is stated in the paper that the prior methods cannot be used to handle mean value constraints. However, it would be important to include experiments that can be solved with prior methods too, for example the experiments in Achiam at al. for proper comparison. The results in Table 2 are confusing: what makes the bolded results better than the others? If the criterion is highest return and torque < 25%, then \\lambda=0.1 should be chosen for Hopper-v2. Also, The results seem to have high variance, and judging based on Table 2 and Figure 3, it is not obvious how well RCPO actually works.\n\nTo summarize, while the topic is undoubtedly important, the paper would need be improved in terms of better differentiating the theory from practice, and by including a rigorous comparison to prior work.\n\nMinor points:\n- What is exactly the difference between discounted sum constraint and mean value constraint?\n- Could consider use colors in Table 1.\n- Section 4.1.: What does “... enables training using a finite number of samples” exactly mean in this case?\n- Table 2: The grid for \\lambda is too sparse. \n- Proof of Theorem 1: What does it mean \\theta to be stable?\n- Proof of Theorem 2: “Theorem C” -> “Theorem 1”\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}