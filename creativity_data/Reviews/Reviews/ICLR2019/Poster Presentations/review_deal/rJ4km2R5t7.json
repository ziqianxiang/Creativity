{
    "Decision": {
        "metareview": "This paper provides an interesting benchmark for multitask learning in NLP.\nI wish the dataset included language generation tasks instead of just classification but it's still a step in the right direction.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Multitask learning is one of the most important problems in AI"
    },
    "Reviews": [
        {
            "title": "Interesting new benchmark",
            "review": "Summary:\nGLUE is a benchmark consisting of multiple natural language understanding tasks\nthat functions via uploading to a website and receiving a score based on\nprivately held test set labels.\nTasks include acceptability judgement, sentiment prediction, semantic equivalence\ndetection, judgement of premise hypothesis entailment, question paragraph pair\nmatching, etc..\nThe benchmark also includes a diagnostic dataset with logical tasks such as\nlexical entailment and understanding quantifiers.\n\nIn addition to presenting the benchmark itself, the paper also presents models\nfor performance baselines.\nThere is some brief analysis of the ability of Sentence2Vector vs. more complex\nmodels with e.g. attention mechanisms and of single-task vs. multi-task training.\n\nEvaluation:\nThe GLUE benchmark seems like a well designed benchmark that could potentially\nignite new progress in the area of NLU.\nBut since I'm not an expert in the area of language modeling and know almost\nnothing about existing benchmarks I cannot validate the added benefit over\nexisting benchmarks and the novelty of the suggested benchmarking approach.\n\nDetails:\nThe paper is well written, clear and easy to follow.\n\nThe proposed benchmarks seem reasonable and illustrate the difficulty of\nbenchmark tasks that involve logical structure.\n\nPage 5: showing showing (Typo)\n",
            "rating": "7: Good paper, accept",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "Weak reject",
            "review": "The paper proposes a new benchmark for natural language understanding: GLUE. Models will be evaluated based on a diverse set of existing language understanding tasks which encourages the models to learn shared knowledge across different tasks. The authors empirically show that models trained with multiple tasks in the dataset perform better than models that focused on one specific task. They also point out existing methods are not able achieve good performance in this dataset and request for more general natural language understanding system. The work also collects an expert evaluated diagnostic evaluation dataset for further examination for the models.\n\nQuality: borderline, clarity:good, originality: borderline, significance: good,\n\nPros:\n- The benchmark is set up in a online platform with leaderboard which can be easily accessible to people.\n- The benchmark comes with a diagnostic evaluation dataset with coarse-grained and fine-grained categories that examine different aspect of language understanding abilities.\n- Baseline results for major existing models are provided\n\nCons:\n- The author should provide more detailed analysis and interpretable explanations for the results as opposed to simply stating that the overall performance is better.\nFor example, why attention hurts performance in single task training? Why multi-tasks training actually leads to worse performance on some of the dataset? Do these phenomenons still exist if you train on a different subset of the dataset?\nWhat are the samples that the models failed to perform well? It would be nice to get some more insights and conclusions based on the results obtained from this benchmark to shed some lights on how to improve these models. The results section should be seriously revised.\n\n- The diagnostic evaluation dataset seems to be a way to better understand the model, however, it is hard to see the scope of the data (are the samples under each categories balanced?). Besides, the examples in the dataset seems very confusing even for humans (Table 2).  The evaluation with NLP expert is also far from perfect. I wonder how accurate is this dataset annotated (or even the sentences make sense or not), and how suitable it is for evaluating modelâ€™s language understanding abilities. It would be nice if the authors can include some statistics about the dataset.\n\nThe paper proposes a useful benchmark that measures different aspects of language understanding abilities which would be helpful to the community. However, I feel the novelty or take away messages from the experiment section is limited. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A timely and useful resource",
            "review": "This paper introduces the General Language Understanding Evaluation (GLUE) benchmark and platform, which aims to evaluate representations of language with an emphasis on generalizability. This is a timely contribution and GLUE will be an impactful resource for the NLP community. This is mitigated, perhaps, somewhat by the recent release of decaNLP. But, as discussed the authors, this has a different focus (re-framing all tasks as QQ) and further does not feature the practical tools released here (leaderboard, error analysis) that will help drive progress.\n\nSome comments below. \n\n- The inclusion of the small diagnostic dataset was a nice addition and it would be nice if future corpora included similar. \n\n- Implicit in this and related efforts is the assumption that parameter sharing ought to be possible and fruitful across even quite diverse tasks. While I do not object to this, it would be nice if the authors could make an explicit case here as to why should we believe this to be the case.\n\n- The proposed platform is touted as one of the main contributions here, but not pointed to -- I assume for anonymity preserving reasons, but still would have been nice for this to be made explicit. \n\n- I would consider pushing Table 5 (Appendix) into the main text. \n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}