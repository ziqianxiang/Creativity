{
    "Decision": {
        "metareview": "The paper conveys interesting ideas but reviewers are concern about an incremental nature of results, choice of comparators, and in general empirical and analytical novelty.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Metareview"
    },
    "Reviews": [
        {
            "title": "A good problem discussed and the proposed ML approach seems reasonable.",
            "review": "The authors are proposing an end-to-end learning-based framework that can be incorporated into all classical frequency estimation algorithms in order to learn the underlying nature of the data in terms of the frequency in data streaming settings and which does not require labeling. According to my understanding, the other classical streaming algorithms also do not require labeling but the novelty here I guess lie in learning the oracle (HH) which feels like a logical thing to do as such learning using neural networks worked well for many other problems.\n\nThe problem formulation and applications of this research are well explained and the paper is well written for readers to understand. The experiments show that the learning based approach performs better than their all unlearned versions. \n\nBut the only negative aspect is the basis competitor algorithms are very simple in nature without any form of learning and that are very old. So, I am not sure if there are any new machine learning based frequency estimation algorithms.  \n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Unclear problem setting",
            "review": "Quality/clarity:\n- The problem setting description is neither formal nor intuitive which made it very hard for me to understand exactly the problem you are trying to solve. Starting with S and i: I guess S and i are both simply varying-length sequences in U.\n- In general the intro should focus more on an intuitive (and/or formal) explanation of the problem setting, with some equations that explain the problem you want to work on. Right now it is too heavy on 'related work' (this is just my opinion).\n\nOriginality/Significance:\nI have certainly never seen a ML-based paper on this topic. The idea of 'learning' prior information about the heavy hitters seems original.\n\nPros:\nIt seems like a creative and interesting place to use machine learning. the plots in Figure 5.2 seem promising.\n\nCons:\n- The formalization in Paragraph 3 of the Intro is not very formal. I guess S and i are both simply varying-length sequences in U.\n- In general the intro should focus more on an intuitive (and/or formal) explanation of the problem setting, with some equations that explain the problem you want to work on. Right now it is too heavy on 'related work' (this is just my opinion).\n\n-In describing Eqn 3 there are some weird remarks, e.g. \"N is the sum of all frequencies\". Do you mean that N is the total number of available frequencies? i.e. should it be |D|? It's not clear to me that the sum of frequencies would be bounded if D is not discrete.\n- Your F and \\tilde{f} are introduced as infinite series. Maybe they should be {f1, f2,..., fN}, i.e. N queries, each of which you are trying to be estimate.\n- In general, you have to introduce the notation much more carefully. Your audience should not be expected to be experts in hashing for this venue!! 'C[1,...,B]' is informal abusive notation. You should clearly state using both mathematical notation AND using sentences what each symbol means. My understanding is that that h:U->b, is a function from universe U to natural number b, where b is an element from the discrete set {1,...,B}, to be used as an index for vector C. The algorithm maintains this vector C\\in N^B (ie C is a B-length vector of natural numbers). In other words, h is mapping a varying-length sequence from U to an *index* of the vector C (a.k.a: a bin). Thus C[b] denotes the b-th element/bin of C, and C[h(i)] denotes the h(i)-th element. \n- Still it is unclear where 'fj' comes from. You need to state in words eg \"C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \\in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum.\"\n- What I don't understand is how fj is dependent on h. When you say \"at the end of the stream\", you mean that given S, we are analyzing the frequency of a series of sequences {i_1,...,i_N}?\n- Sorry, it's just confusing and I didn't really understand \"Single Hash Function\" from Sec 3.2 until I started typing this out.\n- The term \"sketch\" is used in Algorithm1, like 10, before 'sketch' is defined!!\n-I'm not going to trudge through the proofs, because I don't think this is self-contained (and I'm clearly not an expert in the area).\n\nConclusion:\nHonestly, this paper is very difficult to follow. However to sum up the idea: you want to use deep learning techniques to learn some prior on the hash-estimation problem, in the form of a heavy-hitter oracle. It seems interesting and shows promising results, but the presentation has to be cleaned up for publication in a top ML venue.\n\n\n\n******\nUpdate after response:\nThe authors have provided improvements to the introduction of the problem setting, satisfying most of my complaints from before. I am raising my score accordingly, since the paper does present some novel results.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "Interesting topic, somewhat trivial algorithms and somewhat narrow results",
            "review": "This paper introduces the study of the problem of frequency estimation algorithms with machine learning advice. The problem considered is the standard frequency estimation problem in data streams where the goal is to estimate the frequency of the i-th item up to an additive error, i.e. the |\\tilde f_i - f_i| should be minimized where \\tilde f_i is the estimate of the true frequency f_i.\n\nPros:\n-- Interesting topic of using machine learned advice to speed up frequency estimation is considered\n-- New rigorous bounds are given on the complexity of frequency estimation under Zipfian distribution using machine learned advice\n-- Experiments are given to justify claimed improvements in performance\n\nCons:\n\n-- While the overall claim of the paper in the introduction seems to be to speed up frequency estimation using machine learned advice, results are only given for the Zipfian distribution.\n\n-- The overall error model in this paper, which is borrowed from Roy et al. is quite restrictive as at it assumes that the queries to the frequency estimation data structure are coming from the same distribution as that given by f_i’s themselves. While in some applications this might be natural, this is certainly very restrictive in situations where f_i’s are updated not just by +/-1 increments but through arbitrary +/-Delta updates, as in this case it might be more natural to assume that the distribution of the queries might be proportional to the frequency that the corresponding coordinate is being updated, for example.\n\n-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.\n\n-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher’18.\n\n-- The analysis is relatively straightforward and boils down to bucketing the error and integration over the buckets.\n\n\nOther comments:\n-- The machine learned advice is assumed to be flawless at identifying the Heavy Hitters, authors might want to consider incorporating errors in the analysis.\n\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}