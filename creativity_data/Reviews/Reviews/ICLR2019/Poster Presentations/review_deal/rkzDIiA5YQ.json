{
    "Decision": {
        "metareview": "The reviewers that provided extensive reviews agree that the paper is well-written and contains solid technical material. The paper however should be edited to address specific concerns regarding theoretical and empirical aspects of this work. ",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "Good paper"
    },
    "Reviews": [
        {
            "title": "The contribution is not clear and significant for accelerating distributed algorithms",
            "review": "Overall, this paper is well written with clearly presentation.\nHowever, the contribution is not good enough to research the ICLR requirement.\nAlthough the authors propose some method to balance the computation between distributed workers, which should be important for distributed optimization algorithm design, but not enough numerical experiments are proposed to prove the efficiency.\nEven though some convergence analysis is given.\nThe main concern of this paper is to significantly increase the algorithm efficiency, but both theoretical and numerical results are lack of strength.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Useful approach to mitigate stragglers in distributed computing setups",
            "review": "This paper studies distributed optimization in the presence of straggling computing nodes. In a synchronous distributed optimization approach, the stragglers delay the entire computation as the synchronization operation cannot be performed till every computing nod has completed its task. This paper aims to mitigate the effect of stragglers by proposing Anytime MiniBatch (AMB) approach, where each computing node is allowed to process the different number of samples between two synchronization steps. In particular, each node is given $T$ unit time to process as many samples as it can. After that, the nodes are allowed to aggregate the information among themselves through a consensus mechanism for another $T_c$ unit time. In contrast with this, the usual Fixed MiniBatch (FMB) approach requires each node to process a fixed number of samples before invoking aggregating step. The presence of stragglers can significantly increase the time between two synchronization step and slow down the overall optimization process. \n\nThis paper combines their AMB approach with the dual averaging method. The paper presents sample-path wise regret bounds for convex optimization under additional standard assumptions (e.g., Lipschitz continuousness, smoothness). The paper then compares analytically and experimentally compare the speed-ups obtained by their AMB approach as compared to the FMB approach. The paper studies an interesting problem and proposes a simple and practical solution. The paper is well written and makes novel contributions with sound analysis. The experimental evaluation on the real system also corroborates the theoretical findings.\n\nComments/questions: \n\nThe reviewer did not find the justification of using $c_i(t)$ in the definition of regret (cf. (17)) very clear. Is it because we also want compare with a centralized setting which does not have the communication overhead? As far as evaluation between distributed schemes (e.g., AMB, FMB etc) is concerned, shouldn't one define the regret with respect to $b_i(t)$s itself?\n\nCan the authors comment on the setup where the communication links are also unpredictable and may experience congestion? In this case, one would encounter variable communication overhead to achieve the consensus error up to $epsilon$.\n\nIn Sec. 4, could authors comment on the settings where $O(sqrt(n-1))$ speed up is achievable?\n\nMinor typos: In eq. (127)  E[S] -> S_F, S_T -> S_A. In eq. (128)  S_T -> S_A.\n\n\n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper with a nice idea",
            "review": "Summary:\nThe paper considers the problem of online stochastic convex optimization in a fully distributed topology. In particular, the authors focus on the synchronous setting and to avoid the slow progress that can be obtained by slow nodes, called stragglers, they propose an online distributed optimization method called Anytime Minibatch (AMB). In the update of AMB rather than fixing the minibatch size, they fix the computation time in each epoch. This characteristic prevents the stragglers from holding up the entire network, while allowing nodes to benefit from the partial work carried out by the slower nodes. \n\nA convergence analysis of AMB is provided showing that the online regret achieves the optimum performance. Numerical evaluations where a comparison of AMB and the \"Fixed MiniBatch\" method (FMB)are also presented.\n\nComments:\nI believe that the idea of the paper is interesting and the convergence analysis seems correct, however i have some concerns regarding  the presentation and the numerical evaluation. \n\n1) In the title the word \"online\" is mentioned but never explained  in the main text. What is this mean? What are the differences compare to the \"static\" setting? See for example the work of [Tsianos, Rabbat (2016)] for more details on that. What are the related literature on this setting?\n\n2) In the last paragraph of Introduction is highlighted that the algorithm AMB has the optimum performance?  The authors should add an appropriate reference there and explain why this is optimum for their setting. I believe that for the convenience of the reader current Section 5 called \"previous work\" can move immediately after introduction and more details of AMB with the existing literature should be provided. Probably rename the section \"Closely relate work\".\n\n3) Section 2 is devoted mostly on the formal presentation of algorithm AMB. I strongly suggest the addition of a pseudocode of the algorithm in the appendix (or even in the main text if there is a space) where the reader can easily understand how the algorithm works.\n\n4) On the Algorithm:  if some nodes are very slow and they do not make any update during the given time T what will happen? How this will affect the performance of the method? In this case does it make sense to increase the value of T.\n\n5) On numerical evaluation:  A comparison of AMB and FMB  is presented both in synthetic and real data showing that AMB can be faster than FMB in terms of wall clock time. \nI am not sure if the performance of the AMB is as good as one should expect especially for the case of synthetic data. Will it be possible to construct a synthetic example with extremely slow nodes where the improvement of the performance is much better than 50%?\n\nIn general i find the paper interesting, with nice ideas and I believe that will be appreciated from researchers that are interested on control theory/signal processing and information theory.  Since the paper is focused on convex optimization I am not sure if it will be particularly interesting for a substantial fraction of the ICLR attendees.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}