{
    "Decision": {
        "metareview": "The paper presents an interesting idea for increasing the robustness of adversarial defenses by combining with existing domain adaptation approaches. All reviewers agree that the paper is well written and clearly articulates the approach and contribution.\n\nThe main areas of weakness is that the experiments focus on small datasets, namely CiFAR and MNIST.  That being said, the algorithm is reasonably ablated on the data explored and the authors provided valuable new experimental evidence during the rebuttal phase and in response to the public comment. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Using domain adaptation for robust adversarial learning"
    },
    "Reviews": [
        {
            "title": "Good idea and experimental evidence but lacking in rigor and more empirical analysis",
            "review": "The paper casts the problem of learning from adversarial examples to make models resistant to adversarial perturbations to a domain adaptation problem. The proposed method Adversarial training with Domain adapatation( ATDA) learns a representation that is invariant to clean and adversarial data achieving state of the art results on CIFAR. \n\nquality - Paper is well written, explanation of the mathematical parts are good, experimental quality can be much better.\nclarity - the problem motivation as well as the methodology is clearly explained. the learning from the experiments are unclear and need more work.\noriginality - The casting of the problem as domain adaptation is original but from the experiments it was not conclusive as to how much benefit we get. \nsignificance of this work -  Current models being sensitive to adversarial perturbations is quite a big problem so the particular problem authors are trying to address is very significant.\n\npros\n\nA good idea, enough experiments that indicate the benefit of casting this as a domain adaptation problem.\n\ncons \n\nI feel, the authors should have extended the experiments to ImageNet which is a much larger dataset and validate the findings still hold, I feel the discussion section and comparison to other methods needs to be worked to be more thorough and to tease out the benefit of each of the various terms added to the loss functions as currently all we have is final numbers without much explanation and details. TSNE embeddings part is also very qualitative and while the plots indicate a better separation for ATDA, I feel authors should do more quantitative analysis on the embeddings instead of just qualitative plots.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A new adversarial training with domain adaptation demonstrating fair performance improvement",
            "review": "Authors propose a new adversarial training with domain adaptation method to overcome the weak generalisation problem in adversarial training for adversarial examples from different attacks. Authors consider the adversarial training as a domain adaptation task with limited number of target labeled data. They demonstrate that by combining unsupervised and supervised domain adaptation with adversarial training, the generalisation ability on adversarial examples from various attacks can be improved for efficient defence. The experimental results on several benchmark datasets suggest that\nthe proposed approach achieves significantly better generalisation results in most cases, when compared to current\ncompeting adversarial training methods. Paper is clearly written and well structured. The novelty of the proposed technique is fair and the originality alike. The results are not very conclusive therefore I think more experiments are needed and possible further adjustments.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper",
            "review": "This paper addresses the generalization of adversarial training by proposing a new domain adaptation method. In order to have robust defense for adversarial examples, they combine supervised and unsupervised learning for domain adaptation. The idea of domain adaptation is to increase the similarity between clear and adversarial examples. For this purpose, in their objective, they are minimizing the domain shift by aligning the covariance matrix and mean vector of the clean and adversarial examples.\n\nFrom experimental viewpoint, they have lower performance than almost all competitors on clean data, but they are beating them when there is white-box as well as the back-box threats. This means their method gives a good generalization. In CIFAR-100 they do not have this trade-off for accuracy and generalization; they are beating other competitors in clean data as well.\n\nThe paper is clear and well-written. The introduction and background give useful information. \n\nIn general, I think the paper has a potential for acceptance, but I have to mention that I am not an expert in Adversarial networks area.\n\n ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}