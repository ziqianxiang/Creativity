{
    "Decision": {
        "metareview": "This paper provides a mean-field-theory analysis of batch normalization. First there is a negative result as to the necessity of gradient explosion when using batch normalization in a fully connected network. They then provide further insights as to what can be done about this, along with experiments to confirm their theoretical predictions.\n\nThe reviewers (and random commenters) found this paper very interesting. The reviewers were unanimous in their vote to accept.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Interesting and surprising findings with a mean-field-theory analysis of batch normalization"
    },
    "Reviews": [
        {
            "title": "Interesting and counter-intuitive results about batch-normalization",
            "review": "This paper develops a mean field theory for batch normalization (BN) in fully-connected networks with randomly initialized weights. There are a number of interesting predictions made in this paper on the basis of this analysis. The main technical results of the paper are Theorems 5-8 which compute the statistics of the covariance of the activations and the gradients.\n\nComments:\n\n1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?\n\n2. In a similar vein, there a number of highly technical results in the paper and it would be great if the authors provide an intuitive explanation of their theorems.\n\n3. Can the statistics of activations be controlled using activation functions or operations which break the symmetry? For instance, are BSB1 fixed points good for training neural networks?\n\n4. Mean field analysis, although it lends an insight into the statistics of the activations, needs to connected with empirical observations. For instance, when the authors observe that the structure of the fixed point is such that activations are of identical norm equally spread apart in terms of angle, this is quite far from practice. It would be good to mention this in the introduction or the conclusions.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The detailed analysis of the training of DNN with the batch normalization is quite interesting. ",
            "review": "\nThis paper investigates the effect of the batch normalization in DNN learning.\nThe mean field theory in statistical mechanics was employed to analyze the\nprogress of variance matrices between layers. \nAs the results, the batch normalization itself is found to be the cause of gradient explosion. \nMoreover, the authors pointed out that near-linear activation function can improve such gradient explosion. \nSome numerical studies were reported to confirm theoretical findings.\n\nThe detailed analysis of the training of DNN with the batch normalization is quite interesting. \nThere are some minor comments below.\n\n- in page 3, 2line above eq(2): what is delta in the variance of the multivariate normal distribution?\n- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3. \n- The randomized weight is not very practical. Though it may be the standard approach of mean field,\nsome comments would be helpful to the readers. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "Interesting work, strong results",
            "review": "This paper provides a new dynamic perspective on deep neural network. Based on Gaussian weights and biases, the paper investigates the evolution of the covariance matrix along with the layers. Eventually the matrices achieve a stationary point, i.e., fixed point of the dynamic system. Local performance around the fixed point is explored. Extensions are provided to include the batch normalization. I believe this paper may stimulate some interesting ideas for other researchers.\n\nTwo technical questions:\n\n1. When the layers tends to infinity, the covariance matrix reaches stationary (fixed) point. How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough? This somewhat conflicts the commonsense of \"the deeper the better?\" \n\n2. Typos: the weight matrix in the end of page 2 should be N_l times N_{l-1}. Also, the x_i's in the first line of page 3 should be bold.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}