{
    "Decision": {
        "metareview": "This is an interesting paper that develops new techniques for analyzing the loss surface of deep networks, allowing the existence of spurious local minima to be established under fairly general conditions.  The reviewers responded with uniformly positive opinions.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Interesting investigation into the existence of spurious local minima in nonlinear networks"
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "The authors present some theoretical results on the loss surface of neural networks. Their main results are:\n\n(1) They consider a 1 layer hidden neural network where the single nonlinearity is ReLU / ReLU-like. Here they prove that as long as a linear model cannot fit the data, then there exits a local minimum strictly inferior to the global one (They can then scale the parameters to get infinitely many local optima).\n\nThe key idea is to construct a local minima whose risk value is the same as the local least squares solution. Then to construct a set of parameters that has smaller risk value than this local optima. The proof technique is interesting.\n\n(2) They construct a particular dataset for which a one hidden layer neural net with other nonlinear activations (sigmoid, tanh, etc.) also has local optima.\n\nI think this theorem is a bit less interesting since the dataset given has only two data points. I think it is less interesting to prove suboptimality of neural nets in small sample size settings. \n\n(3) Global optimailty of linear networks. The authors show that deep linear networks (i.e. y = W1 W2 W3...W5 x) have only global minima or saddle points.\n\nI'm not familiar enough with the field to know the significant of this result. The deep linear network  just seems like an artificial construction (i.e. in practice one would simply condense W1...W5 to one W) to study nonconvexity / local optima, no one would use it in practice.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review of \"Small nonlinearities in activation functions create bad local minima in neural networks\"",
            "review": "Paper represents theoretical analysis of the loss surface of neural networks. The authors supplied interesting results about local minima properties of neural networks. The paper is written quite well and easy to follow. Furthermore, authors made a comprehensive literature survey and connected their paper to already existing literature. The proofs seem correct (please note that paper is quite long and it requires more time then conference review period, hence, I stated “seem correct”). \nAlthough, paper provides novel theorems, I have several concerns, and these are:\n•\tIsn’t it clear that (generally speaking) a non-convex problem will have many local minima? Previous paper in neural network community is (in my opinion) not theorems. I believe one should read those statements such that in practice (please note here practice means that architectures, e.g. resnet50, inceptionv3, used in day to day life) researchers don’t not observe those “really bad” local minima.\n•\tIn ML literature, we have convex machines which are guaranteed to converge to global optimum. Given image dataset, or text datasets supervised deep learning in in general much better than convex methods e.g. SVMs. The paper clearly shows that there exist, in some cases, exponentially many local minima however  current training methods are able to find better solutions than convex methods (I am completely aware that functions classes are different however success metric, accuracy, is the same). Hence, how relevant are the results without taking in to account architectural choices or optimization methods for deep learning? May be structural risk minimisation is a better approach than empirical risk minimization for quantifying the performance of deep neural networks,\nIn conclusion, paper is interesting however I believe it need to be improved. \n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "elegant construction; interesting phenomenon",
            "review": "\nThe authors provide a clean and easily understood sufficient\ncondition for spurious local minima to exist in networks with\na hidden layer using ReLUs or leaky ReLUs.  This condition,\nthat there is not linear transformation with zero loss,\nis satisfied for almost all inputs with more examples than\ninput variables.\n\nThe construction is elegant.  The mathematical writing in the paper,\nespecially describing the proof of Theorem 1, is very nice -- they\nexpose the main ideas effectively.\n\nI do not know of another paper using a similar proof, but I have not\nstudied the proofs of the most closely related papers prior to doing\nthis review, so I have limited ability to vouch for this paper's\ntechnical novelty.\n\nThe authors also show that networks using many other popular\nactivation functions have spurious local minima for a very\nsimple dataset.  All of these analysis are unified using a\nsimple, if technical, set of conditions on activation function.\n\nFinally, the authors prove a somewhat technical theorem about\noptima in deep linear networks, which generalizes some\nearlier treatments of this topic, providing an checkable\ncondition for global minimality.\n\nThere is extensive discussion of related work.  I am not aware of\nrelated work not covered by the authors.\n\nIn some cases, when the authors discuss previous work, they write as\nif restriction to the realizable case is an assumption, when it seems\nto me to be more of a constraint.  In other words, it seems harder to\nprove the existence of spurious minima in the realizable case.\nThey seem to acknowledge this after their statement of their Theorem 2,\nwhich also uses a realizable dataset.\n\nAlso, a few papers, including the Venturi, et al paper cited by\nthe authors, have analyzed whether spurious local minima exist\nin subsets of the parameter space, including those likely to\nbe reached during training with different sorts of initializations.\nIn light of this work, the authors might want to tone down claims\nabout how their work shows that results about linear networks do\nnot generalize to the non-linear case.  In particular, to make\ntheir construction work in the case of wide networks, they\nneed an overwhelming majority of the hidden units to be \"dead\",\nwhich seems as it is unlikely to arise from training with\ncommonly used initializations.\n\nOverall, I think that this paper makes an interesting and\nnon-obvious contribution on a hot topic.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}