{
    "Decision": {
        "metareview": "This paper offers a novel perspective for learning latent multimodal representations. The idea of segmenting the information into multimodal discriminative and modality-specific generating factors is found to be intriguing by all reviewers and the AC. The technical derivations allow for an efficient implementation of this idea.\n\nThere have been some concerns regarding the experimental section, but they have all been addressed adequately during the rebuttal period. Therefore the AC suggests this paper for acceptance. It is an overall nice and well-thought work. \n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Novel perspective for learning latent multimodal representations"
    },
    "Reviews": [
        {
            "title": "Multimodal Joint Generative Discriminative Factorization for disentangled representations with good performance and practical application (noise robustness)",
            "review": "This paper presents 'Multimodal Factorization model' that factorizes representations into shared multimodal discriminative factors and modality specific generative factors. This work applies 'Wassertein Auto-Encoders' by Tolstikhin et al (with proofs that this setup works in the multimodal case) for handling factorized joint distributions over the multimodal space. Can this method be considered as a generalization of the wasserstein autoencoder based method with a broader application? - the authors should discuss this more broadly in the paper.\n\nPros:\n- There has been many recent work in the area of disentangling joint representations for improving generative auto-encoding architectures using VAEs, GANs, WAE and some variants of these. This work falls in this category with many interesting experiments showing SOTA generation and discrimination results on several tasks.\n- This work is practical due to its robustness to noisy and/or missing data for one or more of the modalities in a multimodal machine learning classification (or generation) problem. Application of this technique for continuous multimodal time series data modeling and prediction for high accuracy requirement applications is very promising.\n- The methods seems to be easily portable to other tasks. The authors say that they will make the code available to other researchers.\n\nCons:\n- Some more comparison to other disentangling approaches such as beta-VAE, InfoGAN and partitioned VAE methods would have been useful for understanding the advantages and disadvantages of this techniques. (The authors do add a note about comparison with partitioned VAE method in the Appendix)\n- For generation and classification tasks, the authors have chosen the tasks for digit recognition and sentiment analysis - I wonder if the results would hold for other types of multimodal tasks.\n\nOverall the paper is very well-written with many experiments to support the claims.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good work",
            "review": "Multimodality learning is an important topic in multimedia and human computer interaction.  How to efficiently leverage the additional information cross multimodality is the key to the task. Authors proposed the Bayesian latent variable model to factorize the multimodality representation into multimodal discriminative factors and modality-specific generating factors, which is interesting. Approximate inference is also proposed to learn this model via a generalised mean-field assumption. \n\nThe technical quality of the paper is sound and significant, The problem to solve in this paper is also well motivated and important.  In general, this is a well-written paper, \n\nI have a few minor questions which requires authors for further elaboration.\n1. If I understand it correctly, in the current work, the feature Xs are continuous. Does the approach apply to categorical or binary features?\n\n2. In equation(4), MMD is used. How to solve the computation complexity problem since the complexity of MMD is O(n^2)?  It is true that the batch size should be small?  How to select the hyper-parameters of kernels?\n\n ",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Nice Work",
            "review": "The authors splitted the features of multimodal representations to \"common\" (multimodal discriminative) and \"specific\" (modality-specific generative) factors. In this framework, their MFM can capture more detailed features. \n\nPros:\n(*) Learning the feature representations from two perspectives. \n\n(*) Even missing one modality, MFM can still achieve acceptable performance. \n\n(*) Using mutual information and gradient-based method to interpret their method. \n\nCons:\n(*) The work has some similarity to Hsu & Glass (2018), but the comparison between this work is only on CMU-MOSI.\n\n(*) In Table. 3, it shows that language is the most informative feature for prediction. However, in Table. 2, it can be seen that if audio is missing, the result it the worse compared to the other two cases. It seems the interpretation is not convincing to me. Can you give us more explanation about this phenomenon? \n\nComments:\n(*) The details of SVHN-MNIST experiment are missing. Appendix B gave some information about models but specified the targeted datasets.\n\n(*) The appendix is not clear, e.g. In Appendix B, it is said \"subsection 3.3\" but there is no section 3.3.  \n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}