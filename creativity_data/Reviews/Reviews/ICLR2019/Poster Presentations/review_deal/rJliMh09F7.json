{
    "Decision": {
        "metareview": "The paper proposes a regularization term on the generator's gradient that increases sensitivity of the generator to the input noise variable in conditional and unconditional Generative Adversarial networks, and results in multimodal predictions. All reviewers agree that this is a simple and useful addition to current GANs. Experiments that demonstrate the trade off between diversity and generation quality would be important to include, as well as the experiment on using the proposed method on unconditional GANs, which was conducted during the discussion period. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "a simple regularization for preventing mode collapse"
    },
    "Reviews": [
        {
            "title": "An interesting and simple idea.",
            "review": "The paper proposes a regularization term for the conditional GAN objective in order to promote diverse multimodal generation and prevent mode collapse. The regularization maximizes a lower bound on the average gradient norm of the generator network as a function of the noise variable.\n\nThe regularization is a simple addition to existing conditional GAN models and is certainly simpler than the architectural modifications and optimization tweaks proposed in recent work (BicycleGAN, etc). It is useful to a such a simple solution for preventing mode collapse as well as promoting diversity in generation.\n\nIt is shown to promote the generator landscape to be more spread out by lower bounding the expected average gradient norm under the noise distribution. This is a point to be noted when comparing with other work which focus on the vanishing gradients through the discriminator and try to tweak the discriminator gradients. It is a surprising result that such a penalty on the lower bound can prevent mode collapse while also promoting diversity, since I would expect that upper bounding the generator gradient (i.e. lipschitz continuity which wasserstein GANs and related work rely on but for their discriminator instead) makes sense if a smooth interpolation in latent space is desired. \n\nIt is also not evident how the vanishing discriminator gradient problem is solved using this regularization -- will it work if the discriminator is allowed to converge before updating the generator?\n\nThis simple regularization presented in this paper and its connection to preventing mode collapse feels like an important step towards understanding how conditional generative models like cGANs behave. Alleviating the need to investigate significant changes to model families by focusing instead on a novel optimization objective is an important contribution.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Well written paper with a simple idea for preventing mode-collapse in GANs but with insufficiently experimental validation",
            "review": "The paper proposes a simple way of addressing the issue of mode-collapse by adding a regularisation to force the outputs to be diverse. Specifically, a loss is added that maximises the l2 loss between the images generated, normalised by the distance between the corresponding latent codes. This method is also used to control the balance between visual quality and diversity.\n\nThe paper is overall well written, introducing and referencing well existing concepts, and respected the 8 pages recommendation.\n\nWhy was the maximum theta (the bound for numerical stability) incorporated in equation 2? What happens if this is omitted in practice? How is this determined?\n\nIn section 4, an increase of the gradient norm of the generator is implied: does this have any effect on the robustness/sensitivity of the model to adversarial attacks?\n\nIn section 5, how is the “appropriate CGAN” determined?\n\nMy main issue is with the experimental setting that is somewhat lacking. The visual quality of the samples illustrated in the paper is inferior to that observed in the state-of-the-art, begging the question of whether this is a tradeoff necessary to obtain better diversity or if it is a consequence of the additional regularisation.. The diversity observed seems to mainly be attributable to colour differences rather than more elaborate differences. Even quantitatively, the proposed method seems only marginally better than other methods.\n\nUpdate post rebuttal\n-----------------------------\nThe experimental setting that is a little lacking. Qualitatively and quantitatively, the improvements seem marginal, with no significant improvement shown. I would have liked a better study of the tradeoff between visual quality and diversity, if necessary at all.\n\nHowever, the authors addressed well the issues. Overall, the idea is interesting and simple and, while the paper could be improved with some more work, it would benefit the ICLR readership in its current form, so I would recommend it as a poster -- I am increasing my score to that effect.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea with good experimental validation",
            "review": "The paper proposes a method for generating diverse outputs for various conditional GAN frameworks including image-to-image translation, image-inpainting, and video prediction. The idea is quite simple, simply adding a regularization term so that the output images are sensitive to the input variable that controls the variation of the images. (Note that the variable is not the conditional input to the network.) The paper also shows how the regularization term is related to the gradient penalty term. The most exciting feature about the work is that it can be applied to various conditional synthesis frameworks for various tasks. The paper includes several experiments with comparison to the state-of-the-art. The achieved performance is satisfactory. \n\nTo the authors, wondering if the framework is applicable to unconditional GANs.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}