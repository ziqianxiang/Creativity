{
    "Decision": {
        "metareview": "Pros:\n- novel idea of endowing RL agents with recursive reasoning\n- clear, well presented paper\n- thorough rebuttal and revision with new results\n\nCons:\n- small-scale experiments\n\nThe reviewers agree that the paper should be accepted.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Meta-review"
    },
    "Reviews": [
        {
            "title": "A good first step towards endowing deep reinforcement learning agents with recursive reasoning capabilities",
            "review": "The high-level problem this paper tackles is that of endowing RL agents with recursive reasoning capabilities in a multi-agent setting, based on the hypothesis that recursive reasoning is beneficial for the agents to converge to non-trivial equilibria.\n\nThe authors propose the probabilistic recursive reasoning (PR2) framework for an n-agent stochastic game. The conceptual difference between PR2 and non-correlated factorizations of the joint policy is that, from the perspective agent i, PR2 augments the joint policy of all agents by conditioning the policies of agent i's opponents on the action that agent i took. The authors derive the policy gradient for PR2 and show that it is possible to learn these action-conditional opponent policies via variational inference in addition to learning the policy and critic for agent i.\n\nThe proposed method is evaluated on two experiments: one is an iterated matrix game and the other is a differential game (\"Max of Two Quadratics\"). The authors show in the iterated matrix game that baselines with non-correlated factorization rotate around the equilibrium point, whereas PR2 converges to it. They also show in the differential game that PR2 discovers the global optimum whereas baselines with non-correlated factorizations do not.\n\nThis paper is clear, well-motivated, and well-written. I enjoyed reading it. I appreciated the connection to probabilistic reinforcement learning as a means for formulating the problem of optimizing the variational distribution for the action-conditional opponent policy and for making such an optimization practical. I also appreciated the illustrative choice of experiments that show the benefit of recursive reasoning. \n\nCurrently, PR2 provides a proof-of-concept of recursive reasoning in a multi-agent system where the true equilibrium is already known in closed form; it remains to be seen to what extent PR2 is applicable to multi-agent scenarios where the equilibrium the system is optimizing is less clear (e.g. GANs for image generation). Overall, although the experiments are still small scale, I believe this paper should be accepted as a first step towards endowing deep reinforcement learning agents with recursive reasoning capabilities.\n\nBelow are several comments.\n\n1. Discussion of limitations: As the authors noted in the Introduction and Related Work, multi-agent reinforcement problems that attempt to model opponents' beliefs often become both expensive and impractical as the number of opponents (N) and the recursion depth (k) grows because such complexity requires high precision in the approximate the optimal policy. The paper can be made stronger with experiments that illustrate to what extent PR2 practically scales to problems with N > 2 or K > 1 in terms of how practical it is to train.\n2. Experiment request: To what extent do the approximation errors affect PR2's performance? It would be elucidating for the authors to include an experiment that illustrates where PR2 breaks down (for example, perhaps in higher-dimensional problems).\n3. Minor clarification suggestion: In Figure 1: it would be clearer to replace \"Angle\" with \"Perspective.\"\n4. Minor clarification suggestion: It would be clearer to connect line 18 of Algorithm 1 to equation 29 on Appendix C.\n5. Minor clarification suggestion: In section 4.5: \"Despite the added complexity\" --> \"In addition to the added complexity.\"\n6. Minor clarification: How are the importance weights in equation 7 reflected in Algorithm 1?\n7. Minor clarification: In equation 8, what is the significance of integrating over time rather than summing?\n8. Minor clarification: There seems to be a contradiction in section 5.2 on page 9. \"the learning outcomes of PR2-AC and MASQL are extremely sensitive to the way of annealing...However, our method does not need to tune the the annealing parameter at all...\" Does \"our method\" refer to PR2-AC here?",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting and sound ideas and algorithms, but experimental validation is weak",
            "review": "\n# Summary:\nThe paper proposes a new approach for fully decentralized training in multi-agent reinforcement learning, termed probabilistic recursive reasoning (PR2). The key idea is to build agent policies that take into account opponent best responses to each of the agent's potential actions, in a probabilistic sense. The authors show that such policies can be seen as recursive reasoning, prove convergence of the proposed method in self-play, a demonstrate it in a couple of iterated normal form games with non-trivial Nash equilibria where baselines fail to converge.\n\nI believe the community will find intuitions, methods, and theory developed by the authors interesting. However, I find some parts of the argument somewhat questionable as well as experimental verification insufficient (see comments below).\n\n\n# Comments and questions:\n\n## Weaknesses in the experimental evaluation:\nI find it hard to justify a fairly complex algorithm (even though inspired by cognitive science), when most of the simpler alternatives from the literature haven't been really tested on the same iterated games (the baselines in the paper are all simple gradient-based policy search methods).\n\nIn the introduction (paragraph 2), the authors point out potential limitations of previous opponent modeling algorithms, but never compare with them in experiments. If the claim is that other methods \"tend to work only under limited scenarios\" while PR2 is more general, then it would be fair to ask for a comprehensive comparison of PR2 vs alternatives in at least 1 such scenario. I would be interested to see how the classical family of \"Win or Learn Fast\" (WoLF) algorithms (Bowling and Veloso, 2002) and the recent LOLA (Foerster et al, 2018) compare with PR2 on the iterated matrix game (section 5.1).\n\nAlso, out of curiosity, it would be interesting to see how PR2 works on simple iterated matrix games, eg iterated Prisoner's dilemma.\n\n## Regarding the probabilistic formulation (section 4.3)\nEq. 8 borrows a probabilistic formulation of optimality in RL from Levine (2018). The expression given in Eq. 8 is proportional to the probability of a trajectory conditional on that each step is optimal wrt the agent's reward r^i, i.e., not for p(\\tau) but for p(\\tau | O=1).\n\nIf I understand it correctly, by optimizing the proposed KL objective, we fit both \\pi^i and \\rho^{-i} to the distribution of *optimal trajectories* with respect to r^i reward. That makes sense in a cooperative setting, but the problem arises when opponent's reward r^{-i} is different from r^i, in which case I don't understand how \\rho^{-i} happens to approximate the actual policy of the opponent(s). Am I missing something here?\n\nA minor point: shouldn't \\pi^{-i} in eq. 9 be actually \\rho^{-i}? (The derivations in appendix C suggest that.)\n\n## Regarding alternative approaches (section 4.5)\nThe authors point out intractability of trying to directly approximate \\pi^{-i}. The argument here is a little unclear. Wouldn't simple behavioral cloning work? Also, could we minimize KL(\\pi^{-i} || \\rho^{-i}) instead of KL(\\rho^{-i} || \\pi^{-i})?\n\n# Minor\n- I might be misreading it, but the last sentence of the abstract seems to suggest that this paper introduces opponent modeling to MARL, which contradicts the first sentence of paragraph 2 in the introduction.\n- It is very hard to read plots in Figure 3. Would be nice to have them in a larger format.\n\nOverall, I find the paper interesting, but it would definitely benefit from more thorough experimental evaluation.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Significant updates to the new version",
            "review": "The paper introduces a decentralized training method for multi-agent reinforcement learning, where the agents infer the policies of other agents and use the inferred models for decision making. The method is intuitively straightforward and the paper provides some justification for convergence. I think the underlying theory is okay (new but not too surprising, a lot of the connections can be made with single agent RL), but the paper would be much stronger with experiments that have more than two players, one state and one dimensional actions.\n\n(\nUpdate: the new version of the paper addresses most of my concerns. There are a lot more experiments, and I think the paper is good for ICLR. \n\nHowever, I wonder if the reasoning for PR2 is limited to \"self-play\", otherwise Theorem 1 could break because of the individual Q_i functions will not be symmetric. This could limit the applications to other scenarios.\n\nAlso, maybe explain self-play mathematically to make the paper self contained?\n)\n\n1. From the abstract, \" PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario\". Theorem 2 only shows that under relatively strong assumptions (e.g. single Nash equilibrium), the soft value iteration operator is a contraction. This seems to have little to do with the actual convergence of PR2-Q and PR2-AC, especially AC which uses gradient-based approach. Also here the \"convergence\" in the abstract seem to imply convergence to the (single) global optimal solution (as is shown in the experiments), for which I thought you cannot prove even for single agent AC -- the best you can do is to show that gradient norm converges to zero, which gives you a local optima. Maybe things are different with the presence of the (concave) entropy regularization?\n\n2. Theorem 2 also assumes that the opponent model $\\rho$ will find the global optimal solution (i.e. (11, 12) can be computed tractably). However, the paper does not discuss the case where $\\rho$ or $Q_\\theta$ in question is imperfect (similar to humans over/underestimate its opponents), which might cause the actual solution to deviate significantly from the (single) NE. This would definitely be a problem in more high-dimensional MARL scenarios. I wonder if one could extend the convergence arguments by extending Prop 2.\n\n3. The experiments mostly demonstrates almost the simplest non-trivial Markov games, where it could be possible that (11, 12) are true for PR2. However, the effectiveness of the method have not been demonstrated in other (slightly higher-dimensional) environments, such as the particle environments in the MADDPG paper. It does not seem to be very hard to implement this, and I wonder if this is related to the approximation error in (11, 12). The success in such environments would make the arguments much stronger, and provide sound empirical guidance to MARL practitioners.\n\nMinor points:\n- Are the policies in question stationary? How is PR2 different from the case of single agent RL (conditioned on perfect knowledge of a stationary opponent policy)?\n- I have a hard time understanding why PR2 would have different behavior than IGA even with full knowledge of the opponent policy, assuming each policy is updated with infinitesimally small (but same) learning rates. What is the shape of the PR2 optimization function wrt agent 1?\n- I wonder if using 2 layer neural networks with 100 units each on a 1 dimensional problem is overkill.\n- Figure 4(a): what are the blue dots?\n- Does (11) depend on the amount of data collected from the opponents? If so, how?\n- I would recommend combining Prop 1 and prop 2 to save space. Both results are straightforward to prove, but the importance sampling perspective might be useful.\n- Have you tried to compare with SGA (Balduzzi et al) or Optimistic mirror descent?\n- I am also curious about an ablation study over the components used to infer opponent policies. A much simpler case would be action-dependent baselines, which seem to implicitly use some information about the opponents.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}