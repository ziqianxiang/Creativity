{
    "Decision": {
        "metareview": "The paper presents the use of information bottlenecks as a way to identify key \"decision states\" in exploration, in a goal-conditioned model. The concept of \"decision states\" is actually common in RL, states where exploring can lead to very diverse/new states. The implementation of the \"information bottleneck\" is done by adding a regularizing term, the conditional mutual information I(A;G|S).\n\nThe main weaknesses of the paper were its lack of clarity and the experimental section. It seems to me that the rebuttals, and the additional experiments and details, made the paper worthy of publication. The authors cleared enough of the gray areas and showcased the relative merits of the methods.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "An interesting link between generalization and exploration"
    },
    "Reviews": [
        {
            "title": "Review Number 3 (So sorry for the delay!)",
            "review": "The authors propose a new regularizer for policy search in a multi-goal RL setting. The objective promotes a more efficient exploration strategy by encouraging the agent to learn policies that depend as little as possible on the target goal. This is achieved by regularizing standard RL losses with the negative conditional mutual information I(A;G|S). Although this regularizer cannot be optimize, the authors propose a tractable bound. The net effect of this regularizer is to promote more effective exploration by encouraging the agent to visit decision states, in which goal-depend decisions play a more important role. The idea of using this particular regularizer is inspired by an existing line of work on the information bottleneck.\n\nI find the idea proposed by the authors to be interesting. However, I have the following concerns, and overall I think this paper is borderline.\n\n1. The quality of the experimental validation provided by the authors is in my opinion borderline acceptable. Although the method performs better on toy settings, it seems barely better on more challenging ones. Experiments in section 4.5 lack detail and context.\n2. The clarity of the presentation is also not great.\n    2.1. The two-stage nature of the method was confusing to me. I didn’t understand the role of the second stage. Most focus is on the first stage, and only very little on the second stage. For example, I was confused about why the sign of the regularizer was flipped.\n    2.2. I was confused by how exactly the bounds (3) and (4) we applied and in what order.\n    2.3. I think the intuition of the method could be better explained and better validated by experiments.\n\nI also have the following additional comments:\n* How is the regularizer applied with other policy search algorithms besides Reinforce? Was it done in the paper? I can’t say for sure. Specifically, when comparing to PPO, was the algorithm compared to a version of PPO augmented with this regularizer? Why yes or why no?\n* More generally, experiments where more modern policy search algorithms are combined with the regularizer would be helpful. In particular, does it matter which policy search algorithm we use with this method?\n* Experimental plots in section 4.4 are missing error bars, and I can’t tell if the results are significant without them.\n* I thought the motivation for choosing this regularizer was lacking. The authors cite the information bottleneck literature, but we shouldn’t need to read all these papers, the main ideas should be summarized here.\n* The argument for how the regularizer improves exploration seemed to me very hand-wavy and not well substantiated by experiments.\n* I would love to see a better discussion of how the method is useful when he RL setting is not truly multi-goal.\n* The second part of the algorithm needs to be explained much more clearly.\n* What is the effect of the approximation on Q?\n\n---\n\nI have read the response of the authors, and they have addressed a significant numbers of concerns that I had. I am upgrading my rating to a 7.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea. Not sure the significance of its experimental results",
            "review": "This paper proposes the concept of decision state, which is the state where decision is made “more” dependent to a particular goal. The authors propose a KL divergence regularization to learn the structure of the tasks, and then use this information to encourage the policy to visit the decision states. The method is tested on several different experiment setups.\n\nIn general the paper is well-written and easy to follow. Learning a more general policy is not new (as also discussed in the paper), but using the learned structure to further guide the exploration of the policy is novel and interesting.\n\nI have a couple questions about the experimental part though, mostly about the baselines.\n1. What is the reasoning behind the selection of the baselines, e.g. A2C as the baseline for the miniGrid experiments? \n2. What are the performances of the methods in Table 2, in direct policy generalization? Or is there any reason not reporting them here?\n3.  What is the reasoning of picking “Count-base baseline” for Figure 4, rather than the method of curiosity-based exploration?\n4. For the Mujoco tasks, there are couple ones outperforming PPO, e.g. TD3, SAC etc.. [1,2] The authors should include their results too. \n5. As an ablation study, it would be interesting to see how the bonus reward of visiting decision states can help the exploration on the training tasks, compared to the policy learned from equation (1), and the policies learned without information of other tasks.\n6. Lastly, the idea of decision states can also be used in other RL algorithms. It would be also interesting to see if this idea can further improve their performances.\n\nOther comments:\n1. Equation (3) should be \\le.\n2. Why would Equation (5) hold? \n3. Right before section 2.2, incomplete sentence.\n\n\nDisclaimer: The reviewer is not familiar with multitask reinforcement learning, and the miniGrid environment in the paper. Other reviewers should have better judgement on the significance of the experimental results.\n\n[1] Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" arXiv preprint arXiv:1801.01290 (2018).\n[2] Fujimoto, Scott, Herke van Hoof, and Dave Meger. \"Addressing Function Approximation Error in Actor-Critic Methods.\" arXiv preprint arXiv:1802.09477 (2018).",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Potentially useful but poorly motivated and evaluated",
            "review": "The paper proposes a method of regularising goal-conditioned policies with a mutual information term. While this is potentially useful, I found the motivation for the approach and the experimental results insufficient. On top of that the presentation could also use some improvements. I do not recommend acceptance at this time.\n\nThe introduction is vague and involves undefined terms such as \"useful habits\". It is not clear what problems the authors have in mind and why exactly they propose their specific method. The presentation of the method itself is not self-contained and often relies on references to other papers to the point where it is difficult to understand just by reading the paper. Some symbols are not defined, for example what is Z and why is it discrete?\n\nThe experiments are rather weak, they are missing comparison to strong exploration baselines and goal-oriented baselines.",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}