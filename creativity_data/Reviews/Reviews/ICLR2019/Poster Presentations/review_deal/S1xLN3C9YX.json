{
    "Decision": {
        "metareview": "The authors propose a method to learn a neural network architecture which achieves the same accuracy as a reference network, with fewer parameters through Bayesian Optimization. The search is carried out on embeddings of the neural network architecture using a train bi-directional LSTM. The reviewers generally found the work to be clearly written, and well motivated, with thorough experimentation, particularly in the revised version. Given the generally positive reviews from the authors, the AC recommends that the paper be accepted.\n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "Architecture search through Bayesian Optimization"
    },
    "Reviews": [
        {
            "title": "interesting idea but...",
            "review": "In this work, the authors propose a new strategy to compress a teacher neural network. Briefly, the authors propose using Bayesian optimization (BO) where the accuracy of the networks is modelled using a Gaussian Process function with a squared exponential kernel on continuous neural network (NN) embeddings. Such embeddings are the output of a bidirectional LSTM taking as input the “raw” (discrete) NN representations (when regarded as a covariance function of the “raw” (discrete) NN representations, the kernel is a deep kernel).\n\nThe authors apply this framework for model compression. In this application, the search space is the space of networks obtained by sampling reducing operations on a teacher network. In applications to CIFAR-10 and CIFAR-100 the authors show that the accuracies of the compressed network obtained through their method exceeds accuracies obtained through other methods for compression, manually compressed networks and random sampling.\n\nI have the following concerns/questions:\n\n1)\tThe authors motivate their work in the introduction by discussing the importance of learning a good embedding space over network architectures to “generate a priority ordering of architectures for evaluation”. Within the proposed BO framework, this would require the optimization of the expected improvement in a high-dimensional and discrete space (the space of NN architectures), which “is non-trivial”. In this work, the authors do not try to solve this general problem, but specialize their work to model compression, which has a much lower dimensional search space (space of networks obtained by sampling reducing operations on a teacher network). For this reason, I believe the presentation and motivation of this work is not presented clearly. Specifically, while I agree that the methods and results in this paper can be relevant to the problem of getting NN embeddings for a larger search space, this should be discussed in the conclusion/discussion as future direction, rather than as motivating example. Generally, I think the method should be described in the context of model compression rather than as a general method for neural architecture search (NAS) method (in my understanding, its use for NAS would be unfeasible). \n\n2)\tI have been wondering why the authors optimize the kernel parameters by maximizing the predictive GP posterior rather than maximizing the GP log marginal likelihood as in standard GP regression?\n\n3)\tThe sampling procedure should be explained in greater detail. How many reducing operations are sampled? This would be important to fully understand the random search method the authors consider for comparison in their experiments. I expect that the results from that method will strongly depend on the sampling procedure and different choices should probably be explored for a fair comparison. Do the authors have any comment on this?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "interesting idea but the paper needs further work",
            "review": "================\nPost-Rebuttal\n================\n\nI thank the authors for the larger amount of additional work they put into the rebuttal. Since the authors addressed my main concerns, i e. comparison to existing methods,  clarifications of the proposed approach, adding references to related work, I will  increase my score and suggest to accept the paper.\n\n\n\n\nThe paper describes a new neural architecture search strategy based on Bayesian optimization to find a compressed version of a teacher network. The main contribution of the paper is to learn an embedding that maps from a discrete encoding of an architecture to a continuous latent vector such that standard Bayesian optimization can be applied. \nThe new proposed method improves in terms of compressing the teacher network with just a small drop in accuracy upon an existing neural architecture search method based on reinforcement learning and random sampling.\n\n\nOverall, the paper presents an interesting idea to use Bayesian optimization on high dimensional discrete problems such as neural architecture search. I think a particular strength of this methods is that the embedding is fairly general and can be combined with various recent advances in Bayesian optimization, such as, for instance, multi-fidelity modelling.\nIt also shows on some compression experiments superior performance to other state-of-the-art methods.\n\nHowever, in its current state I do not think that the paper is read for acceptance:\n\n- Since the problem is basically just a high dimensional, discrete optimization problem, the paper misses comparison to other existing Bayesian optimization methods such as TPE [1] / SMAC [2] that can also handle these kind of input spaces. Both of these methods have been applied to neural architecture search [3][4] before. Furthermore, since the method is highly related to NASBOT [5], it would be great to also see a comparison to it.\n\n- I assume that in order to learn a good embedding, similar architectures need to be mapped to latent vector that are close in euclidean space, such that the Gaussian process kernel can model any correlation[7]. How do you make sure that the LSTM learns a meaningful embedding space? It is also a bit unclear why the performance f is not used directly instead of p(f|D). Using f instead of p(f|D) would probably also make continual training of the LSTM easier, since function values do not change.\n\n- The experiment section misses some details:\n  - Do the tables report mean performances or the performance of single runs? It would also be more convincing if the table contains error bars on the reported numbers.\n  - How are the hyperparameters of the Gaussian process treated?\n  \n- The related work section misses some references to Lu et al.[6] and Gomez-Bombarelli et al.[7] which are highly related.\n\n- What do you mean with the sentence  \"works on BO for NAS can only tune feed-forward structures\" in the related work section? There is no reason why other Bayesian optimization should not be able to also optimize recurrent architectures (see for instance Snoek et al.[8]). \n\n- Section 3.3 is a bit confusing and to be honest I do not get the motivation for the usage of multiple kernels. Why do the first architectures biasing the LSTM? Since Bayesian optimization with expected improvement samples around the global optimum, should not later evaluated, well-performing architectures more present in the training dataset for the LSTM?\n\n\n[1] Algorithms for Hyper-Parameter Optimization\n    J. Bergstra and R. Bardenet and Y. Bengio and B. Kegl\n    Proceedings of the 25th International Conference on Advances in Neural Information Processing Systems (NIPS'11)\n\n[2] Sequential Model-Based Optimization for General Algorithm Configuration\n    F. Hutter and H. Hoos and K. Leyton-Brown\n    Proceedings of the Fifth International Conference on Learning and Intelligent Optimization (LION'11)\n\n[3] Towards Automatically-Tuned Neural Networks\n    H. Mendoza and A. Klein and M. Feurer and J. Springenberg and F. Hutter\n    ICML 2016 AutoML Workshop\n\n[4] Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures\n    J. Bergstra and D. Yamins and D. Cox\n    Proceedings of the 30th International Conference on Machine Learning (ICML'13)\n\n[5] Neural Architecture Search with Bayesian Optimisation and Optimal Transport\n    K. Kandasamy and W. Neiswanger and J. Schneider and B. P{\\'{o}}czos and E. Xing\n    abs/1802.07191\n\n[6] Structured Variationally Auto-encoded Optimization\n    X. Lu and J. Gonzalez and Z. Dai and N. Lawrence\n    Proceedings of the 35th International Conference on Machine Learning\n\n[7] Automatic chemical design using a data-driven continuous representation of molecules\n    R. Gómez-Bombarelli and J. Wei and D. Duvenaud and J. Hernández-Lobato and B. Sánchez-Lengeling and D. Sheberla and J. Aguilera-Iparraguirre and T. Hirzel. and R. Adams and A. Aspuru-Guzik\n    American Chemical Society Central Science\n\n[8] Scalable {B}ayesian Optimization Using Deep Neural Networks\n    J. Snoek and O. Rippel and K. Swersky and R. Kiros and N. Satish and N. Sundaram and M. Patwary and Prabhat and R. Adams\n    Proceedings of the 32nd International Conference on Machine Learning (ICML'15)",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A nice paper questioned by the significance of the results",
            "review": "Review:\n\nThis paper proposes a method for finding optimal architectures for deep neural networks based on a teacher network. The optimal network is found by removing or shrinking layers or adding skip connections. A Bayesian Optimization approach is used by employing a Gaussian Process to guide the search and the acquisition function expected improvement. A special kernel is used in the GP to model the space of network architectures. The method proposed is compared to a random search strategy and a method based on reinforcement learning.\n\t\nQuality: \n\n\tThe quality of the paper is high in the sense that it is very well written and contains exhaustive experiments with respect to other related methods\n\nClarity: \n\n\tThe paper is well written in general with a few typos, e.g., \n\n\t\"The weights of the Bi-LSTM θ, is learned during the search process. The weights θ determines\"\n\nOriginality: \n\n\tThe proposed method is not very original in the sense that it is a combination of several known techniques. May be the most original contribution is the proposal of a kernel for network architectures based on recurrent neural networks.\n\n\tAnother original idea is the use of sampling to avoid the problem of doing kernel over-fitting. Something that can be questioned, however, in this regard is the fact that instead of averaging over kernels the GP prediction to account for uncertainty in the kernel parameters, the authors have suggested to optimize a different acquisition function per each kernel. This can be problematic since for each kernel over-fitting can indeed occur, although the experimental results suggest that this is not happening.\n\t\nSignificance:\n\n\tWhy N2N does not appear in all the CIRFAR-10 and CIFAR-100 experiments? This may question the significance of the results.\n\n\tIt also seems that the authors have not repeated the experiments several times since there are no error bars in the results.\n\tThis may also question the significance of the results. An average over several repetitions is needed to account for the randomness in for example the sampling of the network architectures to learn the kernels.\n\n\tBesides this, the authors may want to cite this paper\n\n\tHernández-Lobato, D., Hernandez-Lobato, J., Shah, A., & Adams, R. (2016, June). Predictive entropy search for multi-objective Bayesian optimization. In International Conference on Machine Learning (pp. 1492-1501).\t\n\n\twhich does multi-objective Bayesian optimization of deep neural networks (the objectives are accuracy and prediction time).\n\nPros:\n\n\t- Well written paper.\n\t\t\n\t- Simply idea.\n\n\t- Extensive experiments.\n\nCons:\n\t\n\t- The proposed  approach is a combination of well known methods.\n\n\t- The significance of the results is in question since the authors do not include error bars in the experiments.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}