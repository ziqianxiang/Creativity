{
    "Decision": {
        "metareview": "The authors discuss an improved distillation scheme for parallel WaveNet using a Gaussian inverse autoregressive flow, which can be computed in closed-form, thus simplifying training. The work received favorable comments from the reviewers, along with a number of suggestions for improvement which have improved the draft considerably. The AC agrees with the reviewers that the work is a valuable contribution, particularly in the context of end-to-end neural text-to-speech systems. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Well written paper with detailed experiments"
    },
    "Reviews": [
        {
            "title": "A solid and insightful experimental contribution to neural spectrum-to-waveform and speech synthesis (same rating after reviewing responses)",
            "review": "This paper proposes some modifications to established procedures for neural speech synthesis and investigates their effect experimentally. The proposed modifications are mostly fairly straightforward conceptually, but appear to work well, and this reviewer feels the paper has huge value in its experimental contributions extending and clarifying certain aspects of WaveNet training and distillation. The paper is well-written and fairly concise, with a short-and-sweet experimental results section.\n\nMajor comments:\n\nThe conceptual novelty seems a little overstated in the abstract. For example, the value seems to not really be in the \"proposing\" a text-to-wave neural architecture for speech synthesis (which, aside from important experimental tweaks, is essentially Tacotron 2 training all parameters from scratch) but in showing that it works well experimentally. Conceptually the paper is extremely close to the parallel wavenet paper, the main differences being slightly different component distributions (Gaussian instead of logistic), a different set of loss terms in addition to the reverse KL, and joint training of the spectral synthesis and waveform synthesis parts of the model.\n\nIt would be super insightful to include log probabilities on the test set (everywhere MOS results have been reported) in the experimental results. This would help tease apart the effects of architecture inductive bias, different divergences, distillation, etc. One of the really nice things about flow-based models is the ability to compute the log probability tractably.\n\n\nMinor comments:\n\nPerhaps mention that teacher forcing is maximum likelihood in the introduction? Currently it almost sounds like the paper is contrasting teacher forcing for WaveNet (paragraph 2) and MLE (list item 1).\n\nAt the end of paragraph 3 in the introduction, it would be helpful to mention that the intractable KL divergence being referred to is the frame-level one-step-ahead predictions, not the entire sequence-level prediction. Also, for 1D distributions isn't taking a large number of samples quite effective in practice?\n\nIn introduction list item 3, suggest mentioning Tacotron 2 (Shen et al) and contrasting with the present work for clarity.\n\nIn section 3.1, it surprises me slightly that clipping at -7 is essential. It would be helpful to state what exactly goes wrong if this is not done. Does it lead to overfitting and so bad test log likelihoods? What effect is noticeable in the generated samples?\n\nEquation (6) is incorrect. It should be conditioned on < t, not <= t. Conditioning on z <= t would make x_t deterministic.\n\nEquation (7) is technically true as written, but only because all the distributions involved are deterministic. If <= t is replaced with < t (which based on the mistake in (6) is what I suspect the authors intended) then it is no longer true. This equation is not used anywhere as far as I can tell. It seems to me like the property that enables non-recursive-over-time (\"parallel\") sampling is (5), not (7). Incidentally, when multiple one-step-ahead samples are taken per frame for parallel wavenet, the samples viewed at the sequence level are highly correlated, and do not obey anything like (7), but it doesn't affect the correctness of the expected value.\n\nThe IAF doesn't really \"infers its output x at all time steps\". Maybe \"models\" instead of \"infers\"?\n\nLearning an \"IAF directly through maximum likelihood\" doesn't seem all that impractical. People train networks with recursive dependence such as RNNs (which is essentially what would be required to train certain forms of IAF with MLE) as opposed to non-recursive dependence such as CNNs all the time, after all. It seems like this claim depends on the details of the transform $f$.\n\nOut of interest, did the authors consider reversing the sequence being generated in time between successive IAF blocks? This would limit the ability to do low latency synthesis but might improve performance considerably.\n\nThe first paragraph in section 3.3 seems like it should probably be part of section 3.3.1 (it's not related to other losses such as spectrogram frame loss, for example). It would be helpful to state explicitly that: (a) the goal is to minimize the sequence-level reverse KL; (b) this can be approximated by taking a single sample z, but this may have high variance; (c) the variance of this estimate can be reduced by marginalizing over the one-step-ahead predictions for each frame; (d) parallel wavenet's mixture of logistics means it has to use a separate Monte Carlo sampling at the frame-level, whereas the proposed Gaussian allows this one-step-ahead marginalization to be performed analytically. This one-step-ahead marginalization is an example of Rao-Blackwellization.\n\nIt didn't seem clear from section 3.3 and 3.3.1 that parallel wavenet also uses the one-step-ahead marginalization trick to reduce the variance.\n\nIt might be helpful to mention that using the reverse KL would be expected to have mode-fitting behavior, making samples sound better but log probability on the test set worse.\n\nIt was not clear to me what difference or similarity was being demonstrated in Figure 1.\n\nSmall point, but \"Oord et al\" should be \"van Oord et al\" throughout (it's a surname).\n\nIn section 3.3.2, can the authors give any insight as to why training with reverse KL alone leads to whispering, and why adding the STFT term fixes this? (If it's only something that's been noticed empirically, \"will lead\" -> \"empirically we found\"?)\n\nI noticed quite a large qualitative perceptual difference between the student and teacher samples, particularly in the speech synthesis case (experiment 3), even though I think I'd rate the quality on a linear scale as fairly similar (in line with the MOS results). The teacher sounds noticeably \"harsher\" but \"clearer\" Do the authors have any insight as to why this perceptual difference occurs (if they also perceive a qualitative difference)? Is it probably a difference in inductive bias between an AF (which WaveNet can be seen as) and IAF?\n\nI found it fascinating that reverse KL and forward KL lead to roughly the same MOS for spectrum-to-waveform. I assumed reverse KL would be better due to its preference for high-quality samples due to mode fitting.\n\nOut of curiosity, what is responsible for the pops at the start of the spectrogram-conditioned distilled models? Also why are the synthesized samples shorter than the ground truth (less initial silence)?\n\n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A strong result but unclear experiments and contribution",
            "review": "After reading other reviews and author comments, I have raised my rating to a 6. My main concerns remain (lack of significant contribution and lack of an ablation study with more comprehensive experiments). However, I'm not against the paper as an interesting finding in and of itself. It would be great if the authors (or interested members of the research community) may analyze how general-purpose their proposals are (e.g., of Gaussian base distribution) and how extensive the results are on TTS benchmarks.\n\n--\n\nOverall, I very much like the direction this paper pursues. However, the content doesn't substantiate their two claimed contributions. I highly recommend the authors either back up their claims in more detail, or center their work in terms of the result and less so about the ideas (which at the moment, are not convincing to use outside of this specific setup).\n\nThe authors propose two contributions:\n\n1. They build on parallel WaveNet which uses distillation by minimizing a KL divergence from a Logistic IAF as a student to a Mixture of Logistic AF as a teacher. Instead, they simply use Gaussians which has a closed-form KL divergence and makes training during distillation significantly simpler. Because of stability problems, they also add 1. a penalty term to discourage the original loss from dividing by a standard deviation close to zero; and 2. converting van den Oord et al. (2018)'s average power loss penalty to a frame-level loss penalty.\n\nTheir choice of Gaussians requires a restriction on the likelihood, and they show one result arguing the likelihood choice doesn't make much of a difference. This result comprises 4 human-evaluated numbers, with a fixed architecture and training hyperparameters of their choice. Unfortunately, I'm not convinced. Can the authors provide more compelling evidence? If the authors argue this is one of their main contributions, I find that lack of a more comprehensive empirical or theoretical study disconcerting.\n\nSimilarly, while I like that using Gaussian KLs makes the distillation objective in closed-form, there isn't evidence indicating the benefit. The one result (the 4 numbers above) are conflated by both the change in model as well as utilizing the closed-form loss. The same goes for their one result (2 numbers) comparing forward to reverse KL.\n\n2. They \"propose the first text-to-wave neural architecture for TTS, which can be trained from scratch in an end-to-end\nmanner.\" I'm not an expert on speech so I can't accurately assess the novelty here. However, it would be nice to show these results independent of the other proposed changes.\n\nWriting-wise, the paper was clear, although potentially too packed with background information. As a expert on generative models, most of Sections 1-3 are already well-known and could be made more concise by referencing past works for more details. They add various details (such as the architecture notes at the end of 3.1) which should be better placed elsewhere to tease out what the important changes are in this paper.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good and significant work, paper could be improved",
            "review": "Paper summary:\n\nThe paper presents two distinct contributions in text-to-speech systems:\na) It describes a method for distilling a Gaussian WaveNet into a Gaussian Inverse Autoregressive Flow that uses an analytically computed KL between their conditionals.\nb) It presents a text-to-speech system that is trained end-to-end from text to waveforms.\n\nTechnical quality:\n\nThe distillation method presented in the paper is technically correct. The evaluation is based on Mean Opinion Score and seems to follow good practices.\n\nThe paper makes three claims:\na) A WaveNet with Gaussian conditionals can model speech waveforms equally well as WaveNets with other types of conditionals.\nb) Analytically computing KL divergence stabilizes distillation.\nc) A text-to-speech system trained end-to-end from text to waveforms outperforms one that has separately trained text-to-spectrogram and spectrogram-to-waveform subsystems.\n\nClaims (a) and (c) are clearly demonstrated in the experiments. However, there is nothing in the paper that substantiates claim (b). I think the paper would be strengthened if the performance of sample-based KL distillation was added into Table 2, and if learning curves were reported that evaluate the amount of stabilization that an analytical KL may offer vs a sample-based KL.\n\nFurther points about the experiments:\n- It wasn't clear to me whether distillation happens at the same time as the autoregressive WaveNet is trained on data, or after it has been fully trained. I think the paper should make this clear.\n- The paper says that distillation makes generation three orders of magnitude faster. I think it would be good if actual generation times (e.g. in seconds) were reported.\n\nClarity:\n\nThe paper is generally well-written. Sections 1 and 2 in particular are excellent.\n\nHowever, section 3 contains several notational errors and technical inaccuracies, that makes it rather confusing to read. In particular:\n- q(x_t | z_{<=t}) is used in several places to mean the Gaussian conditional q(x_t | z_{<t}) (e.g. in Eqs (6) and (7), and elsewhere). This is confusing, as q(x_t | z_{<=t}) is actually a delta distribution.\n- q(x | z) is used in several places to mean q(x) (e.g. in Eq. (7), in Alg. 1 and elsewhere). This is confusing, as q(x | z) is also a delta distribution.\nI believe that section 3, especially subsections 3.2 and 3.3.1, should be reworked to be made clearer, and the notation should be carefully revised.\n\nI don't think the paper needs to span 9 pages. Section 3 is rather wordy, and should be compressed to the important points.\n\nOriginality:\n\nDistilling a Gaussian autoregressive model to another Gaussian autoregressive model by matching their Gaussian conditionals with an analytical KL is rather straightforward, and, methodologically speaking, I wouldn't consider it an original contribution on its own. However, I think its application and demonstration in text-to-speech constitutes an original contribution.\n\nSignificance:\n\nThe paper contains a substantial amount of significant work that I think is important to be communicated to the ICLR community, especially the text-to-speech community.\n\nReview summary:\n\nPros:\n+ Substantial amount of good work.\n+ Significant improvement in text-to-speech end-to-end software.\n+ Generally well-written (with the exception of section 3 which needs work).\n\nCons:\n- Some more experiments would be good to substantiate the claim that analytical KL is better.\n- Notational errors and confusion in section 3.\n- Too wordy, no need for 9 pages.\n\nNitpicks:\n- As I said above, I wouldn't consider distillation of models with Gaussian conditionals using analytical KLs methodologically novel, so I think the phrase \"novel regularized KL divergence\" should be moderated.\n- Eq. (1) should contain theta on the left hand side too.\n- Page 3: \"at Appendix B\" --> \"in Appendix B\".\n- Page 4: In flows we don't just \"suppose z has the same dimension as x\"; rather, it's a necessary condition that must hold.\n- Footnote 5: It's unclear to me what it means to \"make the loss less sensitive\".\n- References: Real NVP, Fourier, Bayes, PixelCNN, WaveNet, VoiceLoop should be properly capitalized.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}