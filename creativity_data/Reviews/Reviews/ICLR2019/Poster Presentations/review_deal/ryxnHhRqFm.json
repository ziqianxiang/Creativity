{
    "Decision": {
        "metareview": "Interesting paper applying memory networks that encode external knowledge (represented in the form of triples) and conversation context for task oriented dialogues. Experiments demonstrate improvements over the state of the art on two public datasets. \nNotation and presentation in the first version of the paper were not very clear, hence many question and answers were exchanged during the reviews. \n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "novel architecture for task oriented dialogue systems"
    },
    "Reviews": [
        {
            "title": "nicely motivated architecture and thorough evaluation, aimed at an interesting and difficult task",
            "review": "The paper presents a new model for reading and writing memory in the context of task-oriented dialogue. The model contains three main components: an encoder, a decoder, and an external KB. The external KB is in the format of an SVO triple store. The encoder encodes the dialogue history and, in doing so, writes its hidden states to memory and generates a \"global memory pointer\" as its last hidden state. The decoder takes as input the global memory pointer, the encoded dialogue state history, and the external KB and then generates a response using a two-step process in which it 1) generates a template response using tags to designate slots that need filling and 2) looks up the correct filler for each slot using the template+global memory pointer as a query. The authors evaluate the model on a simulated dialogue dataset (bAbI) and on a human-human dataset (Stanford Multi-domain Dialogue or SMD) as well as in a human eval. They show substantial improvements over existing models on SMD (the more interesting of the datasets) in terms of entity F1--i.e. the number of correctly-generated entities in the response. They also show improvement on bAbI specifically on cases involving OOVs. On the human evaluation, they show improvements in terms of both \"appropriateness\" and \"human-likeness\". \n\nOverall, I think this is a nice and well-motivated model. I very much appreciate the thoroughness of the evaluation (two different datasets, plus a human evaluation). The level of analysis of the model was also good, although there (inevitably) could have been more. Since it is such a complex model, I would have liked to see more thorough ablations or at least better descriptions of the baselines, in order to better understand which specific pieces of the model yield which types of gains. A few particular questions below:\n\n- You describe the auxiliary loss on the global pointer, and mention an ablation study that show that this improves performance. Maybe I am overlooking something, but I cannot find this ablation in the paper or appendix. It would be nice to see how large the effect is. \n- Following on the above, why no similar auxiliary losses on additional components, e.g. the template generation? Were these tried and deemed unnecessary or vice-versa (i.e. the default was no auxiliary loss and they were only added when needed)? Either way, it would be nice to better communicate the experiments/intuitions that motivated the particular architecture you arrived at.\n- I really appreciate that you run a human eval. But why not have humans evaluate objective \"correctness\" as well? It seems trivial to ask people to say whether or not the answer is correct/communicates the same information as the gold.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "End-to-end task oriented system: An encoder-decoder approach with a shared external knowledge base",
            "review": "This is, in general, a well-written paper with extensive experimentation. \n\nThe authors tried to describe their architecture both with equations as well as graphically. However, I would like to mention the following: \n\nIn Section 2.1 I am not sure all the symbols are clearly defined. For example, I could not locate the definitions of n, l etc. Even if they are easy to assume, I am fond of appropriate definitions. Also, I suspect that some symbols, like n, are not used consistently across the manuscript.\n\nI am also confused about the loss function. Which loss function is used when?\n\nI am missing one more figure. From Fig 2 it's not so straightforward to see how the encoder/decoder along with the shared KB work at the same time (i.e. not independently)\n\nIn Section 2.3, it's not clear to me how the expected output word will be picked up from the local memory pointer. Same goes for the entity table.\n\nHow can you guarantee that that position n+l+1 is a null token?\n\nWhat was the initial query vector and how did you initialise that? Did different initialisations had any effect on performance?\n\nIf you can please provide an example of a memory position.\n\nAlso, i would like to see a description of how the OOV tasks are handled.\n\nFinally, your method is a NN end-to-end one and I was wondering how do you compare not with other end-to-end approaches, but with a traditional approach, such as pydial?\n\n\nAnd some minor suggestions:\n\nNot all the abbreviations are defined. For example QRN, GMN, KVR. It would also be nice to have the references of the respective methods included in the Tables or their captions.\n\nParts of Figs. 1&2 are pixelised. It would be nice to have everything vectorised.\n\n I would prefer to see the training details (in fact, I would even be favorable of having more of those) in the main body of the manuscript, rather than in the appendix.\n\nThere are some minor typos, such as \"our approach that utilizing the recurrent\" or \"in each datasets\"",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Expect more experiments",
            "review": "This paper puts forward a new global+local memory pointer network to tackle task-oriented dialogue problem.\n\nThe idea of introducing global memory is novel and experimental results show its effectiveness to encode external knowledge in most cases.\n\nHere're some comments:\n1. In global memory pointer, the users employ non-normalized probability (non-softmax). What is the difference in performance if one uses softmax?\n\n2. In (11), there's no linear weights. Will higher weights in global/local help?\n\n3. As pointed out in ablation study, it's weird that in task5 global memory pointer does not help.\n\n4. The main competitor of this algorithm is mem2seq. While mem2seq includes DSTC2 and In-car Assistant, and especially in-car assistant provides the first example dialogue, why does the paper not include expeirments on these two datasets?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}