{
    "Decision": {
        "metareview": "The paper studies population-based training for MARL with co-play, in MuJoCo (continuous control) soccer. It shows that (long term) cooperative behaviors can emerge from simple rewards, shaped but not towards cooperation.\n\nThe paper is overall well written and includes a thorough study/ablation. The weaknesses are the lack of strong comparisons (or at least easy to grasp baselines) on a new task, and the lack of some of the experimental details (about reward shaping, about hyperparameters).\n\nThe reviewers reached an agreement. This paper is welcomed to be published at ICLR.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "An interesting new task to study learning cooperation between agents"
    },
    "Reviews": [
        {
            "title": "Well-written submission with good analysis",
            "review": "The paper proposes a new environment - 2vs2 soccer - to study emergence of multi-agent coordinated team behaviors. Learning relies on population-based training of agent's shaped reward mixtures and approach of nash averaging is used for evaluation.\n\nClarity: the paper is well-written and clear. The ablations provided are helpful in understanding how much different introduced components matter, and quantitative and qualitative analysis of resulting behavior is quite nice\n\nOriginality: the individual pieces of this work (PBT, SVG, nash averaging) have been introduced previously, but this paper puts them together in a well-chosen manner.\n\nSignificance: I believe this paper proposes a number of interesting observations (effects of PBT, evaluation, effects of recurrent policies to overcome non-stationarity issues) that I believe would be of value to the part of ICLR community doing research in multi-agent systems. ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Summary: The authors use competition as a way to train agents in a complex continuous team-based control task: a 2 player soccer game. Agents are paired randomly into a team of 2 and play another team of 2. The key aspect of the proposed algorithm is the use of population based training.\n\nStrong Points\n-\tThe authors propose a convincing methodology for speeding up learning in coordinated MARL.\n-\tThe Nash Averaging approach suggested for evaluating in the presence of cycles is interesting and a useful tool for evaluation when there are no easy baselines\n-\tThe authors do convincing ablation studies to show that the PBT is the most important part of the learning algorithms and does well even when paired with a simple feed forward model\n\nQuestions\n-\tThe authors use reward shaping of the form: “We design shaping reward functions {rj : S × A → R}j=1,...,nr P , weighted so that r(·) := nr j=1 αj rj (·) is the agent’s internal reward and, as in Jaderberg et al.” I’m not sure I follow how this works, without the additional dense shaping in the soccer game the reward is 0/1 depending on if one’s team wins or loses, so won’t one’s rewards always be perfectly correlated with those of one’s teammates and perfectly anticorrelated with those of the other team? Does this only work with the dense shaping (e.g. vel-to-ball)?\n-\tI would like to see which of the PBT controlled hyperparameters actually matter for the increase in training speed. Do the learning rates matter (since they’re also being changed by the Adam optimizer as training goes) or is it about the discount factor/entropy regularizer?\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper presents a new simplified RoboCup environment that may be of some interest",
            "review": "This paper introduces a new multiagent research environment---a simplified version of 2x2 RoboSoccer using the MuJoCo physics engine with spherical players that can rotate laterally, move forwards / backwards, and jump.\n\nThe paper deploys a fine-tuned version of population-based sampling on top of a stochastic value gradient reinforcement learning algorithm to train the agents.  Some of the fine-tunings used include deploying different discount factors on multiple different reward channels for reward shaping.\n\nThe claimed novel contributions of the paper are (1) a new multiagent testbed, (2) a decentralized training procedure, (3) fine-tuning reward shaping, and (4) highlighting the challenges in evaluation in novel multiagent competitive environments.\n\nOverall, my judgment is that the paper is fine, but the authors have not helped me to understand the significance of their contributions.\n\nTaking each in turn:\n\n(1) What is the significance of the new environment?  What unique characteristics make it difficult?  What makes this environment an importantly different testbed or development environment?  The connection to RoboSoccer is motivating but tenuous. The new environment should have particular characteristics that expose problems with past algorithms or offer new challenges existing algorithms have not addressed at all.\n\n(2) Why is it important to have a decentralized training procedure when the authors have control over all the agents?  If it will allow faster training, has the authors' algorithm been demonstrated to accomplish that goal?  \n\n(3) It's hard to evaluate new algorithms when the domain studied is also new. We have no sense for state-of-the-art performance on this domain across a range of algorithms.  The authors conduct a careful ablation study on their new algorithm but do not compare their approach to other classes of algorithms.\n\n(4) The authors indicate that evaluating the quality of an algorithm for a competitive context is hard in absence of established benchmarks---whereas in single-agent or cooperative environments progress can be measured against the goal of the environment, progress in competitive environments requires comparison to approaches that are thought to be good.  Here the authors are themselves pointing out a fundamental problem with introducing new competitive multiagent testbeds, and the authors don't resolve this tension.  Since the main contribution of the work is the environment, it's hard to see how this point the authors themselves make doesn't undermine that central contribution.\n\nBesides other comments mentioned above, a couple other ways to improve the paper would be:\n- Clarify why this environment is important to be introducing---what are the unique things that can be studied with this new environment?\n- Hold an open competition to get benchmarks created by other teams of researchers\n\nSome minor comments:\n- $n_r$ is not defined explicitly in the text as far as I have found\n- The authors state: \"The specific shaping rewards use for soccer are detailed in Section 4.2\" but I couldn't find them there. \n\n---\n\nPost-rebuttal \n\nMy main concern was assessing the value of the overall contribution of the paper. The other reviewers seem to appreciate both the new environment being offered and the combination of techniques deployed in the authors' solution. If there is an audience that will appreciate this work at ICLR as seems to be indicated by those reviews, then I would increase my score to marginally above the acceptance threshold.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}