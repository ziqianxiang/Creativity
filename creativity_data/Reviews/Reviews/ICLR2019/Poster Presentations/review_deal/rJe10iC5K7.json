{
    "Decision": {
        "metareview": "The paper proposes a novel method that learns decompositions of an image over parts, their hierarchical structure  and their motion dynamics given temporal image pairs. The problem tackled is of great importance for unsupervised learning from videos. One downside of the paper is the simple datasets used to demonstrate the effectiveness of the method.  All reviewers though agree on it being a valuable contribution for ICLR.\n\nIn the related work section the paper mentions \"...Some systems emphasize\nlearning from pixels but without an explicitly object-based representation (Fragkiadaki et al., 2016 ...\". The paper you cite in fact emphasized the importance of having object-centric predictive models and the generalization that comes from this design choice, thus, it may be potentially not the right citation. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "novel method for learning part hierarchies and their motion dynamics"
    },
    "Reviews": [
        {
            "title": "Official Review",
            "review": "==== Review Summary ====\n\nThe paper demonstrates an interesting and potentially useful idea.  But much of it is poorely explained, and experimental results are not strongly convincing.  The only numerical evaluations are on a simple dataset that the authors made themselves.  The most interesting claim - that this network can learn unsupervised hierarchical object segmentation based on unlabelled video data -  is not well supported by the paper.  \n\n==== Paper Summary ====\n\nThis paper presents a deep neural network which learns object Segmentation, Structure, and Dynamics from unlabelled video.  The idea is quite useful, as it is a step towards learning models that can \"discover\" the concept of objects in a visual scene without any supervision.  \n\nThe central contributions of this paper are:\n(1) To show how one can use coherent motion (the fact that different parts of an object move together) to learn unsupervised object segmentation.\n(2) To show how once can learn the relation between objects (e.g. \"arm\" is part of \"body\") by observing relative motion between segments.\n\n==== General Feedback ====\n\nThe paper would benefit a lot from better explanations and being better tied together (see \"Details\" below for examples).   Figure captions would benefit from much better explanations and integration with the text - each figure caption should at least describe what the figure is intended to demonstrate.  Variables such as ($\\mathcal M$, $\\mathcal S$, $\\mathcal I$, $\\mathcal L$) should be indicated in figures .  \n\nMany things would benefit from being defined precisely with Equations.  For example I have no idea how the \"soft\" structural descriptor S is computed.  Is it (A) a parameter that is shared across data points and learned?  or (B) is it computed per-frame from the network?  And after it is calculated, how are the S_{ik} values (which fall between 0 and 1) used to combine the motion maps?  \n\n==== Scientific Questions ===\n\nI'm confused as to what the latent variables z \"mean\".  It seems strange that there is a 1-d latent variable representing the motion of each part.  Translation of a segment within an image is 2D.  3D if you include planar rotation, then there's scaling motion and out-of-plane rotations, so it seems an odd design choice that motion should be squeezed into a 1D representation.\n\nI find it difficult to assess the quality of the \"next frame predictions\".  There's lots other literature on next-frame prediction to compare against (e.g. https://arxiv.org/abs/1605.08104).  At least you could compare to a naive baseline of simply shifting pixels based on the optical flow.  \n\nI'm confused about how you are learning the \"estimated flow\".  My impression is that the input flow is calculated between the last 2 frames $\\hat M = flow(I_{t-1}, I_t)$.  And that the \"estimated\" flow is an estimate of $flow(I_{t}, I_{t+1})$.  But in Section 4.3 you seem to indicate that the \"estimated\" flow is just trained to \"reconstruct\" the input flow.... In that case why not just feed the input flow directly into the Image Decoder?  What I guess you're doing is trying to Predict the next flow ($flow(I_{t}, I_{t+1})$) but if you're doing this neither Figure 3 nor Section 4.2 indicates this, hence my confusion.  \n\n==== Details ====\n\nFigure 3: \n----\nThe \"shapes\" example is odd, because it's not obvious that there's a structural hierarchy linking the shapes.  Maybe a \"torso/left arm/right arm\" would be better for illustrative purposes?\nIt would be helpful to put the variable names ($\\mathcal M_k$, etc) on the figure.\nShould add a second arrow coming into the (f) the Structural descriptor from a leaf-variable $p_k$\nAlso it would be helpful to indicate where the losses are evaluated.\n\"Next Frame\" should probably be \"Reconstruction\" (though \"Prediction\" might be a more accurate word).\n---\n\nSection 4.2:\nNotational point, it seems k can be from 1 to d.  But in Section 3 you say it can be from 1 to \"n\". Maybe it would be clearer to change both (\"n\" and \"d\") to \"K\" to emphasize that \"k\" is an index which goes up to \"K\".  (edit... now after reading 5.1: Latent Representation, I understand.  If there are n parts in the dataset you use d>n dimensions and the network learns to \"drop\" the extra ones... it would help to clarify that here).\nStructural Descriptor: You say \"in practice, we relax the binary constraints\"... Yet this is important.. should write down the equation and say how the \"soft\" version of [i \\in S] calculated.\nSection 4.3\n\"elbo\" seems like the wrong name for this loss, as it's not an Evidence Lower BOund.  The elbo would be the sum of this loss and the first component of L_{recon}.  It is a regularizing term, so you could call it L_reg.\nIt's not obvious that sparse local motion maps mean a heirarchical tree structure, but I see how it could help.  I suggest that without evidence for this loss you soften the claim to \"this is intended to help encourage the model to learn a heirarchical tree structure\"\nFigure 4: \nIt appears that each row indicates a different video in the dataset, but then in 4f you still have two rows but they appear to correspond to different algorithms... a vertical separator here might help show that the rows in 4f do not correspond to the rows in 4a-e.\n\"Next Frame\" here appears to refer to ground truth, but in Figure 3 \"Next Frame\" appears to be the prediction (which you call reconstruction). \nSection 5.1\n\nFuture Prediction: No explanation either here or in Figure 4 of what it actually shows.  (What would a failure look like?)\nHierarchical structure... You binarize... how?\nFigure 9:\nWhat does this show?  The figure does not obviously demonstrate anything.  Maybe compare to ground-truth future frames?\nSection 5.3:\nFuture Prediction: These images are from the test set, right?  If so, that is worth mentioning. \nObject Segmentation (\"in several different dimensions\"  -> \"corresponding to the active latent dimensions?\")\nObject Segmentation: Visually, it looks nice, but I have now idea how good this segmentation is.  You compare verbally to R-NEM and PSD, but there're no numbers.\nHuman Studies... The line \"For those whose predicted tree structures are not consistent with ours, they all agree with our results and believe ours are more reasonable than others\" .. brings to mind an image of a room full of tortured experimental subjects not being allowed to leave until they sign a confession that their own tree structures were foolish mistakes and your tree structures are far superior.... So probably it should be removed because it sounds shady. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting and novel works but only tested on simple dataset",
            "review": "The paper proposes an unsupervised learning model that learns to (1) disentangle object into parts, (2) predict hierarchical structure for the parts and (3), based on the disentangled parts and the hierarchy, predict motion. The model is trained to predict future frames and motion with L2 loss given current frame and observed motion. The overall objective is similar to a standard VAE. \n\nOne interesting module proposed in this work is the structural descriptor which assumes motions are additive and global motion of an object part can be recovered by adding the local motion of this object with the global motions of its parents. The equation can be applied recursively and it generalizes to any arbitrary hierarchy depth.\n\nPros:\nThe overall methodology is quite novel and results look good. Merging hierarchical inference into the auto-encoder kind of structure for unsupervised learning is new.\nThe results are tested on both synthetic and real videos.\n\nCons:\nThe idea is only tested on relatively simple dataset. For the synthetic data, the objects only have very restrictive motions (ex. Circles always move diagonally). It is also unclear to me whether all the samples in the dataset share the same hierarchical tree structure or not (For human, it seems like every sample uses the same tree).  If this is the case, then it means you need 100K data to learn one hierarchical relationship for very simple video.\nFrom the human dataset results, since the appearance and motions become so different across videos, making the video clean and making the objects aligned (so that motions are aligned) seems important to make the model work. For example, from figure 11(f)(g), the right and left legs are exchanged for the person on the top. This brings up the concern that the model is fragile to more complicated scenes such that objects are not super aligned and the appearances differ a lot. (ex. Videos of people playing different sports shooting from different views)\nShould add more baselines. There are a lot of unsupervised video prediction works which also unsupervisedly disentangle motions and contents. \n\nOthers:\nThe sparsity constraint seems incorrect\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea",
            "review": "The paper describes a method, which learns the hierarchical decomposition of moving objects into parts without supervision, based on prediction of the future. A deep neural network is structured into a sequence of encoders and decoders: the input image is decomposed into objects by a trained head, then motion is estimated from predicted convolutional kernels whose model is trained on optical flow; the latent motion output is encoded into separated motion fields for each object and then composed into a global model with a trainable structured matrix which encodes the part hierarchy. The latent space is stochastic similar to VAEs and trained with similar losses.\n\nStrengths:\n\nThe idea is interesting and nicely executed. I particularly appreciated the predicted kernels, and the trainable structure matrix. Although the field of hierarchical motion segmentation is well studied, up to my knowledge this method seems to be the first of its kind based on a fully end-to-end trainable method where the motion estimators, the decomposition and the motion decoders are learned jointly.\n\nThe method is evaluated on different datasets including fully synthetic ones with synthetic shapes or based on MNIST; very simple moving humans taken from ATARI games, and realistic humans from two different pose datasets. The motion decomposition is certainly not as good as the definition and the output of a state of the art human pose detector; however, given that the decomposition is discovered, the structure looks pretty good.\n\nWeaknesses\n\nI have two issues with the paper. First of all, although the related work section is rich, the methods based on hierarchical motion decompositions are rarer, although the field is quite large. Below are a couple of references:\n\nMihir Jain, Jan Van Gemert, Hervé Jégou, Patrick Bouthemy, and Cees GM Snoek. Action localization with tubelets from motion. CVPR, 2014.\n\nChenliang Xu and Jason J Corso. Evaluation of super-voxel methods for early video processing. CVPR, 2012.\n\nJue Wang, Bo Thiesson, Yingqing Xu, and Michael Cohen. Image and video segmentation by anisotropic kernel mean shift. ECCV, 2004 \n\nChenliang Xu, Caiming Xiong, and Jason J Corso. Streaming hierarchical video segmentation. ECCV 2012.\n\nMatthias Grundmann, Vivek Kwatra, Mei Han, and Irfan Essa. Efficient hierarchical graph-based video segmentation. CVPR, 2010.\n\nPeter Ochs, Jitendra Malik, and Thomas Brox. Segmentation of moving objects by long term video analysis. IEEE PAMI, 2014.\n\nDiscovering motion hierarchies via tree-structured coding of trajectories\nJuan-Manuel Pérez-Rúa, Tomas Crivelli, Patrick Pérez, Patrick Bouthemy, BMVC 2016.\n\nSamuel J Gershman, Joshua B Tenenbaum, and Frank Jäkel. Discovering hierarchical motion structure. Vision Research, 2015.\n\nSecondly, the presentation is not perfect. The paper is densely written with lots of information thrown rapidly at the reader. Readers familiar with similar work can understand the paper (I needed a second pass). But many parts could be better formulated and presented.\n\nI understood the equations, but I needed to ignore certain thinks in order to understand them. One of them is the superscript in the motion matrices M, which does not make sense to me. “g” seems to indicate “global” and “l” local, but then again a local parent matrix gets index “g”, and this index seems to switch whether the same node is seen as the current node or the parent of its child. \n\nFigure 3 is useful, but it is hard to make the connection with the notation. Symbols like z, M etc. should be included in the figure.\n\nThe three lines after equations 2 and 3 should be rewritten. They are understandable but clumsy. Also, we can guess where the binary constraints come from, but they should be introduced nevertheless.\n\nIn essence, the paper is understandable with more efforts than there should be necessary.\n\n\nOther remarks:\n\nThe loss L_struct is L_2, I don’t see how it can favor sparsity. This should be checked and discussed.\n\nA symbolic representation is mentioned in the introduction section. I am not sure that this notion is completely undisputed in science, it should at least not be presented as a fact.\n\nThe ATARI dataset seems to be smallish (a single video and 5000 frames only).\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Overall interesting approach and well written paper but limited experimental results",
            "review": "This paper presents a method for learning about the parts and motion dynamics of a video by trying to predict future frames.  Specifically, a model based on optical flow is defined, noting that the motion of hierarchically related parts are additive.  Flow fields are represented using an encoder/decoder architecture and a binary structural matrix encodes the representations between parts.  This matrix is predicted given the previous frame and flow field.  This is then used to estimate a new flow field and generate a possible future frame.  The system is trained to predict future frames using an L2 loss on the predicted image and motion field and regularized to prefer more compact and parsimonious representations.\n\nThe method is applied to synthetic datasets generated by moving shapes or MNIST digits and shown to work well compared to some recent baseline methods for part segmentation and hierarchy representation.  It is also applied and qualitatively evaluated for future frame prediction on an atari video game and human motion sequences.  The qualitative evaluation shows that part prediction is plausible but the results for future frame prediction are somewhat unclear as there are no baseline comparisons for this aspect of the task.\n\nOverall the approach seems very interesting and well motivated.   However, the experimental comparisons are limited and baselines are lacking.  Further, some relevant related work is missing.\n\nSpecific concerns:\n- Motion segmentation has been studied for a long time in computer vision, a comparison against some of these methods may be warranted.  See, e.g., Mangas-Flores and Jepson, CVPR 2013.\n- There is some missing related work on learning part relations.  See, e.g., Ross, et al IJCV 2010 and Ross and Zemel JMLR 2006.\n- There is also some missing work on future frame prediction.  In particular, PredNet seems relevant to discuss in the context of this work and as a baseline comparison method.  See Lotter et al ICLR 2017.\n- A reasonable baseline might be simply to apply the previous frames motion field to generate the next frame.  This would be a good comparison to include.\n- The \"Human Studies\" section is very unclear.  How is \"same tree structure\" defined exactly and how were humans asked to annotate the tree structure?  If it's about the hierarchical relationship, then I would expect humans to always be pretty consistent with the hierarchy of body parts and suggests that the model is doing relatively poorly.  If it's some other way, then this needs to be clarified.  Further, how was this study performed?  If this section can't be thoroughly explained it should be removed from the paper as it is at best confusing and potentially very misleading.\n- The system only considers a single frame and flow-field for part prediction.  From this perspective, the effectiveness of the method seems somewhat surprising.\n- The system takes as input both a frame and a flow field.  I assume that flow field is computed between I0 and I1 and not I1 and I2, however this is never specified anywhere I can find in the manuscript.  If this is not the case, then the problem setup is (almost) trivial.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}