{
    "Decision": {
        "metareview": "\nPros:\n- Great work on getting rid of the need for QP and the corresponding proof of the update rule\n- Mostly clear writing\n- Good experimental results on relevant datasets\n- Introduction of a more reasonable evaluation methodology for continual learning\n\nCons:\n- The model is arguably a little incremental over GEM.  In the end I think all the reviewers agree though that the practical value of a considerably more efficient and easy to implement approach largely outweighs this concern.\n\nI think this is a good contribution in this area and I recommend acceptance.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Useful improvement over GEM and a good evaluation methodology"
    },
    "Reviews": [
        {
            "title": "A-GEM is a a clear improvement over the previous approach (GEM)",
            "review": "The paper is well-written, with the main points supported by experiments.  The modifications to GEM are a clear computational improvement.\n\nOne complaint: the \"A\" in A-GEM could stand for \"averaging\" (over all task losses) or \"approximating\" (the loss gradient with a sample).  Both ideas are good.  However, the paper does not address the question: how well does GEM do when it uses a stochastic approximation to each task loss?  (Note I'm not talking about S-GEM, which randomly samples a task constraint; rather, approximate each task's constraint by sampling that task's examples).\n\nAnother complaint: reported experimental results lack any associated idea of uncertainty, confidence interval, empirical variation, etc.  Therefore it is unclear whether observed differences are meaningful.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Computational improvement of the GEM algorithm for lifelong learning ",
            "review": "Summary of the paper:\n\nThis paper focuses on the problem of lifelong learning for multi-task\nneural networks. The goal is to learn in a computationally and memory\nefficient manner new tasks as they are encountered while at the same\ntime remembering how to solve previously seen tasks with a focus on\nhaving only one training pass through all the training data. The paper\nbuilds on the GEM method introduced in the paper \"Gradient episodic\nmemory for continuum learning\", NIPS 2017.\n\nThe main novelty over the original GEM paper is that A-GEM simplifies\nthe constraints on what constitutes a feasible update step during its\nSGD training so that GEM's QP problem is replaced by a couple of\ninner-products (and thus makes A-GEM much more computationally\nefficient). This simplification also means that only one gradient\nvector (the average gradient computed from the individual gradients of\nthe task loss of the previously seen tasks) has to be stored at each\nupdate as opposed to GEM where each task specific gradient vector has\nto be stored. Thus the memory requirements of A-GEM is much less than\nGEM and is independent of the number of already learnt tasks.\n\nThe paper then presents experimental evidence that A-GEM does run much\nfaster and uses less memory and results in performance similar to the\noriginal GEM strategy. The latter point is important as the simplified\nA-GEM algorithm - which adjusts the network's parameter to improve\nperformance on the current task while ensuring the average performance\non the previously seen tasks should not decrease - does not guarantee\nas stringently as GEM that the network does not forget how to perform\nall the previous tasks.\n\nThe paper also introduces an extra performance metric is introduced\n  called the \"Learning Curve Area\" which measures how quickly a new\n  task is learnt when it is presented with new material.\n\n\nPros and Cons of the paper:\n\n+/- The paper presents a simple intuitive extension to the original GEM\npaper that is much more computationally efficient and is thus more\nsuited and feasible for real lifelong learning applications. And it\nshows that performance exceeds other methods that have similar\ncomputational demands. The paper can be viewed as somewhat incremental\nbut the increment is probably crucial for any real-world practical\napplication.\n\n+ The validity of the approach is demonstrated experimentally on\n  standard datasets in the field.\n\n\n- Some of the presentation of the material is somewhat vague, in\n  particular section 5. In this section a joint embedding model is\n  described that helps facilitate zero-shot learning. However, not\n  enough detail is given to fully understand or appreciate this\n  contribution, see below for details.\n\n\nRationale for my evaluation:\n\nThe method is somewhat incremental, however, this increment could be\nquite practically important. The presentation is lacking in some regard and would benefit\n from some re-working i.e. section 5. \n \n\nUnclear in the paper:\n\nSection 5 describing the \"Joint Embedding Model Using Compositional Task descriptors\" is very sparse on detail.  Here are some of the details that I feel are missing:\n- In the experiments how is the matrix description (via attributes) of the different tasks $t^k$ learnt/discovered?\n- The size of this attribute matrix is able to vary from one task to the next. How does the function $\\psi_{\\omega}$ deal with this problem?\n- What functions are used in the experiments to represent $\\psi_{\\omega}$?\n- In the second last line of paragraph 2 should $C$ be $C_k$? If it should be $C$ how is $C$ chosen?\n- In equation (12) should the $c$th column of $\\psi_{\\omega}$ be extracted as opposed to the $k$th column?\n\nRepresentative labelled samples from each task are stored in memory\nand these are used when learning for a new task. The system\nhas a fixed memory so when a new task is added then the number of\nimages stored for each task has to be reduced. Then uniform sampling is\nused to randomly decide which images to keep. Could this selection\nprocess be improved upon and would any such improvement have any large\nimpact on performance?\n\nTypos and minor errors spotted:\n\nIn the third paragraph of section 2 it is stated $T^{CV} \\ll T$ in the\nexperiments performed this is not case. I don't think 3 is much less\nthan 10 or 20.\n\nIn figures 4 and 5 it is not entirely clear which curves correspond to\nA-GEM and A-GEM-JE from the legend. In the legend the dashed line with\nthe triangle looks the same the non-dashed line with the triangle. I\npresuming A-GEM is the non-dashed line, but only because that makes\nthings consistent with the previous figures.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "There are some interesting ideas here. A more efficient version of GEM. ",
            "review": "This paper proposes a variant of GEM called A-GEM that substantially improves the computational characteristics of GEM while achieving quite similar performance. To me the most interesting insight of this work is the proof that an inner product between gradients can suffice instead of needing to solve the quadratic program in GEM â€“ which I have found to be a major limitation of the original algorithm.  The additional experiments using task descriptors to enable zero shot learning are also interesting.  Moreover, the discussion of the new evaluation protocol and metrics make sense with further clarification from the authors. Overall, I agree with the other reviewers that this paper makes a clear and practical contribution worthy of acceptance. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}