{
    "Decision": {
        "metareview": "This paper proposes an efficient method to compute the singular values of the linear map represented by a convolutional layer. It makes uses of the special block-matrix form of convolutional layers to construct their more efficient method. Furthermore, it shows that this method can be used to devise new regularization schemes for DNNs. The reviewers did note that the diversity of the experiments could be improved, and R2 raised concerns that the wrong singular values were being computed. The authors should add a section clarifying why the singular values of a convolutional linear map are not found directly by performing SVD on the reshaped kernel - indeed the number of singular values would be wrong. A contrast with the singular values obtained by simple reshaping of the kernel would also be helpful.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "Meta-review"
    },
    "Reviews": [
        {
            "title": "This is an interesting work with huge potential.",
            "review": "The paper is dedicated to computation of singular values of convolutional layers. While singular values of convolutional layers represent sufficient interest for researchers, huge computational complexity made it difficult to investigate their properties in the case of layers of deep neural networks. Using the fact that operator matrix of the convolutional layer has a special form (i.e. can be represented as block-matrix, which blocks are doubly block circulant matrices) the authors proposed a more efficient method of computation of singular values. I really enjoyed reading this paper and I think that it opens a lot of interesting applications. As one of the possible applications the authors proposed a regularization method based on bounding of singular values.\n\nThe paper from my point of view has two main drawbacks:\n\n1.  Diversity of experiments. While the paper has strong theoretical component, the part dedicated to experiments is not broad enough. It would be interesting to see regularization on other architectures and other datasets.\n\n2.The system of references. I would recommend to add not only references to the sources, but also to the theorem numbers or the chapters. For example, I would recommend to replace ‘Poposition 9 ((Lefkimmiatis et al., 2013))’ with ‘Poposition 9 ((Lefkimmiatis et al., 2013, Proposition 1))’. In pure math papers, it is a standard rule to add such additional information since many papers contain a lot of theorems and it significantly simplifies reading and understanding the paper.\n\nDespite these disadvantages this is a great work with huge potential.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper studies the problem on computing the spectrum of singular values of linear convolutional layers. This is an important problem with abundant applications on regularizing deep neural networks. However, there are several technical issues need to be addressed in its current form. \n\nFirst, in the section \"Summary of Results\", at first read of the paper I found it very confusing why the time complexity of computing the spectrum is a function of n, where n is the size of the input feature map. Intuitively, since the size of the convolutional kernel is m x m x k x k, it is expected that the time complexity is expressed as a function of (m, k). Later I realized that this is due to the unnecessary and redundant 0 padding in section 2.1 that leads to this artifact. I understand that in order to apply the described Fourier transform technique it is necessary to introduce the large nxn filter, which is of the same size as the input, but it also introduces redundant computation. This fact further emerges in the introduction of matrix A in Eq (1). \n\nMore importantly, I think the authors didn't perform a detailed analysis on using the basic definition of convolutional filter to compute its spectrum, and this is the reason why they reached a misleading conclusion that simple SVD takes O(n^6 m^3) time. Specifically, each convolution operation corresponds to a inner product operation, so we can reshape the input 3D tensor with shape m x n x n into a 2D matrix, with shape n^2 x mk^2, denoted as X. Note that this creates a unnecessary redundancy in the input feature map, but it does not create redundant weight for the convolutional kernel. As a comparison, the introduced matrix A in the paper is heavily redundant. Similarly, for m channels, we can reshape the 4D convolutional kernel with shape m x m x k x k into a 2D matrix, with shape mk^2 x m, denoted as K. Then the usual convolution layer can be described as the following linear system: Y = X K, where Y with shape n^2 x m is the output, and can be easily reshaped into size m x n x n. Hence to compute the spectrum of the convolution layer corresponds to computing the singular values of the 2D matrix K with size mk^2 x m. Hence a naive application of SVD directly gives us the solution in time O(m^3 k^2) (Note that the time complexity of SVD for matrix with size a x b is O(min{a^2 b, a b^2})), which is much smaller than the one given in the paper O(m^3 n^2) since k << n. \n\nIn experiment the authors made unfair comparison between their proposed method and the full matrix method: the full matrix A is fully redundant, due to its circulant pattern. As this implies a highly redundant information, nobody will form and compute matrix A explicitly in practice. So the time improvements demonstrated in the experiment section are meaningless. A valid baseline would be to compare the proposed method with the one introduced above. But in this case I would imagine the proposed method to be worse due to its unnecessary 0 padding leading to the worst time complexity. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "review": "In this paper, the authors derive exact formulas for computing singular values of convolutional layers of deep neural networks. By appealing to fast FFT transformations, they show that computing the singular values can be done much faster than computing the full SVD of the convolution matrix. This obviates the needs to approximate the singular values. They use these results to then devise regularization schemes for DNN layers, and show that employing this regularization helps with model performance. \n\nThey show that the algorithm with the operator norm regularization can be solved via an alternating projection scheme. They also postulate that since this might be expensive and unnecessary, one can also perform just 2 projections after every few SG iterations, and claim that this acts as a 'warm start' for subsequent iterations. Experiments reveal that this does not degrade the performance too much. \n\n\nThe paper is well written and easy to understand. The proofs follow from standard linear algebra methods, and are easy to follow. ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}