{
    "Decision": {
        "metareview": "Strengths: The paper tackles a novel, well-motivated problem related to options & HRL.\nThe problem is that of learning transition policies, and the paper proposes\na novel and simple solution to that problem, using learned proximity predictors and transition\npolicies that can leverage those. Solid evaluations are done on simulated locomotion and\nmanipulation tasks. The paper is well written.\n\nWeaknesses: Limitations were not originally discussed in any depth. \nThere is related work related to sub-goal generation in HRL.\nAC: The physics of the 2D walker simulations looks to be unrealistic;\nthe character seems to move in a low-gravity environment, and can lean\nforwards at extreme angles without falling. It would be good to see this explained.\n\nThere is a consensus among reviewers and AC that the paper would make an excellent ICLR contribution.\nAC: I suggest a poster presentation; it could also be considered for oral presentation based\non the very positive reception by reviewers.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Well motivated problem; good solution"
    },
    "Reviews": [
        {
            "title": " Useful  learning scheme for transitioning between options in continuous domains.",
            "review": "The paper proposes a scheme for transitioning to favorable starting states for executing given options in continuous domains. Two learning processes are carried out simultaneously: one learns a proximity function to favorable states from previous trajectories and executions of the option,  and the other learns the transition policies based on dense reward provided by the proximity function.\n\t\nBoth parts of the learning algorithms are pretty straightforward, but their combination turns out to be quite elegant. The experiments suggest that the scheme works,  and in particular does not get stuck in local minima. \n\nThe experiments involve fairly realistic robotic applications with complex options,  which renders credibility to the results.    \n\nOverall this is a nice contribution to the options literature. The scheme itself is quite simple and straightforward, but still useful. \n\nOne point that I would like to see elaborated is the choice of exponential (\"discounted\") proximity function. Wouldn't a linear function of \"step\" be \n more natural here? The exponent loses sensitivity as the number of steps away increases, which may lead to sparser rewards.\n  \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An elegant method with comprehensive evaluations",
            "review": "The paper presents a method for learning policies for transitioning from one task to another with the goal of completing complex tasks. In the heart of the method is state proximity estimator, which measures the distance between states in the originator and destination tasks. This estimator is used in the reward for the transition policy. The method is evaluated on number of MojoCo tasks, including locomotion and manipulation.\n\nStrengths:\n+ Well motivated and relevant topic. One of the big downsides in the current state of the art is lack of understanding how to learn complex tasks. This papers tackles that problem.\n+ The paper is well written and the presentation is clear.\n+ The method is simple, yet original. Overall, an elegant approach that appears to be working well.\n+ Comprehensive evaluations over several tasks and several baselines.\n\nQuestions:\n- In the metapolicy, what ensures consistency, i.e. it selects the same policy in the consecutive steps?\n- Can the authors comment on the weaknesses and the limits of the method?",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Potentially very useful idea",
            "review": "** Summary **\nThe authors propose a new training scheme with a learned auxiliary reward function to optimise transition policies, i.e. policies that connect the ending state of a previous macro action/option with good initiation states of the following macro action/option.\n\n** Quality & Clarity **\nThe paper is well written and features an extensive set of experiments.\n\n** Originality **\nI am not aware of similar work and believe the idea is novel.\n\n** Significance **\nSeveral recent papers have proposed to approach the topic of learning hierarchical policies not by training the hierarchy end-to-end, but by first learning useful individual behavioural patterns (e.g. skills) which then later can be used and sequentially chained together by higher-level policies. I believe the here presented work can be quite helpful to do so as the individual skills are not optimised for smooth composition and are therefore likely to fail when naively used sequentially.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}