{
    "Decision": {
        "metareview": "The paper presents a novel idea with a compelling experimental study. Good paper, accept.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Accept"
    },
    "Reviews": [
        {
            "title": "A novel RNN architecture",
            "review": "This paper introduces antisymmetric RNN, a novel RNNs architecture that is motivated through ordinary differential equation (ODE) framework. Authors consider a first order ODE and the RNN that results from the discretization of this ODE. They show how the stability criteria with respect to perturbation of  the initial state results in an ODE  in a trainability criteria for the corresponding  RNN. This criteria ensures that there are  no exploding/vanishing gradients. Authors then propose a specific parametrization, relying on antisymmetric matrix to ensure that the stability/trainability criteria is respected. They also propose a gated-variant of their architecture.  Authors evaluate their proposal on pixel-by-pixel MNIST and CIFAR10 where they show they can outperforms an LSTM.\n\nThe paper is well-written and pleasant to read. However, while the authors argue that their architecture allows to mitigate vanishing/exploding gradient, there is no empirically verification of this claim. In particular, it would be nice to visualize how the gradient norm changes as the gradient is  backpropagated in time, compare the gradient flows of Antisymmetric RNN with a LSTM or report the top eigenvalue of the jacobian for the different models.\n\nIn addition,  the analysis for the antisymmetric RNN assumes no input is given to the model. It is not clear to me how having an input at each timestep affects those results?\n\nA few more specific questions/remarks:\n-\tExperimentally, authors find that the gated antisymmetric RNN sometime outperforms its non-gated counterpart. However, one motivation for the gate mechanism is to better control the gradients flow. It is unclear to me what is the motivation of using gate for the antisymmetric RNN ?\n-\tas the proposed RNN relies on a antisymmetric matrix to represent the hidden-to-hidden transition matrix, which has less degree of liberty, can we expect the antisymmetric RNN to have same expressivity as a standard RNN. In particular, how easily can an antisymmetric RNN forgets information ?\n-\tOn the pixel-by-pixel MNIST, authors report the Arjosky results for the LSTM baseline.\nNote that some papers reported better performance for the LSTM baseline such as Recurrent Batch Norm (Cooijman et al., 2016) .\n\nAntisymmetric RNN appears to be well-motived architecture and seems to outperforms previous RNN variants that also aims at solving exploding/vanishing gradient problem. Overall I lean toward acceptance, although I do think that adding an experiment explicitly showing that the gradient does not explode/vanish would strengthen the paper. \n\n\n* Revision\n\nThanks for your response,  the paper new  version address my main concerns, I appreciate the new experiment looking at the eigenvalues of the  end-to-end Jacobian which clearly shows the advantage of the AntisymmetricRNN.\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good paper with original work, experiments could be improved",
            "review": "In this paper, the authors provide a new approach to analyze the behavior of\nRNNs by relating the RNNs with ODE numerical schemes. They provide analysis on\nthe stability of forward Euler scheme and proposed an RNN architecture called\nAntisymmetricRNN to solve the gradient exploding/vanishing problem. \n\nThe paper is well presented although more recent works in this direction\nshould be cited and discussed. Also, some important issues are omitted and not\nexplained. \nFor example, the analysis begins with \"RNNs with feedback\" rather than vanilla\nRNN, since vanilla RNN does not have the residual structure as eq(3). The\nauthors should note that clearly in the paper. \n\nAlthough there are previous works relating ResNets with ODEs, such as [1],\nthis paper is original as it is the first work that relates the stability of\nODE numerical scheme with the gradient vanishing/exploding issues in RNNs. \n\nIn general, this paper provides a novel approach to analyze the gradient\nvanishing/exploding issue in RNNs and provides applicable solutions, thus I\nrecommend to accept it. \n\n\nDetailed comments:\n\nThe gradient exploding/vanishing issue has been extensively studied these\nyears and more recent results should be discussed in related works.\nAuthor mentioned that existing methods \"come with significant computational\noverhead and reportedly hinder representation power of these models\". However\nthis is not true for [2] which achieves full expressive power with\nno-overhead. \nIt is true that \"orthogonal weight matrices alone does not prevent exploding\nand vanishing gradients\", thus there are architectural approaches that can\nbound the gradient norm by constants [3]. \n\nThe authors argued that the critical criterion is important in preserving the\ngradient norm. However, later on added a diffusion term to maintain the\nstability of forward Euler method. Thus the gradient will vanish\nexponentially w.r.t. time step t as: (1-\\gamma)^t. Could the authors provide\nmore detailed analysis on this issue? \n\nSince eq(3) cannot be regarded as vanilla RNN, it would be better begin the\nanalysis with advanced RNN architectures that fit in this form, such as\nResidual RNN, Statistical Recurrent Units and Fourier Recurrent Units. \n\nWhy sharing the weight matrix of gated units and recurrent units? Is there any\nother reason to do this other than reducing the number of parameters?\n\nMore experiment should be conducted on real applications of RNN, such as\nlanguage model or machine translation. \n\n\n[1] Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond finite layer\nneural networks: Bridging deep architectures and numerical differential\nequations. In ICML, pp. 3276â€“3285, 2018.  \n\n[2] Zhang, Jiong, Qi Lei, and Inderjit S. Dhillon. \"Stabilizing Gradients for\nDeep Neural Networks via Efficient SVD Parameterization.\" In ICML, pp.\n5806-5814, 2018.\n\n[3] Zhang, Jiong, Yibo Lin, Zhao Song, and Inderjit S. Dhillon. \"Learning Long\nTerm Dependencies via Fourier Recurrent Units.\" In ICML, pp. 5815-5823, 2018. \n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "capacity of long-term storage?",
            "review": "This is an interesting paper which proposes a novel angle on the problem of learning long-term dependencies in recurrent nets. The authors argue that most of the action should be in the imaginary part of the eigenvalues of the Jacobian J=F' of the new_state = old_state + epsilon F(old_state, input) incremental type of recurrence, while the real part should be slightly negative. If they were 0 the discrete time updates would still not be stable, so slightly negative (which leads to exponential loss of information) leads to stability while making it possible for the information decay to be pretty slow. They also propose a gated variant which sometimes works better. \n\nThis is similar to earlier work based on orthogonal or unitary Jacobians of new_state = H(old_state,input) updates, since the Jacobian of H(old_state,input) = old_state + epsilon F( old_state,input) is I + epsilon F'. In this light, it is not clear why the proposed architecture would be better than the partially orthogonal / unitary variants previously proposed. My general concern with this this type of architecture is that they can store information in 'cycles' (like in fig 1g, 1h) but this is a pretty strong constraint. For example, in the experiments, the authors did not apparently vary the length of the sequences (which would break the trick of using periodic attractors to store information). In practical applications this is very important. Also, all of the experiments are with classification tasks with few categories (10), i.e., requiring only storing 4 bits of information. Memorization tasks requiring to store many more bits, and with randomly varying sequence lengths, would better test the abilities of the proposed architecture.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}