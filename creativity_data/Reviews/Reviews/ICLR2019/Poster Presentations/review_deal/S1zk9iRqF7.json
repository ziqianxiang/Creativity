{
    "Decision": {
        "metareview": "This paper improves upon the PATE-GAN framework for differentially-private synthetic data generation. They eliminate the need for public data samples for training the GAN, by providing a distribution which can be sampled from instead.\n\nThe authors were unanimous in their vote to accept.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Advances in differentially-private data generation"
    },
    "Reviews": [
        {
            "title": "Interesting setup, surprising that it works",
            "review": "This paper considers using a GAN to generate synthetic data in a differentially private manner [see also https://www.biorxiv.org/content/early/2018/06/05/159756 ]. The key novelty is the integration of the PATE differential privacy framework from recent work. Specifically, rather than a single distinguisher as is usual in a GAN, there is a \"student distinguisher\" and several \"teacher distinguishers\". The student distinguisher is used as usual except that it does not have access to the real data, only the teacher distinguishers have access to the real data (as well as the synthetic data). The data is partitioned amongst the teacher distinguishers and their output is aggregated in a differentially private manner (and gradients are not revealed). The role of the teacher distinguishers is solely to correct the student distinguisher when it errs.\n\nWhat is strange about this setup is that the generator's only feedback is from the gradients of the student distinguisher, which is never exposed to the real data. The entire training process relies on the generator producing realistic data by chance at which point the teacher distinguishers can provide positive feedback. (The paper remarks about this in the middle of page 5.) It's surprising that this works, but there are experimental results to back it up.\n\nI think it would be appropriate to remark that generating private synthetic data is known to be hard in the worst case [ https://eccc.weizmann.ac.il/report/2010/017/ ] and therefore it is necessary to use techniques like GANs.\n\nOverall, I think the paper is interesting, well written, novel, and therefore appropriate for ICLR.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Need improvement",
            "review": "[Post revision update] The authors' comments addressed my concerns, especially on the experiment side. I changed the score.\n\nThis paper applies the PATE framework to GAN, and evaluates the quality of the generated data with some predictive tasks. The experimental results on some real datasets show that the proposed algorithm outperforms DPGAN, and the generated synthetic data is quite useful in comparison with real data.\nThe presentation is clear and easy to follow. However, I think the paper needs to be improved in its novelty, and the techniques and experiments need to be more thorough.\n\nMore details:\n- It might be necessary to consider using Gaussian noise[24] in replace of the Laplace noise, which, according to [24], would improve privacy and accuracy.\n- This paper:\n“Privacy-preserving generative deep neural networks support clinical data sharing” by Brett K. Beaulieu-Jones, Zhiwei Steven Wu, Chris Williams, Casey S. Greene\nseems quite relevant. If so, you may want to add some discussion in the related work section or compare with their result.\n- The last paragraph of the related works section mentioned some related work with shortcomings as working only on low-dimensional data and features of specific types, yet the experiments are also mostly done on low-dimensional datasets. I think it would be better to do a thorough evaluation on data of different kinds, such as image data. \n- If the two evaluation metrics for private GAN is considered an important contribution of the paper, it might be better to make it a separate section and elaborate more on the motivation and method.  \n- It might be better to move some details (for example, instead of presenting the results of the 12 predictive models, presenting only the average, as it’s not very important how each of them performs) of the credit card fraud detection dataset to the appendix and bring the results of the other datasets to the main body. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Differenntially  private synthetic data set generation via combining the PATE framework and GAN",
            "review": "The paper studies the problem of generating synthetic datasets (while ensuring differential privacy) via training a GAN. One natural approach is the teacher-student framework considered in the PATE framework.  In the original PATE framework, while the teachers are ensured to preserve differential privacy, the student model (typically a GAN) requires the presence of publicly data samples. The main contribution of this paper is to get around the requirement of public data via using uniformly random samples in [0,1]^d.\n\nDifferentially private synthetic data generation is clearly an important and a long-standing open problem. Recently, there has been some work on exploiting differentially private variants of GANs to generate synthetic data. However, the scale of these results is far from satisfactory. The current paper claims to bypass this issue by using the PATE-GAN approach.\n\nI am not an expert on deep learning. The idea of bypassing the use of public data by taking uniformly random samples seems interesting. In my view, these random vectors are used in the GAN as some sort of a basis. It is interesting to see if this result extends to high-dimensional settings (i.e., where d  is very large).",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}