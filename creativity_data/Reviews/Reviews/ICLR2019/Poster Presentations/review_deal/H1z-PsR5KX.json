{
    "Decision": {
        "metareview": "Strong points:\n\n-- Interesting, fairly systematic and novel analyses of recurrent NMT models, revealing individual neurons responsible for specific type of information (e.g., verb tense or gender)\n\n-- Interesting experiments showing how these neurons can be used to manipulate translations in specific ways (e.g., specifying the gender for a pronoun when the source sentence does not reveal it)\n\n-- The paper is well written\n\nWeak points\n\n-- Nothing serious (e.g., maybe interesting to test across multiple runs how stable these findings are).\n\nThere is a consensus among the reviewers that this is a strong paper and should be accepted.\n\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "An insightful paper presenting analyses of recurrent machine translation models"
    },
    "Reviews": [
        {
            "title": "Interesting analysis of the contributions of different neurons in NMT",
            "review": "This paper presents unsupervised approaches to discover import neurons in\nneural machine translation systems. Some linguistic properties controlled by the\ndiscovered neurons are discussed and analyzed.\n\nStrengths:\n\nThe paper is well-written and provides valuable information to understand the\nbehaviour of neural machine translation models.\n\nThe ability to control characteristics (such as gender) without training\nspecialized models is promising, even if the results are not good enough for\nimmediate use. It would be interesting to see whether controlling neurons\nin the decoder would be more effective.\n\nWeaknesses:\n\nMultiple NMT systems are necessary to discover important neurons. The authors\nmention that it would be possible to use different checkpoints from a single\nmodel, but don't evaluate how well this would work.\n\nThe findings in this paper do not lead to immediate translation performance\nimprovements.\n\nQuestions and other remarks:\n\nIn Table 4a, why are there 2 results for \"-0.25, -0.125, 0\"?\n\nIn section 4.3 (Tense), it may be worthwhile to mention that the neuron is\nhighly activated on the word \"Spreads\", even if it acts as a noun in this\nspecific sentence.\n\nBottom of p. 6: \"Our supervised methods\" -> \"Our unsupervised methods\"\n\nTo control properties, could SVCCA directions or coefficients be manipulated?\n\nSome parentheses around citations are missing or misplaced.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper presents unsupervised methods for ranking neurons in machine translation. Important neurons are thus identified and used to control the MT output.",
            "review": "Strengths:\n- even though the methods for detecting important neurons are not novel (as also stated in the paper), their application to MT is novel\n- the presentation is very clear\n- the choice of methods is well argued and justified\n- the experiments are well executed and analysed\n- thorough and varied analysis of the experimental findings \n\nI recommend this paper for the best paper award.",
            "rating": "10: Top 5% of accepted papers, seminal paper",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Well-written paper applying a method for finding individual influential neurons to MT, but insight is ultimately limited",
            "review": "The authors propose a number of methods to identify individual important neurons in a machine translation system. The crucial assumption, drawn from the computer vision literature, is that important neurons are going to be correlated across related models (e.g. models that are trained on different subsets of the data). This hypothesis is validated to some extent: erasing the neurons that scored highly on these measures reduced BLEU score substantially. However, it turns out that most of the activation of the important neurons can be explained using sentence position. Supervised classification experiments on the important neurons revealed neurons that tracked properties such as the span of parentheses or word classes (e.g., auxiliary verbs, plural nouns, etc).\n\nStrengths:\n* The paper is very well written and provides solid intuitions for the methods proposed.\n* The methods seem promising, and the degree of localist representation is striking.\n* The methods may be able to address the question of *how* localist the representations are (though no numerical measure of localism is proposed).\n* There is a correlation between the neuron importance metrics proposed in the paper and the effect on BLEU score of erasing those neurons from the network (of course, it’s not clear what particular linguistic properties are affected by this erasure - the decrease BLEU may reflect inability to track specific word tokens more than any higher-level linguistic property).\n\nWeaknesses:\n* It wasn't clear to me why the neurons that track particular properties (e.g., being inside a parentheses) couldn't be identified using a supervised classifier to begin with, without first identifying \"important\" neurons using the unsupervised methods proposed in the paper. The unsupervised methods do show their strength in the more exploratory visualization-based analyses -- as the authors point out (bottom of p. 6), the neuron that activates on numbers but only at the beginning of the sentence does not correspond to a plausible a-priori hypothesis. Still, most of the insight in the paper seems to be derived from the supervised experiments.\n* The particular linguistic properties that are being investigated in the classification experiments are fairly limited. Are there neurons that track syntactic dependencies, for example?\n* I wasn't sure how the GMMs (Gaussian mixture models) for predicting linguistic properties from neuron activations were set up.\n* It's nice to see that individual neurons function as knobs that can change the gender or tense of the output (with varying accuracy). At the same time, I was unable to follow the authors' argument that this technique could be used to reduce gender bias in MT.\n* I wasn't sure what insight was gained from the SVCCA analyses -- this method seems to be a bit of a distraction given the general focus on localist vs. distributed representation. In general, I didn’t come away with an understanding of the pros and cons of each of the methods.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}