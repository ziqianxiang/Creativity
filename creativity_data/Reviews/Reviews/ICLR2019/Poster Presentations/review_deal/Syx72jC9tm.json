{
    "Decision": {
        "metareview": "The paper provides a comprehensive study and generalisations of previous results on linear permutation invariant and equivariant operators / layers for the case of hypergraph data on multiple node sets. Reviewers indicate that the paper makes a particularly interesting and important contribution, with applications to graphs and hyper-graphs, as demonstrated in experiments. \n\nA concern was raised that the paper could be overstating its scope. A point is that the model might not actually give a complete characterization, since the analysis considers permutation action only. The authors have rephrased the claim. Following comments of the reviewer, the authors have also revised the paper to include a discussion of how the model is capable of approximating message passing networks. \n\nTwo referees give the paper a strong support. One referee considers the paper ok, but not good enough. The authors have made convincing efforts to improve issues and address the concerns. \n\n\n\n\n ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Description of linear permutation invariant and equivariant layers"
    },
    "Reviews": [
        {
            "title": "Beautiful work -- problems with experiments",
            "review": "The paper presents a maximally expressive parameter-sharing scheme for hypergraphs, and in general when modeling the high order interactions between elements of a set. This setting is further generalized to multiple sets. The paper shows that the number of free parameters in invariant and equivariant layers corresponds to the different partitioning of the index-set of input and output tensors. Experimental results suggest that the proposed layer can outperform existing methods in supervised learning with graphs.\n\nThe paper presents a comprehensive generalization of a recently proposed model for interaction across sets, to the setting where some of these sets are identical. This is particularly useful and important due to its applications to graphs and hyper-graphs, as demonstrated in experiments.\n\nOverall, I enjoyed reading the paper. My only concern is the experiments:\n\n1) Some of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al’18 and references in there) are missing.\n\n2) Applying the model of Hartford et al’18 to problems where interacting sets are identical is similar to applying convolution layer to a feature vector that is not equivariant to translation. (In both cases the equivariance group of data is a strict subgroup of the equivariance of the layer.)  Do you agree that for this reason, all the experiments on the synthetic dataset is flawed?\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Nice combinatorics, but this is not what graph neural networks actually do",
            "review": "Given a graph G of n vertices, the activations at each level of a graph neural network (G-NN) for G \ncan be arranged in an n^k tensor T for some k. A fundamental criterion is that this tensor must be equivariant \nto permutations of the vertices of G in the sense of each index of of T being permuted simultaneously. \n\nThis paper enumerates the set of all linear maps that satisfy this criterion, i.e., all linear maps \nwhich (the authors claim) can serve as the analog of convolution in equivariant G-NNs. \nThe authors find that for invariant neural networks such maps span a space of dimension just b(k), whereas \nfor equivariant neural networks they span a space of dimension b(2k).\n\nThe proof of this result is simple, but elegant. It hinges on the fact that the set of tensor elements of \nthe same equality type is both closed and transitive under the permutation action. Therefore, the \ndimensionality of the subspace in question is just the number of different identity types, i.e., \npartitions of either {1,...,k} or {1,...,2k}, depending on whether we are talking about invariance or \nequivariance.\n\nMy problem with the paper is that the authors' model of G-NNs doesn't actually map to what is used \nin practice or what is interesting and useful. Let me list my reservations in increasing order of significance.\n\n1. The authors claim that they give a ``full characterization'' of equivariant layers. This is not true. \nEquivariance means that there is *some* action of the symmetric group S_n on each layer, and wrt these actions \nthe network is equivariant. Collecting all the activations of a given layer together into a single object L, \nthis means that L is transformed according to some representation of S_n. Such a representation can always be \nreduced into a direct sum of the irreducible representations of S_n. The authors only consider the case then \nthe representation is the k'th power of the permutation representation (technically called the defining \nrepresentation of the S_n). This corresponds to a specific choice of irreducibles and is not the most general case. \nIn fact, this is not an unnatural choice, and all G-NNs that I know follow this route. \nNonetheless, technically, saying that they consider all possible equivariant networks is not correct.\n\n2. The paper does not discuss what happens when the input tensor is symmetric. On the surface this might seem \nlike a strength, since it just means that they can consider the more general case of undirected graphs (although \nthey should really say so). In reality, when considering higher order activations it is very misleading because \nit leads to a massive overcounting of the dimensionality of the space of convolutions. In the case of k=2, for \nexample, the dimensionality for undirected graphs is probably closer to 5 than 15 for example (I didn't count).\n\n3. Finally, and critically, in actual G-NNs, the aggregation operation in each layer is *not* \nlinear, in the sense that it involves a product of the activations of the previous layer with the adjacency \nmatrix (messages might be linear but they are only propagated along the edges of the graph). \nIn most cases this is motivated by making some reference to the geometric meaning of convolution,  \nthe Weisfeiler-Lehman algorithm or message passing in graphical models. In any case, it is critical that the \ngraph topology be reintroduced into the network at each layer. The algebraic way to see it is that each layer \nmust mix the information from the vertices, edges, hyperedges, etc.. The model in this paper could only aggregated \nedge information at the vertices. Vertex information could not be broadcast to neighboring vertices again. \nThe elemenary step of ``collecting vertex information from the neighbors but only the neighbors'' cannot be \nrealized in this model.\n\nTherefore, I feel that the model used in this paper is rather uninteresting and irrelevant for practical \npurposes. If the authors disagree, I would encourage them to explicitly write down how they think the model \ncan replicate one of the standard message passing networks. It is apparent from the 15 operations listed on \npage 11 that they have nothing to do with the graph topology at all.\n\nMinor gripes:\n\n- I wouldn't call (3) and (4) fixed point equations, that's usually used in dynamical systems. Here there is \nan entire subspace fixed by *all* permutations.\n\n- Below (1), they probably mean that ``up to permutation vec(L)=vec(L^T)''. \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Very interesting paper",
            "review": "This paper explores maximally expressive linear layers for jointly exchangeable data and in doing so presents a surprisingly expressive model. I have given it a strong accept because the paper takes a very well-studied area (convolutions on graphs) and manages to find a far more expressive model (in terms of numbers of parameters) than what was previously known by carefully exploring the implications of the equivariance assumptions implied by graph data. The result is particularly interesting because the same question was asked about exchangeable matrices (instead of *jointly* exchangeable matrices) by Hartford et al. [2018] which lead to a model with 4 bases instead of the 15 bases in this model, so the additional assumption of joint exchangeability (i.e. that any permutations applied to rows of a matrix must also be applied to columns - or equivalently, the indices of the rows and columns of a matrix refer to the same items / nodes) gives far more flexibility but without losing anything with respect to the Hartford et al result (because it can be recovered using a bipartite graph construction - described below). So we have a case where an additional assumption is both useful (in that it allows for the definition of a more flexible model) and benign (because it doesn't prevent the layer from being used on the data explored in Hartford et al.). \n\nI only have a couple of concerns: \n1 - I would have liked to see more discussion about why the two results differ to give readers intuition about where the extra flexibility comes from. The additional parameters of this paper come from having parameters associated with the diagonal (intuitively: self edges get treated differently to other edges) and having parameters for the transpose of the matrix (intuitively: incoming edges are different to outgoing edges). Neither of these assumptions apply in the exchangeable setting (where the matrix may not be square so the diagonal and transpose can't be used). Because these differences aren't explained, the synthetic tasks in the experimental section make this approach look artificially good in comparison to Hartford et al.  The tasks are explicitly designed to exploit these additional parameters - so framing the synthetic experiments as, \"here are some simple functions for which we would need the additional parameters that we define\" makes sense; but arguing that Hartford et al. \"fail approximating rather simple functions\" (page 7) is misleading because the functions are precisely the functions on which you would expect Hartford et al. to fail (because it's designed for a different setting). \n2 - Those more familiar of the graph convolution literature will be more familiar with GCN [kipf et al. 2016] / GraphSAGE [Hamilton et al. 2017] / Monti et al [2017] / etc.. Most of these approaches are more restricted version of this work / Hartford et al. so we wouldn't expect them to perform any differently from the Hartford et al.  baseline on the synthetic dataset, but including them will strengthen the author's argument in favour of the work. I would have also liked to see a comparison to these methods in the the classification results.\n3 - Appendix A - the 6 parameters for the symmetric case with zero diagonal reduces to the same 4 parameters from Hartford et al. if we constrained the diagonal to be zero in the output as well as the input. This is the case when you map an exchangeable matrix into a jointly exchangeable matrix by representing it as a bipartite graph [0, X; X^T, 0]. So the two results coincide for the exchangeable case. Might be worth pointing this out. \n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}