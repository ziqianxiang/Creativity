{
    "Decision": {
        "metareview": "The paper proposes a novel  lossless compression scheme that leverages latent-variable models such as VAEs. Its main original contribution is to improve the bits back coding scheme [B. Frey 1997] through the use of asymmetric numeral systems (ANS) instead of arithmetic coding. The developed practical algorithm is also able to use continuous latents. The paper is well written but the reader will benefit from prior familiarity with compression schemes. Resulting message bit-length is shown empirically to be close to ELBO on MNIST. The main weakness pointed out by reviewers is that the empirical evaluation is limited to MNIST and to a simple VAE, while applicability to other models (autoregressive) and data (PixelVAE on ImageNet) is only hinted to and expected bit-length merely extrapolated from previously reported log-likelihood. The work could be much more convincing if its compression was empirically demonstrated on larger and better models and larger scale data. Nevertheless reviewers agreed that it sufficiently advanced the field to warrant acceptance.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "Novel improved lossless compression scheme using VAEs, with limited empirical validation"
    },
    "Reviews": [
        {
            "title": "Interesting idea, evaluation could be more thorough",
            "review": "The main contribution of this paper is to propose an improvement to the bits back (BB) coding scheme by using asymmetric numeral systems (ANS) rather than arithmetic coding for the implementation. ANS is a natural fit with BB since it traverses the coded sequence stack-style rather than FIFO. A second contribution is show how generative models with continuous latent variables can be used (via discretization) within this scheme. The paper is generally well-written, and the explanation in Sec 2.4 was especially clear. However I have some questions about the evaluation and practical application of this scheme.\n\nThe comparison in Figure 1 is very compelling, but it would be helpful to have some additional information. In particular, does the size reported for BB-ANS include any overhead related to meta information (e.g., number of images stored, their dimensions, format, etc.)? PNG is a general purpose image file format, so it certainly contains such overhead. This makes it unclear how fair of a comparison we have here. Similarly, bz2 is a general purpose file compression scheme. What file format were the images written as before being compressed? Either of those cases (PNG, bz2) could be opened on any other computer without the need for additional information (just a program that knows how to read/decompress those file formats). On the other hand, the BB-ANS bitstream is not interpretable without the models used when compressing, and as discussed in Sec 4.3, there is certainly additional overhead involved in communicating the model which is not indicated here. \n\nIn any case, the compression rate achieved is impressive, but at the same time, not so surprising given that the model was trained on MNIST. Have you checked how well a model trained on a more general image dataset (e.g., ImageNet) compresses other images (e.g., MNIST)?\n\nSec 3.2 mentions finding that around 400 clean bits are required. How does the performance vary as fewer (or more) clean bits are used? More generally, do you have suggestions for how to determine an appropriate number of clean bits for other scenarios? (E.g., does it depend on the number of images to be compressed? their size? some notion of the entropy of the set of images to be compressed? other factors?) \n\nAlso, how does the performance vary with the number of symbols (images) to be compressed? I'd believe that the compression rate approaches the ELBO as the number of compressed images becomes large, but how quickly does this convergence occur? How well does the method do if the VAE is trained using a smaller sample size?\n\nOverall this is an interesting idea, and I believe it could be an excellent lossless compression scheme in scenarios where it's applicable. At the same time, there are many aspects where the paper could be strengthened by providing a more thorough investigation/evaluation.\n\n\nMinor:\n- In Sec 2.1, using p for both a general pdf and the model to be learned (i.e., of both s_n and b_i) is potentially confusing.\n- Sec 2.5.1 talks about using uniform quantization (buckets of equal width \\delta y), but then Appendix B talks about using (nonuniform) maximum entropy discretization. Which was used in the experiments? In an implementation, the quantization strategy needs to be known by both sender and receiver too, so this is additional meta-information overhead, right?\n- The discussion in Sec 4.1 seems very speculative and not particularly convincing.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Several Questions",
            "review": "The paper is very well written and the clarity is overall high. However, I was left with some questions about the significance of this work after reading this paper.\n\nThe authors approach the problem in the Bayesian inference framework. Essentially, the message is modeled as a linear neural network with a single latent layer. The authors only specify the distributions for the posterior and prior in the experimental section, where they set them both to Gaussians. This naturally raise the question how is this model different from the probabilistic PCA model? Moreover, I am confused why would it be necessary to introduce an approximation q(y|s) of the posterior p(y|s), when there is a well known closed form expression for Gaussians? Furthermore, this Gaussian model is well known to have non-unique maximum likelihood solution (due to the invariance to an arbitrary orthogonal transformation). How does that influence the addressed compression problem? Going back to equations (1)-(2), if the authors chose different distributions and the need for the ELBO was justified, wouldn’t that lead to an approximate representation? That is, wouldn’t that necessary imply some loss in compression? And if yes, wouldn't then the proposed approach be not a lossless but a lossy compression algorithm? And then why would this particular approach be better than other numerous lossy compression algorithms which the authors cite?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Very nice paper, but with practical limitations",
            "review": "This paper is built on a simple but profound observation: Frey's bits-back coding algorithm can be implemented much more elegantly when replacing arithmetic coding (AC) with asymmetric numerical systems (ANS), a much more recent development not known at the time, simply due to the fact that it encodes symbols in a stack-like fashion rather than queue-like.\n\nThis simple observation makes for an elegantly written paper, with promising results on MNIST. I truly enjoyed reading it, and I'm convinced that it will spark some very interesting further work in the field of compression with latent-variable models.\n\nHaving said that, I would like to point out some possible limitations of the proposed approach, which I hope the authors will be able to address/clarify:\n\n1. At the beginning of section 2.1, the authors define the symbols as chained conditionals prod_n p(s_n | s_1 ... s_n-1), which is generally permissible in AC as well as ANS, as long as the decoding order is taken into account. That is, in AC, the symbols need to be encoded starting with the first symbol in the chain (s_1), while in ANS, the symbols must be encoded starting with the last symbol in the chain, because the decoding order is inverted.\n\nIn their description of BB-ANS, the authors omit the discussion of conditional chains. It is unclear to me if a conditioning of the symbols is feasible in BB-ANS due to the necessity to maintain a strict decoding order. It would be very helpful if the authors could clarify this, and update the paper accordingly, because this could present a serious limitation. For instance, the authors simply extrapolate the performance of their method to PixelVAE; however, this model is autoregressive, so a conditioning of symbols seems necessary. Similarly, in appendix A, the authors mention the work of Minnen et al. (2018), where the same situation would apply, albeit one probabilistic level higher (on encoding/decoding the latents with an autoregressive prior).\n\n2. Furthermore, in both cases (PixelVAE and Minnen et al.), the symbols (s) and latents (y) are defined as jointly conditioned on each other (i.e., computing the posterior on one element of y requires knowledge of all elements of s, and computing the likelihood on one element of s requires knowledge of all elements of y). This seems to imply that all operations pertaining to one data vector (i.e. to one image) would have to be done in a monolithic fashion, i.e.: first sample all elements of y from the stack, then encode all elements of s, and then encode all elements of y. Hence, if the goal is to compress only one image, the algorithm would never get to the point of reusing the \"bits back\", and the overhead of BB-ANS would be prohibitive. It seems that in the MNIST experiments, the authors avoid this problem by always encoding a large number of images at a time, such that the overhead is amortized.\n\n3. Similarly, although the compression of continuous-valued variables up to arbitrary precision is an exciting development and I do not wish to undermine the importance of this finding, it should be noted that the finer the quantization gets, the larger the potential overhead of the coding scheme will grow. In practice, this would make it necessary to encode more and more images together, in order to still benefit from the method. This would be a good point to make in the discussion.\n\n4. The authors state in the appendix that learned compression methods like Ballé et al. (2018) and Minnen et al. (2018) could be improved by using BB-ANS. The potential gain of BB-ANS for these models seems rather small, though, as the entropy of y must be larger or equal to the entropy of y conditioned on s: H[y] >= H[y|s], the latter of which should represent the potential coding gain. Ballé et al. (2018), however, found that the bits used to encode the hierarchical prior (i.e. H[y]) is only a small fraction of the total bitrate, thus upper bounding the potential gains for this type of model.\n\nOverall, I think this is a well-written, important and elegant paper, and I would like to see it accepted at this conference. If the authors can satisfactorily address some of the above potential limitations, it might turn out to be even better.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}