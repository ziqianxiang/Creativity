{
    "Decision": {
        "metareview": "The paper suggests a new measurement of layer-wise margin distributions for generalization ability. Extensive experiments are conducted. Though there lacks a solid theory to explain the phenomenon. The majority of reviewers suggest acceptance (9,6,5). Therefore, it is proposed as probable accept.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "A layer-wise geometric margin distribution is used to calibrate the generalization ability, with extensive experimental support yet lacking a theory."
    },
    "Reviews": [
        {
            "title": "An empirical study towards the prediction power based on the margin distribution at each layer.",
            "review": "The author(s) suggest using geometric margin and layer-wise margin distribution in [Elsayed et al. 2018] for predicting generalization gap.\n\npros,\na). The author shows large experiments to support their argument.\n\ncons,\na). No theoretical verification (nor convincing intuition) is provided, especially for the following questions,\n    i) what benefit can be acquired when using geometric margin defined in the paper.\n    ii) why does normalization make sense beyond the simple scaling-free reason. For example, spectral complexity as a normalization factor in [Bartlett et al. 2017] is proposed from the fact, that the Lipschitz constant determines the complexity of network space.\n    iii) why does the middle layer margin can help? \n    iv) why a linear (linear log) relation between the statistic and generalization gap.\n\nFurther question towards experiment,\ni) I don't think your comparison with Bartlett's work is fair. Their bounds suggest the gap is approximately Prob(0<X<\\gamma) + Const/\\gamma for a chosen \\gamma, where X is the normalized margin distribution. I think using the extracted signature from margin distribution and a linear predictor don't make sense here.\nii) If you do regression analysis on a five layers cnn, can you have a good prediction on a nine layers cnn (or even residue cnn)?\n\nFinally, I'm not sure the novelty is strong enough since the margin definition comes from [Elsayed et al. 2018] and the strong linear relationship has been shown in [Bartlett et al. 2017, Liao et al. 2018] though in different settings.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A nice empirical paper with good intuitions and encouraging results",
            "review": "This paper does not even try to propose yet another \"vacuous\" generalization bounds, but instead empirically convincingly shows an interesting connection between the proposed margin statistics and the generalization gap, which could well be used to provide some \"prescriptive\" insights (per Sanjeev Arora) towards understanding generalization in deep neural nets.\n\nI have no major complaints but for a few questions regarding clarifications,\n1. From Eq.(5), such distances are defined for only one out of the many possible pairs of labels. So when forming the so-called \"margin signature\", how exactly do you compose it from all such pair-wise distances? Do you pool all the distances together before computing the statistics, or do you aggregate individual statistics from pair-wise distances? And how do you select which pairs to include or exclude? Are you assuming \"i\" is always the ground-truth label class for $x_k$ here?\n\n2. In Eq.(3), the way you define the distance (that flipping i and j would change the sign of the distance) is implying that {i, j} should not be viewed as an unordered pair, in which case a better notation might be (i, j) (i.e. replacing sets \"{}\" with tuples \"()\" to signal that order matters).\n\nAnd why do you \"only consider distances with positive sign\"? I can understand doing this for when neither i nor j corresponds to the ground-truth label of x, because you really can't tell which score should be higher. But when i happens to be the ground-truth label, wouldn't a positive distance and a negative distance be meaningful different and therefore it should only be beneficial to include both of them in the margin samples?\n\nAnd a minor typo: In Eq.(4), $\\bar{x}_k$ should have been $\\bar{x}^l$?",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Well written; technically weak",
            "review": "After author response, I have increased my score. I'm still not 100% sure about the interpretation the authors provided for the negative distances. \n\nThe paper is well written and is mostly clear. (1st line on page 4 has a typo, \\bar{x}_k in eq (4) should be \\bar{x}^l?)\n\nNovelty: I am not sure whether the paper adds any significant on top of what we know from Bartlett et al., Elsayed et al. since:\n\n(i). The fact that \"normalized\" margins are strongly correlated with the test set accuracy was shown in Bartlett et al. (figure 1.). A major part of the definition comes from there or from the reference they cite; \n(ii). Taylor approximation to compute the margin distribution is in Elsayed et al.; \n(iii). I think the four points listed in page 2 (which make the distinction between related work) is misleading: the way I see it is that the authors use the margin distribution in Elsayed et al which simply overcomes some of the obstacles that norm based margins may face. The only novelty here seems to be that the authors use the margin distribution at each layer. \n\nTechnical pitfalls: Computing the d_{f,x,i,j} using Equation (3) is missing an absolute value in the numerator as in equation (7) Elsayed et al.. The authors interpret the negative values as misclassification: why is it true? The margin distribution used in Bartlett et al. (below Figure 4 on page 5 in arxiv:1706.08498) uses labeled data and it is obvious in this case to interpreting negative values as misclassification. I don't see how this is true for eq (3) here in this paper. Secondly, why are negative points ignored?? Misclassified points in my opinion are equally important, ignoring the information that a point is misclassified doesn't sound like a great idea. How do the experiments look if we don't ignore them?\n\nExperiments: Good set of experiments. However I find the results to be mildly taking the claims of the authors made in four points listed in page 2 away: Section 4.1, \"Empirically, we found constructing this only on four evenly-spaced layers, input, and 3 hidden layers, leads to good predictors.\". How can the authors explain this? \n\nBy using linear models, authors implicitly assume that the relationship between generalization gaps and signatures are linear (in Eucledian or log spaces). However, from the experiments (table 1), we see that log models always have better results than linear models. Even assuming linear relationship, I think it is informative to also provide other metrics such as MSE, AIC, BIC etc..",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}