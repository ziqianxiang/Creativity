{
    "Decision": {
        "metareview": "All reviewers agree that the proposed is interesting and innovative. One reviewer argues that  some additional baseline comparisons could be beneficial and the other two suggest inclusion of additional explanations and discussions of the results. The authors’ rebuttal alleviated most of the concerns. All reviewers are very appreciative of the quality of the work overall and recommend probable acceptance. I agree with this score and recommend this work for poster presentation at ICLR.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Metareview"
    },
    "Reviews": [
        {
            "title": "Interesting work",
            "review": "The authors propose a new network architecture for multi-agent reinforcement learning. The new architecture addresses three issues: (1) the applicability of existing algorithms to semi-cooperative or competitive settings; (2) the ability to use local rewards during agent training; (3) the credit assignment problem with global multi-agent rewards. The authors address these issues with a new architecture that is comprised of several LSTM controllers with tied weights that transmit a continuous vector to each other, and that are augment with a gating mechanism that allows them to abstain from communicating.\n\nI think that this paper makes a solid contribution over the existing literature. My main comments are the following:\n* I feel like the paper can be strengthened by comparing to additional baselines. The authors compare mainly to Sukhbataar et al., but I think a more detailed comparison to other approaches (e.g. Foerster et al.)\n* One of the advantages of this method is that it can be used in non-cooperative settings. I am not familiar with this regime, and I would like a better explanation about why we would train competing agent with the same controller, rather than using a different controller for each team.\n* In several experimental results, the proposed method seems to have significantly higher variance than the baselines. I would like to see some discussion about why it is the case.\n* Also, in some places (e.g. Table 1), the method is highlighted in bold, even though it doesn’t actually outperform the baseline. Please correct this and only highlight the best method (if several methods are tied, either highlight them one, or don’t highlight any).\n* Also, in some cases when the error bars contain the previous best result, I am not sure if we can say that the proposed method is obviously better.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "In this work, the authors propose an interesting gating scheme allowing agents to communicate in an multi-agent RL setting. ",
            "review": "From a methodological perspective, this paper describes a simple bu clever learning architecture with individual agents able to decide when to communicate through a learned gating mechanism. Each agent is an LSTM able to decide at each time point which aspects of its internal state should be exposed to other agents through this gating mechanism. The presentation of this method is clear to a level that should allows the reader to implement this him/herself. It would be great if the code associated to this could be released but the presentation allows for reproducibility. \n\nThe experiments are interesting as well. Experimental results are presented on 3 problems and compared with known baselines from the academic community. The obtained results do show the merit of the approach. That being said, while the experimental results are extensive, there are places that could benefit from more clarity. For instance, I have found section 4.2 a bit dry. For instance, I had to read the plots caption and the text several times to map get at the deductions made in 4.2. Given the importance of gating in this work, I recommend expanding on this a bit (if space allows it). Small note: in the caption for Figure 3, on the fourth line, did you mean (f) instead of (d) when arguing that agents stop communicating once they reach the prey ( or am I missing something here)? Also, would it be possible to provide more insights on why IC3Net is doing better than CommNet except for the Combat-10Mv3Ze task (last table before the conclusion, what makes this task harder for IC3Net)? Another observation is on the variance terms that are reported for IC3Net. They are often (not always but definitely in the last table before the conclusion) quite higher when compared to the values associated with the baselines. Can this be explained? Another small thing: please add captions to your tables (at least a table number; I think that Table 2 does not have a caption). \n\n\nOverall, the paper is well written, interesting. Addressing the questions raised above would definitely help me and probably the eventual readers better appreciate its quality. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "very well written paper, good experiment section, method of communication can be more motivated",
            "review": "This work is an extension to the work of Sukbaatar et al. (2016) with two main differences:\n1) Selective communication: agents are able to decide whether they want to communicate.\n2) Individualized reward: Agents receive individual rewards; therefore, agents are aware of their contribution towards the goal.\nThese two new extensions enable their model to work in either cooperative or a mix of competitive and competitive/collaborative settings. The authors also claim these two extensions enable their model to converge faster and better. \nThe paper is well written, easy to follow, and everything has been explained quite well. The experiments are competent in the sense that the authors ran their model in four different environments (predator and prey, traffic junction, StarCraft explore, and StarCraft combat). The comparison between their model with three baselines was extensive; they reported the mean and variance over different runs. I have some concerns regarding their method and the experiments which are brought up in the following:\n \nMethod:\n\nIn a non-fully-cooperative environment, sharing hidden state entirely as the only option for communicate is not very reasonable; I think something like sending a message is a better option and  more realistic (e.g., something like the work of Mordatch & Abbeel, 2017)\n\nExperiment:\n\nThe experiment \"StarCraft explore\" is similar to predator-prey; therefore, instead of explaining StarCraft explore, I would like to see how the model works in StarCraft combat. Right now, the authors explain a bit about the model performance in Starcraft combat, but I found the explanation confusing.\n \nAuthors provide 3 baselines:\n1) no communication, but IR\n2) no communication, no IR\n3) global communication, no IR (commNet)\n\nI think having a baseline that has global communication with IR can show the effect of selective communication better. \n\nThere are some questions in the experiment section that have not been addressed very well. For example:\n Is there any difference between the results of table 1, if we look at the cooperative setup? \nDoes their model outperform a model which has global communication with IR? \nWhy do IRIC and IC work worst in the medium in comparison to hard in TJ in table1? \nWhy is CommNet work worse than IRIC and IC in table 2?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}