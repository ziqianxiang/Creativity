{
    "Decision": {
        "metareview": "The paper presents a method for coarse and fine inference for question answering.  It originally measured performance only on WikiHop and then later added experiments on TriviaQA.  The results are good.\n\nOne of the concerns regarding the paper was the novelty of the work, and lack of enough experiments.  However, the addition of TriviaQA results allays some of that concern.  I'd suggest citing the paper by Swayamdipta et al from last year that attempted coarse to fine inference for TriviaQA:\n\nMulti-Mention Learning for Reading Comprehension with Neural Cascades. \nSwabha Swayamdipta, Ankur P. Parikh and Tom Kwiatkowski. \nProceedings of ICLR 2018.\n\nOverall, there is relative consensus that the paper is good with a new method and some strong results.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Meta Review"
    },
    "Reviews": [
        {
            "title": "Method that is well adapted to the task to be solved; clear and well written",
            "review": "This paper proposes a method for multi-hop QA based on two separate modules, which are called coarse-grained and fine-grained modules. The coarse-grained module reads all of the supporting documents for QA, whereas the fine-grained one reads the local context surrounding each candidate entity's mentions. Both modules are used to predict the score of a candidate entity being the answer, with the final result being the sum of the two scores.\n\nThis is a fine paper and achieves a new state of the art on the Qangaroo multi-hop QA dataset. The paper is clearly written, presents the models intuitively, while not foregoing technical detail should that be interesting to a reader. I appreciated the ablation results, as well as the qualitative analyses. The overall idea of encoding different levels of context is an important one, and I am glad that this paper shows that this approach works for a complex QA task.\n\nThere are two downsides to the paper. The first is that I am not sure it is really accurate to call the coarse-grained model as such, as it still seems to require passing every word in the supporting documents to an encoder. It seems to be more aimed at capturing global information from the supporting documents, rather than to make a quick, high-level pass at inference. The second weakness is that the coarse- and fine-grained modules barely interact at all, as their prediction scores are simply summed at the output layer. It is nice that even such a simple method of interaction already works so well, but I would have expected some exploration or comment on how more interactions could be enabled.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good work; could be more interesting to see application in other related tasks",
            "review": "This paper proposes an interesting coarse-grain fine-grain coattention network architecture to address multi-evidence question answering and achieves the new state-of-the-art results on the Qangaroo WikiHop dataset.  The main idea is to divide the task across the coarse-grain and fine-grain modules in a complimentary manner such that the coarse-grain module learns from efficient modeling of support documents and the query whereas the fine-grain module learns from associations of candidate mentions in the support documents with the query. \n\nThe major strength of the model is observed with learning effective representations of larger numbers of long support documents and the state-of-the-art results are achieved without the use of pretrained contextualized embeddings. The main novelty lies in how the coattention and self-attention strategies are combined hierarchically to learn relevant representations in a complimentary fashion (rather than serial). Overall, the paper is very well-written and presents solid results with meaningful ablation study, quantitative and qualitative analyses. I have a few comments/suggestions:\n\n- It would be interesting to see how the inclusion of pretrained contextualized embeddings such as ELMo, ULMFit, BERT would help the current model. \n\n- \"This is likely because coreference resolution captures intra-document and inter-document dependencies more accurately than hierarchical attention.\" --> Please clarify why this is the case.\n\n- \"We hypothesize that ways to reduce this type of error include using more robust pretrained contextual encoders (McCann et al., 2017; Peters et al., 2018) and coreference resolution.\" --> I agree; also, it would be worth considering some commonsense knowledge to alleviate this issue, because the fact that Scotland is a part of UK and has a border with England should be learned. Here is a relevant work: \"Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge\" by Mihaylov and Frank, 2018.\n\n- \"The second type (28% of errors) results from questions that are not answerable. For example, the support documents do not provide the narrative location of the play “The Beloved Vagabond” for the query narrative location the beloved vagabond.\" --> It would be great if you could release the set of unanswerable questions for the community.\n\n- Please include the memory network-based QA works in the related work section because they involve some forms of reasoning. Also, I would suggest to cover the query-focused multi-document summarization area in the related work section because they also require evidence synthesis from multiple documents to address a query. It would be very interesting if authors can apply their model for the query-focused multi-document summarization task as well, as this would further validate the effectiveness of the proposed architecture for reasoning across multiple documents.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "I am open to increasing my rating score as long as the authors could address my concerns and confusions below",
            "review": "This paper focuses on multi-choice QA and proposes a coarse-to-fine scoring framework. Where the coarse-grained answer scoring model computes the scores with the attention over the whole passages, and the fine-grained one only uses local contexts for each answer option (candidate).\n\nThe proposed approach was evaluated on the only dataset of WikiHop, and achieved large improvement over the other methods on the leaderboard. However, I found the paper lack of motivation about the designs of the coarse and fine scoring models. For example, why using self-attention after GRU and co-attention in the two answer scoring models?\n\nAnother concern I have is about the novelty. Besides the complicated model designs, the coarse and fine scoring models are both following some common ideas in previous work. And each model could achieve on-par results compared to previous baselines. This makes me feel that the whole approach looks more like model combination of two not-so-novel (and not very well-motivated) models.\n\nThirdly, the only evaluation on WikiHop brings more problems to the above two points. Since the motivation of the architecture design is not very clear, I am not sure whether the architectures could generalize to other benchmarks. Similar concern for the model combination approach.\n\nMoreover, the proposed approach is a general architecture for multiple-choice datasets requiring multiple evidence. To verify its generalizability, I suggest the authors add further experiments on one dataset from the following ones: either multi-choice QA datasets like ARC and RACE/RACE-open, or other open-domain QA datasets like TriviaQA, by treating the re-ranking of answer predictions as multi-choice QA problems (like the approach in Evidence Aggregation for Open-Domain QA from ICLR2018).\n\nA minor question: why the CFC w/o encoder could still work so well? At least the fine-grained scoring model should heavily rely on encoders. Otherwise, according to Eq (17), the fine-grained model cannot use any contextual information.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}