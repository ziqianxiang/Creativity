{
    "Decision": {
        "metareview": "The paper proposes a new dataset for studying knowledge grounded conversations, that would be very useful in advancing this field. In addition to the details of the dataset and its collection, the paper also includes a framework for advancing the research in this area, that includes evaluation methods and baselines with a relatively new approach.\nThe proposed approach for dialogue generation however is a simple extension of previous work by (Zhang et al) to user transformers, hence is not very interesting. The proposed approach is also not compared to many previous studies in the experimental results.\nOne of the reviewers highlighted the weakness of the human evaluation performed in the paper. Moving on, it would be useful if further approaches are considered and included in the task evaluation. \n\nA poster presentation of the work would enable participants to ask detailed questions about the proposed dataset and evaluation, and hence may be more appropriate.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Interesting dataset and evaluation framework"
    },
    "Reviews": [
        {
            "title": "Good work",
            "review": "This work proposes a brand new dataset to fill in the vacancy of current conversational AI community, specifically the introduced dataset aims at providing a platform to perform large-scaled knowledge-grounded chit-chat. Overall, the dataset is well-motivated and well-designed, its existence will potentially benefit the community and inspire more effective methods to leverage external knowledge into dialog system. Besides, the paper also utilizes many trending models like Transformers, Memory Networks, etc to ensure the state-of-the-art performance. The clear structure and paragraphs also makes the paper easy to read and follow.\n\nHere are some questions I want to raise about the paper:\n\n1. First of all, the design of the conversation flow though looks reasonable, but it is pretty uncommon for a human to ground his/her every sentence on external knowledges. Therefore, it would probably be better to introduce some random ungrounded turns into the conversation to make it more humanlike.\n\n2. Secondly, the whole framework is based on many modules and every one of them are prone to error. I’m afraid that such cascaded errors will accumulate and lead to compromised performance in the end. Have you thought about using REINFORCE\nalgorithm to alleviate this issue?\n\n3. Finally, it would be better to introduce some noisy or adversarial apprentice to raise unrelated turns and see how the system react. Have you thought about how to deal with such cases?",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting task and dataset",
            "review": "This paper collects a new annotated dataset for knowledge grounded dialog task. The proposed models combine two recent neural networks, Memory Net and Transformer, for the purpose of the task. I highly appreciate the efforts to collect such a precious dialog dataset for the community. Also, the setup in data collection actually narrows down the scope of chitchat dialog into a specific topic by grounding it to a set of knowledge. \n\nHere are summaries of my concerns and questions about the paper. \n\n# applicability of the knowledgeable bot\nWhat is the basic motivation of this work? Once you develop a chatbot that can produce a response grounded by knowledge, how could it be applied to real-world applications? Are you trying to teach a student who is looking for more knowledge about a topic? If so, you should be more careful about what knowledge the student (or apprentice in the paper) knows or don’t know about the topic and how their knowledge models dynamically change over the chat. Otherwise, the proposed model seems a simple knowledge retrieval model given the dialog context. Would you please provide motivations of the work?\n\n# No explicit goal of a dialog makes the chat divergent and open-ended\nWithout a specific goal given to the annotators or a restriction in the instruction, a dialog in the current setting might diverge beyond the context. For example, if an apprentice says about her/his personal opinion about the topic (e.g., I hate the Gouda cheese) or past experience (e.g., I went to a music festival by Michael Jackson 23 years ago), then how do you control the chat between two annotators or how do you train a model not to pay much attention on out-of-topic utterances?  \n\n# Lack of further analysis of the dataset\nData collection part itself seems to be the biggest contribution to this work. Why don’t you bring one of real dialog example in Figure 3 to the main paper and say more about it? For example, what other interesting applications can you develop on this dataset? \n\nCompared to the Wizard, the role of apprentice seems unclear to me. I found from the examples in Figure 3 that most of the apprentices’ responses are a follow-up question about the knowledge, a personal agreement or feeling or their preference. Do you have any post analysis on the types of responses from the apprentices so highlighting utilities of the dataset in a real application? \n\n# Some questions on data collection\nDo you have any incentive mechanism to make annotators more engage in the dialog?\nDid you filter out some bad dialogs? Then, how did you measure the quality of a dialog? \nHow do you penalize bad annotators that often make aggressive words or don’t follow the instruction you set up? \n\n# A question on the model\nCompared to previous works such as (Zhang at al., ACL18), the proposed model seems to have the only replacement with Transformer encoder and a loss term for knowledge selection. Have you tried another way of dealing with the knowledge part? For example, a ranking loss might be better than the attention. \n\n# Questions on the Experiment section\nAny experiment to show the effect of different \\lambda value in the loss of the generative model? \n\nWhen you evaluate the generative model, have you also tried other automatic metrics such as BLEU instead of only PPL and Unigram-F1? For this task, the possible response grounded by the topic+knowledge might be too diverse to measure though. Could you possibly add some constraints to the annotators to do some clear tasks over the dialog so you can systematically evaluate the dialog w.r.t the constraint? Otherwise, evaluation of this task seems to be mostly the same as chitchat systems.\n\nIn Table 5, human evaluators only measure the likeness of the dialog which seems very naive. Why don’t you measure whether the apprentice gets new knowledge of which s/he didn’t know before, whether the knowledge provided from the model was informative, whether the dialog was fun and engaging or more? The current human evaluation seems very weak though. \n\nThis might be an auxiliary question: have you tried to train the model for apprentice and make two models chat with each other? How does the chat look like then?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting new dataset",
            "review": "This paper introduces a new dataset and method for chatbots. In contrast to previous work, this paper specifically probes how well a dialogue system can use external unstructured knowledge. \n\nQuality:\nOverall, this is a very high-quality paper. The dataset is developed well, the experimental setup is well thought-through and the authors perform many ablation studies to test different model variants. The main criticism I have would be that the human evaluation is rather simple (rating 1-5), I would have expected more fine-grained categories, especially ones that relate to how much knowledge the system uses (I appreciate the \"Wiki F1\" metric, but that is an automatic metric). As it is, the human evaluation shows that most of their contributions are not appreciated by human annotators. Further, the paper ends a bit abruptly, I would have expected a more in-depth discussion of next steps.\n\nClarity:\nThe description of the work is clear in most places. I particularly like the abstract and introduction, which set up the rest of the paper nicely. In some places, perhaps due to space restrictions, method descriptions are a bit too short.\n\nOriginality:\nThe paper is fairly original, especially the aspect about specifically using external knowledge. The authors could have been more clear on how the work differs from other work on non-goal directed dialogue work though (last paragraph of related work section).\n\nSignificance:\nThe dataset is really well-developed, hence I believe many working in the dialogue systems community will re-use the developed benchmark and build on this paper.\n\nMore detailed comments:\n- Missing reference for goal-oriented dialogue datasets: Wen et al. 2017, A Network-based End-to-End Trainable Task-oriented Dialogue System, https://arxiv.org/abs/1604.04562\n- How does the proposed dataset differ from the Reddit and Wikipedia datasets discussed in the last paragraph of the related work section? This should be explained.\n- Page 3, paragraph \"Conversational Flow\": what is the maximum number of turns, if the minimum is 5?\n- Page 3, paragraph \"Knowledge Retrieval\": how were the top 7 articles and first 10 sentences choices made? This seems arbitrary. Also, why wasn't the whole text used?\n- Page 3, paragraph \"Knowledge Selection and Response Generation\": how do you deal with co-reference problems if you only ever select one sentence at a time? The same goes for the \"Knowledge Attention\" model described in Section 4.\n- Page 3, paragraph \"Knowledge Selection and Response Generation\": how often do annotators choose \"no sentence selected\"? It would be interesting to see more such statistics about the dataset\n- Section 4.2: did you run experiments for BPE encoding? Would be good to see as this is a bit of a non-standard choice.\n- Section 4.2: it would be good to explain the Cer et al. 2018 method directly in the paper\n- Section 4.2: is there a reference for knowledge dropout? Also, it would be good to show ablation results for this.\n- Section 5.1: why did you choose to pre-train on the Reddit data? There should be some more in-depth description of the Reddit dataset to motivate this choice.\n- Section 5.1: what is the setup you use for multi-task learning on SQuAD? Is it just a hard parameter sharing model, or?\n- Section 5.3: as stated above, the human evaluation is a little bit underwhelming, both in terms of setup and results. I'd expect a more fine-grained way of assessing conversations by humans, and also an explanation of why the retrieval performer without knowledge was assessed as being on par with the retrieval transformer memnet.\n- Section 5.3: I assume higher=better for the human scores? This should be made explicit.\n- Section 5.3: Have others used the \"F1 overlap score\"? If so, cite.\n- Section 5.3: I don't understand the argument that the human evaluation shows that humans prefer more natural responses. How does it show that?\n- Section 5.3: The Wiki F1 score is kind of interesting because it shows to what degree the model uses knowledge. But the side-by-side comparison with the human scores shows that humans don't necessarily prefer chatbot models that use a lot of knowledge. I'd expect this to be discussed, and suggestions for future work to be made accordingly.\n- Section 6: The paper ends a bit abruptly. It's be nice to suggest future areas of improvement.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}