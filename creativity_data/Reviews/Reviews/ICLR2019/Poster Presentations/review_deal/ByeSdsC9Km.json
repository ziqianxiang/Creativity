{
    "Decision": {
        "metareview": "All reviewers recommend acceptance. The problem is an interesting one. THe method is interesting.\nAuthors were responsive in the reviewing process.\n\nGood work. I recommend acceptance :)",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Strong paper"
    },
    "Reviews": [
        {
            "title": "Review for adaptive posterior learning",
            "review": "The paper proposes a novel model that reads in information, decide whether this information is surprising and hence whether or not to keep it in memory and also utilizing information in the memory to quickly adapt or reason. The authors experimented with few-shot Omniglot classification and meta learning reasoning tasks. \n\nNovelty:\n\nThe authors introduced a novel self-contained model that decides what to write to the external memory and making use of the external memory for different tasks.\n\nMy comments are mostly as follows: \n\n1. The paper is well written, the problems are clearly stated, the solution is presented in a clear way, overall very easy to follow.\n\n2. This is an interesting paper that combines a novel technique for writing to external memory based on surprisal and using it for more difficult tasks such as deductive reasoning.  I really like the surprisal mechanism, there are cognitive/ neuroscience materials that supports this approach (that the brain tends to write to memory things that are surprising). This also makes total sense from a machine learning perspective. \n\n3. Could another objective  be used for surprisal? Also, instead of a determinstic encoder, decoder, is it possible to use a variational objective?\n\n4. The experiments look convincing.\n\nOverall a very nice paper, nice idea, could show more resul",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "New idea in using memory for generalization task, more clarification and experiments are needed.",
            "review": "In this paper, authors present an algorithm to generalize learned properties from few observations by using a memory store and a memory controller. The experiments show comparable results on few-shot classification task and better performance and scalability for when the number of labels is unknown .\n\n- The paper is well-written and easy to follow in general. The notations and model specifications are clear. \n\n- The idea of incorporating an external memory store to save previous experiences is interesting especially without the need to backpropagate through the memory at each step. It is done by alignment of a query with the embeddings that are stored in the memory using k-nearest neighbor with Euclidean distance measures.  However, I am not quiet sure about how this is done in practice. It is stated in the paper that this alignment needs to emerge as a byproduct of training which is achieved by getting optimized to be as class-discriminative as possible. Isn't this implicitly optimizing part of the memory? I think more clarification would help a lot in understanding of this part.\n\n-  I liked using a memory controller that decides whether a point is 'surprising'. Authors defined surprise to be negative log of prediction for label. I was wondering if they considered other measures, and investigated the effects that they might have. I think a brief discussion would be helpful.\n\n- I am not an expert in this area but the experiments look convincing in general. Results in table one corresponding to 423-way are convincing since the proposed algorithm is the only candid that is able to perform the task with relatively good performance. On imagenet data set, the results are comparable to Inception-ResNet-v2 for fixed label case. However, more in-depth experiments or settings such as top-5 accuracy are needed to justify the performance of algorithm on this data set.  For the number analogy task the algorithm performs well in achieving high accuracy.\n\n- Title of the paper is too generic. From the looks of it, adaptive posterior learning should cover wider set of tasks or probabilistic models, but it does not. So to avoid confusion (and the expectation that comes with this name), I strongly suggest that the authors change the title or make it more specific to actually represent what is discussed in the paper.\n\n- In figure 4 c, I think x label should be \"class number\" not \"number of classes\". ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting algorithm for a few-shot learning",
            "review": "Summary: the authors propose a new algorithm, APL, for a few-shot and a life-long learning based on an external memory module. APL uses a surprise-based signal to determine which data points to store in memory and an attention mechanism to the most relevant points for prediction. The authors evaluate APL on a few-shot classification task on Omniglot dataset and on a number analogy task.\n\nQuality: the authors consider interesting approach to life-long learning and I really liked the idea of a surprise-based signal to choose the data to store. However, I am not convinced by the learning setting that authors study. While a digit-symbol task from the introduction is interesting to study the properties of APL, I fail to see any real world analogy where it is useful. The same happens in a few-shot omniglot classification. The authors decided to shuffle the labels within episodes that, I guess, is supposed to represent different tasks in a typical life-long learning scenario. Again, it maybe interesting to study the behaviour of the algorithm, but I don't see any practical relevance here. It would make more sense to study the algorithm in a life-long learning setting, for example, considered in [1] and [2].\n\nClarity: the paper is well-written in general. I failed to decode the meaning behind the paragraph under Figure 3 on page 4 and would advise the authors to re-write it. The same goes to the first paragraph on page 3.\n\nOriginality: the paper builds on the prior work of Kaiser et al., 2017 and Santoro et al., 2016, but the proposed modifications are novel to my best knowledge.\n\nSignificance: below average: the paper combines interesting ideas that potentially can be used in different learning contexts and with other algorithms, however, the evaluation does not show the benefit in an obvious way.\n\nOther comments: \n* throughout the whole paper it is not clear if the embeddings are learned or not. I suppose they are, but what then happens to the ones in memory? If they are not, like in ImageNet example, where do they come from?\n* the hyperparameter \\sigma: the authors claim \"the value of \\sigma seems to not matter too much\". Matter for what? It's great if the performance is stable for a wide range of \\sigma, but it seems like it should have a great influence over the memory footprint of APL. I feel this is an important point that needs more attention.\n* it would be interesting to see how APL performs with a simple majority vote instead the decoder layer. This would count for an ablation study and could emphasize the role of the decoder.\n* Figure 4, b) plots are completely unreadable on black-and-white print, the authors might like to address that\n* In conclusion, the first claim about state-of-the-art accuracy with smaller memory footprint: I don't think that the results of the paper justify this claim.\n\n[1] Yoon et al, Lifelong Learning with Dynamically Expandable Networks, ICLR 2017\n[2] Rebuffi et al,  iCaRL: Incremental Classifier and Representation Learning, CVPR 2017\n\n********************\nAfter authors response:\n\nThanks to the authors for a detailed response. The introduction led me to believe that the paper solves a different task from what it actually does. I still like the algorithm and, given that the scope of the paper is limited to a few-shot learning, I tend to change my evaluation and recommend to accept the paper. It was a good idea to change the title to avoid possible confusion by other readers. The introduction is still misleading though. It creates the impression that APL solves a more general problem where it would be good enough to limit the discussion to a few-shot learning setting and explain it in greater detail for an unfamiliar reader. Some details also seem to be missing, e.g. I didn't get that the memory is flushed after each episode and could not find where this is mentioned in the paper.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}