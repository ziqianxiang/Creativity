{
    "Decision": {
        "metareview": "The paper presents generative models to produce multi-agent trajectories. The approach of  using a simple heuristic labeling function that labels variables that would otherwise be latent in training data is novel and and results in higher quality than the previously proposed baselines.\nIn response to reviewer suggestions, authors included further results with models that share parameters across agents as well as agent-specific parameters and further clarifications were made for other main comments (i.e., baselines that train the hierarchical model by maximizing an ELBO on the marginal likelihood?).",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "Generative models to produce coordinated multi-agent behavior"
    },
    "Reviews": [
        {
            "title": "Paper proposes multi-agent sequential generative models. This is influential beyond toy simulations presented in the paper.",
            "review": "Very strong paper, building on top of variational RNNs for multi-agent sequential generation. Dialogue use case is mentioned in Discussion is indeed very exciting. The approach extends VRNN to a hierarchical setup with high level coordination via a shared learned latent variable. The evaluations are not very strong due to toy task setup, however the approach is clear and impactful.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Hierarchical latent variables with weak supervision help learning a global coordination between cooperative agents.",
            "review": "\nThis paper proposes training multiple generative models that share a common latent variable, which is learned in a weakly supervised fashion, to achieve high level coordination between multiple agents. Each agent has a separate VRNN model which is conditioned on the agent’s own trajectory history as well as the shared latent variable. The model is trained to maximize the ELBO objective and log-likelihood over macro-intent labels. Experimental results are conducted over a basketball gameplay dataset (to model the trajectories of the offensive team members) and a synthetic dataset. The results show that the proposed model is on-par with the baseline models in terms of ELBO while showing that it can model multi-modality better and is preferred more by humans. \n\nIn general, the paper is well written and the overall framework captures the essence of the problem that the authors are trying to solve.\nFurthermore, incorporating an auxiliary latent variable to model the coordination between multiple agents is interesting.\nI have several comments related to the strength of the baselines and contribution of individual components in the proposed model.\n\n\nMajor Comments\n\n- It seems that VRNN-single and VRNN-indep are two models on the far two ends of a spectrum. To understand the contribution of the shared macro-intent, how would an intermediate baseline model where a set of parameters are shared between agents and each agent also has an independent set of parameters perform? This could be accomplished by sharing the parameters of the first layer of GRU networks and learning the second layer parameters independently.\n\n- How is the threshold for macro-intent generation selected? How does this parameter affect the overall performance? Since the smoothness of the segments between two macro-intents depend on this parameter, I am wondering its effect on the learned posterior distribution.\n\n- Rather than using the prediction of the macro-intent RNN as a single global vector (\\hat{g}_t), could using separate vectors for each agent (corresponding blocks of \\hat{g}_t) as inputs to VRNN give the same results? Since the macro-intent RNN is already aware of all the macro-intents, it would be interesting to see if individual macro-intents are sufficient for VRNN to generate corresponding trajectories.\n\n\nMinor Comments\n\n- Do results in Table (1) come from sampling or using mode of the distributions? How peaked are the learned posterior distributions?\n- What is the performance of the macro-intent RNN model?\n- In Eq (2), “<=T” should be “<=t” (as in Eq (11) in Chung 2015).\n- In Page 6, bullet point 4: it should be “except we maximize the mutual information…”\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Heuristic labeling enables learning of hierarchical model without needing to marginalize over latent variables",
            "review": "# Summary\n\nThe paper proposes training generative models that produce multi-agent trajectories using heuristic functions that label variables that would otherwise be latent in training data. The generative models are hierarchical, and these latent variables correspond to higher level goals in agent behavior. The paper focuses on basketball offenses as a motivating scenario in which multiple agents have coordinated high-level behavior. The generative models are RNNs where each output is fed into the decoder of a variational autoencoder to produce observed states. The authors add an intermediate layer to capture the latent variables, called macro-intents. The parameters are learned by maximizing an evidence lower bound.\n\nExperiments qualitatively and quantitatively show that the hierarchical model produces realistic multi-agent traces.\n\n# Comments\n\nThe paper presents a sensible solution for heuristically labeling latent variables. It is not particularly surprising that the model then learns useful behavior because it no longer has to maximize the marginal likelihood over all possible macro-intents. What is more interesting is that a heuristic labeling function is sufficient to label macro-intents that lead to learning realistic basketball offenses and swarm behavior.\n\nAre any of the baselines (VRNN-single, VRNN-indep, and VRNN-mi) equivalent to training the hierarchical model by maximizing an ELBO on the marginal likelihood? I do not think this comparison is done, which might be interesting to quantify how much of a difference heuristic labeling makes. Of course, the potentially poor fit of a variational distribution would confound the results.\n\n# Minor things\n\n1) In the caption of Table 1, it says \"Our hierarchical model achieves higher log-likelihoods than baselines for both datasets.\" Are not the reported scores evidence lower-bounds? So it achieves a higher evidence lower bound, but without actually computing the true likelihood, could not the other models have higher likelihoods?\n\n2) Under \"Human preference study\" it says \"All judges preferred our model over the baselines with 98% statistical significance.\" I am not familiar with this terminology. Does that mean that a p value for some null hypothesis is .02?\n\n3) Something is wrong with the citation commands. Perhaps \\citep should be used.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}