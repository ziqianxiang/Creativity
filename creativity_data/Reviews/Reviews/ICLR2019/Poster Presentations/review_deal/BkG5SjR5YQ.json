{
    "Decision": {
        "metareview": "The submission evaluates maximum mean discrepancy estimators for post selection inference.\nIt combines two contributions: (i) it proposes an incomplete u-statistic estimator for MMD, (ii) it evaluates this and existing estimators in a post selection inference setting.\n\nThe method extends the post selection inference approach of (Lee et al. 2016) to the current u-statistic approach for MMD.  The top-k selection problem is phrased as a linear constraint reducing it to the problem of Lee et al.  The approach is illustrated on toy examples and a GAN application.\n\nThe main criticism of the problem is the novelty of the paper.  R1 feels that it is largely just the combination of two known approaches (although it appears that the incomplete estimator is key), while R3 was significantly more impressed.  Both are senior experts in the topic.\n\nOn the balance, the reviewers were more positive than negative.  R2 felt that the authors comments helped to address their concerns, while R3 gave detailed arguments in favor of the submission and championed the paper.  The paper provides an additional interesting framework for evaluation of estimators, and considers their application in a broader context of post-selection inference.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Area chair recommendation"
    },
    "Reviews": [
        {
            "title": "Could be an important contribution but not clear from the paper",
            "review": "The paper proposes a new post selection inference for MMD statistics, i.e., identity the p-values for the dimensions of vector. I believe this is an important problem that has not been addressed in previous literature. The work provides an extension to the original post selection inference work for lasso (Lee et al. 2016). \n\nHowever, I wish the paper could have explained the main idea clearly. Right now it is hard to me to judge whether or not the proposed estimator is correct (the only place that seems to support this is Fig. 4(a). Where the p-value seems to be clear to a uniform distribution. \n\nFor instance, the proposed PSI estimator, we will need to estimate the covariance matrix. This was explained in Section 3.3, and it was said that the algorithm in Fan et al. (2013) was used for the explanation. However, I think more detailed discussions and explanation should be provided here. In order to obtain correct p-value estimate, I believe getting accurate covariance matrix estimate is crucial. How large the sample size is needed, in order for us to get an accurate enough covariance matrix, to perform sub-sequent post selection inference? More discussions are needed here. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "develop post feature selection inference for incomplete mmd",
            "review": "The paper propose a method for post feature selection inference in the case where the distribution is non-Gaussian. The paper developed a statistic called incomplete mmd and showed its asymptotic normal property. Then the incomplete mmd can be plugged into post feature selection framework for computing the p-value. \n\nThe paper is a nice combination of incomplete mmd and post selection inference technique. \nHowever, the combination is straightforward: the asymptotic Gaussian property of the incomplete mmd is the key. \n\nFurthermore, I think the applications (feature selection and test for GAN objective) is not exciting from the machine learning point of view. A better application which can show-case the obtained p-value is very useful will make the paper more interesting. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "clearly written, nice work",
            "review": "The authors focus on the selection problem of k statistically significant features discriminating 2 probability distributions accessible via samples. They propose a non-parametric approach under the PSI (post selection inference) umbrella using MMD (maximum mean discrepancy) as a discrepancy measure between probability distributions. The idea is to apply (asymptotically) normal MMD estimators, rephrase the top-k selection problem as a linear constraint, and reduce the problem to Lee et al., 2016. The efficiency of the approach is illustrated on toy examples and in GAN (generative adversarial network) context. The technique complements the PSI-based independence testing approach recently proposed by Yamada et al., 2018. \n\nThe submission is a well-organized, clearly written, nice contribution; it can be relevant to the machine learning community.\n\nBelow I enlist a few suggestions to improve the manuscript:\n-Section 1: The notion of characteristic kernel (kernel when MMD is metric) has not been defined, but it was referred to. 'Due to the mean embeddings in RKHS, all moment information is stored.': This sentence is somewhat vague.\n-Section 1: 'MMD can be computed in closed form'. This is rarely the case (except for e.g. Gaussian distributions with Gaussian or polynomial kernels). I assume that the authors wanted refer to the estimation of MMD.\n-Section 1: 'K nearest neighbor approaches (Poczos & Schneider, 2011)'. The citation to this specific estimator can go under alpha-divergences. The Wasserstein metric could also be mentioned.\n-Section 3.1: k is used to denote the number of selected features and also the kernel used in MMD. I suggest using different notations.\n-Theorem 1: '\\Phi is the CDF...'. There is no \\Phi in the theorem.\n-Section 3.2: The existence of MMD (mean embedding) requires certain assumptions: E_{x\\sim p}\\sqrt{k(x,x)} < \\infty, E_{x\\sim q}\\sqrt{k(x,x)} < \\infty.\n-Section 3.2.: block estimator: 'B_1 and B_2 are finite'. 'fixed'?\n-Section 3.2.: MMD_{inc}: \n   i) 'S_{n,k}': k looks superfluous.\n   ii) 'l': it has not been introduced (cardinality of D).\n-Section 3.3: typo: 'covraiance' (2x)\n-Section 3.3: Fan et al. 2013: The citation can go to \\citep{}.  \n-Theorem 2: \n   i)'c' is left undefined.\n   ii)Comma is missing before 'where'.\n   iii)\\xrightarrow{d} (Theorem 2, Corollary 3-4): Given that 'd' also denotes dimension in the submission, I suggest using a different notation for convergence in distribution.\n-At the introduction of block-MMD the block size (B) was fixed, while in the experiments (e.g. Figure 3) it is growing with the sample size (B=\\sqrt{n}). The assumption on B should be clearly stated.\n-Section 5.1: (b) mean shift: comma is missing before 'where'.\n-References: \n   i) Abbreviations and names in the titles should be capitalized (such as cramer, wasserstein, hilbert-schmidt, gan, nash). \n   ii) Scholkopf should be Sch\\{\"o}lkopf (in the ALT 2005 work).\n   iii) 'Exact post-selection inference, with application to the lasso': All the authors are listed; 'et al.' is not needed.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}