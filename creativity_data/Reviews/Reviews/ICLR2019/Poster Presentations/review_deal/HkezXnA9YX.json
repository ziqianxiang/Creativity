{
    "Decision": {
        "metareview": "This paper generated a lot of discussion. Paper presents an empirical evaluation of generalization in models for visual reasoning. All reviewers generally agree that it presents a thorough evaluation with a good set of questions. The only remaining concerns of R3 (the sole negative vote) were lack of surprise in findings and lingering questions of whether these results generalize to realistic settings. The former suffers from hindsight bias and tends to be an unreliable indicator of the impact of a paper. The latter is an open question and should be worked on, but in the opinion of the AC, does not preclude publication of this manuscript. These experiments are well done and deserve to be published. If the findings don't generalize to more complex settings, we will let the noisy process of science correct our understanding in the future. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "Interesting, but please add more experiments like this",
            "review": "The paper explores how well different visual reasoning models can learn systematic generalization on a simple binary task. They create a simple synthetic dataset, involving asking if particular types of objects are in a spatial relation to others. To test generalization, they lower the ratio of observed  combinations of objects in the training data. The authors show the result that tree structured neural module networks generalize very well, but other strong visual reasoning approaches do not. They also explore whether appropriate structures can be learned. I think this is a very interesting area to explore, and the paper is clearly written and presented.\n\nAs the authors admit, the main result is not especially surprising. I think everyone agrees that we can design models that show particular kinds of generalization by carefully building inductive bias into the architecture, and that it's easy to make these work on the right toy data. However, on less restricted data, more general architectures seem to show better generalization (even if it is not systematic). What I really want this paper to explore is when and why this happens. Even on synthetic data, when do or don't we see generalization (systematic or otherwise) from NMNs/MAC/FiLM? MAC in particular seems to have an inductive bias that might make some forms of systematic generalization possible. It might be the case that their version of NMN can only really do well on this specific task, which would be less interesting.\n\nAll the models show very high training accuracy, even if they do not show systematic generalization. That suggests that from the point of view of training, there are many equally good solutions, which suggests a number of interesting questions. If you did large numbers of training runs, would the models occasionally find the right solution? Could you somehow test for if a given trained model will show systematic generalization? Is there any way to help the models find the \"right\" (or better) solutions - e.g. adding regularization, or changing the model size? \n\nOverall, I do think the paper has makes a contribution in experimentally showing a setting where tree-structured NMNs can show better systematic generalization than other visual reasoning approaches. However, I feel like the main result is a bit too predictable, and for acceptance I'd like to see a much more detailed exploration of the questions around systematic generalization.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting observations but limited experiments; also doubtful how experiments and learning can be generalized to more complex tasks",
            "review": "Summary: The paper focuses on comparing the impact of explicit modularity and structure on systematic generalization by studying neural modular networks and “generic” models. The paper studies one instantiation of this systematic generalization for the setting of binary “yes” or “no” visual question answering task.  They introduce a new dataset called in which model has to answer questions that require spatial reasoning about pairs of randomly scattered letters and digits in the image. While the models are evaluated on all possible object pairs, they are trained on a smaller subset. They observe that NMNs generalize better than other neural models when an appropriate choice of layout and parametrization is made. They also show that current end-to-end approaches for inducing model layout or learning model parametrization fail to generalize better than generic models.\n\nPros:\n- The conclusions of the paper regarding the generalization ability of neural modular networks is timely given the widespread interest in these class of algorithms. \n- Additionally, they present interesting observations regarding how sensitive NMNs are to the layout of models. Experimental evidence (albeit on specific type of question) of this behaviour will be helpful for the community and hopefully motivate them to incorporate regularizers or priors that steer the learning towards better layouts.  \n- The authors provide a nice summary of all the models analyzed in Section 3.1 and Section 3.2. \n\nCons:\n- While the results on SQOOP dataset are interesting, it would have been very exciting to see results on other synthetic datasets. Specifically, there are two datasets which are more complex and uses templated language to generate synthetic datasets similar to this paper:\n    - CLEVR environment or a modification of that dataset to reflect the form of systematic the authors are studying in the paper. \n    - Abstract Scenes VQA dataset introduced in“Yin and Yang: Balancing and Answering Binary Visual Questions” by Zhang and Goyal et al. They provide a balanced dataset in which there are a pairs of scenes for every question, such that the answer to the question is “yes” for one scene, and “no” for the other for the exact same question. \n- Perhaps because the authors study a very specific kind of question, they limit their analysis to only three modules and two structures (tree & chain). However, in the most general setting NMN will form a DAG and it would have been interesting to see what form of DAGs generalize better than other. \n- It is not clear to me how the analysis done in this paper will generalize to other more complex datasets where the network layout NMN might be more complex, the number of modules and type of modules might also be more. Because, the results are only shown on one dataset, it is harder to see how one might extend this work to other form of questions on slightly harder datasets. \n\nOther Questions / Remarks:\n- Given that the accuracy drop is very significant moving from NMN-Tree to NMN-Chain, is there an explanation for this drop? \n- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper. \n- Small typo in the last line of section 4.3 on page 7. It should say: This is in stark contrast with “NMN-Tree” …..\n- Small typo in the “Layout induction” paragraph, line 6 on Page 7:  … and for $p_0(tree) = 0.1$ and when we use the Find module   \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper presents a targeted empirical evaluation of generalization in models\nfor visual reasoning. The paper focuses on the specific problem of recognizing\n(object, relation, object) triples in synthetic scenes featuring letters and\nnumbers, and evaluates models' ability to generalize to the full distribution of\nsuch triples after observing a subset that is sparse in the third argument. It\nis found that (1) NMNs with full layout supervision generalize better than other\nstate-of-the art visual reasoning models (FiLM, MAC, RelNet), but (2) without\nsupervised layouts, NMNs perform little better than chance, and without\nsupervised question attentions, NMNs perform better than the other models but\nfail to achieve perfect generalization.\n\nSTRENGTHS\n- thorough analysis with a good set of questions\n\nWEAKNESSES\n- some peculiar evaluation and presentation decisions\n- introduces *yet another* synthetic visual reasoning dataset rather than\n  reusing existing ones\n\nI think this paper would have been stronger if it investigated a slightly\nbroader notion of generalization and had some additional modeling comparisons.\nHowever, I found it interesting and think it successfully addresses the set of\nquestions it sets out to answer. If it is accepted, there are a few things that\ncan be done to improve the experiments.\n\nMODELING AND EVALUATION\n\n- Regarding the dataset: the proliferation of synthetic reasoning datasets is\n  annoying because it makes it difficult to compare results without downloading\n  and re-running a huge amount of code. (The authors have, to their credit, done\n  so for this paper.) I think all the experiments here could have been performed\n  successfully using either the CLEVR or ShapeWorld rendering engines: while the\n  authors note that they require a \"large number of different objects\", this\n  could have been handled by treating e.g. \"red circle\" and \"red square\" as\n  distinct atomic primitives in questions---the fact that redness is a useful\n  feature in both cases is no different from the fact that a horizontal stroke\n  detector is useful for lots of letters.\n\n- I don't understand the motivation behind holding out everything on the\n  right-hand side. For models that can't tell that the two are symmetric, why\n  not introduce sparsity everwhere---hold out some LHSs and relations?\n  \n- Table 1 test accuracies: arbitrarily reporting \"best of 3\" for some model /\n  dataset pairs and \"confidence interval of 5\" for others is extremely\n  unhelpful: it would be best to report (mean / max / stderr) for 5. Also, it's\n  never stated which convidence interval is reported.\n\n- Table 1 baselines: why not run Conv+LSTM and RelNet with easier #rhs/lhs data?\n\n- How many MAC cells are used? This can have significant performance\n  implications. I think if you used their code out of the box you'll wind up\n  with way bigger structures than you need for this task.\n\n- I'm not sure how faithful the `find` module used here is to the one in the\n  literature, and one of the interesting claims in this work is that module\n  implementation details matter! The various Hu papers use an attentional\n  parameterization; the use of a ReLU and full convolution in Eq. 14 suggest\n  that that one here can pass around more general feature maps. This is fine but\n  the distinction should be made explicit, and it would be interesting to see\n  additional comparisons to an NMN with purely attentional bottlenecks.\n\n- Why do all the experiments after 4.3 use #rhs/lhs of 18? If it was 8 it would\n  be possible to make more direct comparisons to the other baseline models.\n\n- The comparison to MAC in 4.2 is unfair in the following sense: the NMN\n  effectively gets supervised textual attentions if the right parameters are\n  always plugged into the right models, while the MAC model has to figure out\n  attentions from scratch. A different way of structuring things would be to\n  give the MAC model supervised parameterizations in 4.2, and then move the\n  current MAC experiment to 4.3 (since it's doing something analogous to\n  \"parameterization induction\".\n  \n- The top-right number in Table 4---particularly the fact that it beats MAC and\n  sequential NMNs under the same supervision condition---is one of the most\n  interesting results in this paper. Most of the work on relaxing supervision\n  for NMNs has focused on (1) inducing new question-specific discrete structures\n  from scratch (N2NMN) or (2) finding fixed sequential structures that work well\n  in general (SNMN and perhaps MAC). The result this paper suggests an\n  alternative, which is finding good fixed tree-shaped structures but continuing\n  to do soft parameterization like N2NMN.\n\n- The \"sharpness ratio\" is not super easy to interpret---can't you just report\n  something standard like entropy? Fig 4 is unnecessary---just report the means.\n\n- One direction that isn't explored here is the use of Johnson- or Hu-style\n  offline learning of a model to map from \"sentences\" to \"logical forms\". To the\n  extent that NMNs with ground-truth logical forms get 100% accuracy, this turns\n  the generalization problem studied here into a purely symbolic one of the kind\n  studied in Lake & Baroni 18. Would be interesting to know whether this makes\n  things harder (b/c no grounding signal) or easier (b/c seq2seq learning is\n  easier.)\n\nPRESENTATION\n\n- Basically all of the tables in this paper are in the wrong place. Move them\n  closer to the first metnion---otherwise they're confusing.\n\n- It's conventional in this conference format to put all figure captions below\n  the figures they describe. The mix of above and below here makes it hard to\n  attach captions to figures.\n\n- Some of the language about how novel the idea of studying generalization in\n  these models is a bit strong. The CoGenT split of the CLEVR dataset is aimed\n  at answering similar questions. The original Andreas et al CVPR paper (which btw\n  appears to have 2 bib entries) also studied generalization to structurally\n  novel inputs, and Hu et al. 17 notes that the latent-variable version of this\n  model with no supervision is hard to train.\n\nMISCELLANEOUS\n\n- Last sentence before 4.4: \"NMN-Chain\" should be \"NMN-Tree\"?\n\n- Recent paper with a better structure-induction technique:\n  https://arxiv.org/abs/1808.09942. Worth citing (or comparing if you have\n  time!)",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}