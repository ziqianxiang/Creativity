{
    "Decision": {
        "metareview": "This paper presents good empirical results on an important and interesting task (translation between several language pairs with a single model). There was solid communication between the authors and the reviewers leading to an improved updated version and consensus among the reviewers about the merits of the paper.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Accept"
    },
    "Reviews": [
        {
            "title": "Effective knowledge distillation for multilingual NMT, at the cost of increased training time",
            "review": "The authors apply knowledge distillation for many-to-one multilingual\nneural machine translation, first training separate models for each language\npair. For most language pairs, performance matches or improves upon\nsingle-task baselines.\n\nStrengths:\n\nImprovements upon the baselines are fairly impressive, especially for the\n44-language model.\n\nThe approach is quite simple and could be easily implemented by other groups.\n\nThe paper is well-written and easy to understand.\n\nAt inference, only a single model needs to be retained, which is memory-efficient.\n\nWeaknesses:\n\nThe authors only test distillation in a many-to-one scenario. I believe that\nproviding results for many-to-many multilingual NMT would be valuable.\n\nOverall, this approach increases training time as all single-task models\nmust have converged before beginning distillation.\n\nThe authors provide no direct comparison to other work, which makes it hard to\nknow how strong the baselines are. At least for WMT, I would suggest reporting\nresults with mteval-v13a (or SACREBLEU), so that results can be compared against\nofficial results.\n\nQuestions:\n\nFor the top-K approach, do you normalize the top K probabilities so that they\nsum to 1 or not?\n\nDid you consider applying sequence knowledge distillation (Kim and Rush, 2016)\n(using the baseline beam search output as references) instead of word knowledge\ndistillation?\n\n***\nEDIT: In my opinion, the changes made after the review period clearly improve the quality of the paper. I am increasing my rating from 6 to 7.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Straightforward, effective technique for improving multilingual NMT, some experiments missing.",
            "review": "Summary: Train a multilingual NMT system using the technique of Johnson et al (2017), but augment the standard cross-entropy loss with a distillation component based on individual (single-language-pair) teacher models. Periodically compare the validation BLEU score of the multilingual model with that of each individual model, and turn off distillation for language pairs where the multilingual model is better. On three different corpora (IWSLT, WMT, TED) with into-English translation from numbers of source languages ranging from 6 (WMT) to 44 (TED), this technique outperforms standard distillation for every language pair, and outperforms the individual models for most language pairs. Supplementary experiments justify the strategy of selectively turning off distillation, and quantify the effect using only the top 8 vocabulary items in distillation.\n\nThe main idea makes sense, and the results are very convincing, especially since it appears that hyper-parameters were not tuned extensively (eg, weight of 0.5 on the distillation loss, for all language pairs). Implementation should be very straightforward, especially with the trick of pre-computing top-k probabilities from the teacher model at each corpus position. One small barrier to practical application that the authors fail to acknowledge is the requirement to train individual models, which will at least double training time compared to a single multilingual model.\n\nThe main missing experiment is higher-capacity multilingual models, which Johnson et al show to be beneficial in settings with a large number of language pairs. Using a multilingual model of the same (relatively small) size as the individual models as is done here is likely to be suboptimal, especially for the 44-language pair TED setting. A related point is that the corpora used seem to be quite small (eg 4.5M and 1M sentences for WMT Czech and German, respectively, while the available training corpora are closer to 15M and 4.5M). Although performance relative to individual models is still impressive - and seems to be better than than in previous work - this makes the experiments comparing to the multilingual baseline less meaningful.\n\nAlso missing are experiments on out-of-English translation, which would establish the viability of the proposed technique for many-to-many translation via bridging. Out-of-English is a more difficult problem than into-English. I can’t see any reason the proposed technique wouldn’t also work in this setting, but this remains to be shown.\n\nAlthough it’s great that the technique is shown to work without embellishments, there are a few obvious strategies it would have been interesting to explore, such as making the weight on the distillation loss dependent on the difference in performance between the multilingual and individual models; and allowing for the distillation loss to be turned back on if the performance of the multilingual model starts to drift back down for a particular language pair. I also wondered about the effect of the gradient accumulation strategy in algorithm 1, where individual batches from each language pair are effectively grouped into one giant batch for the purpose of parameter updates. I can see that this could stabilize training, but it would be good to know whether it’s crucial for success, especially when the number of language pairs is large.\n\nFurther details:\n\nAs aforementioned -> As mentioned\n\n(1) 2nd line: Doesn't make sense as written. You need to distinguish the gold\ny_t from hypothesized ones in the 1() function.\n\nAbove (2): is served as -> serves as\n\n3.2 First paragraph. Since D presumably consists of D^l for all languages l,\nL_ALL(D,...) should be a function of teacher parameters theta^l for all\nlanguages l rather than just one as written.\n\nIn top-K distillation, is the teacher distribution renormalized or simply\ntruncated?\n\nGeneralization analysis, pg 8: presumably you are sampling from N(0, sigma^2) -\nthis should be described as such.\n\nReference: \n\nJohnson et al, “Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation” TACL, 2017.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Solid experimentation but...",
            "review": "... I would have liked to see some more insights.\n\nThe authors present a method for distilling knowledge from individual models to train a multilingual model. The motivation stems from the fact that while most s-o-t-a multilingual models are compact (as compared to k individual models) they fall short of the performance of the individual models. The authors demonstrate that using knowledge distillation, the performance of the multilingual model can actually be better than the individual models. \n\nPlease find below my comments and questions.\n\n1) The authors have done a commendable job of validating their hypothesis on multiple datasets. Solid experimentation is definitely the main strength of this paper.\n\n2) However, this strength also makes way for a weakness. The entire experimental section is just filled with tables and numbers. The same message is repeated across these multiple tables (multi+distill > single > multi). Beyond this message there are no other insights. For example, \n\n- How does the performance depend on the divergence between source and target language?\n- Why is there more important on some languages and less on others ?\n- Why are the improvements on the TED dataset so much higher as compared to the other 2 datasets.\n- What happens when the target language is something other than English? All the experiments report results from X-->English, why not in the other direction? The model then is not really \"completely\" multilingual. It is multi-source-->single target. \n- Can you comment on the total training time ?\n- What happens when you do not stop the distillation even when the accuracy of the student crosses that of the teachers ? What do you mean by accuracy here? Only later when you mention that \\threshold = 1 BLEU it became clear that accuracy means BLEU in this context ?\n\n3) Is it all worth it? One disappointing factor is that end of all this effort where you train K individual models and one monolithic model with distillation, the performance gain for most language pairs is really marginal (except on the TED dataset). I wonder if the same improvements could have been obtained by even more carefully fine tuning the baseline models itself.\n\n4) On a positive note, I like the back-distillation idea and the experiments on top-K distillation\n\n+++++++++++++++++++\nI have updated my rating after reading author's responses\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}