{
    "Decision": {
        "metareview": "The authors propose a dynamic inference technique for accelerating neural network prediction with minimal accuracy loss. The method are simple and effective. The paper is clear and easy to follow. However, the real speedup on CPU/GPU is not demonstrated beyond the theoretical FLOPs reduction. Reviewers are also concerned that the idea of dynamic channel pruning is not novel. The evaluation is on fairly old networks.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "borderline"
    },
    "Reviews": [
        {
            "title": "review comments on “Dynamic Channel Pruning: Feature Boosting and Suppression” ",
            "review": "This paper propose a channel pruning method for dynamically selecting channels during testing. The analysis has shown that some channels are not always active. \n\nPros:\n- The results on ImageNet are promising. FBS achieves state-of-the-art results on VGG-16 and ResNet-18.\n- The method is simple yet effective.\n- The paper is clear and easy to follow.\n\nCons:\n- Lack of experiments on mobile networks like shufflenets and mobilenets\n- Missing citations of some state-of-the-art methods [1] [2].\n- The speed-up ratios on GPU or CPU are not demonstrated. The dynamic design of Dong et al., 2017 did not achieve good GPU speedup.\n- Some small typos.\n\n[1] Amc: Automl for model compression and acceleration on mobile devices\n[2] Netadapt: Platform-aware neural network adaptation for mobile applications ",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review for \"Dynamic Channel Pruning: Feature Boosting and Suppression\"",
            "review": "The authors propose a dynamic inference technique for accelerating neural network prediction with minimal accuracy loss.  The technique prunes channels in an input-dependent way through the addition of auxiliary channel saliency prediction+pruning connections.\n\nPros:\n- The paper is well-written and clearly explains the technique, and Figure 1 nicely summarizes the weakness of static channel pruning\n- The technique itself is simple and memory-efficient\n- The performance decrease is small\n\nCons:\n- There is no clear motivation for the setting (keeping model accuracy while increasing inference speed by 2x or 5x)\n- In contrast to methods that prune weights, the model size is not reduced, decreasing the utility in many settings where faster inference and smaller models are desired (e.g. mobile, real-time)\n- The experiments are limited to classification and fairly dated architectures (VGG16, ResNet-18)\n\nOverall, the method is nicely explained but the motivation is not clear.  Provided that speeding up inference without reducing the size of the model is desirable, this paper gives a good technique for preserving accuracy.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "feature suppression to speed up training CNN",
            "review": "This manuscript presents a nice method that can dynamically prune some channels in a CNN network to speed up the training. The main strength of the proposed method is to determine which channels to be suppressed based upon each data sample without incurring too much computational burden or too much memory consumption.  The good thing is that the proposed pruning strategy does not result in a big performance decrease. Overall, this is a nicely written paper and may be empirically useful for training a very large CNN. Nevertheless, the authors did not present a real-world application in which it is important to speed up by 2 or 3 times at a small cost, so it is hard to judge the real impact of the proposed method.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review comments on “Dynamic Channel Pruning: Feature Boosting and Suppression”",
            "review": "Summary: \n\nThis paper proposed a feature boosting and suppression method for dynamic channel pruning. To be specific, the proposed method firstly predicts the importance of each channel and then use an affine function to amplify/suppress the importance of different channels. However, the idea of dynamic channel pruning is not novel. Moreover, the comparisons in the experiments are quite limited. \n\nMy detailed comments are as follows.\n\n\nStrengths:\n\n1. The motivation for this paper is reasonable and very important. \n\n2. The authors proposed a new method for dynamic channel pruning.\n\nWeaknesses:\n\n1. The idea of dynamic channel pruning is not novel. In my opinion, this paper is only an extension to Network Slimming (Liu et al., 2017). What is the essential difference between the proposed method and Network Slimming?\n\n2. The writing and organization of this paper need to be significantly improved. There are many grammatical errors and this paper should be carefully proof-read.\n\n3. The authors argued that the importance of features is highly input-dependent. This problem is reasonable but the proposed method still cannot handle it. According to Eqn. (7), the prediction of channel saliency relies on a data batch rather than a single data. Given different inputs in a batch, the selected channels should be different for each input rather than a general one for the whole batch. Please comment on this issue.\n\n4. The proposed method does not remove any channels from the original model. As a result, both the memory and the computational cost will not be reduced. It is confusing why the proposed method can yield a significant speed-up in the experiments.\n\n5. The authors only evaluate the proposed method on shallow models, e.g., VGG and ResNet18. What about the deeper model like ResNet50 on ImageNet?\n\n6. It is very confusing why the authors only reported top-5 error of VGG. The results of top-1 error for VGG should be compared in the experiments.\n\n7. Several state-of-the-art channel pruning methods should be considered as the baselines, such as ThiNet (Luo et al., 2017), Channel pruning (He et al., 2017) and DCP (Zhuang et al., 2018)\n[1] Channel pruning for accelerating very deep neural networks. CVPR 2017.\n[2] Thinet: A filter level pruning method for deep neural network compression. CVPR 2017.\n[3] Discrimination-aware Channel Pruning for Deep Neural Networks. NIPS 2018.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}