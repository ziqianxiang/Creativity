{
    "Decision": {
        "metareview": "This paper proposes a new approach to domain adaptation based on sub-spacing, such that outliers are filtered out. While similar ideas have been used e.g. in multi-view learning, their application to domain adaptation makes it a novel and interesting approach. \n\nWhile the above is considered by the AC an adequate contribution to ICLR, the authors are encouraged to investigate further the implications of the assumptions made, in a way that the derived criteria seem less heuristic, as R1 pointed out.\n\nThere had been some concerns regarding the experiments, but the authors have been very active in the rebuttal period and addressed these concerns satisfactorily.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Attacking the domain adaptation problem from an interesting angle"
    },
    "Reviews": [
        {
            "title": "effective method for open set domain adaptation",
            "review": "This paper tackles the problem of open-set unsupervised domain adaptation with a method based on \nsubspace learning. Specifically the proposed approach searches for two low-dimensional spaces, one shared \nby the known source and target categories while the other is specific for the unknown classes. \n\nOverall the paper is well organized and easy to read. The mathematical formulation of the method is sound and\nclearly explained in all its variants.\n\nI have few concerns \n- it would be good to have the \"average\" columns in the tables reporting the experimental results. This will help to have an overall idea on the performance of the different proposed and baseline methods.\n- it is not clear whether the authors are reporting the results of AODA from the original paper or if they re-ran the code to get the recognition accuracies. For instance in table 3 the result 70.1 for A->W is lower than those reported in the original paper for this setting.\n- the paper does not discuss how the hyperparameters of the methods are chosen. Only an analysis on epsilon is provided. It would be very helpful to understand the procedure used to select the values of alpha, beta and lambda and to evaluate the robustness of the method to those parameters. Moreover,  the value of the dimensionality d is not explicitly indicated in the text. This should be added together with a discussion about if and how the subspace disagreement measure (that was introduced for closed set domain adaptation) is reliable in the open set condition.\n\n\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A novel method for Open-set Domain Adaptation which is a rather new problem and is thus interesting. Experimental evaluation is good, but the method requires more justification and analysis.",
            "review": "The paper addresses the problem of Domain Adaptation (DA) in an open setting (OSDA): while traditional DA assumes that the set of classes of the source and the target are identical, in Open-set DA, there are samples in the target which do not belong to any class in the source (unknown classes that I will outliers in this review). The main difficulty of Open-set DA is to simultaneously discard outliers and correctly classify other samples in the target. There are only two papers on Open-set DA so far, Busto'17 and Saito'18.\nThe method proposed by the authors can be summarized in a single equation, eq. 2, where they aim at learning a linear mapping to a latent space, which can be separated into two sub-spaces U (private space) and V (shared space) such that target outliers will be mapped to 0 in V while source and target non-outliers will be mapped to 0 in U, and hence separate outliers with non-outliers. To solve eq. 2, the authors convert it to Eqs. 3, 4, and 5 and apply techniques in Lee'07 and Mairal'14. The authors propose an extension for learning a linear classifier simultaneously and an extension for incorporating also unknown source classes (i.e. source outliers) when appropriate. An experimental evaluation on 2 datasets show the good performance of the method.\n\nPros:\n-A novel method for a rather new and understudied so far, the work is then interesting for this setting\n-Good results reported\n\nCons:\n-The criterion used for choosing when examples are outliers seems heuristic, more discussion would be welcomed as well as some qualitative analysis for showing the interest of the method\n-Existing baseline of Saito'18 not used in the 1st experiment\n-Some parts require more justification\n\n*Comments:\n\n-The idea of the method is similar to the one of Jia'10 (Eq.6) for multi view learning, but this is rather new for Open-set DA.\n\n-In order to separate target samples to either private or shared, the authors \"encourage that either of these two parts (i.e. vectors T_i^u and T_i^v) goes to zero for each sample\", which is reasonable. To achieve this the authors use sparse coding method coming from Lee'07. However, this does not make sense to me, because the sparse coding algorithm will encourage both T_i^u and T_i^v to be sparse, but nothing forces one of them to go to the zero-vector.\nThe authors should then better justify this choice. In particular, I wonder if adding explicitly the criterion used for identifying outliers as a new constraint to satisfy. Then, the optimization problem considered would make more sense to me.\n\nAnyway, the authors could perform additional experiments to show the effectiveness of their method: (i) apply on a classic DA problem where we will expect that ||T^u|| or ||U|| (private subspace for outliers) should be close to zero. \nAdd a qualitative analysis on the values of  |T^u|| and |T^v|| - both in Open-set DA and classic DA - showing that the results are as expected. \n\n- The 1st method (Eq.2) learns the latent space without using any label in the source (i.e there are only two labels: outlier or non-outlier, and all source samples are labeled non-outlier). Thus, the authors resort to the assumption that outliers are farther from source samples than non-outliers. This assumption is strong and may not hold in practice for two reasons: (1) the domain shift can be large and (2) without clustering techniques, many outliers can easily fall into the safe non-outliers zone (consider 0-4 for outliers and 5-9 for non-outliers, high chance this method will incorrectly classify 0 or 3 as non-outliers since 6,8,9 are already non-outliers). \n\n- The Lagrange dual method (Lee'07, Eq. 6) solves an optimization problem with multiple quadratic constraints, i.e. ||U_j||^2 \\le c for every j. However, the authors apply it to solve a problem (eq. 3 and 4) with a single linear constraint which is not quadratic: \\sum ||U_j|| \\le 1. Please explain:\n(i) Why do you use that constrain instead of the one in Lee'07?\n(ii) With your constrain, does the Lagrange dual method still work? \n\n-The authors mention that they reported the results reported by Busto'17 in their experiment. Does this mean that the experiments were not reproduced? If so this seems rather unfair for other baselines since they may have worked on different instances. \nMany baselines are not specific to Open-set DA, so it is rather expected to see bad results.\nSince OSDA is new, it is true that there exists only two true baselines: Busto'17 and Saito'18. However, Saito'18 does not appear in BCIS benchmark (although appears in Office benchmark). Please add Saito'18 to the BCIS benchmark.\n\n-The authors use fixed parameters for all the subproblems, I am a bit surprised by this choice, I would rather expect a parameterization task-dependent. Does this mean that the method is hard to tune ?\n\n-The method seems complex, is there any convergence guarantee?\n\n--\nAfter rebuttal: thanks many points were answered.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Intriguing formulation and performance; flaws in experiments",
            "review": "Pros:\n- Paper proposes a somewhat complicated but easy to understand idea for open set classification. Formulation is quite intriguing.\n- Outperforming recent baselines on most scenarios, despite being a linear classifier on fixed CNN features.\n\nCons:\n- Experiment setup somewhat flawed (but the same flaw is in prior work too)\n    To elaborate: DeCAF7 is trained on ImageNet, which gives the underlying network extra categorical information of the 1000 classes. Some of these clases are arguably in the \"unknown classes\" in the open set setting. This may jeopardize the premise since the feature knows those classes are semantically different from known classes. Unfortunately (Busto & Gall, 2017) and (Saito et al., 2018) do this too.\n    This is especially problematic since DeCAF7 has a near-linear relationship to the final sigmoid logits, which are the 1000-way ImageNet class scores. This makes the authors formulation (separate subspaces for known and unknown classes) more easily exploit this leaked information. This is because the 1000-way scores obviously have subspaces for all 1000 ImageNet classes, and by extension, the \"known\" and \"unknown\" classes in the open set setting. \n    If this is true and is the main reason that the proposed method outperforms, I would not consider the conclusion of the paper very informative. Instead, its signifies the need of a better experiment setup for the problem.\n    A way to strengthen the paper is to use a network pre-trained on other datasets (e.g. Places, or a subset of ImageNet) to verify the findings of the paper.\n- Lacks clarity for what is being done at test time. \n    I cannot find whether the final SVM is trained on original DeCAF features, or S and T. If it is the latter, how are the representations of target domain data obtained at test time? Are they d dimentional or 2d dimentional?\n    Can you clarify that the test samples are not used for unsupervised training?\n- Experiment elaborate but feels incomplete.\n    It feels like the authors are proposing 3 variations of the method, and there is not one of them that consistently outperform the others. If so, the paper would lack some ablation analysis that provides insights of what makes the FRODA-SVM outperform prior art. For example, how much do the hyperparameters matter? What happens if e.g. d or lambda1 is very large/small?\n\nClarity:\n- Abstract spends too much time on defining problem setup\n- \"Faster than prior work\" refers to the training time, and excludes the DeCAF feature extraction.\n\nOriginality:\nI am not familiar with the related work.\n\nSignificance:\nIt is quite impressive that a linear model on fixed CNN activations outperforms prior art. However, see the first point in the cons.\n\n\n-----------\nEdit: most of the issues listed in \"cons\" are addressed. Although the additional experiments are not very comprehensive, they can better support the claims. I am bumping up the rating to 7.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}