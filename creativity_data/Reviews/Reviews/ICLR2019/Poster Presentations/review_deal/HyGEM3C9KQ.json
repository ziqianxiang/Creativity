{
    "Decision": {
        "metareview": "\npros:\n- Identification of several interesting problems with the original DNC model: masked attention, erasion of de-allocated elements, and sharpened temporal links\n- An improved architecture which addresses the issues and shows improved performance on synthetic memory tasks and bAbI over the original model\n- Clear writing\n\ncons:\n- Does not really show this modified DNC can solve a task that the original DNC could not and the bAbI tasks are effectively solved anyway.  It is still not clear whether the DNC even with these improvements will have much impact beyond these toy tasks.\n\nOverall the reviewers found this to be a solid paper with a useful analysis and I agree.  I recommend acceptance.\n\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Good paper; solid improvements to DNC"
    },
    "Reviews": [
        {
            "title": "Solid improvements to DNC.",
            "review": "Summary:\n\nThis paper is built on the top of DNC model. Authors observe a list of issues with the DNC model: issues with deallocation scheme, issues with the blurring of forward and backward addressing, and issues in content-based addressing. Authors propose changes in the network architecture to solve all these three issues. With toy experiments, authors demonstrate the usefulness of the proposed modifications to DNC. The improvements are also seen in more realistic bAbI tasks.\n\nMajor Comments:\n\nThe paper is well written and easy to follow. The proposed improvements seem to result in very clear improvements. The proposed improvements also improve the convergence of the model. I do not have any major concerns about the paper. I think that contributions of the paper are good enough to accept the paper.\n\nI also appreciate that the authors have submitted the code to reproduce the results.\n\nI am curious to know if authors observe similar convergence gains in bAbI tasks as well. Can you please provide the mean learning curve for bAbI task for DNC vs proposed modifications?\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Promising modifications to the Differentiable Neural Computer (DNC) architecture, but needs stronger empirical evidence ",
            "review": "\nOverview: \nThis paper proposes modifications to the original Differentiable Neural Computer architecture in three ways. First by introducing a masked content-based addressing which dynamically induces a key-value separation. Second, by modifying the de-allocation system by also multiplying the memory contents by a retention vector before an update. Finally, the authors propose a modification in the link distribution, through renormalization. They provide some theoretical motivation and empirical evidence that it helps avoiding memory aliasing. \nThe authors test their approach in the some algorithm task from the DNC paper (Copy, Associative Recall and Key-Value Retrieval), and also in the bAbi dataset.\n\n\nStrengths: Overall I think the paper is well-written, and proposes simple adaptions to the DNC architecture which are theoretically grounded and could be effective for improving general performance. Although the experimental results seem promising when comparing the modified architecture to the original DNC, in my opinion there are a few fundamental problems in the empirical session (see weakness discussion bellow).\n\nWeaknesses: Not all model modifications are studied in all the algorithmic tasks. For example, in the associative recall and key-value retrieval only DNC and DNC + masking are studied. \n\nFor the bAbi task, although there is a significant improvement (43%) in the mean error rate compared to the original DNC, it's important to note that performance in this task has improved a lot since the DNC paper was release. Since this is the only non-toy task in the paper, in my opinion, the authors have to discuss current SOTA on it, and have to cite, for example the universal transformer[1], entnet[2], relational nets [3], among others architectures that shown recent advances on this benchmark. \nMoreover, the sparse DNC (Rae el at., 2016) is already a much better performant in this task. (mean error DNC: 16.7 \\pm 7.6, DNC-MD (this paper) 9.5 \\pm 1.6, sparse DNC 6.4 \\pm 2.5). Although the authors mention in the conclusion that it's future work to merge their proposed changes into the sparse DNC, it is hard to know how relevant the improvements are, knowing that there are much better baselines for this task.\nIt would also be good if besides the mean error rates, they reported best runs chosen by performance on the validation task, and number of the tasks solve (with < 5% error) as it is standard in this dataset.\n\n\nSmaller Notes. \n1) In the abstract, I find the message for motivating the masking from the sentence  \"content based look-up results... which is not present in the key and need to be retrieved.\"  hard to understand by itself. When I first read the abstract, I couldn't understand what the authors wanted to communicate with it. Later in 3.1 it became clear. \n\n2) page 3, beta in that equation is not defined\n\n3) First paragraph in page 5 uses definition of acronyms DNC-MS and DNC-MDS before they are defined.\n\n4) Table 1 difference between DNC and DNC (DM) is not clear. I am assuming it's the numbers reported in the paper, vs the author's implementation? \n\n5)In session 3.1-3.3, for completeness. I think it would be helpful to explicitly compare the equations from the original DNC paper with the new proposed ones. \n\n--------------\n\nPost rebuttal update: I think the authors have addressed my main concern points and I am updating my score accordingly. ",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Well written, has implications beyond DNC",
            "review": "The authors propose three improvements to the DNC model: masked attention, erasion of de-allocated elements, and sharpened temporal links --- and show that this allows the model to solve synthetic memory tasks faster and with better precision. They also show the model performs better on average on bAbI than the original DNC.\n\nThe negatives are that the paper does not really show this modified DNC can solve a task that the original DNC could not. As the authors also admit, there have been other DNC improvements that have had more dramatic improvements on bAbI.\n\nI think the paper is particularly clearly written, and I would vote for it being accepted as it has implications beyond the DNC. The fact that masked attention works so much better than the standard cosine-weighted content-based attention is pretty interesting in itself. The insights (e.g. Figure 5) are interesting and show the study is not just trying to be a benchmark paper for some top-level results, but actually cares about understanding a problem and fixing it. Although most recent memory architectures do not seem to have incorporated the DNC's slightly complex memory de-allocation scheme, any resurgent work in this area would benefit from this study.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}