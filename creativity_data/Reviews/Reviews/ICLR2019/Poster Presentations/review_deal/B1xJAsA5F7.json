{
    "Decision": {
        "metareview": "The revisions made by the authors convinced the reviewers to all recommend accepting this paper. Therefore, I am recommending acceptance as well. I believe the revisions were important to make since I concur with several points in the initial reviews about additional baselines. It is all too easy to add confusion to the literature by not including enough experiments. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "after revisions the reviewers reached a consensus on accepting the paper"
    },
    "Reviews": [
        {
            "title": "review on \"Learning Multimodal Graph-to-Graph Translation for Molecule Optimization\"",
            "review": "This paper proposed an extension of JT-VAE [1] into the graph to graph translation scenario. To help make the translation model predicting diverse and valid outcomes, the author added the latent variable to capture the multi-modality, and an adversarial regularization in the latent space. Experiment on molecule translation tasks show significant improvement over existing methods.\n\nThe paper is well written. The author explains the GNN, JT-VAE and GAN in a very organized way. The idea of modeling the molecule optimization as translation problem is interesting, and sounds more promising (and could be easier) than finding promising molecule from scratch. \n\nTechnically I think it is reasonable to use latent variable model to handle the multi-modality. Using GAN to align the distribution is also a well adapted method recently. Thus overall the method is not too surprising to me, but the paper executes it nicely. Given the significant empirical improvement, I think this paper has made a valid contribution to the area.\n\nRegarding the results in Table 1, I’m curious why the VSeq2Seq is better than JT-VAE and GCPN (given the latter two are the current state-of-the-art)? \n\nAnother thing I’m curious about is the ‘stacking’ of this translation model. Suppose we keep translating the molecule X1 -> X2 -> X3 ...  using the learned translation model, would the model still gets improvement after X2? When would it get maxed out?\nOr if we train with ‘path’ translation (i.e., train with improvement path with variable length), instead of just the pair translation, would that be helpful? I’m not asking for more experiments, but some discussion might be useful.\n\n[1] Jin et.al, Junction tree variational autoencoder for molecular graph generation, ICML 2018\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea, issues in the execution",
            "review": "Update:\nThe score has been updated to reflect the authors' great efforts in improving the manuscript. This reviewer would suggest to accept the paper now.\n\n\nOld Review Below:\n\nThe paper describes a graph-to-graph translation model for molecule optimization inspired from matched molecular pair analysis, which is an established approach for optimizing the properties of molecules. The model extends a chemistry-specific variational autoencoder architecture, and is assessed on a set of three benchmark tasks.\n\n\nWhile the idea of manuscript is interesting and promising for bioinformatics, there are several outstanding problems, which have to be addressed before it can be considered to be an acceptable submission. This referee is willing to adjust their rating if the raised points are addressed. Overall, the paper might also be more suited at a domain-specific bioinformatics conference.\n\n\nMost importantly, the paper makes several claims that are currently not backed up by experiments and/or data. \n\nFirst, the authors claim that MMPs “only covers the most simple and common transformation patterns”. This is not correct, since these MMP patterns can be as complex as desired. Also, it is claimed that the presented model is able to “learn far more complex transformations than hard-coded rules”. The authors will need to provide compelling evidence to back up these claims. At least, a comparison with a traditional MMPA method needs to be performed, and added as a baseline. Also, it has to be kept in mind that the reason MMPA was introduced was to provide an easily interpretable method, which performs only local transformations at one part of the molecule. “Far more complex transformations” may thus not be desirable in the context of MMPA. Can the authors comment on that?\n\nSecond, the authors state that they “sidestep” the problem of non-generalizing property predictors in reinforcement learning, by “unifying graph generation and property estimation in one model”. How does the authors’ model not suffer from the same problem? Can they provide evidence that their model is better in property estimation than other models?\n\n\nIn the first benchmark (logP) the GCPN baseline is shown, but in the second benchmark table, the GCPN baseline is missing. Why? The GCPN baseline will need to be added there. Can the authors also comment on how they ensure the comparison to the GPCN and VSeq2Seq is fair? Also, can the authors comment on why they think the penalized logP task is a good benchmark?\n\nAlso, the authors write that Jin et al ICML 2018 (JTVAE) is a state of the model. However, also Liu et al NIPS 2018 (CGVAE) state that their model is state of the art. Unfortunately, both JTVAE and CGVAE were never compared against the strongest literature method so far, by Popova et al, which was evaluated on a much more challenging set of tasks than JT-VAE and CGVAE. The authors cite this paper but do not compare against it, which should to be rectified. This referee understands it is more compelling to invent new models, but currently, the literature of generative models for molecules is in a state of anarchy due to lack of solid comparison studies, which is not doing the community a great service.\n\n\nFurthermore, the training details are not described in enough detail. \nHow exactly are the pairs selected? Where do the properties for the molecules come from? Were they calculated using the logP, QED and DRD2 models? How many molecules are used in total in each of these tasks?\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A paper proposing a quite complex system (with no explicit probabilistic factorisation) which seems to obtain good experimental results.",
            "review": "As a reviewer I am expert in learning in structured data domains. \nThe paper proposes a quite complex system, involving many different choices and components, for obtaining chemical compounds with improved properties starting from a given corpora. \nOverall presentation is good, although some details/explanations/motivations are missing. I guess this was due to the need to keep the description of a quite complex system in the given space limit. Such details/explanations/motivations could, however, have been inserted in the appendix. As an example, let consider the description of the decoding of the junction tree. In that section, it is not explained when the decoding process stops. My understanding is that this is when, being in the root node, the choice is to go back to the parent (that does not exist). In the same section, it is not explicitly discussed that the probability to select between adding a node or going back to the parent should have a different distribution according to \"how many\" nodes have been generated before, i.e. we do not want to have a high probability to \"go back\" at the beginning of the decoding, while I guess it is desirable that such probability increases proportionally with the number of generated nodes. This leads to an issue that I personally think is important: the paper does lack an explicit probabilistic modelling of the different involved components, which may help for a better understanding of all the assumptions made in the construction of the proposed system. \nThe complexity of the proposed system is actually an issue since the author(s) do not attempt (except for  the presence or absence of the adversarial scaffold regularization and the number of trials in appendix) an analysis of the influence of the different components (and corresponding hyper-parameters). \nReference to previous relevant work seems to be complete.\nI think the paper is relevant for ICLR (although there is no explicit analysis of the obtained hidden representations) and of interest for a good portion of attendees.\n\nMinor issues:\n- Tree and Graph Encoding: asynchronous update implies that T should be a multiple of the diameter of the input graph to guarantee a proper propagation of information across the graph. A discussion about that would be needed.\n- eq.(6): \\mathbb{u}^d is not defined.\n- Section 3.3:\n   - first paragraph is not clear. An example and/or figure is needed to understand the argument, which is related to the presence of cycles.\n  - the definition of f(G_i) involves  \\mathbb{x}_u. I guess they should be  \\mathbb{x}_u^G.\n  - not clear how the log-likelihood of ground truth subgraphs is computed given that the predicted junction tree, especially at the beginning of training, may be way different from the correct one. Moreover, what is the assumed bias of this choice ?\n- Table I: please provide an explanation of why using a larger value for \\delta does provide worst performance than a smaller value. From an optimisation point of view it should provide at least an as good performance. This is a clear indication that the used procedure is suboptimal.\n- diversity could be influenced by the cardinality of the sample. Is this false ? please discuss why diversity is (not) biased versus larger sets.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}