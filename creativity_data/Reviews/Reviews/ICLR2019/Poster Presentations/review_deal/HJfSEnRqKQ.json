{
    "Decision": {
        "metareview": "This paper is on active deep learning in the setting where a label hierarchy is available for multiclass classification problems: a fairly natural and pervasive setting. The extension where the learner can ask for example labels as well as a series of questions to adequately descend the label hierarchy is  an interesting twist on active learning.  The paper is well written and develops several natural formulations which are then benchmarked on CIFAR10, CIFAR100, and Tiny ImageNet using a ResNet-18 architecture.  The empirical results are carefully analyzed and appear to set interesting new baselines for active learning. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Sets strong experimental baselines for active learning in hierarchical classification settings"
    },
    "Reviews": [
        {
            "title": "Interesting novel Active Learning setting",
            "review": "The authors introduce a new Active Learning setting where instead of querying for a label for a particular example, the oracle offers a partial or weak label. This leads to a simpler and more natural way of retrieving this information that can be of use many applications such as image classification. \n\nThe paper is well-written and very easy to follow. The authors first present the overview of the learning scenario and then suggest three sampling strategies based on the existing AL insights (expected information gain, expected remaining classes, expected decrease in classes). \n\nAs the labels that the algorithm has to then use are partial, they make use of a standard algorithm to learn from partial labels -- namely, minimizing a partial log loss. It would be nice to properly reference related methods in the literature in Sec. 2.1.\n\nThe way of solving both the learning from partial labels and the sampling strategies are not particularly insightful. Also, there is a lack of theoretical guarantees to show value of a partial label as compared to the true label. However, as these are not the main points of the paper (introduction of a novel learning setting), I see these as minor concerns.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper, but lack of theoretical analysis",
            "review": "This paper proposes active learning with partial feedback, which means at each step, the learner actively chooses both which example to label and which binary question to ask, then learn the multi-class classifier with these partial labels. Three different sampling strategies are used during active learning. Experimental results demonstrate that the proposed ALPF strategy outperforms existing baselines on the predicting accuracy under a limited budget.\n\nThis paper is well-written. The main ideas and claims are clearly expressed. ALPF combines active learning with learning from partial labels. This setting is interesting and important, especially when the number of categories is large and share some hierarchical structure. The experimental results are promising. My main concern about this work is the lack of theoretical guarantees, which is usually important for active learning paper. itâ€™s better to provide some analysis on the efficiency of ALPF to further improve the quality of the paper.\nI have the following questions for the authors:\n+Why vanilla active learning strategy does not work well? Which uncertainty measurement do you use here?\n+The performances of this work heavily rely on the taxonomy of labels, while in some cases the taxonomy of labels is not tree structure but a graph, i.e. a label may belong to multiple hyper-labels. Can ALPF still work on these cases?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting setting combining active learning and learning with partial labesl. Nice experimental contribution, lack of conceptual insights. ",
            "review": "The paper considers a multiclass classification problem in which labels are grouped in a given number M of subsets c_j, which contain all individual labels as singletons. Training takes place through an active learning setting in which all training examples x_i are initially provided without their ground truth labels y_i. The learner issues queries of the form (x_i,c_j) where c_j is one of the given subsets of labels. The annotator only replies yes/no according to whether the true label y_i of x_i belongs to c_j or not. Hence, for each training example the learner maintains a \"version space\" containing all labels that are consistent with the answers received so far for that example. The active learning process consists of the following steps: (1) use the current learning model to score queries (x_i,c_j); (2) query the best (x_i,c_j); (3) update the model.\nIn their experiments, the authors use a mini-batched version, where queries are issued and re-ranked several times before updating the model. Assuming the learner generates predictive models which map examples to probability distributions over the class labels, several uncertainty measures can be used to score queries: expected info gain, expected remaining classes, expected decrease in remaining classes. Experiments are run using the Res-18 neural network architecture over CIFAR10, CIFAR100, and Tiny ImageNet, with training sets of 50k, 50k, and 100k examples. The subsets c_j are computed using the Wordnet hierarchy on the label names resulting in 27, 261, and 304 subsets for the three datasets. The experiments show the advantage of performing adaptive queries as opposed to several baselines: random example selection with binary search over labels, active learning over the examples with binary search over the labels, and others. \n\nThis paper develops a natural learning strategy combining two known approaches: active learning and learning with partial labels. The main idea is to exploit adaptation in both choosing examples and queries. The experimental approach is sound and the results are informative. In general, a good experimental paper with a somewhat incremental conceptual contribution.\n\nIn (2) there is t+1 on the left-hand side and t on the right-hand side, as if it were an update. Is it a typo?\n\nIn 3.1, how is the standard multiclass classifier making use of the partially labeled examples during training?\n\nHow are the number of questions required to exactly label all training examples computed? Why does this number vary across the different methods?\n\nWhat specific partial feedback strategies are used by AQ for labeling examples?\n\nEDC seems to consistently outperform ERC for small annotation budgets. Any intuition why this happens?",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}