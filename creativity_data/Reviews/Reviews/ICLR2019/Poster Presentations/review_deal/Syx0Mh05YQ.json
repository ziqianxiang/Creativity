{
    "Decision": {
        "metareview": "The authors have presented a simple yet elegant model to learn grid-like responses to encode spatial position, relying only on relative Euclidean distances to train the model, and achieving a good path integration accuracy. The model is simpler than recent related work and uses a structure of 'disentangled blocks' to achieve multi-scale grids rather than requiring dropout or injected noise. The paper is clearly written and it is intriguing to get down to the fundamentals of the grid code. On the negative side, the section on planning does not hold up as well and makes unverifiable claims, and one reviewer suggests that this section be replaced altogether by additional analysis of the grid model. Another reviewer points out that the authors have missed an opportunity to give a theoretical perspective on their model. Although there are aspects of the work which could be improved, the AC and all reviewers are in favor of acceptance of this paper.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "A simple and elegant approach to grid cells that begs for theoretical insight",
            "review": "This paper proposes a simple and elegant approach to learning \"grid-cell like\" representations that uses a high-dimensional encoding of position, together with a matrix for propagating position that involves only local connections among the elements of the vector.  The vectors are also constrained to have their inner products reflect positional similarity.  The paper also shows how such a representation may be used for path planning.\n\nBy stripping away the baggage and assumptions of previous approaches, I feel this paper starts to get at the essence of what drives the formation of grid cells.   It is still steps away from having direct ties to neurobiology, but is trying to get at the minimal components necessary for bringing about a grid cell like solution.  But I feel the paper also stops just a few steps short of developing a fuller theoretical understanding of what is going on.  For example the learned solution is quite Fourier like, and we know that Fourier transforms are good for representing position shift in terms of phase shift.  That would correspond to block size of two (i.e., complex numbers) in terms of this model.  So what's wrong with this solution (in terms of performance) and what is gained by having block size of six, beyond simply looking more grid like?  It would be nice to go beyond phenomenology and look at what the grid-like solution is useful for.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Elegant but simplistic model for grid cells; unnecessary extension to path planning",
            "review": "Updated score from 6 to 7 after the authors addressed my comments below.\n\nPrevious review:\n\nThis paper builds upon the recent work on computational models of grid cells that rely on trainable (parametric) models such as recurrent neural networks [Banino et al, 2018; Cueva & Wei, 2018]. It focuses entirely on path integration in 2D and 3D, from velocity inputs only, and it relies on two sub-networks: the motion model (an RNN) and the localization model (a feed-forward network). The avowed goal of the paper is to build a very simple and linear model for grid cells.\n\nBy linearly embedding the position x into a high-dimensional hidden vector v(x) (e.g., 96 elements), it can model motion using a linear model relying on matrix-vector multiplication: v(x + dx) = M(dx) v(x), where dx is the 2D or 3D displacement, v(.) is a vector and M(.) is a matrix. The embeddings v(.) are learnable and the paper assumes a square or cubic grid of N*N or N*N*N possible positions x (with N=40); these embeddings are also normalized to unit length and obey the kernel constraint that the dot-product between any two positions' vectors v(x) and v(y) is a Gaussian or an exponential function. The motion matrix is represented as block diagonal, where each block is a rotation of subvector v_k(x) into v_k(x + dx), where each block corresponds to a specific grid cell, and where the diagonal block is further expressed as a quadratic function of dx_1, dx_2, dx_3 elements of the displacement vector.\n\nThe strengths of the paper are that:\n1) The supervision of the localization subnetwork only depends on Euclidean proximity between two positions x and y and therefore uses relative positions, not absolute ones. Similarly, the path integration supervision of the motion model uses only relative displacements.\n2) The resulting rate maps of the hidden units seem perfect; the model exhibits multi-scale grid behaviour.\n3) The idea of using disentangled blocks, rather than injecting noise or using dropout and a softmax bottleneck as in [Banino et al, 2018], is interesting.\n4) The model accumulates little path integration error over 1000 step-long episodes.\n\nThe weakness of the paper is its simplicity:\n1) The assumption that A(x, y) can be modeled by a Gaussian or exponential (Laplacian?) kernel is limiting, in particular for positions x and y that are far apart.\n2) There is no discussion about what egocentric vs. allocentric referentials, and dx is assumed to be aligned with (x, y) axes (which are also the axes defining the bounding box of the area).\n3) Unlike the other work on learning path integration using an RNN, the linear matrix model can only handle allocentric displacements dx_1, dx_2 (and optional dx_3 in 3D).\n4) No consideration is given to non-square areas: would the network also exhibit grid-like behavior if the area was circular?\n5) What happens if the quadratic parameterisation of block diagonals is dropped?\n6) The paper did not use metrics accepted in the neuroscience community for computing a gridness score of the grid cells (although the grid cell nature is evident). There should however be metrics for quantifying how many units represent the different scales, offsets and orientations.\n7) The authors did not consider (but mentioned) embedding locations from vision, and did not consider ambiguous position embeddings.\n\nThe experiments about path planning are unconvincing. First of all, the algorithm requires to input absolute positions of every obstacle into equation (9) - (10), which assumes that there is perfect information about the map. Secondly, the search algorithm is greedy and it is not obvious how it would handle a complex maze with cul-de-sac. Saying that \"there is no need for reinforcement learning or sophisticated optimal control\" is very misleading: the problem here is simplified to the extreme, and fully observed, and any comparison with deep RL algorithms that can handle partial observations is just out of place.\n\nIn summary, the authors have introduced an interesting and elegant model for grid cells that suffers from simplifications. The part on path planning should be cut and replaced with more analysis of the grid cells and an explanation of how the model would handle egocentric velocity.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The motivation for this work needs to be clarified",
            "review": "\n\n=Major Comments=\nThe prior work on grid cells and deep learning makes it clear that the goal of the work is to demonstrate that a simple learning system equipped with representation learning will produce spatial representations that are grid-like. Finding grid-like representations is important because these representations occur in the mammalian brain. \n\nYour paper would be improved by making a similar argument, where you would need to draw much more explicitly on the neuroscience literature. Namely, the validation of your proposed representations for position and velocity are mostly validated by the fact that they yield grid-like representations, not that they are useful for downstream tasks.\n\nFurthermore, you should better justify why your simple model is better than prior work? What does the simplicity provide? Interpretability? Ease if optimization? Sample complexity for training?\n\nThis is important because otherwise it is unclear why you need to perform representation learning. The tasks you present (path integral and planning) could be easily performed in basic x-y coordinates. You wouldn’t need to introduce a latent v. Furthermore, this would mprove your argument for the importance of the block-diagonal M, since it would be more clear why interpretability matters.\n\n\nFinally, you definitely need to discuss the literature on randomized approximations to RBF kernels (random Fourier features). Given the way you pose the representation learning objective, I expect that these would be optimal. With this, it is clear why grid-like patterns would emerge.\n\n=Additional Comments=\nWhat can you say about the quality of the path returned by (10)? Is it guaranteed to converge to a path that ends at y? Is it the globally optimal path? \n\nI don’t agree with your statement that your approach enables simple planning by steepest descent. First of all, are the plans that your method outputs high-quality? Second, if you had solved (10) directly in x-y coordinates, you could have done this easily since it is an optimization problem in just 2 variables. That could be approximately solved by grid search.\n\nI would remove section 5.4. The latent vector v is a high-dimensional encoding of low-dimensional data, so of-course it is robust to corruptions. The corruptions you consider don’t come from a meaningful noise process, however? I can imagine, for example, that the agent observes corrupted versions of (x,y), but why would v get corrupted?\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}