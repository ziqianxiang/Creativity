{
    "Decision": {
        "metareview": "The reviewers all agreed that the problem application is interesting, and that there is little new methodology, but disagreed as to how that should translate into a score. The highest rating seemed to heavily weight the importance of the method to biological application, whereas the lowest rating heavily weighted the lack of technical novelty. However, because the ICLR call for papers clearly calls out applications in biology, and all reviewers agreed on its strength in that regard, and it was well-written and executed, I would recommend it for acceptance.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "Mixed reviews, strong application paper"
    },
    "Reviews": [
        {
            "title": "This is an application oriented paper with little technical contribution",
            "review": "This paper designed a GapNet-PL architecture and applied GapNet-PL, DenseNet, Multi-scale CNN etc. to the protein image (multi-labels) classification dataset.\n\nPros:\n\n1. The proposed method has a good performance on the given task. Compared to the claimed baselines (Liimatainen et al. and human experts), the proposed architecture shows a much higher performance.\n\nCons:\n\n1. The novelty of the proposed architecture is limited. The main contribution of this work is the application of CNN-based methods to the specific biological images.\n\n2. The existing technical challenge of this task is not significant and the motivation of the proposed method could be hardly found in this paper. \n\n3. The baselines are not convincing enough. Since the performance of Liimatainen et al. is calculated on a different test dataset, the results here are not comparable. The prediction from a human expert, which may vary from individuals, fails to provide a confident performance comparison.\n\n4. Compared to the existing models (DenseNet, Multi-scale CNN etc.), the performance improvement of the proposed model is limited.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "high-performance method with minor methodological contributions",
            "review": "The paper proposes a CNN variant tailored for high-resolution\nimmunofluorescence confocal microscopy data.  The authors show\nthat the method outperforms a human expert.\n\nThe proposed method is evaluated on benchmark instances\ndistributed by Cyto Challenge '17, which is presumably the best\ndata source for the target application.  Indeed, the method\nperforms better than several competitors plus a single human\nexpert.\n\nThe paper is well written and easy to follow.  I could not spot any\nmajor technical issues.\n\nThis is an applicative paper targeting a problem that is very\nrelevant in bioinformatics, but it sports little methodological\ninnovation.  On the biological side, the contribution looks\nsignificant.  Why not targeting a bioinformatics venue?\n\n\nDetailed comments:\n\nPapers that stretch multiple fields are always hard to review.  On\none hand, having contributions that cross different fields is a\nhigh-risk (but potentially highly rewarding) route, and I applaud\nthe authors for taking the risk.  On the other hand, there's the risk\nof having unbalanced contributions.\n\nI think that the contribution here is mostly on the bioinformatics\nside, not on the deep learning side.  Indeed, the method boils\ndown to a variant of CNNs.  I am skeptical that this is enough to\nspark useful discussion with practitioners of deep learning\n(although I could be wrong?).\n\nFinally, I am always skeptical of \"human-level\" performance claims.\nThese are strong claims that are also hard to substantiate.  I don't\nthink that comparing to a *single* expert is quite enough.  The fact\nthat \"the human expert stated that he would be capable to localize\nproteins with the provided data\" doesn't sound quite enough.  I\nagree that the user study could be biased (and that \"It would be\na tremendous effort to find a completely fair experimental\nsetting\"), but, if this is the case, the argument that the method\nreaches human-level performance is brittle.\n\n\nOther remarks and questions:\n\n- Why wasn't the dataset of Liimatainen et al. used for the\ncomparison?\n\n- The authors say that \"due to memory restrictions, the smallest\nvariant of DenseNet was used\".  How much of an impact could have\nthis had on performance?\n\n- \"One random crop per training sample is extracted in every epoch\".\nDoesn't this potentially introduce labeling errors?  Did you observe\nthis to occur in practice?\n\n- The authors claim that the method is close to perfect in terms\nof AUC.  In decision-making applications, the AUC is a very\nindirect measure of performance, because it is independent of\nany decision threshold.  In other words, the AUC does not measure\nthe yes/no decisions suggested by the method.  Why is the AUC\nimportant in the biological application at hand?  Why is it important\nto the users (biologists, I suppose) of the system?\n\nIn particular, \"our method performs nearly perfect, achieving an\naverage AUC of 98% and an F1 score of 78%\" seems inconsistent\nto me---the F1 is indeed \"only\" 78%.\n\n- I would appreciate if there was a thorough discussion of the\nfailure mode of the expert.  What kind of errors did he/she\nmake?  How are these cases handled by the model?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A CNN that boosts the state of the art on an important image classification task in biology",
            "review": "This manuscript describes a deep convolutional neural network for\nassigning proteins to subcellular compartments on the basis of\nmicroscopy images.\n\nPositive points:\n\n- This is an important, well-studied problem.\n\n- The results appear to improve significantly on the state of the art.\n\n- The experimental comparison is quite extensive, including\n  reimplementations of four, competing state-of-the-art methods, and\n  lots of details about how the comparisons were carried out.\n\n- The manuscript also includes a human-computer competition, which the\n  computer soundly wins.\n\n- The manuscript is written very clearly.\n\nConcerns:\n\nThere is not much here in the way of new machine learning methods.\nThe authors describe a particular neural network architecture\n(\"GapNet-PL\") and show empirical evidence that it performs well on a\nparticular dataset.  No claims are made about the generalizability of\nthe particular model architecture used here to other datasets or other\ntasks.\n\nA significant concern is one that is common to much of the deep\nlearning literature these days, namely, that the manuscript fails to\nseparate model development from model validation. We are told only\nabout the final model that the authors propose here, with no\ndiscussion of how the model was arrived at.  The concern here is that,\nin all likelihood, the authors had to try various model topologies,\ntraining strategies, etc., before settling on this particular setup.\nIf all of this was done on the same train/validation/test split, then\nthere is a risk of overfitting.\n\nThe dataset used here is not new; it was the basis for a competition\ncarried out previously.  It is therefore somewhat strange that the\nauthors chose to report only the results from their reimplementations\nof competing methods.  There is a risk that the authors'\nreimplementations involve some suboptimal choices, relative to the\nmethods used by the originators of those methods.\n\nAnother concern is the potential circularity of the labels.  At one\npoint, we are told that \"Most importantly, these labels have not been\nderived from the given microscopy images, but from other\nbiotechnologies such as microarrays or from literature.\"  However,\nearlier we are told that the labels come from \"a large battery of\nbiotechnologies and approaches, such as microarrays, confocal\nmicroscopy, knowledge from literature, bioinformatics predictions and\nadditional experimental evidence, such as western blots, or small\ninterfering RNA knockdowns.\"  The concern is that, to the extent that\nthe labels are due to bioinformatics predictions, then we may simply\nbe learning to re-create some other image processing tool.\n\nThe manuscript contains a fair amount of biology jargon (western\nblots, small interfering RNA knockdowns, antibodies, Hoechst staining,\netc.) that will not be understandable to a typical ICLR reader.\n\nAt the end, I think it would be instructive to show some examples\nwhere the human expert and the network disagreed.\n\nMinor:\n\np. 2: \"automatic detection of malaria\" -- from images of what?\n\np. 2: Put a semicolon before \"however\" and a comma after.\n\np. 2: Change \"Linear Discriminant\" to \"linear discriminant.\" Also, remove\nthe abbreviations (SVM and LDA), since they are never used again in\nthis manuscript.\n\np. 5: Delete comma in \"assumption, that.\"\n\np. 8: \"nearly perfect\" -> \"nearly perfectly\"\n\nThe confusion matrices in Figure 5 should not be row normalized --\njust report raw counts.  Also, it would be better to order the classes\nso that confusable ones are nearby in the list.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}