{
    "Decision": {
        "metareview": "\n* Strengths\n\nThe paper addresses a timely topic, and reviewers generally agreed that the approach is reasonable and the experiments are convincing. Reviewers raised a number of specific concerns (which could be addressed in a revised version or future work), described below.\n\n* Weaknesses\n\nSome reviewers were concerned the baselines are weak. Several reviewers were concerned that relying on failures observed during training could create issues by narrowing the proposal distribution (Reviewer 3 characterizes this in a particularly precise manner). In addition, there was a general feeling that more steps are needed before the method can be used in practice (but this could be said of most research).\n\n* Recommendation\n\nAll reviewers agreed that the paper should be accepted, although there was also consensus that the paper would benefit from stronger baselines and more close attention to issues that could be caused by an overly narrow proposal distribution. The authors should consider addressing or commenting on these issues in the final version.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "reasonable approach, convincing experiments, important topic"
    },
    "Reviews": [
        {
            "title": "Effective application of an importance sampling framework to testing RL agent policies for rare failures",
            "review": "Summary:\nProposes an importance sampling approach to sampling failure cases for RL algorithms. The proposal distribution is based on a function learned via a neural network on failures that occur during agent training. The method is compared to random sampling on two problems where the \"true\" failure probability can be approximated through random sampling. The IS method requires substantially fewer samples to produce failure cases and to estimate the failure probability.\n\nReview:\nThe overall approach is technically sound, and the experiments demonstrate a significant savings in sampling compared to naive random sampling. The specific novelty of the approach seems to be fitting the proposal distribution to failures observed during training. \n\nI think the method accomplishes what it sets out to do. However, as the paper notes, creating robust agents will require a combination of methodologies, of which this testing approach is only a part. \n\nI wonder if learning the proposal distribution based on failures observed during training presents a risk of narrowing the range of possible failures being considered. Of course identifying any failure is valuable, but by biasing the search toward failures that are similar to failures observed in training, might we be decreasing the likelihood of discovering failures that are substantially different from those seen during training? One could imagine that if the agent has not explored some regions of the state space, we would actually like to sample test examples from the unexplored states, which becomes less likely if we preferentially sample in states that were encountered in training.\n\nThe paper is well-written with good coverage of related literature. I would suggest incorporating some of the descriptions of the models and methods in Appendix D into the main paper.\n\nComments / Questions:\n* Sec 4.2: How are the confidence bounds for the results calculated?\n* What are the \"true\" failure probabilities in the experiments?\n* Sec 4.3: There is a reference to non-existant \"Appendix X\"\n\nPros:\n* Overall approach is sound and achieves its objectives\n\nCons:\n* Small amount of novelty; primarily an application of established techniques",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Timely topic, reasonable approach, and good experimental results",
            "review": "This paper proposed an adversarial approach to identifying catastrophic failure cases in reinforcement learning. It is a timely topic and may have practical significance. The proposed approach is built on importance sampling for the failure search and function fitting for estimating the failure probabilities. Experiments on two simulated environments show significant gain of the proposed approaches over naive search. \n\nThe reviewer is not familiar with this domain, but the baseline, naive search, seems like straightforward and very weak. Are there any other methods for the same problem in the literature? The authors may consider to contrast to them in the experiments. \n\nWhat is the certainty equivalence approach? A reference would be helpful and improve the presentation quality of the paper.\n\nWhat is exactly the $\\theta_t$ in Section 3.3? What is the dimension of this vector in the experiments? What quantities should be encoded in this vector in practice? \n\nI am still concerned about the fact that the FPP depends on the generalization of the binary classification neural network, although the authors tried to give intuitive examples and discussions. Nonetheless, I understand the difficulty. Could the authors give some conditions under which the approach would fail? Any alternative approaches to the binary neural network? What is a good principle to design the network architecture? \n\nOverall, this paper addresses a practically significant problem and has proposed reasonable approaches. While I still have concerns about the practical performance of the proposed methods, this work along the right track in my opinion.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Relevant, convincing experiments with a potential weak point in the method",
            "review": "PAPER SUMMARY\n-------------\n\nThe paper proposes a method for evaluating the failure probability of a learned agent, which is important in safety critical domains. \n\nUsing plain Monte Carlo for this evaluation can be too expensive, since discovering a failure probability of epsilon requires on the order of 1/epsilon samples. Therefore the authors propose an adversarial approach, which focuses on scenarios which are difficult for the agent, while still yielding unbiased estimates of failure probabilities. \n\nThe key idea of the proposed approach is to learn a failure probability predictor (FPP). This function attempts to predict at which initial states the system will fail. This function is then used in an importance sampling scheme to sample the regions with higher failure probability more often, which leads to higher statistical efficiency.\nFinding the FPP is itself a problem which is just as hard as the original problem of estimating the overall failure probability. However, the FPP can be trained using data from different agents, not just the final agent to be evaluated (for instance the data from agent training, containing typically many failure cases). The approach hinges on the assumption that these agents tend to fail in the same states as the final agent, but with higher probability. \n\nThe paper shows that the proposed method finds failure cases orders of magnitude faster than standard MC in simulated driving as well as a simulated humanoid task. Since the proposed approach uses data acquired during the training of the agent, it has more information at its disposal than standard MC. However, the paper shows that the proposed method is also orders of magnitudes more efficient than a naive approach using the failure cases during training.\n\n\nREVIEW SUMMARY\n--------------\n\nI believe that this paper addresses an important problem in a novel manner (as far as I can tell) and the experiments are quite convincing.\nThe main negative point is that I believe that the proposed method has some flaws which may actually decrease statistical efficiency in some cases (please see details below).\n\n\nDETAILED COMMENTS\n-----------------\n\n- It seems to me that a weak point of the method is that it may also severly reduce the efficiency compared to a standard MC method. If the function f underestimates the probability of failure at certain x, it would take a very long time to correct itself because these points would hardly ever be evaluated. It seems that the paper heuristically addresses this to some extent using the exponent alpha of the function. However, I think there should be a more in-depth discussion of this issue. An upper-confidence-bound type of algorithm may be a principled way of addressing this problem.\n\n- The proposed method relies on the ability to initialize the system in any desired state. However, on a physical system, where finding failure cases is particularly important, this is usually not possible. It would be interesting if the paper would discuss how the proposed approach would be used on such real systems.\n\n- On page 6, in the first paragraph, the state is called s instead of x as before. Furthermore, the arguments of f are switched.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}