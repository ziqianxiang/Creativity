{
    "Decision": {
        "metareview": "The paper summarizes existing work on binary neural network optimization and performs an empirical study across a few datasets and neural network architectures. I agree with the reviewers that this is a valuable study and it can establish a benchmark to help practitioners develop better binary neural network optimization techniques.\n\nPS: How about \"An empirical study of binary neural network optimization\" as the title?\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Accept"
    },
    "Reviews": [
        {
            "title": "Good review, relevant recommendations for a valuable research area",
            "review": "This is a good review paper covering techniques proposed across many of the well-known works in this area and doing an in-depth analysis of the value each of the techniques brings. Additionally, based on these studies the paper offers insights into the best algorithms and procedures to combine to achieving good results.\n\nOne recent whitepaper that has related work (not fully overlapping though), that may be worth looking by the authors is at https://arxiv.org/abs/1806.08342. It is fairly new and not very well-known so not surprising that the authors missed it.\n\nPros\n- Well written paper with lots of in-depth experiments \n- Does well at teasing out the impact of each of the techniques and gives some intuitive explanations of why they matter.\n- Provides better insights into how to make training of binary neural networks faster.\n- As the importance of low precision networks grows, this is a valuable paper in pushing the area of research forward.\n\nCons\n- A review paper, which doesn't add much new to the existing suite of techniques. Note: This is true for most review papers.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Useful empirical study of existing methods",
            "review": "The paper systematically studies the training of binary neural networks, where binary in this case refers to single bit weight elements in the network. In particular, different existing training methods are tested and compared for training both MLPs and CNNs.\n\nThe main findings of the paper are:\n- Using methods such as AdaGrad, AdaDelta, RMSProp and ADAM yields better performance than simpler momentum-based methods such as vanilla momentum and Nesterov momentum, which in turn are much better than vanilla SGD\n- When training binary models, it is common to clip weights and/or gradients for the proxy weights in the network. In the paper it is however shown that these methods hinder using a fast learning rate in the beginning of training, while the methods are required in later stages of training in order to achieve good results\n- Pre-training the model with full-precision training works well in speeding up training\n\nFor a practitioner, the paper presents a very useful reference for what methods work well when training binary networks. Although there are some proposals and hypotheses for reasons behind the results, I see the paper as a review paper of existing methods for training binary networks, showing experiments where the methods are tested using the same benchmark and training procedure in order to give a fair comparison.\n\nAs a practical guide, the paper therefore has clear value. What is lacking compared to typical ICLR papers is rigorously presenting new findings. The authors present a hypothesis for why different batch sizes are needed in the beginning compared to the end of training, but I found neither the justification nor the results very convincing with respect to the hypothesis. The way I see it, the actual novel proposals that are made in the paper are two two-stage training methods: one in which the tricks of weight and gradient clipping are only used towards the end of training, and one where the first stage of training is done using a full precision model. It is however quite well known that some training schemes with different stages can lead to improved performance: for instance with ADAM, even if it is an adaptive method, lowering the learning rate towards the end of training is often beneficial. It might therefore be fair to compare the methods to other multi-stage training methods. In addition, I could not find the training curves or final performance figures of the method where clipping is only activated towards the end of training.\n\nTo put it all together, the paper is clearly useful for the community as it provides a useful summary of the performance of different methods for training binary neural networks. In addition, it presents two two-stage training schemes that seem to make training even faster. What the paper lacks is rigorous theoretical justifications and clearly novel ideas.\n\nSmall comments:\n- How are the training lengths decided for the different methods? If I am not mistaken, in Figure 2, it seems like the SGD and momentum methods have not yet converged when training is halted. Is there a budget for wall clock time or is early stopping used or something similar? Considering the nature of the paper, I would see this kind of decisions as important to report.\n- In the abstract, you might want to refer to binary weights somewhere. Based on the abstract it is easy to mix the binary networks in this paper with stochastic binary networks that can also be trained using the STE estimator\n- Are the differences in the performances in Table 3 statistically significant?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The authors made several claims and provide suggestions on training binary networks. However, the experiments are somewhat not sufficient to support the proposed hypothesis.",
            "review": "The authors made several claims and provide suggestions on training binary networks, however, they are not proved or theoretically analyzed.  The empirical verification of the proposed hypothesis was viewed as weak as the only two datasets used are small datasets MNIST and CIFAR-10, and the used network architectures are also limited. Much more rigorous and thorough testing is required for an empirical paper which proposes new claims. \n\nTake the first claim \"end-to-end training of binary networks crucially relies on the optimiser taking advantage of second moment gradient estimates\" as an example. As it is known that choice of optimizer is highly dependent on the specific dataset and network structure, it is not convincing to jump to this conclusion using the observations on two small datasets and limited network architectures.  E.g, many binarization papers use momentum for ImageNet dataset with residual networks. Does Adam also outperforms momentum in this case? Similarly, it is also hard for me to judge whether the other conclusions made about weight/gradient clipping, the momentum in batch normalization and learning rate, are correct or not.\n\nSome minor issues are:\n1. In Figure 4, different methods are not run to convergence, and the comparison may not be fair.\n2. The second paragraph in section 4: \"It can be seen that not clipping weights when learning rates are large can completely halt the optimisation (red curve in Figure 5).\" However, in figure 5, the red curve is \"Clipping gradients\", which one is correct?\n3. The authors propose a recipe for faster training of binary networks, is there experiments supporting that training networks with the proposed recipe is faster than the original counterpart? ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}