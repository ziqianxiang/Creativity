{
    "Decision": {
        "metareview": "This paper proposes an approach for probing an environment to quickly identify the dynamics. The problem is relevant to the ICLR community. The paper is well-written, and provides a detailed empirical evaluation. The main weakness of the paper is the somewhat small originality over prior methods on online system identification. Despite this, the reviewer's agreed that the paper exceeds the bar for publication at ICLR. Hence, I recommend accept.\n\nBeyond the related work mentioned by the reviewers, the approach is similar to work in meta-learning. Meta-RL and multi-task learning has typically been considered in settings where the reward is changing (e.g. see [1],[2],[3],[4], where [4] also uses an embedding-based approach). However, there is some more recent work on meta-RL across varying dynamics, e.g. see [5],[6]. The authors are encouraged to make a conceptual connection between this approach and the line of work in model-based meta-RL (particularly [5] and [6]) in the final version of the paper.\n\n[1] Duan et al. https://arxiv.org/abs/1611.02779\n[2] Wang et al. CogSci '17 https://arxiv.org/abs/1611.05763\n[3] Finn et al. ICML '17 https://arxiv.org/abs/1703.03400\n[4] Hausman et al. ICLR '17: https://openreview.net/forum?id=rk07ZXZRb\n[5] Sæmundsson et al. https://arxiv.org/abs/1803.07551\n[6] Nagabandi et al. https://arxiv.org/abs/1803.11347\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "meta review"
    },
    "Reviews": [
        {
            "title": "great paradigm, but paper could be more efficient",
            "review": "Some argumentation might better be supported by some reference, like : \n\n\"When humans are tasked to perform in a new environment, we do not explicitly know what param-\neters affect performance. Instead, we probe the environment to gain an intuitive understanding of\nits behavior (Fig. 1). The purpose of these initial interactions is not to complete the task imme-\ndiately, but to extract information about the environment. This process facilitates learning in that\nenvironment. Inspired by this observation,\n\"\n\nThe overall idea is interesting, the implementation is correct via a TRANSITION PREDICTION MODELS\n\nMore place could be taken for more detailed results, use appendix to swap some text...\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting work about environment generalization for reinforcement learning",
            "review": "This paper proposes an “Environment-Probing” Interaction (EPI) policy used as an additional input for reinforcement learning (RL). This EPI allows to extract environment representations and implicitly understand the environment in order to improve the generalization on novel testing environments.\n\nPros:\n\nThis paper is well written and clear and the contribution is relevant to ICLR. Although I am not familiar with RL , the contribution seems novel and the model performances are compared with strong and appropriate baselines.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Learning policies for probing environment parameters to accelerate task learning. Interesting though limited novelty.",
            "review": "The submission presents a reinforcement learning method for exploring/probing the environment to determine an environment’s properties and exploit these during later tasks. The method relies on jointly learning an embedding that simplifies prediction of future states and a policy that maximises a curiosity/intrinsic motivation like reward to learn to explore areas where the prediction model underperforms. In particular, the reward is based on the difference between prediction based on the learned embedding and prediction based on a prior collected dataset, such that the reward optimises to collect data with a large difference between the prediction accuracy of both models. The subsequently frozen policy and embedding are then used in other domains in a system identification like manner with the embedding utilised as input for a final task policy. The method is evaluated on a striker and hopper environment with varying dynamics parameters and shown to outperform a broad set of baselines. \n\nIn particular the broad set of baselines and small performed ablation study on the proposed method are quite interesting and beneficial for understanding the approach. However, the ablation study could be in more detail with respect to the additional training variations (Section 4.1.3; e.g. without all training tricks). Additionally, information about the baselines should be extended in the appendix as e.g. different capacities alone could have an impact where the performances of diff. algorithms are comparably similar. In particular, additional information about the training procedure for the UP-OSI (Yu et al 2017) baseline is required as the original approach relies on iterative training and it is unclear if the baseline implementation follows the original implementation (similar to Section 4.1.3.). \n\nOverall the submission provides an interesting new direction on learning system identification approaches, that while quite similar to existing work (Yu et al 2017), provides increased performance on two benchmark tasks. The contribution of the paper focuses on detailed evaluation and, overall, beneficial details of the proposed method. The novelty of the submission is however limited and highly similar to current methods.\n\nMinor issues:\n- Related work on learning system identification:\nLearning to Perform Physics Experiments via Deep Reinforcement Learning\nMisha Denil, Pulkit Agrawal, Tejas D Kulkarni, Tom Erez, Peter Battaglia, Nando de Freitas\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}