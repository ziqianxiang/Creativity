{
    "Decision": {
        "metareview": "As per R3: This paper presents a novel approach for doing hierarchical deep RL (HRL) on UMDPs by:\n(a) use of hindsight experience replay at multiple levels;  combined with (b) max T timesteps\nat each level. By effectively learning from missed goals at multiple levels, it allows for fairly \ndata-efficient learning and can (in principle) work for an arbitrary number of levels.\nHRL is an important open problem.\n\nThe weaknesses described reviewers include limited comparisons to other HRL methods; its applicaiton to fairly simple domain;\nits still unclear what the benefit of >=4 levels is, and what the diminishing returns are wrt to the claim of working\nfor an arbitrary number of levels.  R1(5) and R3(7)  stand by their scores. R1(5) still has some remaining concerns\nregarding some experiments not being done across all tasks, an older version of the HAL algo baseline being used, and \nlack of insight regarding >= 4 levels.\n\nBased on the balance of the reviewers comments and the AC's own reading of the paper and results, \nand the importance of the problem, the AC leans towards accept.  Using Hindsight Exp Replay across multiple levels\nis a simple-but-interesting idea, and the terminate-after-T steps is an interesting heuristic to make this effective.\nWhile the paper does not give insight for large (>=4) levels, it does make for an interesting framework that\nwill inspire further work.  The AC recommends that the claims regarding an \"arbitrary number of levels\" be significantly toned down.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "lean towards accept"
    },
    "Reviews": [
        {
            "title": "The technique is sound and demonstrated good performance on a range of RL tasks, however its significance is not fully demonstrated.  ",
            "review": "Pros:\n1. A nice idea combining universal MDP formulation and Hindsight experience replay for HRL that can deal with hierarchies with more than two levels of policies in continuous tasks.\n2. Good empirical results \n\nCons:\n1. One limitation of this work is that the goal set is known. What if the goals are unknown?\n\n2. The current domains seems relative simple comparing other existing papers on HRL, hence it is hard to tell the significance of the method.\n\n3. It Lacks thorough experimental analysis. Some comments are suggestions are provided here.\n---Since the proposed framework can deal with arbitrary level of hierarchies, it might be better to include include an experiment comparing the more than 2 subgoal layers. This will help understand whether there is any diminishing return by increasing the number of layers.\n\n---What kind of policy representations and hyperparameters of the training algorithm are used? Are they the same for different domains? Some critical details and some ablation test should be provided.\n\n---The paper can also be strengthened if some comparisons to other HRL methods can be included.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Very nice approach for hierarchical RL",
            "review": "This paper presents a novel approach for doing hierarchical deep RL. Each level of the hierarchy is rewarded for reaching a goal state. The top level's goal state is the environment goal, lower level goals states are the actions of the higher levels. The lowest level's actions are primitive actions. Each level can act until it reaches it goal or a maximum of T steps. Then HER is used to still learn from missed subgoals. For example, if the lowest level is given a subgoal and fails to achieve it, it is trained with a new experience where the goal was the achieved state. In addition, the level above is trained with an experience where the action it chose (the subgoal that was not achieved) is replaced with the subgoal that was achieved. So HER is replacing goals on one level and replacing actions on the higher level. The paper shows nice empirical results across 6 domains.\n\nThe two main differences from prior work are:\n1. Explicit constraint on how long the policies at each level can be.\n2. Use of HER in a novel way (on goals and actions) to learn from failed attempts at reaching subgoals from lower levels.\n\nThe use of HER in this work is really powerful and everything fits together nicely to make it work.\n\nThe only un-satisfying part of the algorithm is the need for a subgoal testing phase. Some actions are randomly decided to be testing phases, where all exploration is turned off at lower levels and the agent at the level selecting that subgoal is given negative reward if the subgoal is not achieved. This feels a bit unnatural to me. Does it not work if you punish a level for selecting a failed subgoal even if exploration is on? Does this phase unnecessarily punish levels for selecting subgoals that aren't reached early in learning, where even with no exploration a lower level may not have learned to reach the subgoal yet?\n\nThe main drawback of the paper is that there is no empirical comparison to related work. Instead the approach is only compared to doing learning with no hierarchy. Still, in all 6 domains, there is a clear improvement to using the hierarchy vs a flat hierarchy. \n\nPros:\n- Nice approach for hierarchical deep RL\n- Great use of HER to improve subgoal learning\n- Good empirical results showing benefit of approach over flat learning\nCons:\n- No empirical comparison to related work\n- Subgoal testing phase seems a bit hacky.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An HRL framework with an arbitrary number of levels",
            "review": "This paper proposed a framework that can improve the performances of reinforcement learning algorithms in tasks that involve long time horizons and sparse rewards. The proposed method is a hierarchical reinforcement learning framework that can use policy hierarchies with an arbitrary number of levels. To improve the sample efficiency in the learning process, the authors proposed to apply the hindsight experience replay mechanism at each level. Also, in order to avoid the actor function to output an unrealistic subgoal, the authors proposed the subgoal testing technique. \n\nThe proposed framework is interesting. And the example in Section 3.5 clearly demonstrate how this framework works. The authors proposed to solve a UMDP by solving a hierarchy of k UMDPs, where k is a hyperparameter. Each level (except for the bottom most level) will output subgoal states for the next level to achieve. This hierarchy is reasonable and easy to understand. However, from the definition on Page 3, it seems that all of the intermediate levels i (the case where 0 < i < k - 1) has the same state and action spaces. They are all equal to the state set of the original UMDP. Under this setting, will adding more intermediate levels help improve the performance a lot? We only see results with at most one intermediate level in the experiment. It will be better if the authors can show results on more levels (i.e. at least 4 levels in total). \n\nMoreover, the proposed framework has a policy limit parameter T, meaning that we only consider if a goal can be achieved within T steps or not, at each level. Is this parameter necessary to be the same for all levels? Also, it will be better if the author can show some results on the performances of the proposed method according to different values for T. The authors also proposed the subgoal testing technique. It is also better if the authors can show some performance comparisons on the cases with and without this technique.\n\nThe authors claimed that their method has the advantage over some existing HRL methods (e.g. the Option-Critic Architecture [1]) that their method can use policy hierarchies with an arbitrary number of levels while these methods can only use policy hierarchies with two levels. In the experiments, the authors also showed that, in some of their experiments, the 3-layer agent (with 2 subgoal layers) outperforms the 2-layer agent (with 1 subgoal layer), under their framework. However, the authors did not compare their 2-layer agent's performance with these existing HRL methods, which means that we do not know if their 3-layer agent's performance is better than that of some of the existing 2-layer agent methods. In addition to that, as I mentioned before, it is better if the authors can show experiment results on more levels (e.g. 4 levels and more) to show that their method can perform well in practice for policy hierarchies with many levels.\n\n\nReferences:\n\n[1] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. CoRR, abs/1609.05140, 2016.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}