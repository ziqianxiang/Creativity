{
    "Decision": {
        "metareview": "The paper presents a novel with compelling experiments. Good paper, accept. \n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Accept"
    },
    "Reviews": [
        {
            "title": "interesting work introducing graph neural nets as regularization, with practical limitations",
            "review": "The paper presents a an interesting novel approach to train neural networks with so called peer regularization which aims to provide robustness to adversarial attacks. The idea is to add a graph neural network to a spatial CNN. A graph is defined over similar training samples which are found using a Monte Carlo approximation.\n\nThe regularization using graphs reminds me of recent work at ICML on semi-supervised learning (Kamnitsas et al. (2018) Semi-supervised learning via compact latent space clustering) which is using a graph to approximate cluster density which acts as a regularizer for training on labelled data.\n\nThe main problem I see with these approaches is that they rely on sufficiently large batch sizes which could be (currently) problematic for many real-world applications. Memory and computation limitations are mentioned, but not sufficently discussed. It would be good to add further details on practical limitations.\n\nExperiments are limited to benchmark data using MNIST, CIFAR-10, CIFAR-100. Comprehensive evaluation has been carried out with insightful experiments and good comparison to state-of-the-art. Both white- and black-box adversarial attacks are explored with promising results for the proposed approach.\n\nHowever, it is difficult to draw conclusions for real-world problems of larger scale. The authors state that proposed framework can be added to any baseline model, but miss to clearly mention the limitations. It is stated that future work will aim at scaling PeerNets to benchmarks like ImageNet, but it is unclear how this could be done. Is there any hope this could be applied to problems like 3D imaging data or videos?\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Analysis and experimental comparisons are lacking",
            "review": "After reading the authors' response, I'm revising my score upwards from 5 to 6.\n\nThe authors propose a defense against adversarial examples, that is inspired by \"non local means filtering\". The underlying assumption seems to be that, at feature level, adversarial examples manifest as IID noise in feature maps, which can be \"filtered away\" by using features from other images. While this assumption seems plausible,  no analysis has been done to verify it in a systematic way. Some examples of verifying this are:\n\n1. How does varying the number of nearest neighbors change the network behavior?\n2. At test time, a fixed number of images are used for denoising - how does the choice of these images change accuracy or adversarial robustness?\n3. Does just simple filtering of the feature map, say, by local averaging, perform equally well? \n4. When do things start to break down? I imagine randomly replacing feature map values (i.e. with very poor nearest neighbors) will cause robustness and accuracy to go down - was this tested?\n\nBased on the paper of Athalye et. al., really the only method worth comparing to for adversarial defense, is adversarial training. It is hard to judge absolute adversarial robustness performance without a baseline of adversarial training.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}