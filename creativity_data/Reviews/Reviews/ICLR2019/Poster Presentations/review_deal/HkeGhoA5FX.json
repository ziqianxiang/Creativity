{
    "Decision": {
        "metareview": "1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.\n\n- strong qualitative and quantitative results\n- a good ablative analysis of the proposed method.\n \n2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.\n\n- clarity could be improved (and was much improved in the revision).\n- somewhat limited novelty.\n \n3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.\n\nNo major points of contention.\n \n4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.\n\nThe reviewers reached a consensus that the paper should be accepted.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "somewhat limited novelty but significant advancement of SOTA"
    },
    "Reviews": [
        {
            "title": "excellent application oriented paper; new state-of-the-art results; yet limited novelty",
            "review": "The authors propose a residual non-local attention net (RNAN) which combines local and non-local blocks to form a deep CNN architecture with application to image restoration.\n\nThe paper has a compact description, provides sufficient details, and including the appendix has an excellent experimental validation.\n\nThe proposed approach provides top results on several image restoration tasks:  image denoising, demosaicing, compression artifacts reduction, and single image super-resolution.\n\nThe main weakness of the paper is the limited novelty, as the proposed design builds upon existing ideas and concepts. However, up to some point all the new ConvNet designs can be seen as incremental developments of the older ones, yet they are needed for the progress of the field.\n\nI would suggest to the authors the inclusion of related works such as:\nTimofte et al., \"NTIRE 2018 Challenge on Single Image Super-Resolution: Methods and Results\", CVPRW 2018\nWang et al., \"A fully progressive approach to single-image super-resolution\", CVPRW 2018\nNote that DIV2K dataset was introduced in:\nAgustsson et al., NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study, CVPRW 2017\n\nalso, the more recent related works:\nBlau et al., \"2018 PIRM Challenge on Perceptual Image Super-resolution\", ECCVW 2018\nZhang et al., \"Image Super-Resolution Using Very Deep Residual Channel Attention Networks\", ECCV 2018\n\nAlso, I would like a response from the authors on the following:\nWhy not using dilated convolutions instead of or complementary with the mask branch or other design choices from this paper?\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "excellent results, but unclear novelty and lacking explanations",
            "review": "The paper proposes a convolutional neural network architecture that includes blocks for local and non-local attention mechanisms, which are claimed to be responsible for achieving excellent results in four image restoration applications.\n\n\n# Results\nThe strongest point of the paper is that the quantitative and qualitative image restoration results appear to be very good, although they seem almost a bit too good.\n\n\n# Novelty\nI'm not sure about the novelty of the paper, but I suspect it to be rather incremental. The paper says \"To the best of our knowledge, this is the first time to consider residual non-local attention for image restoration problems.\" Does that mean non-local attention (in a very similar way) has already been used, just not in a residual fashion? If so, that would not constitute much novelty. I have to admit that I'm not familiar with the related work on attention, but I did not understand *why* the results of the proposed method are supposed to be much better than that of previous work.\n\n\n# Clarity\nI think the paper is not self-contained enough, since it seems to implicitly assume substantial background knowledge on attention mechanisms in CNNs. \n\nFurthermore, the introduction of the paper identifies three problems with existing CNNs that I don't necessarily fully agree with. None of these supposed problems are backed up by (experimental) evidence.\n\nI don't think it is sufficient to just show superior results than previous methods. It is also important to disentangle why the results are better. However, the presented ablation experiments are not very illuminating to me.\n\nThe attempts at explaining what the novel attention blocks do and why they lead to superior results are very vague to me. Maybe they are understandable in the context of related work, but I found many statements, such as the following, devoid of meaning:\n- \"Without considering the uneven distribution of information in the corrupted images, [...]\"\n- \"However, in this paper, we mainly focus on learning non-local attention to better guide feature extraction in trunk branch.\"\n- \"We only incorporate residual non-local attention block in low-level and high-level feature space. This is mainly because a few non-local modules can well offer non-local ability to the network for image restoration.\"\n- \"The key point in mask branch is how to grasp information of larger scope, namely larger receptive field size, so that it’s possible to obtain more sophisticated attention map.\"\n\n\n# Experiments\n- The experimental results are the best part of the paper. However, it would've been nice to include some qualitative results in the main paper.\n- The proposed RNAN model is trained on a big dataset (800 images with ~2 million pixels each). Are the competing methods trained on datasets of similar size? If not, this could be a major reason for improved performance of RNAN over competing methods. At least in the appendix, RNAN and FFDNet are compared more fairly since they are trained with the same/similar data.\n- The qualitative examples in the appendix mostly show close-ups/details of very structured regions (mostly stripy patterns). Please also show some other regions without self-similar structures.\n\n\n# Misc\n- Residual non-local attention learning (section 3.3) was not clear to me.\n- The word \"trunk\" is used without definition or explanation.\n- Fig. 2 caption is too short, please expand.\n\n# Update (2018-11-29)\nGiven the substantial author feedback, I'm willing to raise my score.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Technical contribution is not high, but good performing approach on several image restoration tasks",
            "review": "- Summary\nThis paper proposes a residual non-local attention network for image restoration. Specifically, the proposed method has local and non-local attention blocks to extract features which capture long-range dependencies. The local and non-local blocks consist of trunk branch and (non-) local mask branch. The proposed method is evaluated on image denoising, demosaicing, compression artifacts reduction, and super-resolution.\n\n- Pros\n  - The proposed method shows better performance than existing image restoration methods.\n  - The effect of each proposed technique such as the mask branch and the non-local block is appropriately evaluated.\n\n- Cons\n  - It would be better to provide the state-of-the-art method[1] in the super-resolution task. \n    [1] Y. Zhang et al., Image Super-Resolution Using Very Deep Residual Channel Attention Networks, ECCV, 2018.\n  - The technical contribution of the proposed method is not high, because the proposed method seems to be just using existing methods.\n  - The contribution of the non-local operation is not clear to me. For example, how does the global information (i.e., long-range dependencies between pixels) help to solve image denoising tasks such as image denoising?\n\nOverall, the technical contribution of the proposed method is not so high, but the proposed method is valuable and promising if we focus on the performance.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}