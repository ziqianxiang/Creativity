{
    "Decision": {
        "metareview": "\n+ An interesting and original idea of embedding words into the (very low dimensional) Wasserstein space, i.e. clouds of points in a low-dimensional space\n+ As the space is low-dimensional (2D), it can be directly visualized. \n+ I could imagine the technique to be useful in social / human science for data visualization, the visualization is more faithful to what the model is doing than t-SNE plots of high-dimensional embeddings\n+ Though not the first method to embed words as densities but seemingly the first one which shows that multi-modality  / multiple senses are captured (except for models which capture discrete senses)\n+ The paper is very well written\n\n-  The results are not very convincing but show that embeddings do capture word similarity (even when training the model on a small dataset)\n-  The approach is not very scalable (hence evaluation on 17M corpus)\n-  The method cannot be used to deal with data sparsity, though (very) interesting for visualization\n-  This is mostly an empirical paper (i.e. an interesting application of an existing method)\n\nThe reviewers are split. One reviewer is negative as they are unclear what the technical contribution is (but seems a bit biased against empirical papers). Another two find the paper very interesting. \n\n\n\n\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "An interesting word embedding method"
    },
    "Reviews": [
        {
            "title": "A simple and interesting idea on how to map data in a discrete (Wasserstein) space",
            "review": "This paper learns embeddings in a discrete space of probability distributions, endowed with a (regularized) Wasserstein distance.\n\npros:\n\n- interesting idea, nice results, mostly readable presentation.\n- the paper is mostly experimental but the message delivers clearly the paper’s objective\n- the direct visualisation is interesting\n- the paper suggests interesting problems related to the technique\n\ncons:\n\n- to be fair with the technique, the title should mention the fact that the paper minimises a regularised version of Wasserstein distances (Wasserstein -> Sinkhorn ? put “regularised\" ?)\n- and to be fair, the paper should put some warnings related to regularisation -- this is not a distance anymore, sparsity is affected by regularisation (which may affect visualisation). Put some reminders in the conclusion, reword at least the third paragraph in the introduction.\n- the paper could have been a little bit more detailed on Section 2.3, in particular for its third paragraph. Even when it is an experimental paper.\n- the direct visualisation is interesting in the general case but has in fact a problem when distributions are highly multimodal, which can be the case in NLP. This blurs the interpretation.\n- the paper delivers a superficial message on the representation: I do not consider that nice having modes near physical locations (Paris, France) is wrong. It is also a city. However, it would have been interesting to see the modes of “city” (or similar) to check whether the system indeed did something semantically wrong.\n\nQuestions:\n\n- beyond that last remark comes the problem as to whether one can ensure that semantic hierarchies appear in the plot: for example if Nice was only a city, would we observe a minimal intersection with the support of word “city” ? (intersection to be understood at minimal level set, not necessarily 0).\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A very nice and original work.",
            "review": "The paper ‘Learning Discrete Wasserstein Embeddings' describes a new embedding method that,\ncontrary to usual embedding approaches, does not try to embed (complex, structured) data into an \nHilbertian space where Euclidean distance is used, but rather to the space of probability measures\nendowed with the Wasserstein distance. As such, data are embed on an empirical \ndistribution supported by Diracs, which locations can be determined by a map that is learnt from data.\nInterestingly, authors note a 'potential universality' for W_p(R^3) (from a result of Andoni et al., 2015), \nsuggesting that having Diracs in R^3 could embed potentially any kind of metric on symbolic data. \n\nExperimental validations are presented on graph and word embedding, and a discussion on visualization of \nthe embedding is also proposed (since the Diracs are located in a low dimensional space).   \n\nAll in all the paper is very clear and interesting. The idea of embedding in a Wasserstein space is \noriginal (up to my knowledge) and well described. I definitely believe that this work should be presented\nat ICLR. I have a couple of questions and remarks for the authors:\n - It is noted in section 3.2 that both Diracs location and associated weights could be optimized. Yet the authors \n   chose to only optimize locations. Why not only optimizing the weights (as in an Eulerian view of probability\n   distributions) ? The sentence involving works of Brancolini and Claici 2018 is not clear to me. Why weighting \n   does not improve asymptotically the approximation quality ? \n - Introducing the entropic regularization is mainly done for being able to differentiate the Wasserstein \n   distance. However, few is said on the embeddability of metrics in W^\\lambda_p(R). Is using an entropic \n   version of W moderating the capacity of embedding ? At least experimentally, a discussion could be made  \n   on the choice of the regularization parameter, at least in section 4.1. In eq. (9), it seems that it is not \n   the regularized version of W. ? \n - I assume that the mapping is hard to invert, but did the authors tried to experiment reconstructing an object \n   of interest by following a geodesic in the Wasserstein space ?  \n - It seems to me that authors never give generalization results. What is the performance of the metric approximation \n   when tested on unseen graphs or words ? This point should be clarified in the experiment.        \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Very Limited Contribution",
            "review": "The paper proposes embedding the data into low-dimensional Wasserstein spaces. These spaces are larger and more flexible than Euclidean spaces and thus, can capture the underlying structure of the data more accurately. However, the paper simply uses the automatic differentiation to calculate the gradients. Thus, it offers almost no theoretical contribution (for instance, how to calculate these embeddings more efficiently (e.g. faster or more efficient calculation of the Sinkhorn divergence), how to motivate loss functions than can benefit the structure of the Wasserstein spaces, what the interpretation of phi(x) is for each problem, e. g. word embedding, etc.). Additionally, the experiments are unclear and need further improvement. For instance, which method is used to find the Euclidean embedding of the datasets? Have you tried any alternative loss functions for the discussed problems? Are these embeddings useful (classification accuracy, other distortion measure)? How does the 2-D visualizations compare to DR methods such as t-SNE? What is complexity of the method? How does the runtime compare to similar methods?",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}