{
    "Decision": {
        "metareview": "\npros:\n- The paper presents an interesting forward chaining model which makes use of meta-level expansions and reductions on predicate arguments in a neat way to reduce complexity.  As Reviewer 3 points out, there are a number of other papers from the neuro-symbolic community that learn relations (logic tensor networks is one good reference there). However using these meta-rules you can mix predicates of different arities in a principled way in the construction of the rules, which is something I haven't seen.\n- The paper is reasonably well written (see cons for specific issues)\n- There is quite a broad evaluation across a number of different tasks.  I appreciated that you integrated this into an RL setting for tasks like blocks world.\n- The results are good on small datasets and generalize well\n\ncons:\n- (scalability) As both Reviewers 1 and 3 point out, there are scalability issues as a function of the predicate arity in computing the set of permutations for the output predicate computation.\n- (interpretability) As Reviewer 2 notes, unlike del-ILP, it is not obvious how symbolic rules can be extracted.  This is an important point to address up front in the text. \n- (clarity) The paper is confusing or ambiguous in places:\n\n-Initially I read the 1,2,3 sequence at the top of 3 to be a deduction (and was confused) rather than three applications of the meta-rules.  Maybe instead of calling that section \"primitive logic rules\" you can call them \"logical meta-rules\".\n\n-Another confusion, also mentioned by reviewer 3 is that you are assuming that free variables (e.g. the \"x\" in the expression \"Clear(x)\") are implicitly considered universally quantified in your examples but you don't say this anywhere.  If I have the fact \"Clear(x)\" as an input fact, then presumably you will interpret this as \"for all x Clear(x)\" and provide an input tensor to the first layer which will have all 1.0's along the \"Clear\" relation dimension, right?\n\n-It seems that you are making the assumption that you will never need to apply a predicate to the same object in multiple arguments?  If not, I don't see why you say that the shape of the tensor will be m x (m-1) instead of m^2.  You need to be able to do this to get reflexivity for example: \"a <= a\".\n\n-I think you are implicitly making the closed world assumption (CWA) and should say so.\n\n-On pg. 4 you say \"The facts are tensors that encode relations among multiple objectives, as described in Sec. 2.2.\".  What do you mean by \"objectives\"?  I would say the facts are tensors that encode relations among multiple objects.\n\n-On pg. 5 you say \"We finish this subsection, continuing with the blocks world to illustrate the forward\npropagation in NLM\".  I see no mention of blocks world in this paragraph. It just seems like a description of what happens at one block, generically.\n\n-In many places you say that this model can compute deduction on first-order predicate calculus (FOPC) but it seems to me that you are limited to horn logic (rule logic) in which there is at most one positive literal per clause (i.e. rules of the form: b1 AND b2 AND ... AND bn => h).  From what I can tell you cannot handle deduction on clauses such as b1 AND b2 => h1 or (h2 and h3).\n\n-There is not enough description of the exact setup for each experiment. For example in blocks world, how do you choose predicates for each layer?  How many exactly for each experiment?  You make it seem on p3 that you can handle recursive predicates but this seems to not have been worked out completely in the appendix.  You should make this clear.\n\n-In figure 1 you list Move as if its a predicate like On but it's a very different thing. On is  predicate describing a relation in one state.  Move is an action which updates a state by changing the values of predicates.  They should not be presented in the same way.\n\n-You use \"min\" and \"max\" for \"and\" and \"or\" respectively.  Other approaches have found that using the product t-norm t-norm(x,y) = x * y helps with gradient propagation.  del-ILP discusses this in more detail on p 19.  Did you try these variations?\n\n-I think it would be helpful to somewhere explicitly describe the actual MLP model you use for deduction including layer sizes and activation functions.\n\n-p. 5. typo: \"Such a parameter sharing mechanism is crucial to the generalization ability of NLM to\nproblems ov varying sizes.\" (\"ov\" -> \"of\")\n\n-p. 6. sec 3.1 typo: \"For ∂ILP, the set of pre-conditions of the symbols is used direclty as input of the system.\" (\"direclty\" -> \"directly\")\n\nI think this is a valuable contribution and novel in the particulars of the architecture (eg. expand/reduce) and am recommending acceptance.  But I would like to see a real effort made to sharpen the writing and make the exposition crystal clear.  Please in particular pay attention to Reviewer 3's comments.\n\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Interesting forward chaining approach to neural deduction"
    },
    "Reviews": [
        {
            "title": "Interesting approach to model FOL in NN, with concerns in scalability",
            "review": "This paper presents a model to combine neural network and logic programming. It proposes to use 3 primitive logic rules to model first-order predicate calculus in the neural networks. Specifically, relations with different numbers of arguments over all permutations of the groups of objects are represented as tensors with corresponding dimensions. In each layer, a MLP (shared among different permutations) is applied to transform the tensor. Multiple layers captures multiple steps of deduction. On several synthetic tasks, the proposed method is shown to outperform the memory network baseline and shows strong generalization.  \n\nThe paper is well written, but some of the contents are still a bit dense, especially for readers who are not familiar with first-order predicate calculus. \n\nThe small Python example in the Appendix helps to clarify the details. It would be good to include the details of the architectures, for example, the number of layers, and the number of hidden sizes in each layer, in the experiment details in the appendix. \n\nThe idea of using the 3 primitive logic rules and applying the same MLP to all the permutations are interesting. However, due to the permutation step, my concern is whether it can scale to real-world problems with a large number of entities and different types of relations, for example, a real-world knowledge graph.\n\nSpecifically:\n\n1. Each step of the reasoning (one layer) is applied to all the permutations for each predicate over each group of objects, which might be prohibitive in real-world scenario. For example, although there are usually only binary relations in real-world KG, the number of entities is usually >10M. \n\n2. Although the inputs or preconditions could be sparse, thus efficient to store and process, the intermediate representations are dense due to the probabilistic view, which makes the (soft) deduction computationally expensive. \n\nSome clarification questions: \n\nIs there some references for the Remark on page 3? \n\nWhy is there a permutation before MLP? I thought the [m, m-1, …, m-n+1] dimensions represent the permutations. For example, if there are two objects, {x1, x2}. Then the [0, 1, 0] represents the first predicate applied on x1, and x2. [1, 0, 0] represents the first predicate applied on x2 and x1. Some clarifications would definitely help here. \n\nI think this paper presents an interesting approach to model FOPC in neural networks. So I support the acceptance of the paper. However, I am concerned with its scalability beyond the toy datasets. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "In this paper the authors propose a neural-symbolic architecture, called Neural Logic Machines (NLMs), that can learn logic rules.\n\nThe paper is pretty clear and well-written and the proposed system is compelling. I have only some small concerns.\nOne issue concerns the learning time. In the experimental phase the authors do not state how long training is for different datasets.\nMoreover it seems that the “rules” learnt by NSMs cannot be expressed in a logical formalism, isn’t it? If I am right, I think this is a major difference between dILP (Evans et. al) and NLMs and the authors should discuss about that. If I am wrong, I think the authors should describe how to extract rules from NLMs.\nIn conclusion I think that, once these little issues are fixed, the paper could be considered for acceptance.\n\n[minor comments]\np. 4\n“tenary” -> “ternary”\n p. 5\n“ov varying size” -> “of varying size”\n“The number of parameters in the block described above is…”. It is not clear to me how the number of parameters is computed.\n“In Eq. equation 4” -> “In Eq. 4”\n\np. 16\n“Each lesson contains the example with same number of objects in our experiments.”. This sentence sounds odd.\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "interesting directions but unclear novelty and some claims that are too strong",
            "review": "The paper introduces Neural Logic Machines, a particular way to combine neural networks and first order but finite logic. \n\nThe paper is very well written and structured. However, there are also some downsides.\n\nFirst of all, Section 2.1 is rather simple from a logical perspective and hence it is not clear what this gets a special term. Moreover, why do mix Boolean logic (propostional logic) and first order logic? Any how to you deal with the free variables, i.e., the variables that are not bounded by a quantifier? The semantics you define later actually assumes that all free variables (in your notation) are bounded by all quantifiers since you apply the same rule to all ground instances. Given that you argue that you want a neural extension of symbolic logic (\"NLM is a neural realization of (symbolic) logic machines\") this has to be clarified as it would not be an extension otherwise. \n\nFurthermore, Section 2.2 argues that we can use a MLP with a sigmoid output to encode any joint distribution. This should be proven. It particular, given that the input to the network are the marginals of the ground atoms. So this is more like a conditional distribution? Moreover, it is not clear how this is different to other approaches that encode the weight of weighted logical rule (e.g. in a MLN) using neural networks, see\ne.g. \n\nMarco Lippi, Paolo Frasconi:\nPrediction of protein beta-residue contacts by Markov logic networks with grounding-specific weights. \nBioinformatics 25(18): 2326-2333 (2009)\n\nNow of course, and this is the nice part of the present paper, by stacking several of the rules, we could directly specify that we may need a certain number of latent predicates. \nThis is nice but it is not argued that this is highly novel. Consider again the work by Lippi and Frasconi. We unroll a given NN-parameterized MLN for s fixed number of forward chaining steps. This gives us essentially a computational graph that could also be made differentiable and hence we could also have end2end training. The major difference seems to be that now objects are directly attached with vector encodings, which are not present in Lippi and Frasconi's approach. This is nice but also follows from Rocktaeschel and Riedel's differentiable Prolog work (when combined with Lippi and Frasconi's approach).\nMoreover, there have been other combinations of tensors and logic, see e.g. \n\nIvan Donadello, Luciano Serafini, Artur S. d'Avila Garcez:\nLogic Tensor Networks for Semantic Image Interpretation. \nIJCAI 2017: 1596-1602\n \nHere you can also have vector encodings of constants. This also holds for \n\nRobin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, Luc De Raedt:\nDeepProbLog: Neural Probabilistic Logic Programming. CoRR abs/1805.10872 (2018)\n\nThe authors should really discuss this missing related work. This should also involve\na clarification of the \"ILP systems do not scale\" statement. At least if one views statistical relational learning methods as an extension of ILP, this is not true. Probabilistic ILP aka statistical relational learning has been used to learn models on electronic health records, see e.g., the papers collectively discussed in \n\nSriraam Natarajan, Kristian Kersting, Tushar Khot, Jude W. Shavlik:\nBoosted Statistical Relational Learners - From Benchmarks to Data-Driven Medicine. Springer Briefs in Computer Science, Springer 2014, ISBN 978-3-319-13643-1, pp. 1-68\n\nSo the authors should either discuss SRL and its successes, separating SRL from ILP, or they cannot argue that ILP does not scale. In the related work section, they decided to view both as ILP, and, in turn, the statement that ILP does not scale is not true. Moreover, many of the learning tasks considered have been solved with ILP, too, of course in the ILP setting. Any ILP systems have been shown to scale beyond those toy domains.   \nThis also includes the blocks world. Here relational MDP solvers can deal e.g. with BW worlds composed of 10 blocks, resulting in MDPs with several million states. And the can compute relational policies that solve e.g. the goal on(a,b) for arbitrary number of blocks. This should be incorporated in the discussion of the introduction in order to avoid the wrong impression that existing methods just work for toy examples. \n\nComing back to scaling, the current examples are on rather small datasets, too, namely <12 training instances. Moreover, given that we learn a continuous approximation with a limit depth of reasoning, it is also very likely that the models to not generate well to larger test instances. So the scaling issue has to be qualified to avoid to give the wrong impression that the present paper solves this issue. \n\nFinally, the BW experiments should indicate some more information on the goal configuration. This would help to understand whether an average number of moves of 84 is good or bad. Moreover, some hints about the MDP formulation should be provided, given that there have been relational MDPs that solve many of the probabilistic planning competition tasks. And, given that the conclusions argue that NLMs can learn the \"underlying logical rules\", the learned rules should actually be shown. \n\nNevertheless, the direction is really interesting but there several downsides that have to be addressed. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}