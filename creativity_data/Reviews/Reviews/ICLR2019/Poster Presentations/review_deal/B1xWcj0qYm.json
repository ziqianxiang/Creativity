{
    "Decision": {
        "metareview": "This paper studies the task of learning a binary classifier from only unlabeled data. They first provide a negative result, i.e., they show it is impossible to learn an unbiased estimator from a set of unlabeled data. Then they provide an empirical risk minimization method which works when given two sets of unlabeled data, as well as the class priors. \n\nThe four submitted reviews were unanimous in their vote to accept. The results are impactful, and might make for an interesting oral presentation.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Nice results for learning a classifier from unlabeled data"
    },
    "Reviews": [
        {
            "title": "An interesting contribution and a thorough review of the existing work in the field",
            "review": "The authors propose an unbiased estimator that allows for training models with weak supervision on two unlabeled datasets with known class priors. The theoretical properties of the estimator are discussed and an empirical evaluation shows promising performance.\n\nThe paper provides a thorough overview of the related work.\nThe experiments compare to the relevant baselines.\n\nMinor remarks:\n\nThe writing seems like it could be improved in multiple places and the main thing that makes some sections of the paper hard to follow is that the concepts often get mentioned and discussed before they are formally defined/introduced. Concepts that are introduced via citations should also be explained even if not in-depth.\n\nFigure 2: the curves suggest that the models should have been left to train for a longer time - some of the small PN and small PN prior-shift risks are still decreasing\n\nFigure 2: the scaling seems inconsistent - the leftmost subplot in each row doesn’t start at (0,0) in the lower left corner, unlike the other subplots in each row - and it should probably be the same throughout - no need to be showing the negative space.\n\nFigure 2: maybe it would be good to plot the different lines in different styles (not just colors) - for BW print and colorblind readers\n\nFor small PN and small PN prior-shift, the choice of 10% seems arbitrary. At what percentage do the supervised methods start displaying a clear advantage - for the experiments in the paper?\n\nWhen looking into the robustness wrt noise in the training class priors, both are multiplied by the same epsilon coefficient. In a more realistic setting the priors might be perturbed independently, potentially even in a different direction. It would be nice to have a more general experiment here, measuring the robustness of the proposed approach in such a way.\n\n5.2 typo: benchmarksand ; datasetsfor",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper covering proofs as well as experiments on a newly defined unbiased risk estimator for unlabeled classification",
            "review": "This paper proposes a methodology for training any binary classifier from only unlabeled data. They proved that it is impossible to provide an unbiased estimator if having only a single set of unlabeled data, however, they provide an empirical risk minimization method for only two sets of unlabeled data where all the class priors are given. Some experiments and comparisons with state-of-the-art are provided, together with a study on the robustness of the method.\n\npros:\n\n- The paper is clear, and it provides an interesting proven statement as well as a methodology that can be applied directly. Because they show that only two sets with different (and known) priors are sufficient to have an unbiased estimator, the paper has a clear contribution.\n- The impact of the method is a clear asset, because learning from unlabeled data is applicable to a large number of tasks and is raising attention in the last years.\n- The large literature on the subject has been well covered in the introduction.\n- The importance made on the integration of the method to state-of-the-art classifiers, such as the deep learning framework, is also a very positive point.\n- The effort made in the experiments, by testing the performance as well as the robustness of the method with noisy training class priors is very interesting. \n\nremarks:\n\n- part 4.1 : the simplification is interesting. However, the authors say that this simplification is easier to implement in many deep learning frameworks. Why is that?\n- part 4.2 : the consistency part is too condensed and not clear enough.\n- experiments : what about computation time?\n- More generally, I wonder if the authors can find examples of typical problems for classification from unlabeled data with known class priors and with at least two sets?\n\nminor comments:\n- part 1: 'but also IN weakly-supervised learning'\n- part 2. related work : post- precessing --> post-processing\n- part 2. related work : it is proven THAT the minimal number of U sets...\n- part 2. related work : In fact, these two are fairly different --> not clear, did you mean 'Actually, ..' ?\n- part 4.1 : definition 3. Why naming l- and l+ the corrected loss functions? both of them integrate l(z) and l(-z), so it can be confusing.\n- part 5.1 Analysis of moving ... closer: ... is exactly THE same as before.\n- part 5.2 : Missing spaces : 'from the webpage of authors.Note ...' and 'USPS datasetsfor the experiment ...' ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "thorough review of material and refinement that surpasses the state-of-the-art; needs some more development on real-world experiment or justified statement to defer issues until later paper",
            "review": "Summary: \nThe authors introduce the task of learning from unlabeled data clearly and concisely with sufficient reference to background material. They propose a learning approach, called UU, from two unlabeled datasets with known class priors and prove consistency and convergence rates. Their experiments are insightful to the problem, revealing how the two datasets must be sufficiently separated and how UU learning outperforms state-of-the-art approaches. The writing is clear and the idea is an original refinement of earlier work, justified by its exceeding state-of-the-art approaches. However, the paper needs more experimentation.  \n\nFurther details:\nWhile the introduction and set-up is long, it positions the paper well by making it approachable to someone not directly in the subject area and delineating how the approach differs from existing theory. The paper flows smoothly and the arguments build sequentially. A few issues are left unaddressed:\n- How does the natural extension of UU learning extend beyond the binary setting? \n- As the authors state, in the wild the class priors may not be known. Their experiment is not completely satisfying because it scales both priors the same. It would be more interesting to experimentally consider them with two different unknown error rates. If this were theoretically addressed (even under the symmetrical single epsilon) this paper would be much better. \n- In Table 2, using an epsilon greater than 1 seems to always decrease the error with a seeming greater impact when theta and theta' are close. This trend should be explained. In general, the real-world application was the weakest section. Expounding up on it more, running more revealing experiments (potentially on an actual problem in addition to benchmarks), and providing theoretical motivation would greatly improve the paper. \n- In the introduction is is emphasized how this compares to supervised learning but the explanation is how this compares to unsupervised clustering is much more terse. Another sentence or two explaining why using the resulting cluster identifications for binary labeling is inferior to the \"arbitrary binary classifier\" would help. It's clear in the author's application because one would like to use all data available, including the class priors, for classification. \n\nMinor issues: \n-At the bottom of page 3 the authors state, \" In fact, these two are fairly different, and the differences are reviewed and discussed in Menon et al. (2015) and van Rooyen & Williamson (2018). \" It would be clearer to immediately state the key difference instead of waiting until the end of the paragraph. \n- In the first sentence of Section 3.1 \"imagining\" is mistyped as \"imaging.\"\n- What does \"classifier-calibrated\" mean in Section 3.1? \n- In Section 3.1, \"That is why by choosing a model G, g∗ = arg ming∈G R(g) is changed as the target to which\" was a bit unclear at first. The phrase \"is changed as the target to which\" was confusing because of the phrasing. Upon second read, the meaning was clear. \n- In the introduction it was stated \"impossibility is a proof by contradiction, and the possibility is a proof by construction.\" It would be better to (re)state this with each theorem. I was immediately curious about the proof technique after reading the theorem but no elaboration was provided (other than see the appendix). The footnote with the latter theorem is helpful as it alludes to the kind of construction used without being overly detailed.\n- In section 5.2, in the next to last sentence of the first paragraph there are some issues with missing spaces. \n- Some more experiment details, e.g. hyperparameter tuning, could be explained in the appendix for reproducibility. ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Nice conceptual contribution + thoroughly executed; however dense writing, and in a crowded area",
            "review": "This paper studies the weak supervision setting of learning a general binary classifier from two unlabeled (U) datasets with known class balances. The authors establish that this is possible by constructing an unbiased estimator, analyze its convergence theoretically, and then run experiments using modern image classification models.\n\nPros:\n- This work demonstrates, theoretically and empirically, a simple way to train generic models using only the known class balances of several sets of unlabeled data (having the same conditional distributions p(x|y))---a very interesting configuration of weak supervision, an increasingly popular and important area\n\n- The treatment is thorough, proceeding from establishing the minimum number of U datasets, constructing the estimator, analyzing convergence, and implementing thorough experiments\n\nCons:\n- This is a crowded area (as covered in their related work section). As they cite, (Quadrianto et al., 2009) proposed this setting and considered linear models for k-wise classification.  Moreover, the two U datasets with known class balances can equivalently be viewed as two weak / noisy label sources with known accuracies.  Thus this work connects to many areas- both in noisy learning, as they cite heavily, but also in methods (in e.g. crowdsourcing and multi-source weak supervision) where several sources label unlabeled datasets with unknown accuracies (which are often estimated in an unsupervised fashion).\n\n- The overall clarity of the paper's writing could be improved. For example, the introduction and related work sections take up a large portion of the paper, but are very dense and heavy with jargon that is not internally defined upfront; for example \"risk rewrite\" is introduced in paragraph 2 with no internal definition and then used subsequently throughout the paper (this defn would be simple enough to give: in the context of this paper, \"risk rewrite\" means a linear combination of the class-conditional losses; or more generally, the expected loss w.r.t. distribution over classes...).  Also intuition could be briefly given about the theorem proof strategies.\n\n- The difference between the two class distributions over the U datasets seems like an important quantity (akin, in e.g. weak supervision / crowd source modeling papers, to quantity of how bounded away from random noise the labelers are). This is treated empirically, but would be stronger to have this show up in the theory somewhere.\n\n- Other prior work here has handled k classes with k U sets; could have extended to cover this setting too, since seems natural\n\nOverall take: This learning from label proportions setting has been covered before, but this paper presents it in an overall clean and general way, testing it empirically on modern models and datasets, which is an interesting contribution.\n\nOther minor points:\n- The argument for / distinction between using eqns. (3) and (4) seems a bit ad hoc / informal (\"we argue that...\").  This is an important point...\n- Theorem 1 proof seems fine, but some intuition in the main body would be nice.\n- What does \"classification calibrated\" mean?\n- Saying that three U sets are needed, where this includes the test set, seems a bit non-standard?  Also I'm confused- isn't a labeled test set used?  So what is this third U set for?\n- The labels l_+ and l_- in Defn. 3 seem to imply that the two U sets are positive vs. negative; but this is not the case, correct…?\n- Stating both Lemma 5 and Thm 6 seems unnecessary\n- In Fig. 2, seems like could have trained for longer and perhaps some of the losses would have continued decreasing?  In particular, small PN?  Also, a table of the final test set accuracies would have been very helpful.\n- More detail on experimental protocol would be helpful: what kind of hyperparameter tuning was done? repeated runs averaging?  It seems odd, for example in Fig. 3, that the green lines are so different in (a) vs. (c), and not in the way that one would expect given the decrease in theta\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}