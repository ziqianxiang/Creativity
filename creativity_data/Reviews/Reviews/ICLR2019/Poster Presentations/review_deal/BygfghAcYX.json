{
    "Decision": {
        "metareview": "I agree with the reviewers that this is a strong contribution and provides new insights, even if it doesn't quite close the problem. \n\np.s.: It seems that centering the weight matrices at initialization is a key idea. The authors note that Dziugaite and Roy used  bounds that were based on the distance to initialization, but that their reported numerical generalization bounds also increase with the increasing network size. Looking back at that work, they look at networks where the size increases by a very large factor (going from e.g. 400,000 parameters roughly to over 1.2 million, so a factor of 2.5), at the same time the bound increases by a much smaller factor. The type of increase also seems much less severe than those pictured in Figures 3/5. Since Dzugate and Roy's bounds involved optimization, perhaps the increase there is merely apparent.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "strong paper"
    },
    "Reviews": [
        {
            "title": "Solid paper.",
            "review": "The authors aim to shed light on the role of over-parametrization in generalization error. They do so for the special case of 2 layer fully connected ReLU networks, a \"simple\" setting where one still sees empirically that the test error decreasing as over-parametrization increases.\n\nBased on empirical observations of norms (and norms relative to initialization) in trained overparametrized networks, the authors are led to the definition of a new norm-bounded class of neural networks. Write u_i for the vector of weights incoming to hidden node i. Write v_i for the weights outgoing from hidden node i. They study classes where the Euclidean norm of v_i is bounded by a constant alpha_i and where the Euclidean norm of u_i - u^0_i is bounded by beta_i, where u^0_i is the value of u_i after random initialization. Call this class F_{alpha,beta} where alpha,beta are specific vectors of bounds.\n\nThe main result is a bound on the empirical Rademacher complexity of F_{alpha,beta}. \nThe authors also given lower bounds on the empirical Rademacher complexity for carefully chosen data points, showing that the bounds are tight. These Rademacher bounds yield standard bounds on the ramp loss for fixed alpha,beta, and margin, and then a union bound argument extends the bound to data-dependent alpha,beta and margin.\n\nThe authors compare the bounds to existing norm-based bounds in the literature. The basic argument is that the terms in other bounds tend to grow as networks get much larger, while their terms shrink. Note that at no point are the bounds in this paper \"nonvacuous\", ie they are always larger than one.\n\nIn summary, I think this is a strong paper. The explanatory power of the results are still oversold in my opinion, even if they use hedged language like \"could explain the role...\". But the work is definitely pointing the way towards an explanation and deserves publication. The technical results in the appendix will be of interest to the learning theory community.\n\nissues:\n\n\"could explain role of over-parametrization\". Perhaps this work might point the way to an explanation, but it does not yet provide an explanation.  It is a big improvement it seems.\n\n\"bound improves over the existing bounds\". From this statement and the discussion comparing the bounds, it is not clear whether this bound formally dominates existing bounds or merely does so empirically (or under empirical conditions). \n\ntypos: \n\nbigger than the Lipschitz CONSTANT of the network class\n\nH undefined\n\nRademacher defined for H but must be defined on loss class (or a generic function class, not H)\n\n\"we need to cover\" --> \"it suffices to\"\n\n\"the following two inequaliTIES hold by Lemma 8\"\n\nbibliography is a mess: half of the arxiv papers are published. typos everywhere, very sloppy.\n\n(This review was requested late in the process due to another reviewer dropping out of the process.)\n\n[UPDATE]. The authors addressed my concerns stated in my review above. I think the bibliography has improved and I recommend acceptance. ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Promising paper, with a couple of clarifications needed",
            "review": "Let me start by apologizing for the delayed review - in fact I was asked today to replace an earlier assigned reviewer. Hopefully the clarifications I request won't be too time consuming to meet the deadline coming up. \n\n###\n\nFirst of all, the problem which the authors are attempting to answer is quite important: the effect of over-parametrization is not well understood on a theoretical level. As the paper illustrate, 2-layer networks are already capable of generalizing while being over-parameterized, therefore justifying their setting. \n\nNext this paper motivates the study of complexity quantities that tend to decrease with the number of parameters, in particular figure 3 motivates the conjecture that the complexity measure in Theorem 2 can control generalization error. The paper also does a great job comparing related work, motivating their results. \n\n###\n\nAt this point, I would like to request a couple of clarifications in the proofs. Perhaps it's due to the fact that I only spent a day reading, but at least I think we could improve on its readability. Regardless, I currently do not yet trust a couple of the proofs, and I believe the acceptance of this paper should be conditioned on confirming the correctness of these proofs.\n\n(1) Let's start with Lemma 10. In the middle equation block, we obtain a bound \n  \\| alpha^prime \\|_p^p <= beta^p ( 1 + D/K )\nand the proof concludes alpha^prime is in Q. However this cannot be the case for all alpha^prime. \n\nConsider x=0 which is in S_{p, beta}^D, then we have alpha^prime = 0 as well. In the definition of Q, we require all the j's to sum up to K+D, which is not met here. \n\nAt the same time, the next claim \n  \\| alpha \\|_2 <= D^{1/2 - 1/p} \\| alpha^prime \\|_p\ndoes not seem to follow from the above calculations. In particular, alpha^prime seems to be defined with respect to an x in S_{p, beta}, however in this case we did not specify such an x. Perhaps did you mean there exist such an alpha^prime?\n\n(2) In the proof of Theorem 3, there is an important inequality needed to complete the proof \n  max{ <s, f_i> , <s, -f_i> } >= 1/2 * ( <s, [f_i]_+> + <s, [-f_i]_+> )\n\nPerhaps I am missing something obvious, but I believe this inequality fails when we choose s as a constant vector, and f_i to have the same number of positive and negative signs (which is possible in a Hadamard matrix). In this case, the left hand side should be equal to zero, where as the right hand side will be positive. \n\n###\n\nTo summarize, if these proofs can be confirmed, I believe this paper would have made significant contribution to the problem of over-parametrization in deep learning, and of course should be accepted. \n\n###\n\nI corrected several typos and found minor issues as I read, perhaps this will be useful to improve readability as well.\n\nPage 13, proof of Lemma 8\n  - after the V_0 term is separated, there is a sup over \\|V_0\\|_F <= r in the expectation, which should be \\|V-V_0\\|_F <= r instead.\n\nPage 14, Lemma 9\n  - the lemma did not define rho_{ij} in the statement\n\nPage 15, proof of Lemma 9\n  - in equation (12), there is an x_y vector that should x_t\n\nPage 15, proof of Theorem 1\n  - while I eventually figured it out, it's unclear how Lemma 8 is applied here. Perhaps one more step identifying the exact matrices in the statement of Lemma 8 will be helpful to future readers, and maybe explain where the sqrt(2) factor come from as well. \n\nPage 16, proof of Lemma 10\n  - in the beginning of the proof, to stay consistent with the notation, we should replace S_{p, beta} with S_{p, beta}^D\n  - I believe the cardinality of Q should be (K + D - 1) choose (D - 1), as we need to choose positive j's to sum up to (K+D) in the definition of Q. This reduces down to the problem of choosing natural numbers j's summing K, which is (K+D-1) choose (D-1). Consider the stack exchange post here:\nhttps://math.stackexchange.com/questions/919676/the-number-of-integer-solutions-of-equations\n\nPage 16, proof and statement of Lemma 11\n  - I believe in the first term, the factor should be m instead of sqrt(m). I think the mistake happened when applying the union bound, as it should only affect the term containing delta\n\nPage 17, Lemma 12\n  - same as Lemma 11, we should have m instead of sqrt(m)\n\nPage 18, proof of Theorem 3\n  - at the bottom the statement \"F is orthogonal\" does not imply the norm is less than 1, but rather we should say \"F is orthonormal\"\n\nPage 19, proof of Theorem 3\n  - at the top, \"we will omit the index epsilon\" should be \"xi\" instead\n  - in the final equation block, we have the Rademacher complexity of F_{W_2}, instead it should be F_{W^prime}\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The authors present a novel bound for the generalization error of 1-layer neural networks with multiple outputs and ReLU activations. ",
            "review": "It is shown empirically that common algorithms used in supervised learning (SGD) yield networks for which such upper bound decreases as the number of hidden units increases. This might explain why in some cases overparametrized models have better generalization properties.\n\nThis paper tackles the important question of why in the context of supervised learning, overparametrized neural networks in practice generalize better. First, the concepts of \\textit{capacity} and \\textit{impact} of a hidden unit are introduced. Then, {\\bf Theorem 1} provides an upper bound for the empirical Rademacher complexity of the class of 1-layer networks with hidden units of bounded \\textit{capacity} and \\textit{impact}. Next, {\\bf Theorem 2} which is the main result, presents a new upper bound for the generalization error of 1-layer networks. An empirical comparison with existing generalization bounds is made and the presented bound is the only one that in practice decreases when the number of hidden units grows. Finally {\\bf Theorem 3} is presented, which provides a lower bound for the Rademacher complexity of a class of neural networks, and such bound is compared with existing lower bounds.\n\n## Strengths\n- The paper is theoretically sound, the statement of the theorems\n    are clear and the authors seem knowledgeable when bounding the\n    generalization error via Rademacher complexity estimation.\n\n- The paper is readable and the notation is consistent throughout.\n\n- The experimental section is well described, provides enough empirical\n    evidence for the claims made, and the plots are readable and well\n    presented, although they are best viewed on a screen.\n\n- The appendix provides proofs for the theoretical claims in the\n    paper. However, I cannot certify that they are correct.\n\n- The problem studied is not new, but to my knowledge the\n    presented bounds are novel and the concepts of capacity and\n    impact are new. Theorem 3 improves substantially over\n    previous results.\n\n- The ideas presented in the paper might be useful for other researchers\n    that could build upon them, and attempt to extend and generalize\n    the results to different network architectures.\n\n- The authors acknowledge that there might be other reasons\n    that could also explain the better generalization properties in the\n    over-parameterized regime, and tone down their claims accordingly.\n\n## Weaknesses\n\\begin{itemize}\n- The abstract reads \"Our capacity bound correlates with the behavior\n    of test error with increasing network sizes ...\", it should\n    be pointed out that the actual bound increases with increasing\n    network size (because of a sqrt(h/m) term), and that such claim\n    holds only in practice.\n\n- In page 8 (discussion following Theorem 3) the claim\n    \"... all the previous capacity lower bounds for spectral\n        norm bounded classes of neural networks (...) correspond to\n        the Lipschitz constant of the network. Our lower bound strictly\n    improves over this ...\", is not clear. Perhaps a more concise\n    presentation of the argument is needed. In particular it is not clear\n    how a lower bound for the Rademacher complexity of F_W translates into a\n    lower bound for the rademacher complexity of l_\\gamma F_W. This makes the claim of tightness of Theorem 1 not clear. Also this makes\n    the initial claim about the tightness of Theorem 2 not clear.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}