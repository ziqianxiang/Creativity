{
    "Decision": {
        "metareview": "While the reformulation of RNNs is not practical as it is missing sigmoids and tanhs that are common in LSTMs it does provide an interesting analysis of traditional RNNs and a technique that's novel for many in the ICLR community.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Reasonable strong theory paper"
    },
    "Reviews": [
        {
            "title": "Good quality paper, experimentation can be improved",
            "review": "The paper rewrites equations of Elman RNN in terms of so-called max-affine spline operators. Paper claims that this reformulation allows better analysis and, in particular, gives an insight to use initial state noise to regularize hidden states and fight exploding gradients problem.\n\nThe paper seems to be theoretically sound. The experiment with sequential MNIST looks very promising, thought it would be great to check this method on other datasets (perhaps, toy data) to check that this is not a fluke. The bird audio dataset is not well benchmarked in the literature. The paper could make much stronger claim with more extensive experimentation.\n\nSome typos:\n- p3: an simple -> a simple\n- Figure 2 caption is not finished\n- p5 last paragraph: extra full stop\n- Fig 3: correct and negative probably switched around\n- p7: in regularize -> in regularization\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting view point, but with limited applications",
            "review": "In this paper, the authors provide a novel approach towards understanding\nRNNs using max-affine spline operators (MASO). Specifically, they rewrite RNNs\nwith piecewise affine and convex activations MASOs and provide some\nexplanation to the use of noisy initial hidden state. \n\nThe paper can be improved in presentation. More high level explanation should\nbe given on MASOs and why this new view of RNN is better. \n\nTo best of my knowledge, this is the first paper that related RNNs with MASOs\nand provides insights on this re-formulation. However, the authors failed to\nfind more useful applications of this new formulation other than finding that\nnoisy initial hidden state helps in regularization. Also, the re-formulation\nis restricted to piecewise affine and convex activation functions (Relu and\nleaky-Relu). \n\nIn general, I think this is an original work providing interesting viewing\npoint, but could be further improved if the authors find more applications of\nthe MASO form. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Very promising paper, in particular regarding applications, yet I found that heavy notation not so well explained made it hard to read",
            "review": "This paper builds upon recent work by Balestriero and Baraniuk (ICML 2018) that concern max-affine spline opertaor (MASO) interpretation of a substantial class of deep networks. In the new paper a special focus is put on Recurrent Neural Networks (RNNs), and it is highlighted based on theoretical considerations leveraging the MASO and numerical experiments that in the case of a piecewise affine and convex activation function, using noise in initial hidden state acts as regularization.  \nOverall I was impressed by the volume of contributions presented throughout the paper and also I very muched like the light shed on important classes of models that turn out to be not as black box as they could seem. My enthouasiasm was somehow tempered when discovering that the MASO modelling here was in fact a special case of Balestriero and Baraniuk (ICML 2018), but it seems that despite this the specific contribution is well motivated and justified, especially regarding application results. Yet, the other thing that has annoyed me and is causing me to only moderately champion the paper so far is that I found the notation heavy, not always well introduced nor explained, and while I believe that the authors have a clear understanding of things, it appears to me that the the opening sections 1 and 2 lack notation and/or conceptual clarity, making the paper hard to accept without additional care. To take a few examples:\na) In equation (3), the exponent (\\ell) in A and B is not discussed. On a different level, the term \"S\" is used here but doesn't seem to be employed much in next instances of MASOs...why? \nb) In equation (4), sure you can write a max as a sum with an approxiate indicator (modulo unicity I guess) but then what is called Q^{(\\ell)} here becomes a function of A^{(\\ell)}, B^{(\\ell)}, z^{(\\ell-1)}...?\nc) In proposition 1, the notation A_sigma is not introduced. Of course, there is a notation table later but this would help (to preserve the flow and sometimes clarify things) to introduce notations upon first usage...\nd) Still in prop 1, braket notation not so easy to grasp. What is A[z]z? \ne) Still in prop 1, recall that sigma is assumed piecewise-linear and convex? \nf) In th1, abusive to say that the layer \"is\" a mapping, isn't it?  \ng) In Theorem 2, what is f? A generic term for a deterministic function? \nAlso, below the Theorem, \"affine\" or \"piecewise affine\"? \nh) I found section 4 somehow disconnected and flow-breaking. Put in appendix and use space to better explain the rest? \ni) Section 5 is a strong and original bit, it seems. Should be put more to the fore in abstract/intro/conclusion? ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}