{
    "Decision": {
        "metareview": "There has been a recent focus on proving the convergence of Bayesian fully connected networks to GPs. This work takes these ideas one step further, by proving the equivalence in the convolutional case.\n\nAll reviewers and the AC are in agreement that this is interesting and impactful work. The nature of the topic is such that experimental evaluations and theoretical proofs are difficult to carry out in a convincing manner, however the authors have done a good job at it, especially after carefully taking into account the reviewers’ comments.\n\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Interesting work taking recent advances one step further"
    },
    "Reviews": [
        {
            "title": "REVIEW OF DEEP BAYESIAN CONVOLUTIONAL NETWORKS WITH MANY CHANNELS ARE GAUSSIAN PROCESSES",
            "review": "The paper establishes a connection between  infinite channel Bayesian convolutional neural network and Gaussian processes. The authors prove that taking the number of channels in a Bayesian CNN to infinite leads to a GP with a specific Kernel (GP-CNN) and provide a Monte Carlo approach to evaluate the kernels when it is intractable. They show that without pooling the kernel fails to maintain the equivariance property that is achievable with a CNN without pooling.  GP-CNN with pooling maintains the invariance property. They make extensive  experimental comparison with CNN, demonstrating that as the number of channels become large, CNN achieve performance close to a GP-CNN. A discussion on reasons for best CNN to give a performance better than GP-CNN (especially with pooling), and a experimental comparison with finite width Bayesian CNN would have made the paper more concrete.  The paper has both strong theoretical and experimental contribution, and is also very relevant to the ICLR conference.\n\nQuality\n\nThe paper provides a theoretical connection between Bayesian CNN with infinite wide channels and Gaussian processes with a recursive kernel (GP-CNN). The derivations and arguments seem correct. The experiments are conducted comparing the performance of SGD trained CNN with  GP-CNN, and other models on mainly on CIFAR-10 data set. \nHowever, some discussion and clarity on the following points will be  useful to improve the paper.\n\n- (Page 5) on convergence of K^l :  From Equations (3) and (4), it can be seen that  K^l converges to  C(K^{l-1}), with C(K^{l-1}) defined slightly different from the paper, in that the expectation over z  is taken w.r.t z~ N(0;A(K)) instead of z ~ N(0; K). Is this equivalent to the expressions (7) and (8) described in the paper for a non-linear function \\phi ?\n- Experimental comparison with Bayesian CNN, demonstrating the effect of increasing the number of channels.\n- (Page 7) GP-CNN with pooling :  Paper proposes subsampling one particular pixel to improve computational efficiency. Has some experiments been performed to evaluate the performance of this approach ? How accurate is this approach ?\n- Discussion on the positive semi-definiteness of the recursive GP-CNN kernel\n- More explanations on why the best SGD-trained CNN gives a better performance than GP-CNN, especially with pooling. Does the Monte-Carlo approximation of GP-CNN kernel  computation could impact this performance? I suppose hyper-parameters of the GP-CNN kernel are not learnt from the data, could this result in a lower accuracy  ?\n- Discussion on learning the hyper-parameters of the GP-CNN kernel and its impact on the performance of the model. \n- Demonstrate  through some sample figures that GP-CNN with pooling achieves invariance while GP-CNN with out pooling fail to capture it.\n- Is the best result on CIFAR-10  achieved using the proposed method ? See Deep convolutional Gaussian processes by Kenneth Blomqvist, Samuel Kaski, Markus Heinonen\n- Include the results with CNN-GP both with pooling and without pooling in Table 1 and Table 2. \n- Provide the results of best SGD trained CNN against CNN-GP, both with pooling, as in Figure 3.c. Is the same trend observed in this case also ?\n- Experimental comparison and results on other Image datasets, specifically MNIST. Does the same observations hold on MNIST too ? \n\nClarity\n\nThe paper is relatively well written and clearly provides main ideas leading to the results. However, notations could have made more succinct, and figures could have been more legible( Axis labels are missing for some figures in Figure 3, and provide legends wherever possible). The is also an ambiguity in what CNN-GP refers to, with pooling to without pooling.\n- The term CNN-GP is overloaded in many places in the experimental section. I guess in Table 1, its CNN-GP without pooling, while in Table 2, its CNN-GP with pooling. Kindly make the distinction clear in the nomenclature itself, by calling one of them by a different name. Its also not clear when they mention SGD trained CNN, if it is with pooling or without pooling.  \n- What is the difference between the top and bottom pair of figures in Figure 3 (b). Why is the GP performance different in top and bottom cases.?\n- What does 10, 100, 1000 correspond to in Figure 3 ? Please explain it in caption.\n\nOriginality\n\nPrevious works of  Lee and G. Matthews (2018) had shown the equivalence between Deep Neural Networks and GPs. This paper has extended it  to deep convolutional  neural network setting, but is interesting in its own way. The have come up with an equivalent kernel corresponding to infinite wide Bayesian convolution neural network and provided a monte-carlo approach to compute it. Along with the theoretical contribution, they have also provided extensive experimental comparison. \n\nSignificance\n\nThe paper has made significant contributions connecting the Bayesian convolutional neural networks with Gaussian processes, in deriving the equivalent kernel for GPs, and in demonstrating the performance of the proposed approach on Image datasets",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "BAYESIAN CONVOLUTIONAL NEURAL NETWORKS WITH MANY CHANNELS ARE GAUSSIAN PROCESSES",
            "review": "Overall Score: 7/10.\nConfidence Score: 3/10. (This paper includes so many ideas that I have not been able to prove that are right due to\nmy limited knowledge, but I think that there are correct).\n\nSummary of the main ideas: This paper establishes a theoretical correspondence between BCNN with many channels and GP and\npropsoes a Monte Carlo method to estimate the GP corresponding to a NN architecture. It is a very strong and complete\npaper since its gives theoretical contents and experiments content. I think that it is a really good result that should\nbe read by anyone interested in Neural Network and GP equivalences, and that Machine Learning in general needs these kind\nof papers that establish this complicated equivalences.\n\nRelated to: The work by Lee and G. Matthews (2018) regarding equivalence between Deep Neural Networks and GPs and the\nConvolutional Neural Network framework.\n\nStrengths:\nTheoretical content, Experiments and methodology content (even a Monte Carlo approach) makes it a very complete paper.\nHaving been able to establish complicated and necessary equivalences.\n\nWeaknesses:\nVery difficult for newcomers or non expert technical readers.\n\nDoes this submission add value to the ICLR community? : Yes, it adds, and a lot.\n\nQuality:\nIs this submission technically sound?: Yes it is, it is a necessary step in GP-NN equivalence research.\nAre claims well supported by theoretical analysis or experimental results?: Yes, quite sure.\nIs this a complete piece of work or work in progress?: Complete piece of work.\nAre the authors careful and honest about evaluating both the strengths and weaknesses of their work?: Yes, they are.\n\nClarity:\nIs the submission clearly written?: Yes, but I suggest giving formal introductions to some concepts in the introduction\nand include a figure with the ideas given or the equivalences.\nIs it well organized?: Yes, although sometimes section feel a little but put one after the another. More cohesion would be\nadded if they are introduce before.\nDoes it adequately inform the reader?: Yes.\n\nOriginality:\nAre the tasks or methods new?: The monte carlo is new, the other methods not but the task of the equivalence is new.\nIs the work a novel combination of well-known techniques?: It is kind of a combination, but the proposed ideas are new, it is very theoretical.\nIs it clear how this work differs from previous contributions?: Yes, authors bother in explaining it clearly.\nIs related work adequately cited?: Yes, this is a huge positive point of the paper.\n\nSignificance:\nAre the results important?: From my point of view, yes they are.\nAre others likely to use the ideas or build on them?: I think so, because the topic is hot right now.\nDoes the submission address a difficult task in a better way than previous work?: It is a new task.\nDoes it advance the state of the art in a demonstrable way?: Yes, clearly.\nDoes it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach?: Yes, the theoretical approach is sound.\n\n\nArguments for acceptance: It is a paper that provides theory, methodology and experiments regarding a very difficult and challenging task that add value to the community and makes progress in the area of the equivalence between NN and GPs.\n\nArguments against acceptance: I do not have.\n\nTypos:\n\n-> Define the channel concept in introduction.\n-> Put in bold best results of the experiments.\n-> Why not put \"deep\" in the title?\n-> In the introduction, introduce formally a CNN. (brief)\n-> Define the many channel limit.\n-> Put a figure with the equivalences and with the contents of the paper explaining a bit.\n\n\n\nAfter rebuttal:\n=============\n\nAuthors have addressed many topics that not only I but rev 3 address and hence I score this paper with a 7 and recommend it for publication.",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review",
            "review": "****Reply to authors' rebuttal****\n\nDear Authors,\n\nI greatly appreciate the effort you have put into the rebuttal. The changes you have made have addressed most of my concerns and I believe that the few outstanding ones can be fixed without significantly affecting the main message of the paper. I will thus be recommending acceptance of the paper.\n\nBest wishes,\nRev 3\n\n\nSeveral remarks on the updated version:\n\n- (p.20, A.5.1) To ensure the random variables are well-defined, please state explicitly which sigma algebra is F (I am assuming the product Borel sigma-algebra + the relevant definitions of the random variables). This is important for the reader to understand what convergence in distribution on this particular space does and does not imply. Some readers might also appreciate if you used the mentioned \"infinite width, finite fan-out, networks\" (Matthews et al.) construction (or similar) which would ensure that the collection of random variables {z_i^l}_{i \\in N*} is well-defined for any network width and l, which currently does not seem to be the case according to Eqs. (28-29). If the full countably infinite vectors of random variables are not defined for all networks in the sequence, it is not possible to prove their convergence in distribution to the relevant GPs.\n\n- (p.21, A.5.3) Thank you for clarifying the definition of elements of the sequential limit. If possible, I would further recommend first fixing the probability space and then defining the random variables (the argument just before Theorem A.2 seems somewhat circular as R.V.s should first be defined on some space, and not put on a probability space post-hoc; perhaps some product space with the product sigma-algebra would work here?!). Furthermore, if I understand correctly, there are now L sequences of neural networks (one sequence for networks with 0, ..., L-1 \"infinite layers\"), rather than a single sequence, and the \"infinite layers\" are squashed into a single \"infinite layer\" which is represented by z_i^\\infty? In other words, all the infinite layers are replaced by iid samples from a particular GP and only the finite layers have the standard neural network structure? If I am mistaken (or not), perhaps a further explanatory footnote would help the reader.\n\n- (p.21, A.5.3 & p.23, A.5.4) Thank you for improving the discussion of joint convergence. Please clarify that proving convergence for any finite m is sufficient for proving convergence in distribution of the countably infinite vector {z_i}_{i \\in N*} for the **product Borel sigma-algebra** (e.g. using an argument like the one on p.19 of Billingsley (1999)).\n\n- (p.21) \"Uniformly square-integrable\": to me, this phrase suggests that the collection of squares of the functions has to be uniformly integrable but the definition in Eq. (27) only states one of the conditions in definition of uniform integrability. Please clarify that \"uniform square-integrability\" here is not related to the standard notion of \"uniform integrability\" in the literature.\n\n\n\n\n\n****Summary****\n\nThis paper extends recent results on convergence of Bayesian fully connected networks (FCNs) to Gaussian processes (GPs), to the equivalent relationship between convolutional neural networks (CNNs) and GPs. This is currently an area of high interest, with Xiao et al. (2018) examining the same relationship from a mean-field perspective, and two other concurrent papers making contributions:\n\nhttps://arxiv.org/abs/1808.05587\nhttps://arxiv.org/abs/1810.10798\n\nThus the scope of the paper fits well within the aims of the conference.\n\nI really appreciate that the authors did not shy away from studying the effect of pooling layers, and find the connection to locally connected networks they describe intriguing and insightful. On the experimental side, the investigation of the relative importance of compositionality, equivariance and invariance on performance of CNNs is very interesting.\n\nThese experiments and investigations are however based on a theoretical foundation which suffers from several issues. The main problems are an incorrect proof of convergence of the joint distribution of filters, and an improper use of convergence in probability in cases where random variables do not share a common underlying probability space. Unfortunately, either of these by itself invalidates the main theoretical claims which is why I am recommending rejection of the paper.\n\nHowever, I believe that the argument in (A.4.3) can potentially be rectified, and, as I detail below, is of greater interest to the community relative to the ones in (A.4.1) and (A.4.2). If this is accomplished and the proofs in (A.4.1) and (A.4.2) are either also fixed or left out (A.4.3 is sufficient to justify the claims in the main body), I am willing to significantly improve my rating of this paper and potentially recommend acceptance. For this reason, a \"detailed comments\" section is appended at the end of the standard review where the technical issues are described in much greater detail.\n\n\n****General comments****\n\n**Bayesian vs. infinite neural networks**\n\nThe main theoretical claims concerning the relationship between Bayesian CNNs and GPs are within Section 2. Therein on top of page 4, the authors say \"In Appendix A.4 we give several **alternative** derivations of the correspondence\" (emphasis mine), and then progress to outline the skeleton of the argument (A.4.2) in Sections 2.2.1-2.2.3. Section 2.2.3 is concluded by statement of the main theoretical result of this paper, Eq. (10), which comes from (A.4.3) and can only be linked to the rest of Section 2 through the claim of equivalence between the \"alternative derivations\" (A.4.1), (A.4.2) and (A.4.3). The problem is that the equivalence claim does not hold, as explained below:\n\nThe most important distinction here is between what I will call a \"sequential\" and a \"simultaneous\" limit. In the \"sequential\" case (A.4.1 & A.4.2, Sections 2.2.1-2.2.3), layers are taken to infinity one by one, whereas in the \"simultaneous\" case (A.4.3, used to obtain the result concluding Section 2.2.3) all layers are **finite** for **all** members of the sequence, growing in width simultaneously.\n\nThe \"simultaneous\" limit (A.4.3) is in my view more interesting as it tells us that **finite** BNNs do indeed converge to GPs in distribution, i.e. that for each expectation of a continuous bounded function of the outputs of the limiting GP, there exists a BNN with a **finite** number of neurons in **each** layer for which the expectation of the same function is arbitrarily close. From a practical perspective, \"simultaneous\" limit tells us that inference algorithms for BNNs (which can be inaccurate and/or computationally expensive) can sometimes be replaced by exact or approximate inference algorithms for the limiting GP (cf. Section 5 in (Matthews et al., 2018, extended version)).\n\nThe \"sequential\" limit (A.4.1 & A.4.2) on the other hand does not establish existence of finite BNNs arbitrarily close to a particular GP, or justify use of the GP limit as approximation for finite BNNs as above. This is because the width of individual layers goes to infinity in a sequence from first to last. This means that most of the networks that constitute the sequence converging to the GP will have **one or more infinitely wide layers** and thus do not correspond to the finite BNNs we usually work with. In other words, \"sequential\" limit can only ever establish that there exists a network with **all but the final hidden layer infinite** that is arbitrarily close to the limiting GP. The only case where \"sequential\" and \"simultaneous\" limits agree is thus in the single hidden layer case first studied by Neal (1996). I will call the networks with one or more infinite layers \"infinite networks\", inspired by the work of Williams (1997) and others. Notice that infinite networks cannot be described by Eqs. (1) and (2) as the weights would be zero with probability one and thus output of the network would only depend on biases. It is not immediately obvious how to formally replace Eqs. (1) and (2) in the case of infinite networks which is one of the technical issues with the approaches in (A.4.1) and (A.4.2) (see the detailed comments section for further discussion).\n\nOthers may of course disagree and find \"sequential\" limits more interesting, but if the authors wish to keep the description of (A.4.2) in the main paper (Sections 2.2.1-2.2.3), it would be highly beneficial if readers were given the opportunity to understand the differences between the two types of limits so that they can form their own judgement. The authors should then also make clearer that the approach described in Sections 2.2.1-2.2.3 cannot be used to obtain the final result, Eq. (10). I would rather recommend reworking Sections 2.2.1-2.2.3 based on the \"simultaneous\" limit argument in (A.4.3) which unlike the current one can justify the result in Eq. (10) stated at the end.\n\n\n**Other comments**\n\n- (p.2, top) You say your results are \"strengthening and extending the result of Matthews et al. (2018)\" which is somewhat confusing. Matthews et al. prove a result for FCNs whereas this paper focuses on CNNs. Extension of (A.4.3) to FCNs may well be possible but is not included in this paper. Results in (A.4.1) and (A.4.2) are for the \"sequential\" whereas Matthews et al. study the \"simultaneous\" limit. Further differences:\n\t- Matthews et al. prove convergence for any countable rather than only finite input sets.\n\t- In Matthews et al.'s work, Gaussianity is obtained through use of a particular version of CLT, whereas this work exploits Gaussianity of the prior over weights and biases. Going forward, an extension to more general priors/initialisations (like uniform or any sub-Gaussian) is likely to be easier using the CLT approach.\n\t- Matthews et al.'s assumption on the activation functions is independent of the input set (p.7, Definition 1), whereas this work uses an assumption that is explicitly dependent on input (Eq. (37)) which might be potentially difficult to check.\n\n- (p.15, A.2 end) Should also mention Titsias (2009), \"Variational Learning of Inducing Variables in Sparse Gaussian Processes\", as a classical reference for approximate GP inference.\n\n\n****Questions****\n\n- (Section 4) Can you please provide more details on the MC approximation? Specifically, is only the last kernel approximated, or rather all of them, sequentially resampling from the Gaussian with empirical covariance in each layer? In case you tried, is there any qualitative or quantitative difference between the two approaches?\n\n- (Section 4 and Appendix A) Daniely et al. (2016) assume that the inputs to the neural network are l^2 normalised. You mention that the inputs have been normalised in the experiments (A.6). Is this assumption used in any of your proofs? Have you observed that l^2 normalisation improves empirical performance?\n\n- (p.8, Figure 6) How was \"the best CNN with the same parameters\" selected? If training error is zero for all, was it selected by validation accuracy? I was assuming that what is plotted is an estimate of the **expected** generalisation error, whereas the above selection procedure would be estimating supremum of the support of the generalisation error estimator which does not seem like a fair comparison. Can you please clarify?\n\n- (p.8 and A.6) Why only neural networks with zero training loss were allowed as benchmarks? How did the ones with non-zero training error fared in comparison? Can you please expand on footnote 3?\n\n- (p.8, last sentence) \"an observation specific to CNNs and FCNs or LCNs\": Matthews et al. (2018, extended version) observed in Section 5.2 that BNNs and their corresponding GP limits do not always perform the same even in the FCN case (cf. their Figure 8). Their paper unfortunately does not compare to equivalent FCNs trained by SGD. Have you experimented with or have an intuition for whether the cases where SGD trained models prevail coincide with the cases where BNNs+MCMC posterior inference outperform their GP limit?\n\n- (p.15, Table 3) The description says you were using erf activation (instead of the more standard ReLU): why? Have you observed any significant differences? Further, how big a proportion of the values in the image is black due to the numerical issues mentioned in A.6.4?\n\n- (p.18, just after Eq. 39) Use of PSD_{|X|d} in (A.4.3) suggests this proof assumes \"same\" padding is used?! Does the proof generalise to any padding/changing dimensions of filters inside the network?\n\n- (A.6) Can you comment on the pros & cons of \"label regression\" for classification and how does it compare with approximate inference when softmax is put on top of a GP (perhaps illustrating by a simple experiment on a toy dataset)?\n\n\n[end of standard review]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[detailed comments]\n\n****Technical concerns****\n\nNotation-wise, I would strongly encourage incorporating the dependence on network width into your notation, at the very least throughout the appendix. It would greatly reduce the amount of mental book-keeping the reader currently has to do, and significantly increase clarity at several places.\n\nOne of my main concerns is that the random variables and their underlying probability space are never formally set-up. This is problematic because convergence in probability is only defined for random variables sharing the same underlying space. At the moment, networks with different widths are not set-up to share a probability space. The practical implication for the approaches relying on convergence in probability of the empirical covariance matrices K is that the convergence in probability is not well-defined exactly because the empirical covariance matrices are not set-up on the same underlying probability space. A possible way to address this issue is to use an approach akin to what Matthews et al. (2018, extended version) call \"infinite width, finite fan-out, networks\" on page 20. This puts the networks on the same underlying space and because the empirical covariance matrices are measurable functions of thus defined random variables, they will also share the same underlying probability space.\n\nAlso regarding convergence in probability, please state explicitly with respect to which metric is the convergence considered when first mentioned (A.4.3 is explicitly using l^\\infty; A.4.2 perhaps l^2 or l^\\infty?), and make any necessary changes (e.g. show continuity of the mapping C in A.4.2).\n\nAt several places within the paper, you state that the law of large numbers (LLN) or the central limit theorem (CLT) can be applied. Apart from other concerns detailed later, these come with conditions on finiteness of certain expectations (usually the first one or two moments of the relevant random variables). Please provide proofs that these expectations are indeed finite and make any assumptions that you need explicit in the main text.\n\nAnother major concern is that none of (A.4.1), (A.4.2) and (A.4.3) successfully proves joint convergence of the filters at the top layer as claimed in the main text (e.g. Eq. (10)), and instead only focuses on marginal convergence of each filter which is not sufficient (cf. the comment on joint vs. pairwise Gaussianity below). This is perhaps sufficient if a single filter is the output of the network, but insufficient otherwise, especially when proving convergence with additional layers added on top of the last convolutional layer (as in Section 3) whenever the number filters is taken to infinity.\n\nIt would be nice, but not necessary for acceptance of the paper, to extend the proofs to uncountable index sets. I think you could use the same argument as described towards the end of Section 2.2  in (Matthews et al., 2018, extended version) and references therein.\n\n\n**Other comments**\n\n- I would strongly encourage distinguishing more clearly between probability distributions and density functions. For example, I would infer that lower case p refers to the probability distribution from Eq. (6); however, in Eqs. (8) and (9) the same notation is used for density functions (whilst integrating against the Lebesgue measure). This is quite confusing in this context as the two objects are not the same (see next two comments). I would suggest using capital P when referring to distribution, and lower case p when referring to its density.\n\n- (p.4, Eq. 6) If p is a density, it cannot be equal to a delta distribution. If it is a probability distribution then I am similarly confused - convergence in probability is a statement about behaviour of random variables, not probability distributions; in that case possibly Eq. (6) is trying to say that the empirical distribution of K^l (which is a random variable) conditioned on K^{l-1} converges weakly to the delta distribution on the RHS in probability? Please clarify.\n\n- (p.5, Eq. 10) I would recommend stating explicitly the mode of convergence. If p is the density then even assuming A.4.3 can be fixed to prove weak convergence of the **joint** distribution of filters is not enough not justify Eq. (10) - convergence in distribution does not imply pointwise convergence of the density function. If p is the distribution, then I would possibly use the more standard notation '\\otimes' instead of '\\prod'.\n\n- (p.17, end of A.4.2) You say \"Note that addition of various layers on top (as discussed in Section 3) does not change the proof in a qualitative way\". Can you please provide the formal details? At the very least, joint convergence of filters will have to be established if fully connected layers are added on top. This is the main reason why joint convergence of filters in the top layer is important.\n\n\n****Specific comments & issues for individual proofs****\n\n**Approaches suited infinite networks (\"sequential limit\")**\n\nAs mentioned in the beginning, it is not entirely clear how to formalise infinite networks in a way analogous to Eqs. (1) and (2) in your paper. This is important because you are ultimately proving statements about random variables, like convergence in probability, and this is not possible if those random variables are not formally defined. This section only comments on technical issues with the approaches described in (A.4.1) and (A.4.2). From now on, I assume that the authors' were able to formally define all the mentioned random variables in a way that fits with (A.4.1) and (A.4.2).\n\n\n(i) Hazan and Jaakola type approach (A.4.1)\n\nThis approach essentially iteratively applies a version of the recursion first studied by Hazan and Jaakola (2015), \"Steps Toward Deep Kernel Methods from Infinite Neural Networks\".\n\n- (p.16, A.4.1) Please provide reference for the claim that \"pairwise independent Gaussian implies joint independent Gaussian\". This seems to assume that the variables are jointly Gaussian which is, as far as I can see, not established here.\n\t- see second part of the linked answer for a nice example of three random variables with pairwise standard normal marginals, but joint not the multivariate standard normal:\n\n\thttps://stats.stackexchange.com/questions/180708/x-i-x-j-independent-when-i%E2%89%A0j-but-x-1-x-2-x-3-dependent/180727#180727 \n\n- (p.16, A.4.1) The application of the multivariate CLT is slightly more complicated than the text suggests. Except for the necessity of proving finiteness of the relevant moments, multivariate CLT does not out-of-the-box apply to infinite dimensional random variables like {z_j^{l+1}}_{1 \\leq j \\leq \\infty} as claimed. Hence joint convergence is not proved which will be problematic for the reasons explained earlier.\n\n\n(ii) Lee et al. type approach (A.4.2)\n\nThis type of approach follows the technique used by Lee et al. (2018), \"Deep Neural Networks as Gaussian Processes\".\n\nApplication of the weak law of large numbers (wLLN): As mentioned before, convergence in probability is only possible between random variables on the same underlying space. This is usually not a problem when wLLN is applied as the random variables converge to a constant random variable. Because every constant random variable generates the trivial sigma-algebra, it is measurable for any underlying probability space and thus convergence in probability is well-defined. The situation here is more complicated because the target is constant only conditionally on the previous layer, i.e. is not constant. As a side note, even the conditioning is only well-defined if all random variables live on the same space (conditioning on a random variable is technically conditioning on the sub-sigma-algebra it generates on the shared space).\n\nAssuming the problem with all K^{l, t} (t denotes the dependence on network width), for all l \\in {1, ... L} and t \\in {1, 2, 3, ...}, being on the same underlying probability space is solved, the next point is application of the wLLN itself. You claim \"we can apply the law of large numbers and conclude that [Eq. (6)]\" (p.4) which is not entirely correct here. Focusing on the application when the sizes of all the previous layers are held fixed, the two conditions that have to be checked here are: (i) the conditional expectation of the iid summands in Eq. (3) is finite; (ii) the sequence of iid variables is fixed. Please provide an explicit proof of (i). Regarding (ii), I am specifically concerned with the fact that with changing t (and thus network widths), the sequence of random variables changes (because the previous K^{l-1,t} matrix changes) which means that completely different size of the current layer may be necessary to get sufficiently close to the target (which has itself changed with t). In other words, instead of having a fixed infinite sequence of iid random variables, you currently have a sequence of growing finite sets of random variables which are iid only within the finite sets, but not between members of the sequence (different t). The direct implication is that this type of proof is not applicable to the \"simultaneous limit\" case as claimed in the main text (Section 2.2 says all proofs are equivalent and lead to Eq. (10) which explicitly takes the simultaneous limit), since the application would require some form of uniform convergence in probability akin to (A.4.3). I think that the approach taken in (A.4.3) is a correct way to address this issue and would thus recommend focusing on (A.4.3) and leaving (A.4.2) out. The appendix seems to acknowledge that (A.4.2) does not work for the \"simultaneous limit\" - please adapt the main text accordingly.\n\nA note on convergence in probability: In Eq. (3), the focus is on convergence in probability of individual entries of the K matrices. This in general does not imply convergence of all entries jointly. However, the type of convergence studied here is convergence to a constant random variable which is fortunate because simultaneous convergence of all entries in probability can be obtained for free in this case (thanks to having a **finite** number of entries of K). I think it might be potentially beneficial for the reader if this was explicitly stated as a footnote with an appropriate reference included.\n\nA note on marginal vs joint probability: As you say above Eq. (23), you are only proving convergence of a single filter marginally, instead of the full sequence {z_j^L}_{1 \\leq j \\leq \\infty} jointly. Convergence of the marginals does not imply convergence of the joint, which will be problematic for the reasons explained earlier.\n\n\n**Approaches for BNNs (\"simultaneous limit\")**\n\n(iii) The proof in (A.4.3)\n\nMy biggest concern about this approach is that it only establishes convergence of a single filter marginally, instead of the full sequence {z_j^L}_{1 \\leq j \\leq \\infty} jointly. Convergence of the marginals does not imply convergence of the joint, which will be problematic for the reasons explained earlier.\n\nOther comments:\n\n- (p.17) You say \"Using Theorem A.1 and the arguments in the above section, it is not difficult to see that a sufficient condition is that the empirical covariance converges in probability to the analytic covariance\".\n\t- Can you please provide more detail as it is unclear what exactly do you have in mind?\n\t- I will be assuming from now on that you show that a particular combination of the Portmanteau theorem and convergence of K^L in probability to get pointwise convergence of the characteristic function is sufficient.\n\n- (p.18) Condition on activation function: The class \\Omega(R) is dependent on the considered input set X through the constant R. This seems slightly cumbersome as it would be desirable to know whether a particular activation function can be used without any reference to the data. It would be nice (but not necessary) if you can derive a condition on \\phi which would not rely on the constant R but allows ReLU.\n\n- (p.19, Eq. 48) I see where Eq. (48) is coming from, i.e. from Eq. (44) and the assumption of \\bar{\\varepsilon} ball around A(K_\\infty^l) being in PSD(R), but it would be nicer if you could be a bit more verbose here and also write out the bound explicitly (caveat: I did not check if the definition of \\bar{\\varepsilon} matches up but assume a potential modification would not affect the proof in a significant way).\n\n- (p.19) The second part of the proof is a little confusing, especially after Eq. (49) - please be more verbose here. For example, just after Eq. (49), it is said that because the two random variables have the same distribution, property (3) of \\Omega(R)'s definition can be applied. However the two random variables are not identical and importantly are not constructed on the same underlying probability space. Property (3) is a statement about the the set of random variables {T_n (Sigma)}_{Sigma \\in PSD_2(R)} and not about the different 2x2 submatrices of K^{l+1}, but it needs to be applied to the latter. When this is clarified, the next point that could be made clearer is in the following sentence where changing t will affect the 2x2 submatrices of K^{l+1,t} as well as the bound through U(t) and V(t); it is not immediately obvious that the proof goes through as claimed so please be a bit more verbose.\n\n\n****Typos and other minor remarks****\n\n- (p.2, top) \"hidden layers go to infinity uniformly\": The use of word uniformly is non-standard in this context. Please clarify.\n\n- (p.3, Eq. 2) Using x for both inputs and post-activations is slightly confusing.\n\n- (p.4, Eq. 5) Should v_\\beta multiply \\sigma_\\beta^2 ?\n\n- (p. 4) The summands in Equation (3) are iid -> \"conditionally iid\" (please also specify the conditioning variables/sigma-algebra).\n\n- (p.4, Eq. 4) Eq. (4) is slightly confusing given you mention that K is a 4D object on the previous page.\n\t- I only understood K is \"flattened\" into |X|d x |X|d matrix when I reached (A.4.3) - this should be stated in main text as otherwise the above confusion arises.\n\n- (p.5, 3 and 3.1) The introduction of \"curly\" K is slighlty confusing. Please provide more detail when introducing the notation, e.g. state in what space the object lives.\n\n- (p.5, before Eq. (11)) Is R^{n^(l+1)} the right space for vec(z^L) ? It seems that the meaning of z changes here as compared to the definition in Eq. (2). If z is still defined as in Eq. (2), how exactly is the vec operator defined here? Please clarify.\n\n- (p.16, A.4.2) \"law of large number\" -> \"weak law of large numbers\"\n\n- (p.17) T_n is technically not a function from PSD_2 only but also from some underlying probability space into a measurable space (i.e. can be viewed as a random variable from the product space of PSD_2 and some other measurable space).\n\n- (p.18, Eq. 38) Missing dot at the end. Also the K matrix either should or shouldn't have the superscript \"l\" (now mixed); it does have the superscript in Eq. (39) so probably \"should\".\n\n- (p.18, Eq. 39) Slightly confusing notation. Please clarify that both K and A(K) should have diagonal within the given range.\n\n- (p.18) \"squared integrable\" -> \"square integrable\" or \"square-integrable\"\n\n- (p.18) Last display before Eq. (43): second inequality can be replaced by equality?!\n\n- (p.19, Eq. 47) The absolute value should be sup norm.\n\n- (p.19, Eq. 49) LHS is a scalar, RHS a 2x2 matrix (typo).\n\n- (p.19, last sentence of the proof) It does not seem the inequalities need to be strict.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review ",
            "review": "This paper extends the recent results concerning GP equivalence of infinitely wide FC nets to the convolutional case. This paper is generally of a high quality (notwithstanding the lack of keys on figures) and provides insights to an important class of model. I recommend that this paper be accepted, but I think it could be improved in a few ways. \n\nFirstly, and rather mundanely: the figures. Fig 1 is not easy to read due to the density of plotting, and as there is no key it isn’t possible to tell what it shows. Figure 2 is rather is called a ‘graphical model’ but the variables (weights and biases) are not shown. It should be specified that this is the graphical model of the infinite limit, in which case the K variables should not be random. Also, the caption on this figure refers to variables that aren’t in the figure, and is grammatically incorrect (perhaps something like ‘the limit of an infinitely wide convolutional’ is missing?). Figure 3 has a caption which seems to be inconsistent with the coloring (for example green is center pixel in the text, but blue in the key). Figure 6 is also missing a key. In Figure 5, what does the tick symbol denote? Finally, the value some of Table 1 is questionable as so many entries are missing. For example, the Fashion-MNIST column has only two values, which seems to me of little use. [I would have given the paper a rating of 7 were it not for these issues]\n\nRegarding the presentation of the content, I found this paper generally easy to follow and the arguments sound. Here are few points:\n\nThere is an important distinction between finite width Bayesian-CNNs and the infinite limit, and this distinction is indeed made in the paper but not clearly enough in my view. I would anticipate that some readers might come away after a cursory reading thinking that Bayesian-CNNs are fundamentally worse than their parametric counterparts, but this is emphatically not the message of the paper. It seems that the infinite limit that is the cause of two problems. The first problem (or perhaps benefit) is that the infinite limit gives Gaussian inner layers, just as in the fully connected case. The second problem (and I’d say this is definitely a problem this time) is that the infinite limit loses the covariance between the pixels, at least with a fully connected final layer. I would recall [Matthews 2018, long version] section 7, which discusses that point that taking the infinite limit in the fully connected is actually potentially undesirable. To quote Matthews 2018, “MacKay (2002, p. 547) famously reflected on what is lost when taking the Gaussian process limit of a single hidden layer network, remarking that Gaussian processes will not learn hidden features”. Some discussion of this would enhance the presented paper, in my view. \n\nThe discussion of eq (7) could be made more clear. Eq (7) is only defined on K, and not in composition with A. It is important that the alpha dependency is preserved by the A operation, and while I suppose this is obvious I would welcome a bit more detail. It would help to demonstrate the application of the results of [Cho and Saul 2009] to the convolution case explicitly (i.e. for C o A), in my view. \n\nRegarding results, effort has clearly gone to keep the comparisons as fair as possible, but with these large datasets it is difficult to disentangle the many factors that might effect performance (as acknowledged on p9). It is a weakness of the paper that there is no toy example. An example demonstrating a situation which can only be solved with hierarchical features (e.g. features that are larger than the receptive field of a single layer) would be particularly interesting, as in this case I think the GP-CNN would fail, even with the average pooling, whereas the finite Bayesian-CNN would succeed (with a sufficiently accurate inference method).  \n\nIt would improve readability to stress the 1D notation in the main text rather than in a footnote. On first reading I missed this detail and was confused as I was trying to interpret everything as a 2D convolution. On reflection I think notation is used in the paper is good, but I think the generalization to 2D should be elevated to something more than the footnote. Perhaps a paragraph explaining how the 2D case works would be appropriate, especially as all the experiments are in 2D cases. \n\nSome further smaller points on specific [section, paragraph, line]s\n\n1,2,4 I think ‘easily’ is a bit of an overstatement. In this work the kernel is itself defined via a recursive convolutional operation, which doesn’t seem to me much more interpretable than the parametric convolution. At least the filters can be examined in parametric case, which isn’t the case here. I do agree with the sentiment that a function prior is better than an implicit weight prior, however.\n\n1,2,-1 This seems too vague to me, as at least to some extent, Matthew 2018 did indeed consider using NN-GPs to gain insight about equivalent NN models (e.g. section 5.3)\n\n1.1,:,: I find it very surprising that there are no references to Cho and Saul 2009 in this section (one does appear in 2.2.2, however). \n\n1.1,3,-2:-1 ‘Our work differs from all of these in that our GP corresponds exactly to a fully Bayesian CNN in the many channel limit’ I do not think this is completely true, as the deep convolution GP does correspond to an infinite limit of a Bayesian CNN, just not the same limit as the one taken in this paper. Similarly a DGP following the Danianou and Lawrence 2013 is an infinite limit of a NN, but one with bottlenecks between layers. It is important that readers appreciate that infinite limits can be taken in different ways, and the resulting models may be very different. This certain limit taken in this work has desirable computational properties, but arguably undesirable modelling implications.\n\n1.1,-1,-2 It should be made more clear here that the SGD trained models are non-Bayesian. \n\nFigure 3 The MC-CNN-GP appears to have performance that is nearly independent of the depth, even including 1 layer. Could this be explained?\n\n2.2,2,: The z^l variables are zero mean Gaussian with a fixed covariance, not delta functions, as I understand it. They are independent of each other due to the deterministic K^l, certainly, but they are not themselves deterministic. Could this be clarified? \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}