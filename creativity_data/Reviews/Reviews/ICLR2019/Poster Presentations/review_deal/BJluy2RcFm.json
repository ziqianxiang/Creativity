{
    "Decision": {
        "metareview": "AR1 is concerned about whether higher-order interactions are modeled explicitly and if pi-SGD convergence conditions can be easily satisfied. AR2 is concerned that basic JP has been conceptually discussed in the literature and \\pi-SGD is not novel because it was realized by Hamilton et al. (2017) and Moore & Neville (2017). However, the authors provide some theoretical analysis for this setting in contrast to prior works. AR1 is also concerned that the effect of higher-order information has not been 'disentangled' experimentally from order invariance. AR4 is concerned about  poor performance of higher order Janossy pooling compared to k =1 case and asks about the number of hyper-parameters. The authors showed a harder task of computing the variance of a sequence of numbers in response.\n\nOn balance, despite justified concerns of AR2 about novelty and AR1 about experimental verification, the work appears to tackle an interesting topic.  Reviewers find the problem interesting and see some hope in the proposed solutions. On balance, AC recommends this paper to be accepted at ICLR. The authors are asked to update manuscript to reflect honestly weaknesses as expressed by reviewers, e.g. issue with effects of 'higher-order information' and 'disentangled' from order invariance.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Interesting take on permutation invariances."
    },
    "Reviews": [
        {
            "title": "Emergency review for Janossy Pooling",
            "review": "I have found the ideas proposed in the paper very insightful and interesting. The paper, in general, is written very well and is accessible.  My most important concern is \n\n The whole development seems not as effective as k =1 in Table.2 (BTW, there is a typo there). One wonders, why for k =2, k =1 is not included? That is, can the formulation be changed in a way that \\downarrow operator represents l \\in {1 \\cdots k} projections?  In the end, the method creates k tuples and feed them through specific fs so why not having smaller tuples?\n\nThe rest of my review below hopefully can help improving the paper;\n\n\n- Is there any reason as to why higher order Janossy poolings do not perform as good as k =1 for the sum experiment? \n\n- Can you report the number of parameters for the developments (Janossy -k)? Some examples according to the experiments help.\n\n- I am a bit lost to grasp the paragraph below Eq.4, can you rephrase it and possibly provide references?\n\n- When it comes to testing, how do you use Eq.13? Do you sample a few permutation and compute 13? If yes, how many in practice? \n\n- In preposition 2.1,  n seems confusing, why not |h|\n\n- In P6, x_i is a sequence. this needs to be mentioned \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A few of comments and request for clarifications",
            "review": "In this paper, the authors presented a new pooling method called Janossy Pooling (JP), which is designed to better capture high-order information by addressing two limitations of existing works - fixed pooling function and fixed-size inputs. The studied problem is important and the motivation is clear, where the inputs are sets of objects such as values or vectors and how we can learn a good aggregation function to maximally preserve the information in the original sequence. The authors attacked this problem by firstly formally formulating this problem and introducing a general approach as well as a few of approximation methods to realize it in practice. They also discussed the connections of this work and some existing works such as deep set, which I found is quite useful. \n\nIn general, JP was proposed to learn permutation-invariant function for aggregating the information of the input sequence. The basic idea of JP is to simply take all generated order of sequences from the original sequence input, which however I found is not new since it has been conceptually discussed already in the literature.  Since this approach is computationally prohibitive, there are several ways of approximations to approach the solution. As the authors are aware of the existing works in the literature, these approaches were discussed before either in the same context or in some particular learning tasks. From this perspective, the proposed solutions are not novel either. \n\nThe experimental results are particularly weak. It is little interesting on the first toy problem and the results on graph embedding are not promising. In Table 2, it is clearly shown that the LSTM aggregation functions on the randomly sampled sequences are really beating the simple mean aggregation function. I think the authors need much more experiments to demonstrate why we need LSTM based pooling for realizing JP in terms of both the final accuracy and computational cost. \n\n-------------------------------------------------\nAfter reading the authors' rebuttals, they have addressed part of my concerns but I still think the current form is not below the acceptance threshold due to its weak experimental results and unclear technical details. \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting demonstration that standard pooling methods are insufficiently flexible",
            "review": "I really enjoyed this paper. It takes an idea which at first glance seems to be obviously bad (if you want permutation invariance, build a model that considers all permutations) and uses it to make the important point that the universal approximation results contained in Deep Sets [Zaheer et al. 2017] are not the last word on pooling. Janossy Pooling is intractable for most problems of interest (because it sums over all n! permutations of the input set) so the authors suggest 3 tractable alternatives: canonical orderings, k-ary dependencies and SGD / sampling-based approaches. Only the latter two are explored in detail, so I’ll focus on them:\n\nK-ary dependencies\nFunctions that are restricted to k-ary dependencies in Janossy Pooling require summing over only |h|! / (|h| - k)! terms - that is they sum over the permutations of subsets of h of length k. In the experimental section, the authors show that this recovers some of the performances lost by using sum / mean pooling (as in Deep Sets), but this suggests the natural question: is it the fact that you’re explicitly modelling higher-order interactions that improves performance? Or is it that you’re doing Janossy pooling over the higher order interactions (i.e. summing over permutations of non-invariant functions)? \n\nThese two effects could be separated by comparing to invariant models that allow higher order interactions. E.g. you could compare against Santoro et al. [2017] who explicitly model pairwise interactions (or similarly any of the graph convolutional models [Kipf and Welling 2016, Hamilton et al 2017, etc.] with a fully connected graph would do the same); similarly Hartford, et al. [2018] allow for k-wise interactions by extending Deep Sets to exchangeable tensors - the permutation invariant analog of k-ary Janossy Pooling. All of these approaches model k-wise interactions through sum-pooling over permutation invariant functions so this lets you address the question - is it the permutation invariance that’s the problem (necessitating k-ary Janossy pooling) or is it the lack of higher-order interaction terms? \n\nSGD approaches:\nI think that the point that the sampling-based approaches are bias with respect to the Janossy sum is important to make and I liked the discussion around it, but I don’t follow the relevance of Proposition 2.2? I see that it gives conditions under which we can expect \\pi-SGD to converge, but we aren’t provided with any guidance about how likely those conditions are to be satisfied? Furthermore - these conditions don’t seem to be specific to \\pi-SGD - any SGD algorithm with “slightly biased” gradients that satisfy these conditions would converge. The regularization idea is interesting, but it isn’t evaluated so we’re left with theory that doesn’t provide guidance and isn’t evaluated.\n\nSummary:\nThere are two ways to read this paper:\n 1. Janossy pooling as a framework & proposed pooling approach implemented in one of the two ways discussed above.\n 2. Janossy pooling as an intractable upper bound on what we might want from a pooling method (with approximations in the form of the LSTM approaches) and a demonstration that our current invariant pooling methods are insufficient.\n\nI liked the paper based on reading (2). Janossy pooling clearly demonstrates limitations to sum / mean pooling which is widely used in practice which shows the need for better alternatives and it is on this basis that I’m arguing for it’s acceptance. My view is that the experimental section is too limited to support reading (1) which asserts that k-ary pooling or LSTM + sampling approaches are the right solution to this problem. \n\n[Zaheer et al. 2017] - Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and\nAlexander Smola. Deep Sets\n[Santoro et al. 2017] - Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Tim Lillicrap. A simple neural network module for relational reasoning.\n[Kipf and Welling 2016] - Thomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional Net- works\n[Hamilton et al 2017] - William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive Representation Learning on Large Graphs\n[Hartford, et al. 2018] - Jason S. Hartford, Devon R. Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. Deep models of interactions across sets",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}