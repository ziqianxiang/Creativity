{
    "Decision": {
        "metareview": "Strengths: Strong results on future frame video prediction using a 3D convolutional network. Use of future video prediction to jointly learn auxiliary tasks shown to to increase performance. Good ablation study.\n\nWeaknesses: Comparisons with older action recognition methods. Some concerns about novelty, the main contribution is the E3D-LSTM architecture, which R1 characterized as an LSTM with an extra gate and attention mechanism. \n\nContention: Authors point to novelty in 3D convolutions inside the RNN.\n\nConsensus: All reviewers give a final score of 7- well done experiments helped address concerns around novelty. Easy to recommend acceptance given the agreement.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Well executed exploration of a 3D CNN LSTM method"
    },
    "Reviews": [
        {
            "title": "The authors propose a futurue video prediction model based on recurrent 3D-CNNs and propose a novel memory mechanism (Eidetic Memory) to capture long term relationships inside the recurrent layer itself. They obtain surpass the state of the art on two commonly used, (relatively) simple benchmark video prediction datasets. They further apply their model to early action recognition, performing an ablation study to evaluate the strengths of each model building block.",
            "review": "AFTER REBUTTAL:\n\nThis is an overall good work, and I do think proves its point. The results on the TaxiBJ dataset (not TatxtBJ, please correct the name in the paper) are compelling, and the concerns regarding some of the text explainations have been corrected.\n\n-----\n\nThe proposed model uses a 3D-CNN with a new kind of 3D-conv. recurrent layer named E3D-LSTM, an extension of 3D-RCNN layers where the recall mechanism is extended by using an attentional mechanism, allowing it to update the recurrent state not only based on the previous state, but on a mixture of previous states from all previous time steps.\n\nPros:\nThe new approach displays outstanding results for future video prediction. Firstly, it obtains better results in short term predictions thanks to the 3D-Convolutional topology. Secondly, the recall mechanism is shown to be more stable over time: The prediction accuracy is sustained over longer preiods of time (longer prediction sequences) with a much smaller degradation. Regarding early action recognition, the use of future video prediction as a jointly learned auxiliary task is shown to significantly increase the prediction accuracy. The ablation study is compelling.\n\nCons:\nThe model does not compare against other methods regarding early action recognition. Since this is a novel field of study in computer vision, and not too much work exists on the subject, it is understandable. Also, it is not the main focus of the work.\n\nIn the introduction, the authors state that they account for uncertainty by better modelling the temporal sequence. Please, remove or rephrase this part. Uncertainty in video prediction is not due to the lack of modelling ability, but due to the inherent uncertainty of the task. In real world scenarios (eg. the KTH dataset used here) there is a continuous space of possible futures. In the case of variational models, this is captured as a distribution from which to sample. Adversarial models collapse this space into a single future in order to create more realistic-looking predictions. I don't believe your approach should necessarily model that space (after all, the novelty is on better modelling the sequence itself, not the possible futures, and the model can be easily extended to do so, either through GANs or VAEs), but it is important to not mislead the reader.\n\nIt would have been interesting to analyse the work on more complex settings, such as UCF101. While KTH is already a real-world dataset, its variability is very limited: A small set of backgrounds and actions, performed by a small group of individuals.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "well-written, well-experimented paper with limited novelty",
            "review": "The paper proposes a spatiotemporal modeling of videos based on two currently available spatiotemporal modeling paradigms: RNNs and 3D convolutions. The main idea of this paper is to get the best world of both in a unified way. The method first encodes a sequence of frames using 3D-conv to capture short-term motion patterns, passes it to a specific type of LSTM (E3D-LSTM) which accepts spatiotemporal feature maps as input. E3D-LSTM captures long-term dependencies using an attention mechanism. Finally, there are 3D-conv based decoders which receive the output of E3D-LSTM and generate future frames. The message of the paper, I believe, is that 3D-conv and RNNs can be integrated to perform short and long predictions. They show in the experiments how the model can remember far past for reasoning and prediction.\nThe nice point of the method is that it is heavily investigated through experiments. It's evaluated on two datasets, with ablation studies on both. Moreover, the paper is well-written and clear. technically, the paper seems correct.\nHowever, my only big concern is about the limited novelty of the method. E3D-LSTM is the core of the novelty, which is basically an LSTM with extra gate, and attention mechanism.  \n\nother comments:\n- As the method by essence is a spatiotemporal learning model, why the method is not evaluated on full-length videos of the something-something dataset for classical action classification task, in order to compare it with the full architecture of I3D, or S3D?\n\n- While the paper discusses self-supervised learning, I would suggest showing its benefit on online action recognition task. One without frame-prediction loss and one with. \n\n- the something-something dataset has 174 classes, how was the process of selecting 41 classes out of it?",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice experiments although it lacks a bit of novelty",
            "review": "# 1. Summary\nThis paper presents a model for future video prediction, which integrates 3D convolutions into RNNs. The internal operations of the RNN are modified by adding historical records controlled via a gate-controlled self-attention module. The authors show that the model is effective also for other tasks such as early activity recognition.\n\nStrengths:\n* Nice extensive experimentation on video prediction and early activity recognition tasks and comparison with recent papers\n* Each choice in the model definition are motivated, although some clarity is still missing (see below)\n\nWeaknesses:\n* Novelty: the proposed model is a small extension of a previous work (Wang et al., 2017) \n\n\n# 2. Clarity and Motivation\nIn general, the paper is clear and general motivation makes sense, however some points need to be improved with further discussion and motivation:\n\nA) Page 2 “Unlike the conventional memory transition function, it learns the size of temporal interactions. For longer sequences, this allows attending to distant states containing salient information”: This is not obvious. Can the authors add more details and motivate these two sentences? How is long-term relations are learned given Eq. 1? \nB) Page 5 “These two terms are respectively designed for short-term and long-term video modeling”: How do you make sure that Recall(.) does not focus on the short-term modeling instead? Not clear why this should model long-term relations.\nC) Page 5 and Eq 1: motivation why layer norm is required when defining C_t^k is not clear\nD) What if the Recall is instead modeled as attention? The idea is to consider only C_{1:t-1}^k (not consider R_t) and have an attentional model that learn what to recall based only on C. Also, why does Recall need to depend on R_t?\nE) Page 5 “to minimize the l1 + l2 loss over every pixel in the frame”: this sentence is not clear. How does it relate to Eq. 2?\n\n\n# 3. Novelty\nNovelty is the major concern of this paper. Although the introduced new concepts and ideas are interesting, the work seems to be an extension of ST-LSTM and PredRNN where Eq 1 is slightly modified by introducing Recall. \nIn addition the existing relation between the proposed model and ST-LSTM is not clearly state. Page 2, first paragraph: here the authors should state that model is and extension of ST-LSTM and highlight what are the difference and advantage of the new model.\n\n\n# 4. Significance of the work\nThis paper deals with an interesting and challenging topic (video prediction) as well as it shows some results on the early activity recognition task. These are definitively nice problem which are far to be solved. From the application perspective this work is significant, however from the methodological perspective it lacks a bit of significance because of the novelty issues highlighted above.\n\n\n# 5. Experimentation\nThe experiments are robust with nice comparisons with recent methods and ablation study motivating the different components of the model (Table 1 and 2). Some suggested improvements:\n\nA) Page 7 “Seq 1 and Seq 2 are completely irrelevant, and ahead of them, another sub-sequence called prior context is given as the input, which is exactly the same as Seq 2”: The COPY task is a bit unclear and need to be better explained. Why are Seq. 1 and 2 irrelevant? I would suggest to rephrase this part.\nB) Sec. 4.2, “Dataset and setup”: which architecture has been used here?\nC) Sec. 4.3, “Hyper-parameters and Baselines“: the something-something dataset is more realising that the other two “toy” dataset. Why did the authors choose to train a 2 layers 3D-CNN encoders, instead of using existing pretrained 3D CNNs? I would suspect that the results can improve quite a bit.\n\n\n# 6. Others\n* The term “self-supervised auxiliary learning” is introduced in the abstract, but at this point it’s meaning is not clear. I’d suggest to either remove it or explain its meaning.\n* Figure 1(a): inconsistent notation with 2b. Also add citation (Wang et al., 2017) since it ie the same model of that paper\n\n-------\n# Post-discussion\nI increased my rating: even if novelty is not high, the results support the incremental ideas proposed by the authors.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}