{
    "Decision": {
        "metareview": "Strengths:  This paper gives a detailed treatment of the connections between rate distortion theory and variational lower bounds, culminating in a practical diagnostic tool.  The paper is well-written.\n\nWeaknesses:  Many of the theoretical results existed in older work.\n\nPoints of contention:  Most of the discussion was about the novelty of the lower bound.\n\nConsensus:  R3 and R2 both appear to recommend acceptance (R2 in a comment), and have both clearly given the paper detailed thought.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "Skirts close to previous work but ultimately novel"
    },
    "Reviews": [
        {
            "title": "The proposed criterion has has already been examined in some prior works while  little is discussed on the related works.",
            "review": "This paper considers the optimization of the prior in the latent variable model and the selection of the likelihood function. The authors propose criteria for these problems based on a lower-bound on the negative log-likelihood, which is derived from rate-distortion theory.\n\nThere are some interesting points in the derivation of the proposed quantities and how to compute them while the main criterion c(z) has already been examined in some prior works. Although the results of experiments are promising, they are somewhat weak enough to demonstrate the usefulness of the proposed quantities.\n\n- The note right after Eq.(7) is unclear. It would be nicer to discuss more clearly the property of c(z) about overfitting.\n\n- The derived quantity c(z) in Eq.(6), appearing in the optimality condition (the equation following Eq.(13)), has been pointed out since early times, e.g. in\nLindsay, B. G. (1983). The geometry of mixture likelihoods: A general theory. The Annals of Statistics, 11(1), 86–94.\nIt was used in the machine learning community too:\nNowozin, S., Bakir, G. (2008). A decoupled approach to exemplar-based unsupervised learning. In Proc. ICML.\n, and its connection to rate-distortion theory was pointed out:\nWatanabe, K., Ikeda, S. (2014). Entropic risk minimization for nonparametric estimation of mixing distributions. Machine Learning, 99(1), 19–136.\nHowever, little is discussed on these related works.  \n\n- Section 2: Shannon's rate-distortion theory is formulated by a general source distribution of X. It would be better to mention that the authors consider the empirical distribution of the data sample as the source distribution.\n\n- The results of experiments only show the potential of glossy statistics in some variational auto-encoder models. Isn't it possible or better to demonstrate its practicality more concretely using small toy models? \n\nPros:\n- nice connection between the optimization of the prior (or likelihood function) and rate-distortion theory\n\nCons: \n- lack of discussion on important related works\n- weakness of the experiments\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Novel theoretical analysis, establishing a very formal link between latent-variable models and rate-distortion theory",
            "review": "(Please find my response to the rebuttal and updated version in a comment below)\nThe paper analyses latent-variable modeling from a rate-distortion point-of-view in a novel and interesting fashion, highlighting important fundamental connections. In particular, the paper presents a novel theorem (inspired by how the rate-distortion function is computed) that gives a lower bound on the negative log likelihood. This lower bound allows to quantify by how much a latent-variable model could be improved by either modifying the prior or the likelihood function. The latter is important, since the paper shows a duality between improving one while keeping the other fixed and vice versa. Finally, the paper derives a practical implementation/approximation (founded on solid theoretical analysis) of quantifying the improvement potential of a latent-variable model. These, so called “glossy statistics” are quantitatively analyzed in a set of experiments with different variational autoencoder architectures (various likelihood models and priors) on a number of datasets.\n\nThe main contribution of this paper is to provide novel proofs and theoretical analysis that connect latent-variable modeling with rate-distortion on a very fundamental level. While similar attempts have been reported in the recent literature (perhaps a bit more focused on the empirical aspects), the analysis and results in the paper follow a very fundamental treatment of rate-distortion theory and in particular of computation of the rate-distortion function. The central idea underlying rate-distortion, i.e. lossy compression by discarding irrelevant information, seems very suitable as a guiding principle for representation learning. In particular, learning representations that generalize well is essentially another instance of a lossy compression problem. The paper thus addresses an important and timely topic which should be of broad interest to the representation learning community. The paper is well written and mathematically rigorous. I have checked most parts of the proofs, though there still is a chance that I missed something. I am not entirely convinced by the practical impact of the experimental section of the paper (though the experiments are beyond toy-level and I do not doubt the results), but I also believe that this is not the main contribution of the paper, which is rather laying the mathematical groundwork for future work. I vote and argue for accepting the paper for presentation at the conference. My criticism below is aimed at giving some pointers for potentially improving the paper.\n\n1) As the paper acknowledges, there is a risk of overfitting when improving likelihood functions under fixed priors (and vice versa). While the glossy statistics certainly allow making approximate statements of whether the model can be improved further or not, there is no “threshold value” or other guideline that would indicate a modeling expert that they are entering an over-fitting regime if the model-class is further enriched. Therefore, I am not sure about the practical impact of the experiments: the glossy statistics seem to be indicative of the margin for improvements in the negative log-likelihood - but whether all of these improvements are really desirable is unclear. To test this, one might resort to tasks other than generative modeling, such that models that overfit can easily be “spotted” (by degrading test-set performance).\n\n2) Rate-Distortion can be “made more robust” against overfitting by different choices of \\alpha (essentially limiting channel capacity). Maybe I am missing something, but shouldn’t the \\alpha carry over into the computation of the ratios for c(z)? Was it just assumed to be 1? The same question for Theorem 2 and the equation just above Theorem 2 - does the alpha drop (is it absorbed into the likelihood) or was it set to one? It might be interesting to see how the glossy statistics behave if \\alpha is considered a hyper-parameter of the model, e.g. under “low capacity” do the glossy statistics “flatten out” very early?\n\n3) I would have been excited to see how the glossy statistics evolve during training of a model - it would be interesting to show that the statistics initially predict a large margin of improvement that reduces and slowly flattens out as training converges.\n\n4) In the paragraph after Eq. 14: the argument hinges on the possibility of having an invertible (and continuously differentiable) g(z). To me it is not straightforward that a neural network would necessarily implement such a function (particularly the invertibility might be problematic). Is this just a technical condition required for the formal statement, or do you think that this issue could become problematic in practice as well such that the duality between improving prior and likelihood does not hold any longer?\n\nMinor:\n5) I think the Alemi et al. reference (first reference) has been published under a different name (Fixing a Broken ELBO) at this years’ ICML.\n\n6) Consider calling the quantity l(x|z) below Eq. 1 “the likelihood of the latent variable given the data” (since the data is given, even though the data is not in the conditional, which is why it is a likelihood function).\n\n7) Rather than using “the KL divergence between”, use “the KL divergence from … to” which nicely reflects its asymmetry. \n\n8) Page 4, last Equation: square brackets for E_X missing\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This is an interesting paper that studies the latent variable modeling from an information theoretic perspective. Specifically, the authors argue that the rate-distortion theory for lossy compression provides a natural toolkit for studying latent variable models, and they propose a lower bound (also a gap function) that could be used to assess the goodness of data fitting given a pair of prior distribution over latent factor and a likelihood function. Overall the paper is very well-written, clear to follow, and the authors did a great job in not overclaiming their results. \n\nSeveral questions follow: \n1.  In Eq. (3), why the R.H.S. is an upper bound of the L.H.S.? Under the assumption of (1) should this be equal? \n2.  In section 2, \"must use at use\" -> \"must use at least\". \n3.  Since the mutual information is convex in the conditional distribution Q(Z|X), when considering the Lagrangian, since \\alpha is constrained to be positive, should the sign before \\alpha be positive instead of negative? \n4.  In section 3.3, \"An very common\" -> \"A very common\". \n\nTo me the most interesting result in this paper is in Thm. 1, Eq. (9), where the authors show that the optimization over the prior in latent variable modeling is exactly equivalent to the optimization of the channel in rate-distortion theory. Following this line the authors propose a gap function that could be used to assess the goodness of a model. One drawback of the current framework is that it only links the optimization of the prior, rather than the likelihood function, to rate-distortion theory, while in practice it is usually the other way around. Although the authors argue in section 3.3 that similar conclusion could be achieved for a family of likelihood functions, the analysis is only possible under the very restrictive (in my personal view) assumption that relies on the existence of a smooth and invertible mapping. This assumption usually does not hold in practice, e.g., the ReLU network, and as a result the analysis here is only of theoretical interest. \n\nThe experimental validation basically shows the usefulness of the proposed gap function in assessing the goodness of model fitting in latent variable models. It would be great if there are more direct use of the proposed lower bound, but I appreciate the novelty in this paper on bridging the two subfields. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}