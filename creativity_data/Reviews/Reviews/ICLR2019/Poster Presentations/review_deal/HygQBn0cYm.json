{
    "Decision": {
        "metareview": "Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers' comments into consideration to improve your submission for the camera ready.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Paper decision"
    },
    "Reviews": [
        {
            "title": "An Ok paper that combines  dropout methods with learning policy using observational data. ",
            "review": "- Does the paper present substantively new ideas or explore an under explored or highly novel question? \n\nSomewhat, the paper combines two popular existing approaches (Imitation Learning,  Model Based Control and Uncertainty Quantification using Dropout).  The novelty is in  combining pre-existing ideas. \n\n- Does the results substantively advance the state of the art? \n\nNo, the compared methods are not state-of-the-art.\n\n- Will a substantial fraction of the ICLR attendees be interested in reading this paper? \n\nYes. I think that the topics of this paper would be very interesting to ICLR attendees. \n\n-Quality:  \n\nUnclear motivation to penalize prediction uncertainty to make the predicted states stay in the training data.  Also, in some cases references to existing work that includes real robotic systems is out of context at minimum. So yes there are similarities between this paper and existing works  on  learning control for robotics systems using imitation learning, model based control and uncertainty aware cost function. However there is a profound difference in terms of working in simulation and working with a real system for which model and environment uncertainty is a very big issue. There are different challenges in working with a real uncertain system which you will have to actuate,  and working with set of images for making predictions in simulation.   \n\n \n\n-Clarity: \n\nEasy to read. Experimental evaluation is clearly presented. \n\n-Originality: \n\nSimilar uncertainty penalty was used in other paper (Kahn et al. 2017).  Therefore the originality is in some sense reduced.\n\n- Would I send this paper to one of my colleagues to read?\n\nYes I would definitely send this paper to my colleagues. \n\n- General Comment: \n\nDropout can be used to represent the uncertainty/covariance of the neural network model. The epistemic uncertainty, coming from the lack of data, can be gained through Monte Carlo sampling of the dropout-masked model during prediction. However, this type of uncertainty can only decrease by adding more explored data to current data set. Without any addition of data, the  variance reduction, which results  by penalizing the high variance during training, might indicate over-fitting to the current training data. As the penalty forces the model to predict states only in the training dataset, it is unclear how this shows better test-time performance. The output of the policy network will simply be biased towards the training set as a result of the uncertainty cost. More theoretical explanation is needed or perhaps some intuition.  \n\nThis observation is also related to the fact that the model based controller used  is essentially a  risk sensitive controller. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review for \"Model-Predictive Policy Learning with Uncertainty Regularization for Driving in Dense Traffic\"",
            "review": "The paper addresses the difficulty of covariate shift in model-based reinforcement learning. Here, the distribution over trajectories during is significantly different for the behaviour or data-collecting policy and the target or optimised policy. As a mean to address this, the authors propose to add an uncertainty term to the cost, which is realised by the trace of the covariance of the outputs of a MC dropout forward model. The method is applied to driving in dense traffic, where even single wrong actions can be catastrophic.\n\nI want to stress that the paper was a pleasure to read. It was extraordinarily straightfoward to follow, because the text was well aligned with the necessary equations.\n\nThe introduction and related work seem complete to me, with two exceptions:\n\n- Depeweg, S., Hernandez-Lobato, J. M., Doshi-Velez, F., & Udluft, S. \n  (2018, July). Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning. In *International Conference on Machine Learning* (pp. 1192-1201).\n- Thomas, Philip S. *Safe reinforcement learning*. Diss. University of Massachusetts Libraries, 2015.\n\nThe work by Depeweg et al addresses quite the same question as the authors of this work, but with a broader scope (i.e. not limited to traffic) but very much the same machinery. There are some important theoretical insights in this work and the connection to this submission should be drawn. In particular, the proposed method needs to be either compared to this work or it needs to be clarified why it is not applicable.\n\nThe latter appears to be of less significance in this context, but I found robust offline policy evaluation underrepresented in the related work. \n\nI wonder if there is a way for a neural network to \"hack\" the uncertainty cost. I suppose that the proposed approach is an approximation to some entropy term, and it would be informative to see how exactly. \n\nThe approach shown by Eq 1 appears to be an adhoc way of estimating whether the uncertainty resulting from an action is due to the data or the model. What happens if this approach is not taken?\n\nThe objective function of the forward model is only given in the appendix. I think it needs to be moved to the main text, especially because the sum-of-squares term indicates a homoskedastic Gaussian for a likelihood. This has implications for the uncertainty estimates (see point above).\n\nOverall, the separation of data uncertainty/risk vs model uncertainty is not done. This indicates that heterskedastic environments are candidats where the method can fail, and this limitation needs to be discussed or pointed out.\n\nFurther, the authors did not observe a benefit from using a stochastic forward model. Especially, if the prior instead of the approximate posterior is used. My point would be that, depending on the exact grapical model and the way the sampling is done to train the policy, it is actually mathematically *right* to sample from the prior. This is also how it is described in the last equation of section 2. \n\n## Summary\n\nOverall, I liked the paper and the way it was written. However, there are some shortcomings, such as the comparison to the work by Depeweg et al, which does a very similar thing. Also, justifying the used heuristics as approximations to a principled quantity would help. It appears that the question why and how stochastic forward models should be used requires further investigation.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "a good model-based RL attempt for autonomous driving, however, dataset is very limited ",
            "review": "Pros:\nThe paper formulates the driving policy problem as a model-based RL problem. Most related work on driving policy has been traditional robotics planning methods such as RRT or model-free RL such as policy gradient methods.\n\nThe policy is learned through unrolling a learned model of the environment dynamics over multiple time steps, and training a policy network to minimize a differentiable cost over this rolled-out trajectory.\n\nThe cost combine the objective the policy seeks to optimize (proximity to other cars) and an uncertainty cost representing the divergence from the states it is trained on.\n\nCons:\n\nThe model based RL formulation is pretty standard except that the paper has a additional model uncertainty cost.\n\nRealistically, the output of driving policy should be planning decision, i.e. the waypoints instead of steering angles and acceleration / deceleration commands. There does not seem to be a need to solve the control problem using learning since PID and iLQR has solved the control problem very well. \n\nThe paper did not seem to reach a conclusion on why stochastic forward model does not yield a clear improvement over the deterministic model. This may be due to the limitation of the dataset or the prediction horizon which seems to be 2 second. \n\nThe dataset is only 45 minutes which captured by a camera looking down a small section of the road. So the policies learned might only do lane following and occasionally doing collision avoidance. I would encourage the authors to look into more diverse dataset. See the paper DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents, CVPR 2017.\n\nOverall, the paper makes an interesting contribution: formulate the driving policy problem as a model-based RL problem. The techniques used are pretty standard. There are some insights in the experimental section. However, due to the limitation of the dataset, it is not clear how much the results can generalize to complex settings such as nudging around other cars, cutting in, pedestrian crossing, etc.\n\nResponse to rebuttal:\nIt is good to know that the authors have a new modified VAE posterior distribution for the stochastic model which can achieve significant gain over the deterministic model. Is this empirical and specific to this dataset? Without knowing the details, it is not clear how general this new stochastic model is.\n\nI agree that it is worthwhile to test the model using the 45 minute dataset. However, I still believe the dataset is very limiting and it is not clear how much the experimental results can apply to other large realistic datasets.\n\nMy rating stays the same.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}