{
    "Decision": {
        "metareview": "This paper proposes a method for unsupervised learning that uses a latent variable generative model for semi-supervised dependency parsing. The key learning method consists of making perturbations to the logits going into a parsing algorithm, to make it possible to sample within the variational auto-encoder framework. Significant gains are found through semi-supervised learning.\n\nThe largest reviewer concern was that the baselines were potentially not strong enough, as significantly better numbers have been reported in previous work, which may have a result of over-stating the perceived utility.\n\nOverall though it seems that the reviewers appreciated the novel solution to an important problem, and in general would like to see the paper accepted.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Novel, well-founded, and interesting method. Concerns about baseline"
    },
    "Reviews": [
        {
            "title": "I thought this was an excellent paper - very clear, an important problem, a useful set of techniques and results.",
            "review": "The paper describes a VAE-based approach to semi-supervised learning\nof dependency parsing. The encoder in the VAE is a neural edge-factored\nparser allowing inference using Eisner's dynamic programming algorithms.\nThe decoder generates sentences left-to-right, at each point conditioning\non head-modifier dependencies specified by the tree. A key technical \nstep is to develop a method for \"differentiable\" sampling/parsing,\nusing a modification of the dynamic program, and the Gumbel-max trick.\n\nI thought this was an excellent paper - very clear, an important \nproblem, a very useful set of techniques and results. I would strongly\nrecommend acceptance.\n\nSome comments:\n\n* I do wonder how well this approach would work with orders of magnitude\nmore unlabeled data. The amount of unlabeled data used is quite small.\n\n* Similarly, I wonder how well the approach works as the amount of\nunlabeled data is decreased (or increased, for that matter). It should\nbe possible to provide graphs showing this.\n\n* Are there natural generalizations to multi-lingual data, for example\nsettings where supervised data is only available for languages other\nthan the language of interest?\n\n* It would be interesting to see an analysis of accuracy improvements\non different dependency labels. The \"root\" case is in some sense just\none of the labels (nsubj, dobj, prep, etc.) that could be analyzed.\n\n* I wonder also if this method would be particularly helpful in \ndomain transfer, for example from Wall Street Journal text to\nWikipedia or Web data in general. The improvements could be more\ndramatic in this case - that kind of effect has been seen with \nELMO for example.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting application of VAEs",
            "review": "[Summary]\nThis paper proposes to do semi-supervised learning , via a generative model, of an arc-factored dependency parser by using  amortized variational inference.  The parse tree is the latent variable, the parser is the encoder that maps a sentence to a distribution over parse-trees, and the decoder is a generative model that maps a parse tree to a distribution over sentences.  \n\n[Pros]\nSemi-supervised learning for dependency parsing is both important and difficult and this paper presents a novel approach using variational auto-encoders. And the semi-supervised learning method in this paper gives a small but non-zero improvement over a reasonably strong baseline. \n\n[Cons]\n1. My main concern with this paper currently are the \"explanations\" provided in the paper which are quite hand-wavy. E.g. the authors state that using a KL term in semi-supervised learning is exactly opposite to the \"low density separation assumption\".  And therefore they set the KL term to be zero. One has to wonder that why is the \"low density separation assumption\" so critical for dependency parsing only? VAEs have been used with a prior for semi-supervised learning before, why didn't this assumption affect those models ? \n\nA better explanation will have been that since the authors first trained the parser in a supervised fashion, therefore their inference network already represents a \"good\" distribution over parses, even though this distribution is specified only upto sampling but not in a mathematically closed form. Finally, setting the KL divergence between the posterior of the inference network and the prior to be zero is the same as dynamically specifying the prior to be the same as the inference network's distribution.  \n\n2. A number of important details are missing in the submitted version of the paper which the authors addressed in their reply to my public comment.\n\n3. The current paper does not contain any comparison to self-training which is a natural baseline for this work. The authors replied to my comment saying that self-training requires a number of heuristics but it's not clear to me how much more difficult can these heuristics be than the tuning required for training their VAE.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Novel and nice method, but experiments are not strong enough",
            "review": "This paper proposed a variational autoencoder-based method for semi-supervised dependency parsing. Given an input sentence s, an LSTM-based encoder generates a sentence embedding z, and a NN of Kiperwasser & Goldberg (2016) generates a dependency structure T. Gradients over the tree encoder are approximated by (1) adding a perturbation matrix over the weight matrix and (2) relax dynamic programming-based parsing algorithm to a differentiable format. The decoder combines standard LSTM and Graph Convolutional Network to generate the input sentence from z and T. The authors evaluated the proposed method on three languages, using 10% of the original training data as labeled and the rest as unlabeled data.\n\nPros\n1. I like the idea of this sentence->tree->sentence autoencoder for semi-supervised parsing. The authors proposed a novel and nice way to tackle key challenges in gradient computation. VAE involves marginalization over all possible dependency trees, which is computationally infeasible, and the proposed method used a Gumbel-Max trick to approximate it. The tree inference procedure involves non-differentiable structured prediction, and the authors used a peaked-softmax method to address the issue. The whole model is fully differentiable and can be thus trained end to end.\n\n2. The direction of semi-supervised parsing is useful and promising, not only for resource-poor languages, but also for popular languages like English. A successful research on this direction could be potentially helpful for lots of future work.\n\nCons, and suggestions on experiments\nMy main concerns are around experiments. Overall I think they are not strong enough to demonstrate that this paper has sufficient contribution to semi-supervised parsing. Below are details.\n\n1. The current version only used 10% of original training data as labeled and the rest as unlabeled data. This makes the reported numbers way below existing state-of-the-art performance. For example, the SOTA UAS on English PTB has been >95%. Ideally, the authors should be able to train a competitive supervised parser on full training data (English or other languages), and get huge amount of unlabeled data from other sources (e.g. News) to further push up the performance. The current setting makes it hard to justify how useful the proposed method could be in practice.\n\n2. The best numbers from the proposed model is lower than baseline (Kipperwasser & Goldberg) on English, and only marginally better on Swedish. This probably means the supervised baseline is weak, and it's hard to tell if the gains from VAE will retain if applied to a stronger supervised.\n\n3. A performance curve with different amount of labeled and unlabeled data would be useful to better understand the impact of semi-supervised learning.\n\n4. What's the impact of perturbation? One could simply use T=Eisner(W) as approximation. Did you observe any significant benefits from sampling?\n\nOther questions\n1. What's the impact of keeping the tree constraint on dependencies during backpropagation?  Have you tried removing the tree constraint like previous work?\n\n2. Are sentence embedding and trees generated from two separate LSTM encoders? Are there any parameter sharing between the two?\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}