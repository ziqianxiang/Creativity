{
    "Decision": {
        "metareview": "This paper addresses a promising method for unpaired cross-domain image-to-image translation that can accommodate multi-instance images. It extends the previously proposed CycleGAN model by taking into account per-instance segmentation masks. All three reviewers and AC agree that performing such transformation in general is a hard problem when significant changes in shape or appearance of the object have to be made, and that the proposed approach is sound and shows promising results. As rightly acknowledged by R1 ‘The formulation is intuitive and well done!’\n\nThere are several potential weaknesses and suggestions to further strengthen this work: \n(1) R1 and R2 raised important concerns about the absence of baselines such as crop & attach simple baseline and CycleGAN+Seg. Pleased to report that the authors showed and discussed in their response some preliminary qualitative results regarding these baselines. In considering the author response and reviewer comments, the AC decided that the paper could be accepted given the comparison in the revised version, but the authors are strongly urged to include more results and evaluations on crop & attach baseline in the final revision if possible.\n(2) more quantitative results are needed for assessing the benefits of this approach (R3). The authors discussed in their response to R3 that more quantitative results such as the segmentation accuracy of the synthesized images are not possible since no ground-truth segmentation labels are available. This is true in general for unpaired image-to-image translation, however collecting annotations and performing such quantitative evaluation could have a substantial impact for assessing the significance of this work and can be seen as a recommendation for further improvement. \n(3) the proposed model performs translation for a pair of domains; extending the work to multi-domain translation like StarGAN by Choi et al 2018 or GANimation by Pumarola 2018 would strengthen the significance of the work. The authors discussed in their response to R3 that this is indeed possible. \n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Meta-Review"
    },
    "Reviews": [
        {
            "title": "well-written paper, nice method, somewhat limited results+evaluation",
            "review": "This paper does unpaired cross-domain translation of multi-instance images, proposing a method -- InstaGAN -- which builds on CycleGAN by taking into account instance information in the form of per-instance segmentation masks. \n\n=====================================\n\nPros:\n\nThe paper is well-written and easy to understand. The proposed method is novel, and does a good job of handling a type of information that previous methods couldn’t.\n\nThe motivation for each piece of the model and training objective is clearly explained in the context of the problem. Intuitively seems like a nice and elegant way to take advantage of the extra segmentation information available.\n\nThe results look pretty good and clearly compare favorably with CycleGAN and other baselines. The tested baselines seem like a fair comparison -- for example, the model capacity of the baseline is increased to compensate for the larger proposed model.\n\n=====================================\n\nCons / suggestions:\n\nThe results are somewhat limited in terms of the number of domains tested -- three pairs of categories (giraffe/sheep, pants/skirts, cup/bottle).  In a sense, this is somewhat understandable -- one wouldn’t necessarily expect the method to be able to translate between objects with different scale or that are never seen in the same contexts (e.g. cups and giraffes). However, it would still have been nice to see e.g. more pairs of animal classes to confirm that the category pairs aren’t the only ones where the method worked.\n\nRelatedly, it would have been interesting to see if a single model could be trained on multiple category pairs and benefit from information sharing between them.\n\nThe evaluation is primarily qualitative, with quantitative results limited to Appendix D showing a classification score. I think there could have been a few more interesting quantitative results, such as segmentation accuracy of the proposed images for the proposed masks, or reconstruction error. Visualizing some reconstruction pairs (i.e., x vs. Gyx(Gxy(x))) would have been interesting as well.\n\nI would have liked to see a more thorough ablation of parts of the model. For example, the L_idt piece of the loss enforcing that an image in the target domain (Y) remain identical after passing through the generator mapping X->Y. This loss term could have been included in the original CycleGAN as well (i.e. there is nothing about it that’s specific to having instance information) but it was not -- is it necessary?\n\n=====================================\n\nOverall, while the evaluation could have been more thorough and quantitative, this is a well-written paper that proposes an interesting, well-motivated, and novel method with good results.\n\n\n==========================================================================\n\nREVISION\n\nThe authors' additional results and responses have addressed most of my concerns, and I've raised my rating from 6 to 7.\n\n> We remark that the identity mapping loss L_idt is already used by the authors of the original CycleGAN (see Figure 9 of [2]). \n\nThanks, you're right, I didn't know this was part of the original CycleGAN. As a final suggestion, it would be good to mention in your method section that this loss component is used in the original CycleGAN for less knowledgeable readers (like me) as it's somewhat hard to find in the original paper (only used in some of their experiments and not mentioned as part of the \"main objective\").",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice formulation, good results! ",
            "review": "Post rebuttal: I am satisfied by the points mentioned by authors!\n\n----------------------------------------------------------------\nSummary: The paper proposes to add instance-aware segmentation masks for the problem of unpaired image-to-image translation. A new formulation is proposed to incorporate instance masks with an input image to generate a new target image and corresponding mask. The authors demonstrate it on multiple tasks, and show nice results for each of them.\n\nPros: \n\n1. The formulation is intuitive and well done!\n\n2. The idea of sequential mini-batch translation connects nicely to the old school of making images by layering. \n\n3. Nice qualitative analysis, and good results in comparison with Cycle-GAN (an obvious baseline for the formulation). I would make an observation that two domains for translation (such as sheep to giraffe, jeans to skirts etc) are thoughtfully selected because Cycle-GAN is somewhat bound to fail on them. There is no way Cycle-GAN can work for jeans to skirts because by design the distribution for images from both set would be mostly similar, and it is way too hard for the discriminator to distinguish between two. This ultimately leads the generator to act as an identity mapping (easily observed in all the qualitative examples).\n\n4. The proposed approach can easily find direct application in places where a user-control is required for image editing or synthesis.\n\n5. The literature review is extensive.\n\nCons: \n\n1. My biggest criticism of this work is the absence of simple baselines.  Given the fact that the formulation use an instance segmentation map with the given input, the following obvious baseline need consideration: \n\nSuppose the two domains are sheep and giraffe: \n\na. given the input of sheep and its instance mask, find a shape/mask in giraffe from the training images that is closest (it could be same location in image or some other similarity measure).\n\nb. mask the input image using the sheep mask. Use giraffe mask and add corresponding RGB components of the masked giraffe (from the training set) to the masked input image. \n\nThe above step would give a rough image with some holes.\n\nc. To remove holes, one can either use an image inpainting pipeline, or can also simply use a CNN with GAN loss.\n\nI believe that above pipeline should give competitive (if not better) outputs to the proposed formulation. (Note: the above pipeline could be considered a simpler version of PhotoClipArt from Lalonde et al, 2007).\n\n2. Nearest neighbors on generated instance map needs to be done. This enables to understand if the generated shapes are similar to ones in training set, or there are new shapes/masks being generated. Looking at the current results, I believe that generated masks are very similar to the training instances for that category. And that makes baseline described in (1) even more important.\n\n3. An interesting thing about Cycle-GAN is its ability to give somewhat temporally consistent (if not a lot) -- ex. Horse to Zebra output shown by the authors of Cycle-GAN. I am not sure if the proposed formulation will be able to give temporally consistent output on shorts/skirts to jeans example. It would be important to see how the generated output looks for a given video input containing a person and its segmentation map  of jeans to generate a video of same person in shorts? \n\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea, comparisons need to be improved",
            "review": "This paper proposes a well-designed instance level unsupervised image-to-image translation method which can handle the arbitrary number of instances in a permutation-invariant way. The idea is interesting and the results on various translation datasets are reasonable.  \n\nPros:\n* The proposed method process each instance separately to handle multiple instances. The summarization operation is a simple but effective way to achieve the permutation-invariant property. The context preserving loss is suitable for preserving the background information.\n* The paper is well written and easy to follow.\n\nCons:\n* My main concern is about the comparisons with CycleGAN in Figure 4 to 6. Although the CycleGAN+Seg results are shown in Figure 9 indicating that the proposed method can handle multiple instances better. I think there should also be CycleGAN+Seg results in Figure 4 to 6, since the instance segmentation is an extra information. And in my opinion, the CycleGAN+Seg can handle the situation where there are only a few instances (also can be observed in the 1st row in Figure 9). Besides, CycleGAN+Seg can naturally handle the arbitrary number of instances without extra computation cost.\n\nQuestions:\n*  I wonder what will happen if the network does not permutation-invariant. Except that the results will vary for different the input order, will the generated quality decrease? Since the order information may be useful for some applications.\n\nOverall, I think the proposed method is interesting but the comparison should be fairer in Figure 4 to 6. \n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}