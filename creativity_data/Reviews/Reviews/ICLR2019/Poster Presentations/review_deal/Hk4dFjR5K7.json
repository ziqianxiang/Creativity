{
    "Decision": {
        "metareview": "The submission proposes a method to construct adversarial attacks based on deforming an input image rather than adding small peturbations.  Although deformations can also be characterized by the difference of the original and deformed image, it is qualitatively and quantitatively different as a small deformation can result in a large difference.\n\nOn the positive side, this paper proposes an interesting form of adversarial attack, whose success can give additional insights on the forms of existing adversarial attacks.  The experiments on MNIST and ImageNet are reasonably comprehensive and allow interesting interpretation of how the image deforms to allow the attack.  The paper is also praised for its clarity, and cleaner formulation compared to Xiao et al. (see below).  Additional experiments during rebuttal phase partially answered reviewer concerns, and provided more information e.g. about the effect of the smoothness of the deformation.\n\nThere were some concerns that the paper primarly presents one idea, and perhaps missed an opportunity for deeper analysis (R1).  R2 would have appreciated more analysis on how to defend against the attack.\n\nA controversial point is the relation /  novelty with respect to Xiao et al., ICLR 2018.  As e.g. pointed out by R1: \"The paper originates from a document provably written in late 2017, which is before the deposit on arXiv of another article (by different authors, early 2018) which was later accepted to ICLR 2018 [Xiao and al.]. This remark is important in that it changes my rating of the paper (being more indulgent with papers proposing new ideas, as otherwise the novelty is rather low compared to [Xiao and al.]).\"\n\nOn the balance, all three reviewers recommended acceptance of the paper.  Regarding novelty over Xiao et al., even ignoring the arguable precedence of the current submission, the formulation is cleaner and will likely advance the analysis of adversarial attacks.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Area chair recommendation"
    },
    "Reviews": [
        {
            "title": "Paper introducing an interesting new idea, that I would wish to see further developped",
            "review": "The paper proposes a new way to construct adversarial examples: do not change the intensity of the input image directly, but deform the image plane (i.e. compose the image with Id + tau where tau is a small amplitude vector field).\n\nThe paper originates from a document provably written in late 2017, which is before the deposit on arXiv of another article (by different authors, early 2018) which was later accepted to ICLR 2018 [Xiao and al.]. This remark is important in that it changes my rating of the paper (being more indulgent with papers proposing new ideas, as otherwise the novelty is rather low compared to [Xiao and al.]).\n\nPros:\n- the paper is well written, very easy to read, well explained (and better formalized than [Xiao and al.]);\n- the idea of deforming images is new (if we forget about [Xiao and al.]) and simple;\n- experiments show what such a technique can achieve on MNIST and ImageNet. Interestingly, one can see on MNIST the parts of the numbers that the adversarial attack is trying to delete/create.\n\nCons:\n- the paper is a bit weak, in that it is not very dense, and in that there is not much more content than the initial idea;\n- for instance, more discussions about the results obtained could have been appreciated (such as my remark above about MNIST);\n- for instance, a study of the impact of the regularization would have been interesting (how does the sigma of the Gaussian smoothing affect the type of adversarial attacks obtained and their performance -- is it possible to fool the network with [very] smooth deformations?);\n- for instance, what about generating adversarial examples for which the network would be fully (wrongly) confident? (instead of just borderline unsure); etc.\n- The interpolation scheme (how is defined the intensity I(x,y) for a non-integer location (x,y) within the image I) is rather important (linear interpolation, etc.) and should be at least mentioned in the main paper, and at best studied (it might impact the gradient descent path and the results);\n- question: does the algorithm converge? could there be a proof of this? This is not obvious, as the objective potentially changes with time (selection of the current m best indices k of |F_k - F_l|). Also, the final overshoot factor (1+eta) is not very elegant, and not guaranteed to perform well if tau* starts being not small compared to the second derivative (i.e. g''.tau^2 not small) while I guess that for image intensities, spatial derivatives can be very high if no intensity smoothing scheme is used.\n- note: the approximation tau* = sum_i tau_i (section 2.3) does not stand in the case of non-small deformations.\n- still in section 2.3, I do not understand the statement \"given that \\nabla f is moderate\": where does this property come from? or is \"given\" meant to be understood as \"provided...\" (i.e. under the assumption that...)?\n- computational times could have been given (though I guess they are reasonable).\n\nOther remarks:\n- suggestion: I find the \"slight abuse of notation\" (of confusing the derivative with the gradient) a bit annoying and suggest to use a different symbol, such as \\nabla g. This could be useful in particular in the following perspective:\n- Mathematical side note: the \"gradient\" of a functional is not a uniquely-defined object in that it depends on the metric chosen in the tangent space. More clearly: the space of small deformations tau comes with an inner product (here L2, but one could choose another one), and the gradient \\nabla g obtained depends on this inner product choice M, even though the derivative Dg is the same (they are related by Dg(tau) = < \\nabla_M g | tau >_M for any tau). The choice of the metric can then be seen as a prior over desired gradient descent paths. In the paper, the deformation fields get smoothed by a Gaussian filter at some point (eq. 7), in order to be smoother: this can be interpreted as a prior (gradient descent paths should be made of smooth deformations) and as an associated inner product change (there do exist a metric M such that the gradient for that metric is \\nabla_M g = S \\nabla_L2 g). It is possible to favor other kind of deformations (not just smooth ones, but for instance rigid ones, etc. [and by the way this could make the link with \"Manitest: Are classifiers really invariant?\" by Fawzi and Frossard, BMVC 2015, who observe that a rigid motion can affect the classifier output]). If interested, you can check \"Generalized Gradients: Priors on Minimization Flows\" by Charpiat et al. for general inner products on deformations (in particular favoring rigid motion), and \"Sobolev active contours\" by Sundaramoorthi et al. for inner products more dedicated to smoothing (such as with the H1 norm).\n- Note: about the remark in section 3.2: deformation-induced transformations are a subset of all possible transformations of the image (which are all representable with intensity changes), so it is expected that a training against attacks on the intensity performs better than a training against attacks on spatial deformations.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting attack",
            "review": "In this paper, the authors proposed a new attack using deformation. The results are quite realistic to the naked eyes (at least for the example shown).  The idea is quite simple, generate small displacement and resample (interpolate) image until the label flips.\n\n- I think this is a good contribution, It is a kind of attack we should consider.\n- One thing which is good to consider is the type of interpolation. I believe the success rate would be different for linear versus say B-spline interpolation. Also, the width of the smoothing applied to the deformation field has an impact. The algorithm is straightforward, there is no reason to experiment with those.\n\n- It is useful to report pixel displacement in Table 1. The reported values are not intuitive, the **average** displacement for Inception-v3 is 0.59.  Here is my back of envelope conversion of 0.59 which is probably off:\n\n299 (# pixels of the smaller axis 299 for the Inception) x 1/2 (image are centered) x 0.59 =  88 pixels\n\nThis is huge! I think I am calculating something incorrectly because in Fig3,4 those displacements are not big. \n\n- The results of Table 2 is interesting. Why a networked trained with PGD is more robust to ADef attack that a network trained adversarially with Adef?\n\n\n\n\nMinor:\n- The paper is a bit nationally convoluted for no good reason, the general idea is straightforward. \n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The topic is interesting but the overall technical contribution looks weak. ",
            "review": "The paper introduces an iterative method to generate deformed images for adversarial attack. The core idea is to perturb the correctly classified image by iteratively applying small deformations, which are estimated based on a first-order approximation step, until the image is misclassified. Experimental results on several benchmark datasets (MNIST, ImageNet) and commonly used deep nets (CNN, ResNet, Inception) are reported to show the power of adversarial deformations. \n\nThe idea of gradually adding deformations based on gradient information is somewhat interesting, and novel as far as the reviewer knows about. The method is clearly presented and the results are mostly easy to access.  However, the intuition behind the proposal does not make strong sense to the reviewer: since the main focus of this work is on model attack, why not directly (iteratively or not) adding random image deformations to fool the system? Particularly, the first-order approximation strategy (as shown in Eq.4 and Eq.5) is quite confusing. On one side (see Eq.4), the deformation \\tau should be small enough in scale to make an accurate approximation. On the other side (see Eq. 5), \\tau is required to be sufficiently large in order to generate misclassification. Such seemingly conflicting rules for estimating the deformation makes the proposed method less rigorous in math. \nAs another downside, the related adversarial training procedure is not fully addressed. The authors briefly discussed this point in the experiment section and provided a few numerical results in Table 2. These results, as acknowledged by the authors, do not well support the effectiveness of deformation adversarial attack and defense. In the meanwhile, the mentioned adversarial training framework follows straightforwardly from PGD (Madry et al. 2018), and thus the novelty of this contribution is also weak. More importantly, it is not clear at all, both in theory and algorithm, whether the advocated gradual deformation attack and defense can be unified inside a joint min-max/max-min learning formulation, as what PGD is rooted from.\n\nPros: \n\n- The way of constructing deformation adversarial is interesting and novel\n- The paper is mostly clearly organized and presented.\n\nCons:\n\n- The motivation of approach is questionable. \n- The related adversarial training problem remains largely unaddressed.\n- Numerical study shows some promise in adversarial attack, but is not supportive to the related defense capability. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}