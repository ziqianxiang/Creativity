{
    "Decision": {
        "metareview": "This paper presents a method for transferring source information via the hidden states of recurrent networks.  The transfer happens via an attention mechanism that operates between the target and the source.  Results on two tasks are strong.\n\nI found this paper similar in spirit to Hypernetworks (David Ha, Andrew Dai, Quoc V Le, ICLR 2016) since there too there is a dynamic weight generation for network given another network, although this method did not use an attention mechanism.\n\nHowever, reviewers thought that there is merit in this paper (albeit pointed the authors to other related work) and the empirical results are solid.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Meta Review"
    },
    "Reviews": [
        {
            "title": "Good empirical results on transfer learning; writing could be clearer",
            "review": "== Quality of results ==\nThis paper's empirical results are its main strength. They evaluate on a well-known benchmark for transfer learning in text classification (the Amazon reviews dataset of Blitzer et al 2007), and improve by a significant margin over recent state-of-the-art methods. They also evaluate on several sequence tagging tasks and achieve good results.\n\nOne weakness of the empirical results is that they do not compare against training a model on the union of the source and target domain. I think this is very important to compare against.\n\nNote: the authors cite a paper in the introduction \"Hierarchical Attention Transfer Network for Cross-domain Sentiment\nClassification\" (Li et al 2018) which also achieves state of the art results on the Amazon reviews dataset, but do not compare against it. At first glance, Li et al 2018 appear to get better results. However, they appear to be training on a larger amount of data for each domain (5600 examples, rather than 1400). It is unclear to me why their evaluation setup is different, but some clarification about this would be helpful.\n\n== Originality ==\nA high level description of their approach:\n1. Train an RNN encoder (\"source domain encoder\") on the source domain\n2. On the target domain, encode text using the following strategy:\n  - First, encode the text using the source domain encoder\n  - Then, encode the text using a new encoder (a \"target domain encoder\") which has the ability to attend over the hidden states of the source domain encoder at each time step of encoding.\n\nThey also structure the target domain encoder such that at each time step, it has a bias toward attending to the hidden state in the source encoder at the same position.\n\nThis has a similar flavor to greedy layer-wise training and model stacking approaches. In that regard, the idea is not brand new, but feels well-applied in this setting.\n\n== Clarity ==\nI felt that the paper could have been written more clearly. The authors set up a comparison between \"transfer information across the whole layers\" vs \"transfer information from each cell\" in both the abstract and the intro, but it was unclear what this distinction was referring to until I reached Section 4.1 and saw the definition of Layer-Wise Transfer.\n\nThroughout the abstract and intro, it was also unclear what was meant by \"learning to collocate cross domain words\". After reading the full approach, I see now that this simply refers to the attention mechanism which attends over the hidden states of the source domain encoder.\n\n== Summary ==\nThis paper has good empirical results, but I would really like to see a comparison against training a model on the union of the source and target domain. I think superior results against that baseline would increase my rating for this paper.\n\nI think the paper's main weakness is that the abstract and intro are written in a way that is somewhat confusing, due to the use of unconventional terminology that could be replaced with simpler terms.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper proposed to use RNN/LSTM with collocation alignment as a representation learning method for transfer learning/domain adaptation in NLP.",
            "review": "The proposed method is suitable for many NLP tasks, since it can handle the sequence data.\n\nI find it difficult to follow through the model descriptions.  Perhaps a more descriptive figures would make this easier to follow, I feel that the ART model is a very strait forward and it can be easily described in much simpler and less exhausting (sorry for the strong word) way, while there is nothing wrong with being as elaborating as you are, I feel that all those details belong in an appendix. \nCan you please explain the exact learning process?\nI didn’t fully understand the exact way of collocations, you first train on the source domain and then use the trained source network when training in the target domain with all the collocated words for each training example? I deeply encourage you to improve the model section for future readers. \nIn contrast to the model section, the related work and the experimental settings sections are very thin.\nThe experimental setup for the sentiment analysis experiments is quite rare in the transfer learning/domain adaptation landscape, having equal amount of labeled data from both source and target domains is not very realistic in my humble opinion.\nMore realistic setup is unsupervised domain adaptation (like in DANN and MSDA-DAN papers) or minimally supervised domain adaptation (like you did in your POS and NER experiments).\n\nIn addition to the LSTM baseline (which is trained with target data only), I think that LSTM which is trained on both source and target domains data is required for truly understand ART gains – this goes for the POS and NER tasks as well.\nThe POS and NER experiments can use some additional baselines for further comparison, for example:\nhttp://www.aclweb.org/anthology/Q14-1002\nhttps://hornhehhf.github.io/hangfenghe/papers/14484-66685-1-PB.pdf\n\nI am not sure I understand the “cell level transfer” claim, did you mean that you are the first to apply inner LSTM/RNN cell transfer or that you are the first ones to apply word-level fine grained transfer, the latter has already been done:\nhttps://arxiv.org/pdf/1802.05365.pdf\nhttps://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=4531&context=sis_research\nhttp://www.aclweb.org/anthology/N18-1112\nhttps://openreview.net/pdf?id=rk9eAFcxg\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Reasonable idea but the technical details are quite unclear",
            "review": "This paper presents the following approach to domain adaptation. Train a source domain RNN. While doing inference on the target domain, first you run the source domain RNN on the sequence. Then while running the target domain RNN, set the hidden state at time step i, h^t_i, to be a function 'f' of  h^t_{i-1} and information from source domain \\psi_i; \\psi_i is computed as a convex combination of the state of the source domain RNN, h^s_{i}, and an attention-weighted average of all the states h^s{1...n}. So in effect, the paper transfers information from each of source domain cells -- the cell at time step i and all the \"collocated\" cells (collocation being defined in terms of attention). This idea is then extended in a straightforward way to LSTMs as well. \n \nDoing \"cell-level\" transfer enables more information to be transferred according to the authors, but it comes at a higher computation since we need to do O(n^2) computations for each cell.\n\nThe authors show that this beats a variety of baselines for classification tasks (sentiment), and for sequence tagging task (POS tagging over twitter.)\n\nPros:\n1. The idea makes sense and the experimental results show solid \n\nCons:\n1. Some questions around generalization are not clearly answered. E.g. how are the transfer parameters of function 'f' (that controls how much source information is transferred to target) trained? If the function 'f' and the target RNN is trained on target data, why does 'f' not overfit to only selecting information from the target domain? Would something like dropping information from target domain help?\n\n2. Why not also compare with a simple algorithm of transferring parameters from source to target domain? Another simple baseline is to just train final prediction function (softmax or sigmoid) on the concatenated source and target hidden states. Why are these not compared with? Also, including the performance of simple baselines like word2vec/bow is always a good idea, especially on the sentiment data which is very commonly used and widely cited. \n\n3. Experiments: the authors cite the hierarchical attention transfer work of Li et al (https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16873/16149) and claim their approach is better, but do not compare with them in the experiments. Why?\n\nWriting:\nThe writing is quite confusing at places and is the biggest problem with this paper. E.g.\n\n1. The authors use the word \"collocated\" everywhere, but it is not clear at all what they mean. This makes the introduction quite confusing to understand. I assumed it to mean words in the target sentences that are strongly attended to. Is this correct? However, on page 4, they claim \"The model needs to be evaluated O(n^2) times for each sentence pair.\" -- what is meant by sentence pair here? It almost leads me to think that they consider all source sentence and target sentences? This is quite confusing. \n\n2. The authors keep claiming that \"layer-wise transfer learning mechanisms lose the fine-grained cell-level information from the source domain\", but it is not clear exactly what do they mean by layer-wise here. Do they mean transferring the information from source cell i to target cell i as it is? In the experiments section on LWT, the authors claim that \"More specifically, only the last cell of the RNN layer transfers information. This cell works as in ART. LWT only works for sentence classification.\" Why is it not possible to train a softmax over both the source hidden state and the target hidden state for POS tagging? \n\nnits:\npage 4 line 1: \"i'th cell in the source domain\" -> \"i'th cell in the target domain\". \"j'th cell in target\" -> \"j'th cell in sourcE\".\n\n\nRevised: increased score after author response.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}