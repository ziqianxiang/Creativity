{
    "Decision": {
        "metareview": "The authors propose an efficient scheme for encoding sparse matrices which allow weights to be compressed efficiently. At the same time, the proposed scheme allows for fast parallelizable decompression into a dense matrix using Viterbi-based pruning. \nThe reviewers noted that the techniques address an important problem relevant to deploying neural networks on resource-constrained platforms, and although the work builds on previous work, it is important from a practical standpoint. \nThe reviewers noted a number of concerns on the initial draft of this work related to the experimental methodology and the absence of runtime comparison against the baseline, which the reviewers have since fixed in the revised draft. The reviewers were unanimous in recommending that the revision be accepted, and the authors are requested to incorporate the final changes that they said they would make in the camera-ready version.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Efficient weight encoding which is important for a practical standpoint"
    },
    "Reviews": [
        {
            "title": "poorly written and fundamentally flawed",
            "review": "The paper proposes two additional steps to improve the compression of weights in deep neural networks. The first is to quantize the weights after pruning, and the second is to further encode the quantized weights.\n\nThere are several weaknesses in this paper. The first one is clarity. The paper is not very self-contained, and I have to constantly go back to Lee et al. and Xu et al. in order to read through the paper.\n\nThe paper can be made more mathematically precise. The input and output types of each block in Figure 1. should be clearly stated. For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits. Since the paper relies so heavily on Lee et al., the authors should make an effort to summarize the approach in a mathematically precise way.\n\nThe figures are almost useless, because the captions contain very little information. For example, the authors should at least say that the \"D\" in Figure 2. stands for delay, and the underline in Figure 4. indicates the bits that are not pruned. Many more can be said in all the figures.\n\nThe second weakness is experimental design. There are two conflicting qualities that need to be optimized--performance and compression rate. When optimizing the compression rate, it is important not to look at the test set error. If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set. The test set is typically small compared to the training set, so it is no surprise that the compression rate can be as high as 90%.\n\nOptimizing compression rates should be done on the training set with a separate development set. The test set should not used before the best compression scheme is selected. Both the results on the development set and on the test set should be reported for the validity of the experiments. I do not see these experimental settings mentioned anywhere in the paper, and this is very concerning. Lee et al. seem to make similar mistakes, and it is likely that their experimental design is also flawed.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Can be an accept if they do more analyses",
            "review": "This paper presents a new way to represent a dense matrix in a compact format. First, the method prunes a dense matrix based on the Viterbi-based pruning. Then, the pruned matrix is quantized with alternating multi-bit quantization. Finally, the binary vectors produced by the quantization algorithm are further compressed with the Viterbi-based algorithm. It spots the problem of each existing approach and solve the problems by combining each method. The combination is new and the result is encouraging.\n\nI find this paper is interesting and I like the strong results. It is an interesting combination of methods. However, the experiments are not enough to show that the proposed method is really needed to achieve the results. If these are answered well, I'd be happy to change my evaluation.\n\n1. The method should be compared with other combinations of components. At least, it should be compared with \"Multi-bit quantization only (Xu et al., 2018)\" and \"Multi-bit-quantization + Viterbi-based binary code encoding\".\n\n2. The experiments with \"Don't Care\" should go to the experiment section, and the end-to-end results should be present but not the ratio of incorrect bits.\n\n3. Similarly, the paper will become stronger if it has some experimental results that compare quantization methods. In Section 3.3., it mentions that the conventional k-bit quantization was tried and significant accuracy drops were observed. I feel that this is a kind of things which support the proposed method if it is properly assessed.\n\n4. When you say \"slow\" form something and propose a method to address it, I'd like to see some benchmark numbers. There is an experiment with simulation, but that does not seem to simulate the slow \"sequential sparse matrix decoding process\".\n\nMinor comments:\n\n* It was a bit hard to understand how a matrix is processed through the flowchart in Fig. 1 at first glance. It would help readers to understand it better if it has a corresponding figure which shows how a matrix is processed through the flowchart.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Hard to read, but idea is interesting",
            "review": "Summary:\n\nThis paper addresses the computational aspects of Viterbi-based encoding for neural networks. \n\nIn usual Viterbi codes, input messages are encoded via a convolution with a codeword, and then decoded using a trellis. Now consider a codebook with n convolutional codes, of rate 1/k. Then a vector of length n is represented by inputing a message of length k and receiving n encoded bits. Then the memory footprint (in terms of messages) is reduced by rate k/n. This is the format that will be used to encode the row indices in a matrix, with n columns.  (The value of each nonzero is stored separately.)  However, it is clear that not all messages are possible, only those in the \"range space\" of my codes. (This part is previous work Lee 2018.) \n\nThe \"Double Viterbi\" (new contribution) refers to the storage of the nonzero values themselves. A weakness of CSR and CSC (carried over to the previous work) is that since each row may have a different number of nonzeros, then finding the value of any particular nonzero requires going through the list to find the right corresponding nonzero, a sequential task. Instead, m new Viterbi decompressers are included, where each row becomes (s_1*codeword_1 + s_2*codeword2 + ...) cdot mask, and the new scalar are the results of the linear combinations of the codewords. \n\nPros:\n - I think the work addressed here is important, and though the details are hard to parse and the new contributions seemingly small, it is important enough for practical performance. \n - The idea is theoretically sound and interesting.\n\nCons: \n - My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor. Compressability is evaluated, but that was already present in the previous work. Therefore the novel contribution of this paper over Lee 2018 is not clearly outlined.\n - It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract. \n - Minor grammatical mistakes (missing \"a\" or \"the\" in front of some terms, suggest proofread.)\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}