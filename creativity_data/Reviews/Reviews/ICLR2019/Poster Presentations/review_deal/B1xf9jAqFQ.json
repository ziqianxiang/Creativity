{
    "Decision": {
        "metareview": "The authors obtain nice speed improvements by learning to skip and jump over input words when processing text with an LSTM. At some points the reviewers considered the work incremental since similar ideas have already been explored, but at the end two of the reviewers ended up endorsing the paper with strong support.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Accept"
    },
    "Reviews": [
        {
            "title": "New incremental work on speed reading with slightly better empirical results",
            "review": "The paper proposes a Structural-Jump-LSTM model to speed up machine reading, which is an extension of the previous speed reading models, such as LSTM-Jump, Skim-LSTM and LSTM-Shuffle. The major difference, as claimed by the authors, is that the proposed model has two agents instead of one. One agent decides whether the next input should be fed into the LSTM (skip) and the other determines whether the model should jump to the next punctuation (jump). The sentence-wise jumping makes the jumping more structural than models like LSTM-Jump, while the word-wise skipping operation has a finer skimming decision. The reinforcement learning algorithm in this paper is also different from LSTM-Jump, where LSTM-Jump uses REINFORCE, while this paper applies actor-critic approach. \n\nEmpirical studies show that Structural-Jump-LSTM is (slightly) better than state-of-the-art methods in terms of both accuracy and speed over most but few datasets. My feeling is that the proposed model should work much better than the previous models in very long texts, which I suggest the author should try on. Otherwise, the performance gain looks marginal and it is thus questionable whether the complicated modeling is necessary. \n\nI am confused by Figure 1: why are the “yes/no” placed in front of the “skipped”? “Previous LSTM” is confusing as well, which should be “Previous Output/hidden state”.\n\nMinor comment: The LSTM-Jump takes word2vec as the initialization in CBT, while this paper uses GLOVE. I wonder if this results in the performance difference in accuracy. From my experience, GLOVE is usually better than word2vec in most of the tasks. If this effect also applies to CBT, the experiment is not fair.\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "review",
            "review": "The paper proposes a fast-reading method using skip and jump actions. The paper shows that the proposed method is as accurate as LSTM but uses much less computation.\n\n* pros: \n- very fast reading model (?). \n\n* cons: \n- although the paper is well written, the jump is not described in details. \n- using 'structural-jump' is a little misleading. The model will jump to \".,!\" or end of sentence. What is called \"structural\"? Note that those punctuation marks are not 100% correlated to sentence structure. For example, \"He hate fruits such as apples, pears, and oranges.\" The mode should jump to the end of sentence rather than the first \",\" when reading \"such\". \n- maybe the authors should say a little bit about the used computation-cost-reduction method. (I.e. in an appendix). ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper presents a new speed reading model by combined several existing ideas. The idea is novel and the results are good.",
            "review": "The paper presents a novel model for neural speed reading. In this new model, the authors combined several existing ideas in a nice way, namely, the new reader has the ability to skip a word or to jump a sequence of words at once. The reward of the reader is mixed of the final prediction correctness and the amount of text been skipped. The problem is formulated as a reinforcement learning problem. The results compared with the existing techniques on several benchmark datasets show consistently good improvements.\n\nIn my view, one important (also a little surprising) finding of the paper is that the reader can make jump choices successfully with the help of punctuations. And, blindly jumping a sequence of words without even lightly read them can still make very good predictions.\n\nThe basic idea of the paper, the concepts of skip and jump, and the reinforcement learning formulation are not completely new, but the paper combined them in an effective way. The results show good improvements majorly in FLOPS.\n\nThe way of defining state, rewards and value function are not very clear to me. Two value estimates are defined separately for the skip agent and the jump agent. Why not define a common value function for a shared state? Two values will double count the rewards from reading. Also, the state of the jump agent may not capture all available information. For example, how many words until the end of the sentence if you make a jump. Will this make the problem not a MDP? \n\nOverall, this is a good paper.\n\nI read the authors' response. The paper should in its final version add the precise explanation of how the two states interact and how a joint state definition differs from the current one.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}