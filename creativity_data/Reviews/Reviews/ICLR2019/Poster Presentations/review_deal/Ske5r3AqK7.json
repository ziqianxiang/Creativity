{
    "Decision": {
        "metareview": "Word vectors are well studied but this paper adds yet another interesting dimension to the field.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Interesting submission"
    },
    "Reviews": [
        {
            "title": "Poincare Glove: Hyperbolic Word Embeddings",
            "review": "Summary: \nWords have implicit hierarchy among themselves in a text. Hyperbolic geometry due to the negative curvature and the delta-hyperbolicity is more suitable for representing hierarchical data in the continuous space. As a result it is natural to learn word representations/embeddings in the hyperbolic space. This paper proposes a promising approach that extends the approach presented in [1] to implement a GLOVE based hyperbolic word embedding model. The embeddings are optimized by using the Riemannian Optimization methods presented in [2]. Authors provide results on word-similarity and word-analogy tasks.\n\n\nQuestions: \nWhat are the reasons for choosing a Poincare Ball model of the hyperbolic space instead of hyperboloid or other models of the hyperbolic space?\nCan you expand on the role of gyr[.,.] in Equations 6 and 7.\nBesides the tasks that are presented in this paper including word-analogy and the word-similarity tasks. Have you considered using the embeddings learned in hyperbolic space in a down-stream task such as NLI? \n\nPros:\nThe paper is very well-written, the motivation and the goals are quite clear.\nThe relationship between the Gaussian embeddings and the product spaces is interesting and neat. The paper is theoretically strong and consistent.\nThe idea of learning word-embeddings in hyperbolic space with the proposed approach is novel and relevant.\n\nCons:\n\nThe weakest point of this paper is the experiments. Unfortunately the results reported are underwhelming on WBLESS and the Hyperlex tasks compared to other published results. The paper presents convincing results on Word-analogy and Word-similarity tasks. However they do not compare against the published results on those tasks.\n\n[1] Ganea, O. E., Bécigneul, G., & Hofmann, T. (2018). Hyperbolic Neural Networks. arXiv preprint arXiv:1805.09112.\n[2] Bécigneul, Gary, and Octavian-Eugen Ganea. \"Riemannian Adaptive Optimization Methods.\" arXiv preprint arXiv:1810.00760 (2018).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Adapting Glove word embedding to the Poincare half-plane: interesting but incremental",
            "review": "This paper adapts the Glove word embedding (Pennington et al 2014) to a hyperbolic space given by the Poincare half-plane model.  The embedding objective function is given by equation (3), where h=cosh^2 so that it corresponds to a hyperbolic geometry. The author(s) showed that their hyperbolic version of Glove is better than the original Glove. Besides that,  based on (Costa et al 2015), the author provided theoretical insights on the connection between hyperbolic embeddings with Gaussian word embeddings. Besides, the author(s) proposed a measure called \"delta-hyperbolicity\", that is based on (Gromov 1987) to study the model selection problem of using hyperbolic embeddings vs. traditional Euclidean embeddings.\n\nOverall, I find the contributions are interesting but incremental. Therefore it may not be significant enough to be published in ICLR. Moreover, the experimental evaluation is insufficient to show the advantages of the proposed Poincare Glove model.\n\nAn interesting theoretical insight is that there exists an isometry between the Fisher-geodesic distance of diagonal Gaussians and a product of Poincare half-planes. This is interesting as it revealed a connection between hyperbolic embeddings with Gaussian embeddings, which is not widely known. However, this is not an original contribution. This connection is not related to the optimization of the proposed embedding, as Gaussian word embeddings are optimized based on KL divergence etc. that are easy to compute.\n\nThe computation of analogy based on isometric transformations is interesting but straightforward by applying translation operations in the Poincare ball. The novel contribution is minor and mainly on related empirical results.\n\nThe definition of the delta-hyperbolicity is missing. The explicit form of the definition should be clearly given in section 7. Again, this is not a novel contribution but an application of previous definitions (Gromov 1987).\n\nIn the word similarity and analogy experiments, the baseline is the vanilla Glove, this is not sufficient as it is widely known that hyperbolic embeddings can improve over Euclidean embeddings on certain datasets. The authors are therefore suggested to include another hyperbolic word embedding (e.g. Nickel and Kiela 2017) into the baselines and discuss the advantages and disadvantages of the proposed method.\n\nThere are no novel and well-abstracted theoretical results (theorems) given in the submission.\n\nThe length of the paper is longer than the recommended length (9 pages of main text).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Quality in many respects.",
            "review": "The English, grammar and writing style is very good, as are the citations.\nThe technical quality appears to me to be very good (I am not an expert in Poincare spaces).\nThe authors demonstrate a good knowledge of the mathematical theory with the constructions made in Section 6.\nThe experimental write-up has been abbreviated.  The lexical entailment results Tables 6 and 7 are just sitting there without discussion, as far as I can see, as are the qualitative results Tables 4 and 5.   The entailment results are quite complex and really need supporting interpretation.  For instance, for Hyperlex, WN-Poincare is 0.512, above yours.\nFor your entailment score you say \"For simplicity, we propose dropping the dependence in μ\".  This needs more justification and discussion as it is counter-intuitive for those not expert in  Poincare spaces.\nSection 6.2 presents the entailment score.  Note Nickel etal. give us a nice single formula.  You however, provide 4 paragraphs of construction from which an astute reader would then have to work on to extract your actual method.  I would prefer to see a summary algorithm given somewhere.  Perhaps you need another appendix.\nRADAGRAD is discussed in Section 5, but I'd have preferred to see it discussed again in Section 8 and discussed to highlight what was indded done and the differences.  It certainly makes the paper non-reproducible.\nA significant part of the theory in earlier sections is about the 50x2D method, but in experiments this doesn't seem to work as well.  Can you justify this some other how:  its much faster, its more interpretable?  Otherwise, I'm left thinking, why not delete this stuff?\nThe paper justifies its method with a substantial and winning comparison against vanilla GloVe.  That by itself is a substantial contribution.\nBut now, one is then hit with a raft of questions.  Embedding methods are popping up like daisies all over the fields of academia.  Indeed, word similarity and lexical entailment tasks themselves are proliferating too.  To me, its really unclear what one needs to achieve in the empirical section of a paper.  To make it worse, some folks use 500D, some 100D, some 50D, so results aren't always comparible.  Demonstrating one's work is state-of-the-art against all comers is a massive implementation effort.  I notice some papers now just compare against one other (e.g., Klami etal. ECML-PKDD, 2018).\n\nMy overall feeling is that this paper tries to compress too much into a small space (8 pages).\nI think it really needs to be longer to present what is shown.   Moreover, I would want to see the inclusion of the work on 50x2D justified. So my criticisms are about the way the paper is written, not about the quality of the work.  \nMoroever, though, one needs to consider comparisons against models other than GloVe.\n\nAddendum:  You know, what I really love about ICLR is the effort authors make to refresh their paper and respond to reviewers.  You guys did a great job.  Really impressed.  50x2D now clarified and some of the hasty/unexplained bits fixed.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}