{
    "Decision": {
        "metareview": "This paper discusses the promising idea of using RL for optimizing simulatorsâ€™ parameters. \n\nThe theme of this paper was very well received by the reviewers. Initial concerns about insufficient experimentation were justified, however the amendments done during the rebuttal period ameliorated this issue. The authors argue that due to considered domain and status of existing literature, extensive comparisons are difficult. The AC sympathizes with this argument, however it is still advised that the experiments are conducted in a more conclusive way, for example by disentangling the effects of the different choices made by the proposed model. For example, how would different sampling strategies for optimization perform? Are there more natural black-box optimization methods to use?\n\nThe reviewers believe that the methodology followed has a lot of space for improvement. However, the paper presents some fresh and intriguing ideas, which make it overall a relevant work for presentation at ICLR.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Methodology could be improved but ideas are very intriguing"
    },
    "Reviews": [
        {
            "title": "Great idea, but I don't think the right problems were selected to showcase the method",
            "review": "Pros:\n* Using RL to choose the simulator parameters is a good idea. It does not sound too novel, but at the same time I am not personally aware of this having been explored in the past (Note that my confidence is 4, so maybe other reviewers might be able to chime in on this point)\n* In theory, you don't need domain adaptation or other sim2real techniques if you manage to get the optimal parameters of the simulator with this method.\n* Certain attributes of the method were evaluated sufficiently: eg the number of training epochs for each policy iteration, the dataset size generated in each iteration, and whether initialization was random or not in each iteration.\nCons:\n* Experiments were underwhelming, and the choice of problems/parameters to tune was not the right one for the problem.\n* Parts of the paper could be clearer\n\nQUALITY:\n* I believe that although the idea is great, but the quality of the experiments could have been higher. Firstly, better problems could have been selected to showcase the method. I was excited to see experiments with CARLA, but was underwhelmed when I realized that the only parameter of the simulator that the method controlled was the number and the type of cars in the scene, and the task of interest was a car counting task (for which not much detail was provided). This would have been much more interesting and useful to the community if more parameters, including rendering parameters (like lighting, shading, textures, etc) were part of the search space. Similarly, the semantic segmentation task could have used more than one category. But even for the one category, there were no previous methods considered, and the only comparison was between random parameters and the learned ones, where we only see marginal improvement, and what I perceive to be particularly low IoU for the car (although it'd help to know what's the SOTA there for comparison) For both vision applications I could help but wonder why the authors did not try to simply train on the  validation set to give us another datapoint to evaluate the performance of the method: this is data that *is* used for training the outer loop, so it does beg the question of what is the advantage of having hte inner loop. \n\nCLARITY:\n* The writing of the paper was clear for the most part, however the experimental section could have been clearer. I was wondering how model/hyperparameter selection was performed? Was there another validation set (other than the one used to train the outer loop)\n* The proposed policy is dubbed \"its\". What does it mean?\n* It's not clear what is a \"deliberately adversarial\" initialization. Could you elaborate?\n* The letter R is used to mean \"reward\" and \"rendering\". This is confusing. Similarly some symbols are not explicitly explained (eg S) Generally Section 2.3 is particularly unclear and confusing until one gets to the experimental section.\n* Section 3 discusses the technique and states that \"we can thus generate or oversample unusual situations that would otherwise not be part of the training data\" I believe it is important to state that, as the method is presented, this is only true if the \"validation\" data is varied enough and includes such situations. I believe this would be more applicable if eg rendering parameters were varied and matched the optimal ones.\n* Also the method is presented as orthogonal to domain adaptation and other sim-to-real techniques. However, I do not necessarily believe that this paper should be discussed outside the context of such techniques like domain randomization, Cycada, PixelDA etc. Even though these (esp. the latter ones) focus on vision, I do think it sets the right context.\nORIGINALITY:\n* As far as I'm aware noone has tried something similar yet. However, I'm not confident on this.\nSIGNIFICANCE:\n* Although the idea is good, I don't think that the approach to select the simulation parameters presented in the experiments in such a way is significant. I think that eg doing so for rendering parameters would be a lot more powerful and useful (and probably a lot more challenging). Also, I think that a single set of parameters (which seems to be what the goal is in this work) is not what one wants to achieve; rather one wants to find a good range of parameters that can help in the downstream task.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Sound method, but lacking a proper evaluation and comparison",
            "review": "This work makes use of policy gradients for fitting the parameters of a simulator in order to generate training data that results in maximum performance on real test data (e.g., for classification). The difficulty of the task rises from the non-differentiability of the simulator.\n\n# Quality\n\nThe method is sound, well-motivated, and presented with a set of reasonable experiments. However, and this is a critical weakness of the paper, no attempt is made to compare the proposed method with respect to any related work, beyond a short discussion in Section 3. The experiments do include some baselines, but they are all very weak. \n\n# Clarity\n\nThe paper is well-written and easy to follow. The method is illustrated with various experiments that either study some properties of the algorithm or show some good performance on real data.\n\n# Originality\n\nThe related work is missing important previous papers that have proposed very similar/identical algorithms for fitting simulator parameters in order to best reproduce observed data. For example,\n- https://arxiv.org/abs/1804.01118\n- https://arxiv.org/abs/1707.07113\nwhich both make use of policy gradients for fitting an adversary between fake and real data (which is then used a reward signal for updating the simulator parameters).\n\n# Significance\n\nThe significance of the paper is moderate given some similar previous works. However, the significance of the method itself (regardless of previous papers) is important.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "stimulating idea; potentially flawed method and incomplete evaluation",
            "review": "The paper explores an interesting idea: automatically tuning the parameters of a simulation engine to maximize the performance of a model that is trained using this simulation engine. In the most interesting scenario, the model is trained using such optimized simulation and then tested on real data; this scenario is explored in Section 4.5.\n\nThe basic idea of optimizing simulation parameters for transfer performance on real data is very good. I believe that this idea will be further explored and advanced in future work. The present submission is either the first or one of the first papers to explicitly explore this idea, and deserves some credit and goodwill for this reason. This is the primary reason my rating is \"marginally above acceptance threshold\" and not lower.\n\nThe paper suffers from some issues in the technical formulation and experimental evaluation. The issues are reasonably serious. First, it is not clear at all that RL is the right approach to this optimization problem. There is no multi-step decision making, there are no temporal dynamics, there is no long-term credit assignment. The optimization problem is one-shot: you pick a set of parameters and get a score. Once. That's it. It's a standard black-box optimization setting with no temporal aspect. My interpretation is that RL is used here because it's fashionable, not because it's appropriate.\n\nThe evaluation is very incomplete and unsatisfactory. Let's focus on Table 1, which I view as the main result since it involves real data. First, is the optimization performed using the KITTI validation set? Without any involvement of the test set during the optimization? I hope so, but would like the authors to explicitly confirm.\n\nSecond, the only baseline, \"random params\", is unsatisfactory. I take this baseline to be the average performance of randomized simulation. But this is much too weak. Since the authors have access to the validation set during the optimization, they can simply test which of the random parameter sets performs best on the validation set and use that. This would correspond to the *best* set of parameters sampled during training. It's a valid baseline, there is no reason not to use it. It needs to be added to Table 1.\n\nAlso, 10 sets of random params seems quite low. How many sets of parameters does the RL solver sample during training? That would be the appropriate number of sets of params to test for the baseline. (And remember to take the *best* of these for the baseline.)\n\nThe last few points really boil down to setting up an honest random search baseline. I consider this to be mandatory and would like to ask that the authors do this for the rebuttal. There are also other derivative-free optimization techniques, and a more thorough evaluation would include some of these as well.\n\nMy current hypothesis is that an honest random search baseline will do as well as or better than the method presented in the submission. Then the submission boils down to \"let's automatically tune simulation parameters; we can do this using random search\". It's still a stimulating idea. Is it sufficient for an ICLR paper? Not sure. Something for the reviewers and the ACs to discuss as a group.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}