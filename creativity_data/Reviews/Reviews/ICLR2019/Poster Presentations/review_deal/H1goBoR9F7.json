{
    "Decision": {
        "metareview": "This paper proposes a novel approach for network pruning in both training and inference. This paper received a consensus of acceptance. Compared with previous work that focus and model compression on training, this paper saves memory and accelerates both training and inference. It is activation, rather than weight that dominates the training memory. Reviewer1 posed a valid concern about the efficient implementation on GPUs, and authors agreed that practical speedup on GPU is difficult. It'll be great if the authors can give practical insights on how to achieve real speedup in the final draft. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "novel approach"
    },
    "Reviews": [
        {
            "title": "An interesting method to reduce the memory & time cost for both DNN training and inference",
            "review": "[Overview]\n\nIn this paper, the authors proposed to use dynamic sparse computation graph for reducing the computation memory and time cost in deep neural network (DNN). This method is applicable in both DNN training and inference. Unlike most of previous work that focusing on the reduction of computation during the inference time, this new method propose a dynamic computation graph by pruning the activations on the fly during the training of inference, which is an interesting and novel exploration. In the experiments, the authors performed extensive experiments to demonstrate the effectiveness of the proposed method compared with several baseline methods and original models. It is clear to me that this method helps to reduce the memory cost and computation cost for both DNN training and inference.\n\n\n[Strengthes]\n\n1. This paper addresses the computational burden in both memory and time from a novel angle than previous network pruning methods. It can be applied to reduce the computation in both network training and inference, but also preserve the representation ability of the network.\n\n2. To endow the network compression in training and inference, the authors proposed to mute the low-activated neurons so that the computations merely happened on those selected neurons. \n\n3. For the selection, the authors proposed a simple but efficient dimension reduction methods, random sparse projection, to project the original activations and weights into a lower-dimensional space and compute the approximated response map in such a lower dimension space, which the selection is based on.\n\n4. The authors performed comprehensive experiments to demonstrate the effectiveness the proposed method for network compression. Those results are insightful and solid.\n\n[Questions]\n\n1. Is the sparsity of each layer the same across the whole network? It would be nice if the authors could perform some ablation studies on varied sparsity in different layers, maybe just with some heuristic methods, e.g., decreasing the sparsity from lower layer to upper layers. As the authors mentioned, higher sparsity causes a larger degradation on deeper network. I am curious that whether there are some better way to set the sparsity.\n\n2. During the training of the network, how the activation evolve? It would be interesting to show how the selected activation changes across the training time for the same training sample.  This might provide some insights on when the activations begin to converge to a stable state, and how it varies layer by layer. \n\n3. Following the above questions, is there any stage that the sparsity can be fixed without further computation for selection. In generally, the training proceeds for a number of epochs. It would be nice if we can observe some convergence on the selected activations and then we can suspend the selection for saving the computation burden.\n\n[Conclusion]\n\nThis paper present an interesting and novel approach for network pruning in both training and inference. Unlike most of the previous work, it pruning the activations in each layer though a dimension reduction strategy. From the experiments, this method achieved an obvious improvement for reducing the computation memory and time cost in training and inference stages. I think this paper has prompted a new direction of efficient deep neural network.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Dnn compression and acceleration",
            "review": "REVISED: I am fine accepting. The authors did make it a bit easier to read (although it is still very dense). I am also satisfied with related work and comparisons\nSummary: \nThis paper proposes to activate only a small number of neurons during both training and inference time, in order to speed up training and decrease the memory footprint. This works by constructing dynamic sparse graph for each input, which in turn decides which neurons would be used. This happens at each iteration and it does not permanently remove the neurons or weights. To construct this dynamic sparse graph, authors use dimensionality reduction search which estimates the importance of neurons\n\nClarity:\nOverall I found it very hard to follow. Lots of accronyms, the important parts are skipped (the algorithm is in appendix) and it is very dense and a lot of things are covered very shallowly. It would have been better for clarity to describe the algorithm in more details, instead of just one paragraph, and save space by removing other parts.  I would not be able to implement the proposed solution by just reading the paper\n\nDetailed comments.\nThis reminds me a lot of a some sort of supervised dropout. \n\nMy main concern, apart from clarity, is that there is no experimental comparison with any other method. How does it compare with other methods of dnn compression or acceleration?\n\nAlso i found the literature review is somewhat lacking. What about methods that induce sparsity via the regularization, or those that use saliency criterion, hessian based approaches like Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. NIPS, 2015. , pruning filters Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for, efficient convnets. ICLR, 2017.  etc. \nBasically i don't understand how it compares to alternative methods at all.\n\nQuestions:\nHow does it run during inference? does inference stay deterministic (there is a random projection step there)\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Beautiful approximation idea. Can it be implemented efficiently?",
            "review": "This manuscript introduces a computational method to speed up training and inference in deep neural networks: the method is based on dynamic pruning of the compute graph at each iteration of the SGD to approximate computations with a sparse graph. To select which neurons can be zeros and ignored at a given iteration, the approach computes approximate activations using random projections. The approach gives an overall decrease in run-time of 0.8 to 0.6. I believe that its largest drawback is that it does not lead to the same sparsity pattern in a full minibatch, and hence cannot be implemented using matrix-matrix multiplications (GEMM). As a result, the compute-time speed ups are not huge, though the decrease in memory is important. In my eyes, this is the largest drawback of the manuscript: the total computational speed-up demonstrated is not fully convincing.\n\nThe manuscript is overall well written and easy to understand, though I wish that the authors employed less acronyms which forced me to scan back as I kept forgetting what they mean.\n\nThe strength of the paper are that the solution proposed (dynamic approximation) is original and sensible. The limitations are that I am not sure that it can give significant speedups because I it is probably hard to implement to use well the hardware.\n\nQuestions and comments:\n\n1. Can the strategy contributed be implemented efficiently on GPUs? It would have been nice to have access to some code.\n\n2. Fig 8(b) is the most important figure, as it gives the overall convergence time. Is the \"dense baseline\" using matrix-vector operations (VMM) or mini-batched matrix-matrix operation (GEMM)?\n\n3. Can the method be adapted to chose a joint sparsity across a mini-batch? This would probably mean worst approximation properties but would enable the use of matrix-matrix operations.\n\n4. It is disappointing that figure 8 is only on VGG8, rather than across multiple architectures.\n\n5. The strategy of zeroing inputs of layers can easily create variance that slows down overall convergence (see Mensh TSP 2018 for an analysis of such scenario). In stochastic optimization, there a various techniques to recover fast convergence. Do the authors think that such scenario is at play here, and that similar variance-reduction methods could bring benefits?\n\n6. I could not find what results backed the numbers in the conclusion: 2.3 speed up for training. Is this compared to VMM implementations? In which case it is not a good baseline. Is this for one iteration? In which case, it is not what matters at the end.\n\n7. Is there a link between drop-out and the contributed method, for instance if the sparsity was chosen fully random? Can the contributed method have a regularizing effect?\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}