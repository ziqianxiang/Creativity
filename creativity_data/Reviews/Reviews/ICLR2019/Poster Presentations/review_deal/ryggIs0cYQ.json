{
    "Decision": {
        "metareview": "This paper proposes Switchable Normalization (SN) that leans how to combine three existing normalization techniques for improved performance. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Well motivated simple idea and solution that work well in practice"
    },
    "Reviews": [
        {
            "title": "Simple approach that works",
            "review": "This paper considers normalization by learning to average across well-known normalization techniques.\nThis meta-procedure almost by definition can yield better results, and this is demonstrated nicely by the authors.\nThe idea is simple and easy to implement, and easy works in this case.\n\nI would like to hear more about the connections to other meta-learning procedures, by expanding the discussion on page 3.\nThat discussion is very interesting but quite short, and I am afraid I can't see how SN avoids overfitting compared to other approaches. \nAlso, the section on Inference in page 4 is unclear to me. What parameters are being inferred and why is the discussion focused on convergence instead?",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A clear paper with clear contributions",
            "review": "Summary: \n(1) This paper proposes the concept of Switchable Normalization (SN), which learns a weighted combination of three popular/existing normalization techniques, Instance Normalization (IN) for channel-wise, Layer Normalization (LN) for layer-wise, and Batch Normalization (BN) for minibatch-wise.\n(2) Some interesting technical details: a) A softmax is learned to automatically determine the importance of each normalization; b) Reuse of the computation to accelerate.  c) Geometric view of different normalization methods.\n(3) Extensive experimental results to show the performance improvement. Investigation on the learned importance on different parts of networks and tasks.\n\n\nComments:\n\nThe writing of this paper is excellent, and contributions are well presented and demonstrated.\nIt is good for the community to know SN is an option to consider. Therefore, I vote to accept the paper. \n\nHowever, the proposed method itself is not significant, given many existing efforts/algorithms; it is almost straightforward to do so, without any challenges. \n\nHere is a more challenging question for the authors to consider: Given the general formulation of normalization methods in (2), it sees more interesting to directly learn the pixel set I_k. The proposed SN can be considered as a weak version to learn the pixel set: SN employs the three candidates set pre-defined by the existing methods, and learns a weighted combination over the  “template” sets. This is easy to do in practice, and it has shown promising results. A natural idea to learn more flexible pixel set, and see the advantages. Any thoughts?\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Neat motivation and very extensive experiments",
            "review": "In this work, the authors propose Switchable Normalization (SN), which *learns* to switch / select different normalization algorithms (including batch normalization (BN), Instance Normalization (IN), Layer Normalization (LN)) in layers of the networks and in different applications. The idea is motivated by observations (shown in Fig 1) that, 1) different tasks tend to have applied different normalization methods; 2) some normalization methods (e.g. BN) are fragile to very small batch size.\n\nThe authors propose a general form for different normalization methods, which is a Gaussian normalization and then scale and shift by scalars. Different normalization methods utilize different statistics as the mean and the variance of the Gaussian normalization. The authors further propose to learn the combination weights on mean and variance, which is w_k and w'_k in Eqn (3). To avoid duplicate computation, the authors also do some careful simplification on computing mean and variance with all of the three normalization methods.\n\nIn the experiment part, the authors demonstrate the effectiveness of the proposed SN method on various kinds of tasks, including ImageNet classification, object detection and instance segmentation in COCO, semantic image parsing and video recognition. In all of the tasks tested, which also cover the common application in computer vision, SN shows superior and robust performance.\n\nPros:\n+ Neat motivation;\n+ Extensive experiments;\n+ Clear illustration;\n\nCons\n- There are still some experiment results missing, as the authors themselves mentioned in the Kinetics section (but the reviewer thinks it would be ready); \n- In Page 3 the training section and Page 4, the first paragraph, it mentioned Θ and Φ (which are the weights for different normalization methods) are jointly trained and different from the previous iterative meta-learning style method. The authors attribute \"In contrast, SN essentially prevents overfitting by choosing normalizers to improve both learning and generalization ability as discussed below\". The reviewer does not see it is well justified and the reviewer thinks optimizing them jointly could lead to instability in the training (but it did not happen in the experiments). The authors should justify the jointly training part better.\n- Page 5 the final paragraph, the reviewer does not see the point there. \"We would also like to acknowledge the contributions of previous work that explored spatial region (Ren et al., 2016) and conditional normalization (Perez et al., 2017). \"  Please make it a bit more clear how these works are related. ",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}