{
    "Decision": {
        "metareview": "The paper presents quite a simple idea to transfer a policy between domains by conditioning\nthe orginal learned policy on the physical parameter used in dynamics randomization.  CMA-ES then\nfinds the best parameters in the target domain. Importantly, it is shown to work well, \nfor examples where the dynamics randomization parameters do not span the parameters that are\nactually changed, i.e., as is likely common in reality-gap problems.\n\nA weakness is the size of the contribution beyond UPOSI (Yu et al. 2017), the closest work.\nThe authors now explicitly benchmark against this, with (generally) positive results.\nAC: It would be ideal to see that the method does truly help span the reality gap, by seeing working sim2real transfer.\n\nOverall, the reviewers and AC are in agreement that this is a good idea that is likely to have impact.\nIts fundamental simplicity means that it can also readily be used as a benchmark in future sim2real work.\nThe AC recommend it be considered for oral presentation based on its simplicity, the importance of\nthe sim2real problem, and particularly if it can be demonstrated to work well on actual\nsim2real transfer tasks (not yet shown in the current results).\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Simple idea for sim2real fine tuning;  solid results;  with additional actual sim2real results, could be oral"
    },
    "Reviews": [
        {
            "title": "Simple technique with few assumptions for policy transfer. Questions regarding performance and novelty. ",
            "review": "This paper introduces a simple technique to transfer policies between domains by learning a policy that's parametrized by domain randomization parameters. During transfer CMA-ES is used to find the best parameters for the target domain.\n\nQuestions/remarks:\n- If I understand correctly, a rollout of a policy during transfer (i.e. an episode) contains 2000 samples. Hence, 50000 samples in the target environment corresponds to 25 episodes. Is this correct? Does fine-tuning essentially consists of performing 25 rollouts in the target domain?\n- It seems that for some tasks, there is almost no finetuning happening whereas SO-CMA still outperforms domain randomization (Robust) significantly? How can this be explained? For example, the quadruped task (Fig 6a)  has no improvement for the SO-CMA method, yet it is significantly better than the domain randomization result. It seems that during the first episodes of finetuning, domain randomization and SO-CMA should be nearly equivalent (since CMA-ES will be randomly picking parameters mu). A very similar situation can be seen in Fig 5a\n- Following up on my previous question: fig 4a does show the expected behavior (domain randomization and SO-CMA starting around the same value). However, in this case your method does not outperform domain randomization. Any idea as to why this is the case?\n- It's difficult to understand how good/bad the performance of the various methods are without an oracle for comparison (i.e. just run PPO in the target environment). \n- It seems that the algorithm in this work is almost identical to Hardware Conditioned Policies for Multi-Robot (Tao Chen et al. NIPS 2018), specifically section 5.2 in that paper seems very similar. Please comment.\n\nMinor remarks:\n- fig 5.a y-axis starts at 500 instead of 0.\n- The reward for halfcheetah seems low, but this might be due to the custom setup.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novel approach for adapting domain randomization policy for transfer",
            "review": "This paper presents a novel approach for adapting a policy learned with domain randomization to the target domain. The parameters for domain randomization are explicitly used as input to the network learning the policy. When run in the target domain, CMA-ES is used to search over these domain parameters to find the ones that lead to the policy with the best returns in the target domain.\n\nThis approach is a novel one in the space of domain randomization and sim2real work. The results show that it improves over learning robust policies and over one version of doing an adaptive policy (feedforward network with history input). This approach could\n\nThe paper is well written, clearly explained, has clear results, and also explains and evaluates alternate design choices in the appendix.\n\nPros:\n- Demonstrated transfer across simulated environments\n- Outperforms basic robust and adaptive alternatives\n- Straightforward approach\nCons:\n- Requires explicit domain randomization parameters as input to network. This restricts it from applying to work where the simulator is learned rather than parameterized in this way. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting work with promising evaluation. Good evaluation.",
            "review": "The authors propose a policy transfer scheme which in the source domain simultaneously learns a family of policies parameterised by dynamics parameters and then employs an optimisation framework to select appropriate dynamics parameters based on samples from the target domain. The approach is evaluated on a number of simulated transfer tasks (either transferring from DART to MuJoCo or by introducing deliberate model inaccuracies).\n\nThis is interesting work in the context of system identification for policy transfer with an elaborate experimental evaluation. The policy learning part seems largely similar to that employed by Yu et al. 2017 (as acknowledged by the authors). This makes the principal contribution, in the eyes of this reviewer, the optimisation step conducted based on rollouts in the target domain. While the notion of optimising over the space of dynamics parameters is intuitive the question arises whether this optimisation step makes for a substantive contribution over the original work. This point is not really addressed in the experimental evaluation as benchmarking is performed against a robust and an adaptive policy but not explicitly against the (arguably) most closely related work in Yu et al. It could be argued, of course, that Yu et al. essentially use adaptive policy generation but they do explicitly learn dynamics parameters based on recent history of actions and observations. An explicit comparison therefore seems appropriate (or alternatively a discussion of why it is not required).\n\nAnother point which would, in my view, add significant value is explicit discussion of the baseline performances observed in the various experiments. For example, in the hopper experiment (Sec 5.2) the authors state that the baseline methods were not able to adapt to the new environment. Real value could be derived here if the authors could elaborate on why this is the case. The same applies in Sec 5.3-5.6. \n\n(I would add here, as an aside, that I thought the notion in Sec 5.6 of framing the learning of policies for handling deformable objects as a transfer task based on rigid objects to be a nice idea. And not one this reviewer has come across before - though this could merely be a reflection of limited familiarity with the literature).\n\nThe experimental evaluation seems thorough with the above caveat of a seemingly missing benchmark in Yu et al. I would also encourage the authors to add more detail in the experimental section in the main text specifically with regards to number of trials run to arrive at variances in the figures as well as what metric these shaded areas actually signify. \n\nA minor point: the J in equ 1 seems (to me at least) undefined. I suspect that it signifies the expected cumulative reward and was meant to be introduced in Sec 3 where the J may have been dropped from the latex?\n\nIf the above points were addressed I think this would make a valuable and interesting contribution to the ICLR community. As it stands I believe it is marginally below the acceptance threshold.\n\n[ADDENDUM: given the author feedback and addition of the benchmark experiments requested I have updated my score.]\n\n\nPros:\n———\n- interesting work\n- accessible\n- effective\n- thorough evaluation (though potentially missing a key benchmark)\n\nCons:\n———\n- potentially missing a key benchmark (and therefore seems somewhat incremental)\n- only limited insight offered by the authors in the discussion of the experimental results\n- some more details needed with regards to the experimental setup\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}