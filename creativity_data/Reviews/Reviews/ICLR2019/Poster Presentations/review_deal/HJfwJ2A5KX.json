{
    "Decision": {
        "metareview": "The reviewers and AC note that the strength of the paper includes a) an interesting compression algorithm of neural networks with provable guarantees (under some assumptions), b) solid experimental comparison with the existing *matrix sparsification* algorithms. The AC's main concern of the experimental part of the paper is that it doesn't outperform or match the performance of the \"vanilla\" neural network compression algorithms such as Han et al'15. The AC decided to suggest acceptance for the paper but also strongly encourage the paper to clarify the algorithms in comparison don't include state-of-the-art compression algorithms. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "nice contribution -- guarantees+practice",
            "review": "In this work the authors improve upon the work of Arora et al. mainly with respect to one aspect, i.e.,\nThey provide eps-approximation of a fully connected neural network output neuron-wise. The idea of \ncompression is very natural and has been explored by various previous works (key refs are cited). Intuitively,\nthe number of effective parameters is significantly less than the number of parameters in the neural network.\nThe authors introduce the notion of the coreset that is suitable for compressing the weight parameters \nin definition 1. Their main result is stated as Theorem 4. Finally, the authors experiment on standard benchmarks, \nperform a careful experimental analysis (i.e., they ensure fairness of comparison between methods such as \nSVD and the rest).  It would be interesting to see the histogram/distribution of the weights per layer and at an aggregate level\nfor the datasets used.  Also, in the light of the recent results of Arora et al. that show that the signal out of a layer\nis correlated with the top singular values, how would coresets\ndeveloped in the numerical linear algebraic community  (e.g., Near-optimal Coresets For Least-Squares Regression \nby Boutsidis et al.) perform, even as an experimental heuristic compared to the proposed method?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "principled approach to sparsification of neural network weights",
            "review": "The authors propose to reduce the size of fully connected neural networks, defined as the total number of nonzeros in the weight matrices, by calculating sensitivity scores for each incoming connection to a neuron, and randomly keeping only some of the incoming connections with probability proportional to their share of the total sensitivity. They provide a specific definition for the sensitivity scores and establish that the sparsified neural network, with constant probability for any sample from the training population, provides an output that is a small multiplicative factor away from the output of the unsparisfied neural network. The cost of the sparsification is essentially the application of the trained neural network to a small number of data points in order to compute the sensitivity scores\n\nPros:\n- the method works empirically, in that their empirical evaluations on MNIST, CIFAR, and FashionMNIST classification problems show that the drop in accuracy is lower when the neural net is sparsified using their CoreNet algorithm and variations than when it is randomly sparsified or the neural network size is reduced by using SVD.\n- theory is provided to argue the consistency of the sparsified neural network\n\nCons:\n- no comparison is made to the baseline of using matrix sparsification algorithms on the weight matrices themselves. I do not see why CoreNet should be expected to perform empirically better than simply using e.g. the entry-wise sampling scheme from \"Near-optimal entrywise sampling for data matrices\" by Achlioptas and co-authors, or earlier works addressing the same problem of sparsifying matrices.\n- the theory makes very strong assumptions (Assumptions 1 and 2) that are not explained or justified well. Both depend on the specific weight matrices being sparsified, and it isn't clear a priori when the weight matrices obtained from whatever optimization procedure was used to train the neural net will be such that these assumptions hold.\n- despite the suggestions of the theory, the accuracy drop can be quite large in practice, as in the CIFAR panel of Figure 1\n\nI think the ICLR audience will appreciate the attempt to provide a principled approach to decreasing the size of neural networks, but I do not think this approach is widely compelling as :\n(1) no true guaranteed control on the trade-off between accuracy loss and network size is available\n(2) empirically the method does not perform well consistently\n(3) comparisons with reasonable and informative baselines are missing\n\nUpdated in response to author response: the inclusion of experimental comparisons with linear algebraic sparsification baselines, showing that the proposed method can be significantly more accurate, strengthens the appeal of the method.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A subset of the input edges for each neuron is subsampled with probability proportional to the relative importance of each edge.",
            "review": "Given an additively decomposable function F(X, Q) = sum_over_x_in_X cost(x, Q), one can approximate it using either random sampling of x in X (unbiased, possibly high variance), or using importance sampling and replace the sum_over_x with a sum_over_coreset importance_of_a_point * cost(x, Q) which if properly defined can be both unbiased and have low variance [1]. In this work the authors consider the weighted sum of activations as F and suggest that for each neuron we can subsample the incoming edges. To construct the importance sampling strategy the authors adapt the classic notion of sensitivity from the coreset literature. Then, one has to carefully balance the approximation quality from one layer to the next and essentially union bound the results over all layers and all sampled points. The performed analysis is sound (up to my knowledge).\n\nPro:\n- I commend the authors for a clean and polished writeup.\n- The analysis seems to be sound (apart from the issues discussed below)\n- The experimental results look promising, at least in the limited setup.\n\nCon:\n- There exists competing work with rigorous guarantees, for example [2].\n- The analysis hinges on two assumptions which, in my opinion, make the problem feasible: having (sub) exponential tails allows for strong concentration results, and with proper analysis (as done by the authors), the fact that the additively decomposable function can be approximated given well-behaving summands is not surprising. The analysis is definitely non-trivial and I commend the authors for a clean writeup.\n- While rigorous guarantees are lacking for some previous work, previously introduced techniques were shown to be extremely effective in practice and across a spectrum of tasks. As the guarantees arguably stem from the assumptions 1 and 2, I feel that itâ€™s unfair to not compare to those results empirically. Hence, failing to compare to results of at least [2, 3] is a major drawback of this work.\n- The result holds for n points drawn from P. However, in practice the network might receive essentially arbitrary input from P at inference time. Given that we need to decide on the number of edges to preserve apriori, what are the implications?\n- The presented bounds should be discussed on an intuitive level (i.e. the number of non zero entries is approximately cubic in L).\n\nI consider this to be a well-executed paper which brings together the main ideas from the coreset literature and shows one avenue of establishing provable results. However, given that no comparison to the state-of-the-art techniques is given I'm not confident that the community will apply these techniques in practice. On the other hand, the main strength -- the theoretical guarantees -- hinge on the introduced assumptions. As such, without additional empirical results demonstrating the utility with respect to the state-of-the-art methods (for the same capacity in terms of NNZ) I cannot recommend acceptance.\n\n[1] https://arxiv.org/abs/1601.00617\n[2] papers.nips.cc/paper/6910-net-trim-convex-pruning-of-deep-neural-networks-with-performance-guarantee\n[3] https://arxiv.org/abs/1510.00149\n\n\n========\nThank you for the detailed responses. Given the additional experimental results and connections to existing work, I have updated my score from 5 to 6. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}