{
    "Decision": {
        "metareview": "This paper develops a stagewise optimization framework for solving non smooth and non convex problems. The idea is to use  standard convex solvers to iteratively optimize a regularized objective with penalty centered at previous iterates - which is standard in many proximal methods. The paper combines this with the analysis for non-smooth functions giving a more general convergence results. Reviewers agree on the usefulness and novelty of the contribution. Initially there were concerns about lack of comparison with current results, but updated version have addressed this issue.  The main weakness is that the results only holds for \\mu weekly convex functions and the algorithm depends on the knowledge of \\mu. Despite this limitations, reviewers believe that the paper has enough new material and I suggest for publication. I suggest authors to address these issues in the final version. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "ICLR 2019 decision"
    },
    "Reviews": [
        {
            "title": "Novel idea, Like the paper",
            "review": "Summary:\nThe paper presents an analysis and numerical evaluation of stagewise SGD, ADAGRAD and Stochastic momentum methods for solving stochastic non-smooth non-convex optimization problems. \n\nComments:\nI find the ideas presented in this paper very interesting. The convergence analysis seems correct and the paper is reasonably well written, and tackles an important problem. \n\nThe analysis holds for μ-weekly convex functions. This assumption is really important for the development of the algorithm and the proposed analysis. I like the fact that the authors provide two examples showing that popular objective functions in machine learning satisfy this assumption.\n\nThe numerical evaluation is adequate showing the effectiveness  of the proposed stagewise algorithms.  However i have the follow suggestions/minor comments:\n\n1) It will be nice to have also some plots showing the performance of the proposed method on the ImageNet dataset. \n2) Another possible nice experiment will be a comparison of the four stagewise methods (SGD,ADAGRAD,SHB,SNAG) on the same dataset. Which one behaves better? \n\nMinor Comments:\n1) The captions of the figures can be more informative (mention also the division by column). First column is SGD, Second column Adagrad, etc.\n2) Typos: \nSection 1, last bullet point, second line: \"stagwise\"\nSection 5, second paragraph , first line :\"their their\"\npage 8, 3 line from the bottom:  \"seems, indicate\"\n\n2) Missing reference.\nIn the area of stochastic gradient methods with momentum many papers have been proposed recently for the case of convex optimization that worth to be mentioned:\nGadat, Sébastien, Fabien Panloup, and Sofiane Saadane. \"Stochastic heavy ball.\" Electronic Journal of Statistics 12.1 (2018): 461-529.\nLoizou, Nicolas, and Peter Richtárik. \"Momentum and stochastic momentum for stochastic gradient, Newton, proximal point and subspace descent methods.\" arXiv preprint arXiv:1712.09677 (2017).\nLan, Guanghui, and Yi Zhou. \"An optimal randomized incremental gradient method.\" Mathematical programming (2017): 1-49.\n\nOverall, I suggest to accept this paper.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Comparison over related work should be clarified. Measure of convergence rate should be justified",
            "review": "Non-convex optimization is a hot topic since many machine learning problems can be formulated as non-convex problems. In this paper, the authors propose a universal stage-wise algorithm for weakly convex optimization problems. The idea is to add a strongly convex regularizer centered at an iterate of previous stage to the objective function. This builds a convex function which can be optimized by any standard methods in the convex optimization setting. The authors developed convergence rates in expectation in terms of the gradient of envelope. Empirical results are also reported to show the effectiveness of the method.\n\nComments:\n\n(1) The weakly-convex concept considered in this paper is very similar to the bounded non-convexity considered in the paper (Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter) (not cited). In particular, the Natasha paper also developed a multi-stage algorithm for bounded non-convexity optimization problems by adding strongly-convex regularizers centered at iterates of previous stages. The authors should discuss more extensively the related work to clarify their novelty.\n\n(2) The convergence rate is measured by $\\nabla\\phi_\\gamma(x_\\tau)$. However, according to (3) , this only guarantees an upper bound on $\\text{dist}(0,\\partial\\phi_\\gamma(\\text{prox}_{\\gamma\\phi_\\gamma}(x_\\tau)))$. The output of the algorithm is $x_\\tau$ instead of $\\text{prox}_{\\gamma\\phi_\\gamma}(x_\\tau)$. Is it possible to derive an upper bound on $\\text{dist}(0,\\partial\\phi_\\gamma(x_\\tau))$?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting attempt trying to analyze the practical learning rate setting of SGD",
            "review": "In the paper, the authors try to analyze the convergence of stochastic gradient descent based method with stagewise learning rate and average solution in practice. The paper is very easy to follow, and the experimental results are clear. The following are my concerns:\n\n1. In function (3), for any x in R^d, if \\hat x  = prox_\\gamma f (x), then f(\\hat x ) <= f(x). This inequality looks not correct to me. If x = argmin_x f(x), the above inequality is obviously wrong.  It looks like that function (3) is a very important basis for the whole paper.\n \n2. By using the weakly convex assumption and solving f_s, the authors transform a nonconvex nonsmooth problem to a convex problem. However, the paper didn't mention how to select \\gamma in the algorithm. This parameter is nontrivial, if you set a small value, the problem is not convex and the analysis does not hold. In the experiment, the authors tune \\gamma from 1 to 2000, which means that u < 1 or u < 1/2000.  Given neural network is a u-weakly convex problem or u-smooth problem, the theory does not match the experiment. \n\n3. The authors propose a universal stagewise optimization framework and mention that the stagewise ADAGRAD obtains faster convergence than other analysis. My question is that, if it is a generic framework, how about the convergence rate for other methods? is there also acceleration for SGD or momentum SGD? \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}