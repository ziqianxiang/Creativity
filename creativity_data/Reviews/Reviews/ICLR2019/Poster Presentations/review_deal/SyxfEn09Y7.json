{
    "Decision": {
        "metareview": "This paper proposes a new optimization method for ReLU networks that optimizes in a scale-invariant vector space in the hopes of facilitating learning. The proposed method is novel and is validated by some experiments on CIFAR-10 and CIFAR-100. The reviewers find the analysis of the invariance group informative but have raised questions about the computational cost of the method. These concerns were addressed by the authors in the revision. The method could be of practical interest to the community and so acceptance is recommended.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "Meta-review"
    },
    "Reviews": [
        {
            "title": "good paper",
            "review": "The paper proposes SGD for ReLU networks. The authors focuses on positive scale invariance of ReLU which can not be incorporated by naive SGD. To overcome this issue, a positively scale-invariant space is first introduced. The authors show the SGD procedure in that space, which is based on three component techniques: skeleton method, inverse-chain rule, and weight allocation.\n\nThe basic idea, directly optimizing weight in scale invariant space, is reasonable and would be novel, and experiments verify the claim. Readability might be low slightly.\n\nAnalysis about invariance group (e.g., theorem 3.6) is interesting and informative.\n\nCombining with other optimization algorithms (other than simple SGD) method would be valuable.\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "SGD is recast in positively scale-invariant space, showing improvements over training in weight space with low computational overhead.",
            "review": "Summary:\nIn prior deep learning papers it has been observed that ReLU networks are positively scale invariant due to the non-negative homogeneity property of the max(0, x) function. The present paper proposes to change the optimization procedure so that it is done in a space where this invariance is preserved (in contrast to the weight space, where it is not). To do so, the authors define the group G of positive scaling operators, and note that the \"value of path\" (product of weights along the path) is G-invariant and together with \"activation status of paths\" allows for the definition of an equivalence class. They then build a G-space which has fewer dimensions than the weight space, and proposed g-SGD to optimize the network in this space. In g-SGD gradients are computed normally, then projected to G-space via a sparse matrix in order to update the values of paths. The weights are then updated based on a \"weight allocation method\" that involves the inverse projection.\n\nThe authors conduct experiments on CIFAR-10 and -100 with a ResNet-34 and a similarly deep variant of VGG, showing significant benefits from G-SGD training in all cases. They also evaluate the performance of a simple MLP on Fashion-MNIST as a function of the invariant ratio H/m.\n\nComments:\nThe paper is organized well (with technical details of the proofs delegated to the appendix), and discusses the differences in comparison prior work. While evaluations on large-scale datasets would be helpful here, the present experiments suggest that optimization in G-space indeed consistently improves results, so the proposed method seems promising.\n\nFor completeness, it would be great to include Path-SGD results for the CIFAR experiments in Table 1, together with runtime information to highlight the benefits of the g-SGD algorithm and provide experimental proof that the computational overhead is indeed low.\n\nIf the authors are hoping for a wider adoption of the method, it would be helpful for the community to have the g-SGD code released within one of the standard deep learning frameworks.\n\nQuestions:\n- Does it matter which weights are chosen as \"free skeleton weights\"? If these weights never get updated in the optimization procedure, could you please comment on the intuitive interpretation of the necessity of their presence?\n- The text states that the computational overhead of the gradient of path norm is \"very high\". The Path-SGD paper proposes a method costing (B + 1)T, where the overhead an be small for large batches. It would be good to clarify this a bit in the present text.\n- Is the advantage of g-SGD over SGD expected to be proportional to the invariant ratio for CNNs as well?\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "interesting idea, but not convincing exps",
            "review": "This paper proposed a new training algorithm, G-SGD, by exploring the positively scale-invariant space for relu neural networks. The basic idea is to identify the basis paths in the path graph, and convert the weight update in SGD to the weight rescaling in G-SGD. My major concerns are as follows:\n\n1. Empirical significance of G-SGD: While the idea of exploring the structures of relu neural networks for training based on group theory on graphs is interesting, I do not see significant improvement over SGD. The accuracy differences in Table 1 are marginal, training/testing behaviors in Fig. 3 are very similar, and more importantly there is no evidence to support the claims \"with little extra cost\" in the abstract/Sec. 4.3 in terms of computation. Therefore, I do not see the truly contribution of the proposed method.\n\nPS: After reading the revision, I am happy to see the results on computational time that support the authors' claim. However, I still have doubts on the significance of the improvement on CIFAR10 and CIFAR100, because the performance is heavily dependent on network architectures. In my experience, using resnet101 it can easily achieve >96% accuracy. So can you achieve better than this using G-SGD? The training and testing behaviors on both datasets somehow show improvement over SGD, which I take it more importantly than just those numbers. Therefore, I am glad to raise my score.\n\n2. In Alg. 3 I do not quite understand how to apply step 3 to step 4. The connection needs more explanation.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}