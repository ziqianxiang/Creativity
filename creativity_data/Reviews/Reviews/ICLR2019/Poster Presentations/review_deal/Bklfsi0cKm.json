{
    "Decision": {
        "metareview": "This paper builds on a promising line of literature developing connections between Gaussian processes and deep neural networks.  Viewing one model under the lens of (the infinite limit of) another can lead to neat new insights and algorithms.  In this case the authors develop a connection between convolutional networks and Gaussian processes with a particular kind of kernel.  The reviews were quite mixed with one champion and two just below borderline.\n\nThe reviewers all believed the paper had contributions which would be interesting to the community (such as R1: \"the paper presents a novel efficient way to compute the convolutional kernel, which I believe has merits on its own\" and R2: \"I really like the idea of authors that kernels based on convolutional networks might be more practical compared to the ones based on fully connected networks\").  All the reviewers found the contribution of the covariance function to be novel and exciting.\n\nSome cited weaknesses of the paper were that the authors didn't analyze the uncertainty from the model (arguably the reasoning for adopting a Bayesian treatment), novelty in appealing to the central limit theorem to arrive at the connection, and scalability of the model.\n\nIn the review process it also became apparent that there was another paper with a substantially similar contribution.  The decision for this paper was calibrated accordingly with that work.\n\nWeighing the strengths and weaknesses of the paper and taking into account a reviewer willing to champion the work it seems there is enough novel contribution and interest in the work to justify acceptance.\n\nThe authors provided responses to the reviewer concerns including calibration plots and timing experiments in the discussion period and it would be appreciated if these can be incorporated into the camera ready version.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "A neat connection between deep convolutional networks and Gaussian processes"
    },
    "Reviews": [
        {
            "title": "nice direction of research, but limited applicability",
            "review": "The current paper considers the relation between convolutional neural networks and Gaussian processes from theoretical and practical point of view.\n\nThe main contribution of the paper as presented by the authors is 2-fold:\n1. Some theoretical justifications about the correspondence between GPs and convolutional networks with infinitely many channels are provided.\n2. The formulas for GP kernel computation for the considered network is provided and some experiments are conducted (on MNIST).\n\nI personally enjoy the ideas of the close relation between certain types of neural networks and GPs and I really like the idea of authors that kernels based on convolutional networks might be more practical compared to the ones based on fully connected networks. It might be easier to encode certain invariance for complex objects via multilayered structures then via more simple explicit kernels.\n\nHowever, I see couple of important issues:\n1. The theoretical justification provided is basically heuristic argument and, speaking rigorously, is not a theorem. The proper proof should be based not on layer-by-layer convergence, but on the convergence with all the parameters tending simultaneously to infinity (see Matthews et al, 2018). Also, I doubt that the limit with infinite number of channels is as meaningful as the limit with infinite width of layer, as wide networks are used much more often in practice than networks with many channels.\n2. The practical applicability is very limited as the kernel obtained has very high computational complexity. The authors theirselves comment that computing kernel matrix takes more time than inverting it. Thus, the applicability beyond MNIST is a big question for the proposed approach.\n\nTo sum up, I think that the present paper targets an important direction of work, but the contribution itself is somehow limited (and relative obvious based on the recent papers on relation between fully connected networks and GPs).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "GP kernel inspired by CNNs outperforms previous non-parametric approaches",
            "review": "This paper\n\n1) extends an argument for the GP behaviour of deep, infinitely-wide fully-connected networks to convolutional and residual deep neural networks with infinitely many channels and\n2) provides a computationally tractable approach to compute the corresponding GP kernel. This kernel has few hyper-parameters, and achieves state-of-the-art results on the MNIST dataset. \n\nWhile point (1) is a relatively straightforward adaptation of Lee et. al (2017) and Matthews et al. (2018) to a different network structure, point (2) is original and non-trivial. All in all, I think this paper makes a significant contribution that I believe will spark interesting follow-up work (hinted at in the last section of the paper).\n\nQuestions:\n\n- In my understanding, the kernels of Section 3 do not require the weight matrices W to share the same values across rows. Accordingly, their performance cannot necessarily be explained by properties of convolutional filters (in particular translation invariance). Can the authors comment on that?\n- What would be the performance of a parametric CNN trained with SGD that matches the architecture (# layers) & the squared loss function of ResNet GP? The only point of comparison is Chen & al. (2018), which I suppose optimizes a log loss? Specifically, I would like to understand the impact of the loss function and of the number of layers on the relative performance of the two approaches.\n\nThe paper is clear and easy to follow. A few suggestions:\n\n- I recommend turning the argument in section 2.2 into a formal, self-contained theorem that states a result on A_L, defined in eq. 17 (which I would move to the main text). This would make the precise claim easier to understand.\n- I suggest including a more thorough discussion of the results. Table 1 is only introduced in the related work section.\n- If space is a concern, I would move part of Section 2.2 outside of the main text, since it mostly follows Lee et al. & Matthews et al\n\nSmall questions/comments:\n\n- Eqs 1 and 2: b_j should be multiplied by the all-ones vector, just like in (5) and (6).\n- Below eq. 5: \"while the *elements of the* feature maps themselves display...\"\n- Paragraph above eq. 7: \"in order to achieve an output suitable for *binary* classification or *univariate* regression\"\n- Paragraph above eq. 7: \"if we only need the covariance at *certain* locations in the outputs...\"\n- Algorithm 1: you might want to add a loop over g for clarity",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An efficient convolutional kernel for GPs without proper evaluation of probabilistic predictions ",
            "review": "This paper shows that deep convolutional networks (CNNs, without pooling) with a suitable prior over weights can be seen as shallow Gaussian processes (GPs) with a specific covariance function. It shows that this covariance function can be computed efficiently (when compared to previous attempts at resembling convolutional networks with GPs), with a cost that only depends linearly on the number of layers and the input dimensionality, i.e.~O(N^2 L D). \n\nTo show the equivalence between deep CNNs and shallow GPs, the paper uses similar ideas to those proposed by Matthews et al (2018a) and Lee et al (2017), i.e. using the multivariate central limit theorem in very large networks, where in the case of this paper the limit is taken as the number of channels at each layer goes to infinity. Therefore, from a theoretical perspective, these ideas have been proposed before. However, the paper presents a novel efficient way to compute the convolutional kernel, which I believe has merits on its own. \n\nHowever, the model setting for classification (where deep CNNs have been successful) and consequent evaluation on the MNIST dataset is less than convincing. One of the main motivations for Bayesian CNNs and GPs (and the paper argue for this in the intro) is to be able to provide good uncertainty estimates. However, the classification problem is framed in a regression setting, where neither probabilistic estimates are evaluated or even provided. Indeed, only the error rate is given on Table 1. To me, this is certainly not enough for a Bayesian/GP method and it is a critical deficiency of the paper in its current form. While I understand having a non-Gaussian likelihood will complicate things and conflate the kernel contribution with the approximations, I believe it is necessary to provide and evaluate such probabilistic estimates and compare them to other GP approaches (even using other less than satisfying methods such as calibration/scaling). Along a similar vein, it is unclear what objective function was used for hyper-parameter learning but, given that the authors actually “sample hyper-parameters”, I am guessing a proper probabilistic objective such as the marginal likelihood is out of the question.\n\nOther (perhaps minor) deficiencies is that the method is not scalable to large datasets (I am even surprised the authors managed to run this on full MNIST) and that no theoretical analysis is done (e.g. as in Mattews et al, 2018a). \n\nMinor comments:\n\n* In the intro, “Other methods such as Gaussian Processes”: GPs are not a method and I believe the authors really mean here Gaussian process regression. \n* The prior variance over filters in Eq (3) divides over the number of channels.  Why does a Gaussian prior with infinite precision make sense here?\n* The authors should report the state of the art of using GPs for MNIST classification using non-convolutional kernels).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}