{
    "Decision": {
        "metareview": "This paper proposes an image to image translation technique which decomposes into style and content transfer using a semantic consistency loss to encourage corresponding semantics (using feature masks) before and after translation. Performance is evaluated on a set of MNIST variants as well as from simulated to real world driving imagery. \n\nAll reviewers found this paper well written with clear contribution compared to related work by focusing on the problem when one-to-one mappings are not available across two domains which also have multimodal content or sub-style. \n\nThe main weakness as discussed by the reviewers relates to the experiments and whether or not the set provided does effectively validate the proposed approach. The authors argue their use of MNIST as a toy problem but with full control to clearly validate their approach. Their semantic segmentation experiment shows modest performance improvement. Based on the experiments as is and the relative novelty of the proposed approach, the AC recommends poster and encourages the authors to extend their analysis of the current results in a final version.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "New technique for semantic consistency for transfer across heterogenous domains with preliminary empirical evidence"
    },
    "Reviews": [
        {
            "title": "Interesting and well-written paper, but needs some clarification on the experiments",
            "review": "The paper is well organized with a clear idea of the proposed method and good related work descriptions. Overall, the descriptions are clear and easy to follow, but the experimental results need clarifying.\n\n- Regarding the multi-digit translation task, it is not straightforward to this reviewer how the proposed method could match the digits (semantic) with different colors (style) in different locations. The description in the paper is not enough to explain the results in Fig. 6. To this reviewer, this task is more complex than the street view translation one. In the same line, it is curious what the results would be if digits with different colors are overlapping at random location, rather than the grid-like arrangement.\u0000\n\n- For the potential readers who are not knowledgeable in semantic segmentation, please give the full name of mIoU for reference. \n\n- For further researches in this topic, it would be good to depict the limitations of the proposed method. For examples, the translated images in the CelebA dataset are not photorealistic (Fig. 8)  and there are odd red lights in the middle of the results in GTA5<-BDD (Fig. 12).\n\n- typos: Fig. 2-caption: m_{a}->m_{A}",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Justifying style transfer via conditioning needs more analysis.",
            "review": "The introduction is written to perfection. The paper discusses a core failing and need for I2I translation models. The one-to-one mapping assumption does not apply to most tasks. While the approach seems novel the analysis of the results are insufficient to convince me that the method is really working. This should be a workshop paper.\n\nFor the motivation of the approach I am not convinced how the conditioned style is being used. It would be nice to see some analysis of how the latent space changes given different input images. Why would style information be propagated through the network? Why wouldn't noise work just as well? Although an abiliation study is performed there is no standard deviation reported so it is unclear if this number is fair. \n\nIn Figure 5 the t-sne doesn't look correct. The points all seems to be projected on walls which could indicate some sort of overflow error. The text only devotes 3 lines to discuss this figure. It is not mentioned what part of the model the t-sne is computed from. To me this experiment that studies the internal representation is critical to convincing a reader to use this method. \n\nThe segmentation results sound good. Where is the improvement coming from? The experimental section is cut short. The experiment section is really squeezed in the last two pages while the other sections are overly descriptive and could be reduced.\n\nThe figures should be changed to be visible without color (put a texture on each block).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "interesting submission with good intuition and evaluation",
            "review": "I enjoyed reading this manuscript. The paper is based on a simple idea used by others as well (i.e., the image has two components, one  that encode content which is shared across domains and another one characterizing the domain specific style). The other important idea is the use of feature masks that steer the translation process without requiring semantic labels. This is similar to attention models used by others but I think it is novel when applying to this specific application domain. I was a bit disappointed by the evaluation part. The authors decided to perform ablation and to show the importance of each component using only the MNIST-Single dataset. While this is good as a toy example I would have expected to see such analysis on a more complex example, e.g., street-view translation. This is also surprising considering that it is not even present in the supplementary material. Overall, this is a solid submission with interesting ideas and good implementation.   ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}