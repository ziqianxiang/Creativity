{
    "Decision": {
        "metareview": "After much discussion, all reviewers agree that this paper should be accepted. Congratulations!!",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Meta-Review for Interpolation-Predictions paper"
    },
    "Reviews": [
        {
            "title": "Refreshingly simple approach to irregular data but limited novelty, flawed writing, uninspiring results",
            "review": "I have mixed feelings about this submission, and as such, I look forward to discussing it with both the authors and my fellow reviewers. In short, I like the simplicity of the idea, but I am uncertain about the degree to which it satisfies ICLR's novelty criterion (\"present substantively new ideas or explore an underexplored or highly novel question\"); I do feel confident that some ICLR readers would (perhaps unfairly) describe this approach as \"obvious.\" The paper's presentation suffers, and it fails to communicate essential details clearly. Finally, for folks familiar with healthcare data and MIMIC-III specifically, the results are underwhelming: yes, the proposed approach beats (the authors' own implementations of) baselines, but it underperforms other published results on the MIMIC-III 48-hour mortality task ([1][2] report AUCs of 0.87 or higher). As such, I am assigning the paper a \"weak accept\" to communicate my ambivalence and reserve the right to adjust it up or down after discussion.\n\nSUMMARY\n\nThis paper proposes an \"interpolation layer\" to resample irregularly sampled time series before feeding them into a neural net architecture. The interpolation layer consists of parametric kernels, e.g., radial basis functions, configured to estimate the values of input time series at reference time points based on univariate temporal and then multivariate correlations. The outputs include smooth and transient interpolated values (controlled by kernel bandwidth) and counts (referred to as intensity) at each reference point. As far as I understand, this model can be trained end-to-end. The paper also proposes a simple strategy for combatting overfitting (add an autoencoder and reconstruction error term to the objective in combination with a heuristic in which some points are masked as inputs and must be interpolated from non-masked points). In experiments on two data sets (UWaveGesture and a medical data set) and two tasks (classification and regression) this approach outperforms the main competing approaches [3][4][5][6] in most contexts.\n\nBelow I provide a list of strengths, weaknesses, and general questions or feedback.\n\nSTRENGTHS\n\n- I applaud the simplicity of the idea: this much simpler framework leverages many of the intuitions behind the GP adapter framework (GP-GRU) [4][5] with comparable performance and appears to train orders of magnitude faster (caveat: on one data set and task)\n- It likewise outperforms both commonly used preprocessing (GRU-F) [2][3] and the much more complicated neural net architecture (GRU-HD) from [6] (across two datasets and tasks)\n- The simplicity of this approach probably lends itself to additional customization and innovation\n- The literature review seems quite thorough and does an especially nice job of covering recent work on RNNs for multivariate time series and irregular sampling or missing values\n- The experiments are thorough and well-designed overall. The authors use two data sets and two tasks (classification and regression). More data sets and tasks is always nice, but even two is pretty laudable (many authors might settle on just one given the experimental and computational effort required for these experiments). They include and beat or outperform two baselines that can justifiably be called state-of-the-art (GP-GRU and GRU-HD).\n\nI think a relatively safe takeaway is that for irregularly sampled data, this approach is is preferable to both heuristic preprocessing and more complex models. That seems like a not insignificant finding in empirical machine learning for messy time series data.\n\nWEAKNESSES\n\n- Section 3 is possibly the most critical section (since it describes the contribution) but is hard to follow: I don't envy the authors the task of explaining a variable with two superscripts and three subscripts (Equation 1), but it IS their paper, so it's on them to do it. See feedback section for other examples.\n- Although I consider the related work well done, I can't help but wonder if there isn't older work on RBFs, etc., that might have been missed (I mostly want to encourage the authors to look once more and then come back and tell me I'm wrong).\n- The MIMIC-III experiments omit the GP-GRU model, which weakens the results by leaving the reader to imagine how it might compare (I would expect it to outperform the proposed approach by an even wider margin than it did for UWave).\n- I am sympathetic to the idea of fixing certain architectural choices, e.g., layers and units in the GRU and number of inducing points, across all models because it (a) gives the appearance of a \"fair comparison\" and (b) reduces burden of effort, but I do not agree that it yields a truly fair comparison. The GRU-* model performance on UWave is suspiciously bad, suggesting severe overfitting and the possibility that the models are overparameterized. It leaves the reader wondering if the architectural choices happen to be optimal for the proposed model only (whether by accident or design). A truly fair comparison requires independently tuning hyperparameters for each model.\n- Although the proposed approach outperforms baselines in these experiments, the overall results are underwhelming in the wider context of recent work using MIMIC-III. Multiple publications have reported AUCS of 0.87 [1][2] or higher for 48-hour risk of mortality (it is difficult to compare the LOS results since different papers use different units). Of course, the experiments use different cohorts and variables so they're not directly comparable, but it nonetheless diminishes the potential impact of the results presented here.\n\nFEEDBACK AND QUESTIONS\n\n- I had to read 3.2.1 multiple times to understand the relationships between the different \"layers\" in the interpolator, and I'm still not sure what the relationship is between the smooth and transient kernels or exactly how the intensity values are estimated (are they just windowed counts or weighted sums?).\n- I'm also not 100% clear on (a) which parameters (if any) in the interpolator are optimized during end-to-end learning and which are just fixed or tuned as hyperparameters. This should be stated clearly and even better, I'd recommend writing down the gradient update rules for the interpolator parameters (you can put them in the appendix).\n- Since the model uses global structure for interpolation and requires pre-specifying the number of inducing points, could it be used to make continuous predictions (and how?), e.g., forecast mortality at each hour?\n- On a related note, if the number of inducing points is pre-specified, can the model be applied to sequences of different length?\n- How does performance depend on choice of number of inducing points?\n- How does the proposed approach handle time series that are missing entirely, e.g., if no pH values are measured?\n- What does Table 3 in the appendix mean by \"missingness?\" Given that the paper is concerned with irregular sampling (not missing data), I would expect statistics on sampling rates, not missingness...\n- Why derive your own MIMIC-III subset and tasks rather than use one of several pre-existing benchmarks (both of which include more variables and tasks) [1][2]?\n- FYI: the Che, et al., 2016, paper on missing values [6] has been published in JBIO, so you should cite that version.\n\nREFERENCES\n\n[1] Purushotham, et al. \"Benchmark of Deep Learning Models on Large Healthcare MIMIC Datasets.\" arXiv preprint arXiv:1710.08531 (2017)\n[2] Harutyunyan, et al. \"Multitask learning and benchmarking with clinical time series data.\" arXiv preprint arXiv:1703.07771 (2017)\n[3] Lipton, Kale, and Wetzel, 2016\n[4] Li and Marlin, 2016.\n[5] Futoma, et al., 2017.\n[6] Che, et al., 2016. <-- new JBIO 2018 version!",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Possibly, simple yet effective solution to handle time series data with  missing values",
            "review": "Summary:\nThe authors propose a framework for making predictions on a sparse, irregularly sampled time-series data. The proposed model consists of an interpolation module, and the prediction module, where the interpolation module models the missing values in using three outputs: smooth interpolation, non-smooth interpolation, and intensity. The authors test the proposed method on two different datasets (MIMIC-III and UWave), although only one of the datasets are multi-variate. The proposed method shows comparable training time to other GRU variants, and outperforms all baseline models for mortality prediction and length-of-stay prediction.\n\nPros:\n- Possibly, simple yet effective solution to handle time series data with missing values.\n- I appreciate the thorough survey of the related works.\n\nIssues:\n- My biggest concern is that the authors spend some time to address the disadvantage of discretizing the timeline when modeling missing values (5th paragraph of section 2) and emphasize how their method does not have such limitation. But it seems that, when using the proposed method, the user still needs to pre-define evenly spaced reference points r_1, r_2, ..., r_T. So there is still this dilemma how dense you want the reference points to be. And I couldn't find the values used for the reference points in the experiments section. It's quite possible that one of the baselines can outperform the proposed method with different reference points, given that the evaluation scores overlap with each other wrt standard deviation ranges.\n- Method description in section 3.2.1 is quite confusing. I could follow until Eq.2, but afterwards, the first interpolants (x^{21}) and the second interpolants (x^{12}) become very confusing. It would have been helpful if the authors explicitly described what the interpolation channel 'c' was before talking about the interpolants. \n- \"taking into account learned correlations\" in page 5: I suggest changing that to \"taking into account learnable/trainable correlations\" since \"learned correlations\" gives the impression that the correlations were already learned prior to training the model.\n- Can the authors test the proposed method on logistic regression (LR) and multi-layer perceptron (MLP)? It would be interesting to see if the proposed method improves the performance of LR and MLP.\n\nAfter considering the author feedback and their effort to address my concerns, I've decided to raise my rating to 6. Thank you for the hard work.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting but still immature solutions to a critical issues in EHRs",
            "review": "In the submitted manuscript, the authors introduce a novel deep learning architecture to solve the  problem of supervised learning with sparse and irregularly sampled multivariate time series, with a specific interest in EHRs. The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network, and it is tested on two classification/regression tasks.\n\nThe manuscript is interesting and well written: the problem is properly located into context with extensive bibliography, the method is sufficiently detailed and the experimental comparative section is rich and supportive of the authors’ claim. However, there are a couple of issues that need to be discussed: \n\n\t▪\tthe reported performances represent only a limited improvement over the comparing baselines, indicating that the proposed model is promising but it is still immature\n\t▪\tthe model is sharing many characteristics with (referenced) published methods, which the proposed algorithm is a smart combination of - thus, overall, the novelty of the introduced method is somewhat limited.\n\n\n######### \n\nAfter considering the proposed improvements, I decided to raise my mark to 6. Thanks for the good job done!\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}