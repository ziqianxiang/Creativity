{
    "Decision": {
        "metareview": "This paper examines ways of encoding structured input such as source code or parsed natural language into representations that are conducive for summarization. Specifically, the innovation is to not use only a sequence model, nor only a tree model, but both. Empirical evaluation is extensive, and it is exhaustively demonstrated that combining both models provides the best results.\n\nThe major perceived issue of the paper is the lack of methodological novelty, which the authors acknowledge. In addition, there are other existing graph-based architectures that have not been compared to.\n\nHowever, given that the experimental results are informative and convincing, I think that the paper is a reasonable candidate to be accepted to the conference.",
        "confidence": "2: The area chair is not sure",
        "recommendation": "Accept (Poster)",
        "title": "Minor novelty, extensive and informative empirical comparison"
    },
    "Reviews": [
        {
            "title": "Interesting idea and promising results",
            "review": "This paper presents a structural summarization model with a graph-based encoder extended from RNN. Experiments are conducted on three tasks, including generating names for methods, generating descriptions for a function, and generating text summaries for news articles. Experimental results show that the proposed usage of GNN can improve performance by the models without GNN. I think the method is reasonable and results are promising, but I'd like to see more focused evaluation on the semantics captured by the proposed model (compared to the models without GNN).\n\nHere are some questions and suggestions:\n\n- Overall, I think additional evaluation should be done to evaluate on the semantic understanding aspects of the methods. Concretely, the Graph-based encoder has access to semantic information, such as entities. In order to better understand how this helps with the overall improvement, the authors should consider automatic evaluation and human evaluation to measure its contribution. Also from fig. 3, we can see that all methods get the \"utf8 string\" part right, but it's hard to say the proposed method generates better description. \n\n- In the last table in Tab. 1, why the authors don't have results for adding GNN for the pointer-generator model with coverage?\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A straightforward improvement for abstractive summarization",
            "review": "STRUCTURED NEURAL SUMMARIZATION\n\nSummary:\n\nThis work combines Graph Neural Networks with a sequential approach to abstractive summarization across both natural and programming language datasets. The extension of GNNs is simple, but effective across all datasets in comparison to external baselines for CNN/DailyMail, internal baselines for C#, and a combination of both for Java. The idea of applying a more structured approach to summarization is well motivated given that current summarization methods tend to lack the consistency that a structured approach can provide. The chosen examples (which I hope are randomly sampled; are they?) do seem to suggest the efficacy of this approach with that intuition.\n\nComments:\n\nShould probably cite CNN/DailyMail when it is first introduced as NLSummarization in Section 2 like you do the other datasets.\n\nCan you further elaborate on how your approach is similar to and differs from that in Marcheggiani et al 2017 on Graph CNNs for Semantic Role Labeling, Bastings et al 2017 on Graph Convolutional Encoders for Syntax-aware Machine Translation, and De Cao et al 2018? Why should one elect to go the direction of sequential GNNs over the GCNs of those other works, and how might you compare against them? I would like to see some kind of ablation analysis or direct comparison with similar methods if possible.\n\nWhy would GNNs hurt SelfAtt performance on MethodDoc C# SelfAtt+GNN / SelfAtt?\n\nWhy not add the coverage mechanism from See et al 2017 in order to demonstrate that the method does in fact surpass that prior work? I'm left wondering whether the proposed method's returns diminish once coverage is added.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Limited novelty and missing some key experiments",
            "review": "Note: I changed my original score from 4 to 7 based on the new experiments that answer many of the questions I had about the relative performance of each part of the model. The review below is the original one I wrote before the paper changes.\n\n# Positive aspects of this submission\n\n- The intuition and motivation behind the proposed model are well explained.\n\n- The empirical results on the MethodNaming and MethodDoc tasks are very promising.\n\n# Criticism\n\n- The novelty of the proposed model is limited since it is essentially adding an existing GGNN layer, introduced by Li et al. (2015), on top of an existing LSTM encoder. The most important novelty seems to be the custom graph representation for these sequence inputs to make them compatible with the GGNN, which should then deserve a more in-depth study (i.e. ablation study with different graph representations, etc).\n\n- Since you compare your model performance against Alon et al. on Java-small, it should be fair to report the numbers on Java-med and Java-large as well.\n\n- The \"GNN -> LSTM+POINTER\" experiment results are reported on the MethodDoc task, but not for MethodNaming. Reporting this number for MethodNaming is essential to show the claimed empirical superiority of the hybrid encoder compared to GNN only.\n\n- I have doubts about the usefulness of the proposed model for natural language summarization, for the following reasons:\n\n    - The comparison of the proposed model for NLSummarization against See et al. is a bit unfair, since it uses additional information through the CoreNLP named entity recognizer and coreference models. With the experiments listed in Table 1, there is no way to know whether the increased performance is due to the hybrid encoder design or due the additional named entity and coreference information. Adding the entity and coreference data in a simpler way (i.e. at the token embedding level with a basic sequence encoder) in the ablation study would very useful to answer that question.\n\n    - In NLSummarization, connecting sentence nodes using a NEXT edge can be analogous to using a hierarchical encoder, as used by Nallapati et al. (\"Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond\", 2016). Ignoring the other edges of the GNN graph, what are the theoretical and empirical advantages of your method compared to this sentence-level hierarchical encoder?\n\n    - Adding the coverage decoder introduced by See et al. to your model would have been very useful to prove that the current performance gap is indeed due to the simplistic decoder and not something else.\n\n- How essential is the weighted averaging for graph-level document representation (Gilmer et al. 2017) compared to uniform averaging?\n\n- A few minor comments about writing:\n    - In Table 1, please put the highest numbers in bold to improve readability\n    - On page 7, the word \"summaries\" is missing in \"the model produces natural-looking with no noticeable negative impact\"\n    - On page 9, \"cove content\" should be \"core content\"\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}