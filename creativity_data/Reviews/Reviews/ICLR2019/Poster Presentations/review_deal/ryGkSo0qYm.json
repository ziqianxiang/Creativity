{
    "Decision": {
        "metareview": "The paper is proposed as probable accept based on current ratings with a majority accept (7,7,5).",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Probable accept based on majority vote."
    },
    "Reviews": [
        {
            "title": "Neat contribution",
            "review": "Learning graphs from data fine tunes standard similarity graph constructions such as k-nearest neighbor graphs.  \nThere has been a line of research works that focuses on learning graphs and that shows that this results in superior\nresults in various machine learning tasks.  The current state-of-the-art method is the method proposed by Kalofolias, \nwhich however is slow.   The authors suggest a method to avoid searching for the parameters that achieve a desired\nlevel of sparsity by providing closed a formula. The parameter that determines the sparsity is theta, see proposition 1 on page 4. This was originally shown by Kalofolias. To achieve their goal, the authors first consider the degree of any given node by looking at equation (8), page 4. They prove theorem 1, that is intuitive and  provides the form of the optimal \nweights that connect this node to the rest of the nodes in the graph.  The proof is based on applying the KKT conditions on\nthe objective (8), with the single constraint that there are no negative weights.  Finally, since we care about the \nsparsity of the graph as a whole, the authors use the average of the parameter theta over all nodes. The authors perform \nexperiments on real-world graphs, and show basic properties of their method, as well as the main source of mistakes ,i.e., disconnected nodes, figure 5.\n\nEssentially, this paper starts from the work of Kalofolias  and improves it significantly. This by itself is \na neat contribution, but the authors could improve their paper by showing a more complete view  of graph \nlearning methods, with respect to the quality of the produced graphs and the scalability. I find this aspect of the paper narrowing its contribution, hence my evaluation. Some remarks follow.\n\n- A different family of graph learning methods is based on the objective ||LX||_F^2 or equivalently tr(X^TLLX). \nFor this objective, Daitch et al. proved certain neat properties, such as the existence of a sparse optimal graph. \nThis allows Daitch et al. to solve the primal dual significantly faster than O(n^2) since by their theorem, \nO(nd) edges are required where d is the dimension of the data points. When d is large, a random projection can be applied. \nThe paper should compare with this family of methods that are more scalable both with respect to the accuracy, \nand to the runtimes. \n\n- While the proposed method scales significantly better than Kalofolias, the datasets used are small. \n\n- Using LSH for k-nn graphs results in a  scalable, practical way to construct similarity graphs. The authors should cite\nthe following related work, and compare with such methods.\n“Efficient K-Nearest Neighbor Graph Construction for Generic Similarity Measures“ by Dong, Charikar, Li. \n\n- An interesting experiment would be to inject outliers in the dataset, or use some dataset with outliers. \nWould this affect the tightness of the interval in equation (17)? ",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "marginally bellow threshold",
            "review": "The paper proposes a scalable approximate calculation of graph construction. Based on the sparse optimization formulation of a graph construction, the authors provide a way to select parameter automatically based on user desired connectivity of graph.\n\nThe problem setting, graph construction, is significant for the wide range of ML community. Overall, however, advantage/novelty of the proposed method is unclear for me.\n\nScalability is main advantage of the proposed method, but the authors just employed known nearest neighbor approximation methods, and thus here no technical novelty is shown.\n\nI couldn't find connection between Section 3 and 4, these seem to be an independent topics. Main claim of the paper would be in Section 3, but the novelty would be weak as mentioned above.\n\nSolving reverse problem is interesting, but it just provide the parameter value range which results in given sparsity level k. This doesn't provide exact value of \\theta (and user still have to specify k), and selection would be possible easily without the analytical formula (e.g., by following the regularization path)\n\nPerformance verification is not convincing. Showing accuracy gain for more wide variety of datasets would be convincing.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good paper, very well written",
            "review": "This paper proposed an approximation technique to learn the large-scale graph with the desired edge density. It was well-written and contains thorough experimental results and analysis.\n\nA minor drawback is that while this work was motivated by the use of k-NN graph in graph convolution network (GCN), there was no evidence on how well A-NN performs in compare to k-NN with GCN.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}