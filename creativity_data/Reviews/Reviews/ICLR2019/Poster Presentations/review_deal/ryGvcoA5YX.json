{
    "Decision": {
        "metareview": "This paper presents a promising model to avoid catastrophic forgetting in continual learning. The model consists of a) a data generator to be used at training time to replay past examples (and removes the need for storage of data or labels), b) a dynamic parameter generator that given a test input produces the parameters of a classification model, and c) a solver (the actual classifier). The advantages of such combination is that no parameter increase or network expansion is needed to learn a new task, and no previous data needs to be stored for memory replay.\n\nThere is reviewer disagreement on this paper. AC can confirm that all three reviewers have read the author responses and have significantly contributed to the revision of the manuscript.\n\nAll three reviewers and AC note the following potential weaknesses: (1) presentation clarity needed substantial improvement. Notably, the authors revised the paper several times while incorporating the reviewers suggestions regarding presentation clarity. R2 has raised the final rating from 4 to 5 while retaining doubts about clarity. \n(2) weak empirical evidence: evaluation with more than three tasks and using more recent/stronger baseline methods would substantially strengthen the evaluation (R2, R3). AC would like to report the authors added an experiment with five tasks and provided a verbal comparison with \"Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence\", ECCV-2018 by reporting the authors results on the MNIST dataset. \n(3) as noted by R2, an ablation study of different model components could strengthen the evaluation. The authors included such ablation study in Table 4 of the revised paper. \n(4) reproducibility of the model could be difficult (R1). In their response, the authors promised to make the code publicly available.\n\nAC can confirm that all three reviewers have contributed to the final discussion. Given the effort of the reviewers and authors in revising this work and its potential novelty, the AC decided that the paper could be accepted, but the authors are strongly urged to further improve presentation clarity in the final revision if possible.\n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "Meta-Review"
    },
    "Reviews": [
        {
            "title": "paper needs major revision to improve clarity and empirical validation",
            "review": "This paper proposes a method for continual learning. The model has three components: a) a data generator to be used at training time to replay past examples, b) a parameter generator that takes the input observation to produce parameters for c) the actual classifier. The authors demonstrate the method on simple datasets with a stream of 2 or 3 tasks.\n\nStrenghts:\n- the combination of components is novel\n- the method does not rely on task descriptors neither at training nor test time\nWeaknesses:\n- the paper needs a major rewrite to improve fluency and to better organize and describe the proposed approach\n- the empirical validation is weak.\n\nRelevance\nLearning in a continual setting is certainly very relevant for this venue.\n\nNovelty\nWhile each component is by itself not very novel (replay methods for continual learning have already been used, networks predicting parameters have also become a fairly common approach in meta-learning literature), the proposed combination is novel in this sub-field.\n\nClarity\nClarity is very poor and definitely does not meet the acceptance bar for this conference. I believe that the authors would need to make a major revision to address this issue. While ICLR allows authors to revise papers, I think the revision needed to fix this draft goes beyond the acceptable limit, as reviewers would then need to make a whole new revision.\nFirst, fluency is very poor. There are lots of grammatical errors (see first sentence of introduction \"neural networks suffers..\"),  a plethora of un-necessary acronyms which force the reader to go back and forth to figure out what they refer to (MA, DG, DPG, DPG&S, ...), and several sentences are not well formed (e.g., read first sentence of introduction).\nSecond, some statements are contradictory; e.g., the authors define \"basic unit\" as \"simple MLP with one hidden layer\", but then say it \"is an activation function plus a matrix transformation\"..\nThird, graphics and formulas are too small and not legible.\nFourth, the organization of the paper is poor, it is very wordy yet vague. For instance, the authors should precisely describe how the data generator is trained in sec. 2.3. The authors should provide an algorithm summarizing how the different components interplay both at training and test time. At present, I am making educated guesses about how this system works.\nFor instance, how are real and generated examples interleaved? how is forgetting prevented in the data generator?   \n\nReferences to prior work\nWhile there are lots of references in sec. 4, they are not sufficiently well described - see third paragraph of sec. 4 where the authors cite almost 20 papers by simply saying they are \"some other approaches\". \nAlso, I did not find mention to methods predicting parameters in the meta-learning community but also others like:\nDenil et al. \"Predicting network parameters in deep learning\" NIPS 2013\n \nEmpirical validation\nThe empirical evaluation does show an advantage of the proposed approach on some simple streams composed by up to three tasks. However, a) the tasks are really simple because of the small number of tasks considered and b) the baselines are weak. For instance, EWC is now a bit out-dated as there are variants that work a bit better, like:\nChaundry et al. \"Riemannian Walk ...\" ECCV 2018\nand there are other methods like \"Progress and Compress\" Schwartz ICML 2018 the authors could have compared against.\nBesides, the authors do not mention anything about memory and time cost both at training and test time, possibly including the time to cross validate all the hyper-parameters of this method.\nOverall, I am left with the sense that the proposed approach will be hard to scale to many more tasks and more realistic images (for which we do not quite know how to train efficiently and well generators).\n\nOther comments\nI did not find the formalization in eq. 9 very useful. The first and last term in that equation can be very big and there is no sense of how lose this bound is.\nAlso, it is not clear whether there is a general principle to partition the set of parameters (to determine which ones should be shared).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Unclear description of algorithm, experiments could be improved. ",
            "review": "Summary: \n- In this paper, an algorithm to improve the catastrophic forgetting of the model is proposed. The key idea consists of 1) introducing the dynamic parameter generator (DPG) for \"model's adaptation\" to data at test time and 2) data generator (DG) for remembering previously trained dataset. \n\nPros: \n- Empirical results seem strong. The proposed result outperforms existing algorithms by quite large margin.\n\nCons:\n- In general, I felt that the paper is unorganized and hard to read. Clarity should be definitely improved if this paper is to be published as a conference paper.\n\n- Output of dynamic parameter generator is very high dimensional (it requires weight with dimension of input dim x NN weight dim). I think this approach is not scalable to higher dimension and typically requires even more memory than storing the whole dataset. \n\n- Although auto-encoder and generative replay was considered to reduce memory consumption, there is no description of how much memory is saved by them. In order to make the argument more convincing, the authors should explicitly describe the amount of memory consumed by each algorithms. \n\n- There seems to be a lot of ideas introduced, i.e, DPG for generation of weights, auto-encoders for generation of data and layer output constraint, i.e., Equation (7).  I think each of introduced method deserves some amount of empirical evaluation to validate its contribution to the performance.  \n\n- Experiments only consider 2~3 tasks, which does not seem very representative for the lifelong learning tasks.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting ideas and results. Needs improvement in writing, formalization and possibly more ablation studies.",
            "review": "This paper proposes a Dynamic Parameter Generator (DPG) that given a test input modifies the parameters of a classification model. They also propose to regularize the training using a Data Generator (DG) to slow down catastrophic forgetting. DG is used to constrain the training that the internal representations of data generated by DG does not rapidly change. DG removes the need for storage of data or labels.\n\nPositives:\n- Both ideas of DPG and DG are novel in preventing catastrophic forgetting.\n- DG is novel because it does not require storage of data and does not depend on labels.\n- Experimental results are significantly better than the previous state-of-the-art.\n\nSuggestions and clarification requests:\n- Figures are very small and equations are cramped because of reduced spacing.\n- There are some vague explanations in the intro that could be reduced. It would be nice to first introduce concrete math then give the intuitions. That saves some space.\n- It would nice to compare to the recent Progress & compress [1]. Unfortunately, they have not provided results on benchmark MNIST tasks.\n- This work is related to a recently proposed idea in architecture search [2] that learns to predict the weights of a network given its architecture.\n- Can you clarify whether you have used DG at test time?\n- Can you report results without using DG? It is not clear whether DPG is accountable for preventing the catastrophic forgetting or the sluggishness enforced by DG.\n- Questions 1 and 2 need more formalization if the authors want to clearly prove a statement.\n- As the answer to Question 1 suggests, have you explored enforcing a Lipschitz constraint?\n- The answer to Question 2 is interesting. Could you rewrite it more formally? It seems like you can argue that DGâ€™s objective encourages the employment of unused parameters which is important in tackling catastrophic forgetting.\n- Can you elaborate on how much forgetting happens for DG?\n- It seems that in figure 3.f and 3.c the MA method is unable to reach the best possible performance on the last task. Can you also report the table of accuracies on the last task?\n\n[1] Schwarz, Jonathan, et al. \"Progress & Compress: A scalable framework for continual learning.\" arXiv preprint arXiv:1805.06370 (2018).\n[2] Brock, Andrew, et al. \"SMASH: one-shot model architecture search through hypernetworks.\" arXiv preprint arXiv:1708.05344 (2017).",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}