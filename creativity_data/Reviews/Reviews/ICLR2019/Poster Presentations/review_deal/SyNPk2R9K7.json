{
    "Decision": {
        "metareview": "This paper presents a dataset and method for training a model to infer, from a visual scene, the program that would generate/describe it. In doing so, it produces abstract disentangled representations of the scene which could be used by agents, models, and other ML methods to reason about the scene.\n\nThis is yet another paper where the reviewers disappointingly did not interact. The first round of reviews were mediocre-to-acceptable. The authors, I think, did a good job of responding to the concerns raised by the reviewers and edited their paper accordingly. Unfortunately, not one of the reviewers took the time to consider author responses.\n\nIn light of my reading of the responses and the revisions in the paper, I am leaning towards treating this as a paper where the review process has failed the authors, and recommending acceptance. The paper presents a novel method and dataset, and the experiments are reasonably convincing. The paper has flaws and the authors are advised to carefully take into account the concerns flagged by reviewers—many of which they have responded to—in producing their final manuscript.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "Acceptable"
    },
    "Reviews": [
        {
            "title": "A novel scene representation proposed, but needs more clarification on its advantages and flexiblity.",
            "review": "[Overview]\n\nIn this paper, the authors proposed a new format of representation called scene programs, to describe the visual scenes. To extract the scene programs from scenes, the authors exploited the off-the-shelf object detection and segmentations model, mask r-cnn to extract all objects and the corresponding attributes from the images, and then detect groups for those objects, which are then used to generate the programs which matches the input scenes. The experiments are performed on a synthetics datasets which consists of multiple shapes with different attributes. The experiments shows that the proposed model can infer more accurate programs from the scenes, and those generated programs can be used to recover the input scenes more accurately. Besides, the authors also showed that the generated scene programs can be used for image editing and making visual analogy.\n\n[Strengthes]\n\n1. The authors proposed a new representation, called scene programs, to describe the visual scenes with some textual program. This is a new scene representation, which could be potentially used in various scenarios, such as the image synthesis in graphics.\n\n2. The authors proposed a hierarchical method to model the structures in scenes. Specifically, the objects in a scene are first extracted and then grouped into multiple clusters, which will be used to guide the scene program synthesis. \n\n3. The experimental results demonstrate the effectiveness of the proposed method both qualitatively and quantitatively. The authors also showed the the programs generated  can be sued for image editing and cross-modality matching. \n\n[Weaknesses]\n\n1. It is a bit unfair to compare the proposed method with the two baseline methods listed in Table 2. The authors used a pre-trained mask-rcnn to detect all objects and predict the attributes for all objects. However, the counterpart methods have no access to this supervision. Even in this case, CNN-LSTM seems achieve comparable performance on the first three metrics. \n\n2. The advantage of scene program compared with scene graph (Johnson et al) are not clear to me. Scene graph is also a symbolic representation for images. Also, for all the tasks mentioned in this paper, such as image editing and visual analogy, scene graph can probably also complete well. The authors should comment about the specific advantages of scene program in comparison with scene graph.\n\n3. All the images shown in the paper seems arranged uniformly, which I think contains some bias to the proposed grouping strategy. I would like to see more diverse configurations of the foreground objects. It would be good to see if the proposed model can describe more complicated scenes.\n\n[Summary]\n\nThis paper proposed a novel scene representations, called scene program. To extract the scene program, the authors proposed a hieratchical inference method. The resulting scene programs based on the proposed model outperforms several baseline models quantitatively. The authors also showed the proposed scene program is suitable for image editing and visual analogy making. However, as pointed above, there are some unclear points to me, especially the advantages of scene program compared with scene graph, and the representation power of scene program for complicated scenes.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "good problem; weak evaluation and motivation",
            "review": "This paper investigates a descriptive representation of scenes using programs. Given an input image and an initial set of detections obtained from bottom-up detectors a sequence to sequence network is used to generate programs in a domain specific language (DSL). The authors consider a dataset where simple primitives are arranged in layouts in 3D scenes with varying material and color properties. They argue that the scene representation lead to better generalization on novel scene types and improve over baselines on image analogy tasks. The paper is well written but the evaluation and technical novelty is weak. \n\nFirst, the use of scene programs is not a contribution of this paper. Going beyond the works cited in the related work section, several recent works have proposed and investigated the advantages of program synthesis for shape generation (e.g., CSGNet Sharma et al. CVPR 2018 and Scene derendering, Wu et al., CVPR 2017), visual reasoning (Modular networks, Andreas et al., 2015), among others. \n\nAt a high-level the motivation of the program level representation for the considered tasks is not highlighted. It seems that an attribute-based representation, i.e., the output of the mask R-CNN detector that describes the image as a collection of objects, material properties, and their positions and scales is a sufficient representation. The higher-order relationships can be relatively easily extracted from the detections since the images are clean and clutter free. A baseline approach where the program synthesis was performed using search and grouping should be compared with. \n\nThe considered tasks are relatively simple achieving 99.5% token-level accuracy. The evaluation beyond the synthetic datasets is fairly limited and it is unclear how well the method generalizes to novel images in clutter and occlusion. \n\nIn summary, the paper makes a number of observations that have been motivated in a number of prior works, but the contributions of this paper is not highlighted (e.g., over neural scene derendering). The main claim that higher-order relationships are being modeled is not apparent due to the simplicity of the scenes being considered. For example, the program blocks being considered are somewhat arbitrary and a comparison with a clustering based grouping approach should have been evaluated. The experimental evaluation is weak in several aspects. The generalization to real images is anecdotal with only two examples shown in the Figure 7. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper presents a system that infers programs describing 3D scenes composed of simple primitives. The system consists of three stages each of which is trained separately. First, the perceptual module extracts object masks and their attributes. The objects are then are split into several groups. Finally, each group is mapped to a corresponding DSL program using a sequence-to-sequence network similar to the ones typically employed in neural machine translation.\n\nPros:\n+ The paper is written clearly and easy to read.\n+ Visual program synthesis is very exciting and important direction both for image understanding and generation.\n+ The results on synthetic datasets are good. The authors also demonstrate the applicability of the approach to real-world data (albeit significantly constrained).\n+ I find it surprising that a seq2seq is good at producing an accurate program for a group of objects.\n+ Visual analogy making experiments are impressive.\n\nCons:\n- The proposed model requires rich annotation of training data since all the components of the systems are trained in a supervised fashion. It’s not clear how to use the method on the in-the-wild data without such annotation.\n- Related to the previous point, even when it’s possible to synthesize data, it is non-trivial to obtain the ground-truth grouping of objects. Judging by Table 2, it seems that the system breaks in absence of the grouping information.\n- The data used in the paper is quite simplistic (limited number of primitives located in a regular grid). I’m wondering if there is a natural way to extend the approach to more complex settings. My guess is that the performance will drop significantly.\n\nNotes/questions:\n* Section 2, paragraph 1: The paper by [Ganin et al., 2018] presents both a system for reproducing an image as well as for sampling from a distribution; moreover, it presents experiments on 3D data (i.e., not limited to drawing).\n* Section 3.4, paragraph 2: I’m not sure I understand the last sentence. How can we know that we successfully recovered the scene at test time? Could the authors elaborate on the stopping criterion for sampling?\n* Section 4.2, paragraph 2: Do I understand correctly that the main difference between the test set and the generalization set is the number of groups? (i.e., 2 vs 3). If so, it’s a fairly limited demonstration of generalization capabilities of the system.\n* Section 4.2, paragraph 4: “we search top 3 proposals ...” – How do we decide which one is better? Do we somehow have an access to the ground truth program at test time?\n* Could the authors explain the representation of a program more clearly? How are loops handled? How can one subtract/add programs in the analogy making experiment?\n\nOverall, I think it is a interesting paper and can be potentially accepted on the condition that the authors address my questions and concerns.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}