{
    "Decision": {
        "metareview": "Adversarial training has quickly become important for training robust neural networks.  However this training generally results in poor generalization behavior. This paper proposes using margin loss with adversarial training for better generalization. The paper provides generalization bounds for this adversarial training setup motivating the use of spectral regularization. The experimental results using the spectral regularization with adversarial training are very promising and all the reviewers agree that they show non-trivial improvement. Even though the spectral regularization techniques have been tried in different settings, hence of limited novelty, the experimental results in the paper are encouraging and I believe will motivate further study on this topic. Reviewers also opined that the writing in the paper is currently not that great with limited explanation of the theoretical results. More discussions interpreting the theoretical results and their significance can help the readers appreciate the paper better.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "ICLR 2019 decision"
    },
    "Reviews": [
        {
            "title": "Good paper, but I have some questions about the experimental results",
            "review": "The paper first provides a generalization bounds for adversarial training, showing that the error bound depends on Lipschitz constant. This motivates the use of spectral regularization (similar to Miyato et al 2018) in adversarial training. Using spectral regularization to improve robustness is not new, but it's interesting to combine spectral regularization and adversarial training. Experimental results show significant improvement over vanilla adversarial training. \n\nThe paper is nicely written and the experimental results are quite strong and comprehensive. I really like the paper but I have two questions about the results: \n\n1. The numbers reported in Figure 5 do not match with the performance of adversarial training in previous paper. In PGM L_inf adversarial training/attack (column 3 of Figure 5), the prediction accuracy is roughly 50% under 0.1 infinity norm perturbation. However, previous papers (e.g., \"Obfuscated Gradients Give a False Sense of Security\") reported 55% accuracy under 0.031 infinity norm perturbation. I wonder why the numbers are so different. \n\nMaybe it's because of different scales? Previous works usually scale each pixel to [0,1] or [-1,1], maybe the authors use the [0, 255] scale? But 0.1/255 will be much smaller than 0.031. \n\nAnother factor might be the model structure. If Alexnet has much lower accuracy, it's probably worthwhile to conduct experiments on the same structure with previous works (Madry et al and Athalye et al) to make the conclusion more clear. \n\n2. What's the training time of the proposed method compared with vanilla adversarial training? \n\n3. The idea of using SN to improve robustness has been introduced in the following paper: \n\"Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks\"\n(but this paper did not combine it with adv training). \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "spectral normalization for adversarial training",
            "review": "This paper proposes using spectral normalization (SN) as a regularization for adversarial training, which is based on [Miyato et. al., ICLR 2018], where the original paper used SN for GAN training. The paper also uses the results from [Neyshabur et. al., ICLR 2018], where the original paper provided generalization bounds that depends on spectral norm of each layer. \n\nThe paper is well written in general, the experiments are extensive. \n\nThe idea of studying based on the combination of the results from two previous papers is quite natural, since one uses spectral normalization in practice for GAN training, and the other provides generalization bound that depends on spectral norm. \n\nThe novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily. The experimental result itself is quite comprehensive. \n\nOn the other hand, this paper provides specific generalization bounds under three adversarial attack methods, which explains the power of SN under those settings. However, it is not clear to me that these are some novel results that can better help adversarial training.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The idea is well explained, but results are less clear",
            "review": "This paper is well set-up to target the interesting problem of degraded generalisation after adversarial training. The proposal of applying spectral normalisation (SN) is well motivated, and is supported by margin-based bounds. However, the experimental results are weak in justifying the paper's claims.\n\nPros:\n* The problem is interesting and well explained\n* The proposed method is clearly motivated\n* The proposal looks theoretically solid\n\nCons:\n\n* It is unclear to me whether the \"efficient method for SN in convolutional nets\" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides. There is no direct comparison of performance.\n\n* Fig. 3 needs more explanation. The horizontal axes are unlabelled, and \"margin normalization\" is confusing when shown together with SN without an explanation. Perhaps it's helpful to briefly introduce it in addition to citing Bartlett et al. 2017.\n\n* The epsilons in Fig. 5 have very different scales (0 - 0.5 vs. 0 - 5). Are these relevant to the specific algorithms and why?\n\n* Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem. However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal. Furthermore, the baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm). It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.\n\nA typo in page 6, last line: wth -> with",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}