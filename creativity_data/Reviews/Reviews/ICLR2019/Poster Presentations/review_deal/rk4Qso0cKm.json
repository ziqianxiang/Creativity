{
    "Decision": {
        "metareview": "Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers' comments into consideration to improve your submission for the camera ready.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Paper decision"
    },
    "Reviews": [
        {
            "title": "Interesting contribution",
            "review": "After feedback: I would like to thank the authors for careful revision of the paper and answering and addressing most of my concerns. From the initial submission my main concern was clarity and now the paper looks much more clearer. \n\nI believe this is a strong paper and it represents an interesting contribution for the community.\n\nStill things to fix:\na) a dataset used in 4.2 is not stated\nb) missing articles, for example, p.5 \".In practice, however, we need a weaker regularization for A small dataset or A large model\"\nc) upper case at the beginning of a sentence after question: p.8 \"Is our Adv-BNN model susceptible to transfer attack? we answer\" - \"we\" -> \"We\"\n====================================================================================\n\nThe paper proposes a Bayesian neural network with adversarial training as an approach for defence against adversarial attacks. \n\nMain pro:\nIt is an interesting and reasonable idea for defence against adversarial attacks to combine adversarial training and randomness in a NN (bringing randomness into a new level in the form of a BNN), which is shown to outperform both adversarial training and random NN alone.\n\nMain con:\nClarity. The paper does not crucially lack clarity but some claims, general organisation of the paper and style of quite a few sentences can be largely improved.\n\nIn general, the paper is sound,  the main idea appears to be novel and the paper addresses the very important and relevant problem in deep learning such as defence against adversarial attacks. Writing and general presentation can be improved especially regarding Bayesian neural networks, where some clarity issues almost become quality issues. Style of some sentences can be tuned to more formal.\n\nIn details:\n1. The organisation of Section 1.1 can be improved: a general concept \"Attack\" and specific example \"PGD Attack\" are on the same level of representation, while it seems more logical that \"PGD Attack\" should be a subsection of \"Attack\". And while there is a paragraph \"Attack\" there is no paragraph \"Defence\" but rather only specific examples\n2. The claim “we can either sample w ∼ p(w|x, y) efficiently without knowing the closed-form formula through the method known as Stochastic Gradient Langevin Dynamics (SGLD) (Welling & Teh, 2011)” sounds like SGLD is the only sampling method for BNN, which is not true, see, e.g., Hamiltonian Monte Carlo (Neal’s PhD thesis 1994). It is better to be formulated as \"through, for example, the method ...\"\n3. Issues regarding eq. (7):\n   a) Why there is an expectation over (x, y)? There should be the joint probability of all (x, y) in the evidence.\n   b) Could the authors add more details about why it is the ELBO given that it is unconventional with adversarial examples added?\n   c)  It seems that it should be log p(y | x^{adv}, \\omega) rather than p(x^{adv}, y | \\omega). \n   d) If the authors assume noise component, i.e., y = f(x; \\omega) + \\epsilon, then they do not need to have a compulsory Softmax layer in their network, which is important, for example, for regression models. Then the claim “our Adv-BNN method trains an arbitrary Bayesian neural network” would be more justified\n4. It would make the paper more self-contained if the Bayes by Backprop algorithm would be described in more details (space can be taken from the BNN introduction). And it seems to be a typo that it is Bayes by Backprop rather than Bayes by Prop\n5. There are missing citations in the text:\n    a) no models from NIPS 2017 Adversarial Attack and Defence competition (Kurakin et al. 2018) are mentioned\n    b) citation to justify the claim “C&W attack and PGD attack (mentioned below) have been recognized as\ntwo state-of-the-art white-box attacks for image classification task”\n    c) “we can approximate the true posterior p(w|x, y) by a parametric distribution q_θ(w), where the unknown parameter θ is estimated by minimizing KL(q_θ(w) || p(w|x, y)) over θ” - there are a variety of works in approximate inference in BNN, it would be better to cite some of them here\n    d) citation to justify the claim \"although in these cases the KL-divergence of prior and posterior is hard to compute and practically we replace it with the Monte-Carlo estimator, which has higher variance, resulting in slower convergence rate.”\n6. The goal and result interpretation of the correlation experiment is not very clear\n7. From the presentation of Figure 4 it is unclear that this is a distribution of standard deviations of approximated posterior.\n8. “To sum up, our Adv-BNN method trains an arbitrary Bayesian neural network with the adversarial examples of the same model” – unclear which same model is meant\n9. \"Among them, there are two lines of work showing effective results on medium-sized convolutional networks (e.g., CIFAR-10)\" - from this sentence it looks like CIFAR-10 is a network rather than a dataset\n10. In \"Notations\" y introduction is missing\n11. It is better to use other symbol for perturbation rather than \\boldsymbol\\delta since \\delta is already used for the Dirac delta function\n12. “via tuning the coefficient c in the composite loss function” – the coefficient c is never introduced\n\nMinor:\n1. There are a few missing articles, for example, in Notations, “In this paper, we focus on the attack under THE norm constraint…”\n2. Kurakin et al. (2017) is described in the past tense whereas Carlini & Wagner (2017a) is described in the present tense\n3. Inner brackets in eq. (2) are bigger than outer brackets\n4. In eq. (11) $\\delta$ is not bold\n5. In eq. (12) it seems that the second and third terms should have “-” rather than “+”\n6. Footnote in page 6 seems to be incorrectly labelled as 1 instead of 2\n\n\n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An approach that can work well in practice, but not principled",
            "review": "I have read the feedback and discussed with the authors on my concerns for a few rounds. \n\nThe revision makes much more sense now, especially by removing section 3.3 and replacing it with more related experiments.\n\nI have a doubt on whether the proposed method is principled (see below discussions). The authors responded honestly and came up with some other solution. A principled approach of adversarially training BNNs is still unknown, but I'm glad that the authors are happy to think about this problem. \n\nI have raised the score to 6. I wouldn't mind seeing this paper accepted, and I believe this method as a practical solution will work well for VI-based BNNs. But again, this score \"6\" reflects my opinion that the approach is not principled.\n\n=========================================================\n\nThank you for an interesting read.\n\nThe paper proposes training a Bayesian neural network (BNN) with adversarial training. To the best of my knowledge the idea is new (although from my perspective is quite straight-forward, but see some discussions below). The paper is well written and easy to understand. Experimental results are promising, but I don't understand how the last experiment relates to the main idea, see comments below.\n\nThere are a few issues to be addressed in revision:\n\n1. The paper seems to have ignored many papers in BNN literature on defending adversarial attacks. See e.g. [1][2][3][4] and papers citing them. In fact robustness to adversarial attacks is becoming a standard test case for developing approximate inference on Bayesian neural networks. This means Figure 2 is misleading as in the paper \"BNN\" actually refers to BNN with mean-field variational Gaussian approximations.\n\n2. Carlini and Wagner (2017a) has discussed a CW-based attack that can increase the success rate of attack on (dropout) BNNs, which can be easily transferred to a corresponding PGD version. Essentially the PGD attack tested in the paper does not assume the knowledge of BNN, let alone the adversarial training. This seems to contradict to the pledge in Athalye et al. that the defence method should be tested against an attack that is aware of the defence.\n\n3. I am not exactly sure if equation 7 is the most appropriate way to do adversarial training for BNNs. From a modelling perspective, if we can do Bayesian inference exactly, then after marginalisation of w, the model does NOT assume independence between datapoints. This means if we want to attack the model, then we need to do \n\\min_{||\\delta_x|| < \\gamma} log p(D_adv), \nD_adv = {(x + \\delta_x, y) | (x, y) \\sim \\sim D_tr},\nlog p(D_adv) = \\log \\int \\prod_{(x, y) \\sim D_tr} p(y|x + \\delta_x, w) p(w) dw.\nNow the model evidence log p(D_adv) is intractable and you resort to variational lower-bound. But from the above equation we can see the lower bound writes as\n\\min_{||\\delta_x|| < \\gamma} \\max_{q} E_{q} [\\sum_{(x, y) \\sim D_tr} \\log p(y|x + \\delta_x, w) ] - KL[q||p],\nwhich is different from your equation 7. In fact equation 7 is a lower-bound of the above, which means the adversaries are somehow \"weakened\".\n\n4. I am not exactly sure the purpose of section 3.3. True, that variational inference has been used for compressing neural networks, and the experiment in section 3.3 also support this. However, how does network pruning relate to adversarial robustness? I didn't see any discussion on this point. Therefore section 3.3 seems to be irrelevant to the paper.\n\nSome papers on BNN's adversarial robustness:\n[1] Li and Gal. Dropout Inference in Bayesian Neural Networks with Alpha-divergences. ICML 2017\n[2] Feinman et al. Detecting Adversarial Samples from Artifacts. arXiv:1703.00410\n[3] Louizos and Welling. Multiplicative Normalizing Flows for Variational Bayesian Neural Networks. ICML 2017 \n[4] Smith and Gal. Understanding Measures of Uncertainty for Adversarial Example Detection. UAI 2018",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A nice paper that bridges adversarial training and Bayesian neural nets",
            "review": "The paper extends the PGD adversarial training method (Madry et al., 2017) to Bayesian Neural Nets (BNNs). \nThe proposed method defines a generative process that ties the prediction output and the adversarial input \npattern via a set of shared neural net weights. These weights are then assinged a prior and \nthe resultant posterior is approximated by variational inference.\n\nStrength:\n  * The proposed approach is incremental, but anyway novel.\n  * The results are groundbreaking.\n  * There are some technical flaws in the way the method has been presented, \nbut the rest of the paper is very well-written.\n\nMajor Weaknesses:\n\n  * Equation 7 does not seem to be precise. First, the notation p(x_adv, y | w) is severely misleading. If x_adv is also an input, no matter if stochastic or deterministic, the likelihood should read p(y | w, x_adv). Furthermore, if the resultant method is a BNN with an additional expectation on x_adv, the distribution employed on x_adv resulting from the attack generation process should also be written in the form of the related probability distribution (e.g. N(x_adv|x,\\sigma)).\n\n  * Second, the constraint that x_adv should lie within the \\gamma-ball of x has some implications on the validity of\nthe Jensen's inequality, which relates Equation 7 to proper posterior inference.\n\n  * Blundell et al.'s algorithm should be renamed to \"Bayes-by-BACKprop\". This is also an outdated inference technique for quite many scenarios including the one presented in this paper. Why did not the authors benefit from the local reparametrization trick that enjoy much lower estimator variance? There even emerge sampling-free techniques that nullify this variance altogether and provide much more stable training experience.\n\nAnd Some Minor Issues:\n\n  * The introduction part of paper is unnecessarily long and the method part is in turn too thin. As a reader, I would prefer getting deeper into the proposed method instead of reading side material which I can also find in the cited articles.\n\n  * I do symphathize and agree that Python is a dominant language in the ML community. Yet, it is better scientific writing practice to provide language-independent algorithmic findings as pseudo-code instead of native Python.\n\nOverall, this is a solid work with a novel method and very strong experimental findings. Having my grade discounted due to the technical issues I listed above and the limitedness of the algorithmic novelty, I still view it as an accept case.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}