{
    "Decision": {
        "metareview": "This paper is concerned with solving Online Combinatorial Optimization (OCO) problems using reinforcement learning (RL). There is a well-established traditional family of approaches to solving OCO problems, therefore the attempt itself to solve them with RL is very intriguing, as this provides insights about the capabilities of RL in a new but at the same time well understood class of problems. \n\nThe reviewers agree that this approach is not entirely new. While past similar efforts take away some of the novelty of this paper, the reviewers and AC believe that still the setting considered here contains novel and interesting elements. \n\nAll reviewers were unconvinced that this work can provide strong claims about using RL to learn any primal-dual algorithm. This takes away some of the paper’s impact, but thanks to discussion the authors managed to clarify some “hand-wavy” claims and toned-down the claims that were not convincing. Therefore, it was agreed that the new revision still provides some useful insight into the RL and primal-dual connection, even without a complete formal connection. \n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Not entirely novel but still very interesting approach"
    },
    "Reviews": [
        {
            "title": "Review for A new dog learns old tricks: RL finds classic optimization algorithms ",
            "review": "This paper introduces a new framework to solve online combinatorial problems using reinforcement learning. The idea is to encode the current input, the global parameters, and a succinct data structure (to represent current states of the online problem) as MDP states. Such a problem can then be solved by deep RL methods. To train such models, the authors use a mixture of input distributions. Some come from hard distributions which are used to prove lower bounds in the TCS community, and the others are carefully constructed distributions to fool a specific set of algorithms. The authors made an important point that their algorithms are uniform in the TCS sense, i.e., the algorithm does not depend on the input length.\nFurthermore, the authors show that the learned algorithms have similar behavior as those classical optimal algorithms that are first obtained in the online combinatorial optimization community. \n\nThe idea of using hard distributions to train the model is reasonable, but not extremely interesting/exciting to me since this is by now a standard idea in TCS. Moreover, in many cases (especially those problems that don't have a lower bound), it is even very hard to construct a hard distribution. In general, how should we use construct the input distribution in those cases? Can the proposed methods still generalize if we don't have an appropriate hard distribution? I would like to see some discussion/experiments along this line. \n\nMoreover, it is unclear to me whether the methods proposed here can really learn *uniform* algorithms for the ADWORDS problem. To make the state length independent of the number of advertisers (n), the authors discretized the state space (see Appendix B). This approach might work for small (constant) n. But as we increase n to infinity, it seems to me that due to precision issues this approach will fail. If this is true, then in order to make this algorithm work, we also need to increase the precision of the real numbers used to describe the state as we increase n. If it is the case, why is the algorithm still uniform? If it is not the case, the authors need to provide extra experimental results to show that the effectiveness of the learned algorithm keeps unchanged, even if we keep increasing n and do not change the representation of the states.\n\nIt is an interesting observation that deep RL methods can actually learn an algorithm with similar behavior as optimal classical combinatorial optimization algorithms. However, there is no explanation for this, which is a little bit frustrating. Would this phenomenon be explained by the use of hard distributions? The paper can be strengthened by providing more discussions along this line. \n\nMinor comments:\n\nThe caption of Table 2 seems to contradict its explanation (on top of page 14). Is the state space discretized or the number of advertisers changed?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A review on  \"A new dog learns old tricks: RL finds classic optimization algorithms\"",
            "review": "The overall goal of this paper is to solve online combinatorial optimization (OCO) problems using reinforcement learning. Importantly, the authors are not seeking to establish new results for unsolved problems, but instead they are motivated by analyzing and comparing the quality of solutions predicted by reinforcement learners with respect to the well-known near-optimal strategies for some OCO tasks. In doing so, the authors focused on an MDP framework using policy gradient and DQN methods. This framework was trained on three OCO tasks; online budget allocation, online knapsack, and the secretary problem. For each, the trained model is consistent with the near-optimal “handcrafted’’ algorithms.\n\nThe idea of checking whether a standard RL framework, without prior information about “how to” solve a given OCO task, is capable from experience to reach the performance of existing optimal strategies (especially primal-dual approaches), is clearly interesting. But I am not entirely convinced that the paper is making novel contributions in this direction. My comments are detailed below:\n\n(1) OCO problems have been a subject of extensive research in online learning (see e.g. [1,2,3]). Notably, the main issues related to “input length independence” (Sec 1.1) and “adversarially chosen input distributions” (Sec 1.2) have already been addressed in online learning frameworks. Input length independence is related to “horizon-independence” in online learning (the number $T$ of trials is not known in advance), and well-known approaches have been developed for devising horizon-independent forecasters, or transforming a horizon-dependent forecaster into a horizon-independent one (see e.g. [4]). Also, the study of online learners with respect to different properties of input sequences (stochastic, adversarial, or hybrid), is a well-known topic of research which have been conceptualized and formalized with appropriate metrics (see e.g. [5,6]). \n\n(2) Although the authors are interested in making connection between RL and “primal-dual approaches” in online learning, this connection was not made clear in the paper. Namely, the overall contribution was to show that deep RL architectures can compete with existing, handcrafted online strategies, on three specific tasks. But in order to make a concrete connection with primal-dual approaches, this study should be extended to more general primal (covering) and dual (packing) problems as described in [7], and the RL framework should be compared with “generic” online primal-dual algorithms also described in [7]. We may note in passing that the offline versions of the three tasks examined in the present paper belong to the approximation classes PTAS or APX. By contrast, the complexity class of general covering/packing problems is much higher (a constant approximation ratio is not achievable unless P=NP) and, to this point, it would be interesting to examine whether a standard deep RL framework can compete with existing online strategies (for example in [7,8]) on such hard problems.\n\n(3) Even if we stick to the three problems examined in the paper, the neural nets vary between tasks, with different numbers of layers, different widths, different batch sizes, etc. On the one hand, it is legitimate to seek for an appropriate learning architecture for the input problem. On the other hand, such adjustments are conveying some prior knowledge about “how to” solve this problem using a deep RL model. Moreover, for knapsack and secretary tasks, additional knowledge about the history (i.e. state augmentation) is required for establishing convergence, but the resulting model is no longer a standard MDP. So, unless I missed the point about the overall goal of this paper, these different aspects are somewhat in contradiction with the idea of starting with a deep RL architecture with default settings, in which the varying components are essentially the states, transitions, and rewards of the MDP that encodes the problem description. \n\n[1] Bubeck, S., Introduction to Online Optimization. Lecture Notes, Princeton University, 2011.\n\n[2] Audibert, J-Y., Bubeck, S., and Lugosi, G. Minimax policies for combinatorial prediction games. In COLT, 2011.\n\n[3] Rajkumar, A. and Agarwal, S. Online decision-making in general combinatorial spaces. In NIPS, 2014.\n\n[4] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.\n\n[5] A. Rakhlin, K. Sridharan, and A. Tewari. Online learning: Stochastic, constrained, and smoothed adversaries. In NIPS, 2011.\n\n[6] N. Buchbinder, S. Chen, J. Naor, O. Shamir: Unified Algorithms for Online Learning and Competitive Analysis. In COLT, 2012.\n\n[7] N. Buchbinder and J. Naor. The Design of Competitive Online Algorithms via a Primal-Dual Approach. Foundations and Trends in Theoretical Computer Science, 2009.\n\n[8] S. Arora, E. Hazan, and S. Kale. The Multiplicative Weights Update Method: a Meta-Algorithm and Applications, Theory of Computing, 2012.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Learning RL policies for Online Primal-Dual Algorithms",
            "review": "This paper studies the problem of *learning* online combinatorial algorithms via Reinforcement Learning. In particular, the paper studies three different well-studied problems, namely AdWords/Online Matching, Online Knapsack, and Secretary Problem. The common thread to all three problems is that they are special cases of Online Packing problem and that there exist optimal algorithms with theoretical guarantees. Moreover, all these problems have an algorithm based on the unifying Primal-Dual framework (*). The paper runs extensive experiments and shows that the learned RL policies resemble the algorithms created in theory by comparing many properties of the returned policy. Overall I think this paper tackles a very interesting question, is well-written and has extensive experiments. \n\nI will detail my technical comments and questions to authors. I would especially appreciate detailed answers to (1), (2), (3). (4) and (5) are more open-ended questions and/or beyond the scope of the current paper. It can be viewed as feedback for some future work.\n\n(1) (*) The paper starts with the claim that one of the key insights to this work is the primal-dual framework. Yet this has not been exploited in the paper; at least I can't see where it is used! Can the authors give more details? For example, one way I could see this being used is as follows. In the AdWords problem, the evolution of the Dual variable is well understood (See [Devanur, Kleinberg, Jain '13]). One can experiment to see how the dual values change over the run of the algorithm for the learned policy and compare that with how it changes in the analysis. If they are similar, then that is yet another evidence that the two algorithms are similar.\n\n(2) One key point to note is that all three algorithms include only the \"primal\" observations in the state. This strengthens this work since all these algorithms, in theory, are analyzed by realizing that the primal algorithm can be interpreted as an appropriate process on the evolution of the dual variables.  Thus it seems like the RL policy is actually learning the optimal way to set the dual variables in the online phase. Is this true? I guess the experiments above can indeed verify this. If this is true, it implies that the key message of this paper is that RL algorithms can be used to learn algorithms that can be analyzed via the primal-dual framework. Right now, the authors stop short of this by saying this work is inspired from it. It would be good to see this taken to completion.\n\n(3) It seems like there has been some work on using RL to learn algorithms in combinatorial optimization (see [Dai et al., NIPS 2017]). Can the authors discuss both in the rebuttal and in the paper on how their work compares and differs from this work? \n\n(4) I wonder if the authors experimented with the i.i.d. arrival process for Knapsacks and/or Online Matching/Adwords. It is known that the theoretical algorithms for both these problems do much better than the pessimistic adversarial arrival order. It will be interesting to see if the RL policies also find this. On a related note, did the authors try this on b-matching with b=1? The problem tends to get easier as b is large and/or when bid/budget ratio is small in Adwords. However even when b=1, in theory, we can get 1-1/e [KVV '90].\n\n(5) Finally, I am curious if the authors tested this on problems that are not packing but covering and/or mixed packing and covering problems. Online Set Cover is a candidate example. The other direction is also to test Online Minimization problems. Note that Online Min-cost matching is significantly harder than Online maximum weight matching. Moreover, Online Min-cost matching does not have a primal-dual analysis (to the best of my knowledge). The latter helps because if the RL policy fails to learn the best-known algorithm, then it is further evidence that it is indeed learning through the primal-dual framework.\n\nSome minor comments:\n\nStyling is not consistent throughout the paper. For example, there are places with the header followed by a colon sometimes, a period other times and sometimes no punctuation. Please make these consistent throughout the paper.\n\nThe fonts on the figures are very small to read. It would be easy on the reader if the sizes were made larger.\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}