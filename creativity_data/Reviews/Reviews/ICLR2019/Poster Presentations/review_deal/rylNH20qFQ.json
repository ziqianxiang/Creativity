{
    "Decision": {
        "metareview": "This paper presents a method whereby a model learns to describe 3D shapes as programs which generate said shapes. Beyond introducing some new techniques in neural program synthesis through the use of loops, this method also produces disentangled representations of the shapes by deconstructing them into the program that produced them, thereby introducing an interesting and useful level of abstraction that could be exploited by models, agents, and other learning algorithms.\n\nDespite some slightly aggressive anonymous comments by a third party, the reviewers agree that this paper is solid and publishable, and I have no qualms in recommending it from inclusion in the proceedings.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Good paper"
    },
    "Reviews": [
        {
            "title": "Addresses an important problem; well written; but missing baselines and some discussions",
            "review": "This paper presents an approach to infer shape programs given 3D models. The programs include placing and arranging predefined primitives in layouts and can be written as a program over a domain-specific language (DSL). \n\nThe architecture consists of a recurrent network that encodes a 3D shape represented as a voxel grid and outputs the instructions using a LSTM decoder. The generation is two-step where the first step predicts a program ID and the second step predicts instructions within the program ID. This aspect wasn't completely clear to me, see questions below. A second module that renders the program to 3D is also implemented as a neural network in order to optimize the model parameter in a end-to-end manner by minimizing a reconstruction loss. \n\nThe method is evaluated on 3D shape reconstruction tasks for chairs and tables categories of the ShapeNet dataset. The approach compares favorably to Tulsiani et al., which considers a shape to be composed of a fixed number of cuboids.\n\nThe paper is well written and investigates an important problem. But it is hard to tease of the contributions and the relative importance of various steps in the paper:\n\n1. Structure search vs. prediction. How does the model perform relative to a search-based approach for program generation. That would be slower but perhaps more accurate. The prediction model can be thought of an amortized inference procedure for search problems. What advantages does the approach offer?\n\n2. Choice of the DSL. Compared to CSG modeling instructions of Sharma et al. the proposed DSL is more targeted to the shape categories. While this restricts the space of programs (e.g., no intersection, subtraction operations are used) leading to better generation of chairs and tables, it also limits the range and generalization of the learned models to new categories. Some discussion and comparison with the choice of DSL would be useful. \n\n3. Is the neural render necessary -- Wouldn't it be easier to simply use automatic differentiation to compute gradients of the rendering engine? \n\n4. It is also not clear to me how having a differentiable renderer allows training in an end-to-end manner since the output space is discrete and variable length. In CSGNet (Sharma et al.) policy-gradient techniques were used to optimize the LSTM parameters. The details of the guided adaptation were unclear to me (Section 4.3).\n\n5. Is the neural renderer reliable -- Is is not clear if the neural renderer can provide accurate gradients when the generated programs are incorrect since the model is trained on a clean samples. In practice this means that the encoder has to initialized well. Since the renderer is also learned, would it generalize to new programs within the same DSL but different distribution over primitives -- e.g., a set of tables that have many more legs. Some visualizations of the generated shapes from execution traces could be added, sampling programs from within and outside the program distributions used to train.\n\n6. All the above points give an impression that the choice of DSL and careful initialization are important to get the model to work. Some discussion on how robust the model is to these choices would be useful. In other words how meaningful is the generalization from the supervised training set of templates chairs and tables? \n\n7. Missing baselines: The model is trained on 100,000 chairs and tables with full supervision. What is the performance of a nearest neighbor prediction algorithm? This is an important baseline that is missing. A comparison with a simplified CSGNet with shape primitives and union operations is also important. Tulsiani et al. consider unions but constrain that all instances have the same number of primitives which can lead to poor reconstruction results. Furthermore the training sets are likely different making evaluations unclear. I suggest training the following decoders on the same training set used in this approach (1) fixed set of cuboids (e.g., Tulsiani et al.), (2) A recurrent decoder with cuboids, (3) CSGNet (different primitives and operations), (4) a nearest neighbor predictor with the Hamming or Chamfer distance metric. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper!",
            "review": "This paper introduces a high-level semantic description for 3D shapes. The description is given by the so-called ShapeProgram,  Each shape program consists of several program statements. A program statement can be either Draw, which describes a shape primitive as well as its geometric and semantic attributes, or For, which contains a sub-program and parameters specifying how the sub-program should be repeatedly executed. The ShapeProgram is connected with an input through two networks, the program generator (encoder) and a neural program executor (decoder). Both encoder/decoder are implemented using LSTM. The key ML contribution is on the decoder, which leverages a parametrization to make the decoder differentiable. The major advantage of the proposed technique is that it does not need to specify the ShapeProgram in advance. In the same spriit of training an auto-encoder. It can be learned in a semi-supervised manner. However, in practice, one has to start with a reasonably good initial program. In the paper, this initial program was learned from synthetic data. \n\nThe paper presents many experimental results, including evaluation on synthetic datasets, guided adaptation on ShapeNet, analysis of stability, connectivity measurement, and generalization, and application in shape completion. The presented evaluations, from the perspective of proposed experiments, is satisfactory. \n\nOn the downside, this paper does not present any baseline evaluation, party due to the fact that the proposed problem is new. In fact, existing inverse procedural modeling techniques require the users to specify the program. However, the proposed approach could be even more convincing if it evaluates the performance of semantic understanding. For example, would it be possible to evaluate the performance on shape segmentation?\n\nAdditional comments:\n1. How important is the initial program? \n\n2. The interactions among shape parts usually form a graph, not necessarily hierarchical. This should be discussed.\n\n3. What is the difference between 3D shapes and 3D scenes? Does this approach require a front/up-right orientation?\n\n4. It would be interesting to visualize/analyze the intermediate representations of the neural shape generator. Does it encode meaningful distributions among shape parts?\n\nOverall, it is a good paper, and I would like to see it at ICLR 2019.\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Elegant synthesis approach to a new interesting domain of representing 3D shapes",
            "review": "This paper presents a methodology to infer shape programs that can describe 3D objects. The key intuition of the shape programs is to integrate bottom-up low-level feature recognition with symbolic high-level program structure, which allows the shape programs to capture both high-level structure and the low-level geometry of the shapes. The paper proposes a domain-specific language for 3D shapes that consists of “For” loops for capturing high-level regularity, and associates objects with both their geometric and semantic attributes. It then proposes an end-to-end differentiable architecture to learn such 3D programs from shapes using an interesting self-supervised mechanism. The neural program generator proposes a program in the DSL that is executed by a neural program execution module to render the corresponding output shape, which is then compared with the original shape and the difference loss is back-propagated to improve the program distribution. The technique is evaluated on both synthetic and ShapeNet tasks, and leads to significant improvements compared to Tulsiani et al. that embed a prior structure on learning shape representations as a composition of primitive abstractions. In addition, the technique is also paired with MarrNet to allow for a better 3D reconstruction from 2D images.\n\nOverall, this paper presents an elegant idea to describe 3D shapes as a DSL program that captures both geometric and spatial abstractions, and at the same time captures regularities using loops. CSGNet [Sharma et al. 2018] also uses programs to describe 2D and 3D shapes, but the DSL used here is richer as it captures more high-level regularities using loops and also semantic relationships such as top, support etc. The idea of training a neural program executor and using it for self-supervised training is quite elegant. I also liked the idea of guided adaption to make the program generator generalize beyond the synthetic template programs. Finally, the results show impressive improvements and generalization capability of the model.\n\nCan the authors comment on some notion of completeness of the proposed DSL? In other words, is this the only set of operators, shapes, and semantics needed to represent all of ShapeNet objects? Also, it might be interesting to comment more on how this particular DSL was derived. Some of the semantics operator such as “Support”, “Locker”, etc. look overly specific to chair and tables. Is there a way to possibly learn such abstractions automatically?\n\nWhat is the total search space of programs in this DSL? How would a naive random search perform in this synthesis task?\n\nI also particularly liked the decomposition of programs into draw and compound statements, and the corresponding program generator decomposition into 2 steps BlockLSTM and StepLSTM. At inference time, does the model use some form of beam search to sample block programs or are the results corresponding to top-1 prediction?\n\nWould it be possible to compare the results to the technique presented in CSGNet [Sharma et al. 2018]?  There are some key differences in terms of using lower-level DSL primitives and using REINFORCE for training the program generator, but it would be good to measure how well having higher-level primitives improve the results.\n\nI presume the neural program executor module was trained using a manually-written shape program interpreter. How difficult is it to write such an interpreter? Also, how easy/difficult is to extend the DSL with new semantics operator and then write the corresponding interpreter extension?\n\nMinor typos:\npage 3: consists a variable → consists of a variable\npage 5: We executes → We execute\npage 6: synthetica dataset → synthetic dataset\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}