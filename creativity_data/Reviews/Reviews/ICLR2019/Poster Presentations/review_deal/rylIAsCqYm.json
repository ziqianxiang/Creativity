{
    "Decision": {
        "metareview": "The reviewers all agreed that this paper makes a strong contribution to ICLR by providing the first asynchronous analysis of a Nesterov-accelerated coordinate descent method.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "a clear accept"
    },
    "Reviews": [
        {
            "title": "Review: A2BCD",
            "review": "The authors design an accelerated, asynchronous block coordinate descent algorithm, which, for sufficiently small delays attains the iteration complexity of the current state of the art algorithm (which is not parallel/asynchronous). The authors prove a lower bound on the iteration complexity in order to show that their algorithm is near optimal. They also analyze an ODE which is the continuous time limit of A2BCD, which they use to motivate their approach.\n\nI am a little bit confused about the guarantee of the algorithm, as it does not agree with my intuition. Perhaps I am simply mistaken in my intuition, but I am concerned that there may need to be additional premises to the Theorem.\n\nMy main confusion is with Theorem 1, which says that for $\\psi < 3/7$ the iteration complexity is approximately the iteration complexity of NU_ACDM times a factor of $(1 + o(1))/(1-\\psi)$, i.e. within that factor of the optimal *non-asynchronous/parallel* algorithm. In particular, since $\\psi < 3/7$ this means that the algorithm is within a $7/4 + o(1)$ factor. As mentioned in Corollary 3, this applies for instance when $L_i = L$ for all i and $\\tau = \\Theta( n^{1/2}\\kappa^{-1/4} )$. Therefore, in a regime where $n \\approx \\kappa$, and $n$ very large, this would indicate that the algorithm would be almost as good as the best synchronous algorithm even for delays $\\tau \\approx n^{1/4}$. Perhaps I am missing something, but this seems very surprising to me, in particular, I would expect more significant slowdown due to $\\tau$. \n\nI am also a little bit surprised that the maximum tolerable delay is proportional to the *minimum* smoothness parameter $\\underbar{L}$. It seems like decreasing $\\underbar{L}$ should make optimization easier and therefore more delay should be tolerated. Perhaps this is simply an artifact of the analysis.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "questions about theories",
            "review": "This paper studies the combination of the asynchronous parallelization and the accelerated stochastic coordinate descent method. The proved convergence rate is claimed to be consistent with the non parallel counterpart. The linear speedup is achievable when the maximal staleness is bounded by n^{1/2} roughly, that sounds very interesting result to me. However, I have a few questions about the correctness of the results:\n\n- Theorem 1 essentially shows that every single step is guaranteed to improve the last step in the expectation sense. However, this violates my my experiences to study Nesterov's accelerated methods. To my knowledge, Nesterov's accelerated methods generally do not guarantee improvement over each single step, because accelerate methods essentially constructs a sequence z_{t+1} = A z_t where A is a nonsymmetric matrix with spectral norm greater than 1.\n\n- The actual implemented algorithm is using the sparse update other than the analyzed version, since the analyzed version is not efficient or suitable for parallelization. However, the sparse updating rule is equivalent to the original version only for the non asynchronous version. Therefore, the analysis does not apply the actual implementation.\n\nminors:\n- pp2 line 8, K(epsilon) is not defined \n- Eq. (1.4), the index is missing.\n- missing reference: An Asynchronous Parallel Stochastic Coordinate Descent Algorithm, ICML 2014.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An elegant solution to a long-standing open question as to the speed-up of asynchronous distributed coordinate descent",
            "review": "In distributed optimisation, it is well known that asynchronous methods outperform synchronous methods in many cases. However, the questions as to whether (and when) asynchronous methods can be shown to have any speed-up, as the number of nodes increases, has been open. The paper under review answers the question in the affirmative and does so very elegantly.\n\nI have only a few minor quibbles and a question. There are some recent papers that could be cited:\nhttp://proceedings.mlr.press/v80/zhou18b.html\nhttp://proceedings.mlr.press/v80/lian18a.html\nhttps://nips.cc/Conferences/2018/Schedule?showEvent=11368\nand the formatting of the bibliography needs to be improved. \n\nIn the synchronous case, some of the analyses extend to partially separable functions, e.g.:\nhttps://arxiv.org/abs/1406.0238\nand citations thereof. Would it be possible to extend the present work in that direction?",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}