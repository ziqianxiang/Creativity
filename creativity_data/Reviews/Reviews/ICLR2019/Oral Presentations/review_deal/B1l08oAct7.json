{
    "Decision": {
        "metareview": "The manuscript proposes deterministic approximations for Bayesian neural networks as an alternative to the standard Monte-Carlo approach. The results suggest that the deterministic approximation can be more accurate than previous methods. Some explicit contributions include efficient moment estimates and empirical Bayes procedures. \n\nThe reviewers and ACs note weakness in the breadth and complexity of models evaluated, particularly with regards to ablation studies. This issue seems to have been addressed to the reviewer's satisfaction by the rebuttal. The updated manuscript also improves references to related prior work.\n\nOverall, reviewers and AC agree that the general problem statement is timely and interesting, and well executed. We recommend acceptance.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Oral)",
        "title": "Meta-review"
    },
    "Reviews": [
        {
            "title": "An interesting paper ",
            "review": "The authors propose a new approach to perform deterministic variational inference for feed-forward BNN with specific nonlinear activation functions by approximating layerwise moments. Under certain conditions, the authors show that the proposed method achieves better performance than existing Monte Carlo variational inference. This paper is interesting since most of the existing works focus on Monte Carlo variational inference. The main contribution of this paper is to perform Gaussian approximation. The authors show that for specific activation functions, the Gaussian approximation is reasonable. The main concern is the cumulative error due to the Gaussian approximation. Since the authors argue that the proposed method fixes the issues of stochastic VI for BNN, the authors should also investigate/clarify the following cases. \n(1)  A deep BNN to show that the cumulative error is negligible as the number of the hidden layers increases \n(2)  Small latent dimension since CLT may not hold\n(3)  A heavy-tailed variational distribution since the second moment may not be finite \n(4)  Other nonlinear activations since the Gaussian approximation may not be accurate due to (generalized) Berry-Esseen theorem\n(5) A BNN with skip connections  since a Bayesian multiplayer perceptron with skip connections is also a feed-forward BNN\n \nAmong these cases, I am eager to see some results on a deep thin BNN. For example, a BNN with 5 hidden layers, where the latent dimension at each layer is less than 32. \nFurthermore, I would like to see some empirical comparison on real-world datasets between DVI and MCVI under a *fixed* prior since such comparison demonstrates the approximation accuracy of DVI and rule out the confounding factor introduced by the empirical Bayes approach.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Fixing Variational Bayes: Deterministic Variational Inference for Bayesian Neural Networks",
            "review": "This paper considers a purely deterministic approach to learning variational posterior approximations for Bayesian neural networks.  Variational lower bound gradients are obtained by approximating the lower bound using Gaussian approximations and moment propagation for network activations, and using a closed form expression for the variational expectation of the log-likelihood, the latter being available for the models considered in the paper.  \n\nThis is an interesting paper.  The Gaussian approximations and moment propagation approximations are clever and highly original although the derivation is rather heuristic.  There is some empirical support that the approximations work well.  The paper is generally well written and clearly motivated in the context of the existing literature.\n\nThe approximations work well for the examples presented in the paper.  The experiments are for rather small datasets and for the DVI method if I understand correctly only models with a single hidden layer are considered.  I wonder if the Gaussian and moment propagation approximations cause difficulty when applied repeatedly in deeper networks.  Are the problems with MCVI and high gradient variance most serious for large datasets and more complex models?  If so a comparison of DVI with MCVI in a more complex example is of interest.  The empirical Bayes approximations are interesting - I would have thought similar approximations been used in the literature before, in addition to the work you mention in Section 5?  I don't feel there is much to compare the proposed EB approximations to, although a comparison with manual tuning is given in Section 6.  \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Two advances for variational Bayes on neural networks. Expectations are done deterministically (as in PBP), not by Monte Carlo, thus reducing variance. The weight prior is learned with length scales by empirical Bayes. Both should make VB training more robust, but experiments do not show that.",
            "review": "Summary:\n\nThis work is tackling two difficulties in current VB applied to DNNs (\"Bayes by backprop\"). First, MC approximations of intractable expectations are replaced by deterministic approximations. While this has been done before, the solution here is new and very interesting. Second, a Gaussian prior with length scales is learned by VB empirical Bayes alongside the normal training, which is also very useful.\n\nThe term \"fixing VB\" and some of the intro is not really supported by the rather weak experiments, done on small datasets and networks, where much older work like Barber&Bishop would apply without any problems. While interesting and potentially very useful novelties are presented, and the writing is excellent, both experiments and motivation can be improved.\n\n- Quality: Extremely well written paper, I learned a lot from it. Approximations are\n   tested, great figures to explain things. And the major technical novelty, the\n   expression for <h_j h_l>, is really interesting and useful.\n- Clarity: Excellent writing until it comes to the experiments. Here, important\n   details are just missing, for example what q(w) is (fully factorized Gaussian?).\n   Very nice literature review, also historical.\n- Originality: The idea of matching Gaussian moments along the network graph is\n   previously done in PBP (Lobato, Adams), as acknowledged here. Porting this from\n   ADF to VB gives dDVI. PBP also has the property that a DL system gives you the\n   gradients. Having said that, I think dDVI may be more useful than PBP.\n   While Barber&BIshop 98 is cited, they miss the expression for <h_j h_l> in\n   there. Now, what is done here, is more elegant, does not need 1D quadrature.\n- Significance: Judging from the existing experiments, the significance may be\n   rather small, *if one only looks at test log likelihood*. I'd still give this the\n   benefit of the doubt, as in particular dDVI could be really interesting at large\n   scale as well. But the authors may tone down their language a bit.\n   To increase significance, I recommend to comment beyond just test log\n   likelihood scores. For example:\n   - Does the optimization become simpler, less tuning required, more automatic?\n      Would one not expect so, given you make a big point out of reducing variance?\n      Does it converge faster?\n   - Can you do something with your posterior that normal DNN methods cannot\n      do? Better decisions (bandits, active learning, HPO)? Continual learning?\n      In the end, who really cares about test log likelihood?\n\nExperiments:\n- What is the q(w) family being used here? Fully factorized Gaussian? I\n   suppose so for dDVI. But for DVI? Not said anywhere, in main paper or\n   Appendix\n- A bit disappointing. Why not evaluate at least dDVI with diagonal q(w) on\n   some much larger models and datasets? Why not quote numbers on speed\n   and robustness of learning, etc? Show what you really gain by reducing the\n   variance.\n- Experiments are OK, but on pretty small datasets, and for single hidden\n   layer NNs. On such data and models, the Barber&Bishop 98 method could\n   be run as well\n- Was MCVI run with re-parameterization? This is really important. If not,\n   this would be an important missing comparison. Please be clear in the main\n   text\n- Advantages over MCVI are not very large. At least, dDVI should be faster to\n   converge than MCVI.\n   Can you say something about robustness of training? Is it easier to train\n   dDVI than MCVI?\n- Why not show the PBP-1 results, comparing to dDVI, in the main text? Are they\n   obtained with the same model? dDVI is doing better.\n\nOther points:\n- Please acknowledge the <h_j h_l> expression in Barber&Bishop 98. Yours is\n   more elegant and faster (does not need 1D quadrature)\n- Relation to PBP: Note that dDVI has an advantage in practice. With PBP, I need\n   to compute gradients for every datapoint. In dDVI, I can do mini-batch\n   updates.\n- I just *love* the header \"Wild approximations\". I tend to refer to this kind of work\n   as \"weak analogies\". Why do you not also compare against this, and show it really\n   does not work?\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}