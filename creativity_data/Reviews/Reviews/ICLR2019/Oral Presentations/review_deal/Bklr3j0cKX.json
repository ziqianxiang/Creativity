{
    "Decision": {
        "metareview": "This paper proposes a new unsupervised learning approach based on maximizing the mutual information between the input and the representation. The results are strong across several image datasets. Essentially all of the reviewer's concerns were directly addressed in revisions of the paper, including additional experiments. The only weakness is that only image datasets were experimented with; however, the image-based experiments and comparisons are extensive. The reviewers and I all agree that the paper should be accepted, and I think it should be considered for an oral presentation.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Oral)",
        "title": "meta review"
    },
    "Reviews": [
        {
            "title": "Interesting take on representation learning, but text needs improvement",
            "review": "This paper proposes Deep InfoMax (DIM), for learning representations by maximizing the mutual information between the input and a deep representation. By structuring the network and objectives to encode input locality or priors on the representation, DIM learns features that are useful for downstream tasks without relying on reconstruction or a generative model. DIM is evaluated on a number of standard image datasets and shown to learn features that outperform prior approaches based on autoencoders at classification.\n\nRepresentation learning without generative models is an interesting research direction, and this paper represents a nice contribution toward this goal. The experiments demonstrate wins over some autoencoder baselines, but the reported numbers are far worse than old unsupervised feature learning results on e.g. CIFAR-10. There are also a few technical inaccuracies and an insufficient discussion of prior work (CPC). I don't think this paper should be accepted in its current state, but could be persuaded if the authors address my concerns.\n\nStrengths:\n+ Interesting new objectives for representation learning based on increasing the JS divergence between joint and product distributions\n+ Good set of ablation experiments looking at local vs global approach and layer-dependence of classification accuracy\n+ Large set of experiments on image datasets with different evaluation metrics for comparing representations\n\nWeaknesses:\n- No comparison to autoencoding approaches that explicitly maximize information in the latent variable, e.g. InfoVAE, beta-VAE with small beta, an autoencoeder with no regularization, invertible models like real NVP that throws out no information. Additionally, the results on CIFAR-10 are worse than a carefully tuned single-layer feature extractor (k-means is 75%+, see Coates et al., 2011). \n- Based off Table 9, it looks like DIM is very sensitive to hyperparameters like gamma for classification. Please discuss how you selected hyperparameters and whether you performed a similar scale sweep for your baselines.\n- The comparison with and discussion of CPC is lacking. CPC outperforms JSD in almost all settings, and CPC also proposed a \"local\" approach to information maximization. I do not agree with renaming CPC to NCE and calling it DIM(L) (NCE) as the CPC and NCE loss are not the same. Please elaborate on the similarties and differences!\n- The clarity of the text could be improved, with more space in the main text devoted to analyzing the results. Right now the paper has an overwhelming number of experiments that don't fit concisely together (e.g. an entirely new generative model experimentsin the appendix).\n\nMinor comments:\n- As noted by a commenter, it is known that MI maximization without constraints is insufficient for learning good representations. Please cite and discuss.\n- Define local/global earlier in the paper (intro?). I found it hard to follow the first time.\n- Why can't SOMs represent complex relationships?\n- \"models with reconstruction-type objectives provide some guarantees on the amount of information encoded\": what do you mean by this? VAEs have issues with posterior collapse where the latents are ignored, but they have a reconstruction term in the objective.\n- \"JS should behave similarly as the DV-based objective\" - do you have any evidence (empirical or theoretical) to back up this statement? As you're maximizing JSD and not KL, it's not clear that DIM can be thought of as maximizing MI.\n- Have you tried stochastic encoders? This would make matching to a prior much easier and prevent the introduciton of another discriminator.\n- I'm surprised NDM is much smaller than MINE given that your encoder is deterministic and thus shouldn't throw out any information. Do you have an explanation for this gap?\n- there's a trivial solution to local DIM where the global feature can directly memorize everything about the local features as the global feature depends on *all* local features, including the one you're trying to maximize information with. Have you considered masking each individual local feature before computing the global feature to avoid this trivial solution? \n\n-----------------------\n\nUpdate: Apologies for the slow response. The new version with more baselines, comparisons to CPC, discussion of NCE, and comparisons between JS and MI greatly improve the paper! I've increased my score (5 -> 7) to reflect the improved clarity and experiments. ",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Mutual information-based representation learning with additional tricks for performance gain.",
            "review": "This paper presents a representation learning approach based on the mutual information maximization. \nThe authors propose the use of local structures and distribution matching for better acquisition of representations (especially) for images.\n\nStrong points of the paper are: \n* This gives a principled design of the objective function based on the mutual information between the input data point and output representation. \n* The performance is gained by incorporating local structures and matching of representation distribution to a certain target (called a prior).\n\nA weak point I found was: \nThe local structure and evaluation are specialized for classification task of images. \n\nQuestions and comments.\n* Local mutual information in (6) may trivially be maximized if the summarizer f (E(x) = f \\circ C(x) with \\psi omitted for brevity) concatenates all local features into the global one.\nHow was f implemented? Did you compare this concatenation approach?\n* Can we add DIM like a regularizer to an objective of downstream task? \nIt would be very useful if combining an objective of classification/regression or reinforcement learning with the proposed (8) is able to improve the performance of the given task.\n* C^(i)_\\psi(X) in (6), but X^(i) in (8): are they the same thing?",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Possibly important paper.",
            "review": "Revision 2: The new comparisons with CPC are very helpful.  Most of my other comments are addressed in the response and paper revision.  I am still uncomfortable with the sentence \"Our method ... compares favorably with fully-supervised learning on several classification tasks in the settings studied.\"  This strongly suggests to me that you are claiming to be competitive with SOTA supervised methods.  The paper does not contain supervised results for the resnet-50 architecture.  I would recommend that this sentence should either be dropped from the abstract or have the phrase \"in the settings studied\" replaced by \"for an alexnet architecture\".  If you have supervised results for resnet-50 they should be added to table 3 and the abstract could be adjusted to that.  I apologize that this is coming after the update deadline (I have been traveling).  The authors should simply consider the reaction of the community to over-claiming.  Because of the new comparisons with CPC on resnet-50 I am upping my score.  My confidence is low only because the real significance can only be judged over time.\n\nRevision 1: This is a revision of my earlier review.  My overly-excited earlier rating was based on tables 1 and 2 and the claim to have unsupervised features that are competitive with fully-supervised features. (I also am subject to an a-priori bias in favor of mutual information methods.)  I took the authors word for their claim and submitted the review without investigating existing results on CIFAR10.  It seems that tables 1 and 2 are presenting extremely weak fully supervised baselines.  If DIM(L) can indeed produce features that are competitive with state of the art fully supervised features, the result is extremely important.  But this claim seems misrepresented in the paper.\n\nOriginal review:\n\nThere is a lot of material in this paper and I respect this groups\nhigh research-to-publication ratio. However, it might be nice to have\nthe paper more focused on the subset of ideas that seem to matter.\n\nMy biggest comment is that the top level spin seems wrong.\nSpecifically, the paper focuses on the two bullets on page 3 ---\nmutual information and statistical constraints.  Here mutual\ninformation is interpreted as the information between the input and\noutput of a feature encoder.  Clearly this has a trivial solution\nwhere the input equals the output so the second bullet --- statistical\nconstraints --- are required.  But the empirical content of the paper\nstrongly undermines these top level bullets.  Setting the training\nobjective to be the a balance of MI between input and output under a\nstatistical consrtraint leads to DIM(G) which, according the results in\nthe paper, is an empirical disaster.  DIM(L) is the main result and\nsomething else seems to be going on there (more later).  Furthermore,\nthe empirical results suggest that the second bullet --- statistical\nconstraints --- is of very little value for DIM(L). The key ablation\nstudy here seems to be missing from the paper.  Appendix A.4 states\nthat \"a small amount of the [statistical constraint] helps improve\nclassification results when used with the [local information\nobjective].  No quantitative ablation number is given.  Other measures\nof the statistical constraint seem to simply measure to what extent\nthe constraint has been successfully enforced.  But the results\nsuggest that even successfully enforcing the constraint is of little,\nif any, value for the ability of the features to be effective in\nprediction.  So, it seems to me, the paper to really just about the\nlocal information objective.\n\nThe real power house of the paper --- the local information objective\n--- seems related to mutual information predictive coding as\nformalized in the recent paper from deep mind by van den Oord et al\nand also an earlier arxiv paper by McAllester on information-theoretic\nco-training.  In these other papers one assumes a signal x_1, ... x_T\nand tries to extract low dimensional features F(x_t) such that F(x_1),\n..., F(x_t) carries large mutual information with F(x_{t+1}).  The\nlocal objective of this paper takes a signal x1, ..., x_k (nXn\nsubimages) and extracts local features F(x_1), ... F(x_k) and a global\nfeature Y(F(x_1), ..., F(x_k)) such that Y carries large mutual\ninformation with each of the features F(x_i).  These seem different\nbut related.  The first seems more \"on line\" while the second seems\nmore \"batch\" but both seem to be getting at the same thing, especially\nwhen Y is low dimensional.\n\nAnother comment about top level spin involves the Donsker-Varadhan\nrepresentation of KL divergence (equation (2) in the paper).  The\npaper states that this is not used in the experiments.  This suggests\nthat it was tried and failed.  If so, it would be good to report this.\nAnother contribution of the paper seems to be that the mutual\ninformation estimators (4) and (5) dominate (2) in practice.  This\nseems important.\n\n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}