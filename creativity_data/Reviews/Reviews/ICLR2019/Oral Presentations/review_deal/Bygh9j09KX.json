{
    "Decision": {
        "metareview": "This paper proposes a hypothesis about the kinds of visual information for which popular neural networks are most selective.  It then proposes a series of empirical experiments on synthetically modified training sets to test this and related hypotheses.  The main conclusions of the paper are contained in the title, and the presentation was consistently rated as very clear.  As such, it is both interesting to a relatively wide audience and accessible.\n\nAlthough the paper is comparatively limited in theoretical or algorithmic contribution, the empirical results and experimental design are of sufficient quality to inform design choices of future neural networks, and to better understand the reasons for their current behavior.\n\nThe reviewers were unanimous in their appreciation of the contributions, and all recommended that the paper be accepted.\n\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Oral)",
        "title": "Area chair recommendation"
    },
    "Reviews": [
        {
            "title": "Interesting paper, purely empirical and no novelty",
            "review": "The paper is well written and easy to follow. It was a nice read for me.\n\nThe paper studies the CNNs like AlexNet, VGG, GoogleNet, ResNet50 and shows that these models are heavily biased towards the texture when trained on ImageNet. The paper shows human evaluations and compares model accuracies when various transformations like cue hypothesis, texture hypothesis (terms coined in the paper) are applied to study texture vs shape importance. The paper shows various results on different models clearly and results are easily interpretable. The paper then proposed a new ImageNet dataset which is called Stylized-ImageNet (SIN) where the texture is replaced with randomly selected painting style.\n\nI believe that this is a good empirical study which is needed to understand why the ImageNet features are good (supervised training) and this can inform research in self-supervision, few shot learning domains.\n\nThe paper is an empirical paper and is presenting a quantitive study of role of texture which others have already presented like Gatys et al. 2017. The paper itself has no novel contributions. The paper notes \"novel Stylized-ImageNet dataset\" and shows that models can learn shape/texture features both but there is not much detail/explanation on why \"Stylized\" is the novel approach and also the methodology of constructing data by replacing with painting from AdaIN style transfer (Huang & Belongie, 2017) is not discussed/explored. More specifically, there is no ablation on other ways this dataset could have been constructed and why style transfer was picked as the choice, why was AdaIN chosen. While the choice is valid, I think these questions need to be answered if we have to consider it \"novel\". Additionally, I would like answers to the following questions:\n\n1. In Figure 4, ResNet50 results are missing. I would be very interested in seeing those results. Can authors show those results?\n2. Did authors study deeper networks like RN101/152 and do the observations about texture still hold?\n3. Did authors consider inspecting if the models have same texture biases when trained on other datasets like COCO? If yes, can you share your results?\n4. In Figure 5, can authors also show the results of training VGG, AlexNet, GoogleNet models on SIN dataset? I believe otherwise the results are incomplete since Fig. 4 shows the biases of these models on IN dataset but doesn't show if these biases are removed by training on SIN.\n5. In Section 3.3, Transfer learning, authors show improvement on VOC 2007 Faster R-CNN . Do authors have explanation on why this gain happens? how's the texture learning in pretext task (like image classification training on SIN dataset) tied to the transfer learning no different dataset?\n6. What are the results of transfer learning on other datasets like COCO, Faster R-CNN?",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea and  nice demonstration",
            "review": "This paper talks about the behavior bias between human and advanced CNN classifier when classifying objects. A clear conclusion is that DNN classifiers lean on texture cues more than human, which is in contrast to empirical evidence. The experimental results are delighting and convincing to some extent. This paper is also inspiring and potentially useful to interpret how CNN works in object classification task. \n\nNevertheless, I have several small issues: \n-\tI like the writing of this paper, fluent description and clear topic. Besides, it provides sufficient information about experiment details, thus I think the experiments are fully reproducible. But I want to remind the authors to downplay their claims. Some sentences are not with academic rigor. i.e. “Textures, not object shapes, are the most important cues for CNN object recognition.”I don’t think it a good idea to claim textures as the “most important”cue.\n-\tAlthough adequate experiments are conducted on ResNet-50 on ImageNet, I miss experiments on a different object classification dataset i.e. PASCAL VOC, and a different network backbone such as very deep ResNet-152 or wider DenseNet. This lies in the concern that a different (deeper or wider) framework may behave quite differently and also the slightly shifted data distribution may induce controversial results. The adopted network ResNet-50, AlexNet, VGG-16 and GoogLeNet are not deep enough or either wide as DenseNet. Although transfer learning experiment is carried out upon PASCAL VOC, it’s not straightforward and not so truly telling. We’re curious about universal conclusions rather than that based on one dataset or network architecture of the same category. As a matter of fact, I’m nearly convinced by the provided results. But I think the demanding experiments will make the conclusions more solid. \n\nBesides, I think the constructed dataset is beneficial to further research or fair comparison of future works, and I wonder the authors’ intention to publish such a dataset in the future. \n\nI would raise my scores if the aforementioned problems are convincingly checked and solved.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of ImageNet-trained CNNs are biased towards texture",
            "review": "Review of ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. \n\nIn this submission, the authors provide evidence through clever image manipulations and psychophysical experiments that CNNs image recognition is strongly influenced by texture identification as opposed the global object shape (as opposed to humans). The authors attempt to address this problem by using image stylization to augment the training data. The resulting networks appear much more aligned with human judgements and less biased towards image textures.\n\nIf the authors address my major concerns, I would increasing my rating 1-2 points.\n\nMajor Comments:\n\nThe results of this paper are quite compelling and address some underlying challenges in the literature on how CNN's function. I particularly appreciated Figure 5 demonstrating how the resulting stylized-augmented networks more closely align with human judgements. Additionally, it is surprising to me how poor BagNet performs on Stylized-ImageNet (SIN) implying that ResNet-50 trained on Stylized ImageNet may be better perceptually aligned with global object structure. Very cool.\n\n1. Please make sure to tone down the claims in your manuscript. Although I share enthusiasm for your results, please recognize that stating that your results are 'conclusive' is premature and not appropriate. (Conclusive requires more papers and much work by the larger scientific community for a hypothesis to become readily accepted). Some sentences of concern include:\n\n  --> \"These experiments provide conclusive behavioural evidence in favour of the texture hypothesis\"\n  --> \"we conclude the following: Textures, not object shapes, are the most important cues for CNN object recognition.\"\n\nI would prefer to see language such as \"We provide evidence that textures provide a more powerful statistical signal then global object shape for CNNs.\" or \"We provide evidence that CNNs are overly sensitive to textures in comparison to humans perceptual judgements\". This would be more measured and better reflect what has been accomplished in this study. Please do a thorough read of the rest of your manuscript and identify other text accordingly.\n\n2. Domain shifts and data augmentation. I agree with your comment that domain shifts present the largest confound to Figure 2. The results of Geirhos et al, 2018 (Figure 4) indicate that individual image augmentations/distortions do not generalize well. Given these results, I would like to understand what image distortions were used in training each and all of your networks. Did you try a baseline with no image distortions (and/or just Stylized-ImageNet)?\n\nAlthough the robustness in Figure 6 are great, how much of this can be attributed solely to Stylized-ImageNet versus the other types of image distortions/augmentations in each network. For instance, would contrast-insensitivity in Stylized-ImageNet diminish substantially if no contrast image distortion were used during training?\n\n3. Semantics of 'object shape'. I suspect that others in the field of computer vision may take issue with your definition of 'object shape'. Please provide a crisp definition of what you test for as 'object shape' in each of your experiments (i.e. \"the convex outline of object segmentation\", etc.).\n\nMinor Comments:\n\n- Writing style in introduction. Rather then quoting phrases from individual papers, I would rather see you summarize their ideas in your own language and cite accordingly. This would demonstrate how you regard for their ideas and how these ideas fit together.\n\n- Figure 2. Are people forced to select a choice or could they select 'I don't know'? Did you monitor response times to see if the manipulated images required longer times for individuals to pass decisions? I would expect that for some of the image manipulations that humans would have less confidence about their choices and that to be reflected in this study above and beyond an accuracy score.\n\n- In your human studies, please provide some discussion about how you monitored performance to guard against human fatigue or lack of interest.\n\n- Why did you use AdaIN instead of the original Gatys et al optimization method for image stylization? Was there some requirement/need for fast image stylization?\n\n- Do you have any comment on the large variations in the results across class labels in Figure 4? Are there any easy explanations for this variation across class labels?\n\n- Please use names of Shape-ResNet, etc. in Table 2.\n\n- Are Pascal-VOC mAP results with fixed image features or did you fine-tune (back-propagate the errors to update the image features) during training? The latter would be particularly interesting as this would indicate that the resulting network features are better generic features as opposed to having used better data augmentation techniques.\n\n- A.2. \"not not used in the experiment\" --> \"not used in the experiment\"\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}