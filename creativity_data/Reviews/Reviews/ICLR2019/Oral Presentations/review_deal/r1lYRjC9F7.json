{
    "Decision": {
        "metareview": "All reviewers agree that the presented audio data augmentation is very interesting, well presented, and clearly advancing the state of the art in the field. The authorsâ€™ rebuttal clarified the remaining questions by the reviewers. All reviewers recommend strong acceptance (oral presentation) at ICLR. I would like to recommend this paper for oral presentation due to a number of reasons including the importance of the problem addressed (data augmentation is the only way forward in cases where we do not have enough of training data), the novelty and innovativeness of the model, and the clarity of the paper. The work will be of interest to the widest audience beyond ICLR.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Oral)",
        "title": "metareview"
    },
    "Reviews": [
        {
            "title": "Large dataset of parallel MIDI/Audio enables better piano music transcription, synthesis, and generation",
            "review": "This paper describes a new large scale dataset of aligned MIDI and audio from real piano performances and presents experiments using several existing state-of-the-art models for transcription, synthesis, and generation. As a result of the new dataset being nearly an order of magnitude larger than existing resources, each component model (with some additional tuning to increase capacity) yields impressive results, outperforming the current state-of-the-art on each component task. \nOverall, while the modeling advances here are small if any, I think this paper represents a solid case study in collecting valuble supervised data to push a set of tasks forward. The engineering is carefully done, well-motivated, and clearly described. The results are impressive on all three tasks. Finally, if the modeling ideas here do not, the dataset itself will go on to influence and support this sub-field for years to come. \nComments / questions:\n-Is MAPS actually all produced via sequencer? Having worked with this data I can almost swear that at least a portion of it (in particular, the data used here for test) sounds like live piano performance captured on Disklavier. Possibly I'm mistaken, but this is worth a double check.\n-Refering to the triple of models as an auto-encoder makes me slightly uncomfortable given that they are all trained independently, directly from supervised data. \n-The MAESTRO-T results are less interesting than they might appear at first glance given that the transcriptions are from train. The authors do clearly acknowledge this, pointing out that val and test transcription accuracies were near train accuracy. But maybe that same argument could be used to support that the pure MAESTRO results are themselves generalizable, allowing the authors to simplify slightly by removing MAESTRO-T altogether. In short, I'm not sure MAESTRO-T results offer much over MAESTRO results, and could therefore could be omitted. \n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Learning to generate piano music via MIDI layer",
            "review": "The paper addresses the challenge of using neural networks to generate original and expressive piano music.  The available techniques today for audio or music generation are not able to sufficient handle the many levels at which music needs to modeled.  The result is that while individual music sounds (or notes) can be generated at one level using tools like WaveNet, they don't come together to create a coherent work of music at the higher level.  The paper proposes to address this problem by imposing a MIDI representation (piano roll) in the neural modeling of music audio that serves as an intermediate (and interpretable) representation between the analysis (music audio -> MIDI) and synthesis (MIDI -> music audio) in the pipeline of piano music generation.  In order to develop and validate the proposed learning architecture, the authors have created a large data set of aligned piano music (raw audio along with MIDI representation).  Using this data set for training, validation and test, the paper reports on listening tests that showed slightly less favorable results for the generated music.  A few questions and comments are as follows.  MIDI itself is a rich language with ability to drive the generation of music using rich sets of customizable sound fonts.  Given this, it is not clear that it is necessary to reproduce this function using neural network generation of sounds.  The further limitation of the proposed approach seems to be the challenge of decoding raw music audio with chords, multiple overlayed notes or multiple tracks.  MIDI as a representation can support multiple tracks, so it is not necessarily the bottleneck.  How much does the data augmentation (audio augmentation) help?",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Put three state of the art models together and get impressive results for modeling piano music.",
            "review": "This paper combines state of the art models for piano transcription, symbolic music synthesis, and waveform generation all using a shared piano-roll representation.  It also introduces a new dataset of 172 hours of aligned MIDI and audio from real performances recorded on Yamaha Disklavier pianos in the context of the piano-e-competition.  \n\nBy using this shared representation and this dataset, it is able to expand the amount of time that it can coherently model music from a few seconds to a minute, necessary for truly modeling entire musical pieces.\n\nTraining an existing state of the art transcription model on this data improves performance on a standard benchmark by several percentage points (depending on the specific metric used).\n\nListening test results show that people still prefer the real recordings a plurality of the time, but that the syntheses are selected over them a fair amount.  One thing that is clear from the audio examples is that the different systems produce output with different equalization levels, which may lead to some of the listening results.  If some sort of automatic mastering were done to the outputs this might be avoided.\n\nWhile the novelty of the individual algorithms is relatively meager, their combination is very synergistic and makes a significant contribution to the field.  Piano music modeling is a long-standing problem that the current paper has made significant progress towards solving.\n\nThe paper is very well written, but there are a few minor issues:\n* Eq (1) this is really the joint distribution between audio and notes, not the marginal of audio\n* Table 4: What do precision, recall, and f1 score mean for notes with velocity?  How close does the system have to be to the velocity to get it right?\n* Table 6: NLL presumably stands for Negative Log Likelihood, but this should be made explicity\n* Figure 2: Are the error bars the standard deviation of the mean or the standard error of the mean?\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}