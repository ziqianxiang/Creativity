{
    "Decision": {
        "metareview": "The paper presents a novel strategy for statistically motivated feature selection i.e. aimed at controlling the false discovery rate. This is achieved by extending knockoffs to complex predictive models and complex distributions via (multiple) generative adversarial networks. \n\nThe reviewers and ACs noted weakness in the original submission which seems to have been fixed after the rebuttal period -- primary related to missing experimental details. There was also some concern (as is common with inferential papers) that the claims are difficult to evaluate on real data, as the ground truth is unknown. To this end, the authors provide empirical results with simulated data that address this issue. There is also some concern that more complex predictive models are not evaluated.\n\nOverall the reviewers and AC have a positive opinion of this paper and recommend acceptance.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Oral)",
        "title": "Metareview"
    },
    "Reviews": [
        {
            "title": "Good paper with limitations in empirical evaluation",
            "review": "The paper presents a deep-learning-based version of the knockoff method by Candes et al. for FDR control in feature selection problems to avoid assumptions posed on the distribution of features by the original method. In a supervised feature selection setting, the goal of the knockoff framework is to select a set of input features that are statistically associated to an output variable Y, while controling the FDR. The basic idea behind knockoff is to generate artificial input feature vectors, (i.e. knockoffs) that are independent of Y, when conditioned on the real feature vector X, but after swapping arbitrary elements with X, are distributed as X. Sets of associated features and FDR estimates are obtained by contrasting suited feature selction criteria that measure associations of knockoffs and real features with the target Y.\nLasso coefficients and random forests are used in the paper.\nThe main contribution of the current paper is the use of a GAN to generate knockoffs, and, according to the paper in particular, the use of a discriminator that tries to identify the positions of knockoff features that have been swapped into real feature vectors X to control equlaity in distribution between knockoffs and feature vectors. Additionally, a Wasserstein discriminator and a MINE loss are used to control the knockoff distribution. Otherwise, the paper follows the standard knockoff procedure.\n\nThe approach is evaluated in simulations, varying two degrees of freedom: i) Gaussian distribute features vs. features that follow mixtures of Gaussians. ii) Gaussian and logit distributions of Y conditioned on linear functions of a subset of X features.\nUsing a Lasso-based feature selection criterion, the GAN knockoff method achieves the highest TP rates among a number of methods that empirically are shown to roughly control a target FDR of 10%. However, the figures are too small to judge FDR control more fine-graoned than 10% +-5%. Here, I would have wished i) a higher resolution to demonstrate FDR control, as well as an evaluation of different FDR cutoffs, especially including smaller cutoffs.\n\nAdditionally, an appliction to real data is performed. However, this evaluation is not very informative for several reasons. i) The dataset is not specified, making the experiment intransparent and non-reproducible. ii) A different feature selection criterion based on random forests is used, compared to the Lasso-based criterion in the synthetic experiments. iii) A different FDR cutoff of 5% has been used compared to the simulations. It is not clear, if the method shows FDR control in synthetic settings at 5%. For these reasons, the real-world experiment is hardly comparable to the synthetic settings.\n\n\nThe paper is relatively well-written and clear. Discussion of related work is appropriate.\n\nIn sum, the paper has some limitations in the empirical evaluation, but nonetheless the use of a GAN promises significant gains in statistical power.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Compelling approach to an important problem in FDR-controlled feature selection, with only minor weaknesses.",
            "review": "This manuscript describes an extension of the knockoff framework, which is designed to carry out feature selection while controlling the FDR among selected features, to settings in which the generative distribution of the features is not Gaussian. Specifically, the authors employ a GAN (with several modifications and additions) in which the generator produces knockoffs and the discriminator attempts to identify which features have been swapped between the original and the knockoff.\n\nThe method works as follows: (1) A conditional generator takes random noise and the real features as input, and outputs knockoff features. (2) A modified discriminator is used in such a way that the generator learns to generate knockoffs satisfying the necessary swap condition, so as to control the FDR of the knockoff procedure. (3) A power network uses Mutual Information Neural Estimation (MINE) to estimate the mutual information between each feature and its knockoff counterpart, so as to maximize the power of the knockoff procedure.\n\nResults are provided on synthetic and real data. As for the synthetic data, when the underlying feature distribution is Gaussian, the proposed method, KnockoffGAN, performs almost as well as the original knockoff and outperforms the BHq method; when the underlying feature distribution is non-Gaussian, KnockoffGAN dominates both the original knockoff and BHq methods. As for the real data, the authors claim to identify nine relevant features for cardiovascular disease and eight relevant features for diabetes, whereas the original knockoff procedure identifies zero features from the same data.\n\nGeneral comments: \n\nThis is an extremely impressive piece of work. The manuscript itself is a pleasure to read, and the results clearly demonstrate that the proposed KnockoffGAN both controls FDR and achieves power comparable to the original knockoff procedure in the Gaussian setting and much better than the original knockoff when the underlying distribution is not Gaussian.\n\nStrengths: \n\nThe combination of GANs and knockoff filter is a very promising and intriguing idea.\n\nThe use of the modified discriminator to ensure that the generated knockoffs satisfy the necessary swap condition is novel and intuitively sound.\n\nThe use of MINE to maximize power by maximizing the mutual information between each feature and its knockoff counterpart is also interesting.\n\nThe paper is well written, reads smoothly and the ideas are well exposed.\n\nThe illustrative figure is straightforward.\n\nWeaknesses:\n\nIntuitively, the modified discriminator and the power network should conflict with each other. I expect it was tricky to achieve a good tradeoff between two, but the authors failed to elaborate on these details.\n\nThe authors do not provide the design details of the neural networks. How dependent on the specific parametrization of the network architecture is the performance? How does the training order of four networks matter to the performance? \n\nThe manuscript should cite [[ Jaime Roquero Gimenez, Amirata Ghorbani, and James Zou. \"Knockoffs   for the mass: new feature importance statistics with false discovery   guarantees.\" arXiv:1807.06214, 2018. ]] which proposes a way to generate knockoffs for a Gaussian mixture model, and this method should be included in the relevant supplementary figure.\n\nIn Section 5.1.4, I would like to know, for a fixed data set, how the regularization affects the final values of the other loss terms.\n\nThe analysis of real data in Section 5.2 is unsatisfying in several respects.\n\nFirst, there is an unfortunate oversight in Table 1: the text refers to three features that are \"trivial,\" but only one of these is marked with an asterisk.  This leaves open the question of whether there are other trivial features beyond the three mentioned in the text. In addition, it is not clear exactly what it means for a feature to be \"trivial\" in this context.\n\nThis point gets to a deeper problem with the evaluation, which is that we are told, with no evidence, that these features are supported by literature in PubMed.  I would like to see two things here.  First, it seems obvious to me that if you are going to say that there is support in PubMed, you are obliged to actually report the citations that supposedly give this support. This could be done in the appendix. Equally importantly, there is a potential here for ascertainment bias which should be combatted in some fashion.  Presumably, some human expert had to do the PubMed searches to make this assessment.  I would like to know how \"permissive\" this assessor is.  To assess this, one could give the assessor a collection of terms, some of which were selected by KnockoffGAN and some at random, and then report the results. Obviously, some features that are significant may not be in the list of selected terms (because KnockoffGAN does not achieve 100% power) and so may appear as false negatives. But without some assessment like this, I have trouble believing this assessment.\n\nA related point is that it seems quite unfortunate that the authors chose a data set that cannot be described at all due to the anonymity constraint.  At the very least, it seems that we should be told the dimensionality of the data set. The Knockoff literature contains real data sets that could have been used here.\n\nMinor comments:\n\nOn the first page, the sentence beginning \"On the other hand,\" should clarify that this is only in expectation.\n\np. 3: Missing right paren after [7].\n\np. 5: Write out “Gaussian process.”\n\np. 5: \"as little\" -> \"as little as possible\"\n\np. 6: \"to show that in\" -> \"to show, in\"\n\nIn Figures 2-5, add a horizontal line at 10% FDR for reference.\n\np. 10: \"features ones\" -> \"features\"\n\nNote to program committee:\n\nI did not review the technical details of the proof in the appendix.\n\n\n",
            "rating": "10: Top 5% of accepted papers, seminal paper",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review",
            "review": "This paper introduces a novel feature selection method by utilizing GAN to learn the distributions. The novelty of this paper is to incorporate two recent works, i.e. knockoff for feature selection and W-GAN for generative models. Compared to the latest knockoff work which requires a known multivariate Gaussian distribution for the feature distribution, the proposed work is able to generate knockoffs for any distribution and without any prior knowledge of it.\n\nPros: This paper is very well written. I enjoyed reading this paper. It is novel and addresses an important problem. The numerical study clearly shows the advantage of the proposed work. \n\nCons:\n\nQ1: In the discriminator, instead of training with respect to the full loss, the authors consider to mask some information by using a multivariate Bernoulli random variable $B$ with success probability 0.9. Then the discriminator needs to predict only when $B_i = 0$. Can the authors provide some justification of such choice of the parameters? This choice is a little bit mysterious to me.\n\nQ2: How sensitive are the hyper-parameters $\\eta$ (set to 10 in the experiments), $\\lambda$, and $\\mu$ (set to 1 in the experiments)?\n\nQ3: In the real data example, the feature selection performance is less justified as there is no truth. One suggestion is to evaluate the prediction errors using the selected features and compare with the benchmarks.\n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}