{
    "Decision": {
        "metareview": "The paper investigates a novel formulation of a stochastic, quasi-Newton optimization strategy based on the natural idea of relaxing the secant conditions.  This is an interesting and promising idea, but unfortunately none of the reviewers recommended acceptance.  The reviewers unanimously fixated on weaknesses in the paper's technical presentation.  In particular, the reviewers expressed some dissatisfaction with many aspects, including:\n\n- Key details of the experimental evaluation were omitted (particularly concerning configuration of the baseline competitors), which is an essential aspect of reproducibility.  One consequence is that the reviewers were not confident in the veracity of the experimental comparison.\n\n- The reviewers struggled with a lack of clarity and accurate rendering of some key technical details.  An example is dissatisfaction with the non-symmetry of the inverse Hessian approximation, which was not fully alleviated by the author responses.\n\n- The proposed approach does not appear to possess any intrinsic advantage over standard methods from a computational complexity perspective.\n\nI think this is promising work, but a careful revision that strengthened the underlying technical claims appears necessary to make this a solid contribution.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting idea, but the paper requires further refinement"
    },
    "Reviews": [
        {
            "title": "interesting reformulation of Quasi Newton direction",
            "review": "The authors present an interesting variation of the standard QN methods. Their main point of departure from LBFGS/SR1 is in constructing a simpler Hessian inverse approximation. Recall that SR1 and LBFGS updates all satisfy the secant equation for each of the `m` previous gradient differences stored in memory. The authors choose to get \"close\" to satisfying the equations by solving an l_2 penalization of the secant equations. \n\nThe resulting algorithm is interesting, but it is not clear from the paper what the claimed advantage of doing this is. The LBFGS and SR1 unrolled update rules for H (Hessian inverse approximation) is O(m^2 d) (Sec 7.2 NW 2006),  and this seems to be the same for the authors' method, where the main matrix R_k that forms H has the same order. (BTW, did you mean 'd' in place of 'n'  the computational order discussion preceding Sec  4.2?)\n\nThe experiments show that this method's performance is impressive compared to an LBFGS implementation provided by Bollapragada 2018 , but as I recall that paper presented a variable/increasing batch method, while the authors' method uses fixed batches (as far as I can tell) so it is not clear that comparison on time alone is sufficient. The advantage over LBFGS and SGD seen in MNIST seems to go away by the CIFAR example, so it is unclear what might happen in larger problems like ImageNet. \n\nI am also not able to see the difference between the 'stochastic' line search presented here and the standard backtracking method as applied to mini-batch evaluated estimates. What is different, and new that brings in consideration for the noise? I recall that bollapragada 2018 had an additional variance based rule to check. Some more conservative values are chosen for the step length, but I do not see the justification presented in the appendix, esp Eq 47 : p is not independent from g here, being calucated as p=Hg,  so E[p^t g] is not equal to the product of  the individual expectations.\n\nSome key points were left out in the discussion of the experiments. This is a common slip up when writing conf papers these days, but please do consider discussing the settings of parameters like mini-batches sizes , value of \\lambda in the H derivation, how one calculates the \\sigma^2_g within the algorithm presented in the Appendix. The last must include\nan extra computational cost, or are you using Adam style online variance estimator?\n\nThe MSE error alone seems insufficient in the results. Please publish the test mis-classification results too. Also, why is the MSE loss used with the softmax in CIFAR? Shouldn't cross-entropy, better justified theoretically, be better justified?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Novel idea, but not without issues to be addressed",
            "review": "This paper presents a new quasi-Newton type method for stochastic optimization problems. The primary contributions of the paper include a new stochastic linesearch method as well as a novel way to incorporate second order information which is different from existing approaches such as BFGS or L-BFGS. \n\nIn terms of the clarity, I think this is a very well-written paper with nice organization. The paper does have some typos, though. \n\nIn terms of significance, how to incorporate second-order information in stochastic optimization has long been an important research topic. Most existing stochastic quasi-Newton methods use L-BFGS method to incorporate second order information and choose a fixed, small stepsize, with the only differences being how to compute the curvature pair (s_k, y_k). Therefore, this paper is addressing a very important question and has made respectable attempt to use mechanisms other than L-BFGS method and to incorporate a linesearch scheme. \n\nSpecifically, the paper relaxes the secant equation, which is natural for the stochastic settings because the difference in gradients y_k is computed from stochastic gradients, and the true Hessian only satisfies the secant equation in expectation. I believe replacing the secant equation is an important and promising direction.\n\nHowever, there are concerns about the new approach proposed in this paper:\n\n1.\tThe resulting Hessian inverse approximation in (14) is no longer symmetric, or guaranteed to be positive definite. While the underling true Hessian might not be positive definite because of the nonconvexity, it is always symmetric. Is it possible to impose symmetricity as a constraint in (12)?\n\n2.\tWhat is the correct way to choose regularization parameter λ in (14)?\n\nThe paper also proposes a stochastic linesearch algorithm. For this part, there are several concerns as well:\n\n1.\tThe assumption that the covariance of gradient estimator is a constant multiple of identity is a strong and unrealistic assumption, which is never satisfied in machine learning. \n\n2.\tThe algorithm performs a backtracking linesearch, with the initial trial stepsize decreaing as O(1/k), which means that the stepsize used is always decreasing at least as fast as O(1/k). This is in general in stark contrast with the intuition that a O(1) stepsize should be used for a quasi-Newton method. \n\n3.\tSatisfying the Armijo condition in expectation does not lead to any useful convergence guarantee. \n\nThe paper also presented some numerical experiments. While the numerical results look promising, I would appreciate some clarification about what method they are really comparing against. For example, for LBFGS the authors cite R. Bollapragada et al. “A progressive batching L-BFGS method for machine learning”. Is the paper comparing against progressive batching L-BFGS? The results of LBFGS here seem to be very different from the paper cited. \n\nFinally, the paper could certainly benefit by making some mathematical statement more rigorous. For example, Lemma 1 and 2 are stated in expectation; however, since the algorithm is a stochastic algorithm, the whole sequence {x_k} generated is a stochastic process, and the expectation in the lemmas are conditional expectations. It is important to clarify w.r.t. what the conditional expectation is taken.\n\nIn summary, I believe that this paper has made a novel contribution. However, the author should address the concerns above.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A good attempt, but lacks sufficient explanation and reasoning",
            "review": "This paper presents a new quasi-Newton method for stochastic optimization that solves a regularized least-squares problem to approximate curvature information that relaxes both the symmetry and secant conditions typically ensured in quasi-Newton methods. In addition to this, the authors propose a stochastic Armijo backtracking line search to determine the steplength that utilizes an initial steplength of 1 but switches to a diminishing steplength in later iterations. In order to make this approach computationally tractable, the authors propose updating and maintaining a Cholesky decomposition of a crucial matrix in the Hessian approximation. Although it is a good attempt at developing a new method, the paper ultimately lacks a convincing explanation (both theoretical and empirical) supporting their ideas, as I will critique below.\n\n1. Stochastic Line Search\n\nDetermining a steplength in the stochastic setting is a difficult problem, and I appreciate the authors’ attempt to attack this problem by looking at stochastic line searches. However, the paper lacks much detail and rigorous reasoning in the description and proof for the stochastic line search.\n\nFirst, the theory gives conditions that the Armijo condition holds in expectation. Proving anything about stochastic line searches is particularly difficult, so I’m on board with proving a result in expectation and doing something different in practice. However, much of the detail on how this is implemented in practice is lacking. \n\nHow are the samples chosen for the line search? If we go along with the proposed theory, then when the function is reevaluated in the line search, a new sample is used. If this is the case, can one guarantee that the practical Armijo condition will hold? How often does the line search fail? How does the choice of the samples affect the cost of evaluating the line search?\n\nThe theory also suggests that the particular choice of c is dependent on each iteration, particularly the inner product between the true search direction and the true gradient at iteration k. Does this allow for a fixed c to be used? How is c chosen? Is it fixed or adaptive? What happens as the true gradient approaches 0?\n\nThe algorithm also places a limit on the number of backtracks permitted that decreases as the iteration count increases. What does the algorithm do when the line search fails? Does one simply take the step although the Armijo condition does not hold?\n\nIn deterministic optimization, BFGS typically needs a smaller steplength in the beginning as the algorithm learns the scale of the problem, then eventually accepts the unit steplength to obtain fast local convergence. The line search proposed here uses an initial steplength of $\\min(1, \\xi/k)$ so that in early iterations, a steplength of 1 is used and in later iterations the algorithm uses a $\\xi/k$ steplength. When this is combined with the diminishing maximum number of backtracking iterations, this will eventually yield an algorithm with a steplength of $\\xi/k$. Why is this preferred? Are the other algorithms in the numerical experiments tuned similarly?\n\nThe theory also asks for a descent direction to be ensured in expectation. However, it is not the case that $E[\\hat{p}_k^T \\hat{g}_k] = E[\\hat{p}_k]^T g_k$, so it is not correct to claim that a descent direction is ensured in expectation. Rather, the condition is requiring the angle between the negative stochastic gradient direction and search direction to be acute in expectation.\n\nAll the proofs also depend on a linear Taylor approximation that is not well-explained, and I’m wary of proofs that utilize approximations in this way. Indeed, the precise statement is that $\\hat{f}_{z’} (x + \\alpha \\hat{p}_z) = \\hat{f}_{z’} + \\alpha \\hat{p}_z’ \\hat{g}_z(x + \\bar{\\alpha} \\hat{p}_z)$, where $\\bar{\\alpha} \\in [0, \\alpha]$. How does this affect the proof?\n\nLastly, I would propose for the authors to change the name of their condition to the “Armijo condition” rather than using the term “1st Wolfe condition” since the Wolfe condition is typically associated with the curvature condition (p_k’ g_new >= c_2 p_k’ g_k), hence referring to a very different line search. \n\n2. Design of the Quasi-Newton Matrix\n\nThe authors develop an approach for designing the quasi-Newton matrix that does not strictly impose symmetry or the secant condition. The authors claim that this done because “it is not obvious that enforced symmetry necessarily produces a better search direction” and “treating the [secant] condition less strictly might be helpful when [the Hessian] approximation is poor”. This explanation seems insufficient to me to explain why relaxing these conditions via a regularized least-squares approach would yield a better algorithm, particularly in the noisy or stochastic setting. The lack of symmetry seems particularly strange; one would expect the true Hessian in the stochastic setting to still be symmetric, and one would still expect the secant condition to hold if the “true” gradients were accessible. It is also unclear how this approach takes advantage of the stochastic structure that exists within the problem.\n\nAdditionally, the quasi-Newton matrix is defined based on the solution of a regularized least squares problem with a regularization parameter lambda. It seems to me that the key to the approximation is the balance between the two terms in the objective. How is lambda chosen? What is the effect of lambda as a tuned parameter, and how does it affect the quality of the Hessian approximation? It is unclear to me how this could be chosen in a more systematic way.\n\nThe matrix also does not ensure positive definiteness, hence requiring a multiple of the gradient direction to be added to the search direction. In this case, the key parameter beta must be chosen carefully. What is a typical value of beta that is used for each of these problems? One would hope that beta is small, but if it is large, it may suggest that the search direction is primarily dominated by the stochastic gradient direction and hence the quasi-Newton matrix is not useful. The interplay of these different parameters needs to be investigated carefully.\n\nLastly, since (L-)BFGS use a weighted Frobenius norm, I am curious why the authors decided to use a non-weighted Frobenius norm to define the matrix. How does changing the norm affect the Hessian approximation?\n\nAll of these questions place the onus on the numerical experiments to see if these relaxations will ultimately yield a better algorithm.\n\n3. Numerical Experiments\n\nAs written, although the range of problems is broad and the numerical experiments show much promise, I do not believe that I could replicate the experiments conducted in the paper. In particular, how is SVRG and L-BFGS tuned? How is the steplength chosen? What (initial) batch sizes are used? Is the progressive batching mechanism used? (If the progressive batching mechanism is not used, then the authors should refer to the original multi-batch paper by Berahas, et al. [1] which do not increase the batch size and use a constant steplength.)\n\nIn addition, a more fair comparison would include the stochastic quasi-Newton method in [2] that also utilize diminishing steplengths, which use Hessian-vector products in place of gradient differences. Multi-batch L-BFGS will only converge if the batch size is increased or the steplength diminished, and it’s not clear if either of these are done in the paper.\n\nTypos/Grammatical Errors:\n- Pg. 1: Commas are needed in some sentences, i.e. “Firstly, for large scale problems, it is…”; “…compute the cost function and its gradients, the result is…”\n- Pg. 2: “Interestingly, most SG algorithms…”\n- Pg 3: Remove “at least a” in second line\n- Pg. 3: suboptimal, not sup-optimal\n- Pg. 3: “Such a solution”, not “Such at solution”\n- Pg. 3: Capitalize Lemma\n- Pg. 4: fulfillment, not fulfilment\n- Pg. 7: Capitalize Lemma\n- Pg. 11: Before (42), Cov \\hat{g} = \\sigma_g^2 I\n- Pg. 11: Capitalize Lemma\n\nSummary:\n\nIn summary, although the ideas appear to provide better numerical performance, it is difficult to evaluate if the ideas proposed in this paper actually yield a better algorithm. Many algorithmic details are left unanswered, and the paper lacks mathematical or empirical evidence to support their claims. More experimental and theoretical work is needed before the manuscript can be considered for publication.\n\nReferences:\n[1] Berahas, Albert S., Jorge Nocedal, and Martin Takác. \"A multi-batch l-bfgs method for machine learning.\" Advances in Neural Information Processing Systems. 2016.\n[2] Byrd, Richard H., et al. \"A stochastic quasi-Newton method for large-scale optimization.\" SIAM Journal on Optimization 26.2 (2016): 1008-1031.\n[3] Schraudolph, Nicol N., Jin Yu, and Simon Günter. \"A stochastic quasi-Newton method for online convex optimization.\" Artificial Intelligence and Statistics. 2007.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}