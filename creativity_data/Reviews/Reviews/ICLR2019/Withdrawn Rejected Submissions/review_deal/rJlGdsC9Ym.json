{
    "Decision": "",
    "Reviews": [
        {
            "title": "Reject due to lack of sufficient evaluation",
            "review": "Pros:\n1) Some of the simplifications might be useful.\n2) The MR algorithm sounds interesting.\nCons: \n1) The acknowledgement section was left in the review draft, violating anonymous review.\n2) Results do not seem particularly compelling (presented on just one task, and unclear how it generalizes to other tasks).\n3) Some of the justifications seem a bit strange. For eg, quoting from the paper: \"Why this assumption? Because it can't be otherwise\"\n\n",
            "rating": "3: Clear rejection",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "violation of anonymity, poor writing",
            "review": "The paper contains an acknowledge section with people's names, which is a clear violation of anonymity. In addition, the submission obviously rushed and incomplete. The writing is very poor. There are only 7 references. The experimental results are limited to toy tasks, and the result section has only 8 lines. I don't think this paper worth a serious consideration at the current state.",
            "rating": "2: Strong rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Limited experiments, limited justifications, requires additional knowledge",
            "review": "[The acknowledgment section might be violating the blind review.]\n\nThis paper claims to simplify and improve recently proposed automatic curriculum learning methods. Their experiments in section 4 are very limited to a single setup and most of the simplifications are not well-justified by any new results. Their proposed algorithm in section 5 assumes the presence of addition ordering information about tasks which needs a more thorough comparison to other related works.\n\nSection 4 introduces some simplifications. They “recommend” a few choices by referencing the figures in [2], essentially reiterating the contributions of [2] and presenting no new results. Then they propose modifications to \\eps-greedy with window algorithm of [1] by presenting results on one task. They propose to remove the moving average and essentially reintroducing the naive algorithm of [2].\n\nSection 5 proposes a new algorithm that relies on a additional knowledge of ordering of tasks. It based on a measure called “mastering rate” for a task that it is converted to an “attention” value by incorporating the ordering information. Attention values are propagated in one extra step.\n\nPositives:\nMR algorithm might be good but no compelling experiments are presented.\n\nNegatives:\n- The experiments are limited to one set of tasks introduced by this work. It is not clear if it is a good benchmark for automated curriculum learning. Section 4 does not compare to any method that does not perform any automated curriculum learning.\n- Fig. 6 and 7, the main figures of this paper, show that the baseline gets almost zero reward in most of the experiments. Basically, they are beating a weak baseline without explaining why it has failed. Maybe other algorithms in the previous work would work better in this setting.\n- There is a mistake in analyzing the algorithm of [1]. [1] does not exactly fit in Teacher-Student framework because they use Exp3.S policy that combines Boltzmann exploration with an \\eps-greedy strategy. They also have a hyperparameter \\eta that acts as the temperature of the softmax.\n- There is no ablation study on the importance and effect of each of the last 2 steps of the MR algorithm (Eq. 1, 2, 3). Does one need the ordering?\n- Works such as [3] are also related because of the use of additional ordering information.\n\n\n[1] Graves et al\n[2] Matiisen et al\n[3] Pentina, Anastasia, Viktoriia Sharmanska, and Christoph H. Lampert. \"Curriculum learning of multiple tasks.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}