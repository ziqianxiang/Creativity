{
    "Decision": {
        "metareview": "This paper is concerned with combining past approximation methods to obtain a variant of Deep Recurrent GPs. While this variant is new, 2/3 reviewers make very overlapping points about this extension being obtained from a straightforward combination of previous ideas. Furthermore, R3 is not convinced that the approach is well motivated, beyond “filling the gap” in the literature. \n\nAll reviewers also pointed out that the paper is very hard to read. The authors have improved the manuscript during the rebuttal, but the AC believes that the paper is still written in an unnecessarily complicated way. \n\nOverall the AC believes that this paper needs some more work, specifically in (a) improving its presentation (b) providing more technical insights about the methods (as suggested by R2 and R3), which could be a means of boosting the novelty.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "New model but presentation and insights need to improve."
    },
    "Reviews": [
        {
            "title": "A combination of existing sparse-spectrum techniques and deep recurrent Gaussian processes but not properly justified",
            "review": "This paper addresses the problem of modeling sequential data based on one of the deep recurrent Gaussian process (DRGP) structures proposed by Mattos et al (2016). This structure acts like a recurrent neural net where every layer is defined as a GP. One of the main limitations of the original method proposed by Mattos et al (2016) is that it is limited to a small set of covariance functions, as the variational expectations over these have to be analytically tractable.\n\nThe main contributions of this paper are the use of previously proposed inference, namely (i) the sparse spectrum (SS) of Lazaro-Gredilla et al (2010); its variational improvement by Gal and Turnner (2015) (VSS);  and the inducing-point (IP) framework of Titsias and Lawrence (2010) into the recurrent setting of Mattos et al (2016). Most (if not all) of the technical developments in the paper are straightforward applications of the results in the papers above. Therefore, the technical contribution of the paper is largely incremental. Furthermore, while it is sensible to use random-feature approximation approaches (such as SS and VSS) in GP models, it is very unclear why combining the IP framework with SS approaches makes any sense at all. Indeed, the original IP framework was motivated as a way to deal with the scalability issue in GP models, and the corresponding variational formulation yielded a nice property of an additional regularization term in the variational bound. However, making the prior over a (Equation 9) conditioned on the inducing variables U is rather artificial and lacks any theoretical justification. To elaborate on this, in the IP framework both the latent functions (f in the original paper) and the inducing inputs come from the same GP prior, hence having a joint distribution over these comes naturally. However, in the approach proposed in this paper, a is a simple prior over the weights in a linear-in-the-parameters model, and from my perspective, having a prior conditioned on the inducing variables lacks any theoretical motivation. \n\nThe empirical results are a bit of a mixed bag, as the methods proposed beat (by a small margin) the corresponding benchmarks on 6 out of 10 problems. While one would not expect a proposed method to win on all possible problems (no free lunch), it will be good to have some insights into when the proposed methods are expected to be better than their competitors. \n\nWhile the proposed method is motivated from an uncertainty propagation perspective, only point-error metrics (RMSE) are reported. The paper needs to do a proper evaluation of the full predictive posterior distributions. What is the point of using GPs otherwise?\n\nOther comments:\nI recommend the authors use the notation p(v) = … and q(v) = … everywhere rather than v ~ … as the latter may lead to confusion on how the priors and the variational distributions are defined. \nIt is unnecessary to cite Bishop to explain how one obtains a marginal distribution\nWould it be possible to use the work of Cutajar et al (2017), who use random feature expansions for deep GPs,  in the sequential setting? If so, why aren’t the authors comparing to this?\nThe analysis of Figure 1 needs expanding \nWhat are the performance values obtained with a standard recurrent neural net / LSTM?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Combination of known ideas, hard to read",
            "review": "This paper proposes deep recurrent GP models based on the existing DRGP framework, two works on sparse spectrum approximation as well as that of inducing points. In these models, uncertainty is propagated by marginalizing out the hidden inputs at every layer.\n\nThe authors have combined a series of known ideas in the proposed work. There is a serious lack of discussion or technical insights from the authors for their technical formulations: in particular, what are the non-trivial technical challenges addressed in the proposed work? Furthermore, the authors are quite sloppy in referencing equations and inconsistent in the use of their defined notations and acronyms. I also find it hard to read and understand the main text due to awkward sentence structures.\n\nHave the authors revealed their identity on page 2 of the paper? I quote: \"We refer to the report Foll et al. (2017) for a detailed but preliminary formulation of our models and experiments.\" and \"DRGP-(V)SS code available from http://github.com/RomanFoell/DRGP-VSS.\"\n\n\n\nDetailed comments are provided below:\n\nFor the first contribution stated by the authors, what are the theoretical and practical implications of the different regularization terms/properties between the lower bounds in equations 10 vs. 8? These are not described in the paper.\n\nCan the authors provide a detailed derivation of DVI for equation 13 as well as for the predictive distributions in Sectio 6.3.5?\n\nCan the authors provide a time complexity analysis of all the tested deep recurrent GPs?\n\n\nWould the authors' proposed approach be able to extend the framework of Hoang et al. (2017) (see below) that has generalized the SS approximation of Lazaro-Gredilla et al. (2010) and the improved VSS approximation of Gal & Turner (2015)?\n\nHoang, Q. M.; Hoang, T. N.; and Low, K. H. 2017. A generalized stochastic variational Bayesian hyperparameter learning framework for sparse spectrum Gaussian process regression. In Proc. AAAI, 2007–2014.\n\n\n\nMinor issues:\nJust below equation 6, equation 9, and throughout the entire paper, the authors need to decide whether to italicize their notations in bold or not.\n\nEquations are not properly referenced in a number of instances.\n\nThe authors have used their commas too sparingly, which makes some sentences very hard to parse.\n\nWhat is the difference between REVARB-(V)SS(-IP), DRGP-(V)SS(-IP), and DRGP-VSS-IP?\n\nEquation 7: LHS should be conditioned on U.\nPage 4:  (V)SSGP does not have the same...\nEquation 8: q_a and q_Z should be placed next to the expectation.\nPage 4: choosen?\nPage 5: will makes it possible?\nPage 5: DRGP-SSGP, -VSSGP, -SSGP-IP, -VSSG-IP?\nPage 5: to simplify notation, we write h^{L+1}_{Hx+1:} = y_{Hx+1:}? Such a notation does not look simplified.\n\nEquation after equation 12: On LHS, should U^(l) be a random variable?\n\nPage 17: Should the expressions begin with >=?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "DEEP RECURRENT GAUSSIAN PROCESS WITH VARIATIONAL SPARSE SPECTRUM APPROXIMATION",
            "review": "Overall Score: 7/10.\nConfidence Score: 7/10.\n\nDetailed Comments: This paper introduces various Deep Recurrent Gaussian Process (DRGP) models based on the Sparse Spectrum Gaussian Process (SSGP) models and the Variational Sparse Spectrum Gaussian Process (VSSGP) models. This is a good paper and proposed models are very sound so I recommend for acceptance although as main weakness I can say that is very technical so it can be difficult to follow. Adding more intuitive ideas, motivation and maybe a figure for each step would be a solution. Apart from that it is a really good paper, congratulations.\n\nRelated to: RNN models and Sparse Nystrom approximation.\n\nStrengths: Models are very sound, solutions are solid, the proposed methodology is correct and the empirical results and experiments are valid and properly done.\n\nWeaknesses: It is too difficult to follow and it is written in an extreme technical way. More intuitions and a proper motivation both in the abstract and introduction may be put in order to make the paper easier to read and, hence, more used by researchers and data scientists.\n\nDoes this submission add value to the ICLR community? : Yes it does, the experiments show the efficiency of the proposed methods in some scenarios and are valid methodologies.\n\nQuality:\nIs this submission technically sound?: Yes it is.\nAre claims well supported by theoretical analysis or experimental results?: Experimental results prove empirically the methods and appendixes show the analysis performed in a clear and elegant way.\nIs this a complete piece of work or work in progress?: Complete piece of work.\nAre the authors careful and honest about evaluating both the strengths and weaknesses of their work?: Yes, and I would enfatize that I have liked that some experiments are won by other methods such as GP-LSTM, they are very honest.\n\nClarity:\nIs the submission clearly written?: Yes, but it is difficult for newcomers due to the reasons that I have stated before.\nIs it well organized?: Yes it is.\nDoes it adequately inform the reader?: Yes it is.\n\nOriginality:\nAre the tasks or methods new?: Yes, they are sound.\nIs the work a novel combination of well-known techniques?: Yes it is.\nIs it clear how this work differs from previous contributions?: Yes.\nIs related work adequately cited?: Yes, being a strength of the paper.\n\nSignificance:\nAre the results important?: I would argue that they are and are a clear alternative to consider in order to solve these problems.\nAre others likely to use the ideas or build on them?: If the paper is written in a more friendly way, yes.\nDoes the submission address a difficult task in a better way than previous work?: Yes I think.\nDoes it advance the state of the art in a demonstrable way?: Yes, empirically.\n\nArguments for acceptance: Models are very sound, solutions are solid, the proposed methodology is correct and the empirical results and experiments are valid and properly done\n\nArguments against acceptance: Clarity of the paper.\n\nMinor issues and typos:\n-> (V)SS not defined before being used.\n-> Abstract should be rewritten adding a motivation and focusing more on the problems being solved and less in the details of the solutions.\n-> Recurrent indexes that go backwards (i) of Eq. 1. should be explained why are going backwards before being used like that. Newcomers may be confused.\n-> Section 2 writing style lacks a bit of cohesion, relating the paragraphs may be a solution.\n-> Q is not defined in section 3.1 paragraph 1.\n-> A valid covariance function must produce a PSD matrix, put that in section 3.1. \n-> I do not see how U marginalizes in Eq. 7, kind of confused about that, I think that it should be p(y|X,U).\n-> Section 3.4 statistics should be explained.\n\nReading thread and authors response rebuttal decision:\n=================================================\n\nI consider that the authors have perfomed a good rebuttal and reading the other messages and the authors response I also consider that my issue with clarity is solved. Hence, I upgrade my score to 7 and recommend the paper for publication.",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}