{
    "Decision": {
        "metareview": "The reviewers found that the paper needs more compelling empirical study.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Metareview"
    },
    "Reviews": [
        {
            "title": "Nested Dithered Quantization for Communication Reduction in Distributed Training",
            "review": "In this paper, the authors propose to apply dithered quantization (DQ) to the stochastic gradients computed through the training process. Though an extra noise is added to the gradient, it improves the quantization error. Hence after the noise is removed at the update server, it achieves superior results when compared against unquantized baseline.\n\nThe authors also propose a nested scheme to further reduce communication cost.\n\nThis method strictly improves over previous approaches such as QSGD and TernGrad in terms of quantization error. However, the improved quantization performance does not show up in the experiments. In Table 3, it is clear that DQSG does not significantly improve over QSG and TernGrad once there are 8 workers. And they all use the same amount of bits in communication.\n\nThe proposed NDQSG though capable of reducing the communication cost by 30%, its accuracy on CIFAR-10 shows noticeable drop.\n\nOverall, I think this method is promising, but further tuning is required to make it practical.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper but the contribution is not good enough",
            "review": "Overall, this paper is well written and clearly present their contribution.\nAlthough the idea seems to be interesting and novel, but not enough evidence to prove the efficiency, from both theoretical and numerical perspective, even though many numerical experiments are proposed.\nIn general, this paper is high level in the articles assigned to me.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Establishes a useful connection between distributed optimization and dithered quantization",
            "review": "Authors establish a connection between communication reduction in distributed optimization and dithered quantization. This allows us to understand prior approaches in a new perspective, and also motivates authors to develop two new distributed training algorithms which communication overhead is significantly reduced. The first algorithm, DQSG, uses dithered quantization to reduce the communication bits. The second algorithm, NDQSG, uses nested dithered quantization to further reduce the amount of needed communication. The usefulness of these algorithms are empirically validated by computing the raw communication bits and average entropy of them. Therefore, dithered communication seems to provide both theory and algorithm which are useful.\n\nThe paper is clearly written. It provides a succinct review of dithered quantization and previous works, and figures provide a good insight into why the algorithm works, especially Figure 3.\n\nTheorems in this paper are mostly about plugging in properties of dithered quantization into standard results in stochastic optimization, but they are still useful. The analysis of NDQSG does not seem to be as complete as that of DQSG, however. With NQSG, now workers are divided into two groups, and there would be an interesting tradeoff between assignments to these two: how should we balance two groups? This might be tricky to analyze, but it is still useful to clarify limitations and provide conjectures. At least, this could be analyzed empirically.\n\npros:\n* establishing a connection to other topic of research often facilitates productive collaboration between two fields\n* provides a new perspective to understand prior work\n* provides new useful algorithms\n\ncons:\n* experiments were conducted on small models and small datasets\n* unclear models are large enough to demonstrate the need for communication reduction; in other words, it is unclear wall-time would actually be reduced with these algorithms.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}