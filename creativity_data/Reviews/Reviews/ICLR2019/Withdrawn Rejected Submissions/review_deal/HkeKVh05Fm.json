{
    "Decision": {
        "metareview": "The authors present a method for fine grained entity tagging, which could be useful in certain practical scenarios.\n\nI found the labeling of the CoNLL data with the fine grained entities a bit confusing.  The authors did not talk about the details of how the coarse grained labels were changed to fine grained ones.  This detail is important and is missing from the paper.  Moreover, there are concerns about the novelty of the work, both in terms of the task definition and the model (see the review of Reviewer 1, e.g.).\n\nThere is consensus amongst the reviewers, in that, their feedback is lukewarm about the paper.\n\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Meta Review"
    },
    "Reviews": [
        {
            "title": "Lack of comprehensive related work and lack of clarity in the writing",
            "review": "This paper proposed a entity proposal network for named entity recognition which can be effectively detect overlapped spans. The model obtain good performance on both Multi-grained NER task and traditional NER task. The paper is in general well written, the idea of proposal network break the traditional framework of sequence tagging formulation in NER task and thus can be effectively applied to detect overlapped named entities.\n\nHowever, I still have many concerns regarding the notation, the novelty of the paper, and the comparison with related literature, especially on previous overlapped span detection NER papers. The detailed concerns and questions are as follows:\nThe notations are very confusing. Many of the notations are not defined. For example, what does $T$ in $2D_sl*2T$ below Eq. 4 indicates?  What does $R$ scores means? I guess $R$ does not equal to number of entity types, but I’m not sure what $R$ exactly indicates. If $R$ is not number of entity types, why do you need R scores for being an entity and R scores for not being an entity? And what is $t$ in Eq 5? Is that entity type id or something else?\nI’m still confused how you select the entity spans from a large number of entity candidates. In Figure 5, if the max window length is 5, there may be more span candidates than the listed 5 examples, such as t_3 t_4 t_5. How do you prune it out?\nTable 5 is weird. There is not comparison with any baselines but just a report of the performance with this system. I don’t know what point this table is showing.\nThis is not the first paper that enumerates all possible spans for NER task.The idea of enumerating possible spans for NER task has appeared in [1] and can also effectively detect overlapped span. I would like to see the performance comparison between the two systems. The enumerating span ideas has been applied in many other tasks as well such as coreference resolution [2]and SRL[3], none of which is mentioned in related work.\nI feel that most of the gain is from ELMo but not the model architecture itself, since in Table 4, the improvement from the ELMo is only 0.06. The LSTM-LSTM-CRF is without adding ELMo, which is not a fair comparison. \nThe comparison of baselines is not adequate and is far from enough. The paper only compares with LSTM+CRF frameworks, which are not designed for detecting overlapped spans. There are many papers on detecting overlapping spans, such as [4], [5] and [6]. It’s important to compare with those paper since those methods are especially designed for overlapped span NER tasks.\n[1] Multi-Task Identification of Entities, Relations, and Coreferencefor Scientific Knowledge Graph Construction, EMNLP 2018\n[2] End-to-end neural coreference resolution, EMNLP 2017\n[3] Jointly predicting predicates and arguments in neural semantic role labeling, ACL 2018\n[4] Nested Named Entity Recognition Revisited, NAACL 2018\n[5] A Neural Layered Model for Nested Named Entity Recognition, NAACL 2018\n[6] Neural Segmental Hypergraphs for Overlapping Mention Recognition, EMNLP 2018",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting task of multi-grained NER, reasonable models. ",
            "review": "\n<Summary>\nAuthors propose the “Multi-grained NER (MGNER)  task” which aims at detecting entities at both coarse and fine-grained levels. Authors propose a Multi-grained Entity Proposal Network (MGEPN) which comprises (1) a Proposal Network that determines entity boundaries, and (2) a Classification network that classifies each proposed segment of an entity.\n\nThe task is primarily tested against the proposed method itself. The proposed method does outperform traditional sequence-labeling baseline model (LSTM-LSTM-CRF), validating the proposed approach. When the proposed model (trained with extra MG data) is evaluated on the traditional NER task (on test sets), however, no significant improvement is observed -- I believe this result is understandable though, because e.g. MG datasets have slightly different label distributions from original datasets, hence likely to result in lower recall, etc.\n\n<Comments>\nThe task studied is interesting, and can potentially benefit other downstream applications that consume NER results -- although it seems as though similar tasks have been studied prior to this study. The novelty of the proposed architecture is moderate - while each component of the model does not have too much technical novelty, the idea of separating the model into a proposal network and a classifier seems to be a new approach in the context of NER (that diverges from the traditional sequence labelling approaches), and is reasonably designed for the proposed task.\n\nThe details for creating the MG datasets is missing - are they labeled by human labelers, or bootstrapped? Experts or crowd-sourced? By how many people? Will the new datasets be released? Please provide clarifications.\n\nThe proposed approach does not or barely outperform base models when tested on the traditional NER task -- the proposed work thus can be strengthened by better illustrating the motivation of the MGNER task and/or validating its efficacy in other downstream tasks, etc. \f\n\nAuthors could provide better insights into the new proposed task by providing more in-depth error analysis - especially the cases when MG NER fails as well (e.g. when coarse-grained prediction predicts a false positive named-entity, etc.)\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Nested NER dection",
            "review": "This paper describes multi-grained entity recognition. Experimental results show that the proposed Multi-Grained Proposal Network achieve better performance on NER tasks.\n\nMajor comments:\n\n- A major weakness of this paper is lack of citations to recent related studies. There are studies on nested NER published this June:\n\nA. Katiyar and C. Cardie, Nested Named Entity Recognition Revisited, NAACL/HLT 2018, June, 2018.\nM. Ju, et al., A neural layered model for nested named entity recognition, NAACL/HLT 2018, June 2018.\n\nYou need to compare these conventional methods to your proposed method.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}