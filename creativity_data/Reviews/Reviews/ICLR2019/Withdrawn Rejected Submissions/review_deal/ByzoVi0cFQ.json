{
    "Decision": "",
    "Reviews": [
        {
            "title": "interesting work; more justification can help",
            "review": "I see the contribution of the paper as two-fold. first is introducing a task of estimating conditional average treatment effect by auxiliary dataset in different environments. second is develop algorithms to accomplish this task both with and without base learner and demonstrate success.\n\nthe causal setting it operates on is classical under strong ignobility and overlap.\n\n\nthe results are very interesting; meta-learning regression weights consistently yield significantly better results without using any base learner (that are often theoretically guaranteed with asymptotic optimality). \n\ncan the authors provide any intuitions? what is special about meta-learning regression weights that lead to much gain? does it get around any drawbacks of the popular base learners?\n\ncausal identification: while the authors operates under strong ignobility and overlap; so the contribution is not really causal here but rather in terms of estimation. I wonder if the authors can leverage these auxiliary information to achieve weaker identification condition. for example conditions like in \"Causal inference using invariant prediction: identification and confidence intervals; Jonas Peters, Peter BÃ¼hlmann, Nicolai Meinshausen\"?",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea but somewhat lacking in novelty and in the experimental design",
            "review": "The authors propose methods to address a novel task of transfer learning for estimating the CATE function.  The methods proposed are several different neural network architectures and training strategies, which vary in how they leverage information from one task to another: learning joint representations and then different task-specific \"heads\", warm-starting from one task to another, or aggregate gradient updates across experiments. \nExperimental evaluation is always challenging for CATE tasks. In this paper the evaluation of the method is done using both a synthetic setting based on MNIST, and a real-world experimental dataset from a large Get Out The Vote experiment.\n\n\nThe task itself is of high interest, and should definitely be a subject of research. The paper is mostly clearly written and technically competent. My main concerns are as follows:\n\n1. The degree of novelty is not very high. While the task is new, I believe the technical innovation is not profound. The techniques are mostly straightforward re-use of features. The multi-head architecture has been introduced in the single experiment setting by Shalit et al. (2017). The meta-learner approach is directly adapted from Nichol et al. (2018). \n\nAlso, for this task, I would have liked to see more engagement with the many other transfer learning / domain adaptation methods out there:\nIn the very least, re-weighting based methods (which incidentally bear strong connections to propensity-score re-weighting commonly used in causal inference), as well as at least some well-known deep-learning transfer-learning methods such as Bengio \"Deep learning of representations for unsupervised and transfer learning.\" (2012), Ganin et al. \"Domain-adversarial training of neural networks.\"(2016), Long, Mingsheng, et al. \"Unsupervised domain adaptation with residual transfer networks.\" (2016), or similar work. \n\n2. The experimental evaluation can be improved in two ways:\n2.a. I would like to see more baselines. For example, in the GoTV task, how well does RF do when the \"ground truth\" is a linear model, and vice-versa? Also, Causal Forest (Wager & Athey 2017) and BART (Chipman et al. 2011) have emerged as two very strong and widely successful methods for CATE estimation. Both can be trivially adapted to the transfer learning task, e.g. by learning each task separately or by learning them jointly with an indicator feature for each task.  \n2.b. In the GoTV task, the original data is obtained from an RCT. That means that evaluation can be done using a much more powerful approach than simulating potential outcomes: one can instead evaluate the policy risk implied by the CATE estimator, as done in Shalit et al. (2017) using the LaLonde dataset. \n\n\nOverall I think this paper introduces an interesting problem and some valid approaches, but can be significantly improved both in the methods it presents and the experimental setup.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "No theoretical justification, no validation possible",
            "review": "The authors aim at estimating a conditional average treatment effect (CARE) under strong ignorability assumptions. For this they use neural network regression and compare several transfer learning frameworks on this problem, which basically differ in how to share and update weights in the different experimental regimes.\n\nThe paper is clearly written and well organized. Several experiments have been conducted.\nThe originality of this paper is low as the authors just replace any regression method with neural networks. The term \"transfer learning\" - as used here - hides the fact that it is not based on theory, but just on a bunch of heuristics how to change the functions from one regime to another.\n\nSince the counterfactual hypothesis is not really testable, as also the authors pointed out (i.e. every single instance can only occure in one of the regimes), a theoretical justification for the use of the transfer learning heuristics is needed, but missing.\nSo in this light, the conducted experiments on real world data are close to meaningless as no ground truth is known.\nThe authors compare to other regression methods instead.\n\nClaims such as \"Our methods can perform an order of magnitude better than existing benchmarks\" are claims for  supervised learning tasks or where the groundtruth is known.\nIn this setting it is just unscientific: there is no theoretical justification and it can not even be assessed what the supposedly better numbers actually mean.\n\nThe reviewer suggests taking a more humble and scientific writing style and instead highlighting all sources of uncertainty, assumptions and missing knowledge, etc., in this setting.\nIf one still wants to use regression techniques and transfer learning a paragraph with a fundamental discussion about the justification of such approaches is necessary.\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}