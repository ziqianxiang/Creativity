{
    "Decision": "",
    "Reviews": [
        {
            "title": "Strong results on some benchmarks but novelty and justification for method lacking",
            "review": "This paper addresses the problem of few-shot learning, specifically by incorporating a mechanism for generating additional training examples to supplement the given small number of true training examples in an episode. The example generation method takes as input a support set image and a gallery image (which can be any random image) and produces a new synthesized support example that is a linear weighted combination of the two input images (in terms of pixel patches) and is assumed to belong to the same class as the support set example input. The synthesis process is parameterized as an “image deformation network” whose weights must be learned. Additionally, there is an embedding network that takes as input the full support set (with true and synthesized examples) and is used in a prototypical-net fashion (Snell 2017) to classify the given query set. A set number of gallery images are created from the training base classes and this is the fixed set of gallery images that is used for both training and testing. The image deformation network and embedding network weights are trained end-to-end using the standard meta-learning loss on the query set given the support set. Experiments are conducted on the ImageNet-1K Challenge and Mini-Imagenet benchmarks.\n\nPros:\n- Sizable improvement on ImageNet-1K benchmark relative to previous work.\n- Detailed ablation study is done to display the benefits of choices made in model.\n\nCons:\n- Idea is not that novel relative to all the recent work on learning to supplement training data for few-shot learning (Hariharan 2017, Wang 2018, Gao 2018, and Schwartz 2018). Additionally, it is not made clear why we expect the idea proposed in this paper to be better relative to previous ideas. For example, Delta-Encoder (Schwartz 2018) also supplements training data by working in image-space rather than in feature-space (as done in the other previous work mentioned). One big difference seems to be that for the Delta-Encoder work, the image generation process is not trained jointly with the classifier but this does not seem to have a big impact when we compare the performance between Delta-Encoder and proposed model on Mini-Imagenet benchmark.\n- Benefit in Mini-Imagenet benchmark is not very large. Compared to Delta-Encoder (Schwartz 2018), the proposed model does worse for 1-shot case and intersects the confidence interval for 5-shot case.\n\nRemarks:\n- Could add citation for NIPS 2018 paper 'Low-shot Learning via Covariance-Preserving\nAdversarial Augmentation Networks' (https://arxiv.org/pdf/1810.11730.pdf)\n- Delta-Encoder (Schwartz 2018) paper should be mentioned in related work and contrasted against.\n- How many supplemented examples are used per class? I believe it is n_aug=8 but this could be stated more clearly.\nAdditionally, how many supplemented examples are used in the previous work that are being compared to? It would be useful to have this information and to make sure the comparison is consistent in that the number of supplemented examples are comparable across different models that synthesize examples.\n- In algorithm 1, it seems that only CELoss is used to update parameters of embedding network but on page 6, it says \"we found that using additional cross-entropy loss speeds up convergence and improves recognition performance than using prototypical loss solely\"?\n- At many points in the paper, 'prob image' is used instead of 'probe image'.\n- Page 10: 'Our IDME-Net has different react...' => 'Our IDME-Net has different output...'\n- Page 10: '...why our model work' => '...why our model works'\n\nGao et al. Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks. 2018.\nHariharan et al. Low-shot Visual Recognition by Shrinking and Hallucinating Features. 2017.\nSchwartz et al. Delta-Encoder: an effective sample synthesis method for few-shot object recognition. 2018.\nSnell et al. Prototypical Networks for Few-Shot Learning. 2017.\nWang et al. Low-Shot Learning from Imaginary Data. 2018.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice paper with exhaustive experiments",
            "review": "The paper is very well-written and is a nice read. \n\nThe paper presents 1) a new approach of one-shot learning by training meta learning based deformation networks, 2) end-to-end meta learning framework and 3) a new way to synthesize images which boosts the performance a lot. It's a very well written paper with a exhaustive experimentation on various axis: data augmentation strategies (gaussian noise, flipping), comparison with competitive baselines in one-shot learning literature like matching network (Vinyals et al. 2016), ablations on the design of deformation network showcasing what makes this network work well. The ablations offer explanation behind the design choices. The papers also explains the network design and their approach very succinctly.\n\nThe experimentation section is very well done and high-quality. I also appreciated that the experiment setup used in the paper is the standard setup that other SOTA baselines have used which makes it easy to compare the results and contributions of this paper and also keeps the evaluation strategy consistent in literature. Overall the approach demonstrates great results on 5-shot classification learning over previous approaches.\n\nI have following questions to the authors:\n1. what is the training time for end-to-end model compared to previous SOTA baselines?\n2. Can authors link to the design of ResNet-10 model for the sake of clarity even though it might feel obvious?\n3. in Section 6.1 Setup section, authors set epsilon value to 2 for selecting gallery images. Can authors show ablation on epsilon values and why 2 works the best?\n4. Can authors provide intuition into why the strategy proposed doesn't work as well on 1-shot classification? \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "An interesting approach for one-shot learning in images",
            "review": "This paper is about a novel framework to address one-shot learning task by augmenting the support set with automatically generated images. The framework is composed by two networks: a deformation network which is responsible to generate synthetic images and an embedding networks which projects images into an embedding space used to perform the final classification. Compared to previous work, the idea is similar in spirit to recent work that synthesise new images (Wang et. al. CVPR 2018) with GANs, but exploit a simpler idea based on the fusion of two images directly into the image domain with weights generated by a network. Extensive experiments on the standard ImageNet1K and miniImageNet datasets are reported, with a comprehensive comparison of the state of the art and several ablation studies. The proposed method achieves better performance than the state of the art.\n\nStrengths:\n+ The paper is almost well written, with comprehensive related works and it is easy to read except for few details (see below).\n+ The method is simple and is extensively studied. I appreciated the extensive ablation study and the discussion that follows it.\n\nWeaknesses:\n- The approach has some novelty in the method of generating new images and in the framework itself. Beside that, the idea of generating images was introduced previously (e.g. Wang et al. 2018) and the embedding network is a standard one-shot embedding with prototypical loss (Snell et al. 2017).\n- The clarity on few details can confuse the reader and needs improvement on the use of the gallery.\n- The comparison with the state of the art may be unfair due to the addition of the gallery images, which are proved in the ablation studies to be essential to the better performance of the proposed method. \n\nIn particular:\n- It is not clear to me how the gallery images are exactly used. Since the ablation study (sect 6.2) reported in (2) that the performance is worse using images from the support set and (3) the improved performance comes from the diversified images outside the support set, it may be that just the addition of the gallery images is the reason to have better performance. Also it is not clear to me if they are fixed all the time in advance during the meta training, if they change. It would be interesting to see an experiment where the gallery is available in the support set of the compared state of the art works and the baselines, and see if the performance is improved as well.\n\n- Regarding the clarity, beside the use of the gallery, several small issues should be improved:\n  * The paper use the term deformed images (until sect ~4) and then synthetic images for the generated images; \n  * Figure 2 is introduced early but explained late in the paper and it is not clear how the second and the third images should be interpreted; \n  * In the related work section, Wang et al. 2018 is said to generate imaginary images in contrast to realistic images of the proposed method not synthetic, however the images are clearly not real and are synthetically generated from the heuristic and the weight of the network. Moreover, the paper uses the term synthetic images later, so the comment is misleading.   \n\nMinor issue:\n- In the abstract, loose -> lose\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}