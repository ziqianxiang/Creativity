{
    "Decision": {
        "metareview": "This paper presents a model to identify entity mentions that are synonymous.  This could have utility in practical scenarios that handle entities.\n\nThe main criticism of the paper is regarding the baselines used.  Most of the baselines that are compared against are extremely simple.  There is a significant body of literature that models paraphrase and entailment and many of those baselines are missing (decomposable attention, DIIN, other cross-attention mechanisms).  Adding those experiments would make the experimental setup stronger.\n\nThere is a bit of a disagreement between reviewers, but I agree with the two reviewers who point out the weakness of the experimental setup, and fixing those issues could improve the paper significantly.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Meta Review"
    },
    "Reviews": [
        {
            "title": "Interesting paper though there is room for improvement",
            "review": "This paper studies the problem of identifying (discovering) synonymous entities. The paper proposes using the \"contexts\" of the entities as they occur in associated text corpora (e.g. Wiki) in the proposed neural-network based embedding approach for this task. The key novelties of the approach lie in the \"matching\" system used, where contexts of one entity are matched with that for the other entity to see how well they align with each other (which effectively determines the similarity of the two entities). Experiments are conducted on three different datasets to show the efficacy of the proposed approach.\n\nOverall I found the paper to be an interesting read with some nice ideas mixed in. However I also had some concerns which are highlighted later down below, which I believe if addressed would lead to a very strong work.\n\nQuality: Above average\n\nIn general the method seems to work somewhat better than the baselines and the method does have a couple of interesting ideas.\n\nClarity: Average\n\nI found a few key details to be missing and also felt the paper could have been better written.\n\nOriginality: Average\n\nThe matching approach and use of the leaky units was interesting tidbits. Outside of that the work is largely about the application of such Siamese RNNs based networks to this specific problem. (The use of context of entities has already been looked at in previous works albeit in a slightly more limited manner)\n\nSignificance: Slightly below average\n\nI am not entirely sold on the use of this approach for this problem given its complexity and unclear empirical gains vs more sophisticated baselines. The matching aspect may have some use in other problems but nothing immediately jumps out as an obvious application.\n\n----\n\nStrengths / Things I liked about the paper:\n\n- In general the method is fairly intuitive and simple to follow which I liked.\n- The matching approach was an interesting touch.\n- Similarly for the \"leaky\" unit.\n- Experiments conducted on multiple datasets.\n- The results indicate improvements over the baselines considered on all the three datasets.\n\nWeaknesses / Things that concerned me:\n\n-  (W1) Slightly unfair baselines? One of the first things that struck me in the experimental results was how competitive word2vec by itself was across all three datasets. This made me wonder what would happen if we were to use a more powerful embedding approach say FastText, Elmo, Cove or the recently proposed BERT? (The proposed method itself uses bidirectional LSTMs)\n\nFurthermore all of them are equally capable of capturing the contexts as well. An even more competitive (and fair) set of baselines could have taken the contexts as well and use their embeddings as well. Currently the word2vec baseline is only using the embedding of the entity (text), whereas the proposed approach is also provided the different contexts at inference time. The paper says using the semantic structure and the diverse contexts are weaknesses of approaches using the contexts, but I don't see any method that uses the context in an embedding manner -- say the Cove context vectors. If the claim is that they won't add any additional value above what is already captured by the entity it would be good to empirically demonstrate this.\n\n- (W2) Significance testing: On the topic of experimentation, I was concerned that significance testing / error estimates weren't provided for the main emprical results. The performance gaps seem to be quite small and to me it is unclear how significant these gaps are. Given how important significance testing is as an empirical practice this seems like a notable oversight which I would urge the authors to address.\n\n- (W3) Missing key details: There were some key aspects of the work that I thought were not detailed. Chief among these was the selection of the contexts for the entities. How was this? How were the 20 contexts identified? Some of these entities are likely far more common than just 20 sentences and hence I wonder how these were selected?\n\nAnother key aspect I did not see addressed: How were the entities identified in the text (to be able to find the contexts for them)? The paper claims that they would like to learn from minimal human annotations but I don't understand how these entity annotations in the text were obtained. This again seems like a notable oversight.\n\n- (W4) Concerns about the method: I had two major concerns about the method: \n\n(a) Complexity of method :  I don't see an analysis of the computational cost of the proposed method (which scales quadratically with P the number of contexts); \n\n(b) Effect of redundant \"informative\" contexts: Imagine you have a number of highly informative contexts for an entity but they are all very similar to each other. Due to the way the matching scores are aggregated, these scores are made to sum to 1 and hence no individual score would be very high. Given that this is the final coefficient for the associated context, this seems like a significant issue right?\n\nUnless the contexts are selected to be maximally diverse, it seems like this can essentially end up hurting an entity which occurs in similar contexts repeatedly. I would like to see have seen the rationale for this better explained.\n\n(c) A smaller concern was understanding the reasoning behind the different loss functions in the siamese loss function with a different loss for the positive and the negative, one using a margin and one which doesn't. One which scales to 1/4, the other scaling to (1-m)^2. This seems pretty arbitrary and I'd like to understand this.\n\n-(W5) Eval setting : My last concern was with the overall evaluation setup. Knowledge bases like Freebase are optimized for precision rather than recall, which is why \"discovery\" of new relations is important. However if you treat all missing relationships as negative examples then how exactly are you measuring the true ability of a method? Thus overall I'm pretty skeptical about all the given numbers simply because we know the KBs are incomplete, but are penalizing methods that may potentially discover relations not in the KB.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice approach for automatically discovering synonymous entities",
            "review": "The paper presents a neural network model (SYNONYMNET) for automatically discovering synonymous entities from a large free-text corpus with minimal human annotation. The solution is fairly natural in the form of a siamese network, a class of neural network architectures that contain two or more identical subnetworks, which are an obvious approach for such a task, even though this task's SotA does not cover such architectures. even though the abstract consists the word novel, the chosen architecture is not a novel one but attached to this task, it can be considered as if.\n\n# Paper discussion:\n\nThe introduction and the related work are well explained and the article is well structured. The authors mark very well the utility of automatically discovering synonyms.\n\nSection 2 presents the SynonymNet, mainly the bi-LSTM applied on the contexts and the bilateral matching with leaky unit and the context aggregation for each entity, along with training objectives and the inference phase.\n\nThe novelty does not consist in the model since the model derives basically from a siamese network, but more in the approach, mainly the bilateral matching: one input is a context for an entity, the other input is a context for the synonym entity, and the output is the consensus information from multiple pieces of contexts via a bilateral matching schema with leaky unit (highest matched score with its counterpart as the relative informativeness score) and the context aggregation. The inference phase is a natural step afterward. Also, the usage of the leaky unit is clearly stated.\n\nSection 3 presents the experimental phase, which is correct. The choice of LSTMs is understandable but other experiments could have been done in order to make clearer why it has been chosen. Regarding also the word embeddings choice, other experiments could have been completed (word2vec and GloVe have been competing with many other embeddings recently).\n\nOne noticed misspelling: GolVe (Pennington et al., 2014) ",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper presents a neural network model that detect synonymous entities based on contextual information without supervision.",
            "review": "Strengths:\n- clear explanation of the problem\n- clear explanation of the model and its application (pseudocode)\n- clear explanation of training and resulting hyperparameters\n\nWeaknesses:\n- weak experimental settings: \n-- (a) comparison against 'easy to beat' baselines. The comparison should also include as baselines the very relevant methods listed in the last paragraph of the related work section (Snow et a.l 2005, Sun and Grishman 2010, Liao et al. 2017, Cambria et al. 2018). \n-- (b) unclear dataset selection: it is not clear which datasets are collected by the authors and which are pre-existing datasets that have been used in other work too. It is not clear if the datasets that are indeed collected by the authors are publicly available. Furthermore, no justification is given as to why well-known publicly available datasets for this task are not used (such as CoNLL-YAGO (Hoffart et al. 2011), ACE 2004 (NIST, 2004; Ratinov et al. 2011), ACE 2005 (NIST, 2005; Bentivogli et al. 2010), and Wikipedia (Ratinov et al. 2011)).\n- the coverage of prior work ignores the relevant work of Gupta et al. 2017 EMNLP. This should also be included as a baseline.\n- Section 2 criticises Mikolov et al.'s skip-gram model on the grounds that it introduces noisy entities because it ignores context structure. Yet, the skip-gram model is used in the preprocessing step (Section 3.1). This is contradictory and should be discussed.\n- the definition of synonyms as entities that are interchangeable under certain contexts is well known and well understood and does not require a reference. If a reference is given, it should not be a generic Wikipedia URL.\n- the first and second bulletpoint of contributions should be merged into one. They refer to the same thing. \n- the paper is full of English mistakes. A proficient English speaker should correct them.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}