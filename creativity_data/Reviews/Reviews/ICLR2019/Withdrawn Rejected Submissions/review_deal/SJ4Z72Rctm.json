{
    "Decision": {
        "metareview": "Multiple reviewers had concerns about the clarity of the presentation and the significance of the results.\n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": " Multiple reviewers had concerns about the clarity of the presentation and the significance of the results."
    },
    "Reviews": [
        {
            "title": "Need clearer motivation for algorithm. Lots of little issues need fixing",
            "review": "The authors introduce Divergence Correction (DC) for the problem of transfer learning by composing policies. There approach builds on GPI with a maximum entropy objective. They also prove that DC solves for the max-entropy optimal interpolation between two policies and derive a practical approximation for this algorithm. They provide experimental results in a gridworld problem and study their approximate algorithm in two continuous control problems.\n\nWhile this paper has some interesting ideas (combining GPI with a Max-Entropy objective and DC), these ideas are not properly motivated. The main problem seems to be clarity. One big problem is that the paper never defines the notion of a notion of optimality (or near-optimality). Also, considering that the DC algorithm is one of the main contributions of the paper it is barely motivated. Theorem 3.2 is presented with almost no explanation about how DC was derived. Why do the authors believe that DC is a good idea on a conceptual level? It's very interesting that the paper presents cases where previous approaches (Optimistic and GPI) don't perform well. But the authors don't explain why they believe DC should perform well in these cases. \n\nThe authors make the unjustified claim in the abstract that their approach has \"near-optimal performance and requires less information\". I say this is unjustified because they only try this approach on three benchmarks. In addition, there should be situations where DC also performs poorly since there are known hardness results for solving MDPs. Admittedly, those results may not apply if the authors are making assumptions that are not being clearly discussed in the paper.\n\nMinor Comments:\n1. In the abstract, \"requiring less information\" is very imprecise. Are you referring to sample complexity?\n2. In the introduction, \"can consistently achieve good performance\" is imprecise. What is the notion of near-optimality? What does consistent mean? Having experimental results on 3 tasks doesn't seem to be enough to me to justify this claim.\n3. In the introduction (and rest of the paper), please don't call Haarnoja et al.'s approach optimistic. Optimism already has another widely used meaning in RL literature. Maybe call it \"Uncorrected\".\n4. In section 2.2, the authors introduce \\pi_1, \\pi_2, ... , \\pi_n but never actually use that notation. This section does not clearly explain how GPI works.\n5. In Theorem 3.1, the authors should introduce Q^1, Q^2, ... , Q^n and define the policies in terms of the action-value functions. Also, the statement of this theorem is not self contained, what is the reward function of the MDP? The proof below should be called a proof sketch.\n6. The paper mentions that extending to multiple tasks is possible. Is it trivial? What is the basic idea? It seems straightforward but it might be helpful to explicitly state the idea.\n7. In Theorem 3.2, how was C derived? Please add some commentary explaining the conceptual idea.\n8. In Table 1, what is f(s, a|b)? I don't see where this was defined?\n9. CondQ is usually referred to as UVFA in the literature.\n10. Section 3 really needs a conclusion statement.\n11. Section 4 is very unclear and hard to follow.\n12. In figure 1f, what is LTD? It's never defined. I'm guessing it's DC.\n13. All of the figures are too small and some are not clear in black and white.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good Paper",
            "review": "This paper proposes using Divergence Correction to compose max ent policies. Based on successor features, this method corrects the optimistic bias of Haarnoja 2018. The motivation for composing policies is sound. This paper addresses the problem statement where policies must accomplish different linear combinations of different reward functions. This method does not require observation the reward weights.\n\nAs shown in the experiments, this method outperforms or equally performs past work in both tabular and continuous  environments. The paper is well written and discusses prior work in an informative manner. The tabular examples provide good visualizations of why the methods perform differently.\n\nMinor:\n- Figure 1.e: Why does the Optimistic transfer have high regret when the caption says that \"on the LU task, optimistic transfers well\"\n- Figure 1.i states \"Neither GPI nor the optimistic policies (j shows GPI, by the Optimistic policy is similar)\" but Figure1.j is labeled DC T, is this a typo?\n- Figure 2: Many typos:  \"(b) Finger position at the en (of the trajectoriesstard ting from randomly sampled start states)\"\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting work, but need further improvement",
            "review": "\n-- Contribution, Originality, and Quality --\n\nThis paper has presented two approaches for transfer learning in the reinforcement learning (RL) setting: max-ent GPI (Section 3.1) and DC (Section 3.2). The authors have also established some theoretical results for these two approaches (Theorem 3.1 and 3.2), and also demonstrated some experiment results (Section 5).\n\nThese two developed approaches are interesting. However, based on existing literature (Barreto et al. 2017; 2018, Haarnoja et al. 2018a), neither of them seems to contain *significant* novelty. The derivations of the theoretical results (Theorem 3.1 and 3.2) are also relatively straightforward. The experiment results in Section 5 are interesting.\n\n-- Clarity --\n\nI have two major complaints about the clarity of this paper. \n\n1) Section 4 of the paper is not well written and is hard to follow.\n\n2) Some notations in the paper are not well defined. For instance\n\n2a) In page 3, the notation \\delta has not been defined.\n2b) In page 6, both notation V_{\\theta'_V} and V'_{\\theta_V} have been used. I do not think either of them has been defined. \n\n-- Pros and Cons --\n\nPros:\n\n1) The proposed approaches and the experiment results are interesting.\n\nCons:\n\n1) Neither the algorithm design nor the analysis has sufficient novelty, compared to the typical standard of a top-tier conference.\n\n2) The paper is not very well written, especially Section 4.\n\n3) For Theorem 3.2, why not prove a variant of it for the general multi-task case?\n\n4) It would be better to provide the pseudocode of the proposed algorithm in the main body of the paper.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}