{
    "Decision": {
        "metareview": "This paper proposes an approach for imitation learning from video data. The problem is important and the contribution is timely. The reviewers brought up several concerns regarding the clarity of the paper and the lack of sufficient comparisons. The authors have improved the paper significantly, adding several new comparisons and improving the presentation. However, concerns still remain regarding the description of the method and the presentation of the results. Hence, the reviewers agree that the paper does not meet the bar for publication.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "meta review"
    },
    "Reviews": [
        {
            "title": "Issues with significance of results",
            "review": "This paper proposes an imitation learning method solely from video demonstrations by learning recurrent image-based distance model and in conjunction using RL to track that distance.\n\nClarity: The paper writing is mostly clear. The motivation for using videos as a demonstration source could be more clearly stated. One reason is because it would pave the way to learn from real-world video demonstrations. Another reason is that robot's state space is an ill-suited space to be comparing distances over and image space is more suitable. Choosing one would help the readers identify the paper's motivation and contribution.\n\nOriginality: The individual parts of this work (siamese networks, inverse RL, learning distance functions for IRL, tracking from video) have all been previously studied (which would be good to discuss in a relate work section), so nothing stands out as original, however the combination of existing ideas is well-chosen and sensible.\n\nSignificance: There are a number of factors that limit the significance of this work. \n\nFirst, the demonstration videos come from synthetic rendered systems very similar the characters that imitate them, making it hard to evaluate whether this approach can be applied to imitation of real-world videos (and if this is not the goal, please state this explicitly in the paper). Some evaluation of robustness due to variation in the demonstration videos (character width, color, etc) could have been helpful to assure the reader this approach could scale to real-world videos.\n\nSecond, only two demonstrations were showcased - 2D walking and 3D walking. It's hard to judge how this method (especially using RNNs to handle phase mismatch) would work for other motions.\n\nThird, the evaluation to baselines is not adequate. Authors mention that GAIL does not work well, but hypothesize it may be due to not having a recurrent architecture. This really needs to be evaluated. A possibility is to set up a 2x2 matrix of tests between [state space, image space] condition and [recurrent, not recurrent] model. Would state space + not recurrent reduce to GAIL?\n\nFourth and most major to me is that looking at the videos the method doesn't actually work very well qualitatively, unless I'm misunderstanding the supplementary video. The tracking of 2D human does not match the style of the demonstration motion, and matches even less in 3D case. Even if other issues were to be addressed, this would still be a serious issue to me and I would encourage authors to investigate the reasons for this when attempting to improve their work.\n\nOverall, I do not think the results as presented in the submission are up to the standards for an ICLR publication.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting solution to a challenging problem, but lacking in quantitative results",
            "review": "Summary: This paper aims to imitate, via Imitation Learning, the actions of a humanoid agent given only video demonstrations of the desired task, including walking, running, back-flipping, and front-flipping. Since the algorithm does not have direct access to the underlying actions or rewards, the agent aims to learn an embedding space over instances with the hope that distance in this embedding space corresponds to reward. Deep RL is used to optimize a policy to maximize cumulative reward in an effort to reproduce the behavior of the expert.\n\nHigh-level comments:\n- My biggest concern with this paper is the lack of a baseline example that we can use to evaluate performance. The walking task is interesting, but the lack of a means by which we can evaluate a comparison between different approaches makes it very difficult to optimize. This makes evaluation of quality and significance rather difficult. A number of other questions I have stem from this concern:\n    = The paper is missing a comparison between the recurrent Siamese network and the non-recurrent Siamese network. The difficulty in comparing these approaches without a quantitative performance metric.\n    = The authors also mention that they tried using GAIL to solve this problem, but do not show these results. Again, a success metric would be very helpful here.\n    = Finally, a simpler task for which the reward is more easily specified may be a better test case for the quantitative results. Right now, the provided example of walking agents seems to only provide quantitative results.\n- The authors need to be more clear about the structure of the training data and the procedure. As written, the structure of the triplet loss is particular ambiguous: the condition for positive/negative examples is not clearly specified.\n- There are a number of decisions made in the paper that feel rather arbitrary or lack justification. In particular, the \"normalization\" scaling factor fits into this category. Some intuition or explanation for why this is necessary (or why this functional form should be preferred) would be helpful.\n- A description of what the error bars represent in all of the plots is necessary.\n\nMore minor comments and questions:\n- The choice of RL algorithm is not the purpose of this paper. Much of this section, and perhaps many of the training curves, are probably better suited to appear in the Appendix. Relatedly, why are training curves only shown for the 2D environment? If space was a concern, the appendix should probably contain these results.\n- An additional agent that may be a useful comparison is one that is directly provided the actions. It might then be more clear how well. (Again, this would require a way to compare performance between different approaches.)\n- How many demonstrations are there? At training vs testing?\n- Where are the other demonstrations? The TSNE embedding plot mentions other tasks which do not appear in the rest of the paper. Did these demonstrations not work very well?\n\nA Comment on Quality: Right now, the paper needs a fair bit of cleaning up. For instance, the word \"Rienforcement\" is misspelled in the abstract. There is also at least one hanging reference. Finally, a number of references need to be added. For example, when the authors introduce GAIL, they mention GANs and cite Goodfellow et al. 2014, but do not cite GAIL. There is also a lot of good research on Behavioral Cloning, and where it can go wrong, that the authors mention, but do not cite.\n\nConclusion: At this point it is difficult to recommend this paper for acceptance, because it is very hard to evaluate performance of the technique. With a more concrete way of evaluating performance on a different task with a clearer reward function for comparison, the paper could be much stronger, because this would allow the authors to compare the techniques they propose to one another and to other algorithms (like GAIL).\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea, writing needs significant work and comparisons",
            "review": "\nBrief summary: This work proposes a way to perform imitation learning from raw videos of behaviors, without the need for any special time-alignment or actions present. They are able to do this by using a recurrent siamese network architecture to learn a distance function, which can be used to provide rewards for learning behaviors, without the need for any explicit pose estimation. They demonstrate effectiveness on 2 different locomotion domains. \n\nOverall impression:\nOverall, my impression from this paper is that the idea is to use a recurrent siamese network to learn distances which make sense in latent space and provide rewards for RL. This is able to learn interesting behaviors for 2 tasks. But I think the writing needs significant work for clarity and completeness, and there needs to be many more baseline comparisons. \n\nAbstract comments:\ntrail and error -> trial and error\n\nIntroduction comments:\n\nAlternative reasons why pose estimation won’t work is because for any manipulation tasks, you can’t just detect pose of the agent, you also have to detect pose of the objects which may be novel/different\n\nFew use image based inputs and none consider the importance of learning a distance function in time as well as space -> missed a few citations (eg imitation from observation (Liu, Gupta, et al))\n\nTherefore we learned an RNN-based distance function that can give reward for out of sync but similar behaviour -> could be good to emphasize difference from imitation from observation (Liu, Gupta, et al) and TCN (Semanet et al), since they both assume some sort of time alignment\n\nMissing related work section. There is a lot of related work at this point and it is crucial to add this in. Some things that come to mind beyond those already covered are:\n1. Model-based Imitation Learning from State Trajectories\n2. Reward Estimation via State Prediction\n3. infoGAIL\n4. Imitation from observation \n5. SFV: Reinforcement Learning of Physical Skills from Videos\n6. Universal planning networks\n7. https://arxiv.org/abs/1808.00928\n8. This might also be related to VICE (Fu, Singh et al), in that they also hope to learn distances but for goal images only.\nIt seems like there is some discussion of this in Section 3.1, but it should be it’s own separate section.\n\nSection 3 comments:\na new model can be learned to match this trajectory using some distance metric between the expert trajectories and trajectories produced by the policy π -> what does this mean. Can this be clarified?\n The first part of Section 3 belongs in preliminaries. It is not a part of the approach. \n\nSection 3.2\nEquations 9 and 10 are a bit unnecessary, take away from the main point\n\nWhat does distance from desired behaviour mean? This is not common terminology and should be clarified explicitly.\n\nEquation 11 is very confusing. The loss function is double defined.  what exactly Is the margin \\rho (is it learned?) The exact rationale behind this objective, the relationship to standard siamese networks/triplet losses like TCN should be discussed carefully. This is potentially the most important part of the paper, it should be discussed in detail.Also is there a typo, should it be || f(si) - f(sn)|| if we want it to be distances? Also the role of trajectories is completely not discussed in equation 11.\n\nSection 3.3 \nThe recurrent siamese architecture makes sense, but what the positive and negative examples are, what exactly the loss function is, needs to be defined clearly. Also if there are multiple demonstrations of a task, which distance do we use then?\n\nThe RL simulation environment is it made in-house, based on bullet or something else?\n\nData augmentation - how necessary is this for method success? Can an ablation be done to show the necessity of this?\n\nAlgorithm 1 has some typos \n- > is missing in line 3\n- Describe where reward r is coming from in line 10\n\nSection 4.1\nWalking gate -> walking gait\n\nThere are no comparisons with any of the prior methods for performing this kind of thing. For example, using the pose estimation baseline etc. Using the non-recurrent version. Using TCN type of things. It’s not hard to run these and might help a lot, because right now there are no baseline comparisons\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}