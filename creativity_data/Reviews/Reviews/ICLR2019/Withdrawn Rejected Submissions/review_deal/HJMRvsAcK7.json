{
    "Decision": {
        "metareview": "\nThis is an interesting topic but the reviewers had substantial concerns on the clarity and significance of the contribution.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": " This is an interesting topic but the reviewers had substantial concerns on the clarity and significance of the contribution."
    },
    "Reviews": [
        {
            "title": "Interesting and relevant, but still needs work",
            "review": "This paper presents a reinforcement learning approach to dynamically set the price of items on sale on an e-commerce website, based on a state description consisting of several kinds of features: price, sales, customer traffic and competitiveness. The actions (possible prices) are constrained to lie in item-specific lower and upper bounds. Against a proposed method relying a continuous action space (an implementation of the Deep Deterministic Policy Gradient of Lillicrap et al., 2015), alternatives relying on multi-armed bandits and Deep Q Networks are evaluated. Experimental results from an online deployment are presented.\n\nThe paper is at times hard to understand. In particular, it would benefit from a thorough review of English grammar and style. For instance, the alternating descriptions between the past and present tenses (e.g. the abstract and the introduction) are quite non-natural and somewhat irritating [see specific instances in the detailed comments, below]. \n\nFrom an algorithmic standpoint, the paper does not introduce new methods and relies on well-known reinforcement learning techniques. The proposed methodology of applying specific RL techniques such as DDPG to pricing appears novel. However, there is an abundant literature on optimal pricing and discounting in operations research, much of it based on dynamic programming techniques, and more links to this literature could be provided.\n\nOne could question the choice of the reward function chosen (eq. 1): since it is a ratio, it is extremely sensitive to variations in the denominator, and this could severely impact convergence. \n\nThe primary weakness of the paper lies in the experimental evaluation: at least the following informations are missing to truly understand the methodological impact and economic benefits of the proposed approach:\n\n1. A complete description of the experimental setting, i.e. how many SKUs (stock keeping units) were evaluated, over which product categories, over what time horizon. Of those, how are they distributed in the fast-mover / slow-mover plane (Syntetos et al., 2005)? What special events were material during the time period? How many of those were predicted and incorporated into the state representation?\n2. One or more tables of results giving expected utility gains over baseline of the proposed methods, along with confidence intervals.\n3. For an ICLR submission, there should be additional investigations as to the structure of the learned representations, at least by the DQN and the DDPG models — is the state embedding learned by the networks somewhat suggestive of economically meaningful properties of the items or the sales environment?\n\nAs such, even though the paper is interesting, it is in too early a state to recommend acceptance at ICLR.\n\nDetailed comments:\n\n* p. 1: many past tenses that should be in the present tense, e.g. (just in the abstract): modeled ==> models, defined ==> defines, then it introduced ==> it then introduces, were designed ==> are designed. Many other cases in the rest of the paper.\n* p. 1: has draw ==> has drawn\n* p. 2: Forth ==> Fourth\n* p. 2: overtime ==> over time\n* p. 2: is assumed, to ==> is assumed to\n* p. 2: described ==> describe\n* p. 3: “looks deep inside learning while earning approaches” ==> sentence not clear\n* p. 3: this is not clear: “since the number of page visitors may ﬂuctuate dramatically and this could lead to non-concavity“ ==> why would it lead to non-concavity?\n* p. 4: The whole paragraph before eq. (1) is not clear\n* p. 5: The D in eq. (5) should be explained immediately, not after eq. (6).\n* p. 5: In eq. (5), $\\theta’$ is not explained: how does it differ from $\\theta$ ?\n* p. 5: In eq. (6), how is $\\theta^{Q’}$ different from $\\theta^{Q}$ ?\n* p. 6: in eq. (8), the denominator r_t could be a small number, leading to a noisy error; this should be discussed.\n* p. 6: Below eq. (12), these sentences are not clear: “For dynamic pricing problem, we particularly concern the outcome from changing between prices. To have a well knowledge between two prices before and after pricing.”",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for Dynamic Pricing on E-commerce Platform with Deep Reinforcement Learning",
            "review": "The authors proposed a deep reinforcement learning for dynamic pricing problem. The major contribution is on the problem formulation and application end. From the algorithm point of view, the authors adopt the existing deep reinforcement learning algorithms like DQN and policy gradient. The experiments are conducted both online and offline using dataset from Tmall.\n\n[Advantage Summary]\n1. A very interesting application to apply deep RL on dynamic pricing.\n\n2. Experimented on industry dataset based real users. Both online and offline experiments are conducted.\n\n[Weakness Summary]\n1. This is not the first work to apply deep reinforcement learning for dynamic pricing problem as claimed by the authors.\n\n2. Limited technical contribution.\n\n3. Illustration and analysis of the experiment can be further approved.\n\n[Details in weakness and questions]\n\n1. This is not the first work to apply deep reinforcement learning for dynamic pricing problem as claimed author.\nFor instance, \n\"Reinforcement Learning for Fair Dynamic Pricing\"\nSince applying deep RL to dynamic pricing is one of the significant contributions of this paper, this limits the overall contribution.\n\n2.  The technical contribution is very limited by just applying existing algorithms.\n\n3. How to determine the step t seems to be a very important issue that can affect the performance of the algorithm and not well explained. From the experiment, the authors seem to set the period as one-day and update the price daily. But in reality, the time to update the price in a dynamic pricing system should not be a fixed value. For instance, the system should adjust the price in real time if there are changes in the environment(e.g., demand-supply change)\n\n4. Experiment part needs more analysis. For instance, day 16 seems to be an outlier, and conversion rate drops dramatically in the following days. Why? How is the conversion rate from day 16 gets calculated in the final evaluation?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Dynamic Pricing with RL",
            "review": "In this paper, the authors study the problem of Dynamic Pricing. This is a well-studied problem in Economics, Operations Research, and Computer Science. The basic problem is to find the right price for a product based on repeated interaction with the market. The significant challenge in this problem is to figure the right set of prices without knowing the future demand. The authors in this paper propose a deep reinforcement learning based approach to tackle this problem. Algorithms to this problem in the past have made assumptions such as concavity of the demand curve to make it tractable mathematically. In this paper, they do not make assumptions (although they do not provide theoretical guarantees) and one of the contributions of this paper is to handle the full generality of the problem. The other contribution is to test their methods in both offline and real-time setting and compare it against a natural MAB style algorithm (LinUCB to be precise). I have several comments on this paper.\n\nFirst, the paper is not well-written. To begin with, the authors do not formally define the Dynamic Pricing problem. In literature, there are many versions of this problem. Is there limited supply or is the supply unlimited? Should we find the price for a single item or multiple items separately? It is unclear. There are pieces of information across the paper which indicate that the seller is selling multiple products for which the algorithm is pricing separately. I am still unsure about whether the supply is finite or not. Along these lines, the authors should spend some time reading through the draft carefully to fix many grammatical mistakes throughout the paper. \n\nSecond, the authors have missed out some critical related work, both in the related work section *and* in their experimental comparison. The paper Dynamic Pricing with limited supply [Babiaoff et al EC 12] and Bandits with Knapsacks (BwK) [Badanidiyuru et al FOCS 2013] provide MAB style algorithms for this problem. These algorithms overcome the reasoning they provide in their experiments - \n\n\"We noticed that, LinUCB outperformed DRL methods in offline policy evaluation. One of the main reasons is that, MAB methods aim to maximize immediate reward while RL methods maximize long-term reward\". \n\nThe BwK algorithm precisely optimizes for long-term rewards under limited supply constraints. I think it will be useful to add experiments that compare the current approaches of the paper with this MAB algorithm. Overall I find the baselines in this paper to be weak and suggest more experiments and comparison against tougher relevant benchmarks. In fact, they have failed to compare, in any meaningful way, to a long line of work on Dynamic Pricing (the two Besbes and Zeevi papers they have cited for instance). I wonder how the experiments compare against those algorithms. Or they should clearly state why such a comparison is not relevant.\n\nOverall I find the goal to be interesting and novel. But I think the current state of the draft and the experiments make it weak. In particular, the paper falls below the bar on clarity and quality in this current state.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}