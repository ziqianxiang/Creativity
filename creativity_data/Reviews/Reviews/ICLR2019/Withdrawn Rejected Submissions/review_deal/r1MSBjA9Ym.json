{
    "Decision": {
        "metareview": "The paper studies difficulties in training deep and narrow networks. It shows that there is high probability that deep and narrow ReLU networks will converge to an erroneous state, depending on the type of training that is employed. The results add to our current understanding of the limitations of these architectures. \n\nThe main criticism is that the analysis might be very limited, being restricted to very narrow networks (of width about 10 or less) which are not very common in practice, and that the observed collapse phenomenon can be easily addressed by non symmetric initialization. \n\nThere were some issues with the proofs that were covered in the discussed between authors and reviewers. The revision is relatively extensive. \n\nThis is a borderline case. The paper receives one good rating, one negative rating, and a borderline accept rating. Although the paper contributes interesting insights to a relevant problem that clearly needs contributions in this direction, the analysis presented in the paper and its applicability in practice seems to be very restrictive at this point.  ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Contributes to understanding of limitations in narrow nets, but restrictive analysis "
    },
    "Reviews": [
        {
            "title": "resutls are simple, interesting but seem not very helpful in practice",
            "review": "This paper shows that the training of deep ReLU neural networks will converge to a constant classifier with high probability over random initialization (symmetric weight distributions) if the widths of all hidden layers are too small.\n\nOverall, the paper is clearly written. I like the main message of the paper and the simplicity of its analysis. To some extent, I think that the results could add to our current understanding of the limitations of deep narrow networks, both theoretically and practically. \n\nOn the other hand, my main concern at the moment is that the results seem to be informative only for low dimensional data and networks of small width. In particular, the bound on depth in eq (5) scales too fast with width. Figure 6 shows that with width 16 the bound on depth is already too loose that it could be of any use in practice.\n\nOther comments and questions:\nIn Figure 6+7, it's not clear how many times each experiment is repeated in order to get the numerical estimations of probabilities, and which exactly weight distributions are used here?\n\nThe statement of Theorem 1 and its proof looks a bit suspicious to me. This theorem first makes an assumption on a given network with fixed weights, but then makes some statement about random weights...This apparently does not make much sense to me because a given network has nothing to do with random weights, but the current proof is actually using the assumption made on the given network as a constant classifier to prove the probabilistic statement. I hope to see some clarification here.\n\nIt would be interesting to discuss the results of this paper with recent work [1,2] which also studied deep narrow networks but from other perspectives:\n[1] Neural networks should be wide enough to learn connected decision regions. ICML 2018\n[2] The Expressive Power of Neural Networks: A View from the Width. NIPS 2017",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Some interesting results, but not good enough. ",
            "review": "In this paper, the authors investigate the collapse of deep and shallow network to a constant function under the following setting:\n1. Very shallow networks, width ~ 10\n2. ReLU activation function.  \n3. Symmetric weights and biases initialization.  \n4. Vanilla feed forward networks.  \n\nThe main take-home message is: don't use neural networks (NNs) that are both deep and shallow.  \n\nThe theoretical analysis is built on the observation: \n1. Every neuron (after applying ReLU) is equal to zero with probability 1/2. \n2. For narrow network and for any fixed input, there is a high chance that all neurons in a particular hidden layer are all zero. All neurons after that layer are all zero if zero-bias initialization is used.  \n3. The authors conclude that derivatives of all parameters vanish but the bias of the last layer.\n4. As a result, the network collapse to its mean (median) if mean squared loss (L1 loss) is used because only the bias of the last layer is being updated during training. \n\nPros.\n1. I think the phenomenon that shallow and deep NNs collapse to a constant is very interesting. \n2. The authors provide empirical and theoretical insights in favor of wider networks: have a better chance to avoid vanishing gradients.  \n3. For shallow networks, it might be better not to use ReLU.  \n\nCons:\n1.The analysis works in a very limited setting, works for ReLU but not other activations: tanh, erf, SELU etc. \n2. Very shallow networks are not popular in practice. Width>=100 is popular for fully-connected layers. \n3. The phenomenon observed by the paper can easily be addressed using any of the following trick: \n   3.1. Non-symmetric initialization  (set the mean to be non-zero). \n   3.2 . wider networks. \n4. Most of the analysis (and theorems)  are about one single input. In another word, distribution of the inputs have not been taken into account.  \n5. I don't think the author provides a completely rigorous justification for the collapse phenomenon.  \n\nOther comments. \n1. Eq (2) in page 4 is not trivially correct. The expectation operator (w.r.p. to lower layers) is moved into the activation function phi, justification is needed for this step.  \n2. Theorem 4: when the Lebesgue measure of $\\Omega$ is NOT finite, it is unclear how to define a uniform probability distribution on it.  \n3. Theorem 4: the integrability assumption on y should depend on the loss: for L2 loss (L1 loss), squared (absolutely) integrable  should be used. They are not the same.  ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good, interesting results - presentation improved",
            "review": "The paper studies failure modes of deep and narrow networks. I find this research extremely valuable and interesting. In addition to that, the paper focuses on as small as possible models, for which the undesired behavior occurs. That is another great positive, too much of a research in DL focuses on the most complex and general cases in my opinion. I would be more than happy to give this paper a very strong recommendation, if not for numerous flaws in presentation. If those get improved, I am very eager to increase my rating. Here are the things that I think need an improvement:\n1. The formulation of theorems.\nThe paper strives for mathematical style. Yet the formulations of the theorems are very colloquial. Expression \"by assuming random weights\" is not what one wants to see in a rigorous math paper. The formulations of the theorems need to be made rigorous and easy to understand, the assumptions need to be clearly stated and all concepts used strictly defined.\n2. Too many theorems\n9 (!) theorems is way too much. Theorem is a significant contribution. I strongly suggest having 1-2 strong theorems, and downgrading more technical lemmas to a lemma and proposition status.\nIn addition - the problem studied is really a study of bad local minimas for neural networks. More mentions of the previous work related to the topic would improve the scientific quality additionally, in my opinion.\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}