{
    "Decision": {
        "metareview": "This paper proposes to improve MT with a specialized encoder component that models roles. It shows some improvements in low-resource scenarios.\n\nOverall, reviewers felt there were two issues with the paper: clarity of description of the contribution, and also the fact that the method itself was not seeing large empirical gains. On top of this, the method adds some additional complexity on top of the original model.\n\nGiven that no reviewer was strongly in favor of the paper, I am not going to recommend acceptance at this time.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Some clarity issues, and improvements underwhelming"
    },
    "Reviews": [
        {
            "title": "The motivation and goal of this paper are unclear",
            "review": "\n\n[Summary]\nThis paper proposes a “role interaction layer” (RIL) to capture the context-dependent (latent) role for each token.\n\n\n[clarity]\nThe writing is basically clear.\nHowever, It is hard for me to get the motivation and goal of this paper.\nIs the main purpose of the proposed method “improving the performance” or “interpretability”?\n\n\n[originality]\nIt seems that the proposed method consists of several known methods.\nMoreover, even though the purpose differs, technically the proposed method is closely related to the method proposed in [Shu+,2018].\nTherefore, the technical novelty of the proposed method is limited.\n\n[Shu+,2018] Raphael Shu, Hideki Nakayama, “Compressing Word Embeddings via Deep Compositional Code Learning”, ICLR-2018.\n\n\n[significance]\nThe contribution of this paper is not very clear.\nThe improvements from the baseline method (Matched) is less than 1 point BLEU as shown in Table 1 and 2, which is not a significant improvement.\n\n\n\nOverall, this paper is basically well written. However, this paper seems a technical report rather than a conference paper.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea, but the improvement over the baseline is not significant.",
            "review": "\n[Summary]\nThis paper proposes “a role interaction layer” (briefly, RIL) that consists of context-dependent (latent) role assignments and role-specific transformations: Given an RIL layer, different dimensions of an embedding vector are “interacted” based on Eqn. (5), Eqn. (7), etc. The authors work on IWSLT De->En and WMT En->De, En->Fi to verify their proposed algorithm with case study included. \n\n[Pros]\n(+) I think the idea/thought of using a “role interaction layer” is interesting.  The case study in Section 5.3 demonstrates different “roles”. Also, different RIL architectures are designed.\n(+) The paper is easy to follow.\n\n[Cons & Details]\n(1) As stated in the abstract, “…, but that the improvement diminishes as the size of data grows, indicating that powerful neural MT systems are capable of implicitly modeling role-word interaction by themselves…” (1) The main concern is that, considering RIL does not obtain significant gain on large datasets, then we cannot say that the proposed algorithm is better than the baseline. (2) Why the NMT systems trained on large dataset can “implicitly modeling role-word interaction”, while small dataset cannot? Any analysis?\n\n(2) For the “matched baseline”, page 5, you increase the dimensionality of the models. But an RIL is an additional layer, which makes the network deeper. Therefore, a baseline with an additional layer should be implemented. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The improvement seems not enough",
            "review": "The paper proposes contextual role representation which is an interesting point. \nThe writing is clear and the idea is original.\nEven with the interesting point, however, the performance improvement seems not enough compared to the baseline. The baseline might be carefully tuned as the authors said, but the proposed representation is supposed to improve the performance on top of the baseline.\nThe interpretation of the role representation is pros of the proposed model. However, it is somehow arguable, since it is subjective. \n\n- minor issues: \nThere are typos in the notations right before Eq. (8). \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}