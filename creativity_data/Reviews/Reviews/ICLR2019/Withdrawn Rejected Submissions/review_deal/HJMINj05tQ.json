{
    "Decision": "",
    "Reviews": [
        {
            "title": "Misleading claims and unjustified assumptions",
            "review": "The authors proposed a family of ODEs and claim the ODEs’ straightforward discretization generates Nesterov’s AGD method. They also claim to prove convergence with discrete Lyapunov equations. The continuous ODE design seems interesting but it is not clear to me how it could be useful/insightful.\n\nOne broken link in the paper's argument is that the discretized algorithms provided in the paper are not straight-forward discretization of the continuous ODEs. For example, at the top of page 4, the (FE-SC) algorithm is not a discretization of (1st-ODE-SC) because the usage of variable y as an interpolation between v and x. However, no discretization scheme would make such a choice. Hence, such discretization is not straightforward without back engineering from Nesterov’s method and do coefficient matching. Further, it’s not clear whether the continuous limit of the discretized algorithm corresponds to the original ODE. Same argument applies to all cases.\n\nAnother problem is that the Lyapunov type of proof is not generalizable. In other word, it’s not easier to come up with them compared to known proof techniques (e.g. estimated sequence, Ashia’s Lyapunov paper, Gupta’s potential function approach). Consequently, the significance of proving a known theory in this particular way is not clear to me.\n\nLast, the stochastic algorithms proved in this paper assumes that the gradient noise go to zero. It might make sense to prove that gradient noise go to zero (by noise proporitional to sub optimality/distance to optimal etc) and use that as a lemma in proving main theorem. However, simply assuming noise go to zero basically means any convergent deterministic algorithm will converge.\n\nIn summary, this paper has some interesting point, but I failed to understand the significance/impact provided by these claims.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting approach but an important question about why acceleration occurs is missing",
            "review": "The paper introduces an ordinary differential equation (ODE) with a term involving the Hessian and shows that Nesterov's method can be derived from a direct discretization of the proposed ODE. The perspective of connecting Nesterov's method and ODE is not new, but to the best of my knowledge, this is the first work where Nesterov's method can be derived directly from the given ODE. \n\nThe proposed approach is interesting but I find the contribution a bit weak. A large part of the paper is devoted to deriving Nesterov's method from the proposed ODE, which is basically applying Euler method. However, the intuition behind the ODE is missing. In particular, it will be interesting to explain why acceleration is attained from discretizing the given ODE. \n\nI agree that the acceleration is followed by the convergence analysis, but the convergence analysis in the paper is not new, which follows the standard proof of Nesterov's method using Lyapunov functions. The inexact version is not new neither, which can be viewed as a direct application of inexact accelerated gradient method in [1]. As a result, it will be important to discuss why the proposed framework intuitively provides acceleration, which is missing in the current version. \n\nOverall, I find the approach interesting, but a large portion of the paper is spent to reformulate existing result in accelerated gradient descent literature instead of explaining why the proposed ODE helps understanding acceleration. \n\n[1] M. Schmidt,  N. Roux, and F. Bach, Convergence rates of inexact proximal-gradient methods for convex optimization\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A nice exposition, but without novel results",
            "review": "In convex optimisation, Su et al. (2014) made a connection between Nesterov’s acceleration and a particular ordinary differential equation (ODE). Subsequently 280+ papers used and extended the technique, including the present paper. The present paper, however, does not cite most of the recent related work; there are only two citations to papers from the past two years. For example, a dozen of papers by Lessard et al:\nhttps://dblp.uni-trier.de/pers/hd/l/Lessard:Laurent\nare not cited at all. Similarly for the work of Recht et al, e.g.:\nhttps://arxiv.org/abs/1611.02635\nThe work of Scieur et al:\nhttps://scholar.google.fr/citations?user=hNscQzgAAAAJ&hl=fr\nis cited as a single pre-print, while the actual work encompasses two NIPS papers and some pre-prints. Assuming that the authors did not know about the related work is consistent with the fact that the authors reprove (very elegantly!) results of those papers (e.g. Lessard et al and Scieur et al for the acceleration, Recht et  al for the Polyak's method, etc). While I find the subject important, I like the writeup, and find the proofs elegant, I find it hard to justify acceptance. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}