{
    "Decision": {
        "metareview": "The paper introduces a new variant of the Dropout method. The reviewers agree that the procedure is clear. However, motivations behind the method are heuristic, and have to lean much on empirical evidence. A strong motivation behind the procedure is lacking, and the motivation behind the method is unclear. Furthermore, the empirical evidence is lacking in detail and could use better comparisons with existing literature.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Meta-Review"
    },
    "Reviews": [
        {
            "title": "needs further empirical evidence",
            "review": "Authors propose three modifications to dropout, specifically in context of dropout applied to deep networks utilizing the ReLU non-linearity.  The three modifications seem independently motivated and aim to overcome separate potential shortcomings of the current dropout approach.  These three modifications are combined into a new approach termed Jumpout.\n\nOverall I find this to be a weak paper requiring further work, for the following main reasons:\n\n* The proposed modifications are intuitively motivated and then empirically supported.  However, I find the intuitive reasoning unclear and have to lean much more on empirical evidence.  For instance, the motivation for modification 2 “dropout rate adapted to number of active neurons”, is that in case ReLU causes a large number of neurons to ‘shut down’ then the dropout rate in that layer should be reduced (or increased, depending on how it is defined) causing fewer neurons to further dropout.  However, if preventing co-adaptation is a reason to dropout neurons then the issue of conditional correlation (or co-activation given related inputs) will remain regardless of number of active neurons in a layer, thus changing the dropout rate as a function of ReLU activation is not fully justified.  Similarly, modification 3 “rescale outputs to work with batch normalization” proposes exponentiation by -0.75 with weak justification as a compromise.\n\n* I find the empirical evidence and support for the three modifications lacking in detail.  The authors provide results of the combined Jumpout technique on a number of tasks, but do not demonstrate effectiveness and contribution of individual modifications on error rates on the tasks they evaluated.\n\n* I also find the baseline systems to be on the weaker side (e.g. on CIFAR100 many systems now have higher than 82% accuracy with best being over 84, on STL-10 many systems now are well above 85%).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "not well explained and not rigorously tested",
            "review": "This paper proposes jumpout, which is a 3 step modification based on dropoout\nthat is designed to work better with batch normalization. Unfortunately, I did not understand the arguments on locally linear regions and ReLu and its relationship with the monotone dropout scheme,\nor why the half Gaussian is chosen.\n\nStill, jump out the procedure is fairly clear in Algorithm 1, and the results seems good.\nHowever, I could not make out much of why each step is done, and could not find empirical tests of the value of each step.\n\nI think the paper needs more work. All the proposals seem very heuristic, and it is important to test their separate effects. It should be easy to perform a ablation analysis since the 3 proposed steps are pretty independent and can be tested separately. Since two of these have to do with modifying the dropout rate, it would be important to compare with carefully cross-validated dropout rates, which I also do not see.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Unconvincing experimental results and theoretical arguments ",
            "review": "The paper proposes yet another variant of the celebrated Dropout algorithm. Specifically, the proposed method attempts to address the obvious drawbacks of Dropout: (i) the need to heuristically select the Dropout rate; and (ii) the universality of this selection across a layer. \n\nAs the authors have admitted in the paper (Sec. 1.2), there is a variety of methods already addressing the same problem. They argue that contrary to some of these methods \"jumpout does not rely on additional trained models: it adjusts the dropout rate solely based on the ReLU activation patterns. Moreover, jumpout introduces negligible computation and memory overhead relative to the original dropout methods, and can be easily incorporated into existing model architectures.\"\n\nHowever, this is argument is certainly untrue and rather misleading. The works of Kingma et al. (2015) and Molchanov et al. (2017), that the authors cite, does not introduce additional trained models. In addition, there is additional related work that the authors do not cite, but ought to: \n\n[1] Yarin Gal, Jiri Hron, Alex Kendall, \"Concrete Dropout,\" Proc. NIPS 2017.\n[2] Yingzhen Li, Yarin Gal, \"Dropout Inference in Bayesian Neural Networks with Alpha-divergences,\" Proc ICML 2017.\n[3] Harris Partaourides, Sotirios Chatzis, “Deep Network Regularization via Bayesian Inference of Synaptic Connectivity,” J. Kim et al. (Eds.): PAKDD 2017, Part I, LNAI 10234, pp. 30–41, 2017. \n\nThese methods also address a similar problem, without introducing extra networks or imposing extra costs art inference time. Thus, citing them, as well as COMPARING to them, is a necessity for this paper to be convincing.\n\nThese crucial shortcoming aside, there are various theoretical claims in this paper that are not sufficiently substantiated. To begin with, the arguments used in the last paragraph of page 4 seem at least speculative; then,  the authors proceed to propose a solution to the alleged problem in the beginning of page 5. They suggest sampling from a truncated Gaussian, but they do not elaborate on why this selection solves the problem; they limit themselves to noting that other selections, such as the Beta distribution, may also be considered in the future. We must also underline that [3] have suggested exactly that; sampling from a Beta. \n\nFinally, the last two modifications the authors propose seem reasonable, yet they are extremely heuristic. No one knows (which can be guaranteed through theoretical proofs or solid experimental evidence) that without these the method would not work. In addition, previous papers, e.g. [1-3] achieve similar goals in a principled fashion (ie by inferring proper posterior densities); without experimental comparisons, nobody knows which paradigm is best to adopt. \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}