{
    "Decision": "",
    "Reviews": [
        {
            "title": "I'll need better argument for the significance of this paper",
            "review": "In this work, the author(s) proposed to replace the random feature approximation with Nystrom approximation for kernelized deep neural networks.\n\nThe contribution of this work seems pretty incremental to me.  It's well known that Nystrom usually provides a more succinct/concise approximation of the Gram matrix. And its performance also critically depends on the choice of inducing points, which reminds me there is a lack of discussion on this matter in the paper. Often times, Cholesky  factorization works even better compared with Nystrom according to my own experience. \n\nKernelizing deep networks is a  challenging task on its own, and the reality is that it often does nothing more than slowing down the computation without tangible improvements. It takes much more than ``Nystromizing'' to convince me. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nystroem meets deep learning - review",
            "review": "DEEPSTRÖM NETWORKS\n\nQuality: good\nOriginality: original\nSignificance: relevant for ICLR\nPros: - interesting idea -- see detailed comments\nCons: a number of open issues in the presentation -- see detailed comments\n\nThe paper is based on some concepts to link non-linear data representations\nby means of non-linear kernel mappings, with the deep learning approach.\nThe presented concept is compared to former methods going into this direction\nand random fourier feature approaches. Additionally an adaptive scheme is\nsuggested where the internally used landmark matrix (which forms a kind\nof mahalanobis like weighting matrix to the data) is adapted. \n\nI think the main idea of the paper is interesting but the presentation\nlacks a number of details in the description/parametrization and the\nexperimental design. \n\n- 'Indeed, since the Nyström approximation uses an explicit feature map from the data kernel matrix, it is not restricted to a specific kernel function and not limited only to\n   RBF kernels, as in Fastfood approximation.' -- well yes an no, Nystroem works only effectively if the intrinsic dimensionality of the data is reasonable small - otherwise\n   you need a lot of disjunct landmarks. Additionally the parameterization - how many landmarks and which one is not so easy - in this sense random fourier features are easier to\n   handle. Further the random fourier features are not restricted to RBF kernel and not even to shift-invariant ones - there are a few proposals \n- The 'feature map' obtained by the Nystroem approximation is only helpful as long as the number of landmarks is reasonable small - otherwise an out of sample extension (mapping\n  new test points) in the spanned feature space may not be very reliable\n- please correct the spell errors like 'using ful training sets' (full); 'In (24), wo architectures' -- two?\n- random subsampling of the landmarks can lead to rather high approximation errors if the number of landmarks is small - there are some proposal to address this using leverage score sampling and other\n  see e.g. work of K. Zhang, Suykens, C. Musco\n- 'it is restricted to the Gaussian kernel' - well this is not true - see comments above (but I agree that there may not have been done much work to show this in practice, \n  and it is probably not possible to be used for - all - kinds of psd kernels)\n- 'Singular Value Decomposition (SVD) of K 11 . In the case where the size of the subsample L, m,is large, the computational complexity of the SVD is O(m 3 ).'\n  -- well - yes and no, with Nystroem you would expect that N is much ! larger than m - than the m^3 may still be somewhat ok. To learn the weightings on the fly in your\n  adaptive approach will address the complexity issue but may in parts degenerate performance - now it is a question what the user is more interested on\n- You mainly ignore the point how the landmarks have to be selected - random is (often) not a very reasonable choice - see respective work around this\n  In your datasets the problem may not show up because you have images only which have very strong characteristics and are intrinsically very low dimensional.\n  --> I think you have to add another section here - evaluating either different sampling schemes or just use a more state of the art one like leverage score sampling \n  (getting I think O(N^2) complexity); you should also address how you find a good guess for the number of landmarks\n- I suggest to use also additional non-image data. Basically one motivation in your approach is to go beyond the classical deep learning data representation \n  - this may not be so interesting for image recognition problems\n- I may suggest to spend a small subsection to a complexity analysis (including parameter optimization) of the various methods\n- Figure 2 - what do you mean by number of parameters? - there are a few strong fluctuations (fully connected, addative deep linear) - why\n- please increase the level of details of the accuracy axis in Figure 2 - so far it basically has 10% steps\n- if I have not missed it you did not say how the parameter \\sigma (or \\gamma) in the rbf kernel has been determined (same for other parametric kernels)\n- multiple kernels - well how did you combine the kernels (there are many options) \n- there are number of typos in the refs (see e.g. ref 14); some other references are very incomplete e.g. ref 15\n- plots should be (prefered) close to the respective text (the appendix is a bit strange here)\n  \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review of Deepstrom networks",
            "review": "Paper 1358 considers the problem of reducing the cost of fully-connected layers in deep architectures. For this purpose, the deep fried architecture was recently introduced. It involves replacing a fully connected layer by a structured projection layer. Followed by a cos/sin non-linearity, this actually leads to an approximation of the RBF kernel. \n\nPaper 1358 aims at extending this work beyond the RBF kernel. For this purpose, paper 1358 uses the Nyström approximation. It involves computing an explicit embedding based on a subset of training samples {x_i, i=1…m}. Given a sample x, the embedding is the multiplication of the sample kernel vector [k(x,x_1}, …, k(x, x_m)] with a projection matrix. Two variants are proposed: \n1)\tthe projection matrix is the inverse of the m x m sample kernel matrix;\n2)\tthe projection matrix is learned through backpropagation.\n\nThe approach is validated on four public benchmarks.\n\nOn the positive side: the paper is rather clear and easy to follow.\n\nOn the negative side:\n-\tThe novelty of the proposed approach is modest. All in all, it feels very much like applying the Nyström approximation on top of convnet features. Indeed, to the best of my understanding, the gradients were not back-propagated through the convolutional layers. This point is actually unclear and should be clarified. \n-\tIn this sense, the term “deepström” is somewhat misleading. This seems to indicate that the proposed approach is deep. However, if one compounds the matrices W and lc into a single matrix, then the propose Nyström module has a single hidden layer.\n-\tThe claim is made that the deep-fried module is restricted to approximating the RBF kernel. This is incorrect. Other kernels such as the arc-cosine kernel can be approximated by replacing the sin/cos non-linearity by a reLu.\n-\tThe experiments are lacking in some respects.\no\tStraightforward non-deep baselines are missing. For instance, the authors could have applied the “efficient additive kernels” of Vedaldi and Zisserman (CVPR’10) on top of convnet features. \no\tIt is unclear why a single hidden layer was used for the dense architecture baseline. Is this because the results did not improve with more? Or because the proposed deepström architecture has a single hidden layer?\no\tIn the result figures, it is unclear whether the # parameters on the x-axis is the total number of stored parameters, including the samples {x_i, i=1…m}, or just the estimated parameters, i.e. W and lc. \no\tThere is no result at Imagenet scale, contrary to the original deep-fried convnet paper.\no\tAt the end of section 4.1, the authors spend a lot of time explaining that the purpose is not to compete with the state-of-the-art. Yet, in the third paragraph of section 4.2, they claim that their approximation reaches the state-of-the-art. So which is it?\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}