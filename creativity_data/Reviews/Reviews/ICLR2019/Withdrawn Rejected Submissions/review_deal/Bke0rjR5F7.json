{
    "Decision": {
        "metareview": "The paper presents a method to stochastically optimize second-order penalties and show how this could be applied to training fairness-aware classifiers, where the linear penalties associated with common fairness criteria are expressed as the second order penalties. \n\nWhile the reviewers acknowledged the potential usefulness of the proposed approach, all of them agreed that the paper requires: (1) major improvement in clarifying important points related to the approach (see R3’s detailed comments; R2’s concern on using the double sampling method to train non-convex models; see R1’s and R3’s concerns regarding the double summation/integral terms and how this effects runtime), and (2) major improvement in justifying its application to fairness; as noted by R2, “there is no sufficient evidence why non-convex models are actually useful in the experiments”. Given that fairness problems are currently studied on the small scale datasets (which is not this paper’s fault), a comparison to simpler methods for fairness or other applications could substantially strengthen the contribution and evaluation of this work.\nWe hope the reviews are useful for improving and revising the paper. \n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Meta-Review"
    },
    "Reviews": [
        {
            "title": "Motivation and contribution are not very clear",
            "review": "In this paper, the authors propose a method to generate predictions under fairness constraints by optimizing quadratic penalties over non-convex loss functions. The main idea is to replace the linear fairness constraints by the second-order penalty. Meanwhile, an efficient method is derived to compute the gradients associated with the second-order penalties in stochastic mini-batch settings. Finally, the experimental results show the effectiveness of their proposed method empirically. \n\n(1) The authors argued that their experimental results in a more practical training procedure in non-convex, large-data settings. However, in Section 6, the sample size of the data-sets they used are small and the loss functions of learning models are convex. I think they need to provide more experimental results to make their proposed method more convincing. \n\n(2) It is a little difficult to follow the motivation and contributions of this paper. I would recommend the authors to improve the presentation by providing more context for the use of double integral or sampling method and other mostly relevant works in this area. \n\n(3) From the optimization viewpoint, the second-order penalty in Eq. (3) is convex with respect to d. Why replacing the linear penalties with quadratic penalties to solve the shortcomings of using linear penalties or Lagrange multipliers. Isn't the square of an expectation always an expectation of pairs? In other word, Eq. (4) is always equivalient to Eq. (7) without additional conditions?\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Experiments and motivation can be improved",
            "review": "In this paper, the authors propose a method for optimizing quadratic penalties over non-convex loss functions. The authors motivate their approach by showing the complexity of training non-convex models with linear constraints and proposing a simpler method to introduce these constraints as regularization terms. Finally, they show how their proposed method compares to alternative solutions on a series of benchmarks.\n\nUse of double sampling method for estimating the loss and second order penalty appears to be novel, however, there is no discussion of the implications of using this method to train non-convex models --- one would suspect that use of double sampling may make the gradient descent susceptible to high variance. Simulation results in the paper only demonstrate how the use of the loss changes the solution but there is no discussion or experiments on complexity of training models that use this approach. This is particularly important because authors are claiming their method does not suffer from complexity issues which other methods suffer, but this claim is not supported by any evidence. For example, how many iterations were necessary to train the model? How sensitive is the training to initial conditions and changes in hyper parameters?\n\nAuthors motivate their method by pointing out the limitations of using fairness constraints in non-convex models, however, they don’t provide-sufficient evidence for why non-convex models are actually useful in their experiments --- the datasets they used are small and models that are actually convex may perform just as good or nearly as good as highly non-linear, non-convex models which authors are trying to use.\n\nOverall, I would recommend the authors to improve the presentation by providing more context for the use of double sampling method and other relevant works in this area (at least showing the impact of using double sampling on training). Moreover, given the datasets and scope of questions related to fairness they need to provide better experiments that motivate the use of their method (compared to simpler methods) or consider other problems where their approach could be more useful. Given these considerations I believe this paper does not meet the standards for publication.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Unclear contribution",
            "review": "The authors propose a method to generate predictions under fairness constraints. The main idea is to take linear fairness constraints, and replace them with weak squared penalties plugged into the objectives, which supposed to help in cases where the loss function is not convex. The penalty coefficients are chosen by cross-validation, and the effectiveness of this approach is demonstrated empirically.\n\nIn Sec. 3.1, the authors point out several shortcomings of using linear penalties (using Lagrange multipliers) for non-convex losses. These seem valid. Sec. 3.2, however, is not clear on why exactly replacing the linear penalties with quadratic penalties solves these issues. I'm hoping the authors can clarify the following points:\n\n1) The authors note that, for quadratic penalties, \\lambda->0 means no constraints, and \\lambda->\\infty means hard constraints. Isn't this also true for linear constraints?\n\n2a) Why do linear penalties have unique \\lambda_k for each constraint k, but the quadratic objective has only a single \\lambda for all constraints?\n\n2b) Why can CV over \\lambda be used for quadratic constraints - what is the justification? And, more importantly, why *can't* it be used with linear constraints? If it can, then this should be one of the baselines compared to in the experiments.\n\n3) What is the criterion optimized for by CV - accuracy or the constraints? Different parts of the paper give different answers to this question. For example, \"... may be easily determined via standard hyperparameter optimization methods\" vs. \"tuning \\lambda to directly satisfy the desired fairness metrics\". Or even more unclear - \"choose \\lambda ... so that the final solution gives the desired fairness-accuracy trade-off\". How is the desired trade-off defined?\n\n4) If there is a trade-off between fairness and accuracy, and no clear-cut criterion for evaluation is pre-defined, then the evaluation procedure should compare methods across this trade-off (similarly to precision-recall analysis).\n\n5) The authors differentiate between cases where the loss is either convex or non-convex. This is confusing - most losses are convex, and non-linearity appears when they are composed with non-linear predictors. Is this the case here? If so, the fairness constraints are no longer linear, and they're quadratic counterpart is no longer quadratic. It would be helpful if the authors specify where the non-linearity comes from, and what they assume about the loss and predictors.\n\n6) Why is it important to show that the quadratic constraints can be written as an expectation? Isn't the square of an expectation always an expectation of pairs? How does the double summation/integral effect runtime?\n\n7) It would be helpful if the authors differentiate between loss/constraints over the entire distribution vs. over a given sample set.\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}