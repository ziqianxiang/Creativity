{
    "Decision": "",
    "Reviews": [
        {
            "title": "Inhibited Softmax for Uncertainty Estimation in Neural Networks",
            "review": "Paper claims that the modified version of softmax known as inhibited softmax can be used to calculate uncertainty.\n\nInhibited softmax can be thought of as having an extra output, i.e if there were 3 classes then normal softmax would have 3 nodes at the output but inhibited softmax can be thought of as having the 4th node with a constant output(value of constant is hyperparameter).\n\nThe output of this extra node in inhibited softmax is used to quantify uncertainty.\n\n\nPros\n- The method requires only one forward pass, unlike popular Bayesian methods.\n- seems to work for out of distribution detection.\n\nCons\n- No mathematical intuition as to why the output of the extra node can be taken as a measure for uncertainty.\n- In cases where final output is not a softmax (i.e regression) then the method is not applicable in its current form. (This and other cons have been acknowledged in the paper)\n\nMy concerns\n- the method does some modifications such as turning off bias in the penultimate layer etc. What if we do all those changes and keep the softmax layer as it is? This might help to figure out if the advantage was due to inhibited softmax alone or the engineering tricks done to make it work.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "# Summary\n\nThe paper proposes a new method for computing output uncertainty estimates in DNNs for classification problems. The authors claim that replacing the softmax output layer for an Inhibit Softmax layer (a modified softmax with an additional constant additive term in the denominator, proposed in Saito et al., 2016) matches the state-of-the-art methods for uncertainty estimation an outperforms them in out-of-distribution detection tasks.  While the original idea is simple, the final method is convoluted, including several modifications (section 3.1.) to deal with an ill-behaved optimization objective, making it difficult to evaluate why the method works in the end. Finally, the experimental section needs to be extended. The proposed metrics are presented as the gold standard to measure uncertainty calibration; however, it is known that this is itself an open problem due to the fact that ground truth labels about uncertainty are not available. The paper should be extended with experiments that use a simulated data to gain a further understanding of what kind of uncertainty it is capture by the model, as well as considering additional metrics like the brier score, histograms of predictive entropy, …. Also, several state-of-the-art inference methods for BNNs that improve the results of the original backprop paper are missing in the experimental section.\n\n# Details\n\nThe key idea is to use an Inhibited softmax output layer (instead the classical softmax) to get a better estimation of the output uncertainty (epistemic, aleatoric and distributional). They claim that the method matches other state-of-the-art methods in the field and outperform them in out-of-distribution detection tasks. However, the reason why this may be true is unclear and the experimental section needs to be strengthen. \n\nRegarding the inhibited softmax layer, they propose to decompose the log-likelihood in two term: the log-likelihood of the classical softmax plus a term modeling a certainty score that is maximized. What the method is doing is adding a term to all the probabilities which effectively is a simple smoothing of the output probabilities, leaving out some probability for when the test data is far from the training one. This value is fix as a hyper-parameter.  In particular “c” dictates the probability of the instance belonging to a new class, and so, fixing it ad-hoc means that you determine how far a test data should be from the training set in order to consider it out-of-distribution. It seems then a bad idea to fix this with independence of the dataset/model we are looking at. The paper would benefit from a deeper analysis of the method and connections with other methods in the literature, e.g. connections to the open world classification problem would be of interest.\n\nAuthors should elaborate on Section 3.1.. This section summarized 4 modifications that seems to be needed in order for the proposed method to work. Each of these modifications are explained in just a short paragraph, and it seems that choosing correctly these values is critical for the final performance of the algorithm. The number of hyper-parameters to tune increases making more difficult to understand why the method works (despite the analysis in the appendix). \n\nIn the experimental section, I would encourage the authors to propose some experiments using synthetic data where they have more control about the uncertainty of the generated data. These experiments would allow to visualize several properties of the uncertainty estimator and offer more insights to the reader. I would also like to see how the current method performs in the low data regime: when you have a small training set and an over parameterized DNN. In this case, I would expect that the method overfit since all the uncertainty of the lower levels is collapsed before reaching the output. Finally, in recent years, several advances in variational inference have been proposed that improve upon the basic version of Backprop (Blundell et al., 2015), e.g., https://arxiv.org/pdf/1703.01961.pdf, https://arxiv.org/pdf/1511.06233.pdf … that should be included in the experimental section.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea, but needs more careful analysis.",
            "review": "The authors present inhibited softmax, a modification of the softmax through adding a constant activation which under a particular interpretation provides a measure for uncertainty. The method has no learnable parameter, works with only a single forward operation, does not need additional data for training, and does no input \"preprocessing.\"  The authors present experiments on vision problems and sentiment analysis.\n\nThe idea is to add a constant pre-softmax class activation c. In section 3 the authors explain why this \"serves as an uncertainty estimator.\" I read this paragraph several times, I do not see how they reach this conclusion. They point out that \"P_c(x) is maximized only for cases from training distribution.\" Unless we have a reasonable justification that P_c(x) would emit small values on the other samples, assuming this alone does not tell us anything useful.\n\nThe authors chose to split the new formulation to S(x)_i times P_c(x) in equation (2). If we keep the original equation and expand the cross-entropy term we get:\n\nL_i = - x_(y_i) + log(sum(exp(x_i))+exp(c)).\n\nWhat c does, is that it puts a lower bound on the true class activation and an upper bound on the incorrect class activation. The first term increases the activation of the true class, the second term cancels the first term once x_(y_i) >> c. The second term also decreases the other classes' activations if they are >> c and > x_(y_i)), but once they go below c they are no longer penalized. If we instead write the softmax as a hard max, it becomes more clear why this is the case.\n\nL_i = - x_(y_i) + max({x_i}+{c})\n\nUnlike the original cross-entropy loss that squashes other probabilities to 0 and the correct class to 1, the inhibited softmax would be happy if the other probabilities are less than some threshold t > 0 (which is a function of c) and the correct class probability > t.\n\nFrom this perspective, this approach is just another way to penalize the predictions similar to [1] in which the authors penalize the negative entropy of the prediction to encourage a more uniform prediction. That method also has no additional learnable parameters, works with only a single forward pass, requires no more out-of-distribution data, and has no input \"preprocessing.\" \n\nIn section 3.1 the authors:\n- (i) Propose to remove the bias in the final layer of the neural network to eliminate data-independency. Removing the bias from the final layer should have minimal effect on the ability of network in inducing data-independent bias -- the bias of the final layer can be directly absorbed in the bias of the layer immediate before that.\n- (ii) the second paragraph that discusses replacing the penultimate layer with a kernel function is not clear. \n- (iii) The assertion that x_i increases boundlessly is wrong. Adding a constant value to all of x_i's does not change the loss of the original cross-entropy term -- it cancels out in the probability; furthermore, the gradient of - log P_c converges to zero quickly when max(x_i) >> c. If we expand the term - log P_c  to - log(sum(exp(x_i))) + log(sum(exp(x_i))+exp(c)), and then simplify it to - max(x_i) + max({x_i}+{c}), we'll see that when x_i is sufficiently larger than c, the first term and the second term cancel out. This cancelling effect happens immediately with a hard max when max(x_i)>c, but for a softmax, it is more gradual, but quick nonetheless. Therefore the x_i's should not be increasing boundlessly.\n- (iv) Applying l2 regularization only on the weights of the final layer merely pushes the high-magnitude activation to the layer immediate before it. Unless we regularize all the layers, this should have minimal effect.\n\nIt seems, empirically, that the deeper the network, the more the overconfidence problem [2]. The overconfidence makes the detection of out-of-distribution samples in modern neural networks particularly more difficult. The first matter that concerns me in the evaluation of this work is that the networks under study, in addition to being non-standard, are much shallower than the common neural networks that are used for evaluation of out-of-distribution samples. For instance, (Wide)-ResNets are commonly used in previous work such as [3] to evaluate the method for OOD detection. The OOD problem in the shallow networks is a relatively easier problem than the OOD detection in deep networks. The second concern is that the set of chosen datasets for in-distribution and out-of-distribution are far from challenging: the datasets are so different that a linear classifier based on the local image statistics can separate them. Some more challenging datasets that appear in the OOD detection literature are CIFAR100, TinyImageNet, and SUN.\n\nQuality. The provided theoretical analysis are either incomplete or need a more careful review by the authors. See above.\n\nClarity. The paper is overall easy to follow and understand. However, there are a few instances that should be clarified. The paragraph that provides intuition on why inhibited softmax serves as an uncertainty estimator could be improved. The paragraph explaining the adjustment by using a kernel activation function is not clear. The explanation of the experiments (the three bullet points in section 4) can be improved by adding more details. Specifically, the measures should be explained properly.\n\nOriginality. The three main contributions are, (i) the mathematical explanation, which as discussed earlier is not clear (to me) and seems insufficient by itself. (ii) The additional adjustments, which as discussed earlier were not convincing, and (iii) the benchmarks comparing the proposed idea with some of the recent methods that I believe do not reflect a conclusive picture because of the two stated concerns. The idea of inhibited softmax itself comes from another publication (Saito et al. in the paper).\n\nI would be happy to change my rating if I have misunderstood parts of the paper or have made a mistake in my review.\n\nReferences.\n[1] G. Pereyra, G. Tucker, J. Chorowski, Ł. Kaiser, and G. Hinton, “Regularizing Neural Networks by Penalizing Confident Output Distributions,” ICLR Work., 2017\n[2] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “On Calibration of Modern Neural Networks,” ICML, 2017.\n[3] S. Liang, Y. Li, and R. Srikant, “Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks,” ICLR, 2018.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}