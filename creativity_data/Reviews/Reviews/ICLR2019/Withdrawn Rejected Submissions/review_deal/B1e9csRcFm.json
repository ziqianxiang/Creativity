{
    "Decision": {
        "metareview": "This paper provides a generalization analysis for graph embedding methods concluding with the observation that the norm of the embedding vectors provides an effective regularization, more so than dimensionality alone. The main theoretical result is backed up by several experiments.  While the result appears to be correct, norm control, dimensionality reduction and early stopping during optimization are all very well studied in machine learning as effective regularizers, either operating alone or in conjunction. The regularization parameters, iteration count, embedding dimensionality is typically tuned for an application. The AC agrees with Reviewer 2 that the paper does not provide sufficiently interesting insights beyond this observation and is unlikely to influence practical applications of these methods.   Both reviewer 2 and 3 have also raised points on the need for stronger empirical analysis.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Technically correct but lacking in sufficiently interesting insights"
    },
    "Reviews": [
        {
            "title": "The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration",
            "review": "In this paper, the authors proved that the generalization error of linear graph embedding methods is bounded by the norm of embedding vectors, rather than the dimensionality constraints. Interestingly, along with the analysis of Levy & Goldberg (2014), they found that linear graph embedding methods are probably computing a low-norm factorization of the PMI matrix. Correspondingly,  experimental results are provided to support their analysis.\nOverall, this work is theoretically complete and experimentally sufficient.\n\n1. it is unclear whether the embedding dimensions of all cases (with varying value of \\lambda_r) are fixed as a constant in Fig. 1 - Fig. 3.\n\n2. Figure 4 shows the impact of embedding dimension on the generalization performance. Are these results obtained after 50 SGD epochs? Comparing Fig.4 (a) with Fig.3 (a), we may infer that the results in Fig.3(a) when \\lambda_r = 0 are obtained by setting the embedded dimension as about 10^2. How about the generalization performance during SGD for \\lambda_r = 0 if the embedded dimension is set to be smaller than 10?\n\n3. In Claim 1, the degree d and the dimension D are mixed. ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The importance of norm term in generalization bound over-emphasized",
            "review": "The manuscript proposes a theoretical bound on the generalization performance of learning graph embeddings. The authors find that the term in the generalization bound that represents the function complexity involves the norm of the learnt coordinates, based on which they argue that it is the norm of the coordinates that determines the success of the learnt representation.\n\nI am not very familiar with the literature on graph embeddings; however, to the extent of my understanding of the paper, I have a number of concerns:\n\n- In a generalization bound like in Theorem 1, it is very typical of the generalization error to include a term that represents the complexity of the hypothesis function class. In the presented result, this would be the second term on the rhs, which involves the spectral norm of the adjacency matrix and the bounds on the norm of the learnt coordinates. This term captures the Rademacher complexity of the hypothesis function class. In my understanding, there is nothing really surprising about this: most of the results in learning theory would include a term directly or indirectly related to some norm on the hypothesis function class. However, it would then require a lot of further justification to conclude that the key factor determining the performance is the norm of the learnt representation based on this.\n\n- I am not sure if the results in Figure 1 provide a really meaningful justification about the importance of norm. It is observed that the norm increases during the epochs, however, shouldn’t we be also checking the evolution of the error at the same time to draw a meaningful conclusion? In particular, the norm seems to increase rather monotonically throughout the epochs, whereas we expect the error to decrease first, reach an optimal, and then start increasing due to overfitting. So can we really say that the error is proportional to the norm?\n\n- Similarly, in the results in Figure 5, we can observe that the regularization coefficient has an optimal value that maximizes the precision. On the other hand, the norm of the learnt coordinates is expected to decrease monotonically with increasing lambda_r. Again, it seems difficult to conclude that the norm is the key factor determining the error.\n\n- Minor comments: \n1. In page 2, in the expression of L, the node u should be in set V, I guess.\n2. Please define the function sigma used in the objective functions. \n3. Typo right under Section 3 title: “grpah”\n4. The definition of matrix A_sigma is not clear to me. What does the “there exists y” expression mean in the first line?\n\n- To sum up, my feeling is that the presence of the term involving the norm in Theorem 1 is rather classical in learning theory, and its importance seems to be over-emphasized in this study. Moreover, I am not fully convinced about the experimental evidence. Therefore, I cannot recommend accepting this paper. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper needs more clarification on the implication from the theoretical results as well as empirical results",
            "review": "The main contribution of this paper is that it shows both theoretically and empirically that in linear graph embeddings, the generalization error is bounded by the norms of the embedding vectors rather than the dimension of the embedding vectors. \n\nThe list of my concerns or cons of this paper is:\n\n- For the main theorem, i.e., Theorem 1, \na.) why is it intuitive that the size of the training dataset required for learning a norm constrained graph embedding is O(C|A|_2). This is not that intuitive to me. Later, the authors argue that graphs are usually sparse and average node degree is usually smaller than the embedding size, thus it is easily overfitting the training data. However, I would say, in practice, the positive training pairs are not restricted to 1-hop neighbors, but could also be 2 or more hops, in that case, it won't easily overfit. \nb.) the main result from the theorem is that the error gap of norm constrained embeddings scales as O(d^-0.5(lnn)0.5), but I did not see how this is related to the norms of the embedding vectors and how is this evidenced in the empirical studies? It might be better to show a plot of error gap vs. d and/or n. \nc.) how is this analysis related to the later claim that \\lambda_r controls the model capacity of linear graph embedding?\n\n- The linear graph embedding framework considered in this paper assumes that each node only has one set of embeddings, but in practice, one node usually has two sets of embeddings as context node or a center node. How would this affect the whole analysis and claims?\n\n- How would the claims or analysis in this paper be generalized to non-linear graph embedding frameworks?\n\n- For the experiments, \na.) In Figure 3, the y label of (b) is missing, and the Average L2 norm of (c) cannot reflect the Generalization performance \nb.) In Figure 4(a), why after overfitting, we can still observe that the test accuracy increases?\nc.) In Figure 5, why the test precision first increases and then decrease with more regularization?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}