{
    "Decision": {
        "metareview": "The paper proposes an algorithm for semi-supervised learning, which incorporate biased negative data into the existing PU learning framework.\n\nThe reviewers and AC commonly note the critical limitation of practical value of the paper and results are rather straightforward.\n\nAC decided the paper might not be ready to publish as other contributions are not enough to compensate the issue.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Limited practical value"
    },
    "Reviews": [
        {
            "title": "Paper correct and carefully written but results rather straightforward",
            "review": "The authors \frst present standard binary (positive negative or PN) classi\fca-\ntion, followed by positive unlabeled (PU) classi\fcation, that they motivate with\nexamples, such as one-class remote sensing classi\fcation. The new setting that\nthey introduce and study is called positive unlabeled biaised negative (PUbN\nclassi\fcation) and adds a biaised negative sample to PU learning. They give\nmotivating examples and compare this setting to the existing literature. A con-\nvincing case is made regarding the di\u000berence between the PUbN problem and\nthe known problems of semi-supervised learning and dataset shift.\nThey start by recalling the notations and nature of standard binary classi\f-\ncation, PU classi\fcation and the nnPU (non-negative PU) strategy, as in the\nprevious PU learning papers. Then, they present the semi-supervised setting\nunder the name PNU learning, which simply studies the minimization of a con-\nvex combination of the PN risk and the PU risk. As in PU learning, a correction\nexists to avoid considering the estimate of the negative risk to be negative, re-\nferred to as nnPNU.\nFinally, the authors introduce PUbN learning as the problem in which we\nonly have access to negatives that follow the law p(x\\mid y = -1; s = +1), where s\nis a latent variable that formalizes the bias.\nAs in PU learning, the authors derive an unbiased estimator of the risk that\ninvolves only distributions for which data is available. However, they need\nto reweight the P and bN distribution by the unknown posterior probability\n\u001bsigma(x) = p(s = +1\\mid x) of s. Considering s as the label, the problem of learning\na probabilistic classi\fer separating the elements for which s = +1 and s = 􀀀1\ncan be seen as a PU learning problem, which gives an estimator ^\u001b of sigma,\nand makes the method practical.\nThey derive estimation error bounds, that depend on the mean squared\ndi\u000bfference between \u001bsigma and sigma^\u001b and a term of order n^-1/2 where n is the cardinal\nof the smallest sample. They considered the function ^\u001b as a \fxed function in\ntheir bounds, which implies that the bounds are only true if some of the data is\nkept for the estimation of sigma^\u001b. Finally, they present a variant of their algorithm\nfor PU learning, named PUbNnN where unlabeled instances are not all given\nthe same weight, but weighted according to sigma hat\u001b. The experiments use neural networks with stochastic optimization, on the classic datasets MNIST, CIFAR-10 and 20 Newsgroup. They report better per-\nformance using their technique on all datasets. The authors documented their\nexperiences thoroughly in the appendix. However, I did not \ffind information\nabout the nature of the estimator of the posterior probability sigma^\u001b, which is im-\nportant for reproducibility. Furthermore, in appendix B, choosing sigma^\u001b = 0 will\nminimize the criterion . Finally, they proceed to justify the dominance of\nthe variant of their method over usual nnPU learning.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper studied classification problem, with Positive, Unlabeled and biased Negative labeled data. ",
            "review": "This paper studied classification problem, with Positive, Unlabeled and biased Negative labeled data. The paper presents a two-step method, where the first-step is instance weighting and the second-step is standard binary classification. The paper shows theoretical proofs on the error estimation. Experiments on several well-known data sets are conducted and compared. \n\nThe good things of the paper are clear. \n\n1.\tTechnical sound with statistical foundation\n2.\tTheoretical foundation\n3.\tProblem is general\n4.\tPaper is general well written.\n\nSome weak points as well\n1.\tApplication value is not so big, as there is no real application problem and the experiments are based on simulation.\n2.\tAlthough the studied problem is reasonable, the setup is a bit too general and need rather strict condition to have a good method. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "I'm not convinced by the main assumption, it still needs more work to get published.",
            "review": "This paper has proposed a new algorithm for semi-supervised learning, which incorporate biased negative data into the existing PU learning framework.\n\nThe paper was written in clarity and easy to follow overall. However, the original motivation for having biased negative data are not explained very clear. The relation to dataset shift was very interesting, but it’s unclear what’s the exact connection between the proposed algorithm and the dataset shift. Maybe the authors can elaborate a little more on their point here in the future revision.\n\nThe paper has made some assumption about the relation between the latent random variable and the label in section 2.4. In the experiment, data sets are generated following the exact assumption. That’s not surprising to see that the proposed algorithm that fits the assumption will perform better than the previous methods without this assumption. In practice, there’s no way to really verify this assumption. Thus, it’s more interesting to see how the algorithm performs under the more generic semi-supervised learning setting, with unbiased, or biased negatives that don’t really fit the exact assumption in this paper.\n\nMoreover, I’d like to see more intuition on why adding biased negative data will further improve upon nnPNU. The author provided some explanation in section 4.3, which seems just observations on the FPR and FNR, rather than the fundamental explanation for the advantage of this algorithm.\n\nChoice of baseline methods is also limited. The original paper [1] for PNU has included a bunch of benchmark algorithms for semi-supervised learning. The authors should also include more benchmark algorithms for comparison, e.g. those listed in Section 5.2 in [1].\n\n[1] Sakai, Tomoya, et al. \"Semi-supervised classification based on classification from positive and unlabeled data.\" arXiv preprint arXiv:1605.06955 (2016).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}