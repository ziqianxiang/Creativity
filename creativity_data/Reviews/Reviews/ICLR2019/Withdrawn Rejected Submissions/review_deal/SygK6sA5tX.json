{
    "Decision": {
        "metareview": "AR1 is concerned about the overlap of this paper with Gama et al., 2018 as well as lack of theoretical analysis and poor results on REDDIT-5k and REDDIT-5B datasets. AR2 reflects the same concerns (lack of clear cut novelty over Zou & Lerman, 2018, Game, 2018. AR3 also points the same issue re. lack of theoretical results. The austhors admit that Zou and Lerman, 2018, and Gama, 2018, focus on stability results while this submission offers empirical evaluations.\n\nUnfortunately, reviewers did not find these arguments convincing. Thus, at this point, the paper cannot be accepted for publication in ICLR. AC strongly encourages authors to develop their theoretical 'edge' over this crowded market of GCNs and scattering approaches.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Lack of theoretical novelty voiced as one of main issues."
    },
    "Reviews": [
        {
            "title": "Interesting construction but limited novelty",
            "review": "The authors propose an advance in geometric deep learning based on a geometric scattering transform using graph wavelets defined in terms of ran- dom walks on the graph. The paper is well written, easy to understand also for a not-so-tech audience but nevertheless precise in all the mathematical details.\nIntro and references are satisfactory, and also the experimental section is sufficiently convincing. However, there are two big issues undermining the overall structure of the manuscript: \na) the theoretical novelty w.r.t. (Zou & Lerman, 2018) and (Game, 2018) is partial and rather technical, so the originality of the present manuscript is limited\nb) the improvement w.r.t. to other published method is rather small, so the performance gain is only partially justified by the quite complex theoretical construction.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper and ideas, a bit low on results maybe",
            "review": "This paper generalizes scattering transform to graphs. It defines wavelets, scattering coefficients on graph signals. The experimental section describes their use in classification tasks with comparisons with recent methods. It seems scattering performs less well than SOTA methods, but has the advantages of not requiring any training so potentially good candidates for low data regimes application. Interesting and original paper and ideas being developed, but might be a tiny bit weak in term of results, both theoretical and experimental ?\n\nThere is not much theoretical results (mostly definition and hints that some of the results from euclidian case might generalize without formal investigation).\n\nRegarding the results, in particular table3, given that you use particular hyper parameters J and Q, for each dataset, this is arguably a bit of architectural overfitting ? Results would be more convincing IMO if obtained with a single set of hyper parameters. What was the procedure to come up with those parameters ?\n\nRegarding the methodology for training the classifier, I am not familiar with these datasets but using just a 1/10 of the data to train classifier seems a bit extreme ? \nHow about training each on 90% random subset of training set and averaging ? Or just the whole training subset ? That would still be fine in the sense that none of the classifier would have seen the test set ?\n\np2 '~it naturally extends to multiple signals by concatenating their scattering features~'\n\nP4 figure 1: Not very clear what those visualizations are. \\Psi_j is supposedly a n x n matrix so, is this \\Psi_j applied to a two different Dirac on the graph ? Would be good to clarify exactly what is being plotted in the legend.\n\nseems to be the biggest limitations of the proposed approach. By not early mixing of different features one might lose the high frequencies correlations between different signals defined on a single graph.\n\nP4. IMO capsule is not such a great name / already used in ML by Hinton's capsule etc... Why not simply 'moments' or 'statistics' ? \n\n'We can replace (3) with normalized moments of x ... how exactly do you normalize ? ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A good idea, but the delineation to other work needs improvement",
            "review": "# Summary of the paper\n\nInspired by the success of deep filter banks, this paper presents a designed deep filter bank for graphs that is based on random walks.  More precisely, the technique uses lazy random walks, expressed in terms of the graph Laplacian, and re-frames this in terms of graph signal processing. Similarly to wavelets, graph node features are calculated at different scales and subsequently summed in order to remain invariant under permutations. Several experiments on graph data sets demonstrate the performance of the new technique.\n\n# Review\n\nThis paper is written very well and explains its method with high clarity. The principal issues I see are as follows:\n\n- The originality of the contributions is not clear\n- Missing theoretical discussion\n- The experimental setup is terse and slightly confusing\n\nConcerning the originality of the paper, the differences to Gama et al., 'Diffusion Scattering Transforms on Graphs' are not made clear. Cursory reading of this publication shows a large degree of similarity. Both of the papers make use of diffusion geometry, but Gama et al. _also_ define a multi-scale filter bank, similar to Eq. 4 and 5. The paper needs to position itself more clearly vis-Ã -vis this other publication. Is the present approach to be seen more as an application of the theory that was developed in the paper by Gama et al.? What are the key similarities and differences? In terms of space, this could be added to Section 3.2, which could be rephrased as a generic 'Differences to other methods' section and has to be slightly condensed in any case (see my suggestions below). Another publication by Zou & Lerman, 'Graph Convolutional Neural Networks via Scattering', is also cited as an inspiration, but here the differences are larger in my understanding and do not necessitate further justification. Last, the publication 'Graph Capsule Convolutional Neural Networks' by Verma & Zhang is also cited for the definition of 'scattering capsules'. Again, cursory reading of the publication shows that this approach is similar to the presented one; the only difference being which features are used for the definition of capsules. I recommend referring to the invariants as 'capsules' and link it back to Verma & Zhang so that the provenance of the terminology is clear.\n\nConcerning the theoretical part of the paper, I miss a discussion of the complexity of the approach. Such a discussion does not have to be long, but in particular since the paper mentions that the applicability of scattering transforms for transfer learning (and also remarks about the universality of them in Section 4), some space should be devoted to theoretical considerations (memory complexity, runtime complexity). This would strengthen the paper a lot, in particular in light of the complexity of other approaches! Furthermore, an additional experiment about the stability of scattering transforms appears warranted. While I applaud the experimental description in the paper (number of scales, how the maximum scale is chosen, ...), an additional proof or experiment in the appendix should deal with the stability. Let's assume that for extremely large graphs, I am content with 'almost-but-not-quite-as-good' classification performance. Is it possible to achieve this by limiting the number of scales? How much to the results depend on the 'right' choice here?\n\nConcerning the experimental setup, I think that the way (average) accuracies are reported at present is slightly misleading. The paper even remarks about this in footnote 2. While I understand the need of demonstrating the universality of these features, I think that the current setup is not optimal for this. I would recommend (in addition to reporting accuracies) a transfer learning setup rather in which the beneficial properties of the new method can be better explored. More precisely, the claim from Section 4, 4th paragraph ('Since the scattering transform...') needs to be further explored. This appears to be a unique feature of the new method. The current experimental setup does not exploit it. As a side-note, I realize that this might sound like a standard request for 'show more experiments', but I think the paper would be more impactful if it contained one scenario in which its benefits over other approaches are clear.\n\n# Suggestions for improvement\n\nThe paper flows extremely well and it is clear that care has been taken to ensure that everything can be understood. I liked the discussion of invariance properties in particular. There are only a few minor things that can be improved:\n\n- 'covariant' and 'equivariant', while common in (graph) signal processing, could be briefly explained to increase accessibility and impact\n- 'order' and 'layer' are not used consistently: in the caption of Figure 2a, the term 'order' is used, but for Eq. 4 and 5, for example, the term 'layer' is employed. Since 'layer' is more reminiscent of a DNN, I would suggest to use 'order' throughout the paper, because it meshes better with the way the scattering invariants are defined.\n- the notation $Sx$ is slightly overloaded; in Figure 2a, for example, it is not clear at first that the individual cascades are supposed to form a *set*; this is only explained at the end of Section 3.1; to make matters more consistent, the figure should be updated and the combination of individual cascades should be made clear\n- In Eq. 5, the bars of the absolute value are not set correctly; the absolute value should cover $\\psi_j x(v_i)$ and not $(v_i)$ itself.\n- minor 'gripe': $\\psi^{(J)}$ is defined as a set in Eq. 2, but it is treated as a matrix or an operator (and also referred to as such); this should be more consistent\n- The discussion of the aggregation of multiple statistics in Section 3.2 appears to be somewhat redundant in light of the discussion for Eq. 4 and Eq. 5 in the preceding section\n- in the appendix, more details about the training of the FCN should be added; all other parts of the experiments are described in sufficient detail, but the training process requires additional information about learning rates etc.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}