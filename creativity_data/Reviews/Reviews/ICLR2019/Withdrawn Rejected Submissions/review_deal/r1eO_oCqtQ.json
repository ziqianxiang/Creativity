{
    "Decision": {
        "metareview": "perhaps the biggest issue with the proposed approach is that the proposed approach, which supposedly addresses the issue of capturing long-term dependency with a faster convergence, was only tested on problems with largely fixed length. with the proposed k_n gate being defined as a gaussian with a single mean (per unit?) and variance, it is important and interesting to know how this network would cope with examples of vastly varying lengths. in addition, r3 made good points about comparison against conventional LSTM and how it should be done with careful hyperparameter tuning and based on conventional known setups. \n\nthis submission will be greatly strengthened with more experiments using a better set of benchmarks and by more carefully placing its contribution w.r.t. other recent advances.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "reviewers are not convinced (nor am i)"
    },
    "Reviews": [
        {
            "title": "Simple idea with potential but experiments are unconvincing",
            "review": "In the vein of recent work on learning “ticking” behaviour for LSTMs such as Phased LSTM, this paper proposes to add additional data independent gates to LSTM units that are defined as Gaussian functions of time indices.\nThe performance of the modified g-LSTM is compared to LSTM on the Addition, sequential MNIST and sequential CIFAR-10 tasks. The authors argue that g-LSTM results in better performance and has faster convergence on these tasks. \n\nAdditionally, it is proposed that one can reduce the amount of computations performed by the network by adding a computation budget term to the optimized loss that encouraged the cells to update less often. Finally, a technique for gradually transitioning from a g-LSTM to an LSTM during training is proposed, with the objective of speeding up training over a regular LSTM.\n\nThe paper is well written and easy to understand in general. However, the main results of this paper are experimental, and I am not entirely convinced by the experiments that g-LSTM is an improvement over the LSTM baseline for certain scenarios.\n\nOne broad reason for my doubts is that the comparisons don’t seem to utilise proper hyperparameter tuning for the baseline LSTM. Network sizes, learning rates, decay schedules, initialisations etc. all appear to be fixed, so one can not be sure of the “real” performance or convergence behavior of the models. Biased gate initializations are not used, though they have been used successfully in past work to aid in long term memory.\n\nI should note that for long term memory problems such as those proposed by Hochreiter and Schmidhuber (1997), the proposed LSTM did not use a forget gate (or even BPTT) and used biased gate initialisations. However, these features are useful for more realistic tasks, and popular LSTM designs are biased towards them instead of toy problems.\n\nI would consider the addition problem and sequential MNIST and CIFAR-10 to be interesting and difficult toy tasks for initial validation of ideas (and more extensive hyperparameter searches). It is unclear if the proposed techniques will perform provide  improvements over a well-tuned baseline for some realistic tasks, or are they suitable only for toy problems. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Simple ideas but unconvincing results",
            "review": "The work takes inspiration from a recent work on phased LSTM, and proposes to add a Gaussian gate based on time to LSTM cells. With this additional gate, the network can skip updating the states by closing the time-gate, as a result enabling longer memory persistence, and better gradient flow. The authors also propose to add a budget term to force the time-gate to be closed most of the time as a way to save compute. Empirical results suggest the Gaussian-gated LSTMs perform better than regular LSTMs on tasks with long temporal dependencies. The authors also propose to use a curriculum training schedule in which the variance of the gaussian gates is continuously increased to speed up training of LSTMS.  \n\npros:\n1. The paper is clearly written and easy to follow;\n2. The way the authors introduce time-dependent gating into LSTM is easy to follow and re-implement;\n3. Experiments on various tasks of long temporal dependencies do show improvement over the standard LSTM cells;\n4. The experiments on the adding task does show gLSTM is less sensitive to initialization than the phased LSTM;\n5. The experiments on setting curriculum training schedule to improve convergence on LSTMs are insightful. \n\ncons:\n1. The work was framed as an easier-to-optimize alternative to the time-based gating mechanism introduced in phased LSTMs, which takes a parametrization form that is much harder to learn, the gating mechanism covered by the new model gLSTM however, is much more limited. The parametrization introduced in Phased LSTMs allows the memory cells and outputs of LSTMs to be updated periodically. gLSTMs on the other hand only allows updates within a single window over the entire sequence; As a result, one would expect phased LSTM to outperform gLSTM on tasks with periodical temporal dependencies;  \n2.  The empirical results are not convincing enough. gLSTM performs noticeably worse than several state-of-the-art work on improving long-term dependencies in RNNs. The authors did not give any explanation in the performance gap.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novel Contribution; Interesting speedup; too few self-critical; more evaluation needed",
            "review": "\nThis paper focuses on the reduction of training time by various mechanisms. By introducing a time gate during training, it controls when a neuron (weights?) can be updated during training. By introducing and additional budget term in the loss function, training costs (number of computations) are reduced by one order of magnitude. \nA major advantage of the newly introduced Gaussian-gated LSTM (g-LSTM; I suggest using a capital G for Gauss, e.g., GgLSTM).\n\nExperiments are carried out on the adding-problem from 1997; the sequential MNIST and the sequential CIFAR-10 problem. In all experiments, g-LSTM converges faster. A few things would be of interest:\n- clearly state the stopping criterium for training. Especially, I would still be interested to see, how Fig. 3d continues; it seems that the network begins to collapse (also a and be are interesting to see).\nThe \"This work\" in Table 2 is confusing; I would expect it to appear behind g-LSTM; \nIt appears that in the budgeted g-LSTM some units are not used at all (Figure 5b); Please comment on that.\n\nIn general, the paper makes the impression that it is overselling the contribution a bit too much. It would be nice to question the outcomes more and investigate the g-LSTM for the existence of possible problems which might be introduced by the omission of computations.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}