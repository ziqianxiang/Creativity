{
    "Decision": {
        "metareview": "This work provides two contributions: 1) Brain-Score, that quantifies how a given network's responses compare to responses from natural systems; 2) CORnet-S, an architecture trained to optimize Brain-Score, that performs well on Imagenet.\nAs noted by all reviewers, this work is interesting and shows a promising approach to quantifying how brain-like an architecture is, with the limitations inherent to the fact that there is a lot about natural visual processing that we don't fully understand. However, the work here starts from the premise that being more similar to current metrics of brain processes is by itself a good thing -- without a better understanding of what features of brain processing are responsible for good performance and which are mere by-products, this premise is not one that would appeal to most of ICLR audience. In fact, the best performing architectures on imagenet are not the best scoring for Brain-Score. Overall, this work is quite intriguing and well presented, but as pointed out by some reviewers, requires a \"leap of faith\" in matching signatures of brain processes that most of the ICLR audience is unlikely to be willing to take.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting take on quantifying similarity of networks to brain visual processing, unclear significance of that result for ICLR audiences"
    },
    "Reviews": [
        {
            "title": "Not enough analysis",
            "review": "Please consider this rubric when writing your review:\n1. Briefly establish your personal expertise in the field of the paper.\n2. Concisely summarize the contributions of the paper.\n3. Evaluate the quality and composition of the work.\n4. Place the work in context of prior work, and evaluate this work's novelty.\n5. Provide critique of each theorem or experiment that is relevant to your judgment of the paper's novelty and quality.\n6. Provide a summary judgment if the work is significant and of interest to the community.\n\n1. I am a researcher working at the intersection of machine learning and\nbiological vision.  I have experience with neural network models and\nvisual neurophysiology.\n\n2. This paper makes two contributions: 1) It develops Brain-Score - a\ndataset and error metric for animal visual single-cell recordings.  2)\nIt develops (and brain-scores) a new shallow(ish) recurrent network\nthat performs well on ImageNet and scores highly on brain-score. \n\n3. The development of Brain-Score is a useful invention for the field.  A nice\naspect of Brain-Score is that responses in both V4 and IT as well as behavioral\nresponses are provided.   I think it could be more useful if the temporal dynamics (instead of the\nmean number of spikes) was included.    This would allow to compare temporal\nresponses in order to compare \"brain-like\" matches.\n\n4. This general idea is somewhat similar to a June 2018 Arxiv paper\n(Task-Driven Convolutional Recurrent Models of the Visual System)\nhttps://arxiv.org/abs/1807.00053\nbut this is a novel contribution as it is uses the Brain-Score dataset.\n\nOne limitation of this approach relative to the June 2018 ArXiv paper\nis that the Brain-Score method is just representing the mean neural\nresponse to each image - The Arxiv paper shows that different models\ncan have different temporal responses that can also be used to decide\nwhich is a closer match to the brain.\n\n5. More analysis of why CORNET-S is best among compact models would greatly\nstrengthen this paper.  What do the receptive fields look like?  How do they compare\nto the other models.  What about the other high performing networks (e.g. DenseNet-169)?\nHow sensitive are the results to each type of weight in the network?   What about feedback connections\n(instead of local recurrent connections)? \n\n6. This paper makes a significant contribution, in part due to the\ndevelopment and open-sourcing of Brain-Score.  The significance of the\ncontribution of the CORnet-S architecture is limited by the\nlack of analysis into what aspects make it better than other models.\n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting attempt to bridge the gap between ML and neuroscience",
            "review": "In this interesting study, the authors propose a score (BrainScore) to (1) compare neural representations of an ANN trained on imagenet with primate neural activity in V4 and IT, and (2) test whether ANN and primate make the same mistakes on image classification.  They also create a shallow recurrent neural network (Cornet) that performs well according to their score and also reasonably well on imagenet classification task given its shallow architecture.\n\nThe analyses are rigorous and the idea of such a score as a tool for guiding neuroscientists building models of the visual system is novel and interesting.\n\nMajor drawbacks:\n\n1. Uncertain contribution to ML: it remains unclear whether architectures guided by the brain score will indeed generalize better to other tasks, as the authors suggest.\n\n2. Uncertain contribution to neuroscience: it remains unclear whether finding the ANN resembling the real visual system most among a collection of models will inform us about the inner working of the brain.\n\n\nThe article would also benefit from the following clarifications:\n\n3. Are the recurrent connections helping performance of Cornet on imagenet and/or on BrainScore?\n\n4. Did you find a correlation between the neural predictivity score and behavioral predictivity score across networks tested? If yes, it would be interesting to mention.\n\n5. When comparing neural predictivity score across models, is a model with more neurons artificially advantaged by the simple fact that there is more likely a linear combination of neurons that map to primate neural activations? Is cross-validation enough to control for this potential bias?\n\n6. Fig1: what are the gray dots?\n\n7. “but it also does not make any assumptions about significant differences in the scores, which would be present in ranking. “\nWhat does this mean?\n\n8. How does Cornet compare to this other recent work: https://arxiv.org/abs/1807.00053 (June 20 2018) ?\n\nConclusion:\nThis study presents an interesting attempt at bridging the gap between machine learning and neuroscience. Although the impact that this score will have in both ML and Neuroscience fields remains uncertain, the work is sufficiently novel and interesting to be published at ICLR. I am fairly confident in my evaluation as I work at the intersection of deep learning and neuroscience.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "promising work, further tests needed",
            "review": "This is an interesting paper in which the authors propose a shallow neural network, the architecture of which was optimized to maximize brain score, and show that it outperforms other shallow networks at imagenet classification. The goal of this paper — allowing brain data to improve our neural nets — is great. The paper is well written (except for a comment on clarity right below), and presents an interesting take on solving this problem. \n\nIt is a little unclear how the authors made CORnet optimize brain score: “However, note that CORnet-S was developed using Brain-Score as a guiding benchmark and although it was never directly used in model search or optimization, testing CORnet-S on Brain-Score is not a completely independent test.” Making these steps clearer is crucial for evaluating better what the model means. In the discussion “We have tested hundreds of architectures before finding CORnet-S circuitry and thus it is possible that the proposed circuits could have a strong relation to biological implementations.” implies that the authors trained models with different architectures until the brain score was maximized after training. A hundred(s) times of training on 2760 + 2400 datapoints are probably plenty to overfit the brainscore datasets. The brain score is probably compromised after this, and it would be hard to make claims about the results on the brain modeling side. The author acknowledge this limitation, but perhaps a better thing is to add an additional dataset, perhaps one from different animal recordings, or from human fMRI? \n\nArguably, the goal of the paper is to obtain a model that overpowers other simpler models, and not necessarily to make claims about the brain. The interesting part of the paper is that the shallow model does work better than other shallow models. The authors mention that brain score helps CORnet be better at generalizing to other datasets. Including these results would definitely strengthen the claim since both brain score and imagenet have been trained on hundreds of times so far. \n\nAnother way to show that brain score helps is to show it generalizes above or differently from optimizing other models. What would have happened if the authors stuck to a simple, shallow model and instead of optimizing brain score optimized performance (hundreds of times) on some selected image dataset (this selection dataset is separate from imagenet, but the actual training is done on imagenet) and then tested performance on imagenet? Is the effect due to the brain or to the independent testing on another dataset?\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}