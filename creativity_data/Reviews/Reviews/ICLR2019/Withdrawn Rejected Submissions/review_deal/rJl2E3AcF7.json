{
    "Decision": {
        "metareview": "This work proposes a new approximation method for softmax layers with large number of classes. The idea is to use a sparse two-layer mixture of experts. This approach successfully reduces the computation requires on the PTB and Wiki-2 datasets which have up to 32k classes. However, the reviewers argue that the work lacks relevant baselines such as D-softmax and adaptive-softmax. The authors argue that they focus on training and not inference and should do worse, but this should be substantiated in the paper by actual experimental results.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Meta-review"
    },
    "Reviews": [
        {
            "title": "Good empirical results, but only one baseline and poor writing.",
            "review": "The present paper proposes a fast approximation to the softmax computation when the number of classes is very large. This is typically a bottleneck in deep learning architectures. The approximation is a sparse two-layer mixture of experts.\n\nThe paper lacks rigor and the writing is of low quality, both in its clarity and its grammar. See a list of typos below.\n\nAn example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation. Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.\n\nAlgorithm 1 does not include mitosis, which may have an effect on the resulting approximation.\n\nHow are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?\n\nThe results only compare with Shim et al. Why only this method? Why would it be expected to be faster than all the other alternatives? Wouldn't similar alternatives like the sparsely gated MoE, D-softmax and adaptive-softmax have chances of being faster?\n\nThe column \"FLOPS\" in the result seems to measure the speedup, whereas the actual FLOPS should be less when the speed increases. Also, a \"1x\" label seems to be missing in for the full softmax, so that the reference is clearly specified.\n\nAll in all, the results show that the proposed method provides a significant speedup with respect to Shim et al., but it lacks comparison with other methods in the literature.\n\nA brief list of typos:\n\n\"Sparse Mixture of Sparse of Sparse Experts\"\n\"if we only search right answer\"\n\"it might also like appear\"\n\"which is to design to choose the right\"\nsparsly\n\"will only consists partial\"\n\"with Î³ is a lasso threshold\"\n\"an arbitrarily distance function\"\n\"each 10 sub classes are belonged to one\"\n\"is also needed to tune to achieve\"",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Need to discuss more about how Doubly Sparse is superior to Sparsely-Gated MoE",
            "review": "The paper proposes doubly sparse, which is a sparse mixture of sparse experts and learns a two-level class hierarchy, for efficient softmax inference.\n\n[+] It reduces computational cost compared to full softmax.\n[+] Ablation study is done for group lasso, expert lasso and load balancing, which help understand the effect of different components of the proposed\n[-] It seems to me the motivation is similar to that of Sparsely-Gated MoE (Shazeer et al. 2017), but it is not clear how the proposed two-hierarchy method is superior to the Sparsely-Gated MoE. It would be helpful the paper discuss more about this. Besides, in evaluation, the paper only compares Doubly Sparse with full softmax. Why not compare with Sparsely-Gated MoE?\n\nOverall, I think this paper is below the borderline of acceptance due to insufficient comparison with Sparsely-Gated MoE.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "New method for large scale softmax inference",
            "review": "In this paper the authors introduce a new technique for softmax inference. In a multiclass setting, the idea is to take the output of a NN and turn it into a gating function to choose one expert. Then, given the expert, output a particular category. The first level of sparsity comes from the first expert. The second level of sparsity comes from every expert only outputting a limited set of output categories.\n\nThe paper is easy to understand but several sections (starting from section 2) could use an english language review (e.g. \"search right\" -> \"search for the right\", \"predict next word\" -> \"predict the next word\", ...) In section 3, can you be more specific about the gains in training versus inference time? I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well. You motivate some of the work by the fact that the experts have overlapping outputs. Maybe in section 3.7 you can address how often that occurs as well?\n\nNits:\n- it wasn't clear how the sparsity percentage on page 3 was defined?\n- can you motivate why you are not using perplexity in section 3.2?\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}