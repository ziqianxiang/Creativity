{
    "Decision": {
        "metareview": "The reviewers agree that the idea for dataset distillation is novel, however it is unclear how practical it can be. The paper has been significantly improved through the addition of new baselines, however ultimately the performance is not quite good enough for the reviewers to advocate strongly on its behalf. Perhaps the paper would be better motivated by finding a realistic scenario in which it would make sense for someone to use this approach over reasonable alternatives.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Original idea, but lacks a practical use case."
    },
    "Reviews": [
        {
            "title": "An algorithm to reduce datatset size for  NNs",
            "review": "The paper presents an algorithm for compressing the size of entire training data into a few synthetic training samples. The method is based on neural networks and is applied on image datsets. The authors comment two possible applications of their method domain adaptation and effective data poisoning attack.\n\nThe proposed technique seems to be limited to neural networks since it seems that is linked to the initialization of the networks. In this aspect, it could be interesting to have a more general method.\n\nThere are related works that are not commented, for instance :\n\nOlvera-LÃ³pez, J. Arturo, et al. \"A review of instance selection methods.\" Artificial Intelligence Review 34.2 (2010): 133-143.\n\n\nExperimental section is weak. Few datasets are considered, other problems should be added. Additionally, related methods should be included to  compare the performance of the proposal. Some comments about the computational cost should be inserted. In this aspect, the experimental section should be improved following these recommendations.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Original problem and well written paper, but that lacks comparisons to baselines",
            "review": "\n=== Post rebuttal update ===\n\nThanks to the addition of better baselines, I've increased my score for this paper. While I'm still not super convinced of its potential for application, I find the idea original and worth discussing at the conference.\n\n=== Pre-rebuttal review ===\nThis paper presents an approach to compress a dataset into a much smaller number of synthetic samples that are optimized to yield as good performance as possible when a given model is trained on that smaller dataset. This is done by unrolling the gradient descent procedure of training such a model to allow for gradient-based optimization of synthetic samples themselves as well as the used learning rates.\n\nIn summary, my evaluation is as follow:\n\n*Pros*\n- Pretty original problem formulation\n- Generally well written paper\n\n*Cons*\n- Lack of comparison with simple baselines in basic dataset distillation setting\n- Use in practical applications (domain adaptation, data poisoning) yet to be convincingly demonstrated\n- Possibly a mistake in the theoretical analysis of the linear case\n\nIndeed, I found the paper to be generally quite clear and enjoyed reading it. One minor thing I struggled a bit with is the distinction between \"SG steps\" and \"Epochs\" (I believe the former corresponds to when the synthetic samples are different between GD steps, whereas the later corresponds to the number of times the method repeatedly cycles over these samples) so I would perhaps encourage the authors to emphasize that difference. \n\nI also find the problem statement that proposed to be interesting and thought provoking, and the solution that's proposed seems quite appropriate and well thought out.\n\nThat said, I'm worried about the following:\n\n- Unless I misunderstood, in the basic dataset distillation setting a comparison is never provided with training on a randomly selected subset of the training set. Presumably the results are worse, but I think these results should be in the paper. I would also argue for having another baseline, which would try to (approximately) optimize the choice of which training examples are put in the subset. A very simple approach would be to take the 200 runs already performed for the random selection and select the subset providing the best accuracy on the full training set and only report the performance of that subset (instead of the mean and std of all 200 runs). In short, this would help determine to what extent there is value to synthesizing entirely new samples. Moreover, I think a simple alternative baseline for creating synthesized samples should be considered. Specifically, I'd personally would like to know the performance of using per-class k-means clustering and training on the cluster centroids as the distilled dataset.\n\n- While I appreciate that the authors identify potential applications and report some results on them, I think they currently fall short of convincing the reader of the potential of dataset distillation for these applications. For domain adaptation, no actual domain adaptation baseline is compared against (a good candidate would be method from Daume III 2007, at the very least). For data poisoning, I find that the assumptions for attacks are pretty strong, i.e. that a) you have access to the parameters of the pre-trained models to attack and b) that the model is doing additional updates *only* on the synthesized data. If there are reasons to think that such assumptions are reasonable, I'd at least expect the paper to motivate why that is. \n\n- In the analysis of the simple linear case, in Equation 7, there appears to be a mistake, specifically some missing parentheses:\n\nd^Td( (I-\\eta/M d^Td)\\theta_0 + \\eta/M \\tilde{d}^T\\tilde{t}) = d^T t\n\ni.e. there should be parentheses right after \"d^Td\" and right before \"=\". This is from replacing \\theta^* by the expression for \\theta_1 in Equation 6, which is what I think Equation 7 is supposed to be doing. This possibly doesn't affect some of the conclusions taken from this section, but I'd like to see this potential mistake discussed/addressed.\n\n\nThat said, if the authors can sufficiently address the 3 points above, I'd be willing to increase my rating for this paper.\n\nFinally, I have a few other more minor (nice-to-have) points:\n- Having in the related work a discussion on the relationship with coreset methods would be nice\n- Experiments showing how well the distilled datasets transfer to different network architectures than those used in training would be interesting? Even other ML algorithms would be quite interesting?\n- \"We often find that the number of distilled images required to achieve good performance is an informative indicator of the dataset diversity\" => I'm not sure what in the paper actually justifies / demonstrates this statement.\n- Figure 4 is presented as an \"Ablation study\", but an ablation study is where you remove certains parts of a model or algorithm and see what happens, which isn't the case here. I think it's better described as a hyper-parameter sensitivity study. \n- Some typos:\n   * the below objective => the objective below\n   * w.r.t. to => w.r.t\n   * the discrete part rather => the discrete parts rather\n   * necesary => necessary\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach, not fully mature yet",
            "review": "The paper addresses the interesting problem of generating a small number of synthetic examples that can be used to train a classifier, replacing a larger dataset. \n\nThe paper is clearly written, the approach makes sense, and the experiments are interesting. \nMy major concerns are regarding to previous literature, analysis of the algorithm, and details of the experiment. Overall, I expect an ICLR paper to go deeper (rather than wide). I recommend presenting strong convincing evidence on one front. \n\n\nSpecific comments: \n(1)  I'm missing analysis of the proposed procedure. It wasn't fully clear which Loss it minimizes and if it indeed guaranteed to converge to the minimum of that loss. \n\n(2)  The topic of learning from few samples is presented as completely new. It is well known that for classical linear algorithms like the Perceptron and SVM, the weights are a weighted sum of (label-weighted) samples, hence by definition of these algorithm, there is a single sample that can be used to \"train\" the model in one step. I'd expect some discussion of how the proposed approach relate to these classical approaches. \nThere is also existing literature on a related problem of selecting samples (Teaching dimension Goldman&Kearns) that could be somewhat relevant here. \n\n(3) Motivation. The paper provide several motivations for dataset distillation. I support the first motivation of scientific understanding what data is actually needed for a classifier, and this means that deeper analysis is needed. The practical motivations are less convincing, because (a) domain adaptation experiments are not compared with real baselines (b) robustness of poisoning with a single sample is not studied/discussed.\n\n(4) experiments: The intro states that training with 10 images reaches 94% accuracy, but this does not seem consistent with the results in Table 1. The caption of figure 2 suggests that accuracy is between 12% and 94% which means the stated 94% is not representative or typical. Could you clarify? \nFor domain adaptation. The baseline (random images) are very weak, and still perform almost   comparably to the proposed approach. More robust experiments are needed here: stronger baselines, decent hyper-parameter search etc.\n\n(5) Writing and exposition: The paper addresses two issues: (a) learning with few synthetic samples, and (b) learning with few gradient steps. The intro tends to mix the two, and it is not clear why learning with a single gradient step is important. I recommend to separate the two topics more clearly. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}