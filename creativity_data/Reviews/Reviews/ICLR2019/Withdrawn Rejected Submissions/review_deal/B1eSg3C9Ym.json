{
    "Decision": {
        "metareview": "This paper presents a mean field analysis of the effect of batch norm on optimization. Assuming the weights and biases are independent Gaussians (an assumption that's led to other interesting analysis), they propagate various statistics through the network, which lets them derive the maximum eigenvalue of the Fisher information matrix. This determines the maximum learning rate at which learning is stable. The finding is that batch norm allows larger learning rates.\n\nIn terms of novelty, the paper builds on the analysis of Karakida et al. (2018). The derivations are mostly mechanical, though there's probably still sufficient novelty.\n\nUnfortunately, it's not clear what we learn at the end of the day. The maximum learning rate isn't very meaningful to analyze, since the learning rate is only meaningful relative to the scale of the weights and gradients, and the distance that needs to be moved to reach the optimum. The authors claim that a \"higher learning rate leads to faster convergence\", but this seems false, and at the very least would need more justification. It's well-known that batch norm rescales the norm of the gradients inversely to the norm of the weights; hence, if the weight norm is larger than 1, BN will reduce the gradient norm and hence increase the maximum learning rate. But this isn't a very interesting effect from an optimization perspective. I can't tell from the analysis whether there's a more meaningful sense in which BN speeds up convergence. The condition number might be more relevant from a convergence perspective.\n\nOverall, this paper is a promising start, but needs more work before it's ready for publication at ICLR.\n\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "a promising start, but the analysis is mechanical and the maximum learning rate isn't inherently meaningful"
    },
    "Reviews": [
        {
            "title": "Interesting paper",
            "review": "This paper studies the effect of batch normalization via a physics style mean-field theory. The theory yields a prediction of maximal learning rate for fully-connected and convolutional networks, and experimentally the max learning rate agrees very well with the theoretical prediction.\n\nThis is a well-written paper with a clean, novel result: when we fix the BatchNorm parameter \\gamma, a smaller \\gamma stabilizes the training better (allowing a greater range of learning rates). Though in practice the BatchNorm parameters are also trained, this result may suggest using a smaller initialization. \n\nA couple of things I was wondering:\n\n-- As a baseline, how would the max learning rate behave without BatchNorm? Would the theories again match the experimental result there?\n\n-- Is the presence of momentum important? If I set the momentum to be zero, it does not change the theory about the Fisher information and only affects the dependence of \\eta on the Fisher information. In this case would the theory still match the experiments?",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting application of MFT on FIM to understand Batch Normalization",
            "review": "Interesting application of MFT on FIM to understand Batch Normalization\n\nThis paper applies mean field analysis to networks with batch normalization layers. Analyzing maximum eigenvalue of the Fisher Information Matrix, the authors provide theoretical evidence of allowing higher learning rates and faster convergence of networks with batch normalization. \n\nThe analysis reduces to providing lower bound for maximum eigenvalue of FIM using mean-field approximation. Authors provide lower bound of the maximum eigenvalue in the case of fully-connected and convolutional networks with batch normalization layers. Lastly authors observe empirical correlation between smaller \\gamma and lower test loss. \n\nPro: \n - Clear result providing theoretical ground for commonly observed effects. \n - Experiments are simple but illustrative. It is quite surprising how well the maximum learning rate prediction matches with actual training performance curve. \n\t\n\nCon:\n - While mean field analysis a-priori works in the limit where networks width goes to infinity for fixed dataset size, the analysis of Fisher and Batch normalization need asymptotic limit of dataset size. \n - Although some interesting results are provided. The content could be expanded further for conference submission. The prediction on maximum learning rate is interesting and the concrete result from mean field analysis\n - While correlation between batch norm \\gamma parameter and test loss is also interesting, the provided theory does not seem to provide good intuition about the phenomenon. \n\nComments:\n- The theory provides the means to compute lower bound of maximum eigenvalue of FIM using mean-field theory. In Figure 1, is \\bar \\lambda_{max} computed using the theory or empirically computed on the actual network? It would be nice to make this clear. \n- In Figure 2, the observed \\eta_*/2 of dark bands in heatmap is interesting. While most of networks without Batch Norm, performance is maximized using learning rates very close to maximal value, often networks using batch norm the learning rate with maximal performance is not the maximal one and it would be interesting to provide theoretical \n- I feel like section 3.2 should cite Xiao et al (2018). Although this paper is cited in the intro, the mean field analysis of convolutional layers was first worked out in this paper and should be credited. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Theoretical but not rigorous",
            "review": "In this paper, the effect of batch normalization to the maximum eigenvalue of the Fisher information is analyzed. The techinique is mostly developed by Karakida et al. (2018). The main result is an informal bound of the maximum eigenvalue, which is given without proof. Though, the numerical result corresponds to the derived bound.\n\nThe paper is basically well written, but the technical part has several notational problems. For example, there is no definition of \"\\otimes\", \"\\odot\", and \"Hess\" operators.\n\nThe use of the mean-field theory is an interesting direction to analyze batch normalization. However, in this paper, it seems failed to say some rigorous conclusion. Indeed, all of the theoretical outcomes are written as \"Claims\" and no formal proof is given. Also, there is no clear explanation of why the authors give the results in a non-rigorous way, where is the difficult part to analyze in a rigorous way, etc. \n\nAside from the rigor issue, the paper heavily depends on the study of Karakida et al. (2018). The derivation of the bound (44) is directly built on Karakida's results such as Eqs. (7,8,20--22), which reduces the paper's originality.\n\nThe paper also lacks practical value. Can we improve an algorithm or something by using the bound (44) or other results?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}