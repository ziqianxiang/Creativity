{
    "Decision": {
        "metareview": "This paper analyses the dynamics of RNNs, cq GRU and LSTM.  \n\nThe paper is mostly experimental w.r.t. the difficulty of training RNNs; this is also caused by the fact that the theoretical foundations of the paper seem not to be solid enough.  Experimentation with CIFAR10 is not completely stable.\n\nThe review results make the paper balance at the middle.  The merit of the paper for the greater community is doubted, in its current form.",
        "confidence": "2: The area chair is not sure",
        "recommendation": "Reject",
        "title": "borderline"
    },
    "Reviews": [
        {
            "title": "understanding memorization vs processing across two types of curricula in RNNs",
            "review": "This manuscript attempts to use a delayed classification task to understand the dynamics of RNNs.  The hope is to use this paradigm to distinguish memorization from processing in RNNs, and to further probe when specific curricula (VoCu, DeCu) outperform each other, and what can be inferred from that.   \n\nQuality: \n- The experimental design is sensible.  However, it is rather too much a toy example, and too narrow, hence it is unclear how much these results can be generalized across RNNs\n- Highly problematic is that the key concepts in the paper -- memorization and processing -- are not well defined.  This means that the results inevitably are just interpretations rather than any sort of compelling empiricism.  After a careful read of the paper, I found it difficult to take away any particular learnings, other than \"training RNNs is hard.\" \n\nClarity:\n- The paper is fairly straightforward, which is positive.\n- The lack of clarity around particular definitions means that clarity is limited to the empirical results.  If the results are incredibly compelling, that would be acceptable, but absent that (as is the case here), the paper comes across to me as rather unclear in its purpose or its takeaway message.\n\nOriginality: \n- The Barak 2013 paper seems to be the key foundation for this work.  This work is sufficiently original beyond that paper.\n\nSignificance: \n- The combination of lack of clarity and limited results on a toy setting imply that the significance is rather too low.\n\nOverall, this is a genuine effort to explore the dynamics of RNNs.  I suggest improvements can be made by either (1) working hard to clarify in the text *exactly* what question is being asked and answered, or (2) broadening the results to make a much more rigorously supported point, or (3) ideally both.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Learned memory structure differs due to training paradigm",
            "review": "This paper titled <don't judge a book by its cover - on the dynamics of recurrent neural networks> studies how different curriculum learning results in different hidden state dynamics and impacts extrapolation capabilities. By training a 200-GRU-RNN to report the class label of a MNIST frame hidden among noisy frames, authors found different training paradigms resulted in different stability in memory structures quantified as stable fixed points. Their main finding is that training by slowly increasing the time delay between stimulus and recall creates more stable fixed point based memory for the classes.\n\nAlthough the paper was clearly written in a rush, I enjoyed reading it for the most part. These are very interesting empirical findings, and I can't wait to see how well it generalizes.\n\n# I find the title not very informative. Connection from 'Book' to 'Curriculum' is weak.\n\n# The task does not have inherent structure that requires stable fixed points to solve. In fact, since it only requires maximum 19 time frames, it could come up with weird strategies. Since the GRU-RNN is highly flexible, there would be many solutions. The particular strategy that was learned depends on the initial network and training strategy.\n\n# How repeatable were these findings? I do not see any error bars in Fig 2 nor table 1.\n\n# How sensitive is this to the initial conditions? If you use the VoCu trained network as initial condition for a DeCu training, does it tighten the sloppy memory structure and make it more stable?\n\n# I liked the Fig 2b manipulation to inject noise into the hidden states.\n\n# English can be improved in many places.\n\n# Algorithm 1 is not really a pseudo-code. I think it might be better to just describe it in words. This format is unnecessarily confusing and hard to understand.\n\n# Does the backtracking fixed/slow point algorithm assume that the location of the fixed point does not change through training? Wouldn't it make more sense to investigate the pack-projection of desired output at each training step?\n\n# PTMT, PMTP, and TaCu are not described well in the main text.\n\n# The pharse 'basin of attraction' is losely used in a couple of places. If there isn't an attractor, its basin doesn't make sense.\n\n# Fig 4 is not very informative. Also is this just from one network each?\n\n# Fig 5 is too small!\n\n# page 2: input a null label -> output a null label\n\n# it would be interesting to see how general those findings are on other tasks, e.g., n-back task with MNIST.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An intriguing but preliminary investigation into RNN dynamics and generalisation",
            "review": "Post-rebuttal update:\nThe authors have clarified their main messages, and the paper is now less vague about what is being investigated and the conclusions of the experiments. The same experimental setup has been extended to use CIFAR-10 as an additional, more realistic dataset, the use of potentially more powerful LSTMs as well as GRUs, and several runs to have more statistically significant results - which addresses my main concerns with this paper originally (I would have liked to see a different experimental setup as well to see how generalisable these findings are, but the current level is satisfying). Indeed, these different settings have turned up a bit of an anomaly with the GRU on CIFAR-10, which the authors claim that they will leave for future work, but I would very much like to see addressed in the final version of this paper. In addition some of the later analysis has only been applied under one setting, and it would make sense to replicate this for the other settings (extra results would have to fit into the supplementary material).\n\nI did spot one typo on page 4 - \"exterme\", but overall the paper is also better written, which helps a lot. I commend the authors on their work revising this paper and will be upgrading my rating to accept.\n\n---\n\nThe authors investigate the hidden state dynamics of RNNs trained on a single task that mixes (but clearly separates) pattern recognition and memorisation. The authors then introduce two curricula specific to the task, and study how the trained RNNs behave under different deviations from the training protocol (generalisation). They show that under the curriculum that exhibited the best generalisation, there exist more robust (persisting for long time periods) fixed/slow points in the hidden state dynamics. They then extend the optimisation procedure developed by Sussillo & Barak for continuous-time RNNs in order to find these points. Finally, they use this method to track the speed of these points during the course of training, and link spikes in speed to one of the curricula which introduces new classes over time.\n\nUnderstanding RNNs - and in particular how they might \"generalise\" - is an important topic of research. As done previously, studying RNNs as dynamical systems is a principled way to do so. In this line of work some natural objects to look into are fixed points and even slow points (Sussillo & Barak) - how long they can persist, and how large the basins of attraction are. While I believe the authors did a reasonable job following this through, I have some concerns about the experimental setup. Firstly, only one task is used - based on object classification with images - so it is unclear how generalisable these findings are, given that the authors' setup could be extended to cover at least another task, or at least another dataset. MNIST is a sanity check, and many ideas may fail to hold when extended to slightly more challenging datasets like CIFAR-10.\n\nSecondly, as far as I can tell, the results are analysed on one network per setting, so it is hard to tell how significant the differences are. While some analyses may only make sense for single networks, e.g. Figure 3, a proper quantification of some of the results over several training runs would be appropriate.\n\nFinally, it is worth investigating LSTMs on this task. This is not merely because they are more commonly used than GRUs, but they are strictly more powerful - see \"On the Practical Computational Power of Finite Precision RNNs for Language Recognition\", published at ACL 2018. Given the results in this paper and actually the paper that first introduces the forget gate for LSTMs, it seems that performing these experiments solely with GRUs might lead to wrong conclusions about RNNs in general.\n\nThere are also more minor spelling and grammatical errors throughout the text that should be addressed. For example, there is a typo on the task definition on page 2 - \"the network should *output* a null label.\"",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}