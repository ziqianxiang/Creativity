{
    "Decision": {
        "metareview": "Strengths:  The paper presents an alternative regularized training objective for supervised learning that has a reasonable theoretical justification.  It also has a simple computational formula.\n\nWeaknesses:\nThe experiments are minimal proofs of concept on MNIST and fashion MNIST, and the authors didn't find an example where this formulation makes a large difference.  The resulting formula is very close to existing methods.  Finally the paper is a bit dense and the intuitions we should gain from this theory aren't made clear.\n\nPoints of contention:\nOne reviewer pointed out the close connection of the new objective to IWAE, and the authors added a discussion of the relation and showed that they're not mathematically equivalent.  However, as far as I can tell they're almost identical in purpose:  As k -> \\infty in IWAE, the encoder ceases to matter.  And as M -> \\infty in VDB, we take the max over all encoders.  Could the method proposed in this paper lead to an alternative to IWAE in the VAE setting?\n\nConsensus:\nConsensus wasn't reached, but the \"7\" reviewer did not appear to have put much though into their review.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Valid theory, but quite close to existing work"
    },
    "Reviews": [
        {
            "title": "New representation learning objective using Chanel Deficiency ",
            "review": "This paper used the concept based on channel deficiency to derive a variational bound similar to variational information bottleneck. Theoretical analysis shows that this bound is an lower bound on the VIB objective. The empirical analysis shows it outperforms VIB in some sense. \n\nI think this paper's contribution is rather theoretical than practical. The experiments section can be improved in the following aspect:\n-  Figure 2 are hard to read for different M's. It would be better if the authors can show the exact accuracy numbers rather than the overlapped lines\n- I(Z;Y) vs I(Z;X) graph is typically used in a VIB setting. In the paper's variational deficiency setting, although plotting I(Z;Y) vs I(Z;X) is necessary, it would be also helpful for the authors' to plot Deficiency vs I(Z;X), because this is what new objective is trading-off. \n- Again, Figure 3, it is hard to see the benefits for increasing M from the visualizations for different clusterings. \n- How do the paper estimate I(Z;Y) and I(Z;X) for plotting these figures? Does the paper use lower bound or some estimators? It should be made clear in the paper since these are non-trivial estimations.\n\nLast comment is that, although the concept of `deficiency` in a bottleneck setting is novel, the similar idea for tighter bound of log likelihood has already been pursed in the following paper:\n\n- Yuri Burda, Roger Grosse, Ruslan Salakhutdinov. Importance Weighted Autoencoders. ICLR 2016\n\nIt was kind of surprising that the authors did not cite this paper given the results are pretty much the same. It would also be helpful for the authors to do a comparison or connection section with this paper. \n\nI like the paper in general, but given it still has some space for improvement, I would keep my decision as boarder line for now.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The paper presents a method of learning representations that is based on minimizing \"deficiency\" rather than optimizing for information sufficiency.",
            "review": "The paper presents a method of learning representations that is based on minimizing \"deficiency\" rather than optimizing for information sufficiency. While perfect optimization of the sufficiency term in IB is equivalent to minimizing deficiency, the thesis of the paper is that the variational upper bound on deficiency is easier to optimize, and when optimized produces\nbetter (more compressed representations), while performing equally on test accuracy.\n\n\n\nThe paper is well written and easy to read. The idea behind the paper (optimizing for minimizing deficiency instead of sufficiency in IB) is interesting, especially because the variational formulation of DB is a generalization of VIB (in that VIB reduces to VDB for M=1). What takes away from the paper is that while perfect optimization of IB/sufficiency is equivalent to perfect optimization of DB, it is not clear what happens when perfection is not achieved. Further, the authors claim that DB is able to obtain more compressed representations (But is the goal a compressed representation, or an informative one?). The paper would also benefit from evaluation of the representation itself, and comparison to other non-information bottleneck based algorithms.\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Good Writing, Comparisons Needed. ",
            "review": "This paper introduces deficiency bottleneck for learning a data representation and represent  complicated channels using simpler ones. This problem has a natural variational form that can be easily implemented from VIB. Experiments show good performance comparing to VIB. \n\nThis paper is well-written and easy to read. The idea using KL divergence creating a deficiency channel to learn data representation is very natural. It is interesting that this formulation could be understood as minimizing a regularized risk gap of statistical decision problems, which justifies the usage of deficiency bottleneck (eq.9). \n\nMy biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem. However, it looks like authors only compared with VIB which is similar to the proposed method in terms of the objective function. For example, how does the method compare with (variants of) Variational Autoencoder? A discussion on this or some empirical evaluations would be nice. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}