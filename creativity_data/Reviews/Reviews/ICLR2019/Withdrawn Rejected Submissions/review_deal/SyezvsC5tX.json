{
    "Decision": {
        "metareview": "The paper proves that the locus of the global minima of an over-parameterized neural nets objective forms a low-dimensional manifold. The reviewers and AC note the following potential weaknesses:  \n--- it's not clear why the proved result is significant: it neither implies the SGD can find a global minimum, nor that the found solution can generalize. (Very likely, most of the global minima on the manifold cannot generalize.) \n--- the results seem very intuitive and are a straightforward application of certain topological theorem.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "Interesting but contribution is minor and doesn't meet the standards. Would recommend rejection.",
            "review": "This paper considers over-parametrized neural networks (n > d), where n is the number of parameters and d is the number of data, in the regression setting. It consists of three main results:\n\na) (Theorem 2.1) If the output of the network is a smooth function of its parameters and global minimum with zero squared loss is attainable, then the locus of global minima is a smooth $n - d$ dimensional submanifold of R^n.\nb) (Lemma 3.3) A neural network with a hidden layer as wide as the number of data points can attain global minima with zero loss.\nc) (Theorem 4.1) If a neural network has a hidden layer as wide as the number of data points and is a smooth function of its parameters, then the locus of global minima with loss zero is a smooth $n - d$ dimensional submanifold. This is just a combination of a) and b).\n\nI think the only contribution of this paper is Theorem 2.1. It is already well-known (e.g., Zhang et al. 16’) that a network with a hidden layer as wide as the number of data can easily attain zero empirical error. The authors claim that it is an extension over ReLU, but the extension is almost trivial. Theorem 4.1 is just a corollary from Theorem 2.1 and Lemma 3.3.\n\nTheorem 2.1 is an interesting observation, and its implication that the Hessian at minima has only d positive eigenvalues is interesting and explains empirical observations. \n\nFrom my understanding, however, the n - d dimensional submanifold argument can be applied to any regular values of H(w,b) (not necessarily global minima), so the theorem is basically saying there are a lot of equivalence classes in the parameter space that outputs exactly the same outcome. If my understanding is correct, this result does not give us any insight into why SGD can find global minima easily, because we can apply the same proof to spurious local minima and say, “look, the locus of this spurious local minimum is a high-dimensional submanifold, so it is easy to get stuck at poor local minima.”\n\nMoreover, the notation used in this paper is not commonly used in deep learning community, thus might confuse the readers. $n$ is more commonly used for the number of data points and $d$ is used for some dimension. Also, there are overloading uses of $p$: \n* In Section 2.1 it is used for the dimension of the input space\n* In the proof of Prop 2.3 it is now a vector in R^n\n* In section 3 it is now a parameter vector of a neural network\n* Back to the input dimension in Lemma 3.3 and Section 4\n\nAlso, the formulation of neural networks in Section 3 is very non-standard. If you are to end up showing some result on standard fully-connected feed-forward networks, why not just use, for example, $W_2 \\sigma(W_1 x_i + b_1) + b_2$? To me, this graph theoretical formulation only adds confusion. Also at the end of page 4, there is a typo: the activation function (\\sigma) must be applied after the summation.\n\nOverall, I believe this paper does not meet the standards of ICLR at the moment. I would recommend rejection.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The results from the paper are sort of known in previous literature, yet the proof in the paper was still smart and innovative. ",
            "review": "This paper gave an interesting theoretical result that the global minima of an overparameterized neural network is a high-dimensional sub-manifold. This result is particularly meaningful as it connected several previous observations about neural networks and a indirect evidence for why overparameterization for deep learning has been so helpful empirically.\n\nThe proof in the paper was smart and the rough logic was quite easy to follow. The minor issue is that the proof in the paper was too sketchy to be strict. For example, in proof for Thm 2.1, the original statement about Sard’s theorem was about the critical values, but the usage of this theorem in the proof was a little indirect. I can roughly see how the logic can go through, but I still hope the author can give more detailed explaining about this part to make the proof more readable and strict.\n\nOverall, I think the result in this paper should be enough to justify a publication. However, there’re still limitations of the result here. For example, the result only explained about the fitting on training data but cannot explain at all why overfitting is not a concern here. It also didn’t explain why stochastic gradient descent can find these minima empirically. In particular, even though the minima manifold is n-d dimensional, it’s still a zero-measure set which will almost never get hit with a random initialization. Of course, these are harder questions to explore, but maybe worthy some discussion in the final revision.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A short and concise theoretical paper, but I find the contribution limited",
            "review": "The paper shows that the set of global minimums of an overparametrized network with smooth activation function is almost surely a high dimensional manifold. In particular, the dimension of this manifold is exactly n-d, where n is the number of parameters and d is the number of datas. To the best of my knowledge, this is the first proper proof of such non-surprising result.  \n\nThe theoretical analysis is a straightforward combination of neural network's expressiveness result and some classical theorems in the field of differential topology. However, the assumption on the overparametrized neural network is somehow unrealistic since it requires that at least one layer has no less than d neurons, which is as many as the number of datas. This is usually not the case. Moreover, the result is to some extent \"asymptotic\", in the sense that a small perturbation in terms of data may be required in order to make the statement holds.  \n\nMore importantly, the result does not provide any useful characterization distinguishing stationary point/local minimum versus global minimum. It might be possible that the set of stationary points is also a manifold with very high dimension, which is indeed supported by the argument listed in the bottom of page 7: \"the Hessian of the loss function tended to have many zero eigenvalues, some positive eigenvalues, and a small number of negative eigenvalues\". It is possibly the case that the set of stationary points has even higher dimension. This suggests that the dimensionality itself is not right indicator, but the difference in terms of dimension between different type of stationary points that matters. \n\nOverall, the paper is short and concise but I find the contribution a bit limited.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}