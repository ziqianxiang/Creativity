{
    "Decision": {
        "metareview": "This paper proposes an anomaly-detection approach by augmenting VAE encoder with a network multiple hypothesis network and then using a discriminator in the decoder to select one of the hypothesis. The idea is interesting although the reviewers found the paper to be poorly written and the approach to be a bit confusing and complicated.\n\nRevisions and rebuttal have certainly helped to improve the quality of the work. However, the reviewers believe that the paper require more work before it can be accepted at ICLR. For this reason, I recommend to reject this paper in its current state.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting approach, but poorly written. Needs to more work before acceptance."
    },
    "Reviews": [
        {
            "title": "Does not read well, no clear motivation",
            "review": "This paper proposes an anomaly detection system by proposing the combination of multiple-hypotheses approach with variational autoencoders, and using a discriminator to prevent either head of the model to produce modes that are not part of the data.\n\nThe combination between multiple-hypotheses approach with variational autoencoders seems rather artificial to me. Why do we need to parameterized a fixed set of hypothesis if we can generate as many outputs as we want just by sample several times from the prior of the VAE? Maybe I am missing something, which brings me to the following point.\n\nThe paper is difficult to read: the motivation is not well explained, the link between anomaly detection and multiple-hypothesis methods (both in the title of the paper) is not clear. The approach seems to build on top of Breunig et al. (2000), unfortunately this paper is not well described, e.g. what does it mean global neighborhood?\nThere are many other sentences in the paper that I find difficult to understand, for example:\n\"Lfake itself consists of assessment for noise- (xˆz∼N(0,1)) and data-conditioned (xˆz∼N(µz|x,Σz|x)) hypotheses and the best guess given by the WTA objective.\"\n\nOn top of that there are many other elements in the paper hampering the comprehension of the reader. For example:\nWTA is used without being defined before (winner takes all)\none-to-mapping --> one-to-one mapping?\nL_[Hyps] is the same as L_[WTA]?\nMDN is not defined until Sec. 5, and doing so without giving any description about it.\nTable 3 is never referred to.\nIs Table 5 reporting results on the Metal anomaly dataset? If so please mention it in the caption.\n\nIn the experiments it is difficult to see which parts of the models make the main difference. For example, it would be interesting to have an ablation experiment assessing the importance of the discriminator.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Technique to make GANs more robust by adding MHP-based regularization so that detection of out-of-distribution samples can be improved",
            "review": "Summary\n-------\nThe paper proposes a technique to make generative models more robust by making them consistent with the local density. It is hypothesized that robust model will be able to detect out-of-distribution samples better and improve anomaly detection.\n\nMain comments\n-------------\n\n1. The proposed technique adds additional regularizers to the GAN loss that, in effect, state that the best hypothesis under a WTA strategy should have a high likelihood under the discriminator 'D'. This is an interesting idea and certainly a reasonable thing to try. As stated in the abstract, the generative models are inefficient; it is likely that additional structure enforced by the regularizer helps in improving the efficiency.\n\n2. The objective in GANs is to infer the underlying distribution correctly and so far it has been found that their accuracy is heavily dependent on both the architecture as well as the computational complexity (they may improve with more training, but maybe not consistently). Therefore, it becomes hard to compare the three architectures in Figure 2 since they are all different. A more rigorous comparison would try to keep as many pieces of the architecture the same as possible so that ConAD can be compared with 'all other things being same'. Some experiments seem to follow this idea such as 'MDN+ConAD-{2, 4, 8, 16}' in Table 2. But in these experiments the addition of ConAD offers a mild improvement and even degrades for the maximum number of hypothesis (i.e., 16).\n\n3. Page 2, para 2, last two lines: \"For simplicity, imagine an ... the real distribution.\"\n\nThe argument is not clear. It seems too trivial and almost like a straw man argument.\n\n4. Page 4: \"In anomaly detection, this is difficult since there is no anomalous data point contained in the training dataset.\"\n\nThis is not true in real-world applications where most data is contaminated with anomalies. This is part of the challenge in anomaly detection.\n\nThe above also applies to the following on page 6: \"During model training, only data from the normal data class is used...\"\n\n5. Page 5: \"...D minimizes Eq. 3\": Should be 'maximizes' since the reference is to the log likelihood of real data (or, add a negative sign).\n\n6. Eq. 4: The last component should be negative since we trying to maximize the likelihood of the best hypothesis under WTA (right?).\n\n7. Table 1: The datasets are not real anomaly detection datasets (too high proportion of 'anomalies') Moreover, the number of datasets is insufficient for rigor.\n\n8. Section 5.4: \"With our framework ConAD, anomaly detection performance remains competitive or better even with an increasing number of hypotheses available.\"\n\nSection 6: \"... and alleviates performance breakdown when the number of hypotheses is increased.\"\n\nThis is not entirely supported by the results in Tables 2, 3, and also 4 and 5 of supplement. The results for ConAD - {2, 4, 8, 16} are not consistently increasing.\n\nSince experiments are very few (and not real-world for anomaly detection task) because of which the observations cannot be generalized.\n\n9. Page 4 (minor) in two places: \"one-to-mapping\" -> \"one-to-many mapping\"\n\n10. Page 5 (minor): \"chap. 3\" -> \"section 3\"\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Confusing paper, some good ideas",
            "review": "The paper proposes a new method for anomaly detection using deep learning. It works as follows. \n\nThe method is based on the recent Multiple-Hypotheses predictions (MHP) model, the impact of which is yet unclear/questionable. The idea in MHP is to represent the data using multiple models. Depending on the part of the space where an instance falls, a different model is active. In this paper this is realized using VAEs. The details are unclear (the paper is poorly written and lacks some detailed explainations), but I am assuming that for each hypothesis (ie region of the space) different en- and decoder parameters are learned (sharing the same variational prior??). The authors mention that below this final layer all hypothesis share the same network parameters. An adversarial loss is added to the model (how that is done is not described; the relevant equation (5) uses L_hyp which is not defined) to avoid the mode collapse.\n\nWhat is interesting about the paper:\n- First of all, pushing the the MHP framework towards AD could be relevant by its own right for a very small subcommunity that is interested in this method\n- The idea of using the adv loss for avoiding mode collapse can be useful in other settings; this is def a that I learned from the paper\n- The method might actually work rather well in practice\n\nVotum. As outlined above, the paper makes some rather interesting points, but is not well written and lacks some details. I am not entirely convinced that AD and MHP is a killer combination, but the experimental results are ok, nothing to complain here (except the usual bla: make it larger, more, etc), but honestly they really fine (maybe compare also again against more related work, e.g., Ruff et al ICML 2018).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}