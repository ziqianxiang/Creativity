{
    "Decision": {
        "metareview": "This paper explores the addition of feedback connections to popular CNN architectures. All three reviewers suggest rejecting the paper, pointing to limited novelty with respect to other recent publications, and unconvincing experiments. The AC agrees with the reviewers.\n\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "metareview: limited novelty, unconvincing experiments"
    },
    "Reviews": [
        {
            "title": "an ok paper but not good enough",
            "review": "This paper introduces feedback connection to enhance feature learning through incorporating context information. \n\nA similar idea has been explored by (Li, et al. 2018). Compared with that work, the novelty of this work is weaken and seems limited. The difference from Li is not very clear. The authors need to give more discussion. Furthermore, experimental comparison with Li et al. 2018 is also necessary. \n\nThe performance gain is limited. The authors mainly evaluate their method for noisy image classification. Such application is very narrow and deviates a bit from realistic scenarios. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper proposes to add \"recurrent\" connections inside a convolution network with gating mechanism. The idea is not novel and the performance improvement is marginal.",
            "review": "The paper proposes to add \"recurrent\" connections inside a convolution network with gating mechanism. The basic idea is to have higher layers to modulate the information in the lower layers in a convolution network. The way it is done is through upsampling the features from higher layers, concatenating them with lower layers and imposing a gate to control the information flow. Experiments show that the model is achieving better accuracy, especially in the case of noisy inputs or adversarial attacks. \n\n- I think there are lot of related literature that shares a similar motivation to the current work. Just list a few that I know of:\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015.\nLin, Tsung-Yi, et al. \"Feature Pyramid Networks for Object Detection.\" CVPR. Vol. 1. No. 2. 2017.\nNewell, Alejandro, Kaiyu Yang, and Jia Deng. \"Stacked hourglass networks for human pose estimation.\" European Conference on Computer Vision. Springer, Cham, 2016.\nYu, Fisher, et al. \"Deep layer aggregation.\" arXiv preprint arXiv:1707.06484 (2017).\nThe current work is very similar to such works, in the sense that it tries to combine the higher-level features with the lower-level features. Compared to such works, it lacks both novelty and insights about what works and why it works.\n\n- The performance gain is pretty marginal, especially given that the proposed network has an iterative nature and can incur a lot of FLOPs. It would be great to show the FLOPs when comparing models to previous works.\n\n- It is interesting observation that the recurrent network has a better tolerance to noise and adversarial attacks, but I am not convinced giving the sparse set of experiments done in the paper.\n\nOverall I think the current work lacks novelty, significance and solid experiments to be accepted to ICLR.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An experimental mess",
            "review": "This paper presents a novel deep learning module for recurrent processes. The general idea and motivation are generally appealing but the experimental validation is a mess. Architectures and hyper-parameters are casually changed from experiment to experiment (please refer to Do CIFAR-10 Classifiers Generalize to CIFAR-10? By Recht et al 2018 to understand why this is a serious problem.) Some key evaluations are missing (see below). Key controls are also lacking. This study is not the first to look into recurrent / feedback processes. Indeed some (but not all) prior work is cited in the introduction. Some of these should be used as baselines as opposed to just feedforward networks. TBut with all that said, even addressing these concerns would not be sufficient for this paper to pass threshold since overall the improvements are relatively modest (e.g., see Fig. 4 right panel where the improvements are a fraction of a % or left panel with a couple % fooling rates improvements) for a module that adds significant computational cost to an architecture runtime. As a side note, I would advise to tone down some of the claims such as \"our network could outperform baseline feedforward networks by a large margin”...\n\n****\nAdditional comments:\n\nThe experiments are all over the place. What is the SOA on CIFAR-10 and CIFAR-100? If different from VGG please provide a strong rationale for testing the circuit on VGG and not SOA. In general, the experimental validation would be much stronger if consistent improvements were shown across architectures.\n\nAccuracy is reported for CIFAR-10 and CIFAR-100 for 1 and 2 feedback iterations and presumably with the architecture shown in Fig. 1. Then robustness to noise and adversarial attacks tested on ImageNet and with a modification of the architecture. According to the caption of Fig. 4, this is done with 5 timesteps this time! Accuracy on ImageNet needs to be reported ** especially ** if classification accuracy is not improved (as I expect). \n\nThen experiments on fine-grained with ResNet-34! What architecture is this? Is this yet another number of loops and feedback iterations? When reporting that \"Our model can get a top-1 error of 25.1, while that of the ResNet-34 model is 26.5.” Please provide published accuracy for the baseline algorithm.\n\nFor the experiment on occlusions, the authors report using “a multi-recurrent model which is similar to the model mentioned in the Imagenet task”. Sorry but this is not good enough.\n\nTable 4 has literally no explanation. What is FF? What are unroll times?\n\nAs a side note, VGG-GAP does not seem to be defined anywhere.\n\nWhen stating \"We investigated VGG16 (Simonyan & Zisserman, 2014), a standard CNN that closely approximate the ventral visual hierarchical stream, and its recurrent variants for comparison.”, the authors probably meant “coarsely” not “closely\".",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}