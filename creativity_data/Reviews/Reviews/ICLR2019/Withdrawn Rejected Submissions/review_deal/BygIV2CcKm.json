{
    "Decision": {
        "metareview": "This paper proposes and end-to-end trainable architecture for data augmentation, by defining a parametric model for data augmentation (using spatial transformers and GANs) and optimizing validation classification error through the notion of influence functions. Experiments are reported on MNIST and CIfar-10. \n\nThis is a borderline submission. Reviewers found the theoretical framework and problem setup to be solid and promising, but were also concerned about the experimental setup and the lack of clarity in the manuscript. In particular, one would like to evaluate this model against similar baselines (e.g. Ratner et al) on a large-scale classification problem. The AC, after taking these comments into account and making his/her own assessment, recommends rejection at this time, encouraging the authors to address the above comments and resubmit this promising work in the next conference cycle. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting contribution, but not fully developed yet. "
    },
    "Reviews": [
        {
            "title": "valuable and publishable, some potential improvements",
            "review": "This paper proposes an extension of the influence function study of Koh and Liang (2017) to data augmentation.  Influence of augmentation, carried out via a parameterized and differentiable model, on validation loss is approximated and the augmentation model is learned under this approximation.  Overall I think it is a valuable and publishable contribution.  I do find the paper to be unclear and perhaps could be improved in a few ways.  My main comments are:\n\n* The biggest question I have is it seems from Eq. 15 that authors are proposing an augmentation approach where the augmented samples replace the original samples and not co-exist with them in the training set.  I am not sure why Eq. 15 has to be set up like that, please elaborate.\n\n* In Section 3.4 it is stated that only top fully connected layer of F is considered to compute influence function for augmentation.  Does this also mean that when F is updated on augmented data only the top layer is updated?  Please clarify.\n\n* The paper is a bit difficult to follow due to lack of clarity and few errors:\n     - Section 2.1, Adversarial methods, “In these methods, a simple composition…adversarial examples” sentence is unclear\n     - Page 2 footnote “however, they are referred to as unsupervised due to learning is not involved” sentence is unclear\n     - Section 3.3 \\tilde{z} in first line should be \\tilde{z_i}\n     - Eq. 15 LHS should include \\tilde{z_i}\n     - Section 3.4 “adopts” -> “adopt”\n     - Section 3.4 “HVP” used without defining\n\n* Empirical evidence, while not extensive, is satisfactory.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper, Inspiring theory, but need more experiments",
            "review": "[Summary]\n\nThis paper proposes a differentiable framework to learn to augment data for image classification. In particular, it uses spatial transformer and GANs as parametric data augmenters, and it formulates the validation set loss with respect to the data augmenter in a differentiable manner. \n\n[Pros]\n\n1.\tThe proposed method does not require many trials of model training under different training data, and it learns the data augmentation directly using the final classification objective.\n2.\tIt is inspiring to extend the differentiable form of influence function across the training and validation set and then across the original and augmented data. This paper also makes use of the most recent related advance to enable stochastic learning. The theory of the paper is nice.\n3.\tThe experimental results on MNIST (with less labeled data) and Cifar-10 are encouraging.\n\n[Cons]\n\n1.\tExperimental results can be stronger. Especially when compared to Ratner et al., this proposed method results in marginal performance gain. Given that Ratner et al.’s method trained the data augmentation module without supervision, the supervised learning in this paper does not show strong results. In addition, the paper did not report results on a more practical dataset (such ImageNet and Places). Even for Cifar-10, the reported numbers are away from the state-of-the-art. It is important to show the practical significance of the proposed method.\n2.\tData augmentation is naturally expected to be random, but the proposed method seems to learn a deterministic parameter for the augmenting transformation, which looks unnatural and limited. (Please clarify if I missed anything.) \n3.\tThe proposed method requires a parametric model (e.g, STN, GAN). However, differentiable parametric models are not always easy to design. This probably can be the biggest obstacle to apply the proposed method widely.\n\nOverall, the proposed method is very interesting. However, the experimental results are limited, and more discussions are needed. \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "official review for \"Learning to Augment Influential Data\"",
            "review": "This paper proposed an \n\n\n1. For me, the argument of the paper is ambitious. Data augmentation for DNN includes different perspective, including nonlinearity, adversarial etc. Generalization of  spatial and appearance models is not enough. The model formulate from a simple classification setting but does not involve too many for DNN models.  I put more references below. \n\n2. The experimental results are not strong. Not all strong baselines are included (I put some in the references). The improvements are marginal. Besides, I need more experimental setting information.\n\n3. The writing is not clear. For the related work part, it included many paragraph which are not related to the work, (e.g. GANs). In the introduction part, it did not mention the generalization of both spatial and appearance models, which is the main contribution.  \n\nReferences:\na. Good Semi-supervised Learning that Requires a Bad GAN\nb. Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference \nc. Temporal ensembling for semi-supervised learning",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}