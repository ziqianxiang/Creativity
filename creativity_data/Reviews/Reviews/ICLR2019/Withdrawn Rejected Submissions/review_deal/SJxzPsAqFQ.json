{
    "Decision": {
        "metareview": "\nThis paper proposes an adversarial learning framework for dialogue generation. The generator is based on previously proposed hierarchical recurrent encoder-decoder network (HRED) by Serban et al., and the discriminator is a bidirectional RNN. Noise is introduced in generator for response generation.\nThe approach is evaluated on two commonly used corpora, movie data and ubuntu corpus.\n\nIn the original version of the paper, human evaluation was missing, an issue raised by all reviewers, however, this has been added in the revisions. These supplement the previous automated measures in demonstrating the benefits and significant gains from the proposed approach.\n\nAll reviewers raise the issue of the work being incremental and not novel enough given the previous work in HRED/VHRED and use of hierarchical approaches to model dialogue context. Furthermore, noise generation seems new, but is not well motivated, justified and analyzed.\n\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Approach being incremental is a consistent concern amongst reviewers"
    },
    "Reviews": [
        {
            "title": "Reasonable approach, but lacks novelty and better evaluation",
            "review": "This paper presented a dialog response generation method using adversarial learning framework. \nThe generator is based on previously proposed hierarchical recurrent encoder-decoder network (HRED), and the discriminator is a bidirectional RNN. \nNoise samples are introduced in generator for response generation.\nThey evaluated their approach on two datasets and showed mostly better results than the other systems. \n\nThe novelty of the paper is limited. \nModeling longer dialog history (beyond the current turn) is not new, this has been used in different tasks such as dialect act classification, intent classification and slot filling, response generation, etc.\nThe generator is based on previous HRED. \nAdding noise to generate responses is somewhat new, but that doesn’t seem to be well motivated or justified. \nWhy adding Gaussian noise improves the diversity or informativeness of the responses is not explained. \nThe idea of discriminator has been widely used recently for language generation related tasks.  What is new here? Is it the word-based metric? Sharing the context and word information with generator?  It would be helpful if the authors can clarify their contribution. \n  \nRegarding using MLE to first generate multiple hypotheses in generator, how is the quality of the n-best responses? \nIs there a way to measure the goodness of the responses in some kind of reranking framework, not necessarily discriminator? \n\nThe results in the table showed the proposed method outperforms the others in terms of those objective metrics. I feel some subjective evaluations are needed to strengthen the paper.\nFrom the samples responses in the table, it doesn’t look like the new method generates very good responses. \n\n\nDetailed comments: \n- Sec 2, before 2.1, last paragraph, “With the GAN objective, we can match the noise distribution, P(Z_i) to the distribution of the ground truth response, P(X_i+1|X_i).  This needs clarification. \n- Figure 1: caption, “left” and “right” are misplaced. \n- sec 2.1, last paragraph, without Z_i, the net could still learn a mapping from X_i to Y_i, but would produce deterministic outputs.  I think the authors mean that the system generates a probability distribution P(Y_i|X), the output is the most likely one from that. However, if the output is a distribution, the system can also do some sampling and not necessarily output the top one.  This is not that different from adding noise in the history — if that’s based on some distribution, then it may still be deterministic. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Reasonable approach but somewhat incremental; weak evaluation setup",
            "review": "\nThe paper applies conditional GAN to the HRED model [Serban et al., 2016] for dialogue response generation, showing improvements in terms of informativeness and diversity compared to HRED and VHRED [Serban et al., 2017].\n\nThe paper is technically solid and relatively easy to follow and the results are good, but comparisons with previous work (descriptive and experimental) are rather weak. \n\n- Related work is incomplete: The paper specifically argues for the use of GAN to improve diversity in dialogue response generation, but this is not the first paper to do so. [Xu et al., 2017] presents a GAN-like setup that targets exactly the same goal, but that work is not cited in the paper. Same for [Zhang et al., 2018], but the latter work is rather recent (it still should probably be cited).\n\n- Evaluation: There is no evaluation against Xu et al., which targets the same goal. The authors didn’t even compare their methods against baselines used in other GAN works for diverse response generation (e.g., MMI [Xu et al.; Zhang et al.], Li et al.’s GAN approach [Xu et al.]), which makes it difficult to draw comparisons between these related methods. As opposed to these other works, the paper doesn’t offer any human evaluation.\n\n- It would have been nice to include an LSTM or GRU baseline, as these models are still often used in practice and the VHRED paper suggests [Serban et al., 2016; Table 1] that LSTM holds up quite well against HRED (if we extrapolate the results of VHRED vs. LSTM and VHRED vs. HRED). The ablation of GAN and HRED would help us understand which of the two is more important.\n\nIn sum, the work is relatively solid, but considering how much has already been done on generating diverse responses (including 3 other papers also using GAN), I don’t think this paper is too influential. Its main weakness is the evaluation (particularly the lack of human evaluation.)\n\nMinor comments:\n\n- Introduction: “diversity promoting training objective but their model is for single turn conversations”. \nIf these were “single turns”, they wouldn’t really be called conversations; that objective has been used with 3+ turn conversations. It can actually be applied to multi-turn dialogue as with any autoegressive generative models. For example, it has been exploited that way as a baseline for multi-turn dialogue [Li et al. 2016](“Deep Reinforcement Learning for Dialogue Generation“). Note it is not a “training objective”, but only an objective function at inference time, which is a more valid reason to criticize that paper.\n\n- “We use greedy decoding (MLE) on the first part of the objective.” Doesn’t that hurt diversity because of MLE? what about using sampling instead (maybe with temperature)?\n\n- Algorithm 1: the P_theta_G don’t seem to match the text of section 2. h_i is in sometimes written in bold and sometimes not (see also Eq 12 for comparison.)\n\n- End of section 2.1: There are multiple Li et al.; specify which one.\n\n- End of section 2.2 and 2.4: extra closing parenthesis after N(0, …))\n\n- Figures are too small to read the subscripts.\n\n[Xu et al. 2017]: Zhen Xu, Bingquan Liu, Baoxun Wang, Sun Chengjie, Xiaolong Wang, Zhuoran Wang, and Chao Qi. Neural response generation via gan with an approximate embedding layer. EMNLP 2017.\n\n[Zhang et al. 2018]: Zhang, Yizhe & Galley, Michel & Gao, Jianfeng & Gan, Zhe & Li, Xiujun & Brockett, Chris & Dolan, Bill. (2018). Generating Informative and Diverse Conversational Responses via Adversarial Information Maximization.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea but better evaluation needed",
            "review": "This paper presents an adversarial learning model for generating diverse responses for dialogue systems based on HRED (hierarchical recurrent encoder-decoder) network. The contribution of the work mainly lies in: 1. adversarial learning, and 2. injecting Gaussian noise at word-level and sentence-level to encourage the diversity. Overall, the idea is interesting, and the automatic evaluation based on perplexity, BLEU, ROUGE, etc shows that the proposed methods outperform existing methods.\n\nSeveral suggestions:\n- It seems like injection noise at word-level almost always outperforms adding sentence-level noise. It would be better if the authors can explain why this happens and whether it can be applied for other response generation tasks.\n\n- Built on above comment, the authors can also experiment with other response generation datasets, e.g. interactions on social media.\n\n- From examples in Table 3 and 4, the generated responses are of low quality overall. I suggest the authors run human evaluation to see whether there is any significant difference among system responses by different models on aspects of informativeness and fluency at least.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Overall, the proposed model seems like sound and thoughtful approach, but the lack of novelty over the existing literature is a weakness.",
            "review": "This paper propose a new approach to dialogue modeling by introducing two\ninnovations over an established dialogue model: the HRED (Hierarchical\nRecurrent Encoder-Decoder) network. The innovations are: (1) adding a GAN\nobjective to the standard MLE objective of the HRED model; and (2)\nmodifying the HRED model to include an attention mechanism over the local\nconditioning information (i.e. the \"call\" before the present \"response\").  \n\nWriting: The writing was mostly ok, though there were some issues early in\nSection 2. The authors rather awkwardly transition from a mathematical\nformalism that included the two halves of the dialogue as X (call) and Y\n(response), to a formalism that only considers a single sequence X. \n\nNovelty and Impact:  The proposed approach explicitly combines an established\nmodel with two components that are themselves well-established.\nIt's fair to say that the novelty is relatively weak. The model development\nis sensible, but reasonably straightforward. It isn't clear to me that a\ncareful reader of the literature in this area (particularly the GAN for\ntext literature) will learn that much from this paper. \n\nExperiments: Overall the empirical evaluation shows fairly convincingly\nthat the proposed model is effective. I do wonder why would the hredGAN\nmodel outperform the hred model on perplexity. The hred model is\ndirectly optimizing MLE which is directly related to the perplexity\nmeasure, while the hredGAN include an additional objective that should\n(perhaps) sacrifice likelihood. This puzzling result was not discussed and\nreally should be.\n\nThe generated responses, given in table 3 -- while showing some improvement\nover hred and Vhred (esp. in terms of response length and specificity) --\ndo not fit the context particularly well. This really just shows we still\nhave some way to go before this challenging task is solved. \n\nIt would be useful if the authors could run an ablation study to help\nresolve the relative contributions of the two innovations (GAN and\nattention) to the improvements in results. Perhaps the improvement in\nperplexity (discussed above) is do to the use of attention. \n\nDetailed comments / questions\n\n- In the paragraph between Eqns 2 and 3, the authors seem to suggest that\n  teacher forcing is an added heuristic -- however this is just the\n  correct evaluation of the MLE objective. \n\n- In discussing the combined MLE-GAN objective in Eqn. 8 Does the MLE\n  objective use teacher forcing? Some earlier text (discussed above) leads\n  me to suspect that it does not. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}