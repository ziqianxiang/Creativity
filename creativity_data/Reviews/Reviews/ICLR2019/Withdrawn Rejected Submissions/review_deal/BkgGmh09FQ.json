{
    "Decision": {
        "metareview": "This paper targets improving the computation efficiency of super resolution task. Reviewers have a consensus that this paper lacks technical contribution, therefore not recommend acceptance. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "lack technical contributions"
    },
    "Reviews": [
        {
            "title": "summary of known ideas / no new ideas / no better results than other from the SR literature",
            "review": "The authors target single-image super-resolution (SR) task and study the efficiency (runtime, memory) of the current neural networks.\n\nOn the positive side, the paper is a good effort of bringing together works and insights related to efficient designs and efficient SR solutions.\n\nIf we report to the baseline architecture (RCAN) then the proposed efficient variants achieves large reductions in number of parameters or multiplications-additions, at the cost of lower accuracy. However, when the newly proposed trade-offs are compared with the existing literature, we see other methods that do a comparable or better job in the same range.\n\nOn the negative side, from my point of view, the study is far from being thorough and does not lead to or bring new insights or ideas. The experimental results does not reveal new operating points (trade-off between complexity/operations and performance accuracy).\n\nI would suggest to the authors to expand their study, to make some novel observations, and to propose some designs that can stand out in the literature.\n\nI am pointing out also to some recent papers that are related to the topic and can be or are applied to SR:\nGu et al, \"Multi-bin Trainable Linear Unit for Fast Image Restoration Networks\", arxiv 2018\nIgnatov et al, \"Pirm challenge on perceptual image enhancement on smartphones: Report\", arxiv 2018\nand some works proposed for that challenge:\nVu et al, \"Fast and efficient image quality enhancement via desubpixel convolutional neural networks\", ECCVW 2018\nLi et al, \"CARN: Convolutional Anchored Regression Network for Fast and Accurate Single Image Super-Resolution\", ECCVW 2018\nPengfei et al, \"Range scaling global u-net for perceptual image enhancement on mobile devices\", ECCVW 2018\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official review",
            "review": "\nThe paper proposes a detailed empirical evaluation of the trade-offs achieved by various convolutional neural networks on the super resolution problem. The paper provides an extensive evaluation of different architectural changes and the trade-off between savings in terms of memory and computational cost and performance, measured in terms of PSNR and SSIM.\n\nThis is an empirical paper, thus it does not provide technical contributions. I do think that the insights obtained from such an empirical evaluation could be of interest for practitioners and researchers working on the problem. My main concern is the method only evaluates the trade-offs between model efficiency (in terms of memory and/or computation) and performance measured using metrics that are known to not be well correlated with perceptual quality. Thus it is not obvious to me that the insights obtained in this work would translate to the other case.\n\nIt is well known that PSNR favors blurry solutions over perceptually more appealing solutions. This comes from the fact that there is no information in the low resolution image to produce the missing high resolution details. Filling up plausible details in a way that is different from the original image would lead to high PSNR. Models that treat the super resolution problem as a regression task using similarity in pixel space, tend to produce blurry solutions and require very large models to improve the score.  \n\nIn recent years, many works have been studying the use of perceptual losses to mitigate this issue or simply treating the super resolution problem as conditional generative modeling.  For instance, models using L2 losses in a perceptually more relevant (or learned) feature spaces [A, B], or including GAN losses [C, D] (to list a few). To my knowledge, these models are the current state of the art in terms of perceptual quality. This has been evaluated empirically via perceptual tests [D].  \n\nThis line of work needs to be cited. In my view, the paper needs to provide a detailed justification on why models using these losses are not considered. Would the conclusions drawn on this work transfer to that setting? Furthermore, it would be good to perform perceptual tests to perform this evaluation. It would be good to provide some canonical examples in the appendix.\n\nThe overall writing of the paper could be improved. Several sentences are difficult to read, due to typos or the construction of the sentences. The paper evaluates many architectural modifications proposed by other works. It would be good to add an appendix with a small description of what these are. This would make the paper self-contained an easier to read (I had too look up a few of them).\n\nThe authors mentioned that they first train models for scaling factor of x2 and then use them for training settings higher magnification. How is this exactly done? Please provide details.\n\nI am curious of weather using some for of distillation techniques would be useful here.\n\nDid you try scaling factors larger than x4? Scaling factors of x2 does not seem very relevant, as simpler methods can achieve already quite competitive results (such as simple interpolation methods)\n\nThe authors seem to be citing Zhang et al (2018) as a reference to attention mechanisms. To my knowledge the paper that proposed these mechanisms is [E].\n\nThe citation style is not used properly throughout the manuscript. As an example:\n\n“… proposed in StrassenNets Tschannen et al (2017).” Should be “… proposed in StrassenNets (Tschannen et al, 2017).” Or “… proposed in StrassenNets proposed by Tschannen et al (2017).”\n\n[A] Johnson, J. et al. \"Perceptual losses for real-time style transfer and super-resolution.\" ECCV, 2016.\n[B] Bruna, J. et al \"Super-resolution with deep convolutional sufficient statistics.\" ICLR 2016.\n[C] Ledig, C. et al. \"Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.\" CVPR. Vol. 2. No. 3. 2017.\n[D] Sønderby, C. K., et al. \"Amortised map inference for image super-resolution.\" arXiv preprint arXiv:1610.04490(2016).\n[E] Bahdanau, D. et al \"Neural machine translation by jointly learning to align and translate.\" arXiv (2014).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "No new insight is proposed. The techniques are not specifically designed for super-resolution task. The experimental results are also weak.",
            "review": "This paper proposed to improve the system resource efficiency for super resolution networks. \n\nFirst, I am afraid all the techniques considered in this paper have been investigated in previous works. Thus no new idea is proposed in this work. Also, it is also not clear why these improvement is particularly suitable for the task of super resolution. In my viewpoint, these techniques actually can be used to improve a variety of network architectures in both high-level and low-level vision tasks.\n\nSecond, the experimental results are also weak. As this work is aiming to address the super resolution tasks, at least visual comparisons between the proposed methods and other state-of-the-art approaches should be included in the experimental part. But unfortunately, no such qualitative results are presented in the manuscript. \n\nFinally, the presentation of the paper should also be carefully proofread and revised.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}