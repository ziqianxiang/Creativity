{
    "Decision": {
        "metareview": "This paper presents an empirical study of the applicability of genetic algorithms to deep RL problems. Major concerns of the paper include: 1. paper organization, especially the presentation of the results, is hard to follow; 2. the results are not strong enough to support that claims made in this paper, as GAs are currently not strong enough when compared to the SOTA RL algorithms; 3. Not quite clear why or when GAs are better than RL or ES; Lack of insights. Overall, this paper cannot be accepted yet. \n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Improvement needed."
    },
    "Reviews": [
        {
            "title": "GAs for Deep-RL; good performance numbers, but needs more intuition and understanding.",
            "review": "The authors show that using a simple genetic algorithm to optimize the weights of a large DNN parameterizing the action-value function can result in competitive policies. The GA uses selection truncation and elitism as heuristics; compact-encoding of the genome to improve system efficiency. Results on Atari are presented, along with a comparison with standard gradient-based RL algorithms and ES.\n\nPros:\n\nThe knowledge that on a moderately complex optimization problem such as RL on Atari with pixel inputs, learning can be achieved using parameter perturbation (combined with heuristics) is valuable to the community. The authors’ motivation --- that GAs could prove to be another interesting tool for RL --- is well-founded. The efficient implementation using CPU-GPU hybridization and the compact-encoding is a good engineering contribution and could help accelerate further research.\n\nCons:\n\na.) One issue is that the results are presented in a somewhat hard-to-read manner. Table 1. shows numbers on 13 selected Atari games, of which 3 work best with GA (considering 1B frames). Comparison on all games helps to understand the general applicability of GAs. This is provided in Table 6., but the GA (1B) is missing. So are we comparing ES (1B) vs GA (6B) in Table 6 on all games? I can understand that DQN/A3C take days to run that many frames, but what does a ES (1B) vs GA (6B) comparison tell us? Am I reading it right?\n\nb.) Deep RL algorithms are approximations of theoretically sound ideas; so is ES in a way. What would help make this an even better paper is if the authors attempt to demystify the GA results. For example, I would have enjoyed some discussion on the potential reasons for the superiority of GAs on the 3 games in Table 1. Are these hard exploration environments and the parameter-space exploration in GA outperforms the action-space (e-greedy) exploration in A3C (DQN)? Random search is also better than DQN on those 3 environments --- Is the availably of good policies around the initial weight distribution contributing to the GA results in anyway? Furthermore, it would be interesting to pick out the worst games for GA from Table 5. (cases of catastrophic failure of GA), and analyze the shortcomings of GA. This would be helpful in further advancing research in GAs for deep RL. As a suggestion, ablation studies could go some distance in improving the interpretability, since the GA in the paper uses critical choices (parameter-noise variance, population-size etc.) and heuristics (truncation, elitism, etc.) that affect performance w.r.t baselines.\n\nOther comments:\n\nComparing GA+NS with A3C/DQN under environments with deceptive rewards is not completely fair. If the GA is armed with an exploration strategy like NS, so should the standard RL algorithms (e.g. by adding count-based, curiosity exploration). \n\nSection 4.3 (Humanoid discussion) could be move to Appendix since it currently doesn’t work with GA, and if anything, increases doubts on the robustness of GA since the Humanoid DNN is fairly small.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Explore GAs as alternative for DeepRL ",
            "review": "The authors explore the use of GAs as an alternative to gradient based methods for DeepRL and show performance comparisons indicating competitive /on par performance when compared to the existing methods. \n\nPros:\n-I liked the idea of exploring other avenues for approaching DeepRL problems, and challenging existing paradigms or trends. \nThis has a couple of implications - it might lead to expanding the toolbox for DeepRL problems, and it also might lend insight regarding these problems that could lead to new directions in exploring other methods. In other words, exploring WHY GA do better (and even RS!) could be useful, and this study is a foray in that direction. \nIt could also (as they point out) lead to useful hybrid approaches.\n-Their implementation will be made available and includes some useful features (compact network encoding, etc). \n-They demonstrate very nice speeds that can be impactful especially for more resource limited scenarios. \n\nCons: \n-There has been some exploration of use of GA for games, though not in DeepRL (AFAIK). They should cite previous efforts and explain what is their contribution specifically above and beyond previous efforts.\n-Although I applaud the authors for making exploratory efforts into this alternative approach and demonstrating competitive performance, the reader is left with some degree of \"so what?\". In other words, WHEN should this method be applied? How does it fit into the existing toolkit?\n-This is in some ways a conceptual exploration, and it shows interesting things that open \"why\" questions. Why does GA do better in some cases? They address this nicely in the discussion but more exploration in that direction would be useful. \n\nOverall I like this paper for its conceptual contribution that may provide insight that will help the field grow in interesting directions, and for the implementation it will provide. \n\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting exploration, but lacks needed rigor.",
            "review": "This paper demonstrates that Genetic Algorithms can be used to train deep neural policies for Atari, locomotion, and an image-based maze task. It's interesting that GAs can operate in the very high-dimensional parameter space of DNNs. Results show that on the set of Atari games, GAs perform roughly as well as other ES/DeepRL algorithms.\n\nIn general, it's a bit hard to tell what the contribution of this paper is - as an emperical study of GA's applied to RL problems, the results raise questions:\n\n1) Why only 13 Atari games? Since the algorithm only takes 1-4 hours to run it should only take a few days to collect results on all 57 games?\n\n2) Why not examine a standard GA which includes the crossover operator? Do the results change with crossover?\n\n3) The authors miss relevant related work such as \"A Neuroevolution Approach to General Atari Game Playing\" by Hausknecht et al., which examines how neuroevolution (GA based optimization which modifies network topology in addition to parameters) can be used to learn policies for Atari, also scaling to million-parameter networks. This work already showed that GA-based optimization is highly applicable to Atari games.\n\n4) Is there actual neuroevolution going on? The title seems to imply there so, but from my reading of the paper - it seems to be a straightforward GA (minus crossover) being applied to weight values without changes to network topology.\n\nI think this paper could be strengthened by providing more insight into 1) Given that it's already been shown that random search can be competitive to RL in several Mujoco tasks (see \"Simple random search provides a competitive approach to reinforcement learning\") I think it's important to understand why and in what scenarios GAs are preferable to RL and to ES given similar performance between the various methods. 2) Analysis as to whether Atari games in particular are amenable to gradient-free optimization or if GA's are equally applicable to the full range or RL environments?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An empirical evaluation of a specific genetic algorithm, but without much value add",
            "review": "This paper looks at a specific implementation of a \"genetic algorithm\" (GA) when applied to learning Atari games.\nUsing a black-box \"random search\" technique, with a few heuristic adaptations, they find performance that is roughly competitive with several other baseline methods for \"Deep RL\".\nMore generally, the authors suggests that we should revisit \"old\" algorithms in Machine Learning and that, when we couple them with larger amounts of computation, their performance may be good.\n\nThere are several things to like about this paper:\n- The authors place a high importance on implementation details + promise to share code. This seems to be a paper that is heavily grounded in the engineering, and I have high confidence the results can be reproduced.\n- The algorithm appears broadly competitive on several Atari games (although Table 1 is admittedly hard to parse).\n- The algorithm is generally simple, and it's good to raise questions of baseline / what are we really accomplishing.\n\nHowever, there are several places where this paper falls down:\n- The writing/introduction is extremely loose... terms are used and introduced without proper definition for many pages. \n    + How would be think about the venn diagram of \"evolutionary strategies\", \"genetic algorithms\", \"deep Q networks\", \"deep RL algorithms\" and \"random search\"... there is clearly a lot of overlap here.\n    + The proposed deep GA has a \"deep Q network\" (or is it a policy... the paper does not make this clear), forms a type of \"evolutionary strategy\" and, at its heart is a type of \"random search\", but is it not also a \"deep RL algorithm\"?\n    + It is not until page 3 that we get a proper definition of the algorithm, and it's hard to keep track of the differences the authors want to highlight compared to the \"baselines\".\n    + What do we gain from the claim \"old algorithms work well\"... gradient descent is also an old algorithm, as are seemingly all of the alternatives? Is age in-of-itself an asset?\n    + Statements like \"compression rate depends on the number of generations, but in practice is always substantial\" are very loose... what does the \"practice\" refer to here, and why?\n\n- There is very little insight/analysis into *how* or *why* this algorithm performs better/worse than the alternatives. I don't feel I understand if/when I should use this algorithm versus another apart from a wall of seemingly random Atari results. In fact, there is a large literature that explains why GAs give up a lot of efficiency due to their \"black box\" nature... what do the authors think of this?\n\n- This paper seems purely focused on results rather than insight, with many hacks/tweaks to get good results... should we believe that GA+novelty search is a general algorithm for AI, or is it just another tool in the arsenal of a research engineer? \n    + In the end, the algorithm doesn't actually seem to outperform state-of-the-art on these Atari baselines... so what are we supposed to take away from this.\n\nOverall, I don't think that this paper provides very much in the way of scientific insight.\nFurther, the results are not even superior to existing algorithms with stronger groundings.\nFor me, this leads it to be a clear reject... even though they probably have code that reliably solved Atari games in a few hours.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Genetic algorithms are a potential alternative to backpropagation-based algorithms for reinforcement learning tasks",
            "review": "Post-rebuttal update: The review process has identified several issues such as missing citations and lack of clarity with respect to aims of the paper. Although the authors have failed to update the paper within the rebuttal period, their responses show an understanding of the issues that need to be addressed as well as a broad appreciation of work in EC that would be included in a final version, making it a useful resource for the wider ML community. On top of this they will include an even larger amount of empiricial data from the experiments they have already run, which is a valuable resource considering the amount of compute needed to obtain this data.\n\n---\n\nThe current landscape of reinforcement learning - particularly in domains with high-dimensional structured input spaces such as images or text - relies heavily on backpropagation-based reinforcement learning algorithms.  An alternative that has re-emerged is ES, due to its simplicity and scalability. However, ES can also be considered a gradient-based method. In this paper, the authors apply a similar treatment to GAs, another simple method at its most basic. The authors claim to have 3 main contributions: extending the scale to which GAs can operate, suggesting that gradient-based methods may not achieve the best performance, and making available a vast array of techniques available from the EC literature; they demonstrate the latter by utilising novelty search (NS).\n\nIn my opinion the authors do indeed have a valuable contribution in a) demonstrating that a simple GA can successfully be applied to larger networks than was previously thought to be possible and b) introducing a novel software implementation that allows GAs to be efficiently scaled/distributed (similar in nature to the work of Salimans et al.). This is by itself valuable, as, along with recent work on ES, it potentially extends the range of problems that are perhaps best tackled using black-box optimisation techniques. Going against the prevailing trends in order to investigate alternative methods is an underappreciated service to the community, and I believe the evaluation of the methods and the choice of comparative methods to be just about satisfactory. As exemplified by NS, there is a wealth of techniques from the EC literature that could be applied to many topical problems, and the authors' main contributions opens up the road for this.\n\nA lot of care has been put into evaluation on Atari games. The details in the main paper and supplementary material, with, e.g., clear definitions of \"frames\", make me believe that fair comparisons have taken place. All methods in the table, including GAs, perform best at some games (except for RS, which is a necessary baseline for GAs). It would be better to provide more data points that relate to prior works - such as scores at 200M frames to evaluate sample complexity (indeed, the authors note that good solutions can be found by GAs within a few generations, so it would be best to tabulate this) and at ~ 4d to evaluate wall-clock time (is it possible to push performance even further?). Since the GA presented is very rudimentary, I consider the baselines in the main paper to be reasonable, but it would be misleading to not present newer work. The authors do so in the supplementary material, and it is promising to note that the GA still achieves state-of-the-art performance in a few games even when compared to the most sophisticated/distributed state-of-the-art DRL algorithms developed in a concentrated effort over the last few years. Despite having an NS variant, it is a shame that the authors did not show that this could potentially improve performance on Atari, when BCs such as the game RAM or preferably random CNN features are easily available.\n\nThe authors also evaluate on a maze that is a staple task in the EC literature to demonstrate the power of NS. While the results are unsurprising, it is a reasonable sanity check. The final evaluation is on a difficult continuous control task, in which GAs solve the task, but have much poorer sample complexity than ES. Given the range of continuous control tasks used to benchmark RL algorithms nowadays, the authors would do well to present results across more of these tasks. Again, NS was not evaluated on this task.\n\nA major weakness of this paper is the presentation. The authors discuss some interesting findings, but would be better served by being more concise and focused. In particular, the emphasis should be more on showcasing quantitative results. Doing so, with more continuous control tasks, would make the claims of this paper more substantiated.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}