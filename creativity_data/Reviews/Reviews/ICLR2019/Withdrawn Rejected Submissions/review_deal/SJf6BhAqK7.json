{
    "Decision": {
        "metareview": "All reviewers wrote strong and long reviews with good feedback but do not believe the work is currently ready for publication.\nI encourage the authors to update and resubmit.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Good but not good enough"
    },
    "Reviews": [
        {
            "title": "Hard to read and relies on unjustified, shifting assumptions ",
            "review": "Update after Author Rebuttal\n--------------\nAfter reading the rebuttal, I'm pleased that the authors have made significant revisions, but I still think more work is needed. The \"hard/soft\" hybrid approach still lacks justification and perhaps wasn't compared to a soft/soft approach in a fair and fully-correct way (see detailed reply to authors). I also appreciate the efforts on revising clarity, but still find many clarity issues in the newest version that make the method hard to understand let alone reproduce. I thus stand by my rating of \"borderline rejection\" and urge the authors to prepare significant revisions for a future venue that avoid hybrids of hard/soft probabilities without justification. \n\n(Original review text below. Detailed replies to authors are in posts below their responses).\n\nReview Summary\n--------------\nWhile the focus on variadic learning is interesting, I think the present version of the paper needs far more presentational polish as well as algorithmic improvements before it is ready for ICLR. I think there is the potential for some neat ideas here and I hope the authors prepare stronger versions in the future. However, the current version is unfortunately not comprehensible or reproducible.\n\nPaper Summary\n-------------\n\nThe paper investigates developing an effective ML method for the \"variadic\" regime, where the method might be required to perform learning from few or many examples (shots) and few or many classes (ways). The term \"variadic\" comes from use in computer science for functions that can a flexible number of arguments. There may also be unlabeled data available in the few shot case, creating semi-supervised learning opportunities.\n\nThe specific method proposed is called BANDE: Bayesian Nonparametric Deep Embedding. The idea is that each data point's feature vector x_i is transformed into an embedding vector h(x_i) using a neural network, and then clustering occurs in the embedding space via a single-pass of the DP-means algorithm (Kulis & Jordan 2012). Each cluster is assumed to correspond to one \"class\" in the eventual classification problem, though each class might have multiple clusters (and thus be multi-modal).  \n\nLearning occurs in an episodic manner. After each episode (single-pass of DP-means), each point in a query set is embedded to its feature vector, then fed into each cluster's Gaussian likelihoods to produce a normalized cluster-assignment-probability vector that sums to one. This vector is then fed into a cross-entropy loss, where the true class's nearest cluster (largest probability value) is taken to be the true cluster. This loss is used to perform gradient updates of the embedding neural network.\n\nThere is also a \"cumulative\" version of the method called BANDE-C. This version keeps track of cluster means from previous episodes and allows new episodes to be initialized with these.\n\nExperiments examine the proposed approach across image categorization tasks on Omniglot, mini-ImageNet, and CIFAR datasets.\n\n\nStrengths\n---------\n* I like that many clusters are used for each true class label, which is better than rigid one-to-one assumptions.\n\n\nLimitations\n-----------\n* Can only be used for classification, not regression\n* The DP-means procedure does not account for the cluster-specific variance information that is used at other steps of the algorithm\n\n\nSignificance and Originality\n----------------------------\nTo me, the method appears original. Any method that could really succeed across various variadic settings would be significant.\n\n\n\nPresentation Concerns\n---------------------\n\nI have serious concerns about the presentation quality of this paper. Each section needs careful reorganization as well as rewording.\n\n## P1: Algo. 1 contains numerous omissions that make it as written not correct.\n\n* the number of clusters count variable \"n\" is not updated anywhere. As writting this algo can only update one extra cluster beyond the original n.\n* the variable \"c\" is unbound in the else clause. You need a line that clarifies that c = argmin_{c in 1 ... n} d_ic\n\nWould be careful about saying that \"a single pass is sufficient\"... you have *chosen* to do only one pass. When doing k-means, we could also make this choice. Certainly the DP-means objective could keep improving with multiple passes.\n\n## P2: Many figures and tables lack appropriate captions/labels\n\nTable 1: What metric is reported? Accuracy percentage? Not obvious from title/caption. Should also make very clear here how much labeled data was used.\n\nTable 2: What metric is reported? Accuracy percentage? Not obvious from title/caption. Should also make how many labeled and unlabeled examples were used easier to find.\n\n## P3: Descriptions of episodic learning and overall algorithm clarity\n\nReaders unfamiliar with episodic learning are not helped with the limited coverage provided here in 3.1 and 3.2. When exactly is the \"support\" set used and the \"query\" set used? How do unlabeled points get used (both support and query appear fully labeled)? What is n? What is k? What is T? Why are some points in Q denoted with apostrophes but not others? Providing a more formal step-by-step description (perhaps with pseudocode) will be crucial.\n\nIn Sec. 3.2, the paragraph that starts with \"The loss is defined\" is very hard to read and parse. I suggest adding math to formally define the loss with equations. What parameters are being optimized? Which ones are fixed?\n\nAdditionally, in Sec. 3.2: \"computed in the same way as standard prototypical networks\"... what is the procedure exactly? If your method relies on a procedure, you should specify it in this paper and not make readers guess or lookup a procedure elsewhere.\n\n\n## P4: Many steps of the algorithm are not detailed\n\nThe paper claims to set \\lambda using a technique from another paper, but does not summarize this technique. This makes things nearly impossible to reproduce. Please add such details in the appendix.\n\nMajor Technical Concerns\n------------------------\n\n## Alg. 1 concerns: Requires two (not one) passes and mixes hard and soft assingments and different variance assumptions awkwardly\n\nThe BANDE algorithm (Alg. 1) has some unjustified properties. Hard assignment decisions which assume vanishing variances are used to find a closest cluster, but then later soft assignments with non-zero variances are used. This is a bit heuristic and lacks justification... why not use soft assignment throughout? The DP means procedure is derived from a specific objective function that assumes hard assignment. Seems weird to use it for convenience and then discard instead of coming up with the small fix that would make soft assignment consistent throughout.\n\nFurthermore, The authors claim it is a one pass algorithm, but in fact as written in Alg. 1 it seems to require two passes: the first pass keeps an original set of cluster centers fixed and then creates new centers whenever an example's distance to the closest center exceeds \\lambda. But then, the *soft* assignment step that updates \"z\" requires again the distance from each point to all centers be computed, which requires another pass (since some new clusters may exist which did not when the point was first visited). While the new soft values will be close to zero, they will not be *exactly* zero, and thus they matter. \n\n## Unclear if/how cluster-specific variance parameters learned\n\nFrom the text on top of page 4, it seems that the paper assumes that there exist cluster-specific variances \\sigma_c. However, these are not mentioned elsewhere, only a general (not cluster-specific) label variance \\sigma and fixed unlabeled variance sigma_u are used.\n\n## Experiments lack comparison to internal baselines\n\nThe paper doesn't evaluate sensitivity to key fixed hyperparameters (e.g. \\alpha, \\lambda) or compare variants of their approach (with and without soft clustering step, with and without multimodality via DP-means). It is difficult to tell which design choices of the method are most crucial.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novelty is unclear",
            "review": "The paper proposes a meta-learning method that utilizes unlabeled examples along with labeled examples. The technique proposed is very similar to the one by (Ren et al. 2018), only differing in the choice of a different clustering algorithm (Kulis and Jordan, 2012) instead of soft k-means as used by Ren et al. \n\nI feel the contrast to Ren et al, is not provided to the degree it should be. The Appendix paragraph A4 is not sufficient in terms of explaining why this method is conceptually different or significantly better than the related approach. It is hard for me to certify the merits of their work, including explaining the experimental results.\n\nI also do not understand the significance of \"multi-model clustering\" in this context. Also, by their definition of \"variadic\", how is this more variadic than Ren et al. or Snell et al.?\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A work lacking clarity",
            "review": "This work proposes a learning method based on deep subspace clustering. The method is formulated by identifying a deep data embedding, where clustering is performed in the latent space by a revised version of k-means, inspired by the work [1]. In this way, the proposed method can adapt to account for uni-modal distributions. The authors propose some variations of the framework based on soft cluster assignments, and on cumulative learning of the cluster means.\nThe method is tested on several scenarios and datasets, showing promising results in prediction accuracy.\n\nThe idea presented in this work is reasonable and rather intuitive. However, the paper presentation is often unnecessarily convoluted, and fails in clarifying the key points about the proposed methodology. The paper makes often use of abstract terms and jargon, which sensibly reduce the manuscript clarity and readability. For this reason, in my opinion, it is very difficult to appreciate the contribution of this work, from both methodological and applicative point of view. \n\nRelated to this latter point, the use of the term “Bayesian nonparametric” is inappropriate. It is completely unclear in which sense the proposed framework is Bayesian, as it doesn’t present any element related to parameters inference, uncertainty estimation, … Even the fact that the method uses an algorithm illustrated in [1] doesn’t justifies this terminology, as the clustering procedure used here only corresponds to the limit case of a Dirichlet Process Gibbs Sampler when the covariance parameters goes to zero. Moreover, the original procedure requires the iteration until convergence, while it is here applied with a single pass only. The procedure is also known to be sensitive to the order by which the data is provided, and this point is not addressed in this work. \n\nFinally, the novelty of the proposed contribution is questionable. To my understanding, it may consist in the use of embedding methods based on the approach provided in [1]. However, for the reasons illustrated above, this is not clear. There is also a substantial amount of literature on deep subspace embeddings that proposes very similar methodologies to the one of this paper (e.g. [2-5]).  For this reason, the paper would largely benefit from further clarifications and comparison with respect to these methods.  \n\n\n\n\n\n[1] Kulis and Jordan,  Revisiting k-means: New Algorithms via Bayesian Nonparametrics, ICML 2012\n\n[2] Xie, Junyuan, Ross Girshick, and Ali Farhadi. \"Unsupervised deep embedding for clustering analysis.\" International conference on machine learning. 2016.\n[3] Ji, Pan, et al. \"Deep subspace clustering networks.\" Advances in Neural Information Processing Systems. 2017.\n[4] Jiang, Zhuxi, et al. \"Variational deep embedding: An unsupervised and generative approach to clustering.\" IJCAI 2017\n[5] Kodirov, Elyor, Tao Xiang, and Shaogang Gong. \"Semantic autoencoder for zero-shot learning. CVPR 2017.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}