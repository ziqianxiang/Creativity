{
    "Decision": {
        "metareview": "This paper proposes a new type of activations function based on q-calculus. The reviewers found that the papers is significantly lacking in its presentation, in clarity, and in its experimental evaluation. The motivation of the method raises several significant questions to the reviewers, and the proposed method is not sufficiently compared to existing approaches for (noisy) activation functions. After reviews, the authors have failed to present any updates to their paper.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Lacking in presentation and in experimental evaluation"
    },
    "Reviews": [
        {
            "title": "Interesting theory paper, needs emphasis on usefulness in practice.",
            "review": "The authors introduce concept of q-calculus into neural networks along with its advantages. They define a family of stochastic activation functions based on standard functions together with q-calculus.\n\nI have a single question, if the proposed stochastic activation functions can also be achieved through deterministic neurons together with noise schemes like dropout (or any others)? If yes, is it still useful to use q-neurons. sorry, if I missed something obvious.\n\nAs mentioned by the authors the experiments only showcase a slight improvement in performance which may not be consistent when tried across larger set of experiments.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "My neurons activate unanimously to vote NO for this submission",
            "review": "\n############ Updated Review #################\n\nI have read the author(s)' rebuttal. My decision stays unchanged. In my opinion, this first step is not significant enough, and the presentation is clearly below the acceptance threshold for ICLR. Additionally, the author(s) did not update their submission to reflect the changes. I thereby recommending rejection to this submission. \n\n##########################################\n\nThis work proposes to replace the regular deterministic activation functions used in artificial neural nets with stochastic variants. In particular, the author(s) considered the q-derivatives of standard activation functions. \n\nThe author(s) claimed that ``By Proposition 2, the p-derivative of the q-activation g_q(x) agrees with the original activation function f.'' I have trouble understanding this. I assume by original activation function the author(s) meant f(x), then how can Eqn (4) agree with f(x)?\n\nAt the bottom of pp. 3, the author(s) wrote: ``q-neuron ... combines stochasticity and some second-order information in an easy-to-compute way.'' I definitely can not agree with this point. Basically, q-neuron is the ``derivative'' of the original activation function, so there is no surprise that its derivative links to the second derivative of f(x). I can always use the high order derivative of some function as activation and claim now we are combining even higher order information into the neural network, but does that help? I don't think so. \n\nIt really annoys me to see that four out of the eight pages are occupied by gigantic figures, which should be placed in supplementary material in my opinion. A simple table could do the job equally well in the text. We are not interested in nitty-gritty details on how the training evolves. Let alone the datasets tested are all small-scale image classification tasks. At least the author(s) should diversify their test beds (e.g., NLP tasks and ImageNet scale experiments) and model architectures (e.g., RNN, ResNet). \n\nWhat's also missing from their experiments is a fair comparison with the real counterparts. I do not see comparisons with dropouts, and to more direct activation function randomization schemes (additive noise to regular activation functions). \n\nTo summarize, I can not approve this paper as it falls well below the acceptance level of an ICLR. In its current form, it's more like a sketchy note rather than a serious academic paper. I would encourage the authors to significantly enrich the content of this writing before considering resubmitting to another venue. \n",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "On the adoption of q-activations",
            "review": "The authors describe q-activation functions, stochastic relatives of common activation functions used in neural networks.  It seems like the main argument is to use them because you get a performance improvement with them. \n\nWhile the experiments appear to show better training at early epochs, none of the models appear to have been trained to convergence.  Additional justifications for why (or when) to use this should be described.\n\nWhy does the method outperform particularly when dropout is included?\n\nI also expect the lack of monotonicity in the q-activation functions to lead to the creation of (exponentially) more local minima.  Any comments?\n\nQuality: the experiments need some further work.\nClarity: aside from a few points, the paper is written clearly.\nOriginality: the work appears original to me\nSignificance: TBD, but the main argument appears to be that it leads to empirical comparative gains (but on networks not designed to be SOTA).\n\nSmall points:\n\"By prop 2, g_q(x) agrees with with original activation function\".  What does \"agrees with\" mean?\n\"Fig 2. Darker color --> lighter color?\"\n\"(Conclusion) ... can goes[sic] deeper on the error surface.\" To me, the experiments only show marginally better performance",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}