{
    "Decision": {
        "metareview": "The paper presents a CNN that is trained from human games to predict which actions to take for China Competitive Poker (Dou dizhu).\n\nThe paper is poorly written, not because of the English, but because it is hard to understand the details of the proposed solution: it is not straight-forward to reimplement a solution from the presentation in the paper. It lacks explanations for several design decisions. This is unfortunate, as the authors point out in the rebuttal that they actually did way more experiments that are presented in the paper. Moreover, the experimental results lack comparisons to baselines, ablations, so that the proposed solution could be evaluated fairly.\n\nIn its current state, this paper can not be accepted for presentation at ICLR 2019.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Not clear enough, lacking details"
    },
    "Reviews": [
        {
            "title": "Good performance results, but not much scientific contribution",
            "review": "This paper provides a system to play CCP using some deep learning. The system consists of  three modules - the bid module, which is rule based, and the policy and kicker networks, which are simple convolutional neural networks. The authors use a dataset of 8 million game records consisting of 80 million state action pairs, and train the network in a supervised fashion. The resulting model is able beat MicroWe, the current state of the art in playing CCP, and even are able to beat a few \"top amateur players\"\n\n- Why is the bid module also not learned? It seems like the feature set for the bid module is fairly simple, and a linear or MLP can do fairly well compared to a rule based module.\n- It's not clear that separating the policy and kicker networks would be more advantageous than combining them. Thousands of actions is not a too large number - language modeling work routinely deals with outputting many more classes than that.\n- Were the convolutions chosen 1D, 2D, or 3D? The figure seems to imply that the convolutions were over the XZ dimensions, with Y as the channel dimension. If so, this doesn't make too much sense to be, since the Z dimension is not uniform - the last index is all unseen cards, which is significantly more than the middle indices of \"what was played in this round\". There shouldn't be a lot of translational invariance in the Z dimension. I'm also not convinced that translational invariance is helpful in the X dimension.\n- There were no comparisons with baseline models or different model architectures. I would like to see some results on the same structure, but with an Linear model, MLP or LSTM across the time dimension, or search through different types of convolutional networks.\n- What hyperparameters were searched through in the learning process?\n- Missing citations for MicroWe being the best CCP AI, and citations for the accomplishments of the top amateur players.\n- How far away are the top amateur players from professional players? Please provide some context on how far this system is from solving CCP.\n- Fig 3, 4 should just say #of games instead of \"iteration\"\n\nThis paper shows that one choice for a supervised learning system on a CCP game database can achieve amateur level human play. It does not give insight to why the system was designed this way, why the model choices were made, and how good simpler baselines might be able to achieve. The paper is not clearly written enough, and does not provide enough scientific value to be accepted to the conference.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "promising performance, left more mysteries than observations",
            "review": "The authors propose a model that learns to play the China Competitive Poker game. The model uses CNN to predict the actions, and is trained from actual human game records. The model is shown to beat the current best AI and human amateur players.\n\nThe performance is certainly strong (if it were true). But given the double-blinded policy, there is literally no way to verify the correctness of the performance---in other words, the paper is currently not reproducible at all. So the following comments are based on the trust-worthiness of the paper.\n\n(1) immature writing: The writing lacks formality and looks like a final project report. For instance, the super-short Section 2 is rather unprofessional---it is hard to believe that the related works can be described within two paragraphs anyway. Even as someone who understands the game of China Competitive Poker, I find it very hard to follow Section 3. There is a big room for improving the English writing.\n\n(2) ill-illustrated specialty of the model: In particular, it is not clear why the model should be superior than other modeling choices. For instance, what role does the neighboring connections of CNNs play? What are the cons and pros of choosing CNNs? Are there strong motivations to design the model this way? \n\n(3) many unanswered mysteries: why does the model trained with human records readily super-human? Note that this is controversial to common imitation learning where the typical performance is bound by the human-level performance. Even though authors claimed in the response that there are \"many professional records\"---but how many is many? Did the authors analyze the records and separate the professional versus amateur ones?",
            "rating": "2: Strong rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}