{
    "Decision": {
        "metareview": "This work proposes a method for both instance and feature based transfer learning. \nThe reviewers agree that the approach in current form lacks sufficient technical novelty for publication. The paper would benefit from experiments on larger datasets and with more analysis into the different aspects of the proposed model. \n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Insufficient Novelty"
    },
    "Reviews": [
        {
            "title": "Recommend Reject as the contribution is incremental ",
            "review": "This paper proposed to solve the instance-based transfer learning and feature-based transfer learning by stacking with a two-phase training strategy. The source data and target data are hybrid together first to train weak learners, and then the ensembled super learner is utilized to get the final prediction. Details for the stacking process are provided. Experimental results on MNIST-USPS, COIL, and Office-Caltech datasets show the proposed method can boost the performance, compared to TrAdaboost. \n\nPros:\nThe paper proposes to using stacking or ensembling to solve the domain adaptation problem, which shows some insight for further domain adaptation research.\n\nCons:\n1. One of the main issues of this paper is the lack of novelty. The framework is incremented from the previous domain adaptation method such as TrAdaboost or BDA. For feature-based transfer learning, Equation (7)(8)(9) directly from the previous method. \n2. Some arguments in this paper are not solid. For example, in the abstract,   the authors claim that under the two-stage training architecture, the fitting capability and generalization capability can be guaranteed at the same time. However, this is not well-justified in the following literature. Another example is \"the settings of \\lamda and N should be taken into consideration, if \\lambda is too large, the performance of each learner can't be guaranteed, if \\lambda is too small, training data can't be diversified enough\" (page 7line 9~11)\n3. This paper is weakened by the experimental part. Firstly, only TraDaboost method is used as a baseline. The paper can be largely improved by comparing with the state-of-the-art ensembling method for domain adaptation, for example:\nSelf-ensembling for visual domain adaptation, Geoff French,  ICLR 2018.\nSecondly, the datasets used in this paper is small-scale and biased. It would be exciting to see how the proposed method will perform on the state-of-the-art large-scale domain adaptation dataset, for example, Office-Home dataset, Syn2Real dataset. \n\n Others:\n1. Some terminologies used in this paper are confusing: (1) the h_t and c are not defined in Equation (2). in Algorithm 2, how to construct kernel matrix K_t using k_t?   \n2. The written of this paper can be largely improved. Some sentences are grammarly mistaken. Typos examples: \nAbstract line 1: overtting -> overfitting\nSection 2.1, we use TraAdaboost -> We use TrAdaboost\n3. The citation style used in this paper is not correct.\n\nProblems:\n1. In section 2.2, what's the difference between the kernel matrix K with the unbiased estimate of MK-MMD (proposed by Gretton, NIPS 2012, also used in Deep adaptation network, Long, et al. ICML2015)?",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Recommend rejection of the paper due to the limited contributions and preliminary experiments.",
            "review": "The authors proposed a stacking method for both instance-based and feature-based transfer learning based on a two-phase strategy. It first introduced some self-defined parameters to diversify the data or the model, and then adopted some existing transfer learning to train the model.\n\nStrength:\n1) A stacking method for both instance-based and feature-based transfer learning.\n\nWeakness:\n1) Incremental contributions and limited novelty.\n2) Some claims are not well supported.\n3) Experimental results are preliminary.\n\nThe technical contribution of this work is limited. The main difference between the proposed instance-based stacking method and TrAdaboost are twofold: 1) using a self-defined threshold to select a subset of source samples for training; 2) using stacking instead of TrAdaboost to get the final output. The improvements upon TrAdaboost are marginal. Also, for the proposed feature-based stacking method, it just used the different kernel parameter values to diversify the model. The novelty is trivial. \n\nSeveral claims in the paper are not well discussed and/or evidence-supported, such as：\n1) “When the similarity between domains is low, the final estimator can still achieve good performance on target training set.” \n2) “When source domain is not related enough, stacking performs better.”\n3) “We reduce the risk of negative transfer in a simple and effective way without a similarity measure.”\nMore discussions should be given, for example, how to measure the domain similarity and how to reduce the risk of negative transfer. Also, there are no experimental results to support the claims.\n\nFor the evaluation, it is inappropriate to choose Randomforest as the weak leaners since Randomforest is an ensemble method. It is better to give some explanation on how the data distribution and similarity between domains change with the kernel parameters in Figure 5.\n\nFor the algorithm comparison, only TrAdaboost is used as the baseline to compare with the proposed instance-based stacking method. The results could be more convincing if some recent ensemble-based transfer learning methods are included for comparison. For the evaluation of the proposed feature-based stacking method, the authors should at least compare their method with BDA since BDA is used as its base algorithm.\n\nSome symbols used in equations are not defined, such as h_t and c in Equation (2).\n\nThe paper needs a careful proofreading to correct the grammar errors and typos, such as:\n1) Line 1 of page 7: moreover -> Moreover?\n2) Line 1 of Abstract: overtting -> overfitting?\n\nIn summary, the paper has to make significant improvements before it can meet the bar of ICLR.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "No technical contributions, a lot of typos and grammar errors",
            "review": "In this paper, the authors proposed to learn a stacked classifier on top of the outputs of well-known transfer learning models for transfer learning. The authors claimed that their proposed solution can avoid negative transfer.\n\nTechnically, there are no contributions. The proposed solution is a straight-forward A+B, where both A and B are well-known. Specifically, in the proposed solution, different well-known transfer learning models are used as the 1st level classifiers to generate intermediate outputs, then a stacked classifier is trained with the intermediate outputs as its inputs. Stacking techniques are also well-known in ensemble learning. Therefore, I do not see any new technical ideas. \n\nMoreover, the proposed solution cannot really avoid negative transfer. If two domains are indeed very different, the performance of the basic transfer learning models would be very bad, e.g., worse than random guess. In this case building a stacked classifier cannot help to boost the final performance. \n\nThe datasets used to conduct experiments are all of toy sizes.\n\nThere are a lot of typos and grammar errors. The format of citations in the main text are incorrect. \n\nIn summary, the quality of this paper is far below the standard of top conferences.",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}