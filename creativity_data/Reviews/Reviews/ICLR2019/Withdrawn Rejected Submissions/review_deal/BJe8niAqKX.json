{
    "Decision": "",
    "Reviews": [
        {
            "title": "Idea not convincing enough",
            "review": "The paper proposes a method to use videos paired with captions to improve sentence embeddings. They report improved performance on several semantic similarity benchmarks. I do not find the ideas significant. The cluster hypothesis and the perceptual hypothesis should both be naturally captured by projections between visual and textual space. The improvements over projections are also minimal. \n\nI was curious about how the model performance compares to the best model on STS (not just among multimodal baselines).   \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Text-and-video based sentence representations",
            "review": "This submission proposes a model for sentence learning sentence representations that are grounded, based on associated video data. The model consists in a loss function composed of three terms. The first one is a text-only loss, for instance skip-thought. The second one pushes closer together representations of sentences that are describing the same video. The third one enforces a good correlation between similarity scored computed from text and video representations. The obtained representations are evaluated on standard transfer tasks for NLP and caption and image retrieval on COCO.\n\n- First of all, the proposed model has very weak performance, bellow a simple baseline such as average fasttext vector + logistic regression on tasks that are part of senteval. This raises doubts about the importance of grounding, as the baseline sentence representation does not even beat such a trivial baseline. Even for caption and image retrieval, better results could probably be obtained with a CCA between average word vectors and say VGG features.\n\n- Results in Table 4 are hard to parse. Please avoid reporting all possible variants of the model, and move these comparison to a separate ablation study. Moreover, the table lacks many simple baselines, which beat the proposed approach with no visual information whatsoever. \n\n- The related work section does not mention works on image or video captioning. Here is a loose list of at least a few models that cope with two modalities and would be worth mentioning, while the related work states that \"visual grounding of sentences is quite new\":\n- Barnard, Kobus, Pinar Duygulu, David Forsyth, Nando de Freitas, David M. Blei, and Michael I. Jordan. \"Matching words and pictures.\" Journal of machine learning research 3, no. Feb (2003): 1107-1135.\n- Hodosh, Micah, Peter Young, and Julia Hockenmaier. \"Framing image description as a ranking task: Data, models and evaluation metrics.\" Journal of Artificial Intelligence Research 47 (2013): 853-899.\n- Regneri, Michaela, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. \"Grounding action descriptions in videos.\" Transactions of the Association of Computational Linguistics 1 (2013): 25-36.\n- Socher, Richard, Andrej Karpathy, Quoc V. Le, Christopher D. Manning, and Andrew Y. Ng. \"Grounded compositional semantics for finding and describing images with sentences.\" Transactions of the Association of Computational Linguistics 2, no. 1 (2014): 207-218.\n\nOverall, I think that the model proposed in this work is barely novel and the experimental results are very weak. The proposed approach can be outperformed by a very simple word-vector-based baseline. Moreover, the submission lacks a good discussion of work on joint modeling of visual and textual domains. ",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Nice idea but I am not sure about the empirical evaluation.",
            "review": "This paper presents a method for improving text-based sentence embeddings through a joint multimodal framework. \n\nDifferent from typical multimodal work, the proposed model DOES NOT project text and vision derived representations into a single space. Instead the proposed model uses a joint loss function that allows text-derived representations and the vision-derived representations to influence each other, while remaining in their separate representation spaces. In particular the loss functions ensure the following:\n\nSentences that describe the same image should have similar text-derived representations. The similarity of text-derived representations of a pair of sentences should correlate with similarity of the representations of their corresponding images.\n\nThe paper then describes multiple experiments that test whether the vision-based loss functions lead to improvements over text-only representations and if this method of incorporation vision information is better than projection based methods for multimodal representations. \n\nStrengths:\n\n1. The problem is well motivated and the approach is interesting in that it tries to avoid a difficult problem of projecting text and visual features into a single space. These modalities don’t often have 1-1 correspondence in the kinds of information they contain. \n2. The main idea can be implemented in a simple fashion using loss functions, without requiring any changes to the original models that compute the representations.\n\nIssues:\n\nThe main issue I had is with respect to the empirical evaluations. In general I found the presentation hard to follow and the results don’t look convincing for an end application. Here are some follow up questions:\n\n1. The experimental results in Table 4 seem to suggest that there is no single setting works consistently better than the projection method across all tasks. \n\n2. Skipthought’s spearman rho numbers in [Kiros et al., 2015] is much higher (~77 whereas whats reported in Table 1 here is only 52). Is this due to simple testing of the cosine distance as opposed to training?\n\n3. Temporal grounding discussion was unclear for me. There appears to be an attention model that somehow picks out relevant frames using the word embeddings and the frame representation. There seem to be two corresponding vectors that are used but how is this turned into a softmax weight? Is there a dot product that is missing somewhere?\n\n4. Are the sentences associated with each frame treated separately (or) all sentences for a video simply treated as multilabels?\n\n5. Can you argue whether the gains arise due to vision related information or because this setting simply introduces some additional training data . It would be cleaner to control for this if possible (ensuring that the text only and grounded models have same amounts of training somehow). Maybe this is a non issue because the size of the text available from the vision side (i.e. the # of sentences associated with the videos) is orders of magnitude smaller to what you get for text alone. I suspect this is the case but it would be helpful to point this out or control for it. \n\n6. I did not understand the explanation for random frame choice . What do you mean by “even when anchors bear no perceptual semantics”?\n\nTo summarize, I liked the main idea behind the paper. It is simple and avoids a tricky issue of grounding different modalities in one place. The empirical evaluation is a bit hard to follow because of missing details and the results with respect to end tasks does not look convincing.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}