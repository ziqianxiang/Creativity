{
    "Decision": {
        "metareview": "This paper conducts experiments evaluating several different metrics for evaluating GAN-based language generation models. This is a worthy pursuit, and some of the evaluation is interesting.\n\nHowever as noted by Reviewer 2, there are a number of concerns with the execution of the paper: evaluation of metrics with respect to human judgement is insufficient, the diversity of the text samples is not evaluated, and there are clarity issues.\n\nI feel that with a major re-write and tighter experiments this paper could potentially become something nice, but in its current form it seems below the ICLR quality threshold. ",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Valuable goal, but execution is somewhat suspect."
    },
    "Reviews": [
        {
            "title": "Experimental paper studying an important problem with insufficient / unsurprising conclusions",
            "review": "This paper tackles the problem of evaluation of language generation models and particularly focuses on the comparison between GAN-based language models (GAN-LM) vs likelihood-based language models (MLE-LM). Studying the behaviour of current evaluation metrics for language generation as well as finding new ones is an important research topic. I believe that this paper makes a step in the right direction but the magnitude of that step may be insufficient for publication. I appreciate the efforts but I find most of the findings about BLEU not being a good metric and characteristic of reverse PPL rather unsurprising. The majority of the paper is dedicated to describing models / metrics which are well-known instead of performing more solid experimental evaluation (Results start at page 6). Instead, the authors could have focused more on the study of FD for language generation.\n\n-- Details\n\n-\"Our main finding is that, when compared carefully, a conventional neural Language Model performs at least as well as any of the tested GAN models\", however the authors don't compare with the recent MaskGAN model, which, according to (https://arxiv.org/abs/1801.07736) outperforms MLE variants.\n\n- \"We demonstrate that previously used n-gram matching, such as BLEU scores, is an insufficient metric\": the fact that BLEU is not ideal for evaluation natural language generation has been pointed out in multiple related papers (e.g. https://arxiv.org/abs/1603.08023) and thus is not surprising.\n\n- \"We find that reporting results from the best single run or not performing sufficient tuning introduces significant bias in the reported results\": as the authors point out in related works, the variance in GAN results which hinders meaningfulness of the reported results is a also a well-known problem (e.g. https://arxiv.org/pdf/1711.10337.pdf), therefore cannot be considered as a contribution.\n\n- The observed behaviour (sensitivity to mode collapse, word swap, word removal) of the \"reverse PPL\" metric is pretty much expected, but I agree some experimental results are still interesting.\n\n- On the contrary, I liked the study on the FD metric but I would have loved the paper to focus more on the study of the behaviour of that metric: for example, by examining the robustness under different base models, while the authors only test with the model by Conneau et. al, 2017.\n\n- It would have been good to train a state-of-the-art language model architecture, e.g. AWD-LSTM, and to control regularization. I cannot see if the MLE-LM model is overfitting or not.\n\n-- Style remarks:\n\n- Moving the figures closer to the paragraph where they are described avoids the reader the burden of going back and forth through the paper.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Valuable goal, but very limited executions with incorrect claims",
            "review": "The paper sets out to improve the evaluation of GAN models as e.g. the previously used BLEU score is not sensitive to semantic deterioration of generated texts. The paper claims to “propose alternative metrics that better\ncapture the quality and diversity of the generated samples”.\n\n\nStrengths:\n-\tThe paper has a valuable goal\n-\tSome of the evaluations are interesting.\n\n\nWeaknesses:\n1.\tThe claim of the paper to “propose alternative metrics that better capture the quality and diversity of the generated samples” is not met in multiple ways:\na.\tThe paper seems not to propose any new metrics but evaluate existing ones.\nb.\tThe metrics are not extensively compared to human judgments, e.g. by computing correlation. In fact, Figure 5 suggests that they are not very well correlated.\nc.\tThe diversity is not explicitly studied on generated text samples.\n2.\tThe paper concludes that the human eval “assigns better scores to the Language Model”, which is incorrect as Seq gan scores 3.49 vs. 3.37 for language model (even if the seq gan has higher variance).\n3.\tThe metrics are not very well defined, e.g. with formulas, although this is one of the central points of the paper. e.g. what are the reference the blue score is computed against?\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "this paper seems to be timely for this line of work ",
            "review": "===========================\nSince the authors did not provide a proper response to my questions, I have lowered my score from 7 to 6. I think this paper will have a good chance to be a good paper if evaluated more comprehensively, as suggested by reviewers. \n===========================\n\nContributions:\n\nThe main contribution of this paper is the study of the currently adopted evaluation metrics for textual GAN models. It was shown that BLEU and Self-BLEU scores used by previous work are insufficient to evaluate textual GAN models, and the authors propose that Frechet Distance and reverse Language Model scores can be a good complement to the above BLEU score evaluations. \n\nDetailed Comments:\n\n(1) Novelty: It seems to me that this paper is timely, as developing GAN models for text generation gains more and more attention in the research community, and it is indeed much needed to provide good evaluation methods. The proposed new metrics seem proper, and the observation that most GAN models do not yield obviously better results than conventional LM is also insightful. \n\n(2) Presentation: This paper is generally well-written and easy to follow. However, when discussing related work in section 3.1, I think one literature [*] is missed. It uses annealed softmax to approximate argmax for textual GAN. \n\n[*] Adversarial Feature Matching for Text Generation, ICML 2017\n\n(3) Evaluation: The experiments are generally well-executed, with some questions listed below.\n\nQuestions:\n\n(1) I have some concerns in terms of human evaluation. Though human evaluation is the golden metric, it seems that presenting individual sentences to human raters does not account diversity into consideration. Therefore, systems that generate high quality samples but with less diversity will get a high score in terms of human evalution. Can the authors provide some discussion on this? And if this is the case, how will this change the conclusions in this paper?\n\n(2) I understand why the authors use simplified GAN models for evaluation. However, if the models are not simplified, what the performance will be for LeakGAN and MaskGAN, for example? This seems to be relatively easy to evaluate since the code is open sourced. \n\nMinor issues:\n\n(1) I think the citation format needs to be changed. For example, in many places, it is more natural to use \"(Hassan et al., 2018)\" than \"Hassan et al. (2018)\" for example. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}