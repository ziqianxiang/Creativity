{
    "Decision": {
        "metareview": "The paper proposes an interesting idea for more effective imitation learning.  The idea is to include short actions sequences as labels (in addition to the basic actions) in imitation learning.  Results on a few Atari games demonstrate the potential of this approach.\n\nReviewers generally like the idea, think it is simple, and are encouraged by its empirical support.  That said, the work still appears somewhat preliminary in the current stage: (1) some reviewer is still in doubt about the chosen baseline; (2) empirical evidence is all in the similar set of Atari games --- how broadly is this approach applicable?",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Nice work with potential, but contributions need to be strengthened"
    },
    "Reviews": [
        {
            "title": "need more in-depth analysis",
            "review": "[Summary]\n\nThis paper presents an interesting idea that to append the agent's action space with the expert's most frequent action pairs, by which the agent can perform better exploration as to achieve the same performance in a shorter time. The authors show performance gain by comparing their method with two baselines - Dagger and InfoGAIL.\n\n\n[Stengths]\n\nThe proposed method is simple yet effective, and I really like the analogy to mini-moves in sports as per the motivation section.\n\n\n[Concerns]\n\n- How to choose the number and length of the action sequences?\nThe authors empirically add the same number of expert's action sequences as the basic ones and select the length k as 2. However, no ablation studies are performed to demonstrate the sensitivity of the selected hyperparameters. Although the authors claim that \"we limit the size of meta-actions k to 2 because large action spaces may lead to poor convergence\", a more systematic evaluation is needed. How will the performance change if we add more and longer action sequences? When will the performance reach a plateau? How does it vary between different environments?\n\n- Analysis of the selected action sequences.\nIt might be better to add more analysis of the selected action sequences. What are the most frequent action pairs? How does it differ from game to game? What if the action pairs are selected in a random fashion?\n\n- Justification of the motivation\nThe major motivation of the method is to release the burden of memory overheads. However, no quantitative evaluations are provided as to justify the claim. Considering that the input images are resized to 84x84, storing them should not be particularly expensive.\n\n- The choice of baseline.\nInfoGAIL (Li et al., 2017) is proposed to identify the latent structures in the expert's demonstration, hence it is not clear to me how it suits the tasks in the paper. The paper also lacks details describing how they implemented the baselines, e.g. beta in Dagger and the length of the latent vector in InfoGAIL.\n\n- The authors only show experiments in Atari games, where the action space is discrete. It would be interesting to see if the idea can generalize to continuous action space. Is it possible to cluster the expert action sequences and form some basis for the agent to select?\n\n- Typos\n{LRR, RLR/RRL} --> {LRR, RLR, RRL}\nsclability --> scalability\nwe don't need train a ... --> we don't need to train a ...\nAtmost --> At most\n\n\n[Recommendation]\n\nThe idea presents in the paper is simple yet seemingly effective. However, the paper lacks a proper evaluation of the proposed method, and I don't think this paper is ready with the current set of experiments. I will decide my final rating based on the authors' response to the above concerns.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Border line paper",
            "review": "The paper proposes an idea of using the most frequent expert action sequence to assist the novice, which, as claimed, has lower memory overhead than other imitation learning methodologies. The authors present comparison of their proposed method with state-of-the-art and show its superior performance. However I do have the following few questions. \n\n1. The proposed method requires a long time of GA3C training. How is that a fair comparison in Figure 2, where proposed method already has a lead over GA3C? It could be argued that it's not using all of the training outcome, but have the authors considered other form of experts and see how that works?\n\n2. The authors claimed one of the advantages of their method is reducing the memory overhead. Some supporting experiments will be more convincing.\n\n3. In Figure 3, atlantis panel, the score shows huge variance, which is not seen in Figure 2. Are they generated from the same runs? Could the authors give some explanation on the phenomenon in Figure 3?\n\nOverall, I think the paper has an interesting idea. But the above unresolved questions raises some challenge on its credibility and reproducibility. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A well written paper with a simple, yet powerfull, idea that needs further analysis",
            "review": "The paper describes an imitation reinforcement learning approach where\nthe primitive actions of the agent are augmented with the most common\nsequences of actions perform by experts. It is experimentally shown\nhow this simple change has clear improvements in the performance of\nthe system in Atari games. In practice, the authors double the number\nof primitive actions with the most frequent double actions perform by\nexperts. \n\nA positive aspect of this paper comes from the simplicity of the\nidea. There are however several issues that should be taken into\naccount:\n- It is not clear how to determine when the distribution of action\n  pair saturates. This is relevant for the use of the proposed approach.\n- The total training time should consider both the initial time to\n  obtain the extra pairs of frequent actions plus the subsequent\n  training time used by the system. Either obtained from a learning\n  system (15 hours) or by collecting traces of human experts (< 1\n  hour?). \n- It would be interesting to see the performance of the system with\n  all the possible pairs of primitive actions and with a random subset\n  of these pairs, to show the benefits of choosing the most frequent\n  pairs used by the expert.\n- This analysis could be easily extended to triplets and so on, as\n  long as they are the most frequently used by experts.\n- The inclusion of macro-actions has been extensively studied in\n  search algorithms. In general, the utility of those macros depends on\n  the effectiveness of the heuristic function. Perhaps the authors\n  could revise some of the literature.\n- Choosing the most frequent pairs in all the game may not be a\n  suitable strategy. Some sequences of actions may be more frequent\n  (important) at certain stage of the game (e.g., at the beginning/end\n  of the game) and the most frequent sequences over all the game may\n  introduce additional noise in those cases.\n\nThe paper is well written and easy to follow, there are however, some\nsmall typos:\n- expert(whose => expert (whose\n% there are several places where there is no space between a word and\n% its following right parenthesis \n- don't need train => don't need to train\n- experiments4. => experiments.\n- Atmost => At most\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}