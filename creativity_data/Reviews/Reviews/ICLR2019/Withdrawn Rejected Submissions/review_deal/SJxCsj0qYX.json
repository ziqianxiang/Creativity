{
    "Decision": {
        "metareview": "This paper proposes new GAN training method with multi generator architecture inspired by Stackelberg competition in game theory. The paper has theoretical results showing that minmax gap scales to \\eps for number of generators O(1/\\eps), improving over previous bounds. Paper also has some experimental results on Fashion Mnist and CIFAR10 datasets. \n\nReviewers find the theoretical results of the paper interesting.  However, reviewers have multiple concerns about comparison with other multi generator architectures, optimization dynamics of the new objective and clarity of writing of the original submission. While authors have addressed some of these concerns in their response reviewers still remain skeptical of the contributions. Perhaps more experiments on imagenet quality datasets with detailed comparison can help make the contributions of the paper clearer. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "ICLR 2019 decision"
    },
    "Reviews": [
        {
            "title": "Interesting view of GANs from game-theory perspective, but algorithmically the Stackelberg GAN is similar with previous multiple-generator GANs",
            "review": "This paper proposes the Stackelberg GAN framework of multiple generators in the GAN architecture. The architecture is similar with previous multiple-generator GANs (MAD-GAN and MGAN). In fact, it's even simpler in the sense that Stackelberg GAN has simpler loss function for the discriminator compared with the previous two. The authors prove that the minimax duality gap shrinks as the number of generators increases. And this proof has no assumption on the expressive power of generators and discriminator. With this proof, the authors argues that because the duality gap shrinks as the number of generators increases, the training of GANs gets more stable.\n\nFrom the algorithm part, I think the algorithm is very similar (and even simpler) than MAD-GAN and MGAN. The MAD-GAN and MGAN even proposed some specific loss for the discriminator so that it will encourage different generator to generate different modes in the target distribution. The Stackelberg GAN does not do this, but \"partially\" achieved the same goal. However, from Figure 9, we see that the simpler the generator is, the easier different generator will capture different modes. I think that this is due to the simplicity of discriminator loss. Therefore, on the algorithm part, the author may want to address the difference between Stackelberg GAN and MAD-GAN and MGAN. On the experiment part, we need to see more comparison between these three methods. In the current experiment, MGAN result is very similar to the proposed method, and MAD-GAN result is missing. Personally, I think that on cifar dataset (or larger datasets), these three methods should have very similar behavior. \n\nFrom the theoretical part, the authors derived a bound of the minimax duality gap for the Stackelberg GAN, without the assumption on the expressive power of generators and discriminator. Although the bound may not be practical, these are nice efforts. There are many typos in the paper (and appendix), which make me difficult to follow the proofs. For example, \"Let clf (bclf) be the convex(concave) closure of f, which is defined as the function whose epigraph (subgraph) is the convex\n(concave) closed hull of that of function f.\" Do we have concave closed hull of subgraph of function f? What is the concave closed hull of a set? The usage of sub(sup)-script is also very confusing, like in the definition of h_i(u_i). The authors may want to correct typos and improve the presentation. In the conclusion, the authors conclude \"We show that the minimax gap shrinks to \\eps as the number of generators increases with rate e O(1/\\eps).\" This is an over-claim, because the authors only proved this under the assumption of concavity of the maximization w.r.t. discriminators. \n\nFinally, the authors may want to provide some simple results of the Stackelberg GAN from the perspective of density approximation, even assuming infinite capacity of the discriminator set, as other GANs does. Whether the distance defined by the maximization problem a distance or divergence. If we exactly minimizing that objective function, do we get the target distribution? \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A nice approach for training multi-generator",
            "review": "This paper proposes a way of training multi-generator in the GAN setting.\nWhile a proposed approach is simply to put N generators and form a sum of GAN losses to train a model, the paper carefully presents a theoretical analysis on the duality gap, and shows as N goes infinity, the duality gap can shrink to zero.\nOne can think of this as a usual ensemble approach to increase model's capacity and performance, but the main difference to the usual ensemble approach is to form a sum of losses (ensemble losses) instead of a loss on output of ensemble.\nThe paper shows this can be more effective approach to train a multi-generator architecture and I believe that this can be an effective approach to capture multi-modal sample distributions.\nFinally, a paper is well-written and well-organized. ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The name Stackelberg GAN is misleading as the underlying problem formulation is not a Stackelberg game but (still) a zero-sum game. The argument why more data generators help is not convincing.",
            "review": "A Stackelberg competition is a nonzero-sum game where 1) each player has their own objective, which do not sum up to a constant, and 2) there is an order at which the players interact. The proposed formulation only assumes that parameters of one player (data generator) partition in I tuples \\gamma_i of parameters, where each tuple parameterizes a different data generator component (e.g., a separate neural network). Further, each of those components is assumed to contribute a term to the game's objective that only depends on the corresponding parameter tuple \\gamma_i, and the other player's parameters \\theta (e.g., weights of the discriminator). From a game theoretic perspective, this still yields a 2-player zero-sum game where the action space of the data generator is the product space of the I tuple spaces. Hence, I have doubts about the general finding that more data generating components decreases the duality gap.\n\nThe gap between the a maximin and minimax solution is determined by the shape of the objective \\phi(\\gamma,\\theta) and is zero, for example, if \\phi is (quasi) convex in \\gamma=[\\gamma_1, ..., \\gamma_I], and (quasi) concave in \\theta. The authors bound the violation of this property w.r.t. the data generator components' parameters \\gamma_i, and argue that this degree of violation is the same for the whole data generator parametrized by \\gamma=[\\gamma_1, ..., \\gamma_I] if the data generator components are from the same family of mappings (e.g., having the same network architecture). While this conclusion is true under worst cast assumption, e.g., the globally maximal possible gap, this would also imply that all data generator components find the same global best solution, that is, yield the same mapping, in which case the gap would be identical to just using one of those components.\n\nIntuitively, the only reason to have multiple data generator components is to learn different mappings such that the joint data generator -- mixing the outputs of the different components -- is more expressiv than just a single mapping. If the different mappings only result from the inability of finding the global best solution, a worst case argument is not very insightful; in this case, one should study the duality gap in the neighborhood of the starting solutions. On the other hand, if we assume a different family of mappings for each component, the convexity violation of the joint data generator is higher than for each component; hence, the gap does not necessarily decrease with more components.\n\nSo why do multiple data generator components help in practice, and why does the proposed model outperform single-component GANs and the multi-branch GAN in the experiments? Solving a maximin/minimax problem for highly non-convex-concave functions is challenging; there is an infinity of saddle point solutions which yield different \"performances\". The multi-branch GAN can be seen as a model averaging approach giving more stable results, whereas the proposed GAN seems more of an ensemble approach to stabilize the result. Though, this is speculative and I would encourage the authors to study this in-depth; the reasoning in Remark 1 is not convincing to me.\n\nUPDATE:\n\nI read the revision and stick to my vote. In the discussion, I wasn't able to get my points across, e.g., that bounding the worst case duality gap is not enough to conclude that the observed duality gap does not grow for multiple local optimal GANs, where the duality gap is expected to be much smaller. A simple experiment could be to actually measure the duality gap (flip the order of the players and measure the difference of the objectives, when starting with the same initialization). If the authors were right, the maximum of those gap should stay constant when adding more data generators. To justify a Stackelberg setting, the authors may provide an example instantiation that cannot be cast into a standard zero-sum game with minimax solution. I can't see such an example but I'm happy to be proven wrong.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}