{
    "Decision": {
        "metareview": "This paper proposes a new measure to detect memorization based on how well the activations of the network are approximated by a low-rank decomposition. They compare decompositions and find that non-negative matrix factorization provides the best results. They evaluate of several datasets and show that the measure is well correlated with generalization and can be used for early stopping. All reviewers found the work novel, but there were concerns about the usefulness of the method, the experimental setup and the assumptions made. Some of these concerns were addressed by the revisions but concerns about usefulness and insights remained. These issues need to be properly addressed before acceptance.",
        "confidence": "2: The area chair is not sure",
        "recommendation": "Reject",
        "title": "Meta-review"
    },
    "Reviews": [
        {
            "title": "Very interesting but not yet a complete work",
            "review": "The contribution of the paper is in proposing a quantitative measure of memorization based on the assumption that the activations at the deeper layers of a *generalizing* deep network should be invariant to intra-class variations. The measure corresponds to how well can the activation matrix of a batch be approximated by a low-rank decomposition. The paper proposes to use approximate non-negative matrix factorization and compares it to PCA. As for “wellness” it uses the final accuracy of the network after the activation is approximated in some layer(s).\n\nThe composition of the paper and its writing makes it an easy read. The work is novel in the way it proposes to measure memorization to the best of the reviewer’s knowledge. However, the novel insights and/or the practical usefulness of the proposed method seem very limited. Also, there are many questions that comes to my mind that I would appreciate the authors to address:\n\nSpecific questions:\nThe experimental setup is unclear:\nIs the linearization-batch taken from the training set or the test set?\nIf it is taken from the training set, for the case that p>0 (noisy labels), is the batch of a single class obtained from noisy labels or non-noisy labels? \nFor the experiments, is there only one fixed batch used? How is this batch selected? How sensitive the evaluation is to the selection of the batch members and its size?\nDo the batches cover the whole set?\n\n- Figure 2.a and 2.b: How come all networks with different label noise levels end up with the same (100%?) accuracy? Are the fixed samples different for each p? (class labels change for each p).\n\n- Figure 2.c: Why should the performance drop more when linearizing the middle layers (3_2:4_2) than the earlier layers. This seems to be in violation of the assumption about class invariance in deeper layers.\n\n- When k=1 for NMF and PCA, the difference of the activations for different samples becomes a matter of scale. In this case, shouldn’t all classifications become the same for all samples? How does this affect the accuracy? Does it make the evaluation very sensitive to the sampling of the batch? It would be interesting to study the property of the basis obtained in this border case. The same questions can be studied as one gradually increases k.\nSection 4.2: It starts with the sentence “In this section we show our technique is useful for predicting good generalization in a more realistic setting“. Indeed, the high correlation of the test performance and the proposed memorization measure in this section is very interesting. However, as for usefulness, it does not seem to provide a better criterion for early stopping or other practicalities of ReLU networks. \n\n- An experiment describing how well are the approximations (i.e. activation matrix reconstruction error) and how that correlate with memorization is missing.\n\nSome general questions that come to my mind:\n\n- the paper assumes (e.g. in page 4) that “When single-class batches are not approximately linear, even in deep layers, we take this as evidence of memorization”. I have a concern here. Apart from the last layer, this form of simplicity of the support for the intermediate layers of a good classifier does not seem to be *necessarily*. That is, it seems to me that as long as the activation matrix of each class is linearly separable from the activation matrices of the others, there is no need for it to become simpler (by reducing the intra-class variations at the deep layers) for the classification loss to be minimized. Does this mean the paper’s assumption for memorization is not necessarily valid?\n\n- The paper relates the memorization to the extent of local linearity of a deep ReLU network. ReLU networks represent piece-wise linear functions. Thus, in order for this relation to be drawn, probably different linear regions (polytopes in the input space) should be considered for the linearization of the activation matrix. In that regard, how can this empirical measurement be translated into a more formal linearity of the global function?\n\n- The rc number as well as the rank k of a good approximation directly depend on the number of samples in the batch. How can one obtain a measure that is independent of the number of samples in the batch?\n\n\nSummary judgment:\n\nThe paper puts forward an interesting observation using a novel approach. However there are questions about the experiments, discussions around the experiments and the usefulness of the observation for training better models and/or giving additional insights to what we know. Considering that, I think the paper would make a very good workshop paper but needs more work to address the bar of an ICLR conference paper. But I am open to discussion with the authors and other reviewers.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "bad clustering == memorization?",
            "review": "This paper propose a new way of analyzing the robustness of neural network layers by measuring the level of \"non-linearity\" in the activation patterns on samples belonging to the same class, and correlate that to the level of \"memorization\" and generalization.\n\nMore specifically, the paper argues that a good representation cluster all the samples in a class together, therefore, in higher layers, the activation pattern of samples from the same class will be almost identical. In this case, the activation matrix will have a small non-negative rank. An approximation algorithm (via non-negative matrix factorization) is then used to compute the robustness and evaluate the robustness (by replacing the activation matrix with its low rank non-negative activation) is measured in a number of experiments with different amount of random label corruptions. The experiments show that networks trained on random labels are less robust than networks trained on true labels.\n\nWhile the concept is interesting, I find the arguments in the paper a bit vague, and the usefulness of the algorithm might be hampered by its computation complexity, which is not discussed in the paper.\n\nFirst of all, the paper lacks a clear notion of \"memorization\". While it is generally accepted that learns on random labels can be called \"memorization\", the paper seem to be defining it as how well is the network clustering points from the same class. Several questions need to be addressed in order for this notion to be justified:\n\n1. Are (well generalized) networks really clustering samples of the same class to a centroid? It would be great if some empirical verifications are shown. Because the networks are using linear classifier in the last layer to classify the samples, it seems only linearly separability would be suffice for the work, which does not necessarily imply clustering.\n\n2. Given two networks (of the same architecture), assume somehow network-1 decides to use the first 9 layers to compute a well clustered representation, while network-2 decides to use the first 5 layers to do the same thing. Do we say network-1 is (more) memorizing in this case?\n\n3. The notion seems to be more about the underlying task than about the networks. Given the measurement, if a task is more complicated, meaning the input samples in the class have higher variance and requiring more efforts to cluster, then it seems the network will be doing more memorization. In other words, while networks will be doing more memorization when comparing a random label task to a true label task, it might also be \"doing more memorization\" when comparing learning on imagenet to learning on MNIST / CIFAR. One the one hand, this does not seem to fit our \"intuition\" about memorization; on the other hand, the heavy dependency on the underlying data distribution makes it difficult to compare results learned on different data -- especially since the measurements are based on per-class samples, \"random labels\" and \"true labels\" have very different class-conditional distributions.\n\nI also have some questions about Figure 2(c). I will continue numbering the question for easier discussion.\n\n4. Why for all cases, the lower layers all have higher AUC than the higher layers (except the last one)? The argument given in the paper is that the lower layers are the feature extraction phase while the upper layers are memorization phase. I think if clearly verified, this is a very interesting observation. But the paper currently do not have experiments to verify the hypothesis. Also more studies on this with different networks would be good. For example, with deeper networks, does the feature extraction phase include more layers?\n\n5. The p=1 and p<1 curves seem to be very different. If one is to sample more densely between p=0.8 and p=1, would there still be a clear phase transition?\n\nSome other questions:\n\n6. Can you add discussions to the computation requirements for the proposed analysis? This is especially important for the cases where the analysis is used during training as tools to help deciding early stopping.\n\n7. For the early stopping experiment, the main text says \"These include the test error (in blue)\" while in the figure the label axis is \"Test loss\". I'm assuming it is the cross entropy loss given the values are greater than 1. In this case, can you show in parallel the same plots in error rate, as the test error is more important than the test loss and the test loss could sometimes be artificially huge due to high confident mistakes on ambiguous test examples.\n\nSome minor issues:\n\n* Please proof read the paper for typos. E.g. on the 3rd paragraph of the 1st page: \"that networks that networks that\".\n\n* The convention with subplots seem to be putting sub-captions under the figures, not above.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for \"Detecting Memorization in ReLU Networks\"",
            "review": "This paper aims to distinguish between networks which memorize and those with generalize by introducing a new detection method based on NMF. They evaluate this method across a number of datasets and provide comparisons to both PCA and random ablations (as in Morcos et al., 2018), finding that NMF outperforms both. Finally, they show that NMF is well-correlated with generalization error and can be used for early stopping. \n\nThis is an overall excellent paper. The writing is clear and and focused, and the experiments are careful and rigorous. The discussion of prior work is thorough. The question of how to detect memorization in DNNs is one of great interest, and this makes nice steps towards this goal. As such, it will likely have significant impact.  \n\nMajor comments:\n\n1) The early stopping section could benefit from more experiments. In particular, it would be helpful to see a scatter plot of the time of peak test loss as a function of NMF/Ablation AuC local maxima and to measure the correlation between these rather than simply showing 3 examples.\n\nMinor comments: \n\n1) While the comparisons to random ablations are mostly fair, it is worth noting that the variance on random ablations appears to be lower than that of NMF and PCA. \n\n2) The error bars on the plots are often hard to see. Increasing the transparency somewhat would be helpful.\n\nTypos: \n\n1) Section 1, third paragraph: “We show that networks that networks that generalize…” should be “We show that networks that generalize...”\n\n2) Section 3.1, third paragraph: “Because threshold is the…” should be “Because thresholding is the…”\n\n3) Section 3.2, third paragraph: “In the most non-linear case we would…” should be “In the most non-linear case, we would…”\n\n4) Figure 2 caption: “...with increasing level of…” should be “...with increasing levels of…”\n\n5) Section 4.1.1, second to last line of last paragraph: missing space before final sentence\n\n6) Figure 4a label: “Fahsion-MNIST” should be “Fashion-MNIST”\n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}