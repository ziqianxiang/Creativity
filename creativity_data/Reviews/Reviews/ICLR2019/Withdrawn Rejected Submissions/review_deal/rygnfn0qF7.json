{
    "Decision": {
        "metareview": "This paper proposes to pre-train hierarchical document representations for use in downstream tasks. All reviewers agreed that the results were reasonable.\n\nHowever, the methodological novelty is limited. While I believe there is a place for solid empirical results, even if not incredibly novel, there is also little qualitative or quantitative analysis to shed additional insights.\n\nGiven the high quality bar for ICLR, I can't recommend the paper for acceptance at this time.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Reasonable results, limited novelty"
    },
    "Reviews": [
        {
            "title": "Reasonable method, but not too much novelty",
            "review": "Reasonable method, but not too much novelty\n\n[Summary]\n\nThe paper proposed techniques to pretrain two-layer hierarchical bi-directional or single-directional LSTM networks for language processing tasks. In particular, the paper uses the word prediction, either for the next work or randomly missing words, as the self-supervised pretraining tasks. The main idea is to not only train text embedding using context from the same sentence but also take the embedding of the surrounding sentences into account, where the sentence embedding is also context-aware. Experiments are done for document segmentation, answer passage retrieval, extractive document summary.\n\n[Pros]\n\n1.\tThe idea of considering across-sentence/paragraph context for text embedding learning is very reasonable. \n2.\tThe random missing-word completion is also a reasonable self-supervised learning task. \n3.\tThe results are consistently encouraging across all three task. And the performance for “answer passage retrieval” is especially good. \n\n[Cons]\n\n1.\tThe ideas of predicting the next word (L+R-LM) or missing words (mask-LM) have been around and widely used for a long time. Apply this idea to an two-layer hierarchical LSTM is a straightforward extension of this existing idea.\n2.\tFor document segmentation, no comparison with other methods is provided. For extractive document summary, the performance difference between the proposed method and the previous methods are very minor.\n3.\tImportantly, the experiments can be stronger if the learned embedding can be successfully applied to more fundamental tasks, such as document classification and retrieval. \n\nOverall, the paper proposed a reasonable method, but the significance of the paper can be better justified by more solid experiments.\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good incremental work showing the value of pretraining",
            "review": "Summary: \nThis paper proposes to extend the pretraining used for word representations in QA (e.g., ELMO) in the following sense: Instead of just predicting next/previous words in a sentence/paragraph, performing a hierarchical prediction over the whole document, by having a local LSTM and a global LSTM as presented in Fig. 1 + the idea of masked language model. Authors show meaningful improvements in 3 tasks that require document level understanding: extractive summarization, document segmentation, and answer passage retrieval for doc level QA. \n\nPros:\n- Good presentation and clear explanations.\n- Meaningful improvements in various tasks requiring document level understanding.\n\nCons:\n- Novelty is mainly incremental\n\nMinor comment: \n- Use a bigger picture for Fig. 1\n- In page 1, Introduction, paragraph 2, line 10, \"due the long-distance ...\" ==> \"due to the long-distance ...\"\n\n**********\nI would like to thank authors for their feedback. After reading their feedback I still believe that novelty is incremental and would like to keep my score. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good contribution with a few missing baselines and implementation details ",
            "review": "In this work, the authors explore different ways to pre-train contextualized word and sentence representations for use in other tasks. They propose two main methods: a straight-forward extension of the ElMO model for hierarchical uni-directional language models, and a de-noising auto-encoder type method which allows to train bi-directional representations. The learned contextual representations are evaluated on three downstream tasks, demonstrating the superiority of the bi-directional training setting, and beating strong baselines on extractive summarization.\n\nThe method is clearly presented and easy to follow, and the experiments do seem to support the author's claims, but their exposition misses several important details (or could be presented more clearly). For the document segmentation task, are the articles taken from a held-out set, or are they contained in the pre-training set? For passage retrieval, is the representation the same or are the representations re-trained from scratch using paragraph blocks? What exactly are the other features (those can go in the appendix)? And for the extractive summarization task, how many sentences are selected? Is pre-training also done on Wikipedia, or are those representations trained on news text?\n\nA comparison to non-contextualized sentence representations would also be welcome (SkipThought, InferSent, ElMO-pool for settings other than passage retrieval). Note also that the local pre-training is not equivalent to ElMO, as the later sees context form the whole document rather than just the current sentence.\n\nIt is interesting to see that contextualized sentence representations can be used and that the Mask-LM objective yields better results than L+R-LM, but these points would be better made if the above questions were answered.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}