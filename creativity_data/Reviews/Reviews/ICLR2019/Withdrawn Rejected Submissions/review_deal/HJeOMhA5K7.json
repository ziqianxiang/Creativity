{
    "Decision": {
        "metareview": "The paper considers the task of incorporating knowledge expressed as rules into column networks. The reviewers acknowledge the need for such techniques, like the flexibility of the proposed approach, and appreciate the improvements to convergence speed and accuracy afforded by the proposed work.\n\nThe reviewers and the AC note the following as the primary concerns of the paper:\n(1) The primary concerned raised by the reviewers was that the evaluation is focused on whether KCLN can beat one with the knowledge, instead of measuring the efficacy of incorporating the knowledge itself (e.g. by comparing with other forms of incorporating knowledge, or by varying the quality of the rules that were introduced), (2) Even otherwise, the empirical results are not significant, offering slight improvements over the vanilla CLN (reviewer 1), (3) There are concerns that the rule-based gates are introduced but gradients are only computed on the final layer, which might lead to instability, and (4) There are a number of issues in the presentation, where the space is used on redundant information and description of datasets, instead of focusing on the proposed model.\n\nThe comments by the authors address some of these concerns, in particular, clarifying that the forms of knowledge/rules are not limited, however, they focused on simple rules in the paper. However, the primary concerns in the evaluation still remain: (1) it seems to focus on comparing against Vanilla-CLN, instead of focusing on the source of the knowledge, or on the efficacy in incorporating it (see earlier work on examples of how to evaluate these), and (2) the results are not considerably better with the proposed work, making the reviewers doubtful about the significance of the proposed work.\n\nThe reviewers agree that the paper is not ready for publication.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Poor results, Evaluating knowledge or method to incorporate it?"
    },
    "Reviews": [
        {
            "title": "A modified column network",
            "review": "This work proposes a variant of the column network based on the injection of human guidance. The method does not make major changes to the network structure, but by modifying the calculations in the network. Human knowledge is embodied in a defined rule formula. The method is flexible and different entities correspond to different rules. However, the form of knowledge is limited and simple. Experiments have shown that the convergence speed and results are improved, but not significant.\n\nMinorï¼š\nExample 2: \"A\" -> \"AI\".",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting problem, not well executed idea",
            "review": "The  paper  introduces  a  method  to  incorporate  human  advises  to  deep  learning  by  extending  Column  Network  (CLN)  -  a  powerful  graph  neural  network  for  collective  classification. \n\nThe  problem  is  quite  interesting  and  is  practical  in  real-world. However, I have some concerns:\n\nCorrectness\n==========\nIn the main modification to the CLN in Eq (3), the rule-based gates are introduced to every hidden layer. However, the functional gradient with respect to the \"advise gradient\" is only computed for the last layer (at the end of Section 3). The exponential gates may cause some instability issue due to its unboundedness. \n\nEvaluation\n=========\nThe  questions  in  experiment  (Can  K-CLNs  learn  efficiently/effectively  with  noisy  sparse  samples?)  do  not  support  the  problem  statement  about  human  advice  incorporation.  Thus,  all  they  did  in  the  experiment  is  trying  to  compete  against  CLN.\n\nI would believe that the improvement (which I trust is real) depends critically on the quality and quantity of the human-crafted rules, much in the same way that feature engineering plays the major roles in the classical structured output prediction. Hence more details about the rules set used in experiments should be given.\n\nPresentation\n===========\nIn  the  experiment  part,  the  authors  need  to  describe  their  model  configuration.  The  presentation  of  datasets  consumes  a  lot  of  space  and  can  be  reduced (e.g., using a table).  This  paper  displays  many  unnecessary  figures  that  consumes  a  lot  of  space.  The  paper  provides  some  unnecessary  text  highlights  in  bold.  \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "If your human guidance is totally wrong, how your model handle such extreme cases?",
            "review": "This paper formulates a new method called human-guided column networks to handle sparse and noisy samples. Their main idea is to introduce human knowledge to guide the previous column network for robust training.\n\nPros:\n\n1. The authors find a fresh direction for learning with noisy samples. The human advice can be viewed as previledged information.\n\n2. The authors perform numerical experiments to demonstrate the efficacy of their framework. And their experimental result support their previous claims.\n\nCons:\n\nWe have three questions in the following.\n\n1. Motivation: The authors are encouraged to re-write their paper with more motivated storyline. The current version is okay but not very exciting for idea selling. For example, human guidance should be your selling point, and you may not restrict your general method into ColumnNet, which will limit the practical usage.\n\n2. Related works: In deep learning with noisy labels, there are three main directions, including small-loss trick [1], estimating noise transition matrix [2,3], and explicit and implicit regularization [4]. I would appreciate if the authors can survey and compare more baselines in their paper instead of listing some basic ones.\n\n3. Experiment: \n3.1 Baselines: For noisy labels, the authors should add MentorNet [1] as a baseline https://github.com/google/mentornet From my own experience, this baseline is very strong.\n3.2 Datasets: For datasets, I think the author should first compare their methods on symmetric and aysmmetric noisy data. Besides, the authors are encouraged to conduct 1 NLP dataset.\n\nBy the way, if your human guidance is totally wrong, how your model handle such extreme cases? Could you please discuss this important point in your paper?\n\nReferences:\n\n[1] L. Jiang, Z. Zhou, T. Leung, L. Li, and L. Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.\n\n[2] G. Patrini, A. Rozza, A. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017.\n\n[3] J. Goldberger and E. Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In ICLR, 2017.\n\n[4] T. Miyato, S. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: A regularization method for supervised and semi-supervised learning. ICLR, 2016.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}