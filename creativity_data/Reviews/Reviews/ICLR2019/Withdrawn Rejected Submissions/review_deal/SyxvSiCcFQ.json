{
    "Decision": {
        "metareview": "This paper studies the problem of training binary neural networks using quantum amplitude amplification method. Reviewers agree that the problem considered is novel and interesting. However the consensus is that there are only few experiments in the current paper and the paper needs more experiments on different datasets with comparisons to proper baselines. Reviewers opined that the paper was not so easy to follow initially,  though later revisions may have somewhat alleviated this problem.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "ICLR 2019 decision"
    },
    "Reviews": [
        {
            "title": "Review TLDR: good paper interesting subject good fit for ICLR maybe even better for QIP ",
            "review": "Neural Network Cost Landscapes as Quantum States\n\nThe authors describe a method where a deep learning framework can be quantised, this is done by considering the two state form of a Bloch sphere/qubit and mapping binary neural network. onto the quantum object creating a quantum binary neural network\n\nI have to say I liked the paper, it is indeed novel and I haven’t seen this anywhere else. More then that, it addresses the quantum aspects of deep learning which has only recently started getting so much attention rather the regular machine learning algorithms. And such I think it’s a good fit for ICLR.\n\nWith that I have a few concerns/ nitpicking issues I would like the authors to address if possible\n1. While authors show basically a discreet network (much like Soudry’s work) there has also been recently a show of continues variable networks (https://arxiv.org/pdf/1806.06871.pdf), how does your scaling compare to that ? Could one think of this is the continuum limit (albeit you showed a *very* small system) of your model ?\n2. As a general style remark there is a lot of introduction on quantum information and I wondering if authors could just reference classical text such as Isaac Chuang, Michael Nielsen or for the CS flavour Classical and Quantum Computation  ? I would have liked to see all 3.1,3.2 maybe in an appendix and a lot more details on the experiments and setup for example \n3. At the end of section 3.2 authors mention the lack of correspondence principle in some quantum systems, I would be happy for a refinement in the aspect of quantum computing that is true and also in general quantum mechanics but there is a huge body of work and an entire field dedicated to just that, settling the difference in correspondence principle and it’s meaning (for example Chaos and the semiclassical limit of quantum mechanics (is the moon there when somebody looks?) by Michael Berry. While this is defiantly far from a deal breaker I would be happy for a bit more clarification on subtle difference. My guess is that authors are referring to the fact that an essential part of quantum computing is the lack of correspondence that can also be taken advantage of for quantum parallelism/ quantum speed ups ? Same for the last paragraph of 3.1.\n4. In the method section  all of this procedure is described for a fault tolerant machine, can you say something about coupling to error correction codes for near term quantum devices ?\n5. On the same note, there is a feeling that non linearities are swept under the rug, do you have a hunch how can one use non linear activation functions ? (General question, you don’t really have to answer and can take the fifth )\n6. In the problem statement (4.2)  x is in fact the binary representation of the qubit state ?\n7. In your method Fig. 2, should there be some sort of measurements ?\n8. Can you elaborate what exactly goes into the U gates and how are they constructed in Fig.2 ? Or is it some  oracle model that can do all the actions \n9. You report a quadratic speedup, is there some relationship to random walk  is this in fact a case of a RW search over the parameter space ?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "More appropriate for quantum computing literature",
            "review": "Review of \"Neural Network Cost Landscapes as Quantum States\"\n\nPaper summary:\n\nThe paper proposes a new algorithm \"quantum amplitude amplification\"\nfor training and model selection in binary neural networks. (in which\nboth weights and activations are restricted to the set -1, 1)\n\nSection 2 references related work and gives some motivation, that some\nquantum algorithms scale better (in terms of big-O notation) than\nclassical algorithms.\n\nSection 3 explains the basics of quantum computing (qubits and quantum\ngates).\n\nSection 4 explains the proposed method. There are two toy\nproblems. The binary neural network has 8 weight parameters. There are\nhelpful Figures 1-2 which explain the network structure and the\nquantum circuit.\n\nSection 5 explains the results of using the proposed method in a\nquantum computer simulator (not an actual quantum computer). On the\ntwo toy problems the paper observes quadratic speedups with respect to\na brute force search.\n\nComments:\n\nA strong point is that the paper is very well-written and easy to\nunderstand. \n\nHowever there are several weak points which should be addressed before\npublication. Major weak points are (1) only (noiseless?) toy data sets\nare used, (2) some terms in the paper are unclear/undefined, and (3)\nresults are unconvincing.\n\nIt is not clear that this article should be published in the machine\nlearning literature. One of the hallmarks of machine learning is a\nfocus on algorithms for real data sets / problems. In contrast the\nfocus of this paper is quantum computations on toy data /\nproblems. Maybe this paper would be better suited for publication in\nthe quantum computation literature?\n\nThe toy problems are explained in section 4.2. Is there any noise or\nare these noiseless simulations? How does your model/algo perform as a\nfunction of the noise level? How many data points did you simulate\nfrom the model? (e.g. what is the number of observations in the training set?)\n\nThe paper uses the terms \"cost landscape\" and \"meta-cost landscape\"\nwithout explicitly defining them. Equations should be added to clarify\nthese terms.\n\nResults could be made more convincing by\n1. using a real quantum computer.\n2. using real data rather than toy data.\n3. adding error bars or confidence intervals to Figures 4-5.\n4. using a more appropriate baseline -- why not try the algorithms mentioned in section 2.1?\n\nFigure 3 could be clarified by providing ticks and labels on the x\naxes.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea but insufficient clarity and technical depth ",
            "review": " This paper proposes a novel idea of outputting a quantum state that represents a complete cost landscape of all parameters for a given binary neural network, by constructing a quantum binary neural network (QBNN). And then the landscape state is utilized to training the neural network by using the standard quantum amplitude amplification method.   \n\nAlthough this idea is interesting, I trend to reject this submission as I think its presentation is unclear and the technical detail is a little difficult to follow. So, the correctness and soundness of this work is difficult to verify. I urge the authors to revise their draft to provide more and clearer technical details.\n\nDetailed comments and questions:\n\nCould the authors further point out that what the scope of the binary features are, {0, 1} or {-1, +1}? To my understanding, it should be {-1, +1}, or the corresponding variables are always +1. In addition, the construction of the “multiplying values by binary weights” module implies that the value should take -1 or +1, rather than 0 or 1. However, at the bottom of page 5, the authors claim that the binary values take +1 and 0. Could the authors clearly explain the term “parameter” and “value”?\n \nCould the author further explain how to construct the majority activation function?\nIn the part of “calculating accuracy”, the authors mention that “running the QBNN with the weights in superposition for each point in the training set separately” and “there are N qubits containing the prediction of the QBNN”. To my understanding, there are 3 qubits representing the 8 weights, several qubits representing the input values, and N qubits representing the predictions. But how to construct the final landscape state to be optimized with these qubits?\n \nCould the author explain intuitively the main idea of the amplitude amplification method? Specifically, what is the relation between the qubits representing parameters and the qubits presenting prediction results?\n \nDuring the amplitude amplification process, the probabilities change periodically. How to select the best number of steps k in advanced if we do not known the best parameter? Or how to judge if the training is success?\n \nOverall speaking, I think this paper is interesting. However, the presentation is unclear and I suggest the authors to revise their draft by providing more technical details.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}