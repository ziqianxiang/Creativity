{
    "Decision": "",
    "Reviews": [
        {
            "title": "Want to see the generated videos",
            "review": "This paper discusses how to synthesize videos from texts using a text-conditioning scheme. The results are shown on a synthetic moving shape dataset, the Kinetics human action dataset, and CUB bird dataset.\n\nThe proposed algorithm is quite reasonable to me; however, I hope the experimental results could be more convincing.  More specifically,\n\n- There is no comparison against previous works (e.g. T2V by Li et al 2018) in the moving shape experiment. \n\n- The Kinetics experiment is most exciting. However, the submission should provide generated videos for the readers and reviewers. It is a paper of generating videos, so the audience would want to watch these videos.\n\n- The CUB bird dataset is irrelevant since it is all images, not videos.\n\nAt last, it will be nice to provide the optimization details and conduct other ablation experiments using different hyperparameters.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "GAN is extended to video with a video-level discriminator to make the frame generation smooth",
            "review": "This paper presents a GAN-based method for video generation conditioned on text description. A given text description is first encoded, then combined with latent noise, and finally input to the generator for generating video frames. The frame discriminator discriminates between the generated frames and real frames conditioned on the encoded text description. The discriminator ensures that the frame generation is smooth.\n\nPositives: \n- Easy-to-read paper\n- New conditioning method that generates convolution filters from the encoded text, and uses them for a convolution in the discriminator\n- New toy dataset for evaluation\n- High-quality generated videos on Kinetics dataset\n\n\nNegatives and suggestions:\n\n- Novelty is limited. While the paper presents interesting empirical results for computer vision, they may not be of broad interest to the ICLR community. The main contribution seems to be in combining the encoded text with visual features by using the convoluted operation instead of the common concatenation.  \n\n- Clarity can be improved. In section 3, there is no description about the text encoder T (section 3.1). Clarity could be improved by providing a figure of the overall architecture of D, and how to generate a set of convolved f_i from text features (they are described in the appendix, which makes the text hard to read and understand). \n\n- Experiments: \n (a) It not clear how to generate fake pairs (v, t). (By shuffling ground-truth pair or fake video or fake text?) \n(b) It is not clear how to construct the text description from 5 control parameters. \n(c) There are no qualitative results for the Shape-v1 dataset and  the Shape-v2 datasets with background. One suggestion for Shape-v2 is that the background should include color. Otherwise, it is a very easy setting, because the background is a grayscale image while the objects are colored.\n(d) The metric for classifier accuracy seems strange, because the classifier is not perfect. We cannot trust the classifier for evaluating the generated results.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "With a combination of several similar techniques, text-to-video/image performance is improved",
            "review": "Summary:\nThis paper proposed conditional GAN models for text-to-video synthesis:\n(1) Text-feature-conditioned CNN filters is developed; (2) A moving-shape dataset is constructed; (3) Experimental results on video/image generation are shown with improved performance.\n\n\nOriginality: \n\n(0) The overall model can be considered as MoCoGAN [0], but with the text-conditioned information.  Note that [0] also has a conditional version, but based on labels.\n(1) Generating CNN filters conditioned on other feature is not new for video generation, for example, Dynamic Filter Networks in [1], where the CNN filters to produce next frame are generated.\n(2) The idea of creating moving shape datasets is explored and used in the previous papers [2]. This paper creates similar but a bit more comprehensive ones.\n\nThe above three points to the existing work should be made CLEARLY, to reflect the contributions of this submission. \n\n\nClarity: \nA. The current paper only reports the overall performance for a method with several \"new\" modifications: (a) TF, (b) ResNet-style architecture (c) Regularization. Please do some ablation study, and show which component helps solve the problem text-to-video.\n\nB. Following MoCoGAN [0], the latent feature consists of two parts: one controls the global video (content in [0]) and one controls the frame-wise dynamics (motion in [1]).  It is okay to follow [0]. But, did the authors verify the two parts of latent codes can really control the claimed generation aspects? Or, is it really necessary to have to the two separated parts for the problem? if the goal is just to have text-controllable video?\n\n\n\nSignificance: \nText-to-video is a very challenging task, it is encouraging to see the attempt on this problem. However, I hope each contribution to this important problem are concrete and solid. Therefore, it will much more convincing to study each part of \"contributions\" the more comprehensively (one significant contribution would be enough), rather than put several minor/existing techniques together.\n\n\nQuestions: \nA. The generated frames (Figure 5) for one video are so similar, it raise a questions: does this model really generate video (which captures the dynamics)? or it is just a model for the static image generation, with small perturbations? If the answer is the ground-truth frames (the employed training dataset) are very similar, I would suggest to change the dataset at the first place, the employed dataset is not a proper dataset for text-to-VIDEO generation. \n\nB. [3] is also a text-to-video paper, the comparison to it is missing. Why?\n\n\nReferences:\n[0] MoCoGAN: Decomposing motion and content for video generation, 2018\n[1] Dynamic Filter Networks, 2016\n[2] Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks, 2016\n[3] To create what you tell: Generating videos from captions, 2017",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}