{
    "Decision": {
        "metareview": "Following the unanimous vote of the submitted reviews, this paper is not ready for publication at ICLR. Among other concerns raised, the experiments need significant work, and the exposition needs clarification.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Not ready for presentation at ICLR"
    },
    "Reviews": [
        {
            "title": "review",
            "review": "------------------------------------------\nSummary\n------------------------------------------\nThis paper performs unsupervised classification where the number of classes is unknown. The main idea is to use the CycleGAN framework such that one can reconstruct the original image by first moving to latent space that represents another class (via the connector network), then moving back to the original latent space and going back into image space using a generator. Experiments are conducted on MNIST and CIFAR.\n\n------------------------------------------\nEvaluation\n------------------------------------------\nThe paper tackles an important problem: namely, unsupervised classification (i.e. clustering). I think the use cycle-consistency loss in an auxiliary latent space is quite clever. However, experimental results are lacking. Unsupervised clustering (even when number of classes is not known) is a very well studied problem in machine learning. The authors should compare against at least a few reasonable baselines.\n\n------------------------------------------\nPresentation\n------------------------------------------\nI found the presentation to be somewhat wanting. Section 3 is extremely confusing and in my opinion, not well-motivated. For example, why is self-expressivity important? Why can we assume propositions 3.1 and 3.2?\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Maybe an interesting paper but many parts are unclear",
            "review": "This paper develops an unsupervised classification algorithm using the idea of CycleGAN. Specifically, it constructs a piece-wise linear mapping (the Connector Network) between the discriminator network and the generator network. The learning objective is based on the cycle-consistency loss. Experiments show that it can achieve reasonable loss. This paper addresses an important problem, namely, unsupervised image classification, and may present interesting ideas. However, the paper is not in a good shape for publication in its current form.\n\nFirst, the paper is not well written and many of the key ideas are not clear. It devotes more than half of the pages to the review of the preliminary materials in Sections 2-3 while only briefly explained the main algorithm in Section 4. Many of the details are missing. For example, why L1-loss is used in (5)-(7) in Algorithm 1? What is the “random-walk Laplacian matrix L_{sym}” (never defined)? More importantly, it seems that Section 3.4 is a key section to explain how to perform unsupervised classification. However, the ideas (regarding the footprints and footprint mask etc.) are totally unclear. It is assumed that all different classes have equal probabilities. In this setting, it is unclear (in its intuition) why it is possible to assign a cluster index to its true class labels. What is the key signal in the data that enables the algorithm to relate different clusters to their corresponding classes, especially when all classes have equal probability? Furthermore, it is not clear why the mapping from H to Z can be written as a sum of C_1,…,C_k in Proposition 3.2. If the final mapping is piece-wise linear, how can it be written as a sum of linear mappings? Similar question arises in the first paragraph of Section 4.1: if the connector network is constructed as a piecewise linear function (as stated earlier in the paper in abstract and introduction), then how can it be written as a matrix? (Only linear mapping can be expressed as a matrix.)\n\nSecond, the experiment is insufficient. None of the experiment details are presented in the paper. Only the final accuracy of 0.874 is given without any further details. What is the model architecture and size? More experimental analysis should be presented. For example, there are many different hyperparameters in the algorithms. How are the \\lambda_D, \\lambda_G chosen when there is no labeled validation set? How sensitive is the algorithm to different model architecture and model size? Furthermore, none of the baselines results are presented and compared against.\n\nA lot of related works are missing. There have been a lot of emerging works related to unsupervised classification recently, which should be discussed and compared:\n[1] G. Lample, L. Denoyer, and M. Ranzato.  Unsupervised machine translation using monolingual corpora only. ICLR, 2018.\n[2] M. Artetxe, G. Labaka, E. Agirre, and K. Cho.  Unsupervised neural machine translation. ICLR, 2018.\n[3] Y. Liu, J. Chen, and L. Deng.  Unsupervised sequence classification using sequential output statistics. NIPS, 2017\n[4] A. Gupta, A. Vedaldi, A. Zisserman. Learning to Read by Spelling: Towards Unsupervised Text Recognition. arXiv:1809.08675, 2018.\n[5] G. Lample, M. Ott, A. Conneau, L. Denoyer, M. Ranzato. Phrase-based & neural unsupervised machine translation. EMNLP 2018.\n\nThe presentation of the paper should be significantly improved as it is currently hard to read due to many grammar and English usage issues as well as other unclear statements. Just to name a few examples below:\n-\t(1st paragraph of Introduction): “…imagine the learned objectconstruct…”\n-\tThe last paragraph in Section 1 is not in the right position and should be placed somewhere else in the introduction.\n-\tIn the first paragraph of Section 2.2, “one of the clustering algorithm” should be “one of the clustering algorithms”.\n-\tIn the first paragraph of Section 3, it is not clear what it means by “we can make the tuples (Z,X,H) for the whole dataset”.\n-\tAt the end of the first paragraph of Section 3, there is a missing reference in “network in section()”.\n-\tIn the third paragraph on page 4, there is a grammar issue in “H and Z have greater than convexity than X…” and in “it allows the linear combination on these two manifolds in the feature spaces H and Z are and”.\n-\tIn the first paragraph of Section 3.2, it is not clear what it means by “two feature spaces will be trained by the cycle consistency loss to obtain the tuples (Z,X,H) with the correct permutation, where all of elements is in the same-class manifold and shares same learned features.”\n-\tIn the first paragraph of page 5, “cycle-consistency loss z C(D(G(z))) and backward cycle consistency loss x G(C(D(x)))” does not read well. It sounds like z is the loss C(D(G(z))) and x is the loss G(C(D(x)))?\n-\tTypo in Figure 4: “a shows” should be “(a) shows”.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "lack of sufficient experiments",
            "review": "The manuscript proposes a method for unsupervised learning with unknown class number k. The problem is classical and important. The proposed method is interesting and novel, but the experiments are not convincing. In detail, it did not compare other methods in the experiments. \nPros: clear description and novelty of the method\nCons: insufficient experiments. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}