{
    "Decision": "",
    "Reviews": [
        {
            "title": "Simplistic experimental setup, no technical novelty, missing baselines and experimental details",
            "review": "Summary:\nThis paper aim for learning a feature representation from video sequences captured from a scene from different view points. The proposed approach is tested on a table top scenario for synthetic and real scenes. Pairs of frames from captured video is selected, then a pre-trained object detector finds the object proposal bounding boxes. The positive pairs are found using nearest neighbor between cropped bounding boxed from two random frames and finally a network is trained using an n-pair contrastive loss function and hence called object-contrastive network.\n\nPros: Unsupervised feature learning is an interesting area in computer vision and ML and this paper tries to tackle this problem for objects seen from different viewpoints. \n \nCons:\n-Not enough technical novelty compared to current unsupervised feature learning approaches. The proposed approach uses two random frame from a sequence and use nearest neighbor match based on some pre-trained network and compute an n-pair contrastive loss of Sohn 2016 on top. \n\n-Experimental set up for the real experiment is very simplistic and objects with similar appearance and colors are appearing in both train and test sets which is far from random selection of object instances and categories into test and train (plates, bowls and cups with similar colors and similar shapes).\nWhy the proposed method is not trained and tested on tasks similar to [a]? There can be similar setup in training videos of [a] and tested on object detection task on videos of natural scenes (rather than a particular indoor table top scenario). [a] is a relevant baseline which is missed.\n[a] Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos. In ICCV, 2015. \n\nMissing Baselines:\n-Comparing against learned embedding feature with feature trained on (a) ResNet50 pre-trained ImageNet or (b) ResNet50 pre-trained COCO for both NN and linear setup is missed. There is only ResNet50 embedding pre-trained on ImageNet shown in table 1.\n-Comparing against previous self-supervised methods that use tracking is missed.\n-Comparing against previous methods that learn embedding on delta time and/or camera location is missed.\n\nIssues in experimental setups:\n\n-Section 5.2 with title “Instance Detection and Tracking” only shows three qualitative example if instance retrieval and ranking for a pair of views. There is no standard quantitative result for instance tracking in this section such accuracy of trajectory over time. Also the detail of experimental setup for table 2 is missing. Number of instances, pairs, real or synthetic, etc.\n\n-The object appearance is not similar from different view. In the current experimental setup (which is less than 90 degrees different viewpoint) the appearance can be similar. It is not clear if the proposed approach can work with more variation of camera viewpoint.\n\n-There are many hand designed assumptions in the experimental setup which makes it unnatural in real scenario. For instance, the number of objects in all frames are approximately equal and all objects are visible in all frames. In real scenario the objects can appear and disappear from the camera viewpoint based on camera field of view and can can cause drastic changes in the nearest neighbor set up in the method formulation. What happen if in extreme case there is no object in one of the frames when wants to find the pairs? It can match with some random patches then? \n\n-In Page 5, section 4.1, it is mentioned “We randomly define the number of objects (up to 20) in a scene and select half of the objects from two randomly selected categories. The other half is selected from the remaining object categories.”. What is the logistic behind this choice? The reason for this setup is not explained in the paper.\n\n-Throughout the paper the words “attribute”, “class”, “semantic”, “label” are used in a confusing manner based on the current literature. For example, “…naturally encoding attributes like class, color, texture and function…” in Introduction section. Class is not an object attribute.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Technically not novel, experimentally weak, unsupported arguments.",
            "review": "In this paper, an unsupervised representation learning method for visual inputs is proposed. The proposed method incorporates a metric learning approach that pulls nearest neighbor pairs of image patches close to each other in the embedding space while pushing apart other pairs. The train and test scenarios are captured based on a table top scenario with multiple objects of different colors such as cups and bowls. Standard datasets are not used for benchmarking. \n\nPros:\n- Unsupervised feature learning is an active and interesting area of research.\n- The proposed method and network architecture is simple.\n\nCons:\n\n-The paper does not present technical novelty. The proposed network architecture is a ResNet50 that uses the object proposals obtained by Faster RCNN (Ren et al. 2015) and incorporates the n-pair loss function proposed by Sohn 2016. \n\n-Technical details are missing. \n\n>When using object proposals obtained by Faster RCNN (Ren et al. 2015) to generate pairs, do you use all of the object proposals or do you select a subset of them by some threshold? \n>How is the robotic control performed in the robot grasping and pointing tasks? It seems that grasping and pointing tasks are not learned and are based on conventional motion planning and state estimation. These details are not included in the manuscript.\n>Section 4.2 mentions that a mobile robot was needed for collecting the data. What kind of mobile robot was used? Why was a mobile robot needed? There is no details about the robot platform in the manuscript and no visualization is provided for the robot setup.\n>Why is a ResNet pretrained with ImageNet is used throughout the experiments (and not pretrained with COCO?) while object proposals are obtained by Faster RCNN which is pretrained by COCO. How would that affect the results? \n\n-The paper uses imprecise, confusing and sometimes incorrect phrases and wordings throughout the draft. Here are a few examples:\n\n> It is unclear what it is exactly meant by “object correspondences” that naturally emerge (e.g. in the last four lines of abstract). Would a correct object correspondence refer to “similar instances”, “similar colors”, “similar object categories”, “similar object poses”, “similar functionality”? For example, the first column of Fig. 1 shows an example of two cups with “similar green color”, “similar tall shapes” and “similar functionality” and “similar background” that are considered as “negative pairs (with red arrows)” while in the same Fig 1. Two cups with drastically different appearances one of which is “yellow with handle” (second column) and the another is “green without handle” (third column) are considered to be positive pairs (blue arrows). Similar confusing examples and arguments can repeatedly be found in the experiments and embedding visualizations:  Fig. 4, Fig.5, Fig. 10- Fig. 15. \n\n> Throughout the draft and in Figures (e.g. Fig. 1) it is emphasized the the data collection is done by “robot” or it is “robotic”. Why was a robot needed to capture images or videos of around a table? A hand held moving camera could also be simply used.  Capturing images and videos around a space is a very well-known and simple practice done in many previous works in computer vision and machine learning. It does not need a robot and it is not robotic. \n\n> First sentence of the second paragraph of the introduction is not a technically correct statement based on the well-known computer vision literature. Specifically, “class” is not considered as an “attribute”.  What does it mean to disentangle “perceptual” and “semantic” object attributes? This is a very vague statement. “color” is neither a “semantic” attribute nor a “perceptual” attribute. Also, in section 3.2 and Fig. 2, it is confusing to consider “class” label as an attribute. Please refer to the well-known prior work of “Farhadi A, Endres I, Hoiem D, Forsyth D. Describing objects by their attributes. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on 2009 Jun 20 (pp. 1778-1785). IEEE.” for a clear and well-known explanation of object attributes vs categories. If your definition of “attributes” is technically different than that of this aforementioned prior work, it should be clarified to avoid confusion. \n\n-Unconvincing usefulness for robotics applications: The paper explains that the proposed method is effective and useful for robotic tasks however tis claim is not backed up with convincing robotic tasks or scenarios. Also, the performance in the robotic grasping and pointing tasks are not compared with any baselines.\n\n> While usefulness of the method for improving robotic grasping task is listed in the contributions (number 3 in the last paragraph of Section 1) there are only two qualitative grasping scenarios shown in the paper. It is not explained how the robot control is done. In both grasping scenarios the target object is the one in the middle. It seems that the grasping control policies are not learned and are motion planned or are a scripted policy and only the visual matching of the target object with the objects of the scene are done with the proposed method. For evaluating such scenario no robot is needed and only similarity score could be reported. It would had been interesting to see how representation learning can be seamlessly incorporated for robotic grasping which involves control as a tangible contribution however, the presented scenario does is not contributing to that problem and is only doing visual matching. No baseline comparison is provided here.\n\n> The details of the robot control for the robot pointing scenario is also not provided. The objects are places on two rows in all scenarios and it looks like the the actual pointing is done by motion planning after the visual matching step done by the proposed method. The presented method in this paper tries to find the closest visual match to the target object amongst the objects on the table and does not integrate it with any policy learning for improving the “robotic pointing task” so there is no robotic contribution here. No baseline is provided performance comparison in this robotic task as well.  \n\n- The experimental results are weak and unconvincing:\n\n> In the experiments corresponding to Table 6 and Table 1, what is the performance of ResNet 50 embedding (linear)? Can you provide these results?\n\n> Table 6 shows that the performance gain of the proposed method compared to the supervised and unsupervised baselines is marginal. \n\n> What is the performance of a baseline network with similar architecture that uses “contrastive loss” (based on object pairs with similar attributes). This baseline is missed.\n\n> Qualitatively, the visualizations in Fig. 1, Fig. 4-5, Fig. 10-15 show that OCN embedding is noisy. In all these figures, there are many less similar instances that are retrieved on the top and many similar instances that are ranked down in the list.    \n\n> The paper refers to ad-hoc experimental protocols which makes the evaluations unclear and invalid: what is meant to report “class” and “container” attributes in Table 3 and section 5.3? Why are “bottles and cans” are considered in a same group while there were referred to as different objects in all previous explanations of the object types and categories used in the training and testing? What is the difference between “cups and Mugs” and “glasses” that are separated? How are “Balls” , “Bowls”, “Plates”, etc listed as *attributes*?  Aren’t these previously referred to as object categories? \n\n> The paper has some inconsistent experimental setups. Why in Fig 16, the same instances of were removed for the real objects and not for the synthetic objects?\n\n\n-The presentation of the paper can be improved. Here are a few examples:\n\n>Grammatical or unclear wordings and typos (e.g. Caption of Fig. 2 “attracting embedding nearest neighbors”; is not a readable. Repetitive words, Last line of section 3. , etc)\n>In Fig. 10-15, using t-SNE (Maaten LV, Hinton G. Visualizing data using t-SNE. Journal of machine learning research. 2008;9(Nov):2579-605.) instead of a sorted list provides much more expressive visualization of the effectiveness of the approach. It is highly recommended that t-SNE be used instead of sorted lists if image patches.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An interesting idea in an unnatural setup",
            "review": "This paper explored self-supervised learning of object representations. The main idea is to encourage objects with similar features to get further ‘attracted’ to each other. The authors demonstrated that the system works on real objects with simple geometry. \n\nThe problem of self-supervised learning from video data is an important one. This paper is making progress along this direction. The approach is new. The paper is in general well written and clear. \n\nMy main concern is the problem setup is unnatural. I can imagine two types of approaches with video input: object-based or pixel-based. An object-based approach detects and tracks objects over time, and then learns object representations upon correspondence. Tracking objects is hard, but gives object correspondence as the basis for learning. A pixel-based approach does not detect objects, but learns dense feature representations for each pixel via signals such as optical flow. Here, training signals become noisier, but the algorithm no longer suffers from the errors that may arise in object detection. It can also generalize to objects that are hard to be detected, such as soft bodies and liquids.\n\nThe proposed system however lies in an uncanny valley: it performs object detection per frame (which is hard and noisy), but it then discards the temporal signals in the video and therefore loses object correspondence. To compensate that, the authors proposed a sometimes incorrect heuristic (based on nearest neighbors) as supervision. The motivation behind such design is unclear, and I’d love to hear the authors’ feedback on it. \n\nThe authors should also cite and discuss those pixel-based methods (see below).\n\nThe experimental results are neat, but not too impressive. The objects used are all rigid with simple geometry. As said before, such an approach would have a hard time generalize to deformable objects and liquids. The results on the real robot is not very convincing, as the system doesn’t really learn object representations that can be used for manipulation. For example, for the results in Fig 7, I assume the way of grasping is handcrafted, instead of learned by the network. Please let me know if I’m wrong.\n\nIn general, I feel this paper interesting but not exciting, and it’s unclear what we can really learn from it. I’m therefore on the border, leaning slightly toward rejection. I’m happy to adjust my rating based on the discussion and revision.\n\nRelated work\n\nSelf-supervised Visual Descriptor Learning for Dense Correspondence. ICRA 17.\nDense Object Nets: Learning Dense Visual Object Descriptors By and For Robotic Manipulation. CORL 18 (concurrent work, though on arxiv since June).\nUnsupervised learning of object frames by dense equivariant image labelling. NIPS 17.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}