{
    "Decision": {
        "metareview": "This paper proposes a novel and interesting active learning approach, that  trains a classifier to discriminate between the examples in the labeled and unlabeled data at each iteration. The top few samples that are most likely to be from the unlabeled set as per this classifier are selected to be labeled by an oracle, and are moved to the labeled training examples bin in the next iteration. The idea is simple and clear and is shown to have a principled basis and theoretical background, related to GANs and to previous results from the literature. Experiments performed on CIFAR-10 and MNIST benchmarks demonstrate good results in comparison to baselines. \nDuring the review period, authors considered most of the suggestions by the reviewers and updated the paper. Although the proposed method is similar to density-based active learning methods, as also suggested by the reviewers, baselines do not include such approaches in the comparison experiments.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Novel active learning approach, but work could still benefit from revisions and additional baselines"
    },
    "Reviews": [
        {
            "title": "very nice paper with neat idea, good theoretical intuition/justification, clear transparent code/algorithm, and good experimental results",
            "review": "Thank you for this enjoyable paper. \n\nSummary: The authors propose a novel approach to active learning as follows. At each iteration they develop a classifier that can discriminate between the samples in the labeled and unlabeled sets; they select the top few samples that are most likely to be from the unlabeled set as per this classifier, and request the oracle to provide labels for this batch next. This simple idea is shown to have a principled basis and theoretical background, related to GANs and to previous results from the literature. They provide clear algorithms and open source code for easy verification, and public testing. They provide good experimental verification on CIFAR-10 and MNIST benchmarks. I personally look at new papers more for novel ideas and good intuition/theoretical justification than an immediate improvement in benchmark results, so I enjoyed this paper thoroughly. \n\nResults: Among other things they show that their algorithms ranks the samples to be next labeled quite differently than uncertainty sampling based approaches; that their method is at least as accurate/sample-efficient as the state of the art ; and that some previously published experimental results are incorrect(!). As the authors will probably agree I am not convinced the proposed method is better than previous algorithms in any statistically significant way, but the novel idea itself is worth publishing even if it is just as good as the state of the art. \n\nNovelty: I liked the paper very much because it provides quite an innovative new approach to look at active learning, which resembles GANs and Core set ideas in some ways, yet differs in significant ways that are critical for active learning. I've been working and publishing in related areas for a long time so I genuinely found your central idea refreshing and new.\n\nRelevance: The paper is very relevant to the ICLR community and addresses critical questions. \n\nQuestion:\nMy intuition as a Bayesian is that we most need to find labels that maximize the mutual information I(y,w) where w are the weights of the neural net. In practice this corresponds to the samples x which have the maximum class uncertainty, but for which the parameters under the posterior disagree about the outcome the most, eg see discussion below equation 2 for  Bayesian Active Learning by Disagreement (BALD) in this paper https://arxiv.org/pdf/1112.5745.pdf . In essence: The above means that the labels that provide most information about the classification model are most valuable for active learning. \n\nHowever, your approach intuitively ignores the conditional distribution(ie py(|x)), and instead tries to make the original unconditional distribution p(x) between the labeled and unlabeled sets similar. Yet, it works beautifully. So: Why does this work? What is the intuition?\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting but counter-intuitive idea which works well in practice. It needs a better motivation/explanation.",
            "review": "The paper is proposing a distribution matching as a metric for active learning. Basic intuition is: if we can make the distribution of labelled and unlabelled examples similar to each other, training error in one will approximate the training error in the other. Hence, a model learned using labelled ones will do well in unlabelled ones. The main tool to enforce this distributional distance is using adversarial learning similar to GANs or gradient reversal network for domain adaptation.\n\nThe idea is definitely interesting. I am not sure about why should it work (I explained in detail later), but it does work well empirically. Moreover, it is very easy to implement. Given any learned or hard-coded features, learning a simple binary classifier is sufficient to implement the method. The mini-queries idea in 4.1 is especially interesting. Handling large batches in active learning is always a problem but this neat trick make it much easier.\n\nI think the proposed method is counter intuitive as the discussion does not explain why should it work better than random sampling. Clearly if labelled samples are randomly sampled, labelled and unlabelled data is coming from the exactly same distribution. Hence, the distance (H-divergence, TV-distance etc.) between them is 0. My main question to authors is why does this method work better than random sampling? A similar question is; since they are coming from the exact same distribution, what is the meaning of minimizing empirical H-divergence? I think a more detailed study on a toy problem could potentially explain this. Authors can generate 1-D or 2D samples from a well defined distribution (eg. Gaussians with different means/variances for each class) and visualize what is the algorithm actually doing. \n\nConsidering my point that these data points are actually coming from the same distribution, discussion in Section 3.1 is rather unjustified. Most of the entities discussed in that section are probabilistic entities (generally speaking expected values) and does not differ between labelled and unlabelled case since they have same underlying distribution. Their empirical values are different but this is beyond the study of Ben-David(2010). Therefore, I am not sure does the Section 3.1 is contributing to the paper without any explicit connection to the empirical divergence minimization. More importantly a much similar work from domain adaptation is [Unsupervised domain adaptation by backpropagation, ICML 2015] and it should also be discussed in the paper.\n\nSome minor issues:\n- Are the hyper-parameters kept fixed for all experiments. In other words, does the training size of 5k and 15k share hyperparameters? Which might be sub-optimal.\n- The experiments use very large batch sizes. A smaller batch sizes might separate the algorithms better.\n- References in the text have some issues. There are missing commas between references in the text. There are also some cases where \\citep should have been used but \\citet is used. A careful pass over them might be beneficial.\n\nIn summary, I think the paper is interesting, easy to implement and possibly useful to the large part of the community since active learning is very important problem. I think the major weakness of the paper is the fact that authors did not give a clear explain why does it actually work. I think it is crucial for authors to provide a theoretical or an empirical study which answer this question.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Method motivation and experimental results are not very convincing",
            "review": "This paper presents a new approach to an active learning problem where the idea is to train a classifier to distinguish labeled and unlabeled datapoints and select those that look the most like unlabeled.\n\nThe paper is clearly written and easy to follow. The idea is quite novel and evokes interesting thoughts. I appreciated that the authors provide links and connections to other problems. Another positive aspect is that evaluation methodology is quite sound and includes comparison to many recent algorithms for AL with neural networks. The analysis of Section 5.5 is quite interesting.\nHowever, I have a few concerns regarding the methodology. First of all, I am not completely convinced by the fact that selecting the samples that resemble the most unlabeled data is beneficial for the classifier. It seem that in this case just the data from under-explored regions will be selected at every new iteration. If this is the purpose, some simpler methods, for example, relying on density sampling, can be used. Could you elaborate how you method would compare to them? I can see this method as a way to measure the representativeness of datapoints, but I would see it as a component of AL, not an AL alone. What would happen it is combined with Uncertainty and you use it to labeled the points that are both uncertain and resemble unlabeled data? \nBesides, the proposed approach does not take the advantage of all the information that is available to AL, in particular, it does not use at the information about labels. I believe that labels contain a lot of useful information for making an informed selection decision and ignoring it when it is available is not rational.  \nNext, I have conceptual difficulties understanding what would happen to a classifier at next iteration when it is trained on the data that was determined by the previous classifier. Seems that the training data is non-iid and might cause some strange bias. In addition to this, it sounds a bit strange to use classification where overfitting is acceptable.\nFinally, the results of the experimental evaluation do not demonstrate a significant advantage of the proposed method and thus it is unclear is there is a benefit of using this method in practice. \n\nQuestions:\n- Could you elaborate why DAL strategy does not end up doing just random sampling?\n- Nothing restrict DAL from being applied with classifiers other than neural networks and smaller problems. How do you think DAL would work on simpler datasets and classifiers?\n- How does the classifier (that distinguished between labeled and unlabeled data) deal with very unbalanced classes? I suppose that normally unlabeled set is much bigger than labeled. What does 98% accuracy mean in this case?\n- How many experiments were run to produce each figure? Are error bars of most experiments so small that are almost invisible?\n\nSmall comments:\n- I think in many cases citep command should be used instead of cite. \n- Can you explain more about the paragraph 3 of related work where you say that uncertainty-based approach would be different from margin-based approach if the classifier is neural network?\n- Last sentence before 3.1: how do you guarantee in this case that the selected examples are not similar to each other (that was mentioned as a limitation for batch uncertainty selection, last paragraph on page 1)?\n- It was hard to understand the beginning of 5.5, at first it sounds like the ranking of methods is going to be analysed.\n- I am not sure \"discriminative\" is a good name for this algorithm. It suggested that is it opposite to \"generative\" (query synthesis?), but then all AL that rank datapoints with some scoring function are \"discriminative\".",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}