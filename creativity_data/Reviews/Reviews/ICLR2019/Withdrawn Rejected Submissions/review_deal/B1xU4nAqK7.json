{
    "Decision": {
        "metareview": "Strengths\n\nThe paper proposes to include exploration for the PETS (probabilistic ensembles with trajectory sampling)\napproach to learning the state transition function. The paper is clearly written.\n\nWeaknesses\n\nAll reviewers are in agreement regarding a number of key weaknesses: limited novelty, limited evaluation,\nand aspects of the paper are difficult to follow or are sparse on details.\nNo revisions have been posted.\n\nSummary\n\nAll reviewers are in agreement that the paper requires significant work and that it is not ready for ICLR publication.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "incremental, limited evaluation"
    },
    "Reviews": [
        {
            "title": "Weak experimental evaluation and lack of novelty",
            "review": "The authors address the problem of how to use unsupervised exploration in a first phase of reinforcement learning to gather knowledge that can be transferred to new tasks to improve performance in a second task when specific reward functions are available. The authors proposed a model-based approach which uses deep neural networks as a model for the environment. The model is PETS (probabilistic ensembles with trajectory sampling), an ensemble of neural networks whose outputs parametrize predictive distributions for the next state as a function of the current state and the action applied. To collect data during the unsupervised exploration phase, they use a metric of model uncertainty computed as follows: the average over all the particles assigned to each bootstrap is computed and the variance over these computed means is the\nmetric of uncertainty. The authors validate their method on the HalfCheetah OpenAI gym environment where they consider 4 different tasks related to running forward, backward, tumbling forward and tumbling backward. The results obtained show that they outperform random and count based exploration approaches.\n\nQuality:\n\nI am concerned about the quality of the experimental evaluation of the method. The authors only consider a single environment for their experiments and artificially construct 4 relatively similar tasks. I believe this is insufficient to quantify the usefulness of the proposed method.\n\nClarity:\n\nThe paper is clearly written and easy to read.\n\nNovelty:\n\nThe proposed approach seems incremental and lacks novelty. The described method for model-based exploration consists in looking at the mean of the prediction of each neural network in the ensemble and then computing the empirical average. This approach has been used before for active learning with neural networks ensembles:\n\nKrogh, Anders, and Jesper Vedelsby. \"Neural network ensembles, cross validation, and active learning.\" Advances in neural information processing systems. 1995.\n\nThe used model, PETS, is also not novel and the proposed methodology for having first an unsupervised learning phase and then a new specific learning task is also not very innovative.\n\nSignificance:\n\nGiven the lack of a rigorous evaluation framework and the lack of novelty of the proposed methods, I believe the significance of the contribution is very low.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An incremental work and needs more justification/clarification",
            "review": "The authors built upon the PETS algorithm to develop a state uncertainty-driven exploration strategy, for which the main point is to construct a reward function. The proposed algorithm was then tested on a specific domain to show some improvement. \n\nThe contribution of this paper may be limited, as it needs a specific setting, as shown in Figure 1. Furthermore, this paper is a bit difficult to follow, e.g., it was not until the 5th page to describe their algorithm. I summarize the pros and cons as follows.\n\nPros:\n- The idea to include the exploration for PETS is somewhat interesting.\nCons:\n- The paper is a bit difficult to follow. Just to list a few places:\n  1. The term \"unsupervised exploration\" was mentioned a few times in this paper. I am not sure if this is an accurate term. Is there a corresponding \"supervised exploration\" used elsewhere? \n  2. When you introduced r_t in Section 3.3, how did you use it next? Was it used in Phase II?\n  3. For the PETS (oracle) in Figure 4, why are the settings different for forward and backward tasks?\n  4. What does \"random\" mean in Figure 4?\n- The novelty of this paper is somewhat limited, as it requires a specific setting and has been applied in only one domain.\n- There are a few grammar mistakes/typos in this paper. \n  1. What is \"k\" in the equation for r_t?\n  2.  \"...we three methods...\" in Page 6.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Decent paper, but not very novel, sparse on details.",
            "review": "The paper performs model-based reinforcement learning. It makes two main contributions. First, it divides training into two phases: the unsupervised phase for learning transition dynamics and the second phase for solving a task which comes with a particular reward signal. The scope of the paper is a good fit for ICLR.\n\nThe paper is very incremental: the ideas of using an ensemble of models to quantify uncertainty, to perform unsupervised pre-training and to explore using an intrinsic reward signal have all been known for many years.\n\nThe contribution of the paper seems to be the combination of these ideas and the way in which they are applied to RL. I have the following observations / complaints about this.\n\n1. The paper is very sparse on details. There is no pseudocode for the main algorithm, and the quantity v^i_t (the epistemic variance on page 5) isn't defined anywhere. Without these things, it is difficult for me to say what the proposed algorithm is *exactly*.\n\n2. Sections 1 and 2 of the paper seem unreasonably bloated, especially given the fact that the space could have been more meaningfully used as per (1).\n\n3. The experimental section misses any kind of uncertainty estimates. If, as you say, you only had the computational resources for three runs, then you should report the results for all three. You should consider running at least one experiment for longer. This should be possible - a run of 50K steps of HalfCheetah takes about one hour on a modern 10-core PC, so this is something you should be able to do overnight.\n\n4. The exploration mechanism is a little bit of a  mystery - it isn't concretely defined anywhere except for the fact that it uses intrinsic rewards. Again, please provide pseudocode.\n\nAs the paper states now, the lack of details makes it difficult for me to accept. However, I encourage the authors to do the following:\n1. Provide pseudocode for the algorithm.\n2. Provide pseudocode for exploration mechanism (unless subsumed by (1)).\n3. Add uncertainty estimates to evaluation or at least report all runs.\n\nI am willing to re-consider my decision once these things have been done.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}