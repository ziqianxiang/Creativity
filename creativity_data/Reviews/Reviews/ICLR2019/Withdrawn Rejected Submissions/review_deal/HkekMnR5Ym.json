{
    "Decision": {
        "metareview": "This work proposes and interesting approach to learn approximate set membership. While the proposed architecture is rather closely related to existing work, it is still interesting, as recognized by reviewers. Authors's substantial rewrites has also helped make the paper clearer. However, the empirical merits of the approach are still a bit limited; when combined with the narrow novelty compared to existing work, this makes the overall contribution a bit too thin for ICLR. Authors are encouraged to strengthen their work by showing more convincing practical benefit of their approach.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting approach, empirical validation needs strengthening"
    },
    "Reviews": [
        {
            "title": "Unclear paper, difficult to understand how the algorithm works or why",
            "review": "The paper proposes a method whereby a neural network is trained and used as a data structure to assess approximate set membership. Unlike the Bloom filter, which uses hand-constructed hash functions to store data and a pre-specified method for answering queries, the Neural Bloom Filter learns both the Write function and the Read function (both are \"soft\" values rather than the hard binary values used in the Bloom filter). Experiments show that, when there is structure in the data set, the Neural Bloom Filter can achieve the same false positive rate with less space.\n\nI had a hard time understanding how the model is trained. There is an encoding function, a write function, and a query function. The paper talks about one-shot meta-learning over a stream of data, but doesn't make it clear how those functions are learned. A lot of details are relegated to the Appendix. For instance B.2 talks about the encoder architecture for one of the experiments. But even that does not contain much detail, and it's not obvious how this is related to one-shot learning. Overall, the paper is written from the perspective of someone fully immersed in the details of the area, but who is unable to pop out of the details to explain to people who are not already familiar with the approach how it works. I would suggest rewriting to give an end-to-end picture of how it works, including details, without appendices. The approach sounds promising, but the exposition is not clear at all.",
            "rating": "3: Clear rejection",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "Interesting topic, some concerns",
            "review": "SUMMARY\nThe paper proposes a neural network based architecture to solve the approximate set membership problem, in the distributional setting where the in-set and out-of-set elements come from two unknown and possibly different distributions.\n\n\nCOMMENTARY\nThe topic of the paper is interesting, and falls into the popular trend of enhancing classical data structures with learning algorithms. For the approximate set membership problem, this approach was already suggested by (Kraska et al. 2018) and studied further in (Mitzenmacher 2018a,b). The difference in the current paper is that the proposed approach relies on \"meta-learning\", apparently to facilitate online training and/or learning across multiple sets arising from the same distribution; this is what I gather from the introduction, even though as I write below, I feel this point is not properly explained.\n\nMy main issue with the paper is that its conceptual contribution seems limited and unclear. It suggests a specific architecture whose details seem mostly arbitrary, or at least this is the impression the reader is left with, as the paper does rather little in terms of discussing and motivating them or putting them in context. Moreover, since the solution ultimately relies on a backup Bloom Filter as in (Kraska et al. 2018), it is hard to not view it as just an instantiation of the model in (Kraska et al. 2018, Mitzenmacher 2018a) with a different plugging of learning component. It would help to flesh out and highlight what the authors claim are the main insights of the paper.\n\nAnother issue I suggest revising pertains to the writing. The problem setting is only loosely sketched but not properly defined. How exactly do different subsets coming into play? Specifically, the term \"meta-learning\" appears in the title and throughout the paper, but is never defined or explained. The authors should write out what exactly they mean by this notion and what role it plays in the paper. This is important since to my understanding, this is the main point of departure from the aforementioned recent works on learning-enhanced Bloom Filters.\n\nThe experiments do not seem to make a strong case for the empirical advantage of the Neural Bloom Filter. They show little to no improvement on the MNIST tasks, and some improvement on a non-standard database related task. One interesting thing to look at would be the workload partition between the learning component and the backup filter, meaning what is the rate of false negatives emitted by the former and caught by the latter, and how the space usage breaks down between them (vis-a-vis the formula in Appendix B). For example, it seems plausible that on the class familiarity task, the learning component simply learns to be a binary classifier for the chosen two MNIST classes and mostly ignores the backup filter, whereas in the uniform distribution setting, the learning component only memorizes a small number of true and false positives and defers almost the entire task to the backup filter. I am not sure what to expect on the intermediate exponential distribution task.\n\nOther comments/questions:\n1. For the classical Bloom Filter, do the results reported in the experimental plots reflect the empirical false-positive rate measured in the experiment, or just the analytic bound?\n2. On that note, it is worth noting that the false positive rate of the classical Bloom Filter is different than the one you report for the neural-net based architectures. The Bloom Filter FP probability is over its internal randomness (i.e. its hash functions) and is independent of the distribution of queries, which need not be randomized at all. For the neural-net based architectures, the measured FP rate is w.r.t. a specific distribution of queries. See the discussion in (Mitzenmacher 2018a), sections B-C.\n3. The works (Mitzenmacher 2018a,b) should probably at least be referenced in the related work section.\n\n\nCONCLUSION\nWhile I like the overall topic of the paper, I currently find the conceptual contribution to be too thin, raising doubts on novelty and significance. In addition, the presentation is somewhat lacking in clarity, and the practical merit is not well established. Notwithstanding the public nature of ICLR submissions, I would suggest more work on the paper prior to publication.\n\n\nREFERENCES\nM. Mitzenmacher, A Model for Learned Bloom Filters and Related Structures, 2018, see https://arxiv.org/pdf/1802.00884.pdf.\nM. Mitzenmacher, Optimizing Learned Bloom Filters by Sandwiching, 2018, see https://arxiv.org/pdf/1803.01474.pdf.\n\n(Update: score revised, see below.)",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Details of the architecture not well motivated",
            "review": "The paper proposes a learnable bloom filter architecture. While the details of the architecture seemed a bit too complicated for me to grasp (see more on this later), via experiments the authors show that the learned bloom filters are more compact that regular bloom filters and can outperform other neural architectures when it comes to retrieving seen items.\n\nA bloom filter is fairly simple, K hash functions hash seen items into K bit vectors. During retrieval, if all of the bits hashed to are 1 then we say we've seen the query. I think there's simpler ways to derive a continuous, differentiable version of this which begs the question why the authors chose a relatively more elaborate architecture involving ZCA transform and first/second moments. Perhaps the authors need to motivate their architecture a bit better.\n\nIn their experiments, a simple LSTM seems to perform remarkably well (it is close to the best in 2 (a), (b); and crashes in (c) but the proposed technique is also outperformed by vanilla bloom filters in (c)). This is not surprising to me since LSTMs are remarkably good at remembering patterns. Perhaps the authors would like to comment on why they did not develop the LSTM further to remedy it of its shortcomings. Some of the positive results attained using neural bloom filters is a bit tempered by the fact that the experiments were using a back up bloom filter. Also, the neural bloom filters do well only when there is some sort of querying pattern. All of these details would seem to reduce the applicability of the proposed approach.\n\nThe authors have addressed most (if not all) of my comments in their revised version. I applaud the authors for being particularly responsive. Their explanations and additional experiments go a long way towards lending the insights that were missing from the original draft of the paper. I have upped my rating to a 7.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}