{
    "Decision": {
        "metareview": "The reviewers raised a number of concerns including insufficiently demonstrated benefits of the proposed methodology, lack of explanations, and the lack of thorough and convincing experimental evaluation. The authors’ rebuttal failed to alleviate these concerns fully. I agree with the main concerns raised and, although I also believe that the work can result eventually in a very interesting paper, I cannot suggest it at this stage for presentation at ICLR.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "metareview "
    },
    "Reviews": [
        {
            "title": "Promising paper on an appealing topic, but needs a bit more work",
            "review": "Note: This review is coming in a bit late, already after one round of responses. So I write this with the benefit of having read the helpful previous exchange. \n\nI am generally positive about the paper and the broader project. The idea of showing that causal reasoning naturally emerges from certain decision-making tasks and that modern (meta-learning) RL agents can become attuned to causal structure of the world without being explicitly trained to answer causal questions is an attractive one. I also find much about the specific paper elegant and creative. Considering three grades of causal sophistication (from conditional probability to cause-effect reasoning to counterfactual prediction) seems like the right thing to do in this setting.\n\nDespite these positive qualities, I was confused by many of the same issues as other reviewers, and I think the paper does need some more serious revisions. Some of these are simple matters of clarification as the authors acknowledge; others, however, require further substantive work. It sounds like the authors are committed to doing some of this work, and I would like to add one more vote of encouragement. While the paper may be slightly too preliminary for acceptance at this time, I am optimistic that a future version of this paper will be a wonderful contribution.\n\n(*) The authors say at several points that the approach “did not require explicit knowledge of formal principles of causal inference.” But there seem to be a whole of lot of causal assumptions that are critically implicit in the setup. It would be good to understand this better. In particular, the different agents are hardwired to have access to different kinds of information. The interventional agent is provided with data that the conditional agent simply doesn’t get to see. Likewise, the counterfactual agent is provided with information about noise. Any sufficiently powerful learning system will realize that (and even how) the given information is relevant to the decision-making task at hand. A lot of the work (all of the work?) seems to be done by supplying the information that we know would be relevant.\n\n(*) Previous reviewers have already made this point - I think it’s crucial - and it’s also related to the previous concern: It is not clear how difficult the tasks facing these agents actually are, nor is it clear that solving them genuinely requires causal understanding. What seems to be shown is that, by supplying information that’s critical for the task at hand, a sufficiently powerful learning agent is able to harness that information successfully. But how difficult is this task, and why does it require causal understanding? I do think that some of the work the authors did is quite helpful, e.g., dividing the test set between the easy and hard cases (orphan / parented, unconfounded / confounded). But I do not feel I have an adequate understanding of the task as seen, so to say, from the perspective of the agent. Specifically:\n\n(*) I completely second the worry one of the reviewers raised about equivalence classes and symmetries. The test set should be chosen more deliberately - not randomly - to rule out deflationary explanations of the agents’ purported success. I’m happy to hear that the authors will be looking more into this and I would be interested to know how the results look.\n\n(*) The “baselines” in this paper are often not baselines at all, but rather various optimal approaches to alternative formulations of the task. I feel we need more actual baselines in order to see how well the agents of interest are doing. I don’t know how to interpret phrases like “close to perfect” without a better understanding of how things look below perfection. \n\nAs a concrete case of this, just like the other reviewers, I was initially quite confused about the passive agents and why they did better than the active agents. These are passive agents who actually get to make multiple observations, rather than baseline passive agents who choose interventions in a suboptimal way. I think it would be helpful to compare against an agent who makes the same number of observations but chooses them in a suboptimal (e.g., random) way. \n\n(*) In relation to the existing literature on causal induction, it’s telling that implementing a perfect MAP agent in this setting is even possible. This makes me worry further about how easy these tasks are (again, provided one has all of the relevant information about the task). But it also shows that comparison with existing causal inference methods is simply inappropriate here, since those methods are designed for realistic settings where MAP inference is far from possible. I think that’s fine, but I also think it should be clarified in the paper. The point is not (at least yet) that these methods are competitors to causal inference methods that do “require explicit knowledge of formal principles of causal inference,” but rather that we have a proof-of-concept that some elementary causal understanding may emerge from typical RL tasks when agents are faced with the right kinds of tasks and given access to the right kinds of data. That’s an interesting claim on its own. The penultimate paragraph in the paper (among other passages) seems to me quite misleading on this point.\n\n(*) One very minor question I have is why actions were softmax selected even in the quiz phase. What were the softmax parameters? And would some of the agents not perform a bit better if they maximized?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Potentially interesting, but possibly not ready yet",
            "review": "This paper aims at training agents to perform causal reasoning with RL in three settings: observational (the agent can only obtain one observational sample at a time), interventional (the agent can obtain an interventional sample at a time for a given perfect intervention on a given variable) and a counterfactual setting (the agent can obtains interventional samples, but the prediction is about the case in which the same noise variables were sampled, but a different intervention was performed) . In each of these settings, after T-1 steps of information gathering, the algorithm is supposed to select the node with the highest value in the last step. Different types of agents are evaluated on a limited simulated dataset, with weak and not completely interpretable results.\n\nPros:\n-Using RL to learn causal reasoning is a very interesting and worthwhile task.\n-The paper tries to systematize the comparison of different settings with different available data.\n\n\nCons:\n-Task does not necessarily require causal knowledge (predict the node with the highest value in this restricted linear setting)\n-Very limited experimental setting (causal graphs with 5 nodes, one of which hidden, linear Gaussian with +/- 1 coefficients, with interventions in training set always +5, and in test set always -5) and lukewarm results, that don’t seem enough for the strong claims. This is one of the easiest ways to improve the paper.\n-In the rare cases in which there are some causal baselines (e.g. MAP baseline), they seem to outperform the proposed algorithms (e.g. Experiment 2)\n-Somehow the “active” setting in which the agent can decide the intervention targets seems to always perform worse than the “passive” setting, in which the targets are already chosen. This is very puzzling for me, I thought that choosing the targets should improve the results...\n-Seem to be missing a lot of literature on causality and bandits, or reinforcement learning (for example: https://arxiv.org/abs/1606.03203, https://arxiv.org/abs/1701.02789, http://proceedings.mlr.press/v70/forney17a.html)\n-Many details were unclear to me and in general the clarity of the description could be improved\n\nIn general, I think the paper could be opening up an interesting research direction, but unfortunately I’m not sure it is ready yet. \n\n\nDetails:\n-Abstract: “Though powerful formalisms for causal reasoning have been developed, applying them in real-world domains is often difficult because the frameworks make idealized assumptions”. Although possibly true, this sounds a bit strong, given the paper’s results. What assumptions do your agents make? At the moment the agents you presented work on an incredibly small subset of causal graphs (not even all linear gaussian models with a hidden variable…), and it’s even not compared properly against the standard causal reasoning/causal discovery algorithms...\n-Footnote 1: “this formalism for causal reasoning assumes that the structure of the causal graph is known” - (Spirtes et al. 2001) present several causal discovery (here “causal induction”) methods that recover the graph from data.\n-Section 2.1 “X_i is a potential cause of X_j” - it’s a cause, not potential, maybe potentially not direct.\n-Section 3: 3^(N-1)/2 is not the number of possible DAGs, that’s described by this sequence: https://oeis.org/A003024. Rather that is the number of (possibly cyclic) graphs with either -1, 1 or 0 on the edges. \n-“The values of all but one node (the root node, which is always hidden)” - so is it 4 or 5 nodes? Or it is that all possible DAGs on N=6 nodes one of which is hidden? I’m asking because in the following it seems you can intervene on any of the 5 nodes… \n-The intervention action is to set a given node to +5 (not really clear why), while in the quiz phase (in which the agent tries to predict the node with the highest variable) there is an intervention on a known node that is set to -5 (again not clear why, but different from the interventions seen in the T-1 steps). \n-Active-Conditional is only marginally below Passive-Conditional, “indicating that when the agent is allowed to choose its actions, it makes reasonable choices” - not really, it should perform better, not “marginally below”... Same for all the other settings\n-Why not use the MAP baseline for the observational case?\n-What data does the Passive Conditional algorithms in Experiment 2? Only observations  (so a subset of the data)?\n-What are the unobserved confounders you mention in the results of Experiment 2? I thought there is only one unobserved confounder (the root node)? Where do the others come from?\n-The counterfactual setting possibly lacks an optimal algorithm? \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Promising but several shortcomings",
            "review": "\nThe paper presents a meta-learning RL framework to train agents that\ncan learn and do causal reasoning.  The paper sets up three tasks for\nagents to learn to do associational, interventional, and\ncounterfactual reasoning. The training/testing is done on all\n5-variable graphs. The authors demonstrate how the agent can maximize\ntheir rewards, which demonstrate that the agent might have learnt to\nlearn some causal structure and do reasoning in the data.\n\nReview:\n\nI think Causality is an important area, and seeing how RL can help in\nany aspect is something really worth looking into.\n\nHowever, I have a few qualms about the setting and the way the tasks\nare modeled.\n\n1. Why is the task to select the node with the highest \"value\"\n(value=expected value?  the sample? what is it?) under some random\nexternal intervention? It feels very indirect.\n\nWhy not explicitly model certain useful actions that directly query\nthe structure, such as:\n\n- selecting nodes that are parents/children of a node\n- evaluating p(x | y) or p(x | do(y))?\n\n2. The way you generate test data might introduce biases:\n\n- If you enumerate 3^(n(n-1)/2) DAGs, some of them will have loops.  Do you weed them out?\n  Does it matter?\n\n- How do you sample weights from {-1, 0, 1}? uniform?  What happens if\n  wij = 0?  This introduces bias in your training data.  This means\n  your distribution is over DAGs + weights, not just DAGs.\n\n- Your training/test split doesn't take into account certain\n  equivalence/symmetries that might be present in your training data,\n  making it hard to rule out whether your agents are in effect\n  memorizing training data, specially that the number of test graphs\n  is so tiny (300, while test could have been in the thousands too):\n\nExample, if you graph just has one causal connection with weight = 1:\n  X1 -> X2; X3; X4; X5, This is clearly equivalent to X2 -> X1; X3; X4; X5.\n  Or the structure X1 -> X2 might be present in a larger graph, example with these two components:\n  X1 -> X2; X3 -> X4 -> X5;\n\n3. Why such a low number of learning steps T (T=5 in paper) in each episode? no\nexperimentation over choice of T or discussion of this choice is\ngiven.  And it is mentioned in the findings, in several cases, that\nthe active agent is only merely comparable to the passive agent, while\none would think active would be better. If T were reasonably higher\n(not too low, not too high), one would expect to see a difference.\n\n4. Although I have concerns listed above, something about Figure 2(a)\n  seems to suggest that the agent is learning something.  I think if\n  you had tried to probe into what the agent is actually learning, it\n  would have clarified many doubts.\n\nHowever, in Figure 2(c), if the black node is -5, why is the node\nbelow left at -2.5?  The weight on the edge is 1 and other parent is\n0, so -2.5 seems extremely unlikely, given that the variance is 0.1\n(stdev ~ 0.3, so ~8 standard deviations away!).  (Similar issue in\nFigure 3c)\n\n5. Although the random choice would result in a score of -5/4, I think\n  it's quite easy and trivial to beat that by just ignoring the node\n  that's externally intervened on and assigned -5, given it's a small\n  value. This probably doesn't require the agent to be able to do\n  \"causal reasoning\" ...  That immediately gives you a lower bound of\n  0.  That might be more appropriate.\n\n  If you could give a distribution of the max(mean_i(Xi)) over all\n  graphs (including weights in your distribution), it could give an\n  idea of how unlikely it is for the agent to get a high score without\n  actually learning the causal structure.\n\nSuggestions for improving the work:\n\n- Provide results on wider range of experiments (eg more even\n  train-test split, choice of T), or at minimum justify choices\n  made. And address the issues above.\n\n- Focus on more intuitive notions that clearly require causal\n  knowledge, or motivate your objective very clearly to show its\n  sufficiency.\n\n- Perhaps discuss simpler examples (e.g., 3 node), where it's easy to\n  enumerate all causal structures and group them into appropriate\n  equivalence classes.\n\n- Please proof-read and make sure you've defined all terms (there are\n  a few, such as Xp/Xf in Expt 3, where p/f are not really defined).\n\n- You could show a few experiments with larger N by sampling from the space of all possible\n  DAGs, instead of enumerating everything.\n\nOf course, it would be great if you can probe the agent to see what it\nreally learnt. But I understand that could be a long-shot.\n\nAnother open problem  is whether this approach can scale to larger number of\nvariables, in particular the learning might be very data hungry.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper on important topic",
            "review": "This submission is an great ablation study on the capabilities of modern reinforcement learning to discover the causal structure of a synthetic environment. The study separates cases where the agents can only observe or they can also act, showing the expected gains of active intervention.\n\nThe experiments are so far synthetic, but it would be really interesting to see how the lessons learned extend to more realistic environments. It would also be very nice to have a sequence of increasingly complex synthetic environments where causal inference is the task of interest, such that we can compare the performance of different RL algorithms in this task (the authors only used one).\n\nI would change the title to \"Causal Reasoning from Reinforcement Learning\", since \"meta-learning\" is an over-loaded term and I do not clearly see its prevalence on this submission.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}