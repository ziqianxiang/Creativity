{
    "Decision": "",
    "Reviews": [
        {
            "title": "not well-written, confusing with limited novelty",
            "review": "the paper proposes a neural architecture for the task of frame interpolation. It consists of two branches where the first branch interpolates the middle frame directly, while the second branch first extracts motion vectors which are refined using a flownet network and then goes to another network to interpolate the middle frame. In general, the paper is not easily understandable. descriptions are not clear and it's hard to understand the paper. terms are stated before they are defined (e.g. loss function in the second line of section 3). motivation is not clear. there are related works in both section 1 and 2. \ntechnical novelty is limited: the proposed method consists of encoder-decoder modules that are already introduced. the complexity of architecture is not justified in the experiments. \nthe performance improvement is marginal (table 2) and extra flownet, and interpolator network (table 3, 4) is not justified in term of contribution to performance.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good writing and results, with limited novelty",
            "review": "This paper is well written and achieved good performance on Middlebury benchmark. However, I don't see much novelty. The authors claim two contributions:\n\n(1) An end-to-end unsupervised learning with the smoothness constraint for video frame interpolation.\nHowever, it is quite similar to approaches that learn optical flow in unsupervised manner, with image reconstruction loss and local smoothness loss. \n\n(2) Highly accurate frame interpolation network that integrates motion information estimated by a flow estimation network as a driver for the motion-guided frame interpolator.\nI believe this step is crucial to the final good performance, but it is similar to synctx [cvpr18]. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A paper that lacks original ideas and needs significant rewriting to better present this work and results. ",
            "review": "The paper is not well organized and contains many typos or mistakes. The claimed contributions are ambiguous, especially the “unsupervised” learning, and “smoothness constraint”. The proposed method should be evaluated on larger benchmark datasets, e.g., the UCF101 and Vimeo90K datasets, which are commonly used in the state-of-the-art algorithms. The ablation studies are insufficient by only evaluation on the proposed HCD dataset. \n\nClarity: \nModerately clear. Authors need to polish the presentation and term (e.g., SD), and fix typos (e.g., state-of-the-art) throughout the paper.\n\nOriginality:\nMinor novelty. The network takes the SepConv architecture as backbone for both the Flow Network and Interpolator Networks. The authorsThis paper aims to find important regions to classify an image.\nThe main algorithm, FIDO, is trained to find a saliency map based on SSR or SDR objective functions. The main novelty of this work is that it uses generative models to in-fill masked out regions by SSR or SDR. As such, compared to existing algorithms, FIDO can synthesize more realistic samples to evaluate.\n\nI like the motivation of this paper since existing algorithms have clear limitations, i.e., using out-of-distribution samples. This issue can be addressed by using a generative network as described in this paper.\n\nHowever, I think this approach yields another limitation: the performance of the algorithm is bound by the generative network. For example, let’s assume that a head region is important to classify birds. Also assume that the proposed algorithm somehow predicts a mask for the head region during training. If the generative network synthesizes a realistic bird from the mask, then the proposed algorithm will learn that the head region is a supporting region of SSR.\nIn the other case, however, the rendered bird is often not realistic and classified incorrectly.\nThen, the algorithm will seek for other regions. As a result, the proposed method interprets a classifier network conditioned on the generative network parameters. Authors did not discuss these issues importantly in the paper.\n\nAlthough the approach has its own limitation, I still believe that the overall direction of the paper is reasonable. It is because I agree that using a generative network to in-fill images to address the motivation of this paper is the best option we have at this current moment. In addition, authors report satisfactory amount of experimental results to support their claim.\n propose a flow derivation layer to transform convolution kernels to flows, which is the novel part.\n\nSignificance:\nModerately significant. The authors present good video frame interpolation results by inferring flow from estimated convolution kernels. \n\nPros:\n1. The interpolation performance on the Middlebury dataset is good. Especially on the 4 of the 8 compared sequences, the performance is the best with significant margins than the second best approaches. However, larger datasets, such as Vimeo90k and UCF101, should also be compared to understand the generalization of the proposed method.\n\n2. This paper proposes an interesting combination of interpolator network and flow network.  Since it is claimed that ”The interpolator networks and flow network help each other to learn efficiently”(Page2,Line9) and the two networks as a basic block can be repeatedly concatenated in the model (e.g., {interpolator, flow}->{interpolator, flow}-> ... ->{ interpolator }), it is interesting to see with how many repeated blocks the  best performance can be obtained.\n\nCons:\n1. The authors claim that the proposed method is an unsupervised learning approach. However, the term “unsupervised” might be misleading. Within the context of frame interpolation, the ground truth second frame is used as a supervision signal for training. Therefore, it is definitely not an “unsupervised” frame interpolation approach. On the other hand, if the “unsupervised learning” refers to estimating optical flow without using the ground truth flow, this paper should compare the learned optical flow with existing unsupervised learning based optical flow methods (such as Ahmadi et al in ICIP2016; Lai et al in NIPS2017; UnFlow in AAAI2018) on the MPI Sintel, KITTI, and Middlebury datasets. \n\n2. I do not see any relationship of the proposed network with the claimed contribution “smoothness constraint” since there is no flow field’s total variation loss adopted in loss function. The paper claims that “The flow network works as a refinement process that blends the smoothness constraint into the raw initial optical flow of pixels through a convolution–deconvolution neural network” (Page5, Line4-6), which has no support from the experimental results to show the difference between before and after refinement. \n\n3. This paper provides a new dataset for video frame interpolation. However, there are only 6 sequences in this dataset, which are not sufficient enough to evaluate various scenes. The challenging cases only include 3 “subtitle”, 2 “occlusion”, and 1 “complex motion” (or say sports scene, halo) sequence, which do not cover the cases of “large motion” and “small object”. The term “halo” used in Table 2/3/4/5 is not consistent with the descriptions in the body text (Page6, Line23-24, “The last sequence captures a soccer match where the movement of players is fast and complex and the ball is a small object.”) and thus should be clarified clearly.\n\n4. Although this paper obtains good results on the Middlebury dataset, it is noticed that this dataset is small with only 8 sequence for frame interpolation. There are larger benchmark datasets used in previous papers, such as the UCF101 in Liu et al in ICCV17 (379 sequences), and Vimeo90K in Xue et al in arxiv17 (3783 sequences). \n\n5. The proposed method consists of two SepConv and one Flow network, which result in approximately 3 times more parameters than the SepConv (21M parameters) model. To validate the proposed network architecture, an ablation study might be conducted by reducing the network parameters to a comparable capacity with SepConv, so that the contributions of the proposed network architecture can be understood well. Also note that CtxSyn by Niklaus et al in CVPR18 has about 13M parameters which is the second best method in Table 1.\n\n6. Since the first interpolator network can generate intermediate frames, it is better to evaluate the intermediate results as well. In that way, we can observe the improvements made from the Flow Net and Interpolator Net 2 for a better understanding of the proposed model.\n\n7. The Standard Deviation (SD) interpolation error is not defined in the paper. More importantly, since Middlebury benchmark only evaluates the Interpolation Error (IE) and Normalized Interpolation Error, I think the authors might mistakenly use SD to represent Normalized Interpolation Error (NIE). Please check the definition of NIE in Baker et al.’s paper in IJCV 2011(“A Database and Evaluation Methodology for Optical Flow”). It’s suggested to use the common evaluation metrics for a fair comparison.\n\n8. The notations for flow and interpolator network in Section 3 and the Figure 1 is messy. There are “Flow net”, “Flow Network”, “Flow Net”, “Flow network”, and “Interpolator network”, “Interpolator Net”, “interpolator network”. \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}