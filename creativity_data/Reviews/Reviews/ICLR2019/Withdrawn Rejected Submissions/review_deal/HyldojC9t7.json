{
    "Decision": "",
    "Reviews": [
        {
            "title": "a specific way of using distance to define kernel. not clear what's the benefit",
            "review": "The paper proposed a new way to define kernel functions using some distance function as random features. The paper also provides some standard analysis of the random feature approach to generalization ability. Overall, the novelty of the paper is low. It is not clear what's the benefit of this approach and it doesn't seems that one can build upon this and lead to new research directions. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This is an incremental work with insufficient depth and novelty.",
            "review": "The paper proposes a kernel function based on a dissimilarity measure between a pair of instances. A general dissimilarity measure does not necessarily have properties of a metric and the standard transformations from dissimilarities to kernel or similarity functions typically result in indefinite kernel matrices. For example, the two most frequently used transformations are negative double-centering characteristic to multidimensional scaling and the exponentiation of the negative squared dissimilarities between a pair of instances (e.g., exponentiated negative squared geodesic distance defines an indefinite kernel).\n\nThe main idea of the paper is to mimic the random Fourier features approximation of the stationary kernels and use dissimilarities as basis function. In particular, the proposed kernel function based on dissimilarities can be written as\n\nk(x,x')=\\int \\phi(w, x) \\phi(w, x') p(w) dw,\n\nwhere \\phi is a dissimilarity function and p(w) is a probability density function. This is the same format of the kernel function as the one considered in [1-3], with \\phi chosen to be different from the cosine basis function. While the focus in [1-3] was on stationary kernels, their theoretical derivations and presentation was designed for general basis functions. Having this in mind, all the theoretical derivations by the authors are readily obtained from [1-3] and I fail to see any novelty here. For example, the result in Proposition 2 follows directly from [1, Claim 1] or [2].\n\nFor the proposed kernel, I also fail to see a significant difference compared to [4] where dissimilarities are used as features. In fact, for a large number of dissimilarity functions/features and l1 penalty on the linear model the approach by [4] retrieves the proposed approximate kernel with the `optimal' density function. An experiment along these lines was, for instance, reported in [5].\n\nIn the related work part, the authors also mention previous approaches for learning with indefinite kernels and that they suffer from the non-convexity of the optimization problem for risk minimization. A recent approach builds on [6] and alleviates this shortcoming with a non-convex problem for which a globally optimal solution can be found in polynomial time (e.g., see [7]).\n\nIn the experiments, the most appropriate baseline would be the approach from [5] and yet the approach is not even listed in the related work section.\n\n[1] A. Rahimi and B. Recht (NIPS 2008). Random features for large-scale kernel machines.\n[2] A. Rahimi and B. Recht (IEEE 2008). Uniform approximation of functions with random bases.\n[3] A. Rahimi and B. Recht (NIPS 2009). Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning.\n[4] T. Graepel, R. Herbrich, P. Bollmann-Sdorra, and K. Obermayer (NIPS 1999). Classification on pairwise proximity data.\n[5] I. Alabdulmohsin, X. Gao, and X.Z. Zhang (PMLR 2015). Support vector machines with indefinite kernels.\n[6] C.S. Ong, X. Mary, S. Canu, and A. Smola (ICML 2004). Learning with non-positive kernels.\n[7] D. Oglic and T. Gaertner (ICML 2018). Learning in Reproducing Kernel Krein Spaces.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "D2KE - review",
            "review": "D2KE: From Distance to Kernel and Embedding\n\nQuality: average\nOriginality: original\nSignificance: relevant for ICLR\nPros: - interesting idea -- see detailed comments\nCons: some technical issues, validity of the results not clear -- see detailed comments\n\nI have seen this paper already at ICML as a review and I am happy to see\nthat the authors have improved the paper. Although the core idea is interesting\nand novel the paper raises still a number of questions (see below). Beside\nof some technical bits, I am (again) unhappy with the experimental evaluation\n- in particular there are strange mismatches between the results reported in the\nICML submission and this one. The method looks now better in the shown results\n- basically be removing results from ICML on another method and some additional datasets.\n\nFurther some information is not provided (parametrization,\nstandard deviation of the results, significance of the differences).\nSome results are changed unexpectedly and there is still no comparison to some more\nchallenging datasets as provided by other authors from the field. \n\ncomments\n- the paper has a number of typos in the references - please carefully\n  check and also complete missing information\n- I understand that you have to promote your approach but looking\n  on the title your objective is to go from distances to kernels\n  --> although you go a different way (which is fine) you can\n  also do this directly by using concepts as proposed by Gisbrecht et al.\n  'Metric and non-metric proximity transformations at linear costs', Neurocomputing\n  which you should at least take into account\n- to sum up a bit your related work part you should refer to a recent review\n  by Tino et. al about indefinite learning, neural computation\n- 'A line of work has therefore focused on estimating a positive-definite (PD) ...' \n  - yes and by combining the approach from the Gisbrecht paper with the one\n  of Loosli you can have e.g. an SVM in the Krein space without restriction to a single Hilbert space and\n  with a simple and clear out of sample extension to test data. So basically\n  there are ways to stay with an indefinite kernel (or non-metric dissimilarity), \n  not loosing any performance and having no need to put it into a vector space. \n  --> You could point to this option in your introduction\n- And as mentioned by a number of other authors (Pekalska, Tino, ...) there maybe\n  good reasons for not going to a PD kernel - because one may still loose information\n- 'distance kernel learning,' - this is a bit a mismatch - you either have a kernel\n  or a distance and you may define a distance based on a kernel ...\n- 'This type of kernel, however, results in a diagonal-dominance problem, where the diagonal entries of the kernel Gram matrix \n   are orders of magnitude larger than the off-diagonal entries,' -- well yes, but could \n  this not be solved by a renormalization of the kernel matrix (assuming that the diagonal elements\n  are at least positive) such that finally all diagonals are 1?\n- Eq 4 is not a so new idea. Many authors have already plugged distances into an exp to make\n  it a similarity - but this is changing the data representation (this is widely discussed\n  by pekalska). And if d is non-metric the Eq for may not even provide the 'correct' mathematical\n  formulation.  That may not be a problem - e.g. by asking - can I get any kind of\n  similar kernel to my given dissimilarities (I think there is old work around this by \n  G. Wahba et al.), e.g. by learning a proxy kernel such that the similarities of the kernel\n  are close to the dissimilarities. One may also more directly deal with the dissimilarities\n  see e.g. work of Yiming Ying about learning with dissimilarities (or so). \n  But if I would like to keep the classical formulation that from an inner product based on K\n  I can define a dissimilarity - and this should be the same like the original one which I used\n  to get K the Eq 4 may not be so nice (--> see double centering in the book of Pekalska)\n- ', our kernel in Equation (5) is always PD by' - Eq 5 or Eq 4 - later on Eq 5 is never used again\n  - cmp Algo 1 - caption/title\n- if I understand correct in Alg 1 l(x,y) - refers to a loss function (?) and y to a label - not \n  introduced anywhere - but then your approach is supervised - the classical RFF are unsupervised\n  (and hence more generic) - beside of this I may not have any label information if I go from a \n  dissimilarity to similarities --> if this is a restriction of your method you should reflect this\n  in adding e.g. ' ... in supervised learning' in the title \n- 'Since most distance measures are computationally demanding, having quadratic complexity, we adapted or implemented\n  C-MEX programs for them; other codes were written in Matlab.' - software implementation details are\n  not relevant here - please remove \n- please provide the full crossvalidation information including mean/std-dev, aligned with a significance test\n \n- 'For our new method D2KE, since we generate random samples from the distribution, we can use as many as needed to\n   achieve performance close to an exact kernel. We report the best number in the range R = [4, 4096] \n   (typically the larger R is, the better the accuracy).' - if so I will hope you tuned your R on the training data\n  and the reported values are from the test data! - please clarify\n- the other methods may also have meta-parameters e.g. C in the case of classical SVM - which values are used\n  and how are they obtained\n- classical DTW is not defined for multiple timeseries - what did you use\n- your result tables have changed compared to the ICML paper (also one method GDK_LDE by Pekalska has vanished)\n  please explain ! In particular also the runtime of RSM has nonlinearly changed between the two papers and\n  the overall results are now a bit more in favour of your method. (Table 1)\n  The dataset 'mnist-str4' has vanished as well. For the image data you also have a strange change - in the\n  ICML paper GDK_LED was best and now - because this method does not show up anymore your approach looks to be\n  the best. As the runtime change so much between the two submissions I am wondering why you still report them.\n  Basically the O-notation is telling that your method is linear (for reasonable large N and not to large R) \n  and that most other have O(N^2).\n\n  To ensure reproducability of your results I ask you to provide the respective codes e.g. on github (can be done anonymous)\n\n- Repeating myself from the last review: \n \tthere is a lot of work addressing that making a kernel psd may not be good idea - you provide experiments\n  \tfor a small number of data where your kernel is now psd but what is with the other data (where e.g. in\n  \tPekalska and followers it was shown that making them psd is bad ... ) - is your approach solving\n  \tthis - or do we end with an approach which is not very performant (in accuracy) for the hard/crucial datasets?\n\t\n\n- why do you not use some of the datasets provided by Pekalska (simbad EU project - still in the web) \n  (some maybe also be found by Loosli) to have a more realistic comparison\n- once more the number of datasets used in the evaluation is not particular large\n- still a few typos 'dissimilairty' --> spell checker\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}