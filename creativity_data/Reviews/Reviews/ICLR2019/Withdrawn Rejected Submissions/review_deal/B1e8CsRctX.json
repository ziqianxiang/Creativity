{
    "Decision": {
        "metareview": "This paper suggests the use of generative ensembles for detecting out-of-distribution samples. \n\nThe reviewers found the paper easy to read, especially after the changes made during the rebuttal. However, further elaboration in the technical descriptions (and assumptions made) could make the work seem more mature, as R2 and R1 point out. \n\nThe general feeling by reading the reviews and discussions is that this is promising work that, nevertheless, needs some more novel elements. A possible avenue for increasing the contribution of the paper is to follow R1’s advice to extract more convincing insights from the results. \n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Promising but more work needed to reach maturity"
    },
    "Reviews": [
        {
            "title": "Needs a lot of work on improving technical rigor and clarity",
            "review": "Note to Area Chair: Another paper submitted to ICLR under the title “Do Deep Generative Models Know What They Don’t Know?” shares several similarities with the current submission.\n\nThis paper highlights a deficiency of current generative models in detecting out-of-distribution based samples based on likelihoods assigned by the model (in cases where the likelihoods are well-defined) or the discriminator distribution for GANs (where likelihoods are typically not defined). To remedy this deficiency, the paper proposes to use ensembles of generative models to obtain a robust WAIC criteria for anomaly detection.\n\nMy main concern is with the level of technical rigor of this work. Much of this has to do with the presentation, which reads to me more like a summary blog post rather than a technical paper.\n- I couldn’t find a formal specification of the anomaly detection setup and how generative models are used for this task anywhere in the paper.\n- Section 2 seems to be the major contribution of this work. But it was very hard to understand what exactly is going on. What is the notation for the generative distribution? Introduction uses p_theta. Page 2, Paragraph 1 uses q_theta (x). Eq. (1) uses p_theta and then the following paragraphs use q_theta.\n- In Eq. (1), is theta a random variable?\n- How are generative ensembles trained?  All the paper says is “independently trained”. Is the parameter initialization different? Is the dataset shuffling different? Is the dataset sampled with replacement (as in bootstrapping)?\n- “By training an ensemble of GANs we can estimate the posterior distribution over model deciscion boundaries D_theta(x), or equivalently, the posterior distribution over alternate distributions q_theta. In other words, we can use uncertainty estimation on randomly sampled discriminators to de-correlate the OoD classification errors made by a single discriminator” Why is the discriminator parameterized by theta? What is an ensemble of GANs? Multiple generators or multiple discriminators or both? What are “randomly sampled discriminators”? What do the authors mean by \"posterior distribution over alternate distributions\"?\n\nWith regards to the technical assessment, I have the following questions for the authors:\n- In Figure 1, how do the histograms look for the training distribution of CIFAR? If the histograms for train and test have an overlap much higher than the overlap between the train of CIFAR and test set of any other distribution, then ensembling seems unnecessary and anomaly detecting can simply be done via setting a maximum and a minimum threshold on the likelihood for a test point. In addition to the histograms, I'd be curious to see results with this baseline mechanism.\n- Why should the WAIC criteria weigh the mean and variance equally?\n- Did the authors actually try to fix the posterior collapse issue in Figure 3b using beta-VAEs as recommended? Given the simplicity of implementing beta-VAEs, this should be a rather easy experiment to include.\n\nMinor typos:\n- ODIN and VIB are not defined in the abstract\n- Page 3: “deciscion”\n- Page 2, para 2: “log_\\theta p(x)”",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Well below the ICLR level",
            "review": "- Novelty is minimal and is well below the level required by ICLR.\n\n- The reasoning lists the problems of GANs and then the fact that GAN ensembles would target that, based on a toy example in Figure 2. \n\n- Why to choose GANs though in the first place? Given the buildup, and given the other well-known training issues about GANs, are they the right choice for the basic modeling units, i.e. the ensemble units, in such case? A GANs adversary bases its comparisons on individual data points, rather than on distribution comparisons or on groups of points like MMD, etc. I understand the reasoning behind the choice of generative models (GMs), but it is choosing GANs out of the set of GMs in this particular case that I am referring to. \n\n- The paper is quite well written. The ideas as well as the reasoning flow very smoothly. \n\n- Experiments are well prepared. \n\nRather minor:\n- page 1: \"When training and test distributions differ, neural networks may provide ...\" This is true but may be a clarification here regarding the fact that the neural networks involved with several modeling problems, e.g. the ones trained for domain adaptation or meta-learning tasks, target this shift or difference in domains, and typically provide a way to tackle this problem.\n\n\n\nUodate: Read the rebuttal. My score remains unchanged. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting combination of the previous work with useful results.",
            "review": "The authors present an OOD detection scheme with an ensemble of generative models. When the exact likelihood is available from the generative model, the authors approximate the WAIC score. For GAN models, the authors compute the variance over the discriminators for any given input. They show that this method outperforms ODIN and VIB on image datasets and also achieves comparable performance on Kaggle Credit Fraud dataset.\n\nThe paper is overall well-written and easy to follow. I only have a few comments about the work.\n\nI think the authors should address the following points in the paper.\n- What is the size of the ensemble for the experiments?\n- How does the size of the ensemble influence the measured performance?\n- It is Fast Gradient Sign Method (FGSM), not FSGM. See [1]. Citing [1] for FGSM would also be appropriate.\n\nQuality. The submission is technically sound. The empirical results support the claims, and the authors discuss the failure cases. \nClarity. The paper is well-written and easy to follow while providing useful insight and connecting previous work to the subject of study.\nOriginality. To the best my knowledge, the proposed approach is a novel combination of well-known techniques.\nSignificance. The presented idea improves over the state-of-the-art.\n\n\nReferences\n[1] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and Harnessing Adversarial Examples,” in ICLR, 2015.\n-------------------\nRevision. The rating revised to 6 after the discussion and rebuttal.\n ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}