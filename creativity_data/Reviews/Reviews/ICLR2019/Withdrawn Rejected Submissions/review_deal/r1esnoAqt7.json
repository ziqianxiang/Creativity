{
    "Decision": {
        "metareview": "This paper presents a dataset for measuring disentanglement in learned representations. It consists of MNIST digits, sometimes transformed in various ways, and labeled with a variety of attributes. This dataset is used to measure statistics of various learned models.\n\nMeasuring disentanglement is certainly an important problem in our field. This dataset seems to be well designed, and I would recommend its use for papers studying disentanglement. The experiments are well-designed. While the reviewers seem bothered by the fact that it's limited to MNIST, this doesn't strike me as a problem. We continue to learn a lot from MNIST, even today.\n\nBut producing a useful dataset isn't by itself a significant enough research contribution for an ICLR paper. I'd recommend publication if (a) it were very different from currently existing datasets, (b) constructing it required overcoming significant technical obstacles, or (c) the dataset led to particularly interesting findings.\n\nRegarding (a), there are already datasets of similar complexity which have ground-truth attributes useful for measuring disentanglement, such as dSprites and 3D Faces. Regarding (b), the construction seems technically straightforward. Regarding (c), the experimental findings are plausible and consistent with past findings (which is a good validation of the dataset) but not obviously interesting in their own right.\n\nSo overall, this seems like a useful dataset, but I cannot recommend publication at ICLR.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "a useful dataset, but not enough of a contribution for an ICLR paper"
    },
    "Reviews": [
        {
            "title": "Review: Morpho-MNIST: Quantitative Assessment and Diagnostics for Representation Learning ",
            "review": "This paper discusses the problem of evaluating and diagnosing the representations learnt using a generative model. This is a very important and necessary problem.\n\nHowever, this paper lacks in terms of experimental evaluation and has some technical flaws.\n1. Morphological properties deals with only the \"shape\" properties of the image object. However, when the entire image is subject to the generative model, it learns multiple properties from the image apart from shape too - such as texture and color. Additionally, there are lot of low level pixel relations that the model learns to fit the distribution of the given images. However, here the authors have assumed that the latent space of the generative models are influenced only by the morphological properties of the image - which is wrong. Latent space features could be affected by the color or texture of the image as well.\n\n2. Extracting morphological properties of the image is straight-foward for MNIST kind of objects. However, it becomes really difficult for other datasets such as CIFAR or some real world images. Studying the properties of a generative model on such datasets is very challenging and the authors have not added a discussion around that. \n\n3. Now assuming that my GAN model has learnt good representation in Morpho-MNIST dataset, is it guaranteed to learn good representations in other datasets as well? There is no guarantee on generalizability or extensibility of the work. ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting characterisation and extension of MNIST ",
            "review": "Authors present a set of criteria to categorize MNISt digists (e.g. slant, stroke length, ..) and a set of interesting perturbations (swelling, fractures, ...) to modify MNIST dataset. They suggest analysing performance of generative models based on these tools. By extracting this kind of features, they effectively decrease the dimmension of  data. Therefore, statistically comparing the distribution of generated vs test data and binning the generated data is now possible. They perform a thorough study regarding MNIST. Their tools are a handy addition to the analytical surveys in several applications (e.g. how classification fails), but not convincingly for generation. \n\nSince their method is manually designed for MNIST, the manuscript would benefit from a justification or discussion on the  common pitfalls and the correlation between MNIST generation and more complex natural image generation tasks. Since the presented metrics do not show a significant difference between the VAE and Vanilla GAN model, the question remains whether evaluating on MNIST is a good proxy for the performance of the model on colored images with backgrounds or not. For example sharpness and attending to details is not typically a challenge in MNIST generation where in other datasets this is usually the first challenge to be addressed. I'm not convinced that ability of a model in disentangling thickness correlates to their ability in natural image generation.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "not enough contribution",
            "review": "The author proposed an extended version of MNIS where they introduced thickening/thinning/swelling/fracture. The operation is done using binary morphological operations.\n\n* Providing benchmark data for tasks such disentanglement is important but I am not sure generating data is sufficient contribution for a paper. \n* I am not sure what conclusion I should draw from Fig 5 and Fig 6 about the data.\n* Eventually this data can become a benchmark data when it is paired with a method. Then that method/data are a benchmark.\n\n ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}