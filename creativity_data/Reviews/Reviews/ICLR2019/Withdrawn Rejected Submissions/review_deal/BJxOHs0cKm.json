{
    "Decision": {
        "metareview": "This paper proposes a generalization metric depending on the Lipschitz of the Hessian.\n\nPros: Paper has some nice experiments correlating their Hessian based generalization metric with the generalization gap, \n\nCons: The paper does not compare its results with existing generalization bounds, as there is substantial work in the area now.  It is not clear whether existing generalization bounds do not capture this phenomenon with different batch sizes/learning rates, and the necessity of having and explicit dependence on the Lipschitz of the Hessian.\n\nThe bound by itself is also weak because of its dependence on number of parameters 'm'. \n\nThe paper is poorly written and all reviewers complain about its readability.\n\nI suggest authors to address concerns of the reviewers before submitting again. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "ICLR 2019 decision"
    },
    "Reviews": [
        {
            "title": "Worth publishing work deserving a more rigorous presentation",
            "review": "The authors study generalization capabilities of neural networks local minimums thanks to a PAC-Bayesian analysis that grasps the local smoothness properties. Even if some assumptions are made along the way, their analysis provides a metric that gives insight on the accuracy of a solution, as well as an optimization algorithm. Both of these result show good empirical behavior.\n\nHowever, despite my favorable opinion, I consider that the paper presentation lacks rigor at many levels. I hope that the criticism below will be addressed in an eventual manuscript.\n\nIt is confusing that Equations (4) and (9) defines slightly differently \\sigma*_i(w*,\\eta,\\gamma). In particular, the former is not a function of \\eta. \n\nThe toy experiment of Figure 1 is said to be self-explainable, which is only partly true. It is particularly disappointing because these results appear to be really insightful. The authors should state the complete model (in supplementary material if necessary). Also, I do not understand Figures (b)-(c)-(d): Why the samples do not seem to be at the same coordinates from one figure to the other? Why (d) shows predicted green labels, while the sample distribution of (b) has no green labels?\n\nIt is said to justify the perturbed optimization algorithm that Theorem 1 (based on Neyshabur et al. 2017) suggests minimizing a perturbed empirical loss. I think this is a weak argument for two reasons:\n(1) This PAC-Bayes bounds is an upper bound on the perturbed generalization loss, not on the deterministic loss.\n(2) The proposed optimization algorithm is based on Theorem 2 and Lemma 3, where the perturbed empirical loss does not appear directly.\nThat being said, this does not invalidate the method, but the algorithm justification deserves a better justification\n\nThere is a serious lack of rigor in the bibliography:\n- Many peer-reviewed publications are cited just as arXiv preprints\n- When present, there is no consistency in publication names. NIPS conference appears as \"Advances in Neural ...,\", 'NIPS'02\", \"Advances in Neural Information Processing Systems 29\", \"(Nips)\". The same applies to other venues.\n- Both first name initials and complete names are used \n- McAllester 2003: In In COLT\n- Seldin 2012: Incomplete reference\n\n Also, the citation style is inconsistent. For instance, the first page contains both \"Din et al, (2007) later points out...\" and \"Dziugaite & Roy (2017) tries to optimize...\" \n\nTypos:\n- Page 3: ...but KL(w*+u | \\pi) => KL(w*+u || \\pi)\n- In this/our draft: Think to use another word if the paper is accepted\n- Line below Equation (5): \\nabla^2 L => \\nabla L (linear term)\n- it is straight-forward -> straightforward\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An extension of Neyshabur et al. PAC-Bayes bounds.",
            "review": "The authors prove a PAC-Bayes bound on a perturbed deterministic classifier in terms of the Lipschitz constant of the Hessian. They claim their bound suggests how insensitive the classifier is to perturbations in certain directions. \n\nThe authors also “extract” from the bound a complexity measure for a particular classifier, that depends on the local properties of the empirical risk surface: the diagonal entries of the Hessian, the smoothness parameter of the Hessian, and the radius of the ball being considered.  The authors call this “metric” “PAC-Bayes Generalization metric”, or pacGen.\n\nOverall, this seems like a trivial extension of Neyshabur et al. PAC-Bayes bounds. \n\nThe experiments demonstrating that pacGen more or less tracks the generalization error of networks trained on MNIST dataset is not really surprising. Many quantities track the generalization error (see some of Bartlett’s, Srebro’s, Arora’s work). In fact, these other quantities track it more accurately. Based on Figure 2, it seems that pacGen only roughly follows the right “order” of networks generalizing better than others. If pacGen is somehow superior to other quantities, why not to evaluate the actual bound? Or why not to show that it at least tracks the generalization error better than other quantities?\n\nThe introduction is not only poorly written, but many of the statements are questionable. Par 2: What complexity are you talking about? What exactly is being contradicted by the empirical evidence that over-parametrized models generalize? \n\nRegarding the comment in the introduction: “ Dinh et al later points out that most of the Hessian-based sharpness measures are problematic and cannot be applied directly to explain generalization.”, and regarding the whole Section 5, where the authors argue that their bound would not grow much due to reparametrization:\nIf one obtains a bound that depends on the “flatness” of the minima, the bound might still be useful for the networks obtained by SGD (or other algorithms used in practice). The fact that Dinh et al. paper demonstrates that one can artificially reparametrize and change the landscape of a specific classifier does not contradict any generalization bounds that rely on SGD finding flat minima. Dinh et al. did not show that SGD finds classifiers in a sharp(er) minima that generalize (better).\n\nIn the experiment section, the authors compare train and test errors of perturbed (where the perturbation is based on the Hessian) and unperturbed classifiers. However, they don't compare their results to other type of perturbations, e.g. dropout. It’s been shown in previous work that certain perturbations improve generalization and test error.\n\nThere are numerous typos throughout the paper.\n\n\n****************\n\n[UPDATE]\n\nI would like to thank the authors for implementing the changes and adding a plot comparing their algorithm with dropout. While the quality of the paper has improved, I think that the connection between the perturbation level and the Hessian is quite obvious. While it is a contribution to make this connection rigorous, I believe that it is not enough for a publication. Therefore, I recommend a rejection and I hope that the authors will either extend their theoretical or empirical analysis before resubmitting to other venues.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper does some extensive calculations but is weak on qualitative insights and empirical evaluation.",
            "review": "This paper gives various PAC-Bayesian generalization guarantees and some\nempirical results on parameter perturbation in training using an algorithm\nmotivated by the theory.\n\nThe fundamental issue addressed in this paper is whether parameter\nperturbation during training improves generalization and, if so, what\ntheoretical basis exists for this phenomenon.  For continuously\nparameterized models, PAC-Bayesian bounds are fundamentally based on\nparameter perturbation (non-singular posteriors).  So PAC-Bayesian\ntheory is naturally tied to parameter perturbation issues.  A more\nrefined question is whether the size of the perturbation should be\ndone on a per-parameter bases and whether per-parameter noise levels\nshould be adaptive --- should the appropriate noise level for each\nparameter be adjusted on the basis of statistics in the training data.\nAdam and RMS-prop both adapt per-parameter learning rate eta_i to be\nproportional to 1/((E g_i^2) + epsilon) where E g_i^2 is some running\nestimate of the expectation over a draw of a training point of the\nsquare of the gradient of the loss with respect to parameter i.  At\nthe end of the day, this paper, based on PAC-Bayesian analysis,\nproposes that a very similar adaptation be made to per-parameter noise\nduring training but where E g_i^2 is replaced by the RMS value \\sqrt{E\ng_i^2}.  It seems that all theoretical analyses require the square\nroot --- the units need to work.  A fundamental theoretical question,\nperhaps unrelated to this paper, is why in learning rate adaptation the\nsquare root hurts the performance.\n\nThis paper can be evaluated on both theoretical and empirical grounds.\nAt a theoretical level I have several complaints.  First, the\ntheoretical analysis seem fairly mechanical and without theoretical\ninnovation. Second, the analysis obscures the prior being used (the\nlearning bias). The paper first states an assumption that each\nparameter is a-priori taken to be uniform over |w_i| <= \\tau_i and the\nKL-divergence in the PAC-Bayes bound is then log tau_i/sigma_i where\nsigma_i is the width of a uniform posterior over a smaller interval.\nBut later they say that they approximate tau_i by |w_i| + kappa_i with\nkappa_i = \\gamma |w_i| + epsilon.  I believe this works out to be\nessentially a log-uniform prior on |w_i| (over some finite range of\nlog |w_i|).  This seems quite reasonable but should be made explicit.\n\nThe paper ignores the possibility that the prior should be centered at\nthe random initialization of the parameters.  This was found to be\nessential in Dziugaite and Roy and completely changes the dependence\nof k_i on w_i.\n\nAnother complaint is that the Hoefding bound is very loose in cases\nwhere the emperical loss is small compared to its upper bound.  The\nanalysis can be more intuitively related to practice by avoiding the\nrescaling of the loss into the interval [0,1] and writing expressions\nin terms of a maximum bound on the loss L_max.  When hat{L} << L_max\n(almost always the case in practice) the relative Chernoff bound is\nmuch tighter and significantly alters the analysis.  See McAllester's\nPAC-Bayesian tutorial.\n\nThe theoretical discussion on re-parameterization misses an important\npoint, in my opinoin, relative to the need to impose a learning bias\n(the no-free-lunch theorem).  All L_2 generalization bounds can be\ninterpreted in terms of a Gaussian prior on the parameters.  In all\nsuch cases the prior (the learning bias) is not invariant to\nre-parameterization.  All L_2 generalization bounds are subject to the\nsame re-parameterization criticism.  A prior tied to a particular\nparameterization is standard practice in machine learning for in all\nL_2 generalization bounds, including SVMs.  I do think that a\nlog-uniform prior (rather than a Gaussian prior) is superior and\ngreatly reduces sensitivity to re-parameterization as noted by the\nauthors (extremely indirectly).\n\nI did not find the empirical results to very useful.  The value of\nparameter perturbation in training remains an open question. Although\nit is rarely done in practice today, it is an important fundamental\nquestion. A much more thorough investigation is needed before any\nconclusions can be drawn with confidence. Experimentation with\nperturbation methods would seem more informative than theory given the\ncurrent state of the art in relating theory to practice.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}