{
    "Decision": "",
    "Reviews": [
        {
            "title": "Contribution weak",
            "review": "Summary: To carry out \"analytic\" NLP tasks focusing on morphology, syntax, semantics, and assuming we take embeddings from a neural translation model, which translation unit type (word, BPE, morpheme, character) should I use? What is the impact of character-level errors on performance on these tasks.\n\nIs the task which is undertaken in this paper really useful? I would have wished for conclusions to help one decide, based on, say, linguistic knowledge about tasks (is it based more on morphology, orthography, agreement, word order, etc) and language (is the language inflection-rich, does it have flexible word order, etc), which is the best unit size. The research questions and the conclusions are very limited. They make the entire exercise seem academic. The contribution is not really clear: how does one exploit this in practice? What should I have expected before doing the experiments, which I don't find at the end? Did I really need this to realise that (sec 6, Best of All Worlds) different aspects of the language are learnt by different unit sizes?\n\nThe paper is mostly clearly written, and well presented. The experiments are well executed. I am not convinced by the choice of tasks, which is not motivated in the paper (the paper only says \"we don't do sentiment analysis and question answering\", but why?). I could well have imagined a paper checking my linguistic intuition on tasks such as language modelling, agreement, sentence completion.\n\nOverall, despite the good presentation, I am skeptical about the contribution and impact of this paper.\n\n\nMiscellaneous criticism and praise\n* Table 3 is unclear: are the numbers word or character or sentence counts? Here and in the text (sec 5 \"Only 1863... for German\"), it is not clear why and how the de corpus wouold only have a cross validation set and no training/ test set? what does that mean?\n* sect 4: \"The classifier...trained for 10 epochs\": Log reg training is a convex problem, so the number of epochs should only affect the final result very little.\n* Figure 2: I don't understand the token frequency axis scale: absolute frequency? but then how to compare different unit sizes?\n* 5.1 \"character-based..much better..on less frequent and OOV words\": seems to apply on in Russian and German\n* based on the WMT18 systems, it seems that it would have been closer to the state of the art to study Transformer models rather than LSTM-based models.\n* footnote 4 is good. I wish the variance of results had been calculated. I regularly suspect that the papers draws conclusions from insignificant differences in performance.\n\nSuggestions and typos\n1 first bullet at end: I suggest to clarify \"when used to model a,b, and c at word level\", idem in line 2 of sec 5\n3 the connection between z (eg BPE) and x (word) is unclear\n3.1 gerund: why use teletype font?\n3.2 \"ranging from\" has no \"to\": suggest remove\nbelow table 3: I disagree that the tag S\\NP/NP indicates that \"in\" attaches to the verb\n4 The classifier is a *multi-category* logistic regression\n4 500 dim *2 layers * 2 directions (encoder) resp. 1 direction (decoder)\n5.4 performed worse\n6 present\n6 results",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice comparison but lacking variance discussion and clear way to usefulness.",
            "review": "The authors study the impact of using different representation units â€“ words, characters, subword units, and morphological segments on the representations learned in neural machine translation systems. The paper is very well-written and the study is convincing: there are 4 NMT data-sets used and 4 tasks used to asses the quality of representations and their robustness to noise. The results are a bit mixed: character-level representations are most robust to noise and perform best in the morphological tagging task, morphological segments perform best in syntax tasks, and so on. One problem with the study is that the architecture of the NMT network is fixed to be an LSTM. It is unclear how this affects the results: does the fact that LSTM can make many steps on character-level input help? Does it hurt? Architectural variance is not measured nor discussed at all. In the introduction, the authors say \"We make practical recommendations based on our results.\" -- this is hard to see, as the results are very mixed and using a fixed architecture makes it hard to draw any recommendations from them in a wider setting.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Weak reject",
            "review": "This paper investigates the representations learned by NMT models.  Specifically it compares the representations learned by NMT models with different types of input/output units.  The representations are compared by concatenating the hidden states into a feature vector and training classifiers for 3 different tagging tasks (morphology, syntax, semantics) on those features.\n\nThe study is complete and rigorous, and the methods are clearly explained.\n\nOne finding that is somewhat surprising is that word representations performed worst across the board.   A few concerns arise from this finding:\n-- Is this a consequence of limiting the vocabulary to 50K?  Are the word representations somewhat handicapped by this limitation?\n--  As the authors state, word vectors are currently the representation of choice in NLP applications.  Are the authors suggesting that subword representations would be preferable in these applications?  If so, it would be more convincing if the representations were compared in the context of one of these applications, such as question answering or entailment.  If this is not what the authors are suggesting, then what is the broader significance of this finding?\n\nLearning pre-trained representations of language is certainly important in many NLP applications, particularly those for which there is little available labeled data.  This appears to be the first study comprehensively comparing different units in terms of the representation quality.  It is thorough and original.  However, the authors have measured performance on morphological, syntactic, and semantic tagging.  While these tasks seem to have been chosen as being representative of raw language understanding, I'm not sure these would also reflect performance in actual NLP applications.\n\nPros\n- Experiments are rigorous and comprehensive.\n- Very clearly written and easy to understand.\n\nCons\n- Significance/relevance of these particular tasks is limited.\n- Limiting word vocabulary to 50K may be impacting results.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}