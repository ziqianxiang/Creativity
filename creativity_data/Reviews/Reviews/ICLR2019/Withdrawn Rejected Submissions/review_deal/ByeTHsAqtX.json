{
    "Decision": {
        "metareview": "The paper is overally interesting and addresses an important problem, however reviewers ask for more rigorous empirical study and less restrictive settings.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Summary review"
    },
    "Reviews": [
        {
            "title": "This paper describes an interesting phenomenon, but some of the experimental evidence is a bit lacking. ",
            "review": "This paper describes an interesting phenomenon: that most of the learning happens in a small subspace. However, the experimental evidence presented in this paper is a bit lacking. The authors also cook up a toy example on which gradient descent exhibits similar behavior. Here are a few detailed comments:\n\n1. The Hessian overlap metric is suitable for showing the gradient lies in an invariant subspace of the Hessian, but does not show it lies in the dominant invariant subspace. \n2. There are well-established notions of distances between subspaces in linear algebra, and I suggest the authors comment on the connection between their notion of overlap between subspaces and these established notions.\n3. The authors make a few statements along the lines of ``the Hessian is small, so the objective is flat''. This is a bit misleading as it is possible for the gradient to be large but the Hessian to be small.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Agreed with small subspace but not with number of classes. Needs a more thorough study.",
            "review": "The authors build on recent works that study the spectrum of the Hessian of deep networks (e.g. Sagun et al). Previous work argues that Hessian is approximately low-rank i.e. there are few large eigenvalues and many small eigenvalues. This work argues that after some training, large eigenvectors of the Hessian converges to a subspace and stays there.\n\nIntuitively, this papers message makes sense and is interesting. I also agree with the tiny subspace argument in the paper. However, I am not convinced by a couple of things and I believe further evidence is necessary. \n\n1) Authors claim the top subspace has same rank k (where k=number of classes) and backs this up with linear classifier with 2 class (toy model). It is clear that any noiseless k-class linear classifier has its gradient lie on a k dimensional subspace. Similarly, for a deep network, I agree with hessian of the final layer will be rank k however earlier layers can have different ranks as a function of complexity of the lower level representations. My worry is that perhaps the Hessian of the final layer is somehow dominating over the other ones. Ideally, I would like to see analysis of individual layers to reach a conclusion. Is first layer also approximately rank k?\n\n2) Similarly, we need to see the eigenvalue distribution of the Hessian. Say there is a single very large eigenvalue and all others are small. The same claim would still hold. So it is not clear if number of classes really plays a role from the available experiments. This can indeed happen is the classes are correlated for instance. Authors can perhaps plot gradient over top k/2 subspace to reveal if their claim is specific to number of classes.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper with supported experiments",
            "review": "This paper shows that gradient descent mostly happens in a tiny subspace which is spanned by the top eigenvectors of the Hessian. Empirical results are shown to support the claim. This finding is interesting and provides us some insights to design more efficient optimization algorithms. Overall, this paper is interesting and easy to follow. \n\nThe experiments in section 2 do a decent job supporting the claim that gradient descent happens in a tiny subspace and the subspace is mostly preserved over long periods of training. However, I would like to add a couple more points to the discussion: \n\n- It's not surprising that the magnitude of gradient is larger in the high curvature directions, which means that the learning would always first happen in top subspace if the learning rate is small enough. It would be interesting to tune the parameter of learning rate to see if the phenomena would occur across different learning rate (especially large learning rate).\n- The argument of gradient descent happening in a tiny space is quite obvious if the Hessian has only a few large eigenvalues. Therefore, it would be interesting to discuss the spectrum of the Hessian a little bit.\n- Contrary to plain gradient descent, natural gradient is able to learn low curvature directions (small eigenvalues). It would be interesting to show some experiments with natural gradient methods.\n\nFollowing section 2, the authors give a toy model to further backup their claims. However, I find the example is too restrictive and may not explain why the subspace would be preserved over the training. If I understand right, the loss function of the toy model is convex and Hessian is a constant over time. For this kind of toy model, the Hessian (or equivalently the Fisher matrix) only depends on the input distribution, so it's easy to see that the Hessian would be low-rank and preserved throughout the training. However, neural networks are highly non-convex, so it's unclear to me whether the implications of the toy model would generalize. I encourage the authors to analyze more complicated models. \n\nTo summarize, I think this paper is interesting and well-written. However, it lacks convincing explanation why the subspace would preserve over the training (to me, it's more interesting than the point that gradient descent happens in tiny subspace). Anyway, it is not completely reasonable to expect all such possible discussions to take place at once. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}