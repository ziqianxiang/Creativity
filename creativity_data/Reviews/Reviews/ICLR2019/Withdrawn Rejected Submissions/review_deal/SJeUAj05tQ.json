{
    "Decision": {
        "metareview": "The paper provides a distributed optimization method, applicable to decentralized computation while retaining provable guarantees.  This was a borderline paper and a difficult decision.\n\nThe proposed algorithm is straightforward (a compliment), showing how adaptive optimization algorithms can still be coordinated in a distributed fashion.  The theoretical analysis is interesting, but additional assumptions about the mixing are needed to reach clear conclusions: for example, additional assumptions are required to demonstrate potential advantages over non-distributed adaptive optimization algorithms.\n\nThe initial version of the paper was unfortunately sloppy, with numerous typographical errors.  More importantly, some key relevant literature was not cited:\n- Duchi, John C., Alekh Agarwal, and Martin J. Wainwright. \"Dual averaging for distributed optimization: Convergence analysis and network scaling.\" IEEE Transactions on Automatic control 57.3 (2012): 592-606.\nIn addition to citing this work, this and the other related works need to be discussed in relation to the proposed approach earlier in the paper, as suggested by Reviewer 3.\n\nThere was disagreement between the reviewers in the assessment of this paper.  Generally the dissenting reviewer produced the highest quality assessment.  This paper is on the borderline, however given the criticisms raised it would benefit from additional theoretical strengthening, improved experimental reporting, and better framing with respect to the existing literature.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Borderline paper: distributed optimization algorithm with analysis"
    },
    "Reviews": [
        {
            "title": "This paper proposes a consensus-based distributed method, namely DADAM, for online optimization. The technical details are well presented and the empirical results are convincing. ",
            "review": "The proposed DADAM is a sophisticated combination of decentralized optimization and the adaptive moment estimation. DADAM enables data parallelization as well as decentralized computation, hence suitable for large scale machine learning problems. \n\nCorollary 10 shows better performance of DADAM. Besides the detailed derivations, can the authors intuitively explain the key setup which leads to this better performance?\n\nThe experimental results are mainly based on sigmoid loss with simple constraints. The results will be more convincing if the authors can provide studies on more complex objective, for example, regularized loss with both L2 and L1 bounded constraints.  \n\nTh experimental results in Section 5.1 is based on \\beta_1 = \\beta_2 = \\beta_3 = 0.9. From  the expression of \\hat v_{i,t} in Section 2, this setting implies the most recent v_{i,t} plays a more important role than the historical maximum, hence ADAM is better than AMSGrad. I am curious what the results will look like if we set \\beta_3 as a value smaller than 0.5. ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Novel algorithm for an important problem but not sufficiently justified theoretically or empirically.",
            "review": "This paper presents a consensus-based decentralized version of the Adam algorithm for online optimization. The authors consider an empirical risk minimization objective, which they split into different components, and propose running a separate online optimization algorithm for each component, with a consensus synchronization step that involves taking a linear combination of the parameters from each component before applying each component's individual parameter update. The final output is a simple average of the parameters from each component. \n\nThe authors study the important problem of distributed optimization and focus on adapting existing state-of-the-art methods to this setting. The algorithm is clearly presented, and to the best of my knowledge, original. The fact that this work includes both theoretical guarantees for the convex and non-convex settings as well as numerical experiments strengthens the contribution.\n\nOn the other hand, I didn't find the actual method presented by the authors to be motivated very well. The main innovation with respect to the standard Adam/AMSGrad algorithm is the use of a mixing matrix W, but the authors do not discuss how the choice of this matrix influences the performance of the algorithm or how one should specify this input in practice. This seems like an important issue, especially since all of the bounds depend on the second singular value of this matrix. Moreover, arguments such as Corollary 10 do not actually imply that DADAM outperforms ADAM when this singular value is large, making it difficult to assess the impact of this work. The numerical experiments also do not test for the statistical significance of the results. \n\nThere are also many typos that make the submission seem relatively unpolished.\n\nSpecific comments:\n1. page 1: \"note only\". Typo.\n2. page 2: \"decentalized\". Typo.\n3. page 2: \"\\Pi_X[x]. If \\Pi_X(x)....\" Inconsistent notation.\n4. page 3: \"largest singular of matrix\". Typo.\n5. page 3: \"x_t* = arg min_{x \\in X} f_t(x)\". f_t isn't defined in up to this point.\n6. page 4: \"network cost is then given by f_t(x) = \\frac{1}{n} \\sum_{i=1}^n f_{i,t}(x)\" Should the cost be  \\frac{1}{n} \\sum_{i=1}^n f_{i,t}(x_{i,t})? That would be more consistent with the definition of regret presented in Reg_T^C. \n7. page 4: \"assdessed\". Typo.\n8. page 4: \" Reg_T^C := \\frac{1}{n} \\sum_{i=1}^n \\sum)_{t=1}^T f_t(x_{i,t})...\" Why is this f_t and not f_{i,t}?\n9. page 4: \"\\hat{v}_{i,t} = v_3 ...\" You should reference how this assignment in the algorithm relates to the AMSGrad algorithm. Moreover, you should explain why you chose to use a convex combination in the assignment instead of just the max.\n10. page 5: Definition 1. This calculation should be derived and presented somewhere (e.g. in the appendix).\n11. page 5: Assumption 3. The notation for the stochastic gradient is not very clear and easily distinguishable from the notation for the deterministic gradient.\n12. page 5: Theorem 4. D_T can be very large in the bound, which would make the upper bound meaningless. Can you set hyperparameters in such a way to minimize it? Also, what is the typical size of \\sigma_2(W) that one would incur?\n13. page 6: Remark 6. This remark seems misleading. It ignores the log(T) and D_T terms, both of which may dominate the data dependent arguments.\n14. page 6: \"The update rules \\tilde{v}_{i,t}...\". \\tilde{v}_{i,t} is introduced but never defined.\n15. page 6: Last display equation. The first inequality seems like it can be an equality.\n16. page 7: Equation (14). Doesn't the presence of \\sigma_2(W) imply that the O(1/T) term may not be negligible? It would also be helpful to give some examples of how large T needs to be in (15a) and (15b) in order for this statement to take effect.\n17. page8: \"distributed federated averaging SGD (FedAvg)\". What is the reference for this? It should be included here. It should probably also be mentioned in the introduction as related work.\n18. page 9: Figure 1. Without error bars, it is impossible to tell the statistical significance of these results. Moreover, how sensitive are these results to different choices of hyperparameters?\n19. page 9: \"obtain p coefficients\". What is p in these experiments?\n20. page 9: Metropolis constant edge weight matrix W\". What is \\sigma_2(W) in this case?\n21. page 10: Acknowledgements. This shouldn't be included in the submission.\n\n\n\n\n \n\n\n\n\n\n\n\n  ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A consensus-based distributed adaptive gradient method for online optimization",
            "review": "Title: DADAM: A consensus-based distributed adaptive gradient method for online optimization\n\nSummary: \n\nThe paper presented DADAM, a new consensus-based distributed adaptive moment estimation method, for online optimization. The author(s) also provide the convergence analysis and dynamic regret bound. The experiments show good performance of DADAM comparing to other methods. \n\nComments: \n\n1) The theoretical results are nice and indeed non-trivial. However, could you please explain the implication to equation (7a)? Does it have absolute value on the LHS? \n\n2) Can you explain more clearly about the section 3.2.1? It is not clear to me why DADAM outperform ADAM here. \n\n3) Did you perform algorithms on many runs and take the average? Also, did you tune the learning rate for all other algorithms to be the best performance? I am not sure how you choose the parameter \\alpha here. What if \\alpha changes and do not base on that in Yuan et al. 2016? \n\n4) The deep learning experiments are quite simple. In order to validate the performance of the algorithm, it needs to be run on more datasets and networks architectures. MNIST and CIFAR-10 and these simple network architectures are quite standard. I would suggest to provide more if the author(s) have time. \n\nIn general, I like this paper. I would love to have discussions with the author(s) during the rebuttal period. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}