{
    "Decision": {
        "metareview": "This paper focuses on neural network models for source code edits. Compared to prior literature that focused on generative models of source codes, this paper focuses on the generative models of edit sequences of the source code. The paper explores both explicit and implicit representations of source code edits with experiments on synthetic and real code data.\n\nPros:\nThe task studied has a potential real world impact. The reviewers found the paper is generally clear to read.\n\nCons:\nWhile the paper doesn't have a major flaw, the overall impact and novelty of the paper are considered to be relatively marginal. Even after the rebuttal, none of the reviewers felt compelled to increase their score. One point that came up multiple times is that the paper treats the source code as flat text and does not model the semantic and syntactic structure of the source code (via e.g., abstract syntax tree). While this alone would have not been a deal-breaker, the overall substance presented in the paper does not seem strong. Also, the empirical results are reasonable but not impressive given that the experiments are focused more on the synthetic data, and the experiments on the real source code are weaker and less clear as has been also noted by the fourth reviewer. \n\nVerdict:\nPossible weak reject. No significant deal breaker per say but the overall substance and novelty are marginal.",
        "confidence": "2: The area chair is not sure",
        "recommendation": "Reject",
        "title": "the overall substance and novelty are marginal."
    },
    "Reviews": [
        {
            "title": "Great start for the novel task of modeling source code edits",
            "review": "The paper presents a very interesting new task of modeling edits to a piece of source code. The task also has immediate practical applications in the field of Software Engineering to help with code reviews, code refactoring etc. \n\nPros:\n1. The paper is well-written and easy to follow.\n2. The task is novel (to my knowledge) and has various practical applications.\n3. Many obvious questions that would arise have been answered in the paper for eg., the contrast between explicit and implicit data representations.\n4. The use of synthetic data to learn about the types of edits that the models can learn is a very good idea and its inclusion is much appreciated.\n5. Evaluation on a very large real dataset demonstrates the usefulness of the model for real world tasks.  \n\nCons:\n\nIn general, I really like the task and a lot of the models and experiments, but the description of the real world experiments is severely lacking in information and results. Also, there are many unanswered questions about the synthetic experiments.\n\n1. Firstly, where is the data obtained from? Who are the programmers? What was the setting under which the data was collected in ?\n2. The paper doesn't provide examples from this real world dataset nor are there examples of model predictions on this dataset.\n3. What are the kinds of edits on the real world dataset? What happens if someone adds a 100 lines to a file and saves it? How is this edit added to the dataset?\n4. Some Error analysis on the real world data? It's hard to understand how the model is doing by just reporting the accuracy as 61.1%. Lots of the accuracy points maybe obtained for obvious commonplace edits like keywords, punctuation etc.. ? \n5. Some more dataset stats related to the real world data. For eg., how many tokens are in each snapshot?\n6. \"We evaluate models on their ability to predict a sequence of subtokens up to the next token boundary\" <-- Require more details about this. This section needs more clarity, its hard to interpret the results here.\n7. Are you going to release this real world dataset?\n8. If the real world dataset is large, why don't you report running time stats on that? Why use the synthetic dataset for testing scalability? If you have 8 million edits, that seems like a big enough dataset. How long did your models take on these?\n\nRe: the synthetic dataset\n\n1. It's nice that you've tested out different single synthetic edit patterns, but what happens in a dataset with multiple edit patterns? Because this will be the scenario in a real world dataset.\n2. What happens when the order of edits is different in the prediction but the outcome is the same? Suppose we have edit A followed by B and the model predicts B followed by A, but the end result is the same? The accuracy metric will fail here? How can this be addressed?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Promising work for a new task",
            "review": "Although the subject of the task is not quite close to my area and the topic of programming language edit is relatively new to me, it is a comfortable reviewing for me thank to the well-written paper. This paper formalize a new but very interesting problem that how to learn from and predict edit sequence on programming language. \n\nPros:\n+ This paper proposed two different data representation for the tokens in text, implicit and explicit. The paper starts with a very interesting problem of programming language editing, in that the intend of source code developer's intent is predicted. \n+ The two different representations are well described, and dis/advantages are well elaborated.\n+ The writing is very technical and looks solid.\n+ Both synthetic dataset and real source code dataset are exploit to evaluate the performance of all models proposed. \n\nQuestions:\n1.\tThe content of text supposed to be programming language, but neither the model design nor synthetic dataset generation specify the type of text. \n2.\tFurther, if the text type is specified to be source code, I suppose each token will has its own logical meaning, and a line of source code will have complex logic structures that is not necessarily a flat sequence, such an “if…else…”, “try…catch…”, “switch…case…” etc. How do you address this issue with sequence models such as LSTM?\n3.\tIn generating synthetic dataset, where is the vocabulary from?\n\nMinor issues:\n1.\tThe first sentence in Abstract doesn’t look right: “Programming languages are emerging as a challenging and interesting domain for machine learning”. I suppose you meant: “Programming language generation/edit….”\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Good baselines for a new task",
            "review": "The paper provides good baselines for predicting edits in a text (evaluated on source code) learned from a history of changes. This is an interesting problem that has not beed systematically studied in literature, with the exception of several sporadic works from the software engineering community. Fully predicting the code text writing process as opposed to the code itself is an interesting task with possible big impact, IF the accuracy of this edit model manages to significantly outperform simple left-to-right code text prediction techniques.\n\nOne of closest related works to this A somewhat similar system with language models for predicting APIs based on sequences is [1], it would help to compare to it at least on a conceptual level. is [2] that predicts if a \"change\" is complete i.e if it misses to complete a change. However it does not predict the entire edit process, but only the last missing piece of a change (usually a bug if it is missed).\n\nPro:\n - First fine grained text (code) evolution evaluation and formulation of the challenge to provide labels for predicting the process of code writing.\n - Discussion about effectiveness of the introduced explicit vs implicit tasks and good trade-offs discussion.\n - Techniques are applicable and interesting beyond code.\n - Well-written and easy to follow text\n\nAgainst:\n - The introduced models are based on simple sequence representations of code tokens. Once more semantic representation (let's say ASTs) are taken, the implicit attention techniques may need to also be updated.\n - It would help to elaborate more on the process defined in 5 for the real dataset that given a pair of code snapshots, assumes that one is obtained by simply inserting/removing the tokens in sequence. In particular, it could be that a candidate model predicts a different sequence with the same results in the snapshots. Then the training loss function should not penalize such solutions, but it will if the sequence is not strictly left-to-right. Did you need to tweak something here especially for larger changes?\n\n[1] Anh Nguyen, Michael Hilton, Mihai Codoban, Hoan Nguyen, Lily Mast, Eli Rademacher, Tien Nguyen, Danny Dig. API Code Recommendation using Statistical Learning from Fine-Grained Changes\n[2] Thomas Zimmermann, Andreas Zeller, Peter Weissgerber, Stephan Diehl. Mining version\nhistories to guide software changes.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Solid step forward for NNs and source code",
            "review": "Summary: The authors study building models for edits in source code. The application is obvious: a system to accurately predict what the next edit should be would be very valuable for developers. Here, edits are modeled by two types of sequences: one that tracks the state of all edits at each time step (and is thus very long), and one that contains the initial step and a changelist that contains the minimal information required to derive the state at any time. The authors train models on top of both of these representations, with the idea being to match the performance of the explicit (heavy) model with the implicit model. This is shown to be challenging, but a clever model is introduced that achieves this, and is thus the best of both worlds. There are synthetic and real-world code (text) edit experiments.\n\nStrengths: The problem is well-posed and well-motivated. There's a nice application of powerful existing models, combined and tailored to the current work. The writing is generally quite clear. The number of experiments is quite solid. \n\nWeaknesses: The main flaw is that nothing here is really specifically for souce code; the authors are really just modeling edits in text sequences. There's not an obvious way to integrate the kinds of constraints that source code typically satisfies either. There's some confusion (for me) about the implicit/explicit representations. More questions below.\n\nVerdict: This is a pretty solid paper. It doesn't quite match up to its title, but it sets out a clearly defined problem, achieves its aims, and introduces some nice tricks. Although it doesn't produce anything genuinely groundbreaking, it seems like a nice step forward.\n\nComments and Questions:\n\n- The problem is written in the context of source code, but it's really setup just for text sequences, which is a broader problem. Is there a way the authors take can advantage of the structural requirements for source code? I don't see an obvious way, but I'm curious what the authors think.\n\n- What's the benefit of using the implicit representation for the positions? The explicit/implicit  position forms are basically just using the permutation or the inverse permutation form, which are equivalent. I don't see directly what's saved here, the alphabet size and the number of integers to store is the same.\n\n- Similar question. The implicit likelihood is s^0, e^(1),...,e^(t-1), with the e^(i)'s being based on the implicit representations of the positions. Seems like you could do this with the *explicit* positions just fine, they carry enough information to derive s^(i) from s^(i-1). That is, the explicit/implicit problems are not really related to the explicit/implicit position representations.\n\n- Just wanted to point out that this type of approach to sequences and edits has been studied pretty often in the information/coding theory communities, especially in the area of synchronization. There, the idea is to create the minimal \"changelist\" of insertions/deletions from two versions of a file. This could come in handy when building the datasets. See, for example, Sala et al \"Synchronizing files from a large number of insertions and deletions\".\n\n- The problem statement should be stated a bit more rigorously. We'd like to say that the initial state is drawn from some distribution and that the state at each time forms a stochastic process with some transition law. As it stands the problem isn't well-defined, since with no probability distribution, there's nothing to predict and no likelihood.\n\n- The \"analogical decoder\" idea is really nice.\n\n- For the synthetic dataset, why are you selecting a random initial string, rather than using some existing generative text or source code model, which would get you synthetic data that more closely resembles code?\n\n- I really liked the idea of using an oracle that gives the position as upper bound. Would it make sense to also have the opposite oracle that gives the edit symbol, but doesn't tell the location? I'm really curious which is the \"harder\" task, predicting the next symbol or the next location. In the information-theory setting, these two are actually equally hard, but the real-world setting might be pretty different. It would also be interesting to train models on top of the POMP. That would produce genuine upper bounds to the model performances. \n\n- The explicit baseline model performs very well on all the edit types in Table 1. Are there cases where even this explicit case works poorly? Is the improved implicit model *always* upper bounded by the explicit model (to me it seems like the answer should always be yes, but it would be interesting to check it out for cases where explicit is not very high accuracy). ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}