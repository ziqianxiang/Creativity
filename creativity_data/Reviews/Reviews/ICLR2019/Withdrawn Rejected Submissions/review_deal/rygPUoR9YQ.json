{
    "Decision": "",
    "Reviews": [
        {
            "title": "Incremental novelty, weak experimental evaluation, simple test cases ",
            "review": "In this paper a method of generating compositional images using conditional GANs in proposed. The proposed GAN model is conditioned on two input images concatenated channel-wise and is aimed to generate images from a target distribution of images whose content is a composition of the two input images. In all settings, it is assumed that the ground truth foreground masks of the inputs and the target composite images are available; these segmentation masks are used throughout the learning and the backgrounds are removed for simplicity (explained in paragraph 2 of section 3.3, paragraph 1 of section 3.4, etc)\n\n-----Pros-----\n\n-Being able to generate compositional images is an ambitious goal and interesting application.\n-The Supplemental Material in the appendix provides good visualizations.\n\n-----Cons-----\n\n* This paper does not provide a coherent technical novelty. Essentially, the proposed method combines several prior works and uses different tricks to make the final compositional image. Here are some of them:\n\n-A conditional GAN similar to Goodfellow 2016 and Mirza & Osindero (2014) and Isola et al 2017\n-A relative appearance flow network (RAFN) which has an encoder-decoder architecture similar to Zhou et al. 2016\n-A spatial transformer network(STN) Jaderberg et al 2015\n-A self-supervised inpainting network of Pathak et al 2016\n-An Inference Refinement Network which is similar to Azadi et al 2017\n\n* The applicability and extendibility of the proposed method is not demonstrated: \n-Does the proposed method only work with two input images? \n-How would the proposed method work if three or more input images were provided (similarly concatenated channelwise). What is the actual application of this model if only works with two images? \n-How can one extend it to a more general use case? \n-Why is it better to generate compositional images conditioned on images rather than lingual phrases describing the scene?\n\n* The experimental result does not support the claims of the paper for generating compositional images. Evaluation based on known criteria is not done and no comparative study with prior work is conducted.\n\n- The test cases are very simplistic. While the paper claims about dealing with challenging object compositional problems such as “3D object translations” and “view points” and “occlusions” there are only two test cases: (a)chair and desk (b) bottle and basket. These are very narrow test cases. Even in these two test cases the results are not satisfying. While chair can be behind the desk it can also be next to it! And many diverse situations can happen for composing a chair and a desk. However, all the provided results show only one compositional pose and these modes of diversity have never appeared in the results. The same issue exists with “bottle and basket”. The generated examples only show a bottle inside a basket. This is no different than if the GAN was conditioned on the lingual phrase of “bottle inside basket” rather than images.\n\n-The generated images are not evaluated based on any known criteria of image generation. For example, Inception Score (IS) could be used to quantitatively evaluate (a) the quality of the generated images and (b) their diversity. Other known evaluation criteria are Fréchet Inception Distance (FID), precision, recall, F1 score, etc. Even just a simple log-likelihood is not measured to quantitatively show the performance of the proposed approach.\n\n* I strongly suggest improving the the presentation of the paper. There are many formulations which are taken from prior GAN works, there is no need to repeat writing these formulations. Summarizing those formulations or putting them in the appendix can open up space so that you can bring in some of the qualitative results inside the main manuscript.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An interesting but tricky method for compositional image generation",
            "review": "[Overview]\n\nIn this paper, the authors proposed a new method for compositional image generation, called compositional GAN. Given two object images, the proposed model could compose them through spatial transformations into a single image which has a reasonable object layouts. The authors worked on two objects and considered both paired and unpaired training cases. The experimental results shows that the proposed model could learn to compose two objects into a single image with plausible relative layouts.\n\n[Strengths] \n\n1. the authors proposed an interesting method to compose the objects into a single image with meaningful layouts. To achieve this, the authors first used a relative appearance flow network to perform transformation on each object image, and then learns a spatial transformation for each object, through which the file image is generated by putting these two transformed objects into a clean scene.\n\n2. The authors considered both paired training and unpaired training, and also proposed a post-processing method for the image refinement during inference time. In the experiments, the authors presented some generation results on the chair-table and basket-bottle samples.\n\n[Weaknesses]\n\n[1] LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation. Yang et al.\n[2] ST-GAN: Spatial Transformer Generative Adversarial Networks for Image Compositing. Lin et al.\n[3] Language-Driven Synthesis of 3D Scenes from Scene Databases. Ma et al.\n\n1. The paper did not present and explain the proposed method clearly. The writing is organized poorly and the formulas are sloppy and scattered. The pipeline Fig.1 is hard to follow as well.  \n\n2. The experiments are mainly performed on two composition cases, i.e., chair-table and basket-bottle. Without extensive experiments on various object categories, I can hardly buy this method and admit the contributions of the proposed model.\n\n3. As far as I know, there are a number of work that cope with more complicated and natural images compositions [1][2][3], and these methods do not have the paired supervision, or even do not have the input object images. Also, this proposed method does not take the background into account, which I think is important for a natural image and also challenging due to the contextual constraints. \n\n4. Also, I doubt the generalization ability to other objects. The proposed model needs to be trained specially for each pair of object. Firstly, this is not possible in practice since we usually can not always have the training data for combinational number of object pairs.  I would suggest the authors increase the diversity of the training data and demonstrate the generalization ability of the proposed method. Since the authors used ShapeNet. I think it would be straightforward to use more object categories into account. Moreover, the authors can also use the dataset provided in [2] or [3].\n\n[Summary]\n\nThis paper proposed a method for image generation based on composition. The experiments are performed on two paired object, which demonstrate the effectiveness to some extent. However, I doubt the generalization ability of the proposed method, and the strong assumptions made by the authors for simplicity (no background, strong supervision even in unpaired case) also make it unclear to me regarding the contributions. Overall, I think the authors should perform more experiments to show the generalization ability of the proposed model to more object categories and novel combinations.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not much novelty/limited scope",
            "review": "The authors present a model to compose two synthetic objects into a whole. The idea is to learn networks that can perform geometric transformations to objects, compose them into a target configuration and then learn to decompose. Results are shown on synthetic data of chair and table; bottle and basket, and one real dataset. Presumably, the idea is that the learned representations can generalize to some other task where object compositionality is useful; however I did not see such an experiment.\n\nIn this paper, the authors suggest that they have learnt object compositionality without any \"explicit\" prior information about object layout. But the use of segmentation masks seems to be an important prior, which somewhat weakens the claim, is it not?\n\nI found the paper somewhat hard to parse owing to excessive use of notation and verbose writing. It is better for the authors to be precise and to the point. It took me 3-4 readings of the paper to understand the setup and architectures. The novelty of the paper is somewhat limited - it mostly consists in plugging together existing architectures in a somewhat obvious and incrememental fashion. The experiments are fine, but they left me with questions:\n\n1. Is it a surprise that the network learnt to compose objects and inpaint over occluded regions? Given that the data and training was explicitly setup to achieve this in a carefully controlled synthetic environment, I find it not a very interesting result.\n\n2. What might have been more interesting is if the representations learnt were used for various tasks such as 3D pose estimation. Did they try such experiments?\n\n3. For all the 3 datasets, what are the failure cases? How far out of train regime can you go before the inference network also stops working? \n\nI request the authors to address the above questions in their rebuttal.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}