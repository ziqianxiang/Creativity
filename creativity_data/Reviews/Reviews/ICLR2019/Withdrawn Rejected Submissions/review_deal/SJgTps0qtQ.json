{
    "Decision": {
        "metareview": "The paper presents a strategy for randomizing the underlying physical hyper-parameters of RL environments to improve policy's robustness. The paper has a simple and effective idea, however, the machine learning content is minimal. I agree with the reviewers that in order for the paper to pass the bar at ICLR, either the proposed ideas need to be extended theoretically or it should be backed with much more convincing results. Please take the reviewers' feedback into account and improve the paper.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "The paper needs improvement"
    },
    "Reviews": [
        {
            "title": "An interesting view point on robustness.",
            "review": "This paper investigated the robustness of RL policies learning under different environmental conditions. \n\nBased on the observations that policies learnt in different experimental settings lead to different generalizability, the authors proposed an EXP3 based reward-guided curriculum for improving policy robustness. The algorithm was tested on inverse pendulum, cart-pole balancing, and ball-pushing in OpenAI gym.\n\nThe paper is well-organized and easy to understand. Written errors didn't influence understanding. Papers in the references were not properly cited.\n\nIt is an interesting discovery that different environment brewed different policies with different robustness/generalizability in daily life. However, these are also easily derivable in physics, especially in the three experiments tested in the paper. It would be more complete to compare with PID controllers.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Curriculum design for dynamics randomization with a Bandits style method during training. ",
            "review": "The paper looks at the problem of generalization across physical parameter varaition in learning for continuous control. The paper presents a method to develop a sampling based curriculum over env. settings for training robust agents. \n\n\n* The paper makes an interesting observation on inadvertent generalization in robust policy learning. \nHowever, the examples in both the cartpole and the pendulum cases seem not to be watertight. \nFor instance, the authors claim that \nBut from a dynamical system perspective in both cases, the controller is operating near limits. \nThe solution and subsequent generalization depend more on the topology of the solution space. \nA heavy Pendulum is an overdamped system and required the policy to operate at the limits of action to generate momentum for swing up. Hence a solution for a lighter pendulum in implicitly included. Similarly, the rolling ball is an underdamped system, and where the policy operates near zero limits in light ball case to prevent the system from going unstable. Adding mass results in damping which makes it easier. In this case, as well the solution space is implicitly contained.\n\n\nBut this is not a novel observation. Similar observations have been made for Robust control and Model-Reference Adaptive Control. \nThe paper also overlooks a number of related works in model-free randomization [4], adaptive randomization [3], adversarial randomization [5,6]. The method also does not compare with model-based methods for adaptive policy learning and iLQR based methods to handle this problem [2, 7].\n\n\nThe argument that the method is model-free is perhaps not as acceptable since the model parameters need to be known apriori for adaptation. The policy itself may be model-free but that is a design choice. \nA good experimental evaluation for this is generalization across known unknowns and unknown unknowns. \n\n\n* The algorithm itself is reasonable but the problem setup and choice of a discrete dynamics parameter choices are questionable. The bandit style method operates over a discrete decision set. \nIt also assumes in the multi-parameter setting that they are independent, which may not be true very often. \n\nThe algorithm proposed itself isnt novel, but would have been justified if the results supported the use of such a method. \n\n* Experiments are quite weak. \nBoth the experimental domains are rather simplistic with smooth nonlinear dynamics. There are more sophisticated and interesting continuous control environments such as control suite [1] or manipulation suite [2].  \n\nIt would be useful to see how tis method works in more complicated domains and how the performance compares with simpler methods such as joint brute-force randomization both in performance and in computation.  \n\nQuestions: \n1. Please provide details of Algorithm 1. How are the quantities K and M related? \n2. What is the process of task initialization? What information is required and what priors are used. Uniform prior over what range?\n\n\nIn summary, the authors explore an interesting adaptive curriculum design method. However, in its current form, the work needs more thought and empirical evaluation for the sake of completeness. \n\n\nReferences:\n1. Model Reference Adaptive Control [https://doi.org/10.1007/978-1-4471-5102-9_116-1\n]\n2. ADAPT: Zero-Shot Adaptive Policy Transfer for Stochastic Dynamical Systems [https://arxiv.org/abs/1707.04674]\n3. EPOpt: Learning Robust Neural Network Policies Using Model Ensembles [https://arxiv.org/abs/1610.01283]\n4. Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World\n[https://arxiv.org/abs/1610.01283]\n5. Certifying Some Distributional Robustness with Principled Adversarial Training [https://arxiv.org/pdf/1710.10571.pdf]\n6. Adversarially Robust Policy Learning: Active Construction of Physically-Plausible Perturbations [http://vision.stanford.edu/pdf/mandlekar2017iros.pdf]\n7. Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization [https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf]\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The papers proposes methods to robustify reinforcement learning algorithms against environment uncertainty which arises due to parametric variability. This is a interesting paper with promising results. What would make this paper a clear accept is the addition of experiments with high dimensional systems with more unknown parameters.  ",
            "review": "- Does the paper present substantively new ideas or explore an under-explored or highly novel question? \n\nThe paper claimed that there is limited work on the investigating the sensitivity of RL caused by the physics variations of the environment, such as object weight, surface friction, arm dynamics, etc. So the paper proposed learning a stochastic curriculum, guided by episodic reward signals (which is their contribution compared with previous related work) to develop policies robust to environmental perturbation.  Overall the combination of ideas is novel but the experimental results are limited in scope. \n\n- Does the results substantively advance the state of the art?\n\nThe results advance the state of the art, since they are compared against : 1) the best results observed via a grid search (oracle) on policies trained exclusively on specific individual environment settings; 2) Policies trained under a mixed training structure, where the environment settings are varied every episode during training, with the episode settings drawn uniformly at random from a list of values of interest. Their 3 experiment results are competitive with 1) and much better than 2).\n\n- Will a substantial fraction of the ICLR attendees be interested in reading this paper? \n\nYes, because the robustness of RL policies to changes in the physic parameters of the environment has not been well explored. Although previous investigations exist, and this paper’s algorithm is the combination of EXP3 and DDPG, it is still interesting to see them combined together to solve model uncertainty problem of RL with very good simulation results.\n\n- Would I send this paper to one of my colleagues to read? \n\n  I would definitely send the paper to my colleagues to read. \n\n\n- In terms of quality:  \n\nClear motivation; substantiated literature review; but the algorithms proposed are not novel and the question of whether the method will scale to more unknown parameters is not answered. \n\n- I terms of clarity:  \n\nEasy to read.–Experimental evaluation is clearly presented.\n\n- Originality:  The problem of developing an automated curriculum for learning generalization over environment settings for a given RL task is formulated as a multi-armed bandit problem, and EXP3 algorithm is used to minimize regret and maximize the actor’s rewards. Itis a very interesting application of EXP3, although such inspiration is drawn from a former multi-task NLP paper Graves et al. (2017).\n\n- In terms of significance:  \n\n The paper is definitely interesting and presents an  promising  direction. The significance is  limited because of the simplicity of the examples considered in the experimental session. It would be interesting to see how this method performs in problems with more states and more unknown parameters.   \n\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}