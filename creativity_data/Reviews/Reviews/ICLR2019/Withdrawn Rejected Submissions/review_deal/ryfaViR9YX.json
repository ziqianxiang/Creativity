{
    "Decision": {
        "metareview": "The authors propose a generative model based on variational autoencoders that provides means to manipulate the high-level attributes of a given input. The attributes can be either pre-defined ground truth attributes or unknown attributes automatically discovered from the data.\n\nWhile the reviewers acknowledged the potential usefulness of the proposed approach, they raised important concerns that were viewed by AC as a critical issue: (1) very limited experimental evaluation (e.g. no baseline or ablation results, no quantitative results); comparisons on other more complex datasets and more in-depth analysis would substantially strengthen the evaluation and would allow to assess the scope of the contribution of this work  – see, for example, R3’s suggestion to use other dataset like dSprites or CelebA, where the ground truth attributes are known; (2) lack of presentation clarity – see R2’s latest comment how to improve.\n\nA general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. It needs clarification, more empirical studies and polish to achieve the desired goal.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Meta-Review"
    },
    "Reviews": [
        {
            "title": "Paper lacking an experimental section",
            "review": "This paper introduces a new framework for learning an interpretable representation of images and their attributes. The authors suggest decomposing the representation into a set of 'template' latent features, and a set of attribute-based features. The attribute-based features can be either 'free', i.e. discovered from the data, or 'fixed', i.e. based on the ground truth attributes. The authors encourage the decomposition of the latent space into the 'template' and the 'attributes' features by training a discriminator network to predict whether the attributes and the template features come from the same image or not.\n\nWhile the idea is interesting, the paper is lacking an experimental section, so the methodology is impossible to evaluate. Furthermore, while the authors spend many pages describing their methodology, the writing is often hard to follow, so I am still confused about the exact implementation of the attribute features \\phi(x, m) for example. The authors do point to the Appendix for their Experiments section, however this is not a good idea. The paper should be self-contained and the authors should not assume that their readers will read the information presented in the Appendix, which is always optional. \n\nUnfortunately, even the experimental section presented in the Appendix is not comprehensive enough to evaluate the proposed method. The authors train the model on a single dataset (MNIST), no baseline or ablation results are presented, and all the results are purely qualitative. Given that the ground truth attribute decomposition for MNIST is not known, even the qualitative results are impossible to evaluate. I recommend that the authors present quantitive results in the updated version of their paper (i.e. disentanglement metric scores, the log-likelihood of the reconstructions), including new experiments on a dataset like dSprites or CelebA, where the ground truth attributes are known.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting Work",
            "review": "The paper proposes a generative network capable of generating variations of a given input, conditioned on an attribute. Earlier papers generated variations of the input in the presence of the attribute and this attribute was assumed to be known during training. This paper proposes to automatically discover these attribute and thus work to produce variations even in the absence of known attribute information.\n\nThe paper is dense, but it is well written. It has mixed ideas from several papers - the basic VAE architecture, combined with a discriminator and regularizations over latent space. The key thing, of course, is the design of the attribute function. There seems to be an interesting interaction between the encoder, discriminator and the attribute function that requires more investigation. This is acknowledged in the conclusion as well.\n\nThe work is original and the results on the MNIST dataset are very interesting. I think the significance of this work lies in the fact that this can be a starting point for several interesting future works in this direction.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Lack of clarity and almost no experiments",
            "review": "This paper proposes a generalization of variational auto-encoders to account for meta-data (attributes), learning new ones, in a way that these can be controlled to generate new samples. The model learns how to decouple the attributes in an adversarial way by means of a discriminator. The problem is interesting, but I found two main issues with this paper:\n1.- Lack of clarity: I found the paper difficult to follow, even after reading Sec. 2 and 3 several times.\n2.- Almost absence of experiments: The paper only has one experiment, which is in the appendix, and is about sampling using the MNIST dataset. Given that this paper proposes a model, whose properties can be assessed by means of experiments, the fact that there is nothing of the kind provides no support to any benefits the model may have.\n\nOther points:\nWhat in the model prevents the solution of z_* being just random (independently of x)?\n\nThis paper seems relevant Esser, Patrick, Ekaterina Sutter, and Björn Ommer. \"A Variational U-Net for Conditional Appearance and Shape Generation.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}