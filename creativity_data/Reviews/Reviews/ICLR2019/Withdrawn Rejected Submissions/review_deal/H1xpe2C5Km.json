{
    "Decision": {
        "metareview": "This paper proposes a method for tracing activations in a capsule-based network in order to obtain semantic segmentation from classification predictions.\n\nReviewers 1 and 2 rate the paper as marginally above threshold, while Reviewer 3 rates it as marginally below. Reviewer 3 particularly points to experimental validation as a major weakness, stating: \"not sure if the method will generalize well beyond MNIST\", \"I’m concerned that the results are not transferable to other datasets and that the method shines promising just because of the simple datasets only.\"\n\nThe AC shares these concerns and does not believe the current experimental validation is sufficient. MNIST is a toy dataset, and may have been appropriate for introducing capsules as a new concept, but it is simply not difficult enough to serve as a quantitative benchmark to distinguish capsule performance from U-Net. U-Net and Tr-CapsNet appear to have similar performance on both MNIST and the hippocampus dataset; the relatively small advantage to Tr-CapsNet is not convincing.\n\nFurthermore, as Reviewer 1 suggests, it would seem appropriate to include experimental comparison to other capsule-based segmentation approaches (e.g. LaLonde and Bagci, Capsules for Object Segmentation, 2018). This related work is mentioned, but not used as an experimental baseline.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "metareview: insufficient experimental validation"
    },
    "Reviews": [
        {
            "title": "Original and interesting, requires further explanation of the architecture and experiment on multi-class segmentation",
            "review": "Authors present a trace-back mechanism to associate lowest level of Capsules with their respective classes. Their method effectively gets better segmentation results on the two (relatively small) datasets. \n\nAuthors explore an original idea with good quality of experiments (relatively strong baseline, proper experimental setup). They also back up their claim on advantage of classification with the horizontal redaction experiment. \nThe manuscript can benefit from a more clear description of the architecture used for each set of experiments. Specially how the upsampling is connected to the traceback layer.\nThis is an interesting idea that can probably generalize to CNNs with attention and tracing back the attention in a typical CNN as well.\n\nPros:\nThe idea behind tracing the part-whole assignments back to primary capsule layer is interesting and original. It increases the resolution significantly in compare to disregarding the connections in the encoder (up to class capsules). \n\nThe comparisons on MNIST & the Hippocampus dataset w.r.t the U-Net baseline are compelling and indicate a significant performance boost. \n\nCons:\nAlthough the classification signal is counted as the advantage of this system, it is not clear how it will adopt to multi-class scenarios which is one of the major applications of segmentation (such as SUN dataset).\n\nThe assumption that convolutional capsules can have multiple parents is incorrect. In Hinton 2018, where they use convolutional Capsule layers, the normalization for each position of a capsule in layer below is done separately and each position of each capsule type has the one-parent assumption. However, since in this work only primary capsules and class capsules are used this does not concern the current experiment results in this paper.\n\nThe related work section should expand more on the SOTA segmentation techniques and the significance of this work including [2].\n\nQuestion: \nHow is the traceback layer converted to image mask? After one gets p(c_k | i) for all primary capsules, are primary capsule pose parameters multiplied by their p(c_k |i ) and passed all to a deconv layer? Authors should specify in the manuscript the details of the upsampling layer (s) used in their architecture. It is only mentioned that deconv, dilated, bilinear interpolation are options. Which one is used in the end and how many is not clear. \n\n\nComments:\nFor the Hippocampus dataset, the ensemble U-Net approach used in [1] is close to your baseline and should be mentioned cited as the related work, SOTA on the dataset. Also since they use all 9 views have you considered accessing all the 9 views as well?\n\n\n[1]: Hippocampus segmentation through multi-view ensemble ConvNets\nYani Chen ; Bibo Shi ; Zhewei Wang ; Pin Zhang ; Charles D. Smith ; Jundong Liu\n[2]: RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation\nGuosheng Lin, Anton Milan, Chunhua Shen, Ian Reid",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A neat idea",
            "review": "This paper proposes a traceback layer for capsule networks to do semantic segmentation. Comparing to previous works that use capsule networks for semantic segmentation, this paper makes explicit use of part-whole relationship in the capsule layers. Experiments are done on modified MNIST and Hippocampus dataset. Results demonstrate encouraging improvements over U-Net. The writing could be tremendously improved if some background of the capsule networks is included. \n\nI have a question about the traceback layer. It seems to me that the traceback layer re-uses the learned weights c_{ij} between the primary capsules and the class capsules as guidance when “distributing” class probabilities to a spatial class probabilistic heatmap. One piece of information I feel missing is the affine transformation that happens between the primary capsule and the class capsule. The traceback layer doesn’t seem to invert such a transformation. Should it do so? \n\nSince there have been works that use capsule networks for semantic segmentation, does it make sense to compare to them (e.g. LaLonde & Bagci, 2018) ?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good paper which seems technically correct. Not sure if the method will generalize well beyond MNIST.",
            "review": "Based on the CapsNet concept of Sabour the authors proposed a trace-back method to perform a semantic segmentation in parallel to classification. The method is evaluate on MNIST and the Hippocampus dataset.\n\nThe paper is well-written and well-explained. Nevertheless, I think it would be useful to have some illustrations about the network architecture. Some stuff which is explained in text could be easily visualized in a flow chart. For example, the baseline architecture and your Tr-CapsNet could be easily explained via a flow chart. With the text only, it is hard to follow. Please think about some plots in the final version or in the appendix. One question which is aligned to that: How many convolutional filters are used in the baseline model?\n\nAdditionally, think about a pseudo-code for improved understandability. \n\nSome minor concerns/ notes to the authors:\n1.\tAt page 5: You mentioned that the parameters lambda1 and lambda 2 are important hyper-parameters to tune. But in the results you are not explaining how the parameters were tuned. So my question is: How do you tuned the parameters? In which range do you varied the parameters?\n2.\tPage 6; baseline model: Why do you removed the pooling layers?\n3.\tI’m curious about the number of parameters in each model. To have a valid discussion about your model is better than the U-Net-6 architecture, I would take into account the number of parameters. In case that your model is noticeably greater, it could be that your increased performance is just due to more parameters. As long as your discussion is without the number of parameters I’m not convinced that your model is better. A comparison between models should be always fair if two models are architectural similar.\n4.\tWhy is the magnitude of lambda1 so different between the two dataset that you used?\n5.\tCould you add the inference times to your tables and discuss that in addition?\n6.\tWhat kind of noise is added to MNIST?\n7.\tWhat is the state-of-the-art performance on the Hippocampus dataset?\n8.\tWhat would be the performance in your experiments with a MaskRCNN segmentation network?\n9.\tI’m not familiar with the Hippocampus dataset. I missed a reference where the data is available or some explaining illustrations. \n10.\tFor both datasets, more illustrations about the segmentation performance would be fine to evaluate your method. At least in the appendix…\n\t\nMy major concern is that both datasets are not dealing with real background noise. I’m concerned that the results are not transferable to other datasets and that the method shines promising just because of the simple datasets only. For example, due to the black background MNIST digits are well separated (if we skip that you added some kind of noise). So, from that point of view your results are not convincing and the discussion of your results appearing sparse and not complete.\nTo make your results transparent you could think about to publish the code somewhere.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}